formatted_method,if_statement
"def __init__(self, scale, factor, mode): self.index = 0 self.scale = scale if factor is None: self._log_factor = None else: <IF_STMT> raise ValueError(""'factor' must be >= 1.0"") self._log_factor = np.log(factor) if mode not in self.allowed_modes: raise ValueError(""'{0}' is not a recognized mode. Please select from: {1}"".format(mode, self.allowed_modes)) self.mode = mode",if factor < 1.0:
def get_grab_keys(self): keystr = None try: keys = self.display.get_grab_keys() for k in keys: <IF_STMT> keystr = gtk.gdk.keyval_name(k) else: keystr = keystr + '+' + gtk.gdk.keyval_name(k) except: pass return keystr,if keystr is None:
"def _checkAllExamples(self, num_type): for region_code in phonenumberutil.SUPPORTED_REGIONS: numobj_py = phonenumberutil.example_number_for_type(region_code, num_type) <IF_STMT> numobj_pb = PyToPB(numobj_py) alt_py = PBToPy(numobj_pb) self.assertEqual(numobj_py, alt_py)",if numobj_py is not None:
"def _gaf10iterator(handle): for inline in handle: if inline[0] == '!': continue inrec = inline.rstrip('\n').split('\t') <IF_STMT> continue inrec[3] = inrec[3].split('|') inrec[5] = inrec[5].split('|') inrec[7] = inrec[7].split('|') inrec[10] = inrec[10].split('|') inrec[12] = inrec[12].split('|') yield dict(zip(GAF10FIELDS, inrec))",if len(inrec) == 1:
"def __xor__(self, other): inc, exc = _norm_args_notimplemented(other) if inc is NotImplemented: return NotImplemented if inc is NotImplemented: return NotImplemented if self._included is None: <IF_STMT> return _ComplementSet(excluded=self._excluded - inc) else: return _ComplementSet(included=self._excluded.symmetric_difference(exc)) elif inc is None: return _ComplementSet(excluded=exc - self._included) else: return _ComplementSet(included=self._included.symmetric_difference(inc))",if exc is None:
"def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute('PRAGMA synchronous = 0;') yield con if commit_on_success and self.can_commit: con.commit() finally: <IF_STMT> con.close()",if not self._bulk_commit:
"def renderable_events(self, date, hour): """"""Returns the number of renderable events"""""" renderable_events = [] for event in self.events: if event.covers(date, hour): renderable_events.append(event) if hour: for current in renderable_events: for event in self.events: <IF_STMT> for hour in range(self.start_hour, self.end_hour): if current.covers(date, hour) and event.covers(date, hour): renderable_events.append(event) break return renderable_events",if event not in renderable_events:
"def _prepare_cooldowns(self, ctx): if self._buckets.valid: dt = ctx.message.edited_at or ctx.message.created_at current = dt.replace(tzinfo=datetime.timezone.utc).timestamp() bucket = self._buckets.get_bucket(ctx.message, current) retry_after = bucket.update_rate_limit(current) <IF_STMT> raise CommandOnCooldown(bucket, retry_after)",if retry_after:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue <IF_STMT> self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def n_import_from(self, node): relative_path_index = 0 if self.version >= 2.5: if node[relative_path_index].pattr > 0: node[2].pattr = '.' * node[relative_path_index].pattr + node[2].pattr <IF_STMT> if isinstance(node[1].pattr, tuple): imports = node[1].pattr for pattr in imports: node[1].pattr = pattr self.default(node) return pass self.default(node)",if self.version > 2.7:
def logic(): while 1: yield a var = 0 for i in downrange(len(a)): <IF_STMT> var += 1 out.next = var,if a[i] == 1:
"def _extract_networks(self, server_node): """"""Marshal the networks attribute of a parsed request"""""" node = self.find_first_child_named(server_node, 'networks') if node is not None: networks = [] for network_node in self.find_children_named(node, 'network'): item = {} <IF_STMT> item['uuid'] = network_node.getAttribute('uuid') if network_node.hasAttribute('fixed_ip'): item['fixed_ip'] = network_node.getAttribute('fixed_ip') networks.append(item) return networks else: return None",if network_node.hasAttribute('uuid'):
"def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) <IF_STMT> accum.append(arg) elif isinstance(arg, ModelAlias): accum.extend(arg.get_proxy_fields()) elif isclass(arg) and issubclass(arg, Model): accum.extend(arg._meta.declared_fields) return accum","elif isinstance(arg, Query):"
"def on_show_comment(self, widget, another): if widget.get_active(): <IF_STMT> self.treeview.update_items(all=True, comment=True) else: self.treeview.update_items(comment=True) elif another.get_active(): self.treeview.update_items(all=True) else: self.treeview.update_items()",if another.get_active():
"def test_select_figure_formats_set(): ip = get_ipython() for fmts in [{'png', 'svg'}, ['png'], ('jpeg', 'pdf', 'retina'), {'svg'}]: active_mimes = {_fmt_mime_map[fmt] for fmt in fmts} pt.select_figure_formats(ip, fmts) for mime, f in ip.display_formatter.formatters.items(): <IF_STMT> nt.assert_in(Figure, f) else: nt.assert_not_in(Figure, f)",if mime in active_mimes:
"def update_from_data(self, data): super(HelpParameter, self).update_from_data(data) if self.value_sources: self.value_sources = [str_or_dict <IF_STMT> else {'link': {'command': str_or_dict}} for str_or_dict in self.value_sources]","if isinstance(str_or_dict, dict)"
def _reset_library_root_logger() -> None: global _default_handler with _lock: <IF_STMT> return library_root_logger = _get_library_root_logger() library_root_logger.removeHandler(_default_handler) library_root_logger.setLevel(logging.NOTSET) _default_handler = None,if not _default_handler:
"def extract_headers(headers): """"""This function extracts valid headers from interactive input."""""" sorted_headers = {} matches = re.findall('(.*):\\s(.*)', headers) for match in matches: header = match[0] value = match[1] try: <IF_STMT> value = value[:-1] sorted_headers[header] = value except IndexError: pass return sorted_headers","if value[-1] == ',':"
"def _call_user_data_handler(self, operation, src, dst): if hasattr(self, '_user_data'): for key, (data, handler) in self._user_data.items(): <IF_STMT> handler.handle(operation, key, data, src, dst)",if handler is not None:
"def update(self, other=None, **kwargs): if other is not None: <IF_STMT> other = other.items() for key, value in other: if key in kwargs: raise TensorforceError.value(name='NestedDict.update', argument='key', value=key, condition='specified twice') self[key] = value for key, value in kwargs.items(): self[key] = value","if hasattr(other, 'items'):"
def _restore_context(context): for cvar in context: try: <IF_STMT> cvar.set(context.get(cvar)) except LookupError: cvar.set(context.get(cvar)),if cvar.get() != context.get(cvar):
"def __str__(self): s = '{' sep = '' for k, v in self.iteritems(): s += sep <IF_STMT> s += ""'%s'"" % k else: s += str(k) s += ': ' if type(v) == str: s += ""'%s'"" % v else: s += str(v) sep = ', ' s += '}' return s",if type(k) == str:
"def read_file_or_url(self, fname): if isinstance(fname, file): result = open(fname, 'r') else: match = self.urlre.match(fname) <IF_STMT> result = urllib.urlopen(match.group(1)) else: fname = os.path.expanduser(fname) try: result = open(os.path.expanduser(fname), 'r') except IOError: result = open('%s.%s' % (os.path.expanduser(fname), self.defaultExtension), 'r') return result",if match:
"def subclass_managers(self, recursive): for cls in self.class_.__subclasses__(): mgr = manager_of_class(cls) <IF_STMT> yield mgr if recursive: for m in mgr.subclass_managers(True): yield m",if mgr is not None and mgr is not self:
"def star_path(path): """"""Replace integers and integer-strings in a path with *"""""" path = list(path) for i, p in enumerate(path): if isinstance(p, int): path[i] = '*' else: if not isinstance(p, text_type): p = p.decode() <IF_STMT> path[i] = '*' return join_path(path)",if r_is_int.match(p):
"def cookie_decode(data, key): """"""Verify and decode an encoded string. Return an object or None"""""" if isinstance(data, unicode): data = data.encode('ascii') if cookie_is_encoded(data): sig, msg = data.split(u'?'.encode('ascii'), 1) <IF_STMT> return pickle.loads(base64.b64decode(msg)) return None","if sig[1:] == base64.b64encode(hmac.new(key, msg).digest()):"
"def parse_row(cls, doc_row): row = {} for field_name, field in FIELD_MAP.items(): if len(doc_row) > field[1]: field_value = doc_row[field[1]] else: field_value = '' <IF_STMT> field_value = field[2](field_value) row[field_name] = field_value return row",if len(field) >= 3 and callable(field[2]):
"def semantic_masks(self): for sid in self._seg_ids: sinfo = self._sinfo.get(sid) <IF_STMT> continue yield ((self._seg == sid).numpy().astype(np.bool), sinfo)",if sinfo is None or sinfo['isthing']:
"def top_level_subjects(self): if self.subjects.exists(): return optimize_subject_query(self.subjects.filter(parent__isnull=True)) else: <IF_STMT> return optimize_subject_query(Subject.objects.filter(parent__isnull=True, provider___id='osf')) tops = set([sub[0][0] for sub in self.subjects_acceptable]) return [Subject.load(sub) for sub in tops]",if len(self.subjects_acceptable) == 0:
"def resolve(obj): if isinstance(obj, list): for item in obj: resolve(item) return if isinstance(obj, dict): <IF_STMT> with resolver.resolving(obj[u'$ref']) as resolved: resolve(resolved) obj.clear() obj.update(resolved) else: for value in obj.values(): resolve(value)",if '$ref' in obj:
"def read_ansible_config(project_path, variables_of_interest): fnames = ['/etc/ansible/ansible.cfg'] if project_path: fnames.append(os.path.join(project_path, 'ansible.cfg')) values = {} try: parser = ConfigParser() parser.read(fnames) if 'defaults' in parser: for var in variables_of_interest: <IF_STMT> values[var] = parser['defaults'][var] except Exception: logger.exception('Failed to read ansible configuration(s) {}'.format(fnames)) return values",if var in parser['defaults']:
"def test_globalphase(): rule_set = DecompositionRuleSet(modules=[globalphase, r2rzandph]) dummy = DummyEngine(save_commands=True) eng = MainEngine(dummy, [AutoReplacer(rule_set), InstructionFilter(low_level_gates_noglobalphase)]) qubit = eng.allocate_qubit() R(1.2) | qubit rz_count = 0 for cmd in dummy.received_commands: assert not isinstance(cmd.gate, R) <IF_STMT> rz_count += 1 assert cmd.gate == Rz(1.2) assert rz_count == 1","if isinstance(cmd.gate, Rz):"
def _kill_current_player(self): if self._current_player: <IF_STMT> self.voice_client.resume() try: self.voice_client.stop() except OSError: pass self._current_player = None return True return False,if self.voice_client.is_paused():
"def hasAmbiguousLanguage(self, p): """"""Return True if p.b contains different @language directives."""""" languages, tag = (set(), '@language') for s in g.splitLines(p.b): <IF_STMT> i = g.skip_ws(s, len(tag)) j = g.skip_id(s, i) word = s[i:j] languages.add(word) return len(list(languages)) > 1","if g.match_word(s, 0, tag):"
"def terminate(self): n_retries = 10 for i in range(n_retries): try: super(MemmappingPool, self).terminate() break except OSError as e: if isinstance(e, WindowsError): sleep(0.1) <IF_STMT> warnings.warn('Failed to terminate worker processes in multiprocessing pool: %r' % e) self._temp_folder_manager._unlink_temporary_resources()",if i + 1 == n_retries:
"def test_downsampling(self, method, maybe_range, fraction, expected_n_reads): reader = sam.SamReader(test_utils.genomics_core_testdata('test.bam'), downsample_fraction=fraction, random_seed=12345) with reader: <IF_STMT> reads_iter = reader.iterate() elif method == 'query': reads_iter = reader.query(ranges.parse_literal(maybe_range)) else: self.fail('Unexpected method ' + str(method)) self.assertEqual(test_utils.iterable_len(reads_iter), expected_n_reads)",if method == 'iterate':
def verify_acceptable(self): start = time.time() while True: <IF_STMT> return elif time.time() - start > READ_TIMEOUT: raise Exception('Server socket did not accept in time') time.sleep(0.1),if self.select_acceptable():
"def replica_local_creator(next_creator, **kwargs) -> tf.Variable: """"""Variable creator that by default creates replica local variables."""""" if kwargs['synchronization'] == tf.VariableSynchronization.AUTO: kwargs['synchronization'] = tf.VariableSynchronization.ON_READ <IF_STMT> kwargs['aggregation'] = tf.VariableAggregation.ONLY_FIRST_REPLICA if kwargs['trainable'] is None: kwargs['trainable'] = True return next_creator(**kwargs)",if kwargs['aggregation'] == tf.VariableAggregation.NONE:
"def get_optional_nargs(self, name): for n, kwargs in self.conf['optional_args']: if name == n: <IF_STMT> action = kwargs['action'] if action in ('store_true', 'store_false'): return 0 break return 1",if 'action' in kwargs:
"def put(self, userId, bucket, key, data): if not self.initialized: raise Exception('archive not initialized') try: uri = self.uri_for(userId, bucket, key) <IF_STMT> raise Exception('Failed writing file content to disk: {}'.format(uri)) else: return uri except Exception as err: logger.debug('cannot put data: exception - ' + str(err)) raise err","if not self._save_content(uri, data):"
"def get_range(min, max): if max < min: min, max = (max, min) elif min == max: if min < 0: min, max = (2 * min, 0) <IF_STMT> min, max = (0, 2 * min) else: min, max = (-1, 1) return (min, max)",elif min > 0:
"def update_job_weights(): """"""Update job weights."""""" for job in data_types.Job.query(): multiplier = DEFAULT_MULTIPLIER if environment.is_engine_fuzzer_job(job.name): targets_count = ndb.Key(data_types.FuzzTargetsCount, job.name).get() if targets_count and targets_count.count: multiplier = targets_count.count <IF_STMT> multiplier = TARGET_COUNT_WEIGHT_CAP update_job_weight(job.name, multiplier)",if multiplier > TARGET_COUNT_WEIGHT_CAP:
"def _validate_required_settings(self, application_id, application_config, required_settings, should_throw=True): """"""All required keys must be present"""""" for setting_key in required_settings: if setting_key not in application_config.keys(): <IF_STMT> raise ImproperlyConfigured(MISSING_SETTING.format(application_id=application_id, setting=setting_key)) else: return False return True",if should_throw:
"def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): if isinstance(value, dict): <IF_STMT> if not isinstance(org_dict[key], dict): raise ValueError('Mismatch between org_dict and upd_dict at node {}'.format(key)) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value",if key in org_dict:
"def eintr_retry_call(func, *args, **kwargs): while True: try: return func(*args, **kwargs) except EnvironmentError as e: <IF_STMT> continue raise","if getattr(e, 'errno', None) == errno.EINTR:"
"def __init__(self, entity): self._entity = weakref.proxy(entity) self._observables = collections.OrderedDict() self._keys_helper = _ObservableKeys(self._entity, self._observables) for attr_name in sorted(dir(type(self))): type_attr = getattr(type(self), attr_name) <IF_STMT> self._observables[attr_name] = getattr(self, attr_name)","if isinstance(type_attr, define.observable):"
"def check_redundancy(self): starts, sizes = self.allocator.get_allocated_regions() last = -1 for start, size in zip(starts, sizes): if start < last: raise Exception('Block at %d is out of order' % start) <IF_STMT> raise Exception('Block at %d is redundant' % start) last = start + size",if start == last:
"def elfheader(): local_path = pwndbg.file.get_file(pwndbg.proc.exe) with open(local_path, 'rb') as f: elffile = ELFFile(f) sections = [] for section in elffile.iter_sections(): start = section['sh_addr'] <IF_STMT> continue size = section['sh_size'] sections.append((start, start + size, section.name)) sections.sort() for start, end, name in sections: print('%#x - %#x ' % (start, end), name)",if start == 0:
"def orbit(): """"""Define the internal thread for running the orbit."""""" for point in points: self.set_position(point) self.set_focus(focus) self.set_viewup(viewup) self.renderer.ResetCameraClippingRange() self.render() time.sleep(step) <IF_STMT> self.write_frame()",if write_frames:
"def json_format(self): """"""Returns the integer value formatted as a JSON literal"""""" fmt = self._jsonfmt if fmt == NUMBER_FORMAT_HEX: return format(self, '#x') elif fmt == NUMBER_FORMAT_OCTAL: return format(self, '#o') elif fmt == NUMBER_FORMAT_BINARY: return format(self, '#b') elif fmt == NUMBER_FORMAT_LEGACYOCTAL: if self == 0: return '0' <IF_STMT> return '-0%o' % -self else: return '0%o' % self else: return str(self)",elif self < 0:
"def parseTime(timeStr): regex = re.compile(constants.PARSE_TIME_REGEX) parts = regex.match(timeStr) if not parts: return parts = parts.groupdict() time_params = {} for name, param in parts.items(): <IF_STMT> if name == 'miliseconds': time_params['microseconds'] = int(param) * 1000 else: time_params[name] = int(param) return datetime.timedelta(**time_params).total_seconds()",if param:
"def build_extension(self, ext): ext._convert_pyx_sources_to_lang() _compiler = self.compiler try: if isinstance(ext, Library): self.compiler = self.shlib_compiler _build_ext.build_extension(self, ext) <IF_STMT> cmd = self.get_finalized_command('build_py').build_lib self.write_stub(cmd, ext) finally: self.compiler = _compiler",if ext._needs_stub:
"def __init__(self, type, data, name=None): Constant.__init__(self, type, data, name) self.tag.unique_value = None if isinstance(data, np.ndarray) and data.ndim > 0: flat_data = data.ravel() <IF_STMT> if (flat_data == flat_data[0]).all(): self.tag.unique_value = flat_data[0]",if flat_data.shape[0]:
"def _find_machine(deb_arch): for machine in _ARCH_TRANSLATIONS: if _ARCH_TRANSLATIONS[machine].get('deb', '') == deb_arch: return machine <IF_STMT> return machine raise errors.SnapcraftEnvironmentError('Cannot set machine from deb_arch {!r}'.format(deb_arch))","elif _ARCH_TRANSLATIONS[machine].get('uts_machine', '') == deb_arch:"
"def fields_for_form(form, only_fields, exclude_fields): fields = OrderedDict() for name, field in form.fields.items(): is_not_in_only = only_fields and name not in only_fields is_excluded = name in exclude_fields <IF_STMT> continue fields[name] = convert_form_field(field) return fields",if is_not_in_only or is_excluded:
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): if readies[idx] < min_counts[idx]: all_satisfy = False readies[idx] = count_fun(selector) break <IF_STMT> break if timeout and timeout + start_time < time.time(): raise TimeoutError('Wait cluster start timeout') time.sleep(1)",if all_satisfy:
def count_brokers(self): self.nb_brokers = 0 for broker in self.brokers: <IF_STMT> self.nb_brokers += 1 for realm in self.higher_realms: for broker in realm.brokers: if not broker.spare and broker.manage_sub_realms: self.nb_brokers += 1,if not broker.spare:
"def _adapt_polymorphic_element(self, element): if 'parententity' in element._annotations: search = element._annotations['parententity'] alias = self._polymorphic_adapters.get(search, None) <IF_STMT> return alias.adapt_clause(element) if isinstance(element, expression.FromClause): search = element elif hasattr(element, 'table'): search = element.table else: return None alias = self._polymorphic_adapters.get(search, None) if alias: return alias.adapt_clause(element)",if alias:
"def get_all_methods(): estimators = all_estimators() for name, Estimator in estimators: if name.startswith('_'): continue methods = [] for name in dir(Estimator): if name.startswith('_'): continue method_obj = getattr(Estimator, name) <IF_STMT> methods.append(name) methods.append(None) for method in sorted(methods, key=lambda x: str(x)): yield (Estimator, method)","if hasattr(method_obj, '__call__') or isinstance(method_obj, property):"
"def __call__(self, es, params): ops = 0 indices = mandatory(params, 'indices', self) only_if_exists = params.get('only-if-exists', False) request_params = params.get('request-params', {}) for index_name in indices: if not only_if_exists: es.indices.delete(index=index_name, params=request_params) ops += 1 <IF_STMT> self.logger.info('Index [%s] already exists. Deleting it.', index_name) es.indices.delete(index=index_name, params=request_params) ops += 1 return (ops, 'ops')",elif only_if_exists and es.indices.exists(index=index_name):
"def get(): result = [] for b in self.key_bindings: <IF_STMT> match = True for i, j in zip(b.keys, keys): if i != j and i != Keys.Any: match = False break if match: result.append(b) return result",if len(keys) < len(b.keys):
"def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: <IF_STMT> result.append(arg_type.dtype) elif isinstance(arg_type, VectorArg): result.append(None) if arg_type.with_offset: result.append(np.int64) else: raise RuntimeError('arg type not understood: %s' % type(arg_type)) return result","if isinstance(arg_type, ScalarArg):"
def autocommitter(): while True: try: if not self._running: break <IF_STMT> self._auto_commit() self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000) except ReferenceError: break except Exception: self._worker_exception = sys.exc_info() break log.debug('Autocommitter thread exiting'),if self._auto_commit_enable:
"def on_conflict(self, *target_fields: Union[str, Term]) -> 'PostgreSQLQueryBuilder': if not self._insert_table: raise QueryException('On conflict only applies to insert query') self._on_conflict = True for target_field in target_fields: <IF_STMT> self._on_conflict_fields.append(self._conflict_field_str(target_field)) elif isinstance(target_field, Term): self._on_conflict_fields.append(target_field)","if isinstance(target_field, str):"
def change_TV_DOWNLOAD_DIR(tv_download_dir): if tv_download_dir == '': sickbeard.TV_DOWNLOAD_DIR = '' return True if os.path.normpath(sickbeard.TV_DOWNLOAD_DIR) != os.path.normpath(tv_download_dir): <IF_STMT> sickbeard.TV_DOWNLOAD_DIR = os.path.normpath(tv_download_dir) logger.log(u'Changed TV download folder to ' + tv_download_dir) else: return False return True,if helpers.makeDir(tv_download_dir):
"def save_config(self, cmd='save config', confirm=True, confirm_response='y'): """"""Saves Config."""""" self.enable() if confirm: output = self.send_command_timing(command_string=cmd) <IF_STMT> output += self.send_command_timing(confirm_response) else: output += self.send_command_timing(self.RETURN) else: output = self.send_command(command_string=cmd) return output",if confirm_response:
"def apply_gradient_for_batch(inputs, labels, weights, loss): with tf.GradientTape() as tape: outputs = self.model(inputs, training=True) <IF_STMT> outputs = [outputs] if self._loss_outputs is not None: outputs = [outputs[i] for i in self._loss_outputs] batch_loss = loss(outputs, labels, weights) if variables is None: vars = self.model.trainable_variables else: vars = variables grads = tape.gradient(batch_loss, vars) self._tf_optimizer.apply_gradients(zip(grads, vars)) self._global_step.assign_add(1) return batch_loss","if isinstance(outputs, tf.Tensor):"
"def sort(self, items): slow_sorts = [] switch_slow = False for sort in reversed(self.sorts): <IF_STMT> slow_sorts.append(sort) elif sort.order_clause() is None: switch_slow = True slow_sorts.append(sort) else: pass for sort in slow_sorts: items = sort.sort(items) return items",if switch_slow:
"def getmod(self, nm): mod = None for thing in self.path: if isinstance(thing, basestring): owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) <IF_STMT> mod = owner.getmod(nm) else: mod = thing.getmod(nm) if mod: break return mod",if owner:
"def has(self, key): filename = self._get_filename(key) try: with open(filename, 'rb') as f: pickle_time = pickle.load(f) <IF_STMT> return True else: os.remove(filename) return False except (IOError, OSError, pickle.PickleError): return False",if pickle_time == 0 or pickle_time >= time():
"def forward(self, hs): h = self.c0(hs[-1]) for i in range(1, 8): h = F.concat([h, hs[-i - 1]]) <IF_STMT> h = self['c%d' % i](h) else: h = self.c7(h) return h",if i < 7:
def get_custom_behaviour2(self): string = '' for arg in list(self.defaults.keys()) + self.var: <IF_STMT> if str(arg) != str(self.__dict__[arg]): string += str(arg) + '=' + str(self.__dict__[arg]) + ';\n' return string,if arg in self.__dict__:
"def _apply_operation(self, values): """"""Method that defines the less-than-or-equal operation"""""" arg1 = next(values) for strict in self._strict: arg2 = next(values) if strict: if not arg1 < arg2: return False el<IF_STMT> return False arg1 = arg2 return True",if not arg1 <= arg2:
"def i_pshufb(self, op, off=0): dst = self.getOperValue(op, off) src = self.getOperValue(op, off) res = 0 if op.opers[0].tsize == 8: mask = 7 else: mask = 15 for i in range(op.opers[0].tsize): shfl = src & 1 << i * 8 + 7 <IF_STMT> s = 0 else: indx = src >> i * 8 & mask s = src >> indx * 8 & 255 res |= s << i * 8 self.setOperValue(op, 0, res)",if shfl:
"def report_out_of_quota(self, appid): self.logger.warn('report_out_of_quota:%s', appid) with self.lock: <IF_STMT> self.out_of_quota_appids.append(appid) try: self.working_appid_list.remove(appid) except: pass",if appid not in self.out_of_quota_appids:
"def to_py(self, value: _StrUnset) -> _StrUnsetNone: self._basic_py_validation(value, str) if isinstance(value, usertypes.Unset): return value elif not value: return None value = os.path.expandvars(value) value = os.path.expanduser(value) try: <IF_STMT> raise configexc.ValidationError(value, 'must be a valid directory!') if not os.path.isabs(value): raise configexc.ValidationError(value, 'must be an absolute path!') except UnicodeEncodeError as e: raise configexc.ValidationError(value, e) return value",if not os.path.isdir(value):
"def findinDoc(self, tagpath, pos, end): result = None if end == -1: end = self.docSize else: end = min(self.docSize, end) foundat = -1 for j in range(pos, end): item = self.docList[j] if item.find(b'=') >= 0: name, argres = item.split(b'=', 1) else: name = item argres = '' <IF_STMT> tagpath = tagpath.encode('utf-8') if name.endswith(tagpath): result = argres foundat = j break return (foundat, result)","if isinstance(tagpath, str):"
"def has_safe_repr(value): """"""Does the node have a safe representation?"""""" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: if not has_safe_repr(item): return False return True elif isinstance(value, dict): for key, value in value.iteritems(): if not has_safe_repr(key): return False <IF_STMT> return False return True return False",if not has_safe_repr(value):
"def run(self): for obj in bpy.context.scene.objects: <IF_STMT> obj_id = obj['modelId'] if obj_id in self.lights: self._make_lamp_emissive(obj, self.lights[obj_id]) if obj_id in self.windows: self._make_window_emissive(obj) if obj.name.startswith('Ceiling#'): self._make_ceiling_emissive(obj)",if 'modelId' in obj:
"def bitvector_case_fn(rng: Random, mode: RandomizationMode, size: int, invalid_making_pos: int=None): bits = get_random_ssz_object(rng, Bitvector[size], max_bytes_length=(size + 7) // 8, max_list_length=size, mode=mode, chaos=False) if invalid_making_pos is not None and invalid_making_pos <= size: already_invalid = False for i in range(invalid_making_pos, size): <IF_STMT> already_invalid = True if not already_invalid: bits[invalid_making_pos] = True return bits",if bits[i]:
"def get_transaction_execution_results(self, batch_signature): with self._condition: batch_status = self._batch_statuses.get(batch_signature) if batch_status is None: return None annotated_batch = self._batch_by_id.get(batch_signature) if annotated_batch is None: return None results = [] for txn in annotated_batch.batch.transactions: result = self._txn_results.get(txn.header_signature) <IF_STMT> results.append(result) return results",if result is not None:
"def one_xmm_reg_imm8(ii): i, j, n = (0, 0, 0) for op in _gen_opnds(ii): if op_reg(op) and op_xmm(op): n += 1 elif op_imm8(op): i += 1 <IF_STMT> j += 1 else: return False return n == 1 and i == 1 and (j <= 1)",elif op_imm8_2(op):
"def whichmodule(obj, name): """"""Find the module an object belong to."""""" module_name = getattr(obj, '__module__', None) if module_name is not None: return module_name for module_name, module in sys.modules.copy().items(): if module_name == '__main__' or module is None: continue try: <IF_STMT> return module_name except AttributeError: pass return '__main__'","if _getattribute(module, name)[0] is obj:"
def get_ld_header_info(p): info = [] for line in p.stdout: <IF_STMT> info.append(line) else: break return info,"if re.match('[0-9]', line):"
"def write(self, s): if self.closed: raise ValueError('write to closed file') if type(s) not in (unicode, str, bytearray): <IF_STMT> s = unicode.__getitem__(s, slice(None)) elif isinstance(s, str): s = str.__str__(s) elif isinstance(s, bytearray): s = bytearray.__str__(s) else: raise TypeError('must be string, not ' + type(s).__name__) return self.shell.write(s, self.tags)","if isinstance(s, unicode):"
"def generate_forwards(cls, attrs): for attr_name, attr in cls._forwards.__dict__.items(): <IF_STMT> continue if isinstance(attr, property): cls._forward.append(attr_name) elif isinstance(attr, types.FunctionType): wrapper = _forward_factory(cls, attr_name, attr) setattr(cls, attr_name, wrapper) else: raise TypeError(attr_name, type(attr))",if attr_name.startswith('_') or attr_name in attrs:
"def _user_has_dnd(bot, user_id): try: return bot.call_shared('dnd.user_check', user_id) except KeyError: logger.warning('mentions: falling back to legacy _user_has_dnd()') initiator_has_dnd = False <IF_STMT> donotdisturb = bot.memory.get('donotdisturb') if user_id in donotdisturb: initiator_has_dnd = True return initiator_has_dnd",if bot.memory.exists(['donotdisturb']):
"def init(self): """"""Initialize a fighter from the database and validate"""""" self.__item = None if self.itemID: self.__item = eos.db.getItem(self.itemID) <IF_STMT> pyfalog.error('Item (id: {0}) does not exist', self.itemID) return if self.isInvalid: pyfalog.error('Item (id: {0}) is not a Fighter', self.itemID) return self.build()",if self.__item is None:
"def _pg_sku_name_validator(sku_name, sku_info, tier): if sku_name: skus = get_postgres_skus(sku_info, tier) <IF_STMT> error_msg = 'Incorrect value for --sku-name. ' + 'The SKU name does not match {} tier. Specify --tier if you did not. '.format(tier) raise CLIError(error_msg + 'Allowed values : {}'.format(skus))",if sku_name not in skus:
"def _parse_paternity_log(writer, file): parent_map = {} parent_map[0] = 0 for line in file.read().decode('utf-8').split('\n'): <IF_STMT> continue elems = line.split(' ') if len(elems) >= 2: parent_map[int(elems[0])] = int(elems[1]) else: print(""Odd paternity line '%s'"" % line) return parent_map",if not line:
def _get_next_cap(self): self._curr_cap = None if self._curr_cap_idx is None: self._curr_cap_idx = 0 self._curr_cap = self._cap_list[0] return True else: <IF_STMT> self._end_of_video = True return False self._curr_cap_idx += 1 self._curr_cap = self._cap_list[self._curr_cap_idx] return True,if not self._curr_cap_idx + 1 < len(self._cap_list):
"def decode_payload(args): try: if args.token: token = args.token el<IF_STMT> token = sys.stdin.readline().strip() else: raise IOError('Cannot read from stdin: terminal not a TTY') token = token.encode('utf-8') data = decode(token, key=args.key, verify=args.verify) return json.dumps(data) except DecodeError as e: raise DecodeError('There was an error decoding the token: %s' % e)",if sys.stdin.isatty():
"def cell_double_clicked(self, row, column): if column == 3: archive_name = self.selected_archive_name() <IF_STMT> return mount_point = self.mount_points.get(archive_name) if mount_point is not None: QDesktopServices.openUrl(QtCore.QUrl(f'file:///{mount_point}'))",if not archive_name:
"def tiles_around(self, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx <IF_STMT> for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps",if nx >= 0 and nx < self.width:
"def __init__(self, type, data, name=None): Constant.__init__(self, type, data, name) self.tag.unique_value = None if isinstance(data, np.ndarray) and data.ndim > 0: flat_data = data.ravel() if flat_data.shape[0]: <IF_STMT> self.tag.unique_value = flat_data[0]",if (flat_data == flat_data[0]).all():
"def git_convert_standalone_clone(repodir): """"""If specified directory is a git repository, ensure it's a standalone clone"""""" import bb.process if os.path.exists(os.path.join(repodir, '.git')): alternatesfile = os.path.join(repodir, '.git', 'objects', 'info', 'alternates') <IF_STMT> bb.process.run('git repack -a', cwd=repodir) os.remove(alternatesfile)",if os.path.exists(alternatesfile):
"def _rename_recipe_file(oldrecipe, bpn, oldpv, newpv, path): oldrecipe = os.path.basename(oldrecipe) if oldrecipe.endswith('_%s.bb' % oldpv): newrecipe = '%s_%s.bb' % (bpn, newpv) <IF_STMT> shutil.move(os.path.join(path, oldrecipe), os.path.join(path, newrecipe)) else: newrecipe = oldrecipe return os.path.join(path, newrecipe)",if oldrecipe != newrecipe:
def profiling_startup(): if '--profile-sverchok-startup' in sys.argv: global _profile_nesting profile = None try: profile = get_global_profile() _profile_nesting += 1 <IF_STMT> profile.enable() yield profile finally: _profile_nesting -= 1 if _profile_nesting == 0 and profile is not None: profile.disable() dump_stats(file_path='sverchok_profile.txt') save_stats('sverchok_profile.prof') else: yield None,if _profile_nesting == 1:
"def to_scaled_dtype(val): """"""Parse *val* to return a dtype."""""" res = [] for i in val: <IF_STMT> res.append((i[0], i[1]) + i[2:-1]) else: try: res.append((i[0], i[-1].dtype) + i[2:-1]) except AttributeError: res.append((i[0], type(i[-1])) + i[2:-1]) return np.dtype(res)",if i[1].startswith('S'):
"def row(self, indx): if indx not in self.__rows: if indx in self.__flushed_rows: raise Exception('Attempt to reuse row index %d of sheet %r after flushing' % (indx, self.__name)) self.__rows[indx] = self.Row(indx, self) <IF_STMT> self.last_used_row = indx if indx < self.first_used_row: self.first_used_row = indx return self.__rows[indx]",if indx > self.last_used_row:
"def _flow_open(self): rv = [] for pipe in self.pipes: <IF_STMT> raise RuntimeError(f'{pipe.__class__.__name__} pipe has double open methods. Use `open` or `{self._method_open}`, not both.') if 'open' in pipe._pipeline_all_methods_: rv.append(pipe.open) if self._method_open in pipe._pipeline_all_methods_: rv.append(getattr(pipe, self._method_open)) return rv","if pipe._pipeline_all_methods_.issuperset({'open', self._method_open}):"
"def _parse_output(output, strict=False): for pkg in _yum_pkginfo(output): <IF_STMT> continue repo_dict = ret.setdefault(pkg.repoid, {}) version_list = repo_dict.setdefault(pkg.name, set()) version_list.add(pkg.version)","if strict and (pkg.repoid not in repos or not _check_args(args, pkg.name)):"
"def user_defined_os(): if menu.options.os: if menu.options.os.lower() == 'windows': settings.TARGET_OS = 'win' return True <IF_STMT> return True else: err_msg = ""You specified wrong value '"" + menu.options.os + ""' "" err_msg += ""as an operation system. The value, must be 'Windows' or 'Unix'."" print(settings.print_critical_msg(err_msg)) raise SystemExit()",elif menu.options.os.lower() == 'unix':
"def update(self, topLeft, bottomRight): if self._updating: return if self._index: if topLeft.row() <= self._index.row() <= bottomRight.row(): self.updateText() elif self._indexes: update = False for i in self._indexes: <IF_STMT> update = True if update: self.updateText()",if topLeft.row() <= i.row() <= bottomRight.row():
"def _wrapper(self, pipe, _should_terminate_flag, generator, *args, **kwargs): """"""Executed in background, pipes generator results to foreground"""""" logger.debug('Entering _wrapper') try: for datum in generator(*args, **kwargs): <IF_STMT> raise EarlyCancellationError('Task was cancelled') pipe.send(datum) except Exception as e: if not isinstance(e, EarlyCancellationError): pipe.send(e) import traceback logger.warning(traceback.format_exc()) else: pipe.send(StopIteration()) finally: pipe.close() logger.debug('Exiting _wrapper')",if _should_terminate_flag.value:
"def _flatten(*args): arglist = [] for arg in args: if isinstance(arg, _Block): <IF_STMT> arglist.append(arg.vhdl_code) continue else: arg = arg.subs if id(arg) in _userCodeMap['vhdl']: arglist.append(_userCodeMap['vhdl'][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist",if arg.vhdl_code is not None:
"def _get_target_and_lun(self, context, volume): iscsi_target = 0 if not self.target_name or not self._get_group(): lun = 1 return (iscsi_target, lun) luns = self._get_luns_info() if not luns or luns[0] != 1: lun = 1 return (iscsi_target, lun) else: for lun in luns: <IF_STMT> return (iscsi_target, lun + 1)",if luns[-1] == lun or luns[lun - 1] + 1 != luns[lun]:
"def check_find(ref): for c in set(m.used): start = 0 u = m.text while start < m.size: i = u.find(c, start) <IF_STMT> break self.assertEqual(u[i], c) self.assertGreaterEqual(i, start) start = i + 1",if i < 0:
"def _format_column_list(self, data): if 'columns' in data: for c in data['columns']: <IF_STMT> c['attacl'] = parse_priv_to_db(c['attacl'], self.column_acl) if 'cltype' in c: c['cltype'], c['hasSqrBracket'] = column_utils.type_formatter(c['cltype'])",if 'attacl' in c:
"def _animate_strategy(self, speed=1): if self._animating == 0: return if self._apply_strategy() is not None: if self._animate.get() == 0 or self._step.get() == 1: return <IF_STMT> self._root.after(3000, self._animate_strategy) elif self._animate.get() == 2: self._root.after(1000, self._animate_strategy) else: self._root.after(20, self._animate_strategy)",if self._animate.get() == 1:
"def close_all(map=None, ignore_all=False): if map is None: map = socket_map for x in list(map.values()): try: x.close() except OSError as x: if x.args[0] == EBADF: pass el<IF_STMT> raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()",if not ignore_all:
"def iter_imports(path): """"""Yield imports in *path*"""""" for node in ast.parse(open(path, 'rb').read()).body: if isinstance(node, ast.ImportFrom): if node.module is None: prefix = () else: prefix = tuple(node.module.split('.')) for snode in node.names: yield (node.level, prefix + (snode.name,)) <IF_STMT> for node in node.names: yield (0, tuple(node.name.split('.')))","elif isinstance(node, ast.Import):"
"def one_stage_eval_model(data_reader_eval, myModel, loss_criterion=None): score_tot = 0 n_sample_tot = 0 loss_tot = 0 for idx, batch in enumerate(data_reader_eval): score, loss, n_sample = compute_a_batch(batch, myModel, eval_mode=True, loss_criterion=loss_criterion) score_tot += score n_sample_tot += n_sample <IF_STMT> loss_tot += loss.data[0] * n_sample return (score_tot / n_sample_tot, loss_tot / n_sample_tot, n_sample_tot)",if loss is not None:
"def _process_preproc(self, token, content): if self.state == 'include': <IF_STMT> content = content.strip().strip('""').strip('<').strip('>').strip() self.append(content, truncate=True, separator='/') self.state = None elif content.strip().startswith('include'): self.state = 'include' else: self.state = None",if content != '\n' and content != '#':
"def _aggregate_metadata_attribute(self, attr, agg_func=np.max, default_value=0, from_type_metadata=True): attr_values = [] for a in self.appliances: if from_type_metadata: attr_value = a.type.get(attr) else: attr_value = a.metadata.get(attr) <IF_STMT> attr_values.append(attr_value) if len(attr_values) == 0: return default_value else: return agg_func(attr_values)",if attr_value is not None:
"def _remove(self, item): """"""Internal removal of an item"""""" for sibling in self.lines[self.lines.index(item) + 1:]: if isinstance(sibling, CronItem): env = sibling.env sibling.env = item.env sibling.env.update(env) sibling.env.job = sibling break <IF_STMT> self.lines.remove(sibling) else: break self.crons.remove(item) self.lines.remove(item) return 1",elif sibling == '':
"def _validate_command_chain(self) -> None: """"""Validate command-chain names."""""" for command in self.command_chain: <IF_STMT> raise HookValidationError(hook_name=self.hook_name, message=f'{command!r} is not a valid command-chain command.')","if not re.match('^[A-Za-z0-9/._#:$-]*$', command):"
"def _handle_unpaired_tag(self, html_tag): self.handle_ignore(html_tag, is_open=False) jannotations = self.read_jannotations(html_tag) for jannotation in arg_to_iter(jannotations): <IF_STMT> self._close_unpaired_tag() self.extra_required_attrs.extend(jannotation.pop('required', [])) annotation = self.build_annotation(jannotation) self.handle_variant(annotation, is_open=False) self.annotations.append(annotation) self.next_tag_index += 1",if self.unpairedtag_stack:
"def browser(self): if not hasattr(self, '_browser'): self.loop = asyncio.get_event_loop() <IF_STMT> raise RuntimeError('Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.') self._browser = self.loop.run_until_complete(super().browser) return self._browser",if self.loop.is_running():
"def process(self, node): self.vars = [] for child in node.childNodes: if child.nodeType == node.ELEMENT_NODE: child_text = get_xml_text(child) <IF_STMT> continue if child.nodeName == 'Real': for val in re.split('[\t ]+', child_text): self.vars.append(1.0 * eval(val)) return self",if child_text == '':
"def instantiate(self, node, container=None): var = self.vm.program.NewVariable() if container and (not isinstance(container, SimpleValue) or self.full_name in container.all_template_names): instance = TypeParameterInstance(self, container, self.vm) return instance.to_variable(node) else: for c in self.constraints: var.PasteVariable(c.instantiate(node, container)) <IF_STMT> var.PasteVariable(self.bound.instantiate(node, container)) if not var.bindings: var.AddBinding(self.vm.convert.unsolvable, [], node) return var",if self.bound:
"def compare_tables(self, db1, db2): i1 = db1.query('SELECT id, buf FROM test ORDER BY id') i2 = db2.query('SELECT id, buf FROM test ORDER BY id') for id1, buf1 in i1: id2, buf2 = next(i2) self.assertEqual(id1, id2) <IF_STMT> self.assertAlmostEqual(buf1, buf2, places=9) else: self.assertEqual(buf1, buf2) self.assertRaises(StopIteration, i2.__next__)","if isinstance(buf1, float):"
"def list_full_file_paths(directory): """"""List the absolute paths of files in |directory|."""""" directory_absolute_path = os.path.abspath(directory) paths = [] for relative_path in os.listdir(directory): absolute_path = os.path.join(directory_absolute_path, relative_path) <IF_STMT> paths.append(absolute_path) return paths",if os.path.isfile(absolute_path):
"def reparentChildren(self, newParent): while self.element.contents: child = self.element.contents[0] child.extract() <IF_STMT> newParent.appendChild(Element(child, self.soup, namespaces['html'])) else: newParent.appendChild(TextNode(child, self.soup))","if isinstance(child, Tag):"
"def sort(self): sorted_models = [] concrete_models = set() models = list(self.data) while len(sorted_models) < len(models): found = False for model in models: if model in sorted_models: continue dependencies = self.dependencies.get(model._meta.concrete_model) if not (dependencies and dependencies.difference(concrete_models)): sorted_models.append(model) concrete_models.add(model._meta.concrete_model) found = True <IF_STMT> return self.data = OrderedDict(((model, self.data[model]) for model in sorted_models))",if not found:
"def template(self): """"""template property"""""" if self._template is None: results = self._process(self.name, False, self.params, self.data) <IF_STMT> raise OpenShiftCLIError('Error processing template [%s]: %s' % (self.name, results)) self._template = results['results']['items'] return self._template",if results['returncode'] != 0:
"def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True) exit_code = c.wait() <IF_STMT> raise Exception('%s: Editing failed!' % editor) except OSError as e: raise Exception('%s: Editing failed: %s' % (editor, e))",if exit_code != 0:
"def test01e_json(self): """"""Testing GeoJSON input/output."""""" from django.contrib.gis.gdal.prototypes.geom import GEOJSON if not GEOJSON: return for g in self.geometries.json_geoms: geom = OGRGeometry(g.wkt) <IF_STMT> self.assertEqual(g.json, geom.json) self.assertEqual(g.json, geom.geojson) self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))","if not hasattr(g, 'not_equal'):"
"def debug(self): feed_dict = self.get_test_feed_dict() while True: tensor_name = input('Input debug tensor name: ').strip() <IF_STMT> sys.exit(0) try: debug_tensor = self.graph.get_tensor_by_name(tensor_name) except Exception as e: logging.error(e) continue res = self.sess.run(debug_tensor, feed_dict=feed_dict) logging.info(f'Result for tensor {tensor_name} is: {res}')",if tensor_name == 'q':
"def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment if not egg_fragment: continue if '-' in egg_fragment: key = '-'.join(egg_fragment.split('-')[:-1]).lower() else: key = egg_fragment <IF_STMT> return url.split('#', 1)[0] return None",if key == dist.key:
def select(result): for elem in result: parent = elem.getparent() <IF_STMT> continue try: elems = list(parent.iterchildren(elem.tag)) if elems[index] is elem: yield elem except IndexError: pass,if parent is None:
"def execute(self, cmd): mark = utils.random_text(32) path = '/cgi-bin/gdrive.cgi?cmd=4&f_gaccount=;{};echo {};'.format(cmd, mark) response = self.http_request(method='GET', path=path) if response is None: return '' if mark in response.text: regexp = '(|.+?){}'.format(mark) res = re.findall(regexp, response.text, re.DOTALL) <IF_STMT> return res[0] return ''",if len(res):
"def join(s, *p): path = s for t in p: if not s or isabs(t): path = t continue <IF_STMT> t = t[1:] if ':' not in path: path = ':' + path if path[-1:] != ':': path = path + ':' path = path + t return path",if t[:1] == ':':
def do_remove(self): if self.netconf.locked('dhcp'): if not self.pid: pid = read_pid_file('/var/run/udhcpd.pan1.pid') else: pid = self.pid <IF_STMT> logging.info('Stale dhcp lockfile found') self.netconf.unlock('dhcp'),"if not kill(pid, 'udhcpd'):"
"def filter_packages(query, package_infos): if query is None: return package_infos try: if '!' in query: raise ConanException(""'!' character is not allowed"") if ' not ' in query or query.startswith('not '): raise ConanException(""'not' operator is not allowed"") postfix = infix_to_postfix(query) if query else [] result = OrderedDict() for package_id, info in package_infos.items(): <IF_STMT> result[package_id] = info return result except Exception as exc: raise ConanException('Invalid package query: %s. %s' % (query, exc))","if _evaluate_postfix_with_info(postfix, info):"
"def __add__(self, other): if isinstance(other, Vector3): <IF_STMT> _class = Vector3 else: _class = Point3 return _class(self.x + other.x, self.y + other.y, self.z + other.z) else: assert hasattr(other, '__len__') and len(other) == 3 return Vector3(self.x + other[0], self.y + other[1], self.z + other[2])",if self.__class__ is other.__class__:
"def test_scout(): test_status = False with open('/tmp/test_scout_output', 'w') as logfile: if not DockerImage: logfile.write('No $AMBASSADOR_DOCKER_IMAGE??\n') el<IF_STMT> if wait_for_diagd(logfile) and check_chimes(logfile): test_status = True docker_kill(logfile) if not test_status: with open('/tmp/test_scout_output', 'r') as logfile: for line in logfile: print(line.rstrip()) assert test_status, 'test failed'",if docker_start(logfile):
"def visit_Assign(self, node): """"""Handle visiting an assignment statement."""""" ups = set() for targ in node.targets: if isinstance(targ, (Tuple, List)): ups.update((leftmostname(elt) for elt in targ.elts)) <IF_STMT> newnode = self.try_subproc_toks(node) if newnode is node: ups.add(leftmostname(targ)) else: return newnode else: ups.add(leftmostname(targ)) self.ctxupdate(ups) return node","elif isinstance(targ, BinOp):"
"def get_config_h_filename(): """"""Returns the path of pyconfig.h."""""" if _PYTHON_BUILD: <IF_STMT> inc_dir = os.path.join(_PROJECT_BASE, 'PC') else: inc_dir = _PROJECT_BASE else: inc_dir = get_path('platinclude') return os.path.join(inc_dir, 'pyconfig.h')",if os.name == 'nt' and os.name != 'java':
"def is_valid_block(self): """"""check wheter the block is valid in the current position"""""" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False <IF_STMT> return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True",if self.block.pos.x + i >= COLUMNS:
"def __call__(self, execution_result): json_value = execution_result.get_output_in_json() actual_result = jmespath.search(self._query, json_value, jmespath.Options(collections.OrderedDict)) if not actual_result > self._expected_result: expected_result_format = '> {}'.format(self._expected_result) <IF_STMT> raise JMESPathCheckAssertionError(self._query, expected_result_format, actual_result, execution_result.output) raise JMESPathCheckAssertionError(self._query, expected_result_format, 'None', execution_result.output)",if actual_result:
def readline(b): a = 1 while True: if b: <IF_STMT> a = 2 b = None continue b = None a = 5 return a,if b[0]:
"def test_execute_magic(self): """"""execute accepts IPython commands"""""" view = self.client[:] view.execute('a = 5') ar = view.execute('%whos', block=True) ar.get(5) for stdout in ar.stdout: lines = stdout.splitlines() self.assertEqual(lines[0].split(), ['Variable', 'Type', 'Data/Info']) found = False for line in lines[2:]: split = line.split() <IF_STMT> found = True break self.assertTrue(found, 'whos output wrong: %s' % stdout)","if split == ['a', 'int', '5']:"
"def imgFileProcessingTick(output): if isinstance(output, tuple): workerOutput.append(output) workerPool.terminate() else: for page in output: <IF_STMT> options.imgMetadata[page[0]] = page[1] options.imgOld.append(page[2]) if GUI: GUI.progressBarTick.emit('tick') if not GUI.conversionAlive: workerPool.terminate()",if page is not None:
"def _load(xs): ret = [] for x, ctx in zip(xs, context): <IF_STMT> ret.append([y.as_in_context(ctx) for y in x]) else: ret.append(x.as_in_context(ctx)) return ret","if isinstance(x, tuple):"
def _is_64bit_os(): global _IS_64BIT_OS if _IS_64BIT_OS is None: <IF_STMT> import platform _IS_64BIT_OS = platform.machine() == 'AMD64' else: _IS_64BIT_OS = False return _IS_64BIT_OS,if sys.maxsize > 2 ** 32:
"def stepStarted(self, step): self.currentStep = step for w in self.watchers: receiver = w.stepStarted(self, step) if receiver: <IF_STMT> step.subscribe(receiver[0], receiver[1]) else: step.subscribe(receiver) d = step.waitUntilFinished() d.addCallback(lambda step: step.unsubscribe(receiver)) step.waitUntilFinished().addCallback(self._stepFinished)","if isinstance(receiver, type(())):"
"def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute('PRAGMA synchronous = 0;') yield con <IF_STMT> con.commit() finally: if not self._bulk_commit: con.close()",if commit_on_success and self.can_commit:
"def parse_response(self, response): if hasattr(response, 'getheader'): if response.getheader('Content-Encoding', '') == 'gzip': stream = GzipDecodedResponse(response) else: stream = response else: stream = response p, u = self.getparser() while 1: data = stream.read(1024) <IF_STMT> break if self.verbose: print('body:', repr(data)) p.feed(data) if stream is not response: stream.close() p.close() return u.close()",if not data:
"def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return '[%s, %s]' % (i[1], i[2]) elif isinstance(nfrom, ExprCond): <IF_STMT> return '?' elif nfrom.src1 == nto: return 'True' elif nfrom.src2 == nto: return 'False' return ''",if nfrom.cond == nto:
"def gather_command_line_options(filter_disabled=None): """"""Get a sorted list of all CommandLineOption subclasses."""""" if filter_disabled is None: filter_disabled = not SETTINGS.COMMAND_LINE.SHOW_DISABLED_OPTIONS options = [] for opt in get_inheritors(commandline_options.CommandLineOption): warnings.warn('Subclassing `CommandLineOption` is deprecated. Please use the `sacred.cli_option` decorator and pass the function to the Experiment constructor.') <IF_STMT> continue options.append(opt) options += DEFAULT_COMMAND_LINE_OPTIONS return sorted(options, key=commandline_options.get_name)",if filter_disabled and (not opt._enabled):
"def handle_disconnect(self): """"""Socket gets disconnected"""""" try: self.serial.rts = False self.serial.dtr = False finally: self.serial.apply_settings(self.serial_settings_backup) self.rfc2217 = None self.buffer_ser2net = bytearray() if self.socket is not None: self.socket.close() self.socket = None <IF_STMT> self.log.warning('{}: Disconnected'.format(self.device))",if self.log is not None:
"def answers(self, other): if not isinstance(other, TCP): return 0 if conf.checkIPsrc: if not (self.sport == other.sport and self.dport == other.dport): return 0 if conf.check_TCPerror_seqack: if self.seq is not None: if self.seq != other.seq: return 0 <IF_STMT> if self.ack != other.ack: return 0 return 1",if self.ack is not None:
"def _override_options(options, **overrides): """"""Override options."""""" for opt, val in overrides.items(): passed_value = getattr(options, opt, _Default()) <IF_STMT> value = process_value(opt, passed_value.value) value += process_value(opt, val) setattr(options, opt, value) elif isinstance(passed_value, _Default): setattr(options, opt, process_value(opt, val))","if opt in ('ignore', 'select') and passed_value:"
"def _unlock_restarted_vms(self, pool_name): result = [] for vm in await self.middleware.call('vm.query', [('autostart', '=', True)]): for device in vm['devices']: <IF_STMT> continue path = device['attributes'].get('path') if not path: continue if path.startswith(f'/dev/zvol/{pool_name}/') or path.startswith(f'/mnt/{pool_name}/'): result.append(vm) break return result","if device['dtype'] not in ('DISK', 'RAW'):"
"def check_space(arr, task_id): for a in arr: <IF_STMT> found = False for x in shlex.split(a): if task_id in x: found = True if not found: raise AssertionError",if a.startswith('hadoop jar'):
"def clean(self): if self.instance: redirect_to = self.data.get('redirect_to', '') if redirect_to != '': lfs.core.utils.set_redirect_for(self.instance.get_absolute_url(), redirect_to) else: lfs.core.utils.remove_redirect_for(self.instance.get_absolute_url()) if self.data.get('active_base_price') == str(CHOICES_YES): <IF_STMT> self.errors['base_price_amount'] = ErrorList([_(u'This field is required.')]) return self.cleaned_data","if self.data.get('base_price_amount', '') == '':"
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('\\AAL[_-]?(SESS|LB)=', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def unloadOnePlugin(self, moduleOrFileName, verbose=False): moduleName = self.regularizeName(moduleOrFileName) if self.isLoaded(moduleName): <IF_STMT> g.pr('unloading', moduleName) del self.loadedModules[moduleName] for tag in self.handlers: bunches = self.handlers.get(tag) bunches = [bunch for bunch in bunches if bunch.moduleName != moduleName] self.handlers[tag] = bunches",if verbose:
"def __init__(self, **kw): util_schema.validate(instance=kw, schema=self.schema, cls=util_schema.CustomValidator, use_default=False, allow_default_none=True) for prop in six.iterkeys(self.schema.get('properties', [])): value = kw.get(prop, None) <IF_STMT> nodes = [] for node in value: ac_node = Node(**node) ac_node.validate() nodes.append(ac_node) value = nodes setattr(self, prop, value)",if prop == 'chain':
"def initialize(self): for document in self.corpus: frequencies = {} for word in document: if word not in frequencies: frequencies[word] = 0 frequencies[word] += 1 self.f.append(frequencies) for word, freq in iteritems(frequencies): <IF_STMT> self.df[word] = 0 self.df[word] += 1 for word, freq in iteritems(self.df): self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)",if word not in self.df:
"def get_child(self, name): if self.isdir: try: return self.data[name] except: <IF_STMT> for childname, child in list(self.data.items()): if childname.lower() == name.lower(): return child raise",if not self.case_sensitive:
"def set_cover(channel, pixbuf): if self.channel == channel: <IF_STMT> self.imgCover.set_from_pixbuf(self.scale_pixbuf(pixbuf)) if self.show_on_cover_load: self.main_window.show() self.show_on_cover_load = False",if pixbuf is not None:
"def test_infer_shape_matrix(self): x = theano.tensor.matrix() for op in self.ops: if not op.return_inverse: continue <IF_STMT> f = op(x)[2] else: f = op(x)[1] self._compile_and_check([x], [f], [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)], self.op_class)",if op.return_index:
"def Filter(self, match=None, **_): """"""Filter the current expression."""""" arg = self.stack.pop(-1) for filter_name in match.group(1).split(','): filter_object = ConfigFilter.classes_by_name.get(filter_name) if filter_object is None: raise FilterError('Unknown filter function %r' % filter_name) <IF_STMT> logging.debug('Applying filter %s for %s.', filter_name, arg) arg = filter_object().Filter(arg) precondition.AssertType(arg, Text) self.stack[-1] += arg",if not filter_object.sensitive_arg:
"def enqueue_link(self, fuzzresult, link_url, parsed_link): if self.add_path: split_path = parsed_link.path.split('/') newpath = '/'.join(split_path[:-1]) + '/' self.queue_url(urljoin(fuzzresult.url, newpath)) new_link = urljoin(fuzzresult.url, link_url) if not self.regex_param or (self.regex_param and self.regex_param.search(new_link) is not None): <IF_STMT> self.queue_url(new_link) self.add_result('link', 'New link found', new_link)",if self.enqueue_links:
"def old_save(self, *args, **kwargs): """"""Override save to set Subscribers and send Notifications"""""" original = None original_assigned = [] if hasattr(self, 'instance'): try: original = Task.objects.get(pk=self.instance.id) original_assigned = list(original.assigned.all()) except Task.DoesNotExist: pass instance = super(TaskForm, self).save(*args, **kwargs) if original: new_assigned = list(self.cleaned_data['assigned']) <IF_STMT> for assignee in new_assigned: self.instance.subscribers.add(assignee) return instance",if original_assigned != new_assigned:
def get_test_layer(): layers = get_bb_var('BBLAYERS').split() testlayer = None for l in layers: if '~' in l: l = os.path.expanduser(l) <IF_STMT> testlayer = l break return testlayer,if '/meta-selftest' in l and os.path.isdir(l):
"def readable(request): """"""Display a readable version of this url if we can"""""" rdict = request.matchdict bid = rdict.get('hash_id', None) username = rdict.get('username', None) if bid: found = BmarkMgr.get_by_hash(bid, username=username) <IF_STMT> return {'bmark': found, 'username': username} else: return HTTPNotFound()",if found:
"def pythonpath(conanfile): python_path = conanfile.env.get('PYTHONPATH', None) if python_path: old_path = sys.path[:] <IF_STMT> sys.path.extend(python_path) else: sys.path.append(python_path) yield sys.path = old_path else: yield","if isinstance(python_path, list):"
"def _validate(self): on_target_delete = None for cmd in self.val.commands: if isinstance(cmd, qlast.OnTargetDelete): <IF_STMT> raise errors.EdgeQLSyntaxError(f""more than one 'on target delete' specification"", context=cmd.context) else: on_target_delete = cmd",if on_target_delete:
"def _choose_instance(self, timeout_time): """"""Returns an Instance to handle a request or None if all are busy."""""" with self._condition: while time.time() < timeout_time and (not self._quit_event.is_set()): for inst in self._instances: if inst.can_accept_requests: return inst else: inst = self._start_any_instance() <IF_STMT> break self._condition.wait(timeout_time - time.time()) else: return None if inst: inst.wait(timeout_time) return inst",if inst:
"def get_identifiers(self): ids = [] for entry in glob.glob(f'{self._base_path}/ctl-*'): ident = entry.split('-', 1)[-1] <IF_STMT> continue if os.path.exists(os.path.join(entry, 'disk_octets.rrd')): ids.append(ident) ids.sort(key=RRDBase._sort_ports) return ids",if ident.endswith('ioctl'):
"def read_vocab_list(path, max_vocab_size=20000): vocab = {'<eos>': 0, '<unk>': 1} with io.open(path, encoding='utf-8', errors='ignore') as f: for l in f: w = l.strip() <IF_STMT> vocab[w] = len(vocab) if len(vocab) >= max_vocab_size: break return vocab",if w not in vocab and w:
"def n_import_from(self, node): relative_path_index = 0 if self.version >= 2.5: if node[relative_path_index].pattr > 0: node[2].pattr = '.' * node[relative_path_index].pattr + node[2].pattr if self.version > 2.7: <IF_STMT> imports = node[1].pattr for pattr in imports: node[1].pattr = pattr self.default(node) return pass self.default(node)","if isinstance(node[1].pattr, tuple):"
"def get(self): """"""Returns a simple HTML for contact form"""""" if self.user: user_info = models.User.get_by_id(long(self.user_id)) <IF_STMT> self.form.name.data = user_info.name + ' ' + user_info.last_name if user_info.email: self.form.email.data = user_info.email params = {'exception': self.request.get('exception')} return self.render_template('boilerplate_contact.html', **params)",if user_info.name or user_info.last_name:
"def task_management_menu(activation, request): """"""Available tasks actions."""""" actions = [] if request.user.has_perm(activation.flow_class._meta.manage_permission_name): for transition in activation.get_available_transitions(): if transition.can_proceed(activation): url = activation.flow_task.get_task_url(activation.task, transition.name, user=request.user, namespace=request.resolver_match.namespace) <IF_STMT> actions.append((transition.name.replace('_', ' ').title(), url)) return {'actions': actions, 'request': request}",if url:
"def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, 'admin'): continue admin_module = import_module('%s.admin' % app.name) if hasattr(admin_module, 'MisagoAdminExtension'): extension = getattr(admin_module, 'MisagoAdminExtension')() if hasattr(extension, 'register_navigation_nodes'): extension.register_navigation_nodes(site) <IF_STMT> extension.register_urlpatterns(urlpatterns)","if hasattr(extension, 'register_urlpatterns'):"
"def dequeue(self): with self.db(commit=True) as curs: curs.execute('select id, data from task where queue = ? order by priority desc, id limit 1', (self.name,)) result = curs.fetchone() if result is not None: tid, data = result curs.execute('delete from task where id = ?', (tid,)) <IF_STMT> return to_bytes(data)",if curs.rowcount == 1:
"def readHexStringFromStream(stream): stream.read(1) txt = '' x = b_('') while True: tok = readNonWhitespace(stream) <IF_STMT> raise PdfStreamError('Stream has ended unexpectedly') if tok == b_('>'): break x += tok if len(x) == 2: txt += chr(int(x, base=16)) x = b_('') if len(x) == 1: x += b_('0') if len(x) == 2: txt += chr(int(x, base=16)) return createStringObject(b_(txt))",if not tok:
"def test_compute_gradient(self): for y, y_pred in zip(self.y_list, self.predict_list): lse_grad = self.lae_loss.compute_grad(y, y_pred) diff = y_pred - y if diff > consts.FLOAT_ZERO: grad = 1 <IF_STMT> grad = -1 else: grad = 0 self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)",elif diff < consts.FLOAT_ZERO:
"def request_get(request, key, default_value=None): if key in request.args: return request.args.get(key) elif key in request.form: return request.form.get(key) try: json_body = request.get_json(force=True, silent=True) <IF_STMT> return json_body[key] else: return default_value except Exception: return default_value",if key in json_body:
"def _getResourceData(self, jid, dataname): """"""Return specific jid's resource representation in internal format. Used internally."""""" if jid.find('/') + 1: jid, resource = jid.split('/', 1) <IF_STMT> return self._data[jid]['resources'][resource][dataname] elif self._data[jid]['resources'].keys(): lastpri = -129 for r in self._data[jid]['resources'].keys(): if int(self._data[jid]['resources'][r]['priority']) > lastpri: resource, lastpri = (r, int(self._data[jid]['resources'][r]['priority'])) return self._data[jid]['resources'][resource][dataname]",if self._data[jid]['resources'].has_key(resource):
"def GetBoundingBoxMin(self): """"""Get the minimum bounding box."""""" x1, y1 = (10000, 10000) x2, y2 = (-10000, -10000) for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] if point[1] < y1: y1 = point[1] if point[0] > x2: x2 = point[0] <IF_STMT> y2 = point[1] return (x2 - x1, y2 - y1)",if point[1] > y2:
"def produce_etag_headers(self, filename): """"""Produce a dict of curl headers containing etag headers from the download."""""" headers = {} if os.path.exists(filename): self.existing_file_size = os.path.getsize(filename) etag = self.getxattr(self.xattr_etag) last_modified = self.getxattr(self.xattr_last_modified) <IF_STMT> headers['If-None-Match'] = etag if last_modified: headers['If-Modified-Since'] = last_modified return headers",if etag:
"def _find_orientation_offset(self, header): ifd_offset, = self._unpack('L', header[4:]) self.exif_buffer.seek(ifd_offset) for _ in range(self._unpack('H', self.exif_buffer.read(2))[0]): tag, = self._unpack('H10x', self.exif_buffer.read(12)) <IF_STMT> self._offset = self.exif_buffer.tell() - 4 break",if tag == 274:
"def _start(self): try: await self.fire_event('pre_request') except AbortEvent: self.logger.debug('Abort request %s', self.request) else: <IF_STMT> try: self.start_request() except Exception as exc: self.finished(exc=exc)",if self._request is not None:
"def filter(callbackfn): array = this.to_object() arr_len = array.get('length').to_uint32() if not callbackfn.is_callable(): raise this.MakeError('TypeError', 'callbackfn must be a function') T = arguments[1] res = [] k = 0 while k < arr_len: <IF_STMT> kValue = array.get(str(k)) if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value: res.append(kValue) k += 1 return res",if array.has_property(str(k)):
"def action(self, params): if len(params) < 1: return CommandsResponse(STATUS_ERROR, 'Not enough params') else: vrf_name = params[0] <IF_STMT> vrf_rf = params[1] else: vrf_rf = 'ipv4' from ryu.services.protocols.bgp.operator.internal_api import WrongParamError try: return CommandsResponse(STATUS_OK, self.api.count_single_vrf_routes(vrf_name, vrf_rf)) except WrongParamError as e: return WrongParamResp(e)",if len(params) == 2:
"def __init__(self, layers): super(Add, self).__init__() self.layer_names = [] self.layers = layers for i, layer in enumerate(self.layers): if layer.parent is None: if i == 0: layer.parent = 'input' else: layer.parent = layers[i - 1].name <IF_STMT> name = layer.name else: name = layer.__class__.__name__ + str(i) layer.name = name self.layer_names.append(name)","if hasattr(layer, 'name'):"
def _grouping_intervals(grouping): last_interval = None for interval in grouping: if interval == CHAR_MAX: return if interval == 0: <IF_STMT> raise ValueError('invalid grouping') while True: yield last_interval yield interval last_interval = interval,if last_interval is None:
"def infer_expected_xp_and_device(self, x): xp = backend.get_array_module(x) if xp is np: return (xp, None) elif xp is cuda.cupy: return (xp, x.device) elif xp is chainerx: backend_name = x.device.backend.name <IF_STMT> return (np, None) elif backend_name == 'cuda': return (cuda.cupy, cuda.cupy.cuda.Device(x.device.index)) assert False",if backend_name == 'native':
"def _escape_attrib(text): try: if '&' in text: text = text.replace('&', '&amp;') if '<' in text: text = text.replace('<', '&lt;') <IF_STMT> text = text.replace('>', '&gt;') if '""' in text: text = text.replace('""', '&quot;') if '\n' in text: text = text.replace('\n', '&#10;') return text except (TypeError, AttributeError): _raise_serialization_error(text)",if '>' in text:
"def get_block_id_at_height(store, height, descendant_id): if height is None: return None while True: block = store._load_block(descendant_id) if block['height'] == height: return descendant_id descendant_id = block['search_id' <IF_STMT> else 'prev_id']",if util.get_search_height(block['height']) >= height
"def train(config, checkpoint_dir=None): if checkpoint_dir: assert os.path.exists(checkpoint_dir) for step in range(10): <IF_STMT> with tune.checkpoint_dir(step=step) as checkpoint_dir: path = os.path.join(checkpoint_dir, 'checkpoint') with open(path, 'w') as f: f.write(json.dumps({'step': step})) tune.report(test=step)",if step % 3 == 0:
"def onMinimize(self, sender): if self._runDialogListener('onMinimize') is False: return widget = self.child if widget is not None: if widget.isVisible(): widget.setVisible(False) self.setHeight('') self.setWidth('') if self._maximized: self._minimized = self._maximized self._toggleMaximize() else: self._minimized = None else: <IF_STMT> self._toggleMaximize() widget.setVisible(True)",if self._minimized is not None:
"def apply_transformation(self, ti: TransformationInput) -> Transformation: if ti.lineno == ti.document.line_count - 1: buffer = ti.buffer_control.buffer <IF_STMT> suggestion = buffer.suggestion.text else: suggestion = '' return Transformation(fragments=ti.fragments + [(self.style, suggestion)]) else: return Transformation(fragments=ti.fragments)",if buffer.suggestion and ti.document.is_cursor_at_the_end:
"def get_measurements(self, pipeline, object_name, category): if object_name == IMAGE and category == C_COUNT: return [self.object_name.value] elif object_name == self.object_name: <IF_STMT> return [FTR_CENTER_X, FTR_CENTER_Y] elif category == C_NUMBER: return [FTR_OBJECT_NUMBER] elif category == C_WORMS: return [F_ANGLE] return []",if category == C_LOCATION:
"def traverse(tensors): """"""traverse all ops to find attached workload"""""" for t in tensors: op = t.op <IF_STMT> return args_to_workload(op.attrs['workload']) wkl = traverse(op.input_tensors) if wkl: return wkl return None",if 'workload' in op.attrs:
"def _pack(converter, node: Any, inputs: List[str]) -> Any: final_inputs = [] for x_in in inputs: input_c = converter.outputs[x_in] <IF_STMT> final_inputs.append(_nodef_to_private_pond(converter, input_c)) else: final_inputs.append(input_c) return converter.protocol.stack(final_inputs, axis=node.attr['axis'].i)","if isinstance(input_c, tf.compat.v1.NodeDef):"
"def __init__(self, instance=None, data=empty, **kwargs): context = kwargs.get('context', {}) if 'product' in context: instance = self.get_instance(context, data, kwargs) <IF_STMT> quantity = self.fields['quantity'].to_internal_value(data['quantity']) else: quantity = self.fields['quantity'].default instance.setdefault('quantity', quantity) super().__init__(instance, data, context=context) else: super().__init__(instance, data, **kwargs)",if data is not empty and 'quantity' in data:
"def serialize(self, value): if value is not None: try: iter(value) except TypeError: value = [value] <IF_STMT> return [self.element_serialize(val) for val in sorted(value)] return None",if len(value):
"def remove_cloner_curve(self, obj_index): if self.selected_mode == 'Duplicate': curve_name = f'{self.basedata_name}.cloner.{obj_index:04d}' cu = bpy.data.curves.get(curve_name) <IF_STMT> bpy.data.curves.remove(cu)",if cu:
"def update_advance_paid(self): advance_paid = frappe._dict() for d in self.get('accounts'): <IF_STMT> if d.reference_type in ('Sales Order', 'Purchase Order', 'Employee Advance'): advance_paid.setdefault(d.reference_type, []).append(d.reference_name) for voucher_type, order_list in iteritems(advance_paid): for voucher_no in list(set(order_list)): frappe.get_doc(voucher_type, voucher_no).set_total_advance_paid()",if d.is_advance:
"def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) if id is None: continue elif isinstance(id, collections.Hashable): <IF_STMT> d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()",if id not in dict or not dict[id].is_alive():
"def _get_default_factory(self, attribute_name: str) -> Any: if hasattr(self, attribute_name): if str(getattr(self, attribute_name)).startswith('${'): return str(getattr(self, attribute_name)) <IF_STMT> return str(self.__dataclass_fields__[attribute_name].default) elif getattr(self, attribute_name) != self.__dataclass_fields__[attribute_name].default_factory(): return getattr(self, attribute_name) return self.__dataclass_fields__[attribute_name].default_factory()",elif str(self.__dataclass_fields__[attribute_name].default).startswith('${'):
"def showMenu(self, show): if show: <IF_STMT> self.canvas.menu = Menu(self.canvas, tearoff=0) self.canvas.menu.add_command(label='delete', command=self._delete) self.canvas.menu.bind('<FocusOut>', lambda e: self.canvas.menu.unpost()) self._bindMenu() else: pass",if self.canvas.menu is None:
"def __init__(self, db, where=None): self._db = db self._tables = [] self.filters = [] if hasattr(where, 'get_all'): self.where = where self._tables.insert(0, where.get_all) elif hasattr(where, 'get_one') and isinstance(where.get_one, QueryException): self.where = where.get_one else: <IF_STMT> self.filters = where.left self.where = where self._tables = [field._tablename for field, op, val in self.filters]","if isinstance(where, Query):"
"def main(): try: from wsgiref.simple_server import make_server from wsgiref.validate import validator <IF_STMT> port[0] = get_open_port() wsgi_application = WsgiApplication(soap11_application) server = make_server(host, port[0], validator(wsgi_application)) logger.info('Starting interop server at %s:%s.' % ('0.0.0.0', port[0])) logger.info('WSDL is at: /?wsdl') server.serve_forever() except ImportError: print('Error: example server code requires Python >= 2.5')",if port[0] == 0:
"def try_adjust_widgets(self): if hasattr(self.parent, 'adjust_widgets'): self.parent.adjust_widgets() if hasattr(self.parent, 'parentApp'): <IF_STMT> self.parent.parentApp._internal_adjust_widgets() if hasattr(self.parent.parentApp, 'adjust_widgets'): self.parent.parentApp.adjust_widgets()","if hasattr(self.parent.parentApp, '_internal_adjust_widgets'):"
"def copy_file_replace_line(orig_file: Path, new_file: Path, line_re: str, new_line: str) -> None: old_version_fh = orig_file.open('r') new_version_fh = new_file.open('w') for line in old_version_fh: <IF_STMT> new_version_fh.write(new_line + '\n') else: new_version_fh.write(line) old_version_fh.close() new_version_fh.close()","if re.search(line_re, line):"
"def _protoc_plugin_parameters(self, language): """"""Return a tuple of (plugin path, vars) used as parameters for ninja build."""""" path, vars = ('', {}) for p in self.attr['protoc_plugins']: <IF_STMT> path = p.path flag = p.protoc_plugin_flag(self.build_dir) vars = {'protoc%spluginflags' % language: flag} break return (path, vars)",if language in p.code_generation:
"def scan_page(self, address_space, page_offset, fullpage=False): """"""Runs through patchers for a single page"""""" <IF_STMT> pagedata = address_space.read(page_offset, PAGESIZE) for patcher in self.patchers: for offset, data in patcher.get_constraints(): if fullpage: testdata = pagedata[offset:offset + len(data)] else: testdata = address_space.read(page_offset + offset, len(data)) if data != testdata: break else: yield patcher",if fullpage:
"def OnLeftDClick(self, event): pt = event.GetPosition() item, flags = self.tree.HitTest(pt) if item: self.log.WriteText('OnLeftDClick: %s\n' % self.tree.GetItemText(item)) parent = self.tree.GetItemParent(item) <IF_STMT> self.tree.SortChildren(parent) event.Skip()",if parent.IsOk():
"def drop_pathlist(self, pathlist): """"""Drop path list"""""" if pathlist: files = [""r'%s'"" % path for path in pathlist] <IF_STMT> text = files[0] else: text = '[' + ', '.join(files) + ']' if self.new_input_line: self.on_new_line() self.insert_text(text) self.setFocus()",if len(files) == 1:
"def func_set_exporter_funcs_opset_yaml(func_set): if len(list(func_set)[0].split('@')) == 1: yaml_data = {} for nnabla_func, impl_funcs in _onnx_func_info.items(): <IF_STMT> yaml_data[nnabla_func] = impl_funcs return yaml.dump(yaml_data, default_flow_style=False) else: return yaml.dump(list(func_set), default_flow_style=False)",if nnabla_func in func_set:
"def object_hook(obj): obj_len = len(obj) if obj_len == 1: if '$date' in obj: return datetime.fromtimestamp(obj['$date'] / 1000, tz=timezone.utc) + timedelta(milliseconds=obj['$date'] % 1000) if '$time' in obj: return time(*[int(i) for i in obj['$time'].split(':')]) if obj_len == 2 and '$type' in obj and ('$value' in obj): <IF_STMT> return date(*[int(i) for i in obj['$value'].split('-')]) return obj",if obj['$type'] == 'date':
"def start(self, para=None, callback=None): if not self.load(): return if para != None or self.show(): if para == None: para = self.para win = WidgetsManager.getref('Macros Recorder') <IF_STMT> win.write('{}>{}'.format(self.title, para)) if self.asyn and IPy.uimode() != 'no': threading.Thread(target=self.runasyn, args=(para, callback)).start() else: self.runasyn(para, callback)",if win != None:
"def user(self): <IF_STMT> _env_username = os.getenv('CONAN_USERNAME') conan_v2_error(""Environment variable 'CONAN_USERNAME' is deprecated"", _env_username) self._conan_user = _env_username or self.default_user if not self._conan_user: raise ConanException('user not defined, but self.user is used in conanfile') return self._conan_user",if not self._conan_user:
"def _get_vars(cls, func): params = inspect.signature(func).parameters.copy() args = {} for name, param in params.items(): <IF_STMT> args[name] = _get_variable(name) return args",if param.kind is param.POSITIONAL_OR_KEYWORD and param.default is None:
def parts(self): klass = self.__class__ this = list() for token in self: if token.startswith_fws(): if this: yield (this[0] if len(this) == 1 else klass(this)) this.clear() end_ws = token.pop_trailing_ws() this.append(token) <IF_STMT> yield klass(this) this = [end_ws] if this: yield (this[0] if len(this) == 1 else klass(this)),if end_ws:
"def start_fileoutput(self): """"""Start output to configured file."""""" path = os.path.dirname(self.filename) try: <IF_STMT> os.makedirs(path) self.fd = self.create_fd() self.close_fd = True except IOError: msg = sys.exc_info()[1] log.warn(LOG_CHECK, 'Could not open file %r for writing: %s\nDisabling log output of %s', self.filename, msg, self) self.fd = dummy.Dummy() self.is_active = False self.filename = None",if path and (not os.path.isdir(path)):
"def worksheet_id(self, value): if self._worksheet: <IF_STMT> return else: raise InvalidArgumentValue('This range already has a worksheet with different id set.') self._worksheet_id = value",if self._worksheet.id == value:
"def _sanity_check(self, kind, triplets): route_id = self.data.get('route_id', [None])[0] if route_id or [k for k in self.data.keys() if k[:5] in ('route', 'smtp-', 'sourc', 'secur', 'local')]: <IF_STMT> raise ValueError('Can only configure detailed settings for one profile at a time')",if len(triplets) > 1 or kind != 'profile':
"def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if 'value' in msg: <IF_STMT> pass elif msg['value'] is None: msg['value'] = self.values[0] else: if isIn(msg['value'], self.unicode_values): idx = indexOf(msg['value'], self.unicode_values) else: idx = indexOf(msg['value'], self.labels) msg['value'] = self._items[self.labels[idx]] msg.pop('options', None) return msg",if not self.values:
"def emit(self, record): msg = record.getMessage() if record.exc_info: _type, value, tback = record.exc_info tback_text = ''.join(traceback.format_exception(_type, value, tback)) <IF_STMT> msg += '\n' msg += tback_text self.tktext.insert('end', msg + '\n', record.levelname)",if msg:
"def _get_pip_index_urls(sources): index_urls = [] trusted_hosts = [] for source in sources: url = source.get('url') if not url: continue index_urls.append(url) <IF_STMT> continue host = six.moves.urllib.parse.urlparse(source['url']).hostname trusted_hosts.append(host) return (index_urls, trusted_hosts)","if source.get('verify_ssl', True):"
"def _is_binary(fname, limit=80): try: with open(fname, 'rb') as f: for i in range(limit): char = f.read(1) if char == b'\x00': return True <IF_STMT> return False if char == b'': return except OSError as e: if xp.ON_WINDOWS and is_app_execution_alias(fname): return True raise e return False",if char == b'\n':
"def tearDown(self): exc, _, _ = sys.exc_info() if exc: try: if hasattr(self, 'obj') and isinstance(self.obj, SelfDiagnosable): diags = self.obj.get_error_diagnostics() <IF_STMT> for line in diags: ROOT_LOGGER.info(line) except BaseException: pass if self.captured_logger: self.captured_logger.removeHandler(self.log_recorder) self.log_recorder.close() sys.stdout = self.stdout_backup super(BZTestCase, self).tearDown()",if diags:
"def _disconnect(self, sync): <IF_STMT> if sync: try: self._connection.send_all() self._connection.fetch_all() except (WorkspaceError, ServiceUnavailable): pass if self._connection: self._connection.in_use = False self._connection = None self._connection_access_mode = None",if self._connection:
"def _recursive_process(self): super(RecursiveObjectDownwardsVisitor, self)._recursive_process() while self._new_for_visit: func_ea, arg_idx = self._new_for_visit.pop() if helper.is_imported_ea(func_ea): continue cfunc = helper.decompile_function(func_ea) <IF_STMT> assert arg_idx < len(cfunc.get_lvars()), 'Wrong argument at func {}'.format(to_hex(func_ea)) obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx) self.prepare_new_scan(cfunc, arg_idx, obj) self._recursive_process()",if cfunc:
"def to_dict(self) -> JSONDict: data = dict() for key in iter(self.__dict__): <IF_STMT> continue value = self.__dict__[key] if value is not None: if hasattr(value, 'to_dict'): data[key] = value.to_dict() else: data[key] = value if data.get('from_user'): data['from'] = data.pop('from_user', None) return data",if key == 'bot' or key.startswith('_'):
"def get_data(self, path, prefix=''): item = self.store[path] path = '{}/{}'.format(prefix, path) keys = [i for i in item.keys()] data = {'path': path} for k in keys: <IF_STMT> dataset = np.array(item[k].value) if type(dataset) is np.ndarray: if dataset.size != 0: if type(dataset[0]) is np.bytes_: dataset = [a.decode('ascii') for a in dataset] data.update({k: dataset}) return data","if not isinstance(item[k], h5py.Group):"
"def _macros_of_type(root, type, el_func): macros_el = root.find('macros') macro_dict = {} if macros_el is not None: macro_els = macros_el.findall('macro') filtered_els = [(macro_el.get('name'), el_func(macro_el)) for macro_el in macro_els <IF_STMT>] macro_dict = dict(filtered_els) return macro_dict",if macro_el.get('type') == type
"def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None <IF_STMT> name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, '__dict__', None) is o: o = r break elif isinstance(o, dict): name = web.dictfind(o, self.obj) if not isinstance(name, six.string_types): name = None d.append(Object(o, name)) return d","if isinstance(o, dict):"
"def MakeWidthArray(fm): s = '{\n\t' cw = fm['Widths'] for i in xrange(0, 256): if chr(i) == ""'"": s += ""'\\''"" <IF_STMT> s += ""'\\\\'"" elif i >= 32 and i <= 126: s += ""'"" + chr(i) + ""'"" else: s += 'chr(%d)' % i s += ':' + fm['Widths'][i] if i < 255: s += ',' if (i + 1) % 22 == 0: s += '\n\t' s += '}' return s",elif chr(i) == '\\':
def getLatestFile(self): highestNsp = None highestNsx = None for nsp in self.getFiles(): try: if nsp.path.endswith('.nsx'): <IF_STMT> highestNsx = nsp elif not highestNsp or int(nsp.version) > int(highestNsp.version): highestNsp = nsp except BaseException: pass return highestNsp or highestNsx,if not highestNsx or int(nsp.version) > int(highestNsx.version):
"def _check_integrity(self) -> bool: all_files = self.FILE_LIST.copy() all_files.append(self.ANNOTATIONS_FILE) for _, md5, filename in all_files: file, ext = os.path.splitext(filename) extracted_dir = os.path.join(self.root, file) <IF_STMT> return False return True",if not os.path.exists(extracted_dir):
"def load_core(self): for filename in os.listdir(self.path): <IF_STMT> try: name = filename.replace('.py', '') mod = load_python_module(name, self.path) self._load_cmd_from(mod) except: warnings.warn('!! Warning: could not load core command file ' + filename, RuntimeWarning)",if filename != '__init__.py' and filename.endswith('.py'):
"def _make_dataset(key, data, size): if isinstance(data, chainer.get_array_types()): if key is None: key = '_{}'.format(id(data)) return _Array(key, data) elif isinstance(data, list): if key is None: key = '_{}'.format(id(data)) return _List(key, data) elif callable(data): if key is None: raise ValueError('key(s) must be specified for callable') <IF_STMT> raise ValueError('size must be specified for callable') return _Index(size).transform(key, data)",if size is None:
"def main_loop(self) -> None: while True: try: message = self.control.get(block=False) except Empty: message = None if message == 'ABORT': self.log.info('Got ABORT message, main_loop exiting...') break <IF_STMT> self.log.error('One or more watcher died, committing suicide!') sys.exit(1) if self.all_workers_dead(): self.log.error('All workers have died, committing suicide!') sys.exit(1) self.check_and_start_workers() time.sleep(0.1)",if not self.all_watchers_running():
"def execute_map(cls, ctx, op): (x,), device_id, xp = as_same_device([ctx[c.key] for c in op.inputs], op.device, ret_extra=True) axis = cls.get_arg_axis(op.axis, op.inputs[0].ndim) keepdims = op.keepdims with device(device_id): nz = xp.count_nonzero(x, axis=axis) <IF_STMT> slcs = [slice(None)] * op.inputs[0].ndim for ax in op.axis: slcs[ax] = np.newaxis nz = xp.asarray(nz)[tuple(slcs)] ctx[op.outputs[0].key] = nz",if keepdims:
"def setfilter(self, f): filter_exp = create_string_buffer(f.encode('ascii')) if pcap_compile(self.pcap, byref(self.bpf_program), filter_exp, 0, -1) == -1: error('Could not compile filter expression %s' % f) return False el<IF_STMT> error('Could not install filter %s' % f) return False return True","if pcap_setfilter(self.pcap, byref(self.bpf_program)) == -1:"
"def find_parent_for_new_to(self, pos): """"""Figure out the parent object for something at 'pos'."""""" for children in self._editable_children: if children._start <= pos < children._end: return children.find_parent_for_new_to(pos) <IF_STMT> return children.find_parent_for_new_to(pos) return self",if children._start == pos and pos == children._end:
"def process_events(self, events): for event in events: key = (event.ident, event.filter) <IF_STMT> self._force_wakeup.drain() continue receiver = self._registered[key] if event.flags & select.KQ_EV_ONESHOT: del self._registered[key] if type(receiver) is _core.Task: _core.reschedule(receiver, outcome.Value(event)) else: receiver.put_nowait(event)",if event.ident == self._force_wakeup_fd:
"def test_tag(artifact_obj, sagemaker_session): tag = {'Key': 'foo', 'Value': 'bar'} artifact_obj.set_tag(tag) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags(ResourceArn=artifact_obj.artifact_arn)['Tags'] <IF_STMT> break time.sleep(5) assert len(actual_tags) > 0 assert actual_tags[0] == tag",if actual_tags:
"def initialize(self) -> None: """"""Move the API keys from cog stored config to core bot config if they exist."""""" imgur_token = await self.config.imgur_client_id() if imgur_token is not None: <IF_STMT> await self.bot.set_shared_api_tokens('imgur', client_id=imgur_token) await self.config.imgur_client_id.clear()",if not await self.bot.get_shared_api_tokens('imgur'):
"def _sorted_layers(self, structure, top_layer_id): """"""Return the image layers sorted"""""" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) if 'json' not in structure['repolayers'][next_layer]: break if 'parent' not in structure['repolayers'][next_layer]['json']: break next_layer = structure['repolayers'][next_layer]['json']['parent'] <IF_STMT> break return sorted_layers",if not next_layer:
"def __init__(self, bounds, channel_axis, preprocess=None): assert len(bounds) == 2 assert channel_axis in [0, 1, 2, 3] self._bounds = bounds self._channel_axis = channel_axis if preprocess is not None: sub, div = np.array(preprocess) <IF_STMT> sub = 0 if np.all(div == 1): div = 1 assert div is None or np.all(div) self._preprocess = (sub, div) else: self._preprocess = (0, 1)",if not np.any(sub):
"def unpickle(fname): """"""Load pickled object from `fname`"""""" with smart_open(fname, 'rb') as f: <IF_STMT> return _pickle.load(f, encoding='latin1') else: return _pickle.loads(f.read())","if sys.version_info > (3, 0):"
"def get_new_setup_py_lines(): global version with open('setup.py', 'r') as sf: current_setup = sf.readlines() for line in current_setup: <IF_STMT> major, minor = re.findall(""VERSION = '(\\d+)\\.(\\d+)'"", line)[0] version = '{}.{}'.format(major, int(minor) + 1) yield ""VERSION = '{}'\n"".format(version) else: yield line",if line.startswith('VERSION = '):
"def make_buffers_dict(observables): """"""Makes observable states in a dict."""""" out_dict = type(observables)() for key, value in six.iteritems(observables): <IF_STMT> out_dict[key] = _EnabledObservable(value, physics, random_state, self._strip_singleton_buffer_dim) return out_dict",if value.enabled:
"def _callFUT(self, config_file, global_conf=None, _loader=None): import pyramid.paster old_loader = pyramid.paster.get_config_loader try: <IF_STMT> pyramid.paster.get_config_loader = _loader return pyramid.paster.setup_logging(config_file, global_conf) finally: pyramid.paster.get_config_loader = old_loader",if _loader is not None:
"def _csv(self, match=None, dump=None): if dump is None: dump = self._dump(match) for record in dump: row = [] for field in record: <IF_STMT> row.append('%i' % field) elif field is None: row.append('') else: row.append(""'%s'"" % field) yield ','.join(row)","if isinstance(field, int):"
def preprocess_envs(args_envs): envs_map = {} for item in args_envs: i = item.find(':') <IF_STMT> key = item[:i] val = item[i + 1:] envs_map[key] = val return envs_map,if i != -1:
"def _get_most_recent_update(self, versions): recent = None for version in versions: updated = datetime.datetime.strptime(version['updated'], '%Y-%m-%dT%H:%M:%SZ') <IF_STMT> recent = updated elif updated > recent: recent = updated return recent.strftime('%Y-%m-%dT%H:%M:%SZ')",if not recent:
"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(' , ') el<IF_STMT> ostream.write(' - ') return True else: ostream.write(' + ')",if type(self._args[idx]) is _NegationExpression:
"def __init__(self, bert, num_classes=2, dropout=0.0, prefix=None, params=None): super(BERTClassifier, self).__init__(prefix=prefix, params=params) self.bert = bert with self.name_scope(): self.classifier = nn.HybridSequential(prefix=prefix) <IF_STMT> self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=num_classes))",if dropout:
"def __iter__(self): for i, field in enumerate(self.fields): <IF_STMT> yield AdminReadonlyField(self.form, field, is_first=i == 0, model_admin=self.model_admin) else: yield AdminField(self.form, field, is_first=i == 0)",if field in self.readonly_fields:
"def boolean(value): if isinstance(value, str): v = value.lower() <IF_STMT> return True if v in ('0', 'no', 'false', 'off'): return False raise ValueError(value) return bool(value)","if v in ('1', 'yes', 'true', 'on'):"
"def xdir(obj, return_values=False): for attr in dir(obj): <IF_STMT> if return_values: yield (attr, getattr(obj, attr)) else: yield attr",if attr[:2] != '__' and attr[-2:] != '__':
"def get_current_stock(self): for d in self.get('supplied_items'): <IF_STMT> bin = frappe.db.sql('select actual_qty from `tabBin` where item_code = %s and warehouse = %s', (d.rm_item_code, self.supplier_warehouse), as_dict=1) d.current_stock = bin and flt(bin[0]['actual_qty']) or 0",if self.supplier_warehouse:
"def getvars(request, excludes): getvars = request.GET.copy() excludes = excludes.split(',') for p in excludes: <IF_STMT> del getvars[p] if len(getvars.keys()) > 0: return '&%s' % getvars.urlencode() else: return ''",if p in getvars:
"def read(cls, reader, dump=None): code = reader.read_u1() if Opcode.opcodes is None: Opcode.opcodes = {} for name in globals(): klass = globals()[name] try: <IF_STMT> Opcode.opcodes[klass.code] = klass except TypeError: pass instance = Opcode.opcodes[code].read_extra(reader, dump) if dump: reader.debug('' * dump, '%3d: %s' % (reader.offset, instance)) return instance","if name != 'Opcode' and issubclass(klass, Opcode):"
"def clean(self): username = self.cleaned_data.get('username') password = self.cleaned_data.get('password') message = ERROR_MESSAGE if username and password: self.user_cache = authenticate(username=username, password=password) <IF_STMT> raise ValidationError(message % {'username': self.username_field.verbose_name}) elif not self.user_cache.is_active or not self.user_cache.is_staff: raise ValidationError(message % {'username': self.username_field.verbose_name}) return self.cleaned_data",if self.user_cache is None:
"def currentLevel(self): currentStr = '' for stackType, stackValue in self.stackVals: if stackType == 'dict': if isinstance(stackValue, str): currentStr += ""['"" + stackValue + ""']"" else: currentStr += '[' + str(stackValue) + ']' <IF_STMT> currentStr += '[' + str(stackValue) + ']' elif stackType == 'getattr': currentStr += "".__getattribute__('"" + stackValue + ""')"" else: raise Exception(f'Cannot get attribute of type {stackType}') return currentStr",elif stackType == 'listLike':
"def dump(self, out=sys.stdout, code2cid=None, code=None): if code2cid is None: code2cid = self.code2cid code = () for k, v in sorted(code2cid.iteritems()): c = code + (k,) <IF_STMT> out.write('code %r = cid %d\n' % (c, v)) else: self.dump(out=out, code2cid=v, code=c) return","if isinstance(v, int):"
"def __init__(self, text, menu): self.text = text self.menu = menu print(text) for i, option in enumerate(menu): menunum = i + 1 match = re.search('0D', option) if not match: <IF_STMT> print('   %s) %s' % (menunum, option)) else: print('  %s) %s' % (menunum, option)) else: print('\n  99) Return to Main Menu\n') return",if menunum < 10:
"def receive(self, sock): """"""Receive a message on ``sock``."""""" msg = None data = b'' recv_done = False recv_len = -1 while not recv_done: buf = sock.recv(BUFSIZE) if buf is None or len(buf) == 0: raise Exception('socket closed') <IF_STMT> recv_len = struct.unpack('>I', buf[:4])[0] data += buf[4:] recv_len -= len(data) else: data += buf recv_len -= len(buf) recv_done = recv_len == 0 msg = pickle.loads(data) return msg",if recv_len == -1:
"def apply_shortcuts(self): """"""Apply shortcuts settings to all widgets/plugins"""""" toberemoved = [] for index, (qobject, context, name, default) in enumerate(self.shortcut_data): keyseq = QKeySequence(get_shortcut(context, name, default)) try: if isinstance(qobject, QAction): qobject.setShortcut(keyseq) <IF_STMT> qobject.setKey(keyseq) except RuntimeError: toberemoved.append(index) for index in sorted(toberemoved, reverse=True): self.shortcut_data.pop(index)","elif isinstance(qobject, QShortcut):"
"def _resolved_values(self): values = [] for k, v in self.values.items() if hasattr(self.values, 'items') else self.values: <IF_STMT> if isinstance(k, util.string_types): desc = _entity_descriptor(self.mapper, k) values.extend(desc._bulk_update_tuples(v)) elif isinstance(k, attributes.QueryableAttribute): values.extend(k._bulk_update_tuples(v)) else: values.append((k, v)) else: values.append((k, v)) return values",if self.mapper:
"def remove_callback(self, callback, events=None): if events is None: for event in self._plugin_lifecycle_callbacks: <IF_STMT> self._plugin_lifecycle_callbacks[event].remove(callback) else: if isinstance(events, basestring): events = [events] for event in events: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback)",if callback in self._plugin_lifecycle_callbacks[event]:
"def _thd_parse_volumes(self, volumes): volume_list = [] binds = {} for volume_string in volumes or []: try: bind, volume = volume_string.split(':', 1) except ValueError: config.error('Invalid volume definition for docker %s. Skipping...' % volume_string) continue ro = False <IF_STMT> ro = volume[-2:] == 'ro' volume = volume[:-3] volume_list.append(volume) binds[bind] = {'bind': volume, 'ro': ro} return (volume_list, binds)",if volume.endswith(':ro') or volume.endswith(':rw'):
"def __init__(self, model, **kwargs): self.model = model for key, value in kwargs.items(): <IF_STMT> raise TypeError('%s() received an invalid keyword %r' % (self.__class__.__name__, key)) setattr(self, key, value) self.handle_model()","if not hasattr(self, key):"
"def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) if key >= l: raise IndexError('Index %s out of range (%s elements)' % (key, l)) if key < 0: <IF_STMT> raise IndexError('Index %s out of range (%s elements)' % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError(self.impl.__class__.__name__ + ' object does not support slicing') else: return self(key)",if key < -l:
"def _get_formatted(self, model, key): value = model._type(key).format(model.get(key)) if isinstance(value, bytes): value = value.decode('utf-8', 'ignore') if self.for_path: sep_repl = beets.config['path_sep_replace'].as_str() for sep in (os.path.sep, os.path.altsep): <IF_STMT> value = value.replace(sep, sep_repl) return value",if sep:
"def publish(self, name, stat): try: topic = 'stat.%s' % str(name) <IF_STMT> topic += '.%d' % stat['subtopic'] stat = json.dumps(stat) logger.debug('Sending %s' % stat) self.socket.send_multipart([b(topic), stat]) except zmq.ZMQError: if self.socket.closed: pass else: raise",if 'subtopic' in stat:
def logic(): while 1: yield a var = 0 out.next = 0 for i in downrange(len(a)): if a[i] == 0: continue else: for j in downrange(i - 1): <IF_STMT> pass else: out.next = j break break,if a[j] == 0:
"def get_abstract_models(self, appmodels): abstract_models = [] for appmodel in appmodels: abstract_models += [abstract_model for abstract_model in appmodel.__bases__ <IF_STMT>] abstract_models = list(set(abstract_models)) return abstract_models","if hasattr(abstract_model, '_meta') and abstract_model._meta.abstract"
"def _sanitize_field_name(self, field_name: str) -> str: try: if self._meta.get_field(field_name).get_internal_type() == 'ForeignKey': <IF_STMT> return field_name + '_id' except FieldDoesNotExist: pass return field_name",if not field_name.endswith('_id'):
"def find_enabled_item(self, e): x, y = e.local if 0 <= x < (self.width - self.margin - self.scroll_button_size if self.scrolling else self.width): h = self.font.get_linesize() i = (y - h // 2) // h + self.scroll items = self._items <IF_STMT> item = items[i] if item.enabled: return item",if 0 <= i < len(items):
"def addColumn(self, *cols, index=None): """"""Insert all *cols* into columns at *index*, or append to end of columns if *index* is None.  Return first column."""""" for i, col in enumerate(cols): vd.addUndo(self.columns.remove, col) <IF_STMT> index = len(self.columns) col.recalc(self) self.columns.insert(index + i, col) Sheet.visibleCols.fget.cache_clear() return cols[0]",if index is None:
"def _compare_values(self, result, source): from google.protobuf.struct_pb2 import ListValue from google.protobuf.struct_pb2 import Value for found, expected in zip(result, source): self.assertIsInstance(found, ListValue) self.assertEqual(len(found.values), len(expected)) for found_cell, expected_cell in zip(found.values, expected): self.assertIsInstance(found_cell, Value) <IF_STMT> self.assertEqual(int(found_cell.string_value), expected_cell) else: self.assertEqual(found_cell.string_value, expected_cell)","if isinstance(expected_cell, int):"
"def _traverse(op): if topi.tag.is_broadcast(op.tag): if not op.same_as(output.op): if not op.axis: const_ops.append(op) else: ewise_ops.append(op) for tensor in op.input_tensors: <IF_STMT> ewise_inputs.append((op, tensor)) else: _traverse(tensor.op) else: assert op.tag == 'dense_pack' dense_res.append(op)","if isinstance(tensor.op, tvm.te.PlaceholderOp):"
"def update_annotation(parameters: Sequence[cst.Param], annotations: Sequence[cst.Param]) -> List[cst.Param]: parameter_annotations = {} annotated_parameters = [] for parameter in annotations: <IF_STMT> parameter_annotations[parameter.name.value] = parameter.annotation for parameter in parameters: key = parameter.name.value if key in parameter_annotations and (self.overwrite_existing_annotations or not parameter.annotation): parameter = parameter.with_changes(annotation=parameter_annotations[key]) annotated_parameters.append(parameter) return annotated_parameters",if parameter.annotation:
"def _modules(self, module_paths, component_name): for path in module_paths: for filename in os.listdir(path): name, ext = os.path.splitext(filename) <IF_STMT> root_relative_path = os.path.join(path, name)[len(self.root_path) + len(os.path.sep):] module_name = '%s.%s' % (component_name, root_relative_path.replace(os.path.sep, '.')) yield module_name",if ext.endswith('.py'):
"def run(self): for obj in bpy.context.scene.objects: if 'modelId' in obj: obj_id = obj['modelId'] <IF_STMT> self._make_lamp_emissive(obj, self.lights[obj_id]) if obj_id in self.windows: self._make_window_emissive(obj) if obj.name.startswith('Ceiling#'): self._make_ceiling_emissive(obj)",if obj_id in self.lights:
"def get_chart_data(self): rows = [] for row in self.data: row = frappe._dict(row) <IF_STMT> values = [row.range1, row.range2, row.range3, row.range4, row.range5] precision = cint(frappe.db.get_default('float_precision')) or 2 rows.append({'values': [flt(val, precision) for val in values]}) self.chart = {'data': {'labels': self.ageing_column_labels, 'datasets': rows}, 'type': 'percentage'}",if not cint(row.bold):
"def suite(aggressive): """"""Run against pep8 test suite."""""" result = True path = os.path.join(os.path.dirname(__file__), 'suite') for filename in os.listdir(path): filename = os.path.join(path, filename) <IF_STMT> print(filename, file=sys.stderr) result = run(filename, aggressive=aggressive) and result if result: print(GREEN + 'Okay' + END) return result",if filename.endswith('.py'):
"def list_generator(pages, num_results): result = [] page = list(next(pages)) result += page while True: <IF_STMT> break if num_results is not None: if num_results == len(result): break page = list(next(pages)) result += page return result",if not pages.continuation_token:
"def _detect_too_many_digits(f): ret = [] for node in f.nodes: for ir in node.irs: for read in ir.read: if isinstance(read, Constant): value_as_str = read.original_value <IF_STMT> ret.append(node) return ret",if '00000' in value_as_str:
"def write_varint(trans, n): out = [] while True: <IF_STMT> out.append(n) break else: out.append(n & 255 | 128) n = n >> 7 data = array.array('B', out).tostring() if PY3: trans.write(data) else: trans.write(bytes(data))",if n & ~127 == 0:
"def __call__(self, environ, start_response): query_string = environ.get('QUERY_STRING') if 'sql_debug=1' in query_string: import galaxy.app <IF_STMT> galaxy.app.app.model.thread_local_log.log = True try: reset_request_query_counts() return self.application(environ, start_response) finally: log_request_query_counts(environ.get('PATH_INFO'))",if galaxy.app.app.model.thread_local_log:
"def SvGetSocketInfo(socket): """"""returns string to show in socket label"""""" global socket_data_cache ng = socket.id_data.tree_id if socket.is_output: s_id = socket.socket_id elif socket.is_linked: other = socket.other if other and hasattr(other, 'socket_id'): s_id = other.socket_id else: return '' else: return '' if ng in socket_data_cache: if s_id in socket_data_cache[ng]: data = socket_data_cache[ng][s_id] <IF_STMT> return str(len(data)) return ''",if data:
"def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break if parser._subparsers._actions is None: break choices = parser._subparsers._actions[-1].choices value = getattr(args, 'level_%d' % level) if value is None: parser.print_help() return if not choices: break <IF_STMT> parser = choices[value] else: return level += 1","if isinstance(choices, dict):"
"def tag_configure(self, *args, **keys): trace = False and (not g.unitTesting) if trace: g.trace(args, keys) if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get('foreground') underline = keys.get('underline') if val: self.configDict[key] = val <IF_STMT> self.configUnderlineDict[key] = True else: g.trace('oops', args, keys)",if underline:
"def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: <IF_STMT> token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield (index, token, value)",if self.stdlibhighlighting and value in self.stdlib_types:
"def materialize_as_ndarray(a): """"""Convert distributed arrays to ndarrays."""""" if type(a) in (list, tuple): <IF_STMT> return da.compute(*a, sync=True) return tuple((np.asarray(arr) for arr in a)) return np.asarray(a)","if da is not None and any((isinstance(arr, da.Array) for arr in a)):"
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if isinstance(rv, flask.Response): try: result = etag if callable(result): result = result(rv) <IF_STMT> rv.set_etag(result) except Exception: logging.getLogger(__name__).exception('Error while calculating the etag value for response {!r}'.format(rv)) return rv",if result:
"def applyBC(self): """"""apply boundary conditions"""""" deltaR = 2.0 for coord in self.pos: <IF_STMT> coord[0] = -deltaR if coord[0] < -deltaR: coord[0] = width + deltaR if coord[1] > height + deltaR: coord[1] = -deltaR if coord[1] < -deltaR: coord[1] = height + deltaR",if coord[0] > width + deltaR:
def removeInsideIslands(self): self.CleanPath = [] cleanpath = Path('Path') for path in self.NewPaths: for seg in path: inside = False for island in self.IntersectedIslands: issegin = island.isSegInside(seg) == 1 if issegin: <IF_STMT> inside = True break if not inside: cleanpath.append(seg) cleanpath = cleanpath.split2contours() self.CleanPath.extend(cleanpath),if not seg in island:
"def _parse_lines(self, linesource): """"""Parse lines of text for functions and classes"""""" functions = [] classes = [] for line in linesource: if line.startswith('def ') and line.count('('): name = self._get_object_name(line) <IF_STMT> functions.append(name) elif line.startswith('class '): name = self._get_object_name(line) if not name.startswith('_'): classes.append(name) else: pass functions.sort() classes.sort() return (functions, classes)",if not name.startswith('_'):
"def process(self, buckets, event=None): results = [] with self.executor_factory(max_workers=2) as w: futures = {w.submit(self.process_bucket, bucket): bucket for bucket in buckets} for f in as_completed(futures): <IF_STMT> results.append(futures[f]) return results",if f.result():
def build_polymorphic_ctypes_map(cls): mapping = {} for ct in ContentType.objects.filter(app_label='main'): ct_model_class = ct.model_class() <IF_STMT> mapping[ct.id] = camelcase_to_underscore(ct_model_class.__name__) return mapping,"if ct_model_class and issubclass(ct_model_class, cls):"
"def expand_decodings(self, node: Node) -> None: val = node.level.result.value for decoder in self.get_decoders_for(type(val)): inst = self._config()(decoder) res = inst(val) <IF_STMT> continue try: new_node = Node.decoding(config=self._config(), route=inst, result=res, source=node) except DuplicateNode: continue logger.trace('Nesting encodings') self.recursive_expand(new_node, False)",if res is None:
"def test_file(self): a = 3.33 + 4.43j b = 5.1 + 2.3j fo = None try: fo = open(test_support.TESTFN, 'wb') (print >> fo, a, b) fo.close() fo = open(test_support.TESTFN, 'rb') self.assertEqual(fo.read(), '%s %s\n' % (a, b)) finally: <IF_STMT> fo.close() test_support.unlink(test_support.TESTFN)",if fo is not None and (not fo.closed):
"def repl(m): if m.group(2) is not None: high = int(m.group(1), 16) low = int(m.group(2), 16) <IF_STMT> cp = (high - 55296 << 10) + (low - 56320) + 65536 return unichr(cp) else: return unichr(high) + unichr(low) else: return unichr(int(m.group(1), 16))",if 55296 <= high <= 56319 and 56320 <= low <= 57343:
"def generate_credits(user, start_date, end_date, **kwargs): """"""Generate credits data for given component."""""" result = [] base = Change.objects.content() if user: base = base.filter(author=user) for language in Language.objects.filter(**kwargs).distinct().iterator(): authors = base.filter(language=language, **kwargs).authors_list((start_date, end_date)) <IF_STMT> continue result.append({language.name: sorted(authors, key=lambda item: item[2])}) return result",if not authors:
"def history_prev(self): """"""Go back in the history."""""" try: <IF_STMT> item = self._history.start(self.text().strip()) else: item = self._history.previtem() except (cmdhistory.HistoryEmptyError, cmdhistory.HistoryEndReachedError): return self.setText(item)",if not self._history.is_browsing():
"def destroy(self): self._bind() for name in ('jobItems', 'jobFileIDs', 'files', 'statsFiles', 'statsFileIDs'): resource = getattr(self, name) if resource is not None: if isinstance(resource, AzureTable): resource.delete_table() <IF_STMT> resource.delete_container() else: assert False setattr(self, name, None)","elif isinstance(resource, AzureBlobContainer):"
"def user_defined_os(): if menu.options.os: <IF_STMT> settings.TARGET_OS = 'win' return True elif menu.options.os.lower() == 'unix': return True else: err_msg = ""You specified wrong value '"" + menu.options.os + ""' "" err_msg += ""as an operation system. The value, must be 'Windows' or 'Unix'."" print(settings.print_critical_msg(err_msg)) raise SystemExit()",if menu.options.os.lower() == 'windows':
"def test_save(art_warning, image_dl_estimator): try: classifier, _ = image_dl_estimator(from_logits=True) t_file = tempfile.NamedTemporaryFile() model_path = t_file.name t_file.close() filename = 'model_to_save' classifier.save(filename, path=model_path) assert path.exists(model_path) created_model = False for file in listdir(model_path): <IF_STMT> created_model = True assert created_model except ARTTestException as e: art_warning(e)",if filename in file:
"def set_extra_data(self, extra_data=None): if extra_data and self.extra_data != extra_data: <IF_STMT> self.extra_data.update(extra_data) else: self.extra_data = extra_data return True","if self.extra_data and (not isinstance(self.extra_data, str)):"
"def get_image_dimensions(path): """"""Returns the (width, height) of an image at a given path."""""" p = ImageFile.Parser() fp = open(path, 'rb') while 1: data = fp.read(1024) <IF_STMT> break p.feed(data) if p.image: return p.image.size break fp.close() return None",if not data:
def language_suffixes(): for lang in NSLocale.preferredLanguages(): while True: yield ('_' + lang if lang != 'en' else '') <IF_STMT> lang = lang[:lang.rfind('-')] else: break yield '',if '-' in lang:
"def decode_binary(binarystring): """"""Decodes a binary string into it's integer value."""""" n = 0 for c in binarystring: if c == '0': d = 0 <IF_STMT> d = 1 else: raise ValueError('Not an binary number', binarystring) n = n * 2 + d return n",elif c == '1':
"def serialize_groups_for_summary(node): groups = node.osf_groups n_groups = len(groups) group_string = '' for index, group in enumerate(groups): if index == n_groups - 1: separator = '' <IF_STMT> separator = ' & ' else: separator = ', ' group_string = group_string + group.name + separator return group_string",elif index == n_groups - 2:
"def _save(self, req_method, requires): conanfile = GenConanfile() for req in requires: req2, override = req if isinstance(req, tuple) else (req, False) <IF_STMT> conanfile.with_require(req2, override=override) else: conanfile.with_requirement(req2, override=override) self.client.save({'conanfile.py': conanfile}, clean_first=True)",if not req_method:
"def _validate_declarations(declarations: Sequence[Union[qlast.ModuleDeclaration, qlast.DDLCommand]]) -> None: for decl in declarations: <IF_STMT> raise EdgeQLSyntaxError('only fully-qualified name is allowed in top-level declaration', context=decl.name.context)","if not isinstance(decl, qlast.ModuleDeclaration) and decl.name.module is None:"
"def assess_trial(self, trial_job_id, trial_history): _logger.info('assess trial %s %s', trial_job_id, trial_history) id_ = trial_history[0] if id_ in self._killed: return AssessResult.Bad s = 0 for i, val in enumerate(trial_history): s += val <IF_STMT> self._killed.add(id_) _result.write('%d %d\n' % (id_, i + 1)) _result.flush() return AssessResult.Bad return AssessResult.Good",if s % 11 == 1:
"def decProcess(): while 1: yield (clock.posedge, reset.negedge) <IF_STMT> count.next = 0 elif enable: if count == -n: count.next = n - 1 else: count.next = count - 1",if reset == ACTIVE_LOW:
def activate_profile(test=True): pr = None if test: <IF_STMT> pr = cProfile.Profile() pr.enable() else: log.error('cProfile is not available on your platform') return pr,if HAS_CPROFILE:
"def insertTestData(self, rows): for row in rows: if isinstance(row, Log): self.logs[row.id] = row.values.copy() for row in rows: <IF_STMT> lines = self.log_lines.setdefault(row.logid, []) if len(lines) < row.last_line + 1: lines.append([None] * (row.last_line + 1 - len(lines))) row_lines = row.content.decode('utf-8').split('\n') lines[row.first_line:row.last_line + 1] = row_lines","if isinstance(row, LogChunk):"
"def getText(self, stuff): if isinstance(stuff, BaseWrapper): stuff = stuff.item if isinstance(stuff, (Fit, TargetProfile)): val, unit = self._getValue(stuff) if val is None: return '' <IF_STMT> return '{} {}'.format(formatAmount(val, *self.formatSpec), unit) else: return formatAmount(val, *self.formatSpec, unitName=unit) return ''",if self.stickPrefixToValue:
"def wrap(request, *args, **kwargs): """"""Wrap"""""" user = request.user.profile if 'massform' in request.POST: for key in request.POST: <IF_STMT> try: changeset = ChangeSet.objects.get(pk=request.POST[key]) form = MassActionForm(request.user.profile, request.POST, instance=changeset) if form.is_valid() and user.has_permission(changeset, mode='w'): form.save() except Exception: pass return f(request, *args, **kwargs)",if 'mass-changeset' in key:
"def select(self, browser, locator): assert browser is not None if locator is not None: if isinstance(locator, list): self._select_by_excludes(browser, locator) return if locator.lower() == 'self' or locator.lower() == 'current': return <IF_STMT> self._select_by_last_index(browser) return prefix, criteria = self._parse_locator(locator) strategy = self._strategies.get(prefix) if strategy is None: raise ValueError(""Window locator with prefix '"" + prefix + ""' is not supported"") return strategy(browser, criteria)",if locator.lower() == 'new' or locator.lower() == 'popup':
"def test_all(self): for context in get_contexts(): found = False expected_context_name = context.get_name() for calculated_context in get_context(self.HTML, expected_context_name): if calculated_context.get_name() == expected_context_name: found = True <IF_STMT> msg = 'The analysis for %s context failed, got %r instead.' msg = msg % (expected_context_name, get_context(self.HTML, expected_context_name)) self.assertTrue(False, msg)",if not found:
"def visit_title(self, node: Element) -> None: if isinstance(node.parent, addnodes.seealso): self.body.append('.IP ""') return elif isinstance(node.parent, nodes.section): <IF_STMT> raise nodes.SkipNode elif self.section_level == 1: self.body.append('.SH %s\n' % self.deunicode(node.astext().upper())) raise nodes.SkipNode return super().visit_title(node)",if self.section_level == 0:
"def parse_svn_stats(status): stats = RepoStats() for line in status: <IF_STMT> stats.new += 1 elif line[0] == 'C': stats.conflicted += 1 elif line[0] in ['A', 'D', 'I', 'M', 'R', '!', '~']: stats.changed += 1 return stats",if line[0] == '?':
"def setoutput(self, spec, defs=None): self.closespec() self.closedefs() if spec: if type(spec) == StringType: file = self.openoutput(spec) mine = 1 else: file = spec mine = 0 self.specfile = file self.specmine = mine if defs: <IF_STMT> file = self.openoutput(defs) mine = 1 else: file = defs mine = 0 self.defsfile = file self.defsmine = mine",if type(defs) == StringType:
"def __new__(cls, name, bases, d): rv = type.__new__(cls, name, bases, d) if 'methods' not in d: methods = set(rv.methods or []) for key, value in d.iteritems(): <IF_STMT> methods.add(key.upper()) if methods: rv.methods = sorted(methods) return rv",if key in http_method_funcs:
"def draw_lines(col, lines): skip = False for l in lines: if l: col.label(text=l) skip = False <IF_STMT> continue else: col.label(text=l) skip = True",elif skip:
"def adjust_sockets(self): variables = self.get_variables() for key in self.inputs.keys(): if key not in variables and key not in ['Field']: self.debug('Input {} not in variables {}, remove it'.format(key, str(variables))) self.inputs.remove(self.inputs[key]) for v in variables: <IF_STMT> self.debug('Variable {} not in inputs {}, add it'.format(v, str(self.inputs.keys()))) self.inputs.new('SvStringsSocket', v)",if v not in self.inputs:
"def forward(self, g, x): h = x for l, conv in enumerate(self.layers): h = conv(g, h) <IF_STMT> h = self.activation(h) h = self.dropout(h) return h",if l != len(self.layers) - 1:
"def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]: """"""Let the user process the docstrings before adding them."""""" for docstringlines in docstrings: <IF_STMT> self.env.app.emit('autodoc-process-docstring', self.objtype, self.fullname, self.object, self.options, docstringlines) if docstringlines and docstringlines[-1] != '': docstringlines.append('') yield from docstringlines",if self.env.app:
"def wiki(self, query): res = [] for entry in g.current_wiki.get_index(): name = filename_to_cname(entry['name']) name = re.sub('//+', '/', name) if set(query.split()).intersection(name.replace('/', '-').split('-')): page = g.current_wiki.get_page(name) <IF_STMT> res.append(dict(name=name, content=page.data)) return res",if page:
"def checkForFinishedThreads(self): """"""Mark terminated threads with endTime."""""" for t in self.unfinishedThreads: if not t.is_alive(): t.endTime = time.process_time() <IF_STMT> t.status = 'ended'","if getattr(t, 'status', None) is None:"
"def testTicketFlags(self): flags = ('restored', 'banned') ticket = Ticket('test', 0) trueflags = [] for v in (True, False, True): for f in flags: setattr(ticket, f, v) <IF_STMT> trueflags.append(f) else: trueflags.remove(f) for f2 in flags: self.assertEqual(bool(getattr(ticket, f2)), f2 in trueflags) ticket = FailTicket(ticket=ticket) for f2 in flags: self.assertTrue(bool(getattr(ticket, f2)))",if v:
"def decode(obj, encoding='utf-8', errors='strict'): decoder = __decoder(encoding) if decoder: result = decoder(obj, errors) <IF_STMT> raise TypeError('decoder must return a tuple (object, integer)') return result[0]","if not (isinstance(result, tuple) and len(result) == 2):"
"def work(self): """"""Play the animation."""""" if self.loop_mode == LoopMode.ONCE: <IF_STMT> self.frame_requested.emit(self.axis, self.min_point) elif self.step < 0 and self.current <= self.min_point + 1: self.frame_requested.emit(self.axis, self.max_point) self.timer.singleShot(int(self.interval), self.advance) else: self.advance() self.started.emit()",if self.step > 0 and self.current >= self.max_point - 1:
"def get_order(self, aStr): if aStr[0] >= '¤': <IF_STMT> return 157 * (ord(aStr[0]) - 164) + ord(aStr[1]) - 161 + 63 else: return 157 * (ord(aStr[0]) - 164) + ord(aStr[1]) - 64 else: return -1",if aStr[1] >= '¡':
"def validate_literals(self): try: for c in self.literals: <IF_STMT> self.log.error('Invalid literal %s. Must be a single character', repr(c)) self.error = True except TypeError: self.log.error('Invalid literals specification. literals must be a sequence of characters') self.error = True","if not isinstance(c, StringTypes) or len(c) > 1:"
"def filter(self, qs, value): if value: if value.start is not None and value.stop is not None: value = (value.start, value.stop) elif value.start is not None: self.lookup_expr = 'startswith' value = value.start <IF_STMT> self.lookup_expr = 'endswith' value = value.stop return super().filter(qs, value)",elif value.stop is not None:
"def parse_stdout(s): argv = re.search('^===ARGV=(.*?)$', s, re.M).group(1) argv = argv.split() testname = argv[-1] del argv[-1] hub = None reactor = None while argv: <IF_STMT> hub = argv[1] del argv[0] del argv[0] elif argv[0] == '--reactor': reactor = argv[1] del argv[0] del argv[0] else: del argv[0] if reactor is not None: hub += '/%s' % reactor return (testname, hub)",if argv[0] == '--hub':
"def get(self, key): try: res = self.server.get(index=self.index, doc_type=self.doc_type, id=key) try: <IF_STMT> return res['_source']['result'] except (TypeError, KeyError): pass except elasticsearch.exceptions.NotFoundError: pass",if res['found']:
"def _get_target_chap_auth(self, context, volume): """"""Get the current chap auth username and password."""""" try: volume_info = self.db.volume_get(context, volume['id']) <IF_STMT> return tuple(volume_info['provider_auth'].split(' ', 3)[1:]) except exception.NotFound: LOG.debug('Failed to get CHAP auth from DB for %s.', volume['id'])",if volume_info['provider_auth']:
"def merge(self, hosts): for ei in self: host_name = ei.get_name() h = hosts.find_by_name(host_name) <IF_STMT> self.merge_extinfo(h, ei)",if h is not None:
"def __init__(self, user, *args, **kwargs): """"""Sets choices and initial value"""""" super(SettingsForm, self).__init__(*args, **kwargs) self.fields['default_changeset_status'].queryset = ChangeSetStatus.objects.filter(trash=False) try: conf = ModuleSetting.get_for_module('treeio.changes', 'default_changeset_status')[0] default_changeset_status = ChangeSetStatus.objects.get(pk=long(conf.value)) <IF_STMT> self.fields['default_changeset_status'].initial = default_changeset_status.id except Exception: pass",if not default_changeset_status.trash:
"def load(self): """"""Method for loading a feature"""""" with self.filesystem.openbin(self.path, 'r') as file_handle: <IF_STMT> with gzip.open(file_handle, 'rb') as gzip_fp: return self._decode(gzip_fp, self.path) return self._decode(file_handle, self.path)",if self.path.endswith(FileFormat.GZIP.extension()):
"def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: <IF_STMT> return '[%s, %s]' % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return '?' elif nfrom.src1 == nto: return 'True' elif nfrom.src2 == nto: return 'False' return ''",if i[0] == nto:
"def disable_verity(): """"""Disables dm-verity on the device."""""" with log.waitfor('Disabling dm-verity on %s' % context.device): root() with AdbClient() as c: reply = c.disable_verity() if 'Verity already disabled' in reply: return elif 'Now reboot your device' in reply: reboot(wait=True) <IF_STMT> return else: log.error('Could not disable verity:\n%s' % reply)",elif '0006closed' in reply:
"def __demo_mode_pause_if_active(self, tiny=False): if self.demo_mode: wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT if self.demo_sleep: wait_time = float(self.demo_sleep) <IF_STMT> time.sleep(wait_time) else: time.sleep(wait_time / 3.4) elif self.slow_mode: self.__slow_mode_pause_if_active()",if not tiny:
"def dictToKW(d): out = [] items = list(d.items()) items.sort() for k, v in items: <IF_STMT> raise NonFormattableDict(""%r ain't a string"" % k) if not r.match(k): raise NonFormattableDict(""%r ain't an identifier"" % k) out.append('\n\x00{}={},'.format(k, prettify(v))) return ''.join(out)","if not isinstance(k, str):"
"def createCommonCommands(self): """"""Handle all global @command nodes."""""" c = self.c aList = c.config.getCommands() or [] for z in aList: p, script = z gnx = p.v.gnx <IF_STMT> self.seen.add(gnx) script = self.getScript(p) self.createCommonCommand(p, script)",if gnx not in self.seen:
"def _decodeFromStream(self, s): """"""Decode a complete DER OBJECT ID from a file."""""" DerObject._decodeFromStream(self, s) p = BytesIO_EOF(self.payload) comps = list(map(str, divmod(p.read_byte(), 40))) v = 0 while p.remaining_data(): c = p.read_byte() v = v * 128 + (c & 127) <IF_STMT> comps.append(str(v)) v = 0 self.value = '.'.join(comps)",if not c & 128:
"def tiles_around_factor(self, factor, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx <IF_STMT> for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps",if nx >= 0 and nx < self.width * factor:
"def deleteAllMatchers(self): """"""Deletes all matchers."""""" if self.__filter: result = QtWidgets.QMessageBox.question(self, 'Delete All Matchers?', 'Are you sure you want to delete all matchers?', QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No) <IF_STMT> self._itemsLock.lockForWrite() try: for item in list(self._items.values()): item.rpcObject.delete() finally: self._itemsLock.unlock() self.removeAllItems()",if result == QtWidgets.QMessageBox.Yes:
"def _parse_icons(self, icons): <IF_STMT> icons = get_iterated_icons(icons) for icon in icons: if isinstance(icons, list): icon = Icon(icon) else: icon = Icon(icons[icon]) if icon.exists: self.icons.append(icon)","if isinstance(icons, list):"
"def change_misc_visibility(self, on_start=False): if self.misc.isVisible(): self._splitterMainSizes = self._splitterMain.sizes() self.misc.hide() widget = self.mainContainer.get_actual_widget() <IF_STMT> widget.setFocus() else: self.misc.show() self.misc.gain_focus()",if widget:
"def is_checked_sls_template(template): if template.__contains__('provider'): if isinstance(template['provider'], dict_node): if template['provider'].get('name').lower() not in SUPPORTED_PROVIDERS: return False if isinstance(template['provider'], str_node): <IF_STMT> return False return True return False",if template['provider'] not in SUPPORTED_PROVIDERS:
"def check_index(self, is_sorted=True, unique=True, index=None): """"""Sanity checks"""""" if not index: index = self.index if is_sorted: test = pd.DataFrame(lrange(len(index)), index=index) test_sorted = test.sort() <IF_STMT> raise Exception('Data is not be sorted') if unique: if len(index) != len(index.unique()): raise Exception('Duplicate index entries')",if not test.index.equals(test_sorted.index):
"def _update_actions(self, *_ignored): """"""Updates menu actions to reflect the current layer's mode"""""" if self._updating: return self._updating = True rootstack = self._model.layer_stack current = rootstack.current for mode, item in self._menu_items: active = mode == current.mode <IF_STMT> item.set_active(active) item.set_sensitive(mode in current.PERMITTED_MODES) self._updating = False",if bool(item.get_active()) != active:
"def _charlabels(self, options): """"""Get labels for characters (PRIVATE)."""""" self.charlabels = {} opts = CharBuffer(options) while True: w = opts.next_word() <IF_STMT> break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state c = opts.next_nonwhitespace() if c is None: break elif c != ',': raise NexusError(""Missing ',' in line %s."" % options)",if w is None:
def get_and_set_titles(self): all_titles = [] for page in self.pages: <IF_STMT> all_titles.append(page.orig_phrase) all_titles.append(page.orig_phrase_norm) if page.wiki_title != '': all_titles.append(page.wiki_title) all_titles.append(page.wiki_title_norm) return set(all_titles),if page.orig_phrase != '':
"def get_content_length(download): try: meta = download.info() <IF_STMT> return int(meta.getheaders('Content-Length')[0]) elif hasattr(download, 'getheader') and download.getheader('Content-Length'): return int(download.getheader('Content-Length')) elif hasattr(meta, 'getheader') and meta.getheader('Content-Length'): return int(meta.getheader('Content-Length')) except Exception: pass return 0","if hasattr(meta, 'getheaders') and hasattr(meta.getheaders, 'Content-Length'):"
"def connect_reader_to_writer(reader, writer): BUF_SIZE = 8192 try: while True: data = await reader.read(BUF_SIZE) if not data: <IF_STMT> writer.write_eof() await writer.drain() return writer.write(data) await writer.drain() except (OSError, asyncio.IncompleteReadError) as e: pass",if not writer.transport.is_closing():
"def _record_shell(ex, files, bind_rez=True, print_msg=False): ex.source(context_file) if startup_sequence['envvar']: ex.unsetenv(startup_sequence['envvar']) if add_rez and bind_rez: ex.interpreter._bind_interactive_rez() if print_msg and add_rez and (not quiet): ex.info('') ex.info('You are now in a rez-configured environment.') ex.info('') <IF_STMT> ex.command('rezolve context')",if system.is_production_rez_install:
"def set_torrent_ratio(self, torrent_ids, ratio): try: if not self.connect(): return False self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get() self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get() except Exception as err: return False finally: <IF_STMT> self.disconnect() return True",if self.client:
"def __decrypt_bin_sum(encrypted_bin_sum, cipher): decrypted_list = {} for col_name, count_list in encrypted_bin_sum.items(): new_list = [] for event_count, non_event_count in count_list: <IF_STMT> event_count = cipher.decrypt(event_count) if isinstance(non_event_count, PaillierEncryptedNumber): non_event_count = cipher.decrypt(non_event_count) new_list.append((event_count, non_event_count)) decrypted_list[col_name] = new_list return decrypted_list","if isinstance(event_count, PaillierEncryptedNumber):"
"def processVideo(self, track): video = Metadata(self) self.trackCommon(track, video) try: video.compression = track['CodecID/string'].value <IF_STMT> video.width = track['Video/PixelWidth/unsigned'].value video.height = track['Video/PixelHeight/unsigned'].value except MissingField: pass self.addGroup('video[]', video, 'Video stream')",if 'Video' in track:
"def check_br_addr(self, br): ips = {} cmd = 'ip a show dev %s' % br for line in self.execute(cmd, sudo=True).split('\n'): <IF_STMT> elems = [e.strip() for e in line.strip().split(' ')] ips[4] = elems[1] elif line.strip().startswith('inet6 '): elems = [e.strip() for e in line.strip().split(' ')] ips[6] = elems[1] return ips",if line.strip().startswith('inet '):
"def _find_line_in_file(file_path, search_pattern): try: with open(file_path, 'r', encoding='utf-8') as search_file: for line in search_file: <IF_STMT> return True except (OSError, IOError): pass return False",if search_pattern in line:
"def setOption(self, key, value): if key in VALID_OPTIONS: old = self.getOption(key) result = VALID_OPTIONS[key](self, value) self.notifyOptionChanged(key, old, value) <IF_STMT> return result[1] else: raise RopperError('Invalid value for option %s: %s' % (key, value)) else: raise RopperError('Invalid option')",if result:
"def _para_exploit(self, params, part): if len(params) == 0: arr = ['*', 'config'] + self._configs.keys() return suggest(arr, part) if len(params) == 1: arr = [] if params[0] == 'config': arr = self._configs.keys() <IF_STMT> arr = ['stopOnFirst'] return suggest(arr, part) return []",if params[0] == '*':
"def render(self, context): for var in self.vars: value = var.resolve(context, True) if value: first = render_value_in_context(value, context) <IF_STMT> context[self.asvar] = first return '' return first return ''",if self.asvar:
"def insertTestData(self, rows): for row in rows: if isinstance(row, Log): self.logs[row.id] = row.values.copy() for row in rows: if isinstance(row, LogChunk): lines = self.log_lines.setdefault(row.logid, []) <IF_STMT> lines.append([None] * (row.last_line + 1 - len(lines))) row_lines = row.content.decode('utf-8').split('\n') lines[row.first_line:row.last_line + 1] = row_lines",if len(lines) < row.last_line + 1:
"def set_available_qty(self): for d in self.get('required_items'): if d.source_warehouse: d.available_qty_at_source_warehouse = get_latest_stock_qty(d.item_code, d.source_warehouse) <IF_STMT> d.available_qty_at_wip_warehouse = get_latest_stock_qty(d.item_code, self.wip_warehouse)",if self.wip_warehouse:
"def add_pref_observer(self, name, callback): self.log.debug('Adding pref observer for %s', name) try: self._observers[name].add(callback) except KeyError: self._observers[name] = set([callback]) <IF_STMT> self._send(command='global-prefs-observe', add=[name]) else: pass",if self._send:
"def __setattr__(self, key: str, value) -> None: try: object.__getattribute__(self, key) return object.__setattr__(self, key, value) except AttributeError: pass if (key,) in self._internal.column_labels: self[key] = value else: msg = ""Koalas doesn't allow columns to be created via a new attribute name"" <IF_STMT> raise AssertionError(msg) else: warnings.warn(msg, UserWarning)",if is_testing():
"def inverse_transform(self, X): results = [] column_counter = 0 for i, binarizer in enumerate(self.binarizers): n_cols = binarizer.classes_.shape[0] x_subset = X[:, column_counter:column_counter + n_cols] inv = binarizer.inverse_transform(x_subset) <IF_STMT> inv = inv[:, np.newaxis] results.append(inv) column_counter += n_cols return np.concatenate(results, axis=1)",if len(inv.shape) == 1:
"def default_generator(self, dataset, epochs=1, mode='fit', deterministic=True, pad_batches=True): for epoch in range(epochs): for X_b, y_b, w_b, ids_b in dataset.iterbatches(batch_size=self.batch_size, deterministic=deterministic, pad_batches=pad_batches): <IF_STMT> dropout = np.array(False) else: dropout = np.array(True) yield ([X_b, dropout], [y_b], [w_b])",if mode == 'predict':
"def modif(dir, name, fun): """"""Call a substitution function"""""" if name == '*': lst = [] for y in '. Tools extras'.split(): for x in os.listdir(os.path.join(dir, y)): <IF_STMT> lst.append(y + os.sep + x) for x in lst: modif(dir, x, fun) return filename = os.path.join(dir, name) with open(filename, 'r') as f: txt = f.read() txt = fun(txt) with open(filename, 'w') as f: f.write(txt)",if x.endswith('.py'):
"def find_last_match(view, what, start, end, flags=0): """"""Find last occurrence of `what` between `start`, `end`."""""" match = view.find(what, start, flags) new_match = None while match: new_match = view.find(what, match.end(), flags) <IF_STMT> match = new_match else: return match",if new_match and new_match.end() <= end:
"def to_dynamic_cwd_tuple(x): """"""Convert to a canonical cwd_width tuple."""""" unit = 'c' if isinstance(x, str): <IF_STMT> x = x[:-1] unit = '%' else: unit = 'c' return (float(x), unit) else: return (float(x[0]), x[1])",if x[-1] == '%':
"def get_lprobs_and_target(self, model, net_output, sample): lprobs = model.get_normalized_probs(net_output, log_probs=True) target = model.get_targets(sample, net_output) if self.ignore_prefix_size > 0: <IF_STMT> lprobs = lprobs[:, self.ignore_prefix_size:, :].contiguous() target = target[:, self.ignore_prefix_size:].contiguous() else: lprobs = lprobs[self.ignore_prefix_size:, :, :].contiguous() target = target[self.ignore_prefix_size:, :].contiguous() return (lprobs.view(-1, lprobs.size(-1)), target.view(-1))","if getattr(lprobs, 'batch_first', False):"
"def _charlabels(self, options): """"""Get labels for characters (PRIVATE)."""""" self.charlabels = {} opts = CharBuffer(options) while True: w = opts.next_word() if w is None: break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state c = opts.next_nonwhitespace() <IF_STMT> break elif c != ',': raise NexusError(""Missing ',' in line %s."" % options)",if c is None:
"def _parseContributors(self, roleType, Contributors): if Contributors is None: return None try: ret = [] for item in Contributors['items']: <IF_STMT> ret.append(item['name']) return ret except: return None",if item['role'] == roleType:
"def _data_interp(self): if self.pending_points: points = list(self.pending_points) <IF_STMT> values = self.ip()(self._scale(points)) else: values = np.zeros((len(points), self.vdim)) return (points, values) return (np.zeros((0, 2)), np.zeros((0, self.vdim), dtype=float))",if self.bounds_are_done:
def _initCaseSets(self): self._cs = {} self._css = {} for cs in self._caseSets: <IF_STMT> self._cs[cs.CaseSetName] = {} self._css[cs.CaseSetName] = cs else: raise Exception('duplicate case set name') for c in cs.Cases: idx = tuple(c.index) if not self._cs[cs.CaseSetName].has_key(idx): self._cs[cs.CaseSetName][idx] = c else: raise Exception('duplicate case index'),if not self._cs.has_key(cs.CaseSetName):
"def _organize_data(self, data): temporary = {} for line in data.splitlines(): category, _, value = line.partition(' ') <IF_STMT> key, _, value = value.partition(' ') temporary[key] = value else: temporary[category] = value return temporary","if category in ('set', 'tag'):"
"def get(self): """"""Returns a simple HTML for contact form"""""" if self.user: user_info = models.User.get_by_id(long(self.user_id)) if user_info.name or user_info.last_name: self.form.name.data = user_info.name + ' ' + user_info.last_name <IF_STMT> self.form.email.data = user_info.email params = {'exception': self.request.get('exception')} return self.render_template('boilerplate_contact.html', **params)",if user_info.email:
"def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f['f'].splitlines(): cols = line.rstrip().split('\t') if cols[0] == '#bamPEFragmentSize': continue <IF_STMT> continue else: s_name = self.clean_s_name(cols[2].rstrip().split('/')[-1], f['root']) if s_name != lastsample: d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d",elif cols[0] == 'Size':
"def _related(self): if self.__related is None: results = requests.get(f'{self._wordnet_corpus_reader.host()}/api/synsets/{self.pos()}/{self.offset()}/relations/?format=json', timeout=(30.0, 90.0)) <IF_STMT> self.__related = results.json()['results'][0]['relations'] else: self.__related = [] return self.__related",if results and len(results.json()['results']) != 0:
"def autoname(self): if self.company: suffix = ' - ' + frappe.get_cached_value('Company', self.company, 'abbr') <IF_STMT> self.name = self.warehouse_name + suffix else: self.name = self.warehouse_name",if not self.warehouse_name.endswith(suffix):
"def escape_string(self, value): value = EscapedString.promote(value) value = value.expanduser() result = '' for is_literal, txt in value.strings: <IF_STMT> txt = pipes.quote(txt) if not txt.startswith(""'""): txt = ""'%s'"" % txt else: txt = txt.replace('\\', '\\\\') txt = txt.replace('""', '\\""') txt = '""%s""' % txt result += txt return result",if is_literal:
"def downgrade_wsgi_ux_to_1x(environ): """"""Return a new environ dict for WSGI 1.x from the given WSGI u.x environ."""""" env1x = {} url_encoding = environ[ntou('wsgi.url_encoding')] for k, v in list(environ.items()): if k in [ntou('PATH_INFO'), ntou('SCRIPT_NAME'), ntou('QUERY_STRING')]: v = v.encode(url_encoding) <IF_STMT> v = v.encode('ISO-8859-1') env1x[k.encode('ISO-8859-1')] = v return env1x","elif isinstance(v, unicodestr):"
"def nearest_sources_Point(self, point: Point, max_dist=float('inf')): bp, bn, bi, bd = (None, None, None, None) for rfsource in self.rfsources: <IF_STMT> continue hp, hn, hi, hd = rfsource.nearest(point, max_dist=max_dist) if bp is None or (hp is not None and hd < bd): bp, bn, bi, bd = (hp, hn, hi, hd) return (bp, bn, bi, bd)",if not self.get_rfsource_snap(rfsource):
"def restoreParent(self): if self.sid.isRoot: return with self.suspendMouseButtonNavigation(): confirm, opt = self.confirmRestore((self.path,)) if not confirm: return <IF_STMT> return rd = RestoreDialog(self, self.sid, self.path, **opt) rd.exec()",if opt['delete'] and (not self.confirmDelete(warnRoot=self.path == '/')):
"def connect(self): if self.reserved_ports: self.get_reserved_port() self.sock.settimeout(10) max_attempts = 3 for i in range(max_attempts): try: rv = super(WSClient, self).connect() except OSError as e: <IF_STMT> continue raise else: break if self.sock: self.sock.settimeout(None) return rv",if e.errno == errno.EADDRINUSE and i < max_attempts - 1:
"def step(self, action): assert self.action_space.contains(action) if self._state == 4: if action and self._case: return (self._state, 10.0, True, {}) else: return (self._state, -10, True, {}) elif action: if self._state == 0: self._state = 2 else: self._state += 1 <IF_STMT> self._state = self._case return (self._state, -1, False, {})",elif self._state == 2:
"def process(self): inputs = self.node.inputs outputs = self.node.outputs data = [s.sv_get()[0] for s in inputs] for socket, ref in zip(outputs, self.outputs): <IF_STMT> func = getattr(self, ref[2]) out = tuple(itertools.starmap(func, sv_zip_longest(*data))) socket.sv_set(out)",if socket.links:
"def filter_queryset(self, request, queryset, view): if self.filter_name in request.QUERY_PARAMS or self.exclude_param_name in request.QUERY_PARAMS: projects_ids_subquery = self.filter_user_projects(request) <IF_STMT> queryset = queryset.filter(project_id__in=projects_ids_subquery) return super().filter_queryset(request, queryset, view)",if projects_ids_subquery:
"def _is_port_in_range(self, ports_list): for port_range in ports_list[0]: port = force_int(port_range) if port and self.port == port: return True <IF_STMT> try: [from_port, to_port] = port_range.split('-') if int(from_port) <= self.port <= int(to_port): return True except Exception: return CheckResult.UNKNOWN return False",if port is None and '-' in port_range:
"def apply_to(cls, lexer): lexer.setFont(Font().load()) for name, font in cls.__dict__.items(): if not isinstance(font, Font): continue <IF_STMT> style_num = getattr(lexer, name) lexer.setColor(QColor(font.color), style_num) lexer.setEolFill(True, style_num) lexer.setPaper(QColor(font.paper), style_num) lexer.setFont(font.load(), style_num)","if hasattr(lexer, name):"
"def set_columns(worksheet, c, lengths): for col, j in enumerate(c): if j == 'Value': j = ' ' * 18 <IF_STMT> j = 'Descr' lengths[col] = max(len(j) + 5, lengths[col]) worksheet.set_column(col, col, lengths[col])",if j == 'Description':
"def _remove_listners(self): object = self.object kids = self.children_cache for key, val in kids.items(): <IF_STMT> vtk_obj = tvtk.to_vtk(val) messenger.disconnect(vtk_obj, 'ModifiedEvent', self._notify_children) else: object.on_trait_change(self._notify_children, key, remove=True)","if isinstance(val, tvtk.Collection):"
"def add(self, undoinfo, msg=None): if not undoinfo: return if msg is not None: <IF_STMT> undoinfo = (msg,) + undoinfo[1:] elif isinstance(undoinfo, tuple): undoinfo = (msg,) + undoinfo else: undoinfo = (msg, undoinfo) f = 1 else: f = int(isinstance(undoinfo[0], str)) assert isinstance(undoinfo, list) or callable(undoinfo[f]) or isinstance(undoinfo[f], list) self.undoList.append(undoinfo) del self.redoList[:]","if isinstance(undoinfo[0], str):"
"def assert_last_day(self, period_end): if period_end.month in [9, 4, 6, 11]: self.assertEqual(period_end.day, 30) elif period_end.month != 2: self.assertEqual(period_end.day, 31) el<IF_STMT> self.assertEqual(period_end.day, 29) else: self.assertEqual(period_end.day, 28)",if calendar.isleap(period_end.year):
"def remove_callback(self, callback, events=None): if events is None: for event in self._plugin_lifecycle_callbacks: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback) else: <IF_STMT> events = [events] for event in events: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback)","if isinstance(events, basestring):"
"def get_count(self, peek=False): if self.argument_supplied: count = self.argument_value if self.argument_negative: <IF_STMT> count = -1 else: count = -count if not peek: self.argument_negative = False if not peek: self.argument_supplied = False else: count = 1 return count",if count == 0:
"def is_alive(self): if not self.runqemu: return False if os.path.isfile(self.qemu_pidfile): f = open(self.qemu_pidfile, 'r') qemu_pid = f.read() f.close() qemupid = int(qemu_pid) <IF_STMT> self.qemupid = qemupid return True return False",if os.path.exists('/proc/' + str(qemupid)):
"def contains(self, other_route): if isinstance(other_route, list): return self.to_list()[0:len(other_route)] == other_route assert len(other_route.outgoing) <= 1, 'contains(..) cannot be called after a merge' assert len(self.outgoing) <= 1, 'contains(..) cannot be called after a merge' if other_route.task_spec == self.task_spec: if other_route.outgoing and self.outgoing: return self.outgoing[0].contains(other_route.outgoing[0]) elif self.outgoing: return True <IF_STMT> return True return False",elif not other_route.outgoing:
"def _add_connection(self, connection, uri=None): with self._connections_lock: connection_id = connection.connection_id <IF_STMT> self._connections[connection_id] = ConnectionInfo(ConnectionType.OUTBOUND_CONNECTION, connection, uri, None, None)",if connection_id not in self._connections:
def view(input_path): if not exists(input_path): raise IOError('{0} not found'.format(input_path)) ua = None bundle_info = None try: archive = archive_factory(input_path) if archive is None: raise NotMatched('No matching archive type found') ua = archive.unarchive_to_temp() bundle_info = ua.bundle.info finally: <IF_STMT> ua.remove() return bundle_info,if ua is not None:
"def _expect_fail_and_reconnect(self, num_reconnects, fail_last=False): self._fake_backend.connect.expect_call(**_CONNECT_KWARGS).and_raises(FakeDatabaseError()) for i in xrange(num_reconnects): time.sleep.expect_call(_RECONNECT_DELAY) <IF_STMT> self._expect_reconnect(fail=True) else: self._expect_reconnect(fail=fail_last)",if i < num_reconnects - 1:
def _trigger_step(self): if self._enable_step: if self.local_step != self.trainer.steps_per_epoch - 1: self._trigger() el<IF_STMT> self._trigger(),if not self._enable_epoch:
def draw_label(self): if self.hide: <IF_STMT> seed = ' + ({0})'.format(str(int(self.seed))) else: seed = ' + seed(s)' return self.noise_type.title() + seed else: return self.label or self.name,if not self.inputs['Seed'].is_linked:
"def get_adapter(self, pattern=None): adapters = self.get_adapters() if pattern is None: if len(adapters): return adapters[0] else: raise DBusNoSuchAdapterError('No adapter(s) found') else: for adapter in adapters: path = adapter.get_object_path() <IF_STMT> return adapter raise DBusNoSuchAdapterError('No adapters found with pattern: %s' % pattern)",if path.endswith(pattern) or adapter['Address'] == pattern:
"def substituteargs(self, pattern, replacement, old): new = [] for k in range(len(replacement)): item = replacement[k] newitem = [item[0], item[1], item[2]] for i in range(3): <IF_STMT> newitem[i] = old[k][i] elif item[i][:1] == '$': index = int(item[i][1:]) - 1 newitem[i] = old[index][i] new.append(tuple(newitem)) return new",if item[i] == '*':
def profiling_startup(): if '--profile-sverchok-startup' in sys.argv: global _profile_nesting profile = None try: profile = get_global_profile() _profile_nesting += 1 if _profile_nesting == 1: profile.enable() yield profile finally: _profile_nesting -= 1 <IF_STMT> profile.disable() dump_stats(file_path='sverchok_profile.txt') save_stats('sverchok_profile.prof') else: yield None,if _profile_nesting == 0 and profile is not None:
"def align(size): if size <= 4096: if is_power2(size): return size elif size < 128: return min_ge(range(16, 128 + 1, 16), size) <IF_STMT> return min_ge(range(192, 512 + 1, 64), size) else: return min_ge(range(768, 4096 + 1, 256), size) elif size < 4194304: return min_ge(range(4096, 4194304 + 1, 4096), size) else: return min_ge(range(4194304, 536870912 + 1, 4194304), size)",elif size < 512:
"def _validate(self, event): new = self.value if new is not None and (self.start is not None and self.start > new or (self.end is not None and self.end < new)): value = datetime.strftime(new, self.format) start = datetime.strftime(self.start, self.format) end = datetime.strftime(self.end, self.format) <IF_STMT> self.value = event.old raise ValueError('DatetimeInput value must be between {start} and {end}, supplied value is {value}'.format(start=start, end=end, value=value))",if event:
"def parse(filename): dead_links = [] with open(filename, 'r') as file_: for line in file_.readlines(): res = reference_line.search(line) if res: <IF_STMT> dead_links.append(res.group(1)) return dead_links",if not exists(res.group(1)):
"def __getstate__(self): state = super(_GeneralExpressionDataImpl, self).__getstate__() for i in _GeneralExpressionDataImpl.__expression_slots__: state[i] = getattr(self, i) if safe_mode: state['_parent_expr'] = None if self._parent_expr is not None: _parent_expr = self._parent_expr() <IF_STMT> state['_parent_expr'] = _parent_expr return state",if _parent_expr is not None:
"def insertText(self, data, parent=None): data = data if parent != self: _base.TreeBuilder.insertText(self, data, parent) else: if hasattr(self.dom, '_child_node_types'): <IF_STMT> self.dom._child_node_types = list(self.dom._child_node_types) self.dom._child_node_types.append(Node.TEXT_NODE) self.dom.appendChild(self.dom.createTextNode(data))",if Node.TEXT_NODE not in self.dom._child_node_types:
"def main(args): from argparse import ArgumentParser from sys import stdin, stdout argparser = ArgumentParser() argparser.add_argument('-u', '--unescape', action='store_true') argp = argparser.parse_args(args[1:]) for line in (l.rstrip('\n') for l in stdin): <IF_STMT> r = unescape(line) else: r = escape(line) stdout.write(r) stdout.write('\n')",if argp.unescape:
"def validate_user_json(value, json_schema): try: jsonschema.validate(value, from_json(json_schema)) except jsonschema.ValidationError as e: <IF_STMT> raise InvalidModelValueError(""For '{}' the field value {}"".format(e.path[-1], e.message)) raise InvalidModelValueError(e.message) except jsonschema.SchemaError as e: raise InvalidModelValueError(e.message) validate_dates(value)",if len(e.path) > 1:
"def test_mode(self): with support.temp_umask(2): base = support.TESTFN parent = os.path.join(base, 'dir1') path = os.path.join(parent, 'dir2') os.makedirs(path, 365) self.assertTrue(os.path.exists(path)) self.assertTrue(os.path.isdir(path)) <IF_STMT> self.assertEqual(os.stat(path).st_mode & 511, 365) self.assertEqual(os.stat(parent).st_mode & 511, 509)",if os.name != 'nt':
"def __get_annotations(self): if not hasattr(self, '_annotations'): self._annotations = _retrieve_annotations(self._adaptor, self._primary_id, self._taxon_id) <IF_STMT> self._annotations['gi'] = self._identifier if self._division: self._annotations['data_file_division'] = self._division return self._annotations",if self._identifier:
"def string(self): """"""Returns a PlayString in string format from the Patterns values"""""" string = '' for item in self.data: <IF_STMT> string += item.string() elif isinstance(item, Pattern): string += '(' + ''.join([s.string() if hasattr(s, 'string') else str(s) for s in item.data]) + ')' else: string += str(item) return string","if isinstance(item, (PGroup, GeneratorPattern)):"
"def __getattribute__(self, item): try: val = self[item] <IF_STMT> val = import_string(val) elif isinstance(val, (list, tuple)): val = [import_string(v) if isinstance(v, str) else v for v in val] self[item] = val except KeyError: val = super(ObjDict, self).__getattribute__(item) return val","if isinstance(val, str):"
"def get_identifiers(self): ids = [] ifaces = [i['name'] for i in self.middleware.call_sync('interface.query')] for entry in glob.glob(f'{self._base_path}/interface-*'): ident = entry.rsplit('-', 1)[-1] <IF_STMT> continue if os.path.exists(os.path.join(entry, 'if_octets.rrd')): ids.append(ident) ids.sort(key=RRDBase._sort_disks) return ids",if ident not in ifaces:
"def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: if not form.has_changed(): continue <IF_STMT> continue self.new_objects.append(self.save_new(form, commit=commit)) if not commit: self.saved_forms.append(form) return self.new_objects",if self.can_delete and self._should_delete_form(form):
"def _get_seccomp_whitelist(self): whitelist = [False] * MAX_SYSCALL_NUMBER index = _SYSCALL_INDICIES[NATIVE_ABI] for i in range(SYSCALL_COUNT): if i in (sys_exit, sys_exit_group): continue handler = self._security.get(i, DISALLOW) for call in translator[i][index]: <IF_STMT> continue if isinstance(handler, int): whitelist[call] = handler == ALLOW return whitelist",if call is None:
"def start_check(aggregate, out): """"""Start checking in background and write encoded output to out."""""" t = threading.Thread(target=director.check_urls, args=(aggregate,)) t.start() sleep_seconds = 2 run_seconds = 0 while not aggregate.is_finished(): yield out.get_data() time.sleep(sleep_seconds) run_seconds += sleep_seconds <IF_STMT> director.abort(aggregate) break yield out.get_data()",if run_seconds > MAX_REQUEST_SECONDS:
"def _prune_resource_identifiers(self, all_resources, all_operations): used_identifiers = self._get_identifiers_referenced_by_operations(all_operations) for resource, resource_data in list(all_resources.items()): identifiers = resource_data['resourceIdentifier'] known_ids_for_resource = used_identifiers.get(resource, set()) for identifier_name in list(identifiers): <IF_STMT> del identifiers[identifier_name] if not identifiers: del all_resources[resource]",if identifier_name not in known_ids_for_resource:
"def has_valid_checksum(self, number): given_number, given_checksum = (number[:-1], number[-1]) calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) if fragment.isalnum(): calculated_checksum += int(fragment[-1]) <IF_STMT> parameter = 7 elif parameter == 3: parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum",if parameter == 1:
"def _poll_until_not(url, pending_statuses, err_msg): while True: result, _, _ = _do_request(url, err_msg=err_msg) <IF_STMT> time.sleep(2) continue return result",if result['status'] in pending_statuses:
"def wrapper(request, *args, **kw): if switch_is_active('disable-bigquery'): <IF_STMT> response = http.HttpResponse(content_type='text/csv; charset=utf-8') else: response = http.HttpResponse(content_type='application/json', content='[]') response.status_code = 503 return response return f(request, *args, **kw)",if kw.get('format') == 'csv':
"def completion_safe_apply(ctx, f, args): from guild import config with config.SetGuildHome(ctx.parent.params.get('guild_home')): try: return f(*args) except (Exception, SystemExit): <IF_STMT> raise return None",if os.getenv('_GUILD_COMPLETE_DEBUG') == '1':
"def configure(self, **kw): """"""Configure the image."""""" res = () for k, v in _cnfmerge(kw).items(): <IF_STMT> if k[-1] == '_': k = k[:-1] if hasattr(v, '__call__'): v = self._register(v) elif k in ('data', 'maskdata'): v = self.tk._createbytearray(v) res = res + ('-' + k, v) self.tk.call((self.name, 'config') + res)",if v is not None:
"def _editor_lower(self): editorWidget = main_container.MainContainer().get_actual_editor() if editorWidget: editorWidget.textCursor().beginEditBlock() <IF_STMT> text = editorWidget.textCursor().selectedText().lower() else: text = editorWidget._text_under_cursor().lower() editorWidget.moveCursor(QTextCursor.StartOfWord) editorWidget.moveCursor(QTextCursor.EndOfWord, QTextCursor.KeepAnchor) editorWidget.textCursor().insertText(text) editorWidget.textCursor().endEditBlock()",if editorWidget.textCursor().hasSelection():
"def on_key_release(self, symbol, modifiers): if symbol == key.LEFT or symbol == key.RIGHT: self.value = not self.value self.text.text = self.get_label() self.toggle_func(self.value) <IF_STMT> bullet_sound.play()",if enable_sound:
"def remove_checker(self, namespace, checker): for c in pyomo.core.check.ModelCheckRunner._checkers(all=True): <IF_STMT> if namespace.checkers.get(c._checkerPackage(), None) is not None: for i in range(namespace.checkers[c._checkerPackage()].count(c._checkerName())): namespace.checkers[c._checkerPackage()].remove(c._checkerName())",if c._checkerName() == checker:
"def find_executable(names): for name in names: fpath, fname = os.path.split(name) <IF_STMT> if is_executable(name): return name else: for path in os.environ['PATH'].split(os.pathsep): exe_file = os.path.join(path, name) if is_executable(exe_file): return exe_file return None",if fpath:
"def run(self): while True: self.finished.wait(self.interval) if self.finished.isSet(): return try: self.function(*self.args, **self.kwargs) except Exception: <IF_STMT> self.bus.log('Error in perpetual timer thread function %r.' % self.function, level=40, traceback=True) raise",if self.bus:
"def get_user_object(self, user_id, group): if user_id: user = OSFUser.load(user_id) if not user: raise exceptions.NotFound(detail='User with id {} not found.'.format(user_id)) <IF_STMT> raise exceptions.ValidationError(detail='User is already a member of this group.') return user return user_id","if group.has_permission(user, 'member'):"
"def build_term_table(spec): try: return _term_tables_cache[spec] except KeyError: tbl = {} terms = {} i = 0 for t in spec: which = terms.setdefault(t, 0) tbl[t, which] = i tbl['%s_%d' % (t, which)] = i <IF_STMT> tbl[t] = i terms[t] += 1 i += 1 _term_tables_cache[spec] = tbl return tbl",if which == 0:
"def GetQualifiedWsdlName(type): with _lazyLock: wsdlNSAndName = _wsdlNameMap.get(type) if wsdlNSAndName: return wsdlNSAndName el<IF_STMT> ns = GetWsdlNamespace(type.Item._version) return (ns, 'ArrayOf' + Capitalize(type.Item._wsdlName)) else: ns = GetWsdlNamespace(type._version) return (ns, type._wsdlName)","if issubclass(type, list):"
"def train(config, checkpoint_dir=None): restored = bool(checkpoint_dir) itr = 0 if checkpoint_dir: with open(os.path.join(checkpoint_dir, 'ckpt.log'), 'r') as f: itr = int(f.read()) + 1 for i in range(itr, 10): <IF_STMT> raise Exception('try to fail me') with tune.checkpoint_dir(step=itr) as checkpoint_dir: checkpoint_path = os.path.join(checkpoint_dir, 'ckpt.log') with open(checkpoint_path, 'w') as f: f.write(str(i)) tune.report(test=i, training_iteration=i)",if i == 5 and (not restored):
"def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = (key.fileobj, key.data) if mask & selectors.EVENT_READ and reader is not None: if reader._cancelled: self.remove_reader(fileobj) else: self._add_callback(reader) if mask & selectors.EVENT_WRITE and writer is not None: <IF_STMT> self.remove_writer(fileobj) else: self._add_callback(writer)",if writer._cancelled:
"def _validate_mappings(self): for m in self.mapping.mapping_rules: for policy_id in m.policy_ids: if policy_id not in self.policies: raise ReferencedObjectNotFoundError(reference_id=policy_id, reference_type='policy') for w in m.whitelist_ids: <IF_STMT> raise ReferencedObjectNotFoundError(reference_id=w, reference_type='whitelist')",if w not in self.whitelists:
"def _transform_backward(graph, op): no_dequanted_input_vars = True for var_node in op.inputs: <IF_STMT> dequant_var_node = dequantized_vars[var_node.name()] graph.update_input_link(var_node, dequant_var_node, op) no_dequanted_input_vars = False if no_dequanted_input_vars: raise ValueError('There is no dequanted inputs for op %s.' % op.name())",if var_node.name() in dequantized_vars:
"def should_use_pty(self, pty=False, fallback=True): use_pty = False if pty: use_pty = True <IF_STMT> if not self.warned_about_pty_fallback: err = 'WARNING: stdin has no fileno; falling back to non-pty execution!\n' sys.stderr.write(err) self.warned_about_pty_fallback = True use_pty = False return use_pty",if not has_fileno(sys.stdin) and fallback:
"def _get_default_factory(self, attribute_name: str) -> Any: if hasattr(self, attribute_name): <IF_STMT> return str(getattr(self, attribute_name)) elif str(self.__dataclass_fields__[attribute_name].default).startswith('${'): return str(self.__dataclass_fields__[attribute_name].default) elif getattr(self, attribute_name) != self.__dataclass_fields__[attribute_name].default_factory(): return getattr(self, attribute_name) return self.__dataclass_fields__[attribute_name].default_factory()","if str(getattr(self, attribute_name)).startswith('${'):"
"def create_row_processor(self, context, path, loadopt, mapper, result, adapter, populators): for col in self.columns: <IF_STMT> col = adapter.columns[col] getter = result._getter(col, False) if getter: populators['quick'].append((self.key, getter)) break else: populators['expire'].append((self.key, True))",if adapter:
"def test_finds_multiple_songs(self): for _, album in albums_in_dir(self.base): n = re.search(b'album(.)song', album[0]).group(1) <IF_STMT> self.assertEqual(len(album), 2) else: self.assertEqual(len(album), 1)",if n == b'1':
"def _should_update_cache(self, request, response): if not hasattr(request, '_cache_update_cache') or not request._cache_update_cache: return False if self.cache_anonymous_only and has_vary_header(response, 'Cookie'): assert hasattr(request, 'user'), ""The Django cache middleware with CACHE_MIDDLEWARE_ANONYMOUS_ONLY=True requires authentication middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the CacheMiddleware."" <IF_STMT> return False return True",if request.user.is_authenticated():
"def break_next_call(symbol_regex=None): while pwndbg.proc.alive: ins = break_next_branch() <IF_STMT> break if capstone.CS_GRP_CALL not in ins.groups: continue if not symbol_regex: return ins if ins.target_const and re.match('%s$' % symbol_regex, hex(ins.target)): return ins if ins.symbol and re.match('%s$' % symbol_regex, ins.symbol): return ins",if not ins:
"def parser(cls, buf, offset): type_, len_, vendor = struct.unpack_from(ofproto.OFP_ACTION_VENDOR_HEADER_PACK_STR, buf, offset) data = buf[offset + ofproto.OFP_ACTION_VENDOR_HEADER_SIZE:offset + len_] if vendor == ofproto_common.NX_EXPERIMENTER_ID: obj = NXAction.parse(data) else: cls_ = cls._ACTION_VENDORS.get(vendor, None) <IF_STMT> obj = OFPActionVendorUnknown(vendor, data) else: obj = cls_.parser(buf, offset) obj.len = len_ return obj",if cls_ is None:
"def remove_empty_files(root_path): """"""Removes empty files in a path recursively"""""" for directory, _, filenames in walk(root_path): for filename in filenames: path = os.path.join(directory, filename) <IF_STMT> continue try: os.remove(path) except: logs.log_error('Unable to remove the empty file: %s (%s).' % (path, sys.exc_info()[0]))",if os.path.getsize(path) > 0:
"def _test_set_ipv4_src(self, ip, mask=None): header = ofproto.OXM_OF_IPV4_SRC match = OFPMatch() ip = unpack('!I', socket.inet_aton(ip))[0] if mask is None: match.set_ipv4_src(ip) else: mask = unpack('!I', socket.inet_aton(mask))[0] <IF_STMT> header = ofproto.OXM_OF_IPV4_SRC_W match.set_ipv4_src_masked(ip, mask) self._test_serialize_and_parser(match, header, ip, mask)",if mask + 1 >> 32 != 1:
"def is_valid_block(self): """"""check wheter the block is valid in the current position"""""" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False <IF_STMT> return False return True","if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):"
"def __init__(self, *args, **kwargs): dict.__init__(self, *args, **kwargs) for key, value in self.items(): <IF_STMT> raise TypeError('key must be a str, not {}'.format(type(key))) if not isinstance(value, NUMERIC_TYPES): raise TypeError('value must be a NUMERIC_TYPES, not {}'.format(type(value))) if not isinstance(value, float): self[key] = float(value)","if not isinstance(key, string_types):"
"def refresh_committed_offsets_if_needed(self): """"""Fetch committed offsets for assigned partitions."""""" if self._subscription.needs_fetch_committed_offsets: offsets = self.fetch_committed_offsets(self._subscription.assigned_partitions()) for partition, offset in six.iteritems(offsets): <IF_STMT> self._subscription.assignment[partition].committed = offset.offset self._subscription.needs_fetch_committed_offsets = False",if self._subscription.is_assigned(partition):
"def getText(self, stuff): if isinstance(stuff, BaseWrapper): stuff = stuff.item if isinstance(stuff, (Fit, TargetProfile)): val, unit = self._getValue(stuff) <IF_STMT> return '' if self.stickPrefixToValue: return '{} {}'.format(formatAmount(val, *self.formatSpec), unit) else: return formatAmount(val, *self.formatSpec, unitName=unit) return ''",if val is None:
"def __get__(self, inst, owner): try: value, last_update = inst._cache[self.__name__] <IF_STMT> raise AttributeError except (KeyError, AttributeError): value = self.fget(inst) try: cache = inst._cache except AttributeError: cache = inst._cache = {} cache[self.__name__] = (value, time.time()) return value",if self.ttl > 0 and time.time() - last_update > self.ttl:
"def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) <IF_STMT> row = self.get(path[0], 'device') if row: if self.Blueman is not None: if self.menu is None: self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)",if path is not None:
"def groups(self): """"""Return a dictionary mapping group names to JIDs."""""" result = {} for jid in self._jids: groups = self._jids[jid]['groups'] <IF_STMT> if '' not in result: result[''] = [] result[''].append(jid) for group in groups: if group not in result: result[group] = [] result[group].append(jid) return result",if not groups:
"def set_meta(self, dataset, overwrite=True, **kwd): super().set_meta(dataset, overwrite=overwrite, **kwd) try: conn = sqlite.connect(dataset.file_name) c = conn.cursor() version_query = 'SELECT version FROM meta' results = c.execute(version_query).fetchall() if len(results) == 0: raise Exception('version not found in meta table') <IF_STMT> raise Exception('Multiple versions found in meta table') dataset.metadata.gafa_schema_version = results[0][0] except Exception as e: log.warning('%s, set_meta Exception: %s', self, e)",elif len(results) > 1:
"def GetSelectedCount(self): if self.GetStyleL('style') & self.Style.LBS_MULTIPLESEL: return self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) <IF_STMT> return 0 return 1",if result == LB_ERR:
"def emit(self, record): try: item = QListWidgetItem(self.format(record)) <IF_STMT> item.setIcon(QIcon.fromTheme('dialog-warning')) item.setForeground(QBrush(Qt.red)) else: item.setIcon(QIcon.fromTheme('dialog-information')) self.app.exec_in_main(self._add_item, item) except (KeyboardInterrupt, SystemExit): raise except: self.handleError(record)",if record.levelno > logging.INFO:
"def _updater(data): assert data['attrs']['tvm_version'].startswith(from_ver) nodes = data['nodes'] for idx, item in enumerate(nodes): f = node_map.get(item['type_key'], None) <IF_STMT> for fpass in f: item = fpass(item, nodes) elif f: item = f(item, nodes) nodes[idx] = item data['attrs']['tvm_version'] = to_ver return data","if isinstance(f, list):"
def remove_data(self): if self.path is not None: <IF_STMT> os.remove(self.path) if os.path.exists(self.get_json_path()): os.remove(self.get_json_path()),if os.path.exists(self.path):
"def testsingle(self, sym): if self.settings == 'asterisk': return (sym == '*', '*') if self.settings == 'plus': return (sym == '+', '+') if self.settings == 'dash': return (sym == '-', '-') if self.settings == 'single': <IF_STMT> return (self.lastSym == sym, self.lastSym) else: self.lastSym = sym return (True, None) return (None, None)",if self.lastSym:
"def update(self, other_dict, option_parser): if isinstance(other_dict, Values): other_dict = other_dict.__dict__ other_dict = other_dict.copy() for setting in option_parser.lists.keys(): <IF_STMT> value = getattr(self, setting) if value: value += other_dict[setting] del other_dict[setting] self._update_loose(other_dict)","if hasattr(self, setting) and setting in other_dict:"
"def gprv_immv(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: <IF_STMT> continue else: return False elif i == 1: if op_immv(op): continue else: return False else: return False return True","if op.name == 'REG0' and op_luf_start(op, 'GPRv'):"
"def __call__(self, input_tensors, shape): if self.order in 'KA': <IF_STMT> order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER elif self.order == 'C': order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)",if any((t.order == TensorOrder.C_ORDER for t in input_tensors)):
"def check_selected(menu, path): selected = False if 'url' in menu: chop_index = menu['url'].find('?') <IF_STMT> selected = path.startswith(menu['url']) else: selected = path.startswith(menu['url'][:chop_index]) if 'menus' in menu: for m in menu['menus']: _s = check_selected(m, path) if _s: selected = True if selected: menu['selected'] = True return selected",if chop_index == -1:
"def _check_events(self): stack = [] old = self.events[:] for type_, song in self.events: if type_ == 'started': stack.append(song) <IF_STMT> self.assertTrue(stack.pop(-1) is song, msg=old) self.assertFalse(stack, msg=old)",elif type_ == 'ended':
"def __fixdict(self, dict): for key in dict.keys(): if key[:6] == 'start_': tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = (getattr(self, key), end) <IF_STMT> tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = (start, getattr(self, key))",elif key[:4] == 'end_':
"def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): <IF_STMT> if not nested_match(v, value[k]): return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): if not nested_match(x, y): return False return True return False",if k in value:
"def code_match(code, select, ignore): if ignore: assert not isinstance(ignore, unicode) for ignored_code in [c.strip() for c in ignore]: <IF_STMT> return False if select: assert not isinstance(select, unicode) for selected_code in [c.strip() for c in select]: if mutual_startswith(code.lower(), selected_code.lower()): return True return False return True","if mutual_startswith(code.lower(), ignored_code.lower()):"
"def test_cardinality_m2o(self): m2o_type_fields = [f for f in self.fields_and_reverse_objects if f.is_relation and f.many_to_one] self.assertEqual(MANY_TO_ONE_CLASSES, {f.__class__ for f in m2o_type_fields}) for obj in m2o_type_fields: <IF_STMT> reverse_field = obj.field self.assertTrue(reverse_field.is_relation and reverse_field.one_to_many)","if hasattr(obj, 'field'):"
"def flatten_dict(self, request): dct = super(KnowledgeFolderHandler, self).flatten_dict(request) dct['knowledgeType_id'] = None parent = request.data.get('parent') if parent: parent = getOrNone(KnowledgeFolder, pk=parent) <IF_STMT> request.data['parent'] = None return dct","if not parent or not request.user.profile.has_permission(parent, mode='x'):"
def delete_oidc_session_tokens(session): if session: if 'oidc_access_token' in session: del session['oidc_access_token'] <IF_STMT> del session['oidc_id_token'] if 'oidc_id_token_expiration' in session: del session['oidc_id_token_expiration'] if 'oidc_login_next' in session: del session['oidc_login_next'] if 'oidc_refresh_token' in session: del session['oidc_refresh_token'] if 'oidc_state' in session: del session['oidc_state'],if 'oidc_id_token' in session:
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') if sty.italic: fragment = '<i>%s</i>' % fragment if sty.underline: fragment = '<u>%s</u>' % fragment <IF_STMT> fragment = '<s>%s</s>' % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.strikeout:
"def test_reduce_different_name(ray_start_distributed_2_nodes_4_gpus, group_name, dst_rank): world_size = 4 actors, _ = create_collective_workers(num_workers=world_size, group_name=group_name) results = ray.get([a.do_reduce.remote(group_name, dst_rank) for a in actors]) for i in range(world_size): <IF_STMT> assert (results[i] == cp.ones((10,), dtype=cp.float32) * world_size).all() else: assert (results[i] == cp.ones((10,), dtype=cp.float32)).all()",if i == dst_rank:
"def _find_docstrings(self, filename): strs = set() prev = token.INDENT with tokenize_open(filename) as f: tokens = tokenize.generate_tokens(f.readline) for ttype, tstr, start, end, line in tokens: <IF_STMT> strs.update(range(start[0], end[0] + 1)) prev = ttype return strs",if ttype == token.STRING and prev == token.INDENT:
"def on_click(self, event): button = event['button'] if button in [self.button_next, self.button_previous]: <IF_STMT> self.scrolling = True if button == self.button_next: self.active_index += 1 elif button == self.button_previous: self.active_index -= 1 self.active_index %= self.count_stations else: self.py3.prevent_refresh() elif button == self.button_refresh: self.idle_time = 0 else: self.py3.prevent_refresh()",if self.station_data:
"def findRule(instance, ruleSet): """"""find the rule(s) that matches the feture vector passed"""""" ruleNumber = 0 ruleMatches = [] for rule in ruleSet: if ruleMatch(rule, instance): ruleMatches.append(ruleNumber) counts[ruleNumber] += 1 <IF_STMT> print(' ruleMatch found at rule #' + str(ruleNumber)) print(' ', end='') printRule(rule) ruleNumber += 1 return ruleMatches",if False:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_peer_ip().TryMerge(tmp) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
def _check_no_tensors(parameters: Params): flat_params = tf.nest.flatten(parameters.params) for p in flat_params: <IF_STMT> _check_no_tensors(p) if tf.is_tensor(p): raise TypeError('Saw a `Tensor` value in parameters:\n  {}'.format(parameters)),"if isinstance(p, Params):"
"def all_zinc_rsc_invalid_dep_keys(invalid_deps): """"""Get the rsc key for an rsc-and-zinc target, or the zinc key for a zinc-only target."""""" for tgt in invalid_deps: tgt_rsc_cc = compile_contexts[tgt].rsc_cc <IF_STMT> yield self._key_for_target_as_dep(tgt, tgt_rsc_cc.workflow)",if tgt_rsc_cc.workflow is not None:
"def characters(self, ch): if self.Text_tag: if self.Summary_tag: self.Summary_ch += ch elif self.Attack_Prerequisite_tag: self.Attack_Prerequisite_ch += ch <IF_STMT> self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch",elif self.Solution_or_Mitigation_tag:
"def load_tool_from_cache(self, config_file, recover_tool=False): tool_cache = getattr(self.app, 'tool_cache', None) tool = None if tool_cache: <IF_STMT> tool = tool_cache.get_removed_tool(config_file) else: tool = tool_cache.get_tool(config_file) return tool",if recover_tool:
"def _generate_examples(self, archive, directory, labeled=True): """"""Generate IMDB examples."""""" reg_path = '(?P<label>neg|pos)' if labeled else 'unsup' reg = re.compile(os.path.join('^%s' % directory, reg_path, '').replace('\\', '\\\\')) for path, imdb_f in archive: res = reg.match(path) <IF_STMT> continue text = imdb_f.read().strip() label = res.groupdict()['label'] if labeled else -1 yield (path, {'text': text, 'label': label})",if not res:
def startInputThread(self): global input try: input = raw_input except NameError: pass while True: cmd = self._queuedCmds.pop(0) if len(self._queuedCmds) else input(self.getPrompt()).strip() wait = self.execCmd(cmd) <IF_STMT> self.acceptingInput = False self.blockingQueue.get(True) self.acceptingInput = True,if wait:
"def assertS_IS(self, name, mode): fmt = getattr(stat, 'S_IF' + name.lstrip('F')) self.assertEqual(stat.S_IFMT(mode), fmt) testname = 'S_IS' + name for funcname in self.format_funcs: func = getattr(stat, funcname, None) if func is None: <IF_STMT> raise ValueError(funcname) continue if funcname == testname: self.assertTrue(func(mode)) else: self.assertFalse(func(mode))",if funcname == testname:
"def test_compatibility(self) -> None: for expected, user_agent in self.data: result = self.client_get('/compatibility', HTTP_USER_AGENT=user_agent) if expected == 'ok': self.assert_json_success(result) <IF_STMT> self.assert_json_error(result, 'Client is too old') else: assert False",elif expected == 'old':
"def getBranchFromFile(): global _gitdir branch = None if _gitdir: headFile = os.path.join(_gitdir, 'HEAD') <IF_STMT> with open(headFile, 'r', encoding='utf-8') as f: line = f.readline() if line: if line.startswith('ref'): branch = line.split('/')[-1].strip() else: branch = 'HEAD' return branch",if os.path.isfile(headFile):
"def get_job_parameters_dict(self, job_parameters: RunParameters=None): if job_parameters: <IF_STMT> self.job_runtime_conf['job_parameters']['common'] = job_parameters.to_dict() else: self.job_runtime_conf['job_parameters'] = job_parameters.to_dict() return self.job_runtime_conf['job_parameters']","if int(self.job_runtime_conf.get('dsl_version', 1)) == 2:"
"def ConnectHandler(*args, **kwargs): """"""Factory function selects the proper class and creates object based on device_type."""""" device_type = kwargs['device_type'] if device_type not in platforms: <IF_STMT> msg_str = platforms_str else: msg_str = telnet_platforms_str if 'telnet' in device_type else platforms_str raise ValueError(""Unsupported 'device_type' currently supported platforms are: {}"".format(msg_str)) ConnectionClass = ssh_dispatcher(device_type) return ConnectionClass(*args, **kwargs)",if device_type is None:
"def get_next_parent_entities(item, pids): ret = list() for [parent, entity_id] in parents[item]: <IF_STMT> continue if parent in entities: ret.append(parent) else: pids.append(entity_id) for p in get_next_parent_entities(parent, pids): ret.append(p) return ret",if entity_id in pids:
"def load(self, data): ckey = None for key, val in _rx_cookie.findall(data): <IF_STMT> if ckey: self[ckey][key] = _unquote(val) elif key[0] == '$': continue else: self[key] = _unquote(val) ckey = key",if key.lower() in _c_keys:
def getIdentifier(self): start = self.index self.index += 1 while self.index < self.length: ch = self.ccode() <IF_STMT> self.index = start return self.getEscapedIdentifier() if isIdentifierPart(ch): self.index += 1 else: break return self.source[start:self.index],if ch == 92:
"def test_floats_unequal_float(self): try: self.assertEqual(np.array([[1, 2], [3, 4.5]], dtype=np.float32), np.array([[1, 2], [3, 5]], dtype=np.float32)) except AssertionError as e: <IF_STMT> raise self.failureException('Float array mismatch error not raised.')",if not str(e).startswith('Arrays not almost equal to 6 decimals'):
"def _set_counts(self): self['regions_count'] = len(self['regions']) for _, key in self._children: <IF_STMT> continue self[key + '_count'] = sum([region[key + '_count'] for region in self['regions'].values()])",if key == 'vpcs':
"def total_form_count(self): """"""Returns the total number of forms in this FormSet."""""" if self.data or self.files: return self.management_form.cleaned_data[TOTAL_FORM_COUNT] else: initial_forms = self.initial_form_count() total_forms = initial_forms + self.extra if initial_forms > self.max_num >= 0: total_forms = initial_forms <IF_STMT> total_forms = self.max_num return total_forms",elif total_forms > self.max_num >= 0:
"def mouse_down(self, evt): if self.parent.level: toolNo = self.toolNumberUnderMouse(evt.pos) if toolNo < 0 or toolNo > 8: return <IF_STMT> self.selectTool(toolNo) if evt.button == 3: self.showToolOptions(toolNo)",if evt.button == 1:
"def find_comment(line): """"""Finds the index of a comment # and returns None if not found"""""" instring, instring_char = (False, '') for i, char in enumerate(line): if char in ('""', ""'""): if instring: if char == instring_char: instring = False instring_char = '' else: instring = True instring_char = char elif char == '#': <IF_STMT> return i return None",if not instring:
"def __getattr__(self, key): if key == key.upper(): if hasattr(self._django_settings, key): return getattr(self._django_settings, key) <IF_STMT> return getattr(self._default_settings, key) raise AttributeError('%r object has no attribute %r' % (self.__class__.__name__, key))","elif hasattr(self._default_settings, key):"
"def replace_entities(match, entities=entities, encoding=encoding): ent = match.group() if ent[1] == '#': return unescape_charref(ent[2:-1], encoding) repl = entities.get(ent) if repl is not None: <IF_STMT> try: repl = repl.decode(encoding) except UnicodeError: repl = ent else: repl = ent return repl","if hasattr(repl, 'decode') and encoding is not None:"
"def test_floor_div(self): """"""Util.number.floor_div"""""" self.assertRaises(TypeError, number.floor_div, '1', 1) for a in range(-10, 10): for b in range(-10, 10): <IF_STMT> self.assertRaises(ZeroDivisionError, number.floor_div, a, b) else: self.assertEqual((a, b, int(math.floor(float(a) / b))), (a, b, number.floor_div(a, b)))",if b == 0:
"def get(self, method, **kws): resp = None if method in self.responses: resp = self.responses[method].pop(0) <IF_STMT> checks = resp['validate']['checks'] resp = resp['validate']['data'] for check in checks: assert check in kws expected_value = checks[check] assert expected_value == kws[check] return resp",if 'validate' in resp:
def __add_changelisteners(self): NewPlayerSettlementHovered.subscribe(self.on_settlement_change) if self.__current_settlement is not None: inventory = self.__current_settlement.get_component(StorageComponent).inventory <IF_STMT> inventory.add_change_listener(self.refresh),if not inventory.has_change_listener(self.refresh):
"def __call__(self, target): if 'weights' not in target.temp: return True targets = target.temp['weights'] for cname in target.children: <IF_STMT> c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) if deviation > self.tolerance: return True if 'cash' in target.temp: cash_deviation = abs((target.capital - targets.value) / targets.value - target.temp['cash']) if cash_deviation > self.tolerance: return True return False",if cname in targets:
"def copyfileobj(src, dest, length=512): if hasattr(src, 'readinto'): buf = bytearray(length) while True: sz = src.readinto(buf) if not sz: break <IF_STMT> dest.write(buf) else: b = memoryview(buf)[:sz] dest.write(b) else: while True: buf = src.read(length) if not buf: break dest.write(buf)",if sz == length:
def test_api_history_restrict_cat(self): slot_sum = 0 cats = list(self.history_category_options) cats.extend('*') for cat in cats: json = self._get_api_history({'category': cat}) slot_sum += len(json['history']['slots']) for slot in json['history']['slots']: <IF_STMT> assert slot['category'] == cat json = self._get_api_history({'limit': self.history_size}) slot_total = len(json['history']['slots']) assert slot_sum == slot_total,if cat != '*':
"def checker(self): while True: try: ip = self.get_ip() except Exception as e: xlog.info('no ip left') return try: res = self.check_ip.check_ip(ip, sni=host, host=host) except Exception as e: xlog.warn('check fail:%s except:%r', e) continue <IF_STMT> xlog.debug('check fail:%s fail', ip) continue self.write_ip(ip, res.domain, res.handshake_time)",if not res or not res.ok:
"def create_row_processor(self, context, path, loadopt, mapper, result, adapter, populators): for col in self.columns: if adapter: col = adapter.columns[col] getter = result._getter(col, False) <IF_STMT> populators['quick'].append((self.key, getter)) break else: populators['expire'].append((self.key, True))",if getter:
"def indices(dimensions, dtype=int32, sparse=False): dimensions = tuple(dimensions) N = len(dimensions) output = [] s = dimensions for i, dim in enumerate(dimensions): idx = lax.iota(dtype, dim) <IF_STMT> s = (1,) * i + (dim,) + (1,) * (N - i - 1) output.append(lax.broadcast_in_dim(idx, s, (i,))) if sparse: return tuple(output) return stack(output, 0) if output else array([], dtype=dtype)",if sparse:
"def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data['given'] for case in test_data['cases']: if 'result' in case: test_type = 'result' <IF_STMT> test_type = 'error' elif 'bench' in case: test_type = 'bench' else: raise RuntimeError('Unknown test type: %s' % json.dumps(case)) yield (given, test_type, case)",elif 'error' in case:
"def _resolve_task_id(cls, task_id, log=None): if not task_id: task_id = cls.normalize_id(get_remote_task_id()) <IF_STMT> log = log or get_logger('task') log.info('Using task ID from env %s=%s' % (TASK_ID_ENV_VAR[0], task_id)) return task_id",if task_id:
"def _build_contr_port_map(self, fabric_connected_ports, ports_info): contr_port_map = {} for port in fabric_connected_ports: contr = ports_info[port]['contr'] <IF_STMT> contr_port_map[contr] = [] contr_port_map[contr].append(port) LOG.debug('Controller port map: %s.', contr_port_map) return contr_port_map",if not contr_port_map.get(contr):
"def confirm(question): """"""Prompts a given question and handles user input."""""" valid = {'yes': True, 'y': True, 'ye': True, 'no': False, 'n': False, '': True} prompt = ' [Y/n] ' while True: print(BOLD + CYAN + question + prompt + END) choice = input().lower() <IF_STMT> return valid[choice] print(""Please respond with 'yes' or 'no' (or 'y' or 'n').\n"")",if choice in valid:
"def __parse_query(self, model, iter_, data): f, b = (self.__filter, self.__bg_filter) if f is None and b is None: return True else: album = model.get_album(iter_) <IF_STMT> return True elif b is None: return f(album) elif f is None: return b(album) else: return b(album) and f(album)",if album is None:
def get_SV(self): result = [] for sparse_sv in self.SV[:self.l]: row = dict() i = 0 while True: row[sparse_sv[i].index] = sparse_sv[i].value <IF_STMT> break i += 1 result.append(row) return result,if sparse_sv[i].index == -1:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_hostname(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def getFileIdFromAlternateLink(altLink): loc = altLink.find('/d/') if loc > 0: fileId = altLink[loc + 3:] loc = fileId.find('/') <IF_STMT> return fileId[:loc] else: loc = altLink.find('/folderview?id=') if loc > 0: fileId = altLink[loc + 15:] loc = fileId.find('&') if loc != -1: return fileId[:loc] controlflow.system_error_exit(2, f'{altLink} is not a valid Drive File alternateLink')",if loc != -1:
"def show_unknown_key_warning(name: Union[str, object], others: dict): if 'type' in others: others.pop('type') if len(others) > 0: keys = ', '.join(others.keys()) logger = logging.getLogger(__name__) <IF_STMT> name = name.__class__.__name__ logger.debug(f""!!! {name}'s constructor args ({keys}) were ignored.If they should be supported by this library, report this issue to the project :bow: https://github.com/slackapi/python-slackclient/issues"")","if isinstance(name, object):"
"def wrapper(*args, **kwargs): with capture_logs() as logs: try: function(*args, **kwargs) except Exception: <IF_STMT> print('%i errors logged:' % len(logs), file=sys.stderr) for message in logs: print(message, file=sys.stderr) raise else: if logs: for message in logs: print(message, file=sys.stderr) raise AssertionError('%i errors logged' % len(logs))",if logs:
"def _init_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2.0 / n)) <IF_STMT> m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_()","elif isinstance(m, SyncBatchNorm):"
"def cleanup(self): for i, s in enumerate(self.synonyms): if s[-1] == '.': <IF_STMT> c = s[:-1] (print >> sys.stderr, ""Note: cleanup: '%s' -> '%s'"" % (s, c)) self.synonyms[i] = c","if re.search('\\b[a-z]{2,}\\.$', s):"
"def for_module(cls, modname: str) -> 'ModuleAnalyzer': if ('module', modname) in cls.cache: entry = cls.cache['module', modname] <IF_STMT> raise entry return entry try: filename, source = cls.get_module_source(modname) if source is not None: obj = cls.for_string(source, modname, filename or '<string>') elif filename is not None: obj = cls.for_file(filename, modname) except PycodeError as err: cls.cache['module', modname] = err raise cls.cache['module', modname] = obj return obj","if isinstance(entry, PycodeError):"
"def GetDisplayNameOf(self, pidl, flags): item = pidl_to_item(pidl) if flags & shellcon.SHGDN_FORPARSING: if flags & shellcon.SHGDN_INFOLDER: return item['name'] else: <IF_STMT> sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING else: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING parent = shell.SHGetNameFromIDList(self.pidl, sigdn) return parent + '\\' + item['name'] else: return item['name']",if flags & shellcon.SHGDN_FORADDRESSBAR:
"def transact_reraise(exc_class, exceptions): cls, exc, tb = exceptions[0] new_exc = None try: msg = ' '.join((tostring(arg) for arg in exc.args)) <IF_STMT> msg = '%s: %s' % (cls.__name__, msg) new_exc = exc_class(msg, exceptions) new_exc.__cause__ = None reraise(exc_class, new_exc, tb) finally: del exceptions, exc, tb, new_exc","if not issubclass(cls, TransactionError):"
"def add_share(self, share): for filename, (share_hashes, verified_hashes) in self.known.iteritems(): <IF_STMT> break else: filename = self._add_line('%i %s' % (5, share_type.pack(share.as_share()).encode('hex'))) share_hashes, verified_hashes = self.known.setdefault(filename, (set(), set())) share_hashes.add(share.hash) share_hashes, verified_hashes = self.known_desired.setdefault(filename, (set(), set())) share_hashes.add(share.hash)",if share.hash in share_hashes:
"def get_resolved_modules(self) -> Dict[str, ResolvedModule]: """"""Get a {name: ResolvedModule} map of all resolved modules."""""" resolved_modules = {} for name, mod in self._modules.items(): <IF_STMT> resolved_modules[name] = ResolvedModule(mod.module_name, mod.filename, mod.ast) return resolved_modules",if not mod.has_unresolved_pointers:
"def stripe(request): amount = 1 response = None if request.method == 'POST': form = CreditCardForm(request.POST) <IF_STMT> data = form.cleaned_data credit_card = CreditCard(**data) merchant = get_gateway('stripe') response = merchant.purchase(amount, credit_card) else: form = CreditCardForm(initial=GATEWAY_INITIAL['stripe']) return render(request, 'app/index.html', {'form': form, 'amount': amount, 'response': response, 'title': 'Stripe Payment'})",if form.is_valid():
"def get(self, url): now = time.time() for entry in self.repos: <IF_STMT> if now < entry.timestamp + self.timeout: return (entry.url, entry.rev) return (entry.url, -1) return (url, -1)",if url.startswith(entry.url):
"def cleanup(self): for i, s in enumerate(self.synonyms): <IF_STMT> if re.search('\\b[a-z]{2,}\\.$', s): c = s[:-1] (print >> sys.stderr, ""Note: cleanup: '%s' -> '%s'"" % (s, c)) self.synonyms[i] = c",if s[-1] == '.':
"def __get_field(cls, name): try: return cls._doc_type.mapping[name] except KeyError: <IF_STMT> try: return cls._index._mapping[name] except KeyError: pass","if hasattr(cls, '_index') and cls._index._mapping:"
"def command_is_enabled(self, item, focus): cmd = item.command if cmd: enabler_name = cmd + '_enabled' handler = focus while handler: enabler = getattr(handler, enabler_name, None) <IF_STMT> return enabler() handler = handler.next_handler() return True",if enabler:
"def __getitem__(self, key): value = WeakValueDictionary.__getitem__(self, key) while len(self.queue) > 0 and self.queue[0][0] == key: self.queue.popleft() if not (len(self.queue) and self.queue[-1][0] == key): <IF_STMT> self.cull() self.queue.append((key, value)) return value",if len(self) >= self.maxsize or len(self.queue) >= self.maxsize * self.peakmult:
def post_init(self): if os.getenv('SCRCPY_LDD'): <IF_STMT> os.environ['LD_LIBRARY_PATH'] += os.getenv('SCRCPY_LDD') else: os.environ['LD_LIBRARY_PATH'] = os.getenv('SCRCPY_LDD'),if os.getenv('LD_LIBRARY_PATH'):
"def get_summary_output(event: events.Finished) -> Tuple[str, str, int]: parts = get_summary_message_parts(event.results) if not parts: message = 'Empty test suite' color = 'yellow' status_code = 0 else: message = f""{', '.join(parts)} in {event.running_time:.2f}s"" <IF_STMT> color = 'red' status_code = 1 else: color = 'green' status_code = 0 return (message, color, status_code)",if event.results.has_failures or event.results.has_errors:
"def header_check(p_obj): """"""Special disposition for the HTML <head> and <body> elements..."""""" if state.options.host_language in [HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5]: <IF_STMT> if not has_one_of_attributes(node, 'about', 'resource', 'src', 'href'): return p_obj else: return None",if node.nodeName == 'head' or node.nodeName == 'body':
"def get_track_id_from_json(item): """"""Try to extract video Id from various response types"""""" fields = ['contentDetails/videoId', 'snippet/resourceId/videoId', 'id/videoId', 'id'] for field in fields: node = item for p in field.split('/'): if node and isinstance(node, dict): node = node.get(p) <IF_STMT> return node return ''",if node:
"def __init__(self, layers): super(Add, self).__init__() self.layer_names = [] self.layers = layers for i, layer in enumerate(self.layers): <IF_STMT> if i == 0: layer.parent = 'input' else: layer.parent = layers[i - 1].name if hasattr(layer, 'name'): name = layer.name else: name = layer.__class__.__name__ + str(i) layer.name = name self.layer_names.append(name)",if layer.parent is None:
def do_remove(self): if self.netconf.locked('dhcp'): if not self.pid: pid = read_pid_file('/var/run/dnsmasq.pan1.pid') else: pid = self.pid <IF_STMT> logging.info('Stale dhcp lockfile found') self.netconf.unlock('dhcp'),"if not kill(pid, 'dnsmasq'):"
"def findStyleName(element, style): oldStyle = DOM.getAttribute(element, 'className') if oldStyle is None: return -1 idx = oldStyle.find(style) lastPos = len(oldStyle) while idx != -1: <IF_STMT> last = idx + len(style) if last == lastPos or (last < lastPos and oldStyle[last] == ' '): break idx = oldStyle.find(style, idx + 1) return idx",if idx == 0 or oldStyle[idx - 1] == ' ':
"def __str__(self): path = super(XPathExpr, self).__str__() if self.textnode: if path == '*': path = 'text()' el<IF_STMT> path = path[:-3] + 'text()' else: path += '/text()' if self.attribute is not None: if path.endswith('::*/*'): path = path[:-2] path += '/@%s' % self.attribute return path",if path.endswith('::*/*'):
"def insert_after(self, sibling, row=None): if row is not None: value = self._get_marshalable(row[0]) <IF_STMT> position = 0 else: position = self.get_path(sibling)[0] + 1 return self.insert_with_valuesv(position, [0], [value]) assert not self.ATOMIC return super(ObjectStore, self).insert_after(sibling, row)",if sibling is None:
"def source_synopsis(file): line = file.readline() while line[:1] == '#' or not strip(line): line = file.readline() if not line: break line = strip(line) if line[:4] == 'r""""""': line = line[1:] if line[:3] == '""""""': line = line[3:] <IF_STMT> line = line[:-1] while not strip(line): line = file.readline() if not line: break result = strip(split(line, '""""""')[0]) else: result = None return result",if line[-1:] == '\\':
"def _handle_rate_limit(self, exception: RedditAPIException) -> Optional[Union[int, float]]: for item in exception.items: if item.error_type == 'RATELIMIT': amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) <IF_STMT> seconds *= 60 if seconds <= int(self.config.ratelimit_seconds): sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None",if 'minute' in amount_search.group(2):
"def get_html_help_exe(): """"""Return HTML Help Workshop executable path (Windows only)"""""" if os.name == 'nt': hhc_base = 'C:\\Program Files%s\\HTML Help Workshop\\hhc.exe' for hhc_exe in (hhc_base % '', hhc_base % ' (x86)'): <IF_STMT> return hhc_exe else: return",if osp.isfile(hhc_exe):
"def get_net_bridge_owner(name_ignore, sysfspath): brportpath = os.path.join(sysfspath, 'brport') try: <IF_STMT> brlinkpath = os.path.join(brportpath, 'bridge') dest = os.readlink(brlinkpath) ignore, bridge = os.path.split(dest) return bridge except: logging.exception('Unable to determine if device is shared') return None",if os.path.exists(brportpath):
"def get_timestamp(self): if not self._timedelta: url = 'https://%s%s/auth/time' % (API_HOST, API_ROOT) response = get_response_object(url=url, method='GET', headers={}) <IF_STMT> raise Exception('Failed to get current time from Ovh API') timestamp = int(response.body) self._timedelta = timestamp - int(time.time()) return int(time.time()) + self._timedelta",if not response or not response.body:
"def render(self, context): for var in self.vars: value = var.resolve(context, True) <IF_STMT> first = render_value_in_context(value, context) if self.asvar: context[self.asvar] = first return '' return first return ''",if value:
"def test_loc_is_stochastic_parameter(self): param = iap.Laplace(iap.Choice([-100, 100]), 1) seen = [0, 0] for _ in sm.xrange(1000): samples = param.draw_samples((100,)) exp = np.mean(samples) if -100 - 10 < exp < -100 + 10: seen[0] += 1 <IF_STMT> seen[1] += 1 else: assert False assert 500 - 100 < seen[0] < 500 + 100 assert 500 - 100 < seen[1] < 500 + 100",elif 100 - 10 < exp < 100 + 10:
"def get_data(self, path, prefix=''): item = self.store[path] path = '{}/{}'.format(prefix, path) keys = [i for i in item.keys()] data = {'path': path} for k in keys: if not isinstance(item[k], h5py.Group): dataset = np.array(item[k].value) if type(dataset) is np.ndarray: <IF_STMT> if type(dataset[0]) is np.bytes_: dataset = [a.decode('ascii') for a in dataset] data.update({k: dataset}) return data",if dataset.size != 0:
def __del__(self): try: if self._mpz_p is not None: <IF_STMT> _gmp.mpz_clear(self._mpz_p) self._mpz_p = None except AttributeError: pass,if self._initialized:
"def load(self, vocab_file): self.__term2id = {} self.__id2term = {} with open(vocab_file, 'r', encoding='utf-8') as fin: for line in fin.readlines(): fields = line.strip().split('\t') assert len(fields) == 5, 'Vocabulary file [%s] format error!' % vocab_file term = fields[1] id_ = int(fields[2]) <IF_STMT> logger.error('Duplicate word [%s] in vocab file!' % term) continue self.__term2id[term] = id_ self.__id2term[id_] = term",if term in self.__term2id:
"def break_next_call(symbol_regex=None): while pwndbg.proc.alive: ins = break_next_branch() if not ins: break if capstone.CS_GRP_CALL not in ins.groups: continue <IF_STMT> return ins if ins.target_const and re.match('%s$' % symbol_regex, hex(ins.target)): return ins if ins.symbol and re.match('%s$' % symbol_regex, ins.symbol): return ins",if not symbol_regex:
"def test_url_valid_set(): for line in URL_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) <IF_STMT> continue mbox = address.parse(line, strict=True) assert_not_equal(mbox, None)",if match:
"def _clean_fields(self, fields, reverse=False): if not fields: fields = list(self.default_fields) if reverse: for field in ['up.total', 'down.total', 'down.rate']: <IF_STMT> fields[fields.index(field)] = field.replace('.', '_') return fields for required_field in self.required_fields: if required_field not in fields: fields.insert(0, required_field) for field in ['up_total', 'down_total', 'down_rate']: if field in fields: fields[fields.index(field)] = field.replace('_', '.') return fields",if field in fields:
"def client_cert_key_path(self): cache_folder = os.path.dirname(self.filename) try: path = self.get_item('general.client_cert_key_path') except ConanException: path = os.path.join(cache_folder, 'client.key') else: path = os.path.join(cache_folder, path) <IF_STMT> raise ConanException(""Configured file for 'client_cert_key_path' doesn't exists: '{}'"".format(path)) return os.path.normpath(path)",if not os.path.exists(path):
"def handler_click_link(self, link): if link.startswith('[['): link = link[2:-2] self.notify_observers('click:notelink', link) elif platform.system().lower() == 'windows': os.startfile(link) <IF_STMT> subprocess.call(('open', link)) else: subprocess.call(('xdg-open', link))",elif platform.system().lower() == 'darwin':
"def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False if isinstance(value, dict): value = PseudoNamespace(value) elif isinstance(value, tuple): value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and (not isinstance(item, PseudoNamespace)): value[i] = PseudoNamespace(item) <IF_STMT> value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)",if tuple_converted:
"def slots_for_entities(self, entities): if self.store_entities_as_slots: slot_events = [] for s in self.slots: if s.auto_fill: matching_entities = [e['value'] for e in entities if e['entity'] == s.name] if matching_entities: <IF_STMT> slot_events.append(SlotSet(s.name, matching_entities)) else: slot_events.append(SlotSet(s.name, matching_entities[-1])) return slot_events else: return []",if s.type_name == 'list':
"def stream_read_bz2(ifh, ofh): """"""Uncompress bz2 compressed *ifh* into *ofh*"""""" decompressor = bz2.BZ2Decompressor() while True: buf = ifh.read(BUFSIZE) <IF_STMT> break buf = decompressor.decompress(buf) if buf: ofh.write(buf) if decompressor.unused_data or ifh.read(1) != b'': raise CorruptedObjectError('Data after end of bz2 stream')",if not buf:
"def get_for_vars(self): tok = self.tokenizer.get_next_token() if tok['style'] == ScintillaConstants.SCE_PL_WORD and tok['text'] in ('my', 'state'): tlineNo = tok['start_line'] tok = self.tokenizer.get_next_token() <IF_STMT> self.moduleInfo.doSetVar(name=tok['text'], line=tlineNo, scope='my')",if self.classifier.is_variable(tok):
"def generate_dem_tiles(geotiff, output_dir, max_concurrency): try: colored_dem, hillshade_dem, colored_hillshade_dem = generate_colored_hillshade(geotiff) generate_tiles(colored_hillshade_dem, output_dir, max_concurrency) for f in [colored_dem, hillshade_dem, colored_hillshade_dem]: <IF_STMT> os.remove(f) except Exception as e: log.ODM_WARNING('Cannot generate DEM tiles: %s' % str(e))",if os.path.isfile(f):
"def cluster(spawnpoints, radius, time_threshold): clusters = [] diameter = 2 * radius for p in spawnpoints: <IF_STMT> clusters.append(Spawncluster(p)) else: c = min(clusters, key=lambda x: cost(p, x, time_threshold)) if check_cluster(p, c, radius, time_threshold): c.append(p) else: c = Spawncluster(p) clusters.append(c) return clusters",if len(clusters) == 0:
def pop(self): if self._pending_removals: self._commit_removals() while True: try: itemref = self.data.pop() except KeyError: raise KeyError('pop from empty WeakSet') from None item = itemref() <IF_STMT> return item,if item is not None:
"def map_depends(self, dep): if dep.endswith(('-native', '-native-runtime')) or 'nativesdk-' in dep or 'cross-canadian' in dep or ('-crosssdk-' in dep): return dep else: var = self.d.getVar('MULTILIB_VARIANTS') if var: var = var.split() for v in var: <IF_STMT> return dep return self.extend_name(dep)",if dep.startswith(v):
"def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: if system.NUMBER_KEY in letters: stroke = stroke.replace(system.NUMBER_KEY, '') m = _IMPLICIT_NUMBER_RX.search(stroke) if m is not None: start = m.start(2) return stroke[:start] + '-' + stroke[start:] if '-' in letters: <IF_STMT> stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace('-', '') return stroke",if stroke.endswith('-'):
"def _get_py_flags(self): res = dict(self.flags) cflags = res.pop('cflags', '') for fl in cflags.split('|'): fl = fl.strip() if fl == 'GA_USE_DOUBLE': res['have_double'] = True if fl == 'GA_USE_SMALL': res['have_small'] = True <IF_STMT> res['have_complex'] = True if fl == 'GA_USE_HALF': res['have_half'] = True return res",if fl == 'GA_USE_COMPLEX':
"def populate(self): classes = self.applet.Plugins.get_classes() loaded = self.applet.Plugins.get_loaded() for name, cls in classes.items(): <IF_STMT> desc = '<span weight=""bold"">%s</span>' % name else: desc = name self.list.append(active=name in loaded, icon=cls.__icon__, activatable=cls.__unloadable__, name=name, desc=desc)",if cls.is_configurable():
"def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: if isinstance(decorator, NameExpr): if self.process_name_expr_decorator(decorator, o): is_abstract = True <IF_STMT> if self.process_member_expr_decorator(decorator, o): is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)","elif isinstance(decorator, MemberExpr):"
"def hint(self, button): """"""As hilight, but marks GTK Button as well"""""" active = None for b in self.button_widgets.values(): <IF_STMT> b.widget.set_state(Gtk.StateType.NORMAL) if b.name == button: active = b.widget if active is not None: active.set_state(Gtk.StateType.ACTIVE) self.hilight(button)",if b.widget.get_sensitive():
"def read_message_py2(self): chunks = [] while True: hi, lo = self.wire.read(2) <IF_STMT> break size = hi << 8 | lo chunks.append(self.wire.read(size)) message = bytearray(b''.join(map(bytes, chunks))) _, n = divmod(message[0], 16) unpacker = UnpackStream(message, offset=2) fields = [unpacker.unpack() for _ in range(n)] return (message[1], fields)",if hi == lo == 0:
"def offsetToRva(self, offset): if self.inmem: return offset for s in self.sections: sbase = s.PointerToRawData if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva(): ssize = s.VirtualSize else: ssize = max(s.SizeOfRawData, s.VirtualSize) <IF_STMT> return offset - s.PointerToRawData + s.VirtualAddress return 0",if sbase <= offset and offset < sbase + ssize:
"def highlight_from_dir(self, workspace_dir): while True: for f in os.listdir(workspace_dir): <IF_STMT> self.process_trace(os.path.join(workspace_dir, f)) if not self.live_update: break time.sleep(interval)",if f.endswith('trace'):
"def check_tokenize(self, s, expected): result = [] f = StringIO(s) for type, token, start, end, line in generate_tokens(f.readline): <IF_STMT> break type = tok_name[type] result.append('%(type)-10.10s %(token)-13.13r %(start)s %(end)s' % locals()) self.assertEqual(result, expected.rstrip().splitlines())",if type == ENDMARKER:
"def enable(self): """"""enable the patch."""""" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return if not self.PY2: raise IncompatiblePatch('Python 2 not supported!') <IF_STMT> if self.PY3 == SKIP: return if not self.PY3: raise IncompatiblePatch('Python 3 not supported!') self.pre_enable() self.do_enable() self.enabled = True",if pyv == 3:
"def __xor__(self, other): inc, exc = _norm_args_notimplemented(other) if inc is NotImplemented: return NotImplemented if inc is NotImplemented: return NotImplemented if self._included is None: if exc is None: return _ComplementSet(excluded=self._excluded - inc) else: return _ComplementSet(included=self._excluded.symmetric_difference(exc)) el<IF_STMT> return _ComplementSet(excluded=exc - self._included) else: return _ComplementSet(included=self._included.symmetric_difference(inc))",if inc is None:
"def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) <IF_STMT> if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning('Configuration file {} does not exist.'.format(value)) elif isinstance(value, type(None)): pass else: raise ValueError('Cannot interpret {}'.format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)","elif isinstance(value, str):"
"def maybe_add_0000_to_all_niigz(folder): nii_gz = subfiles(folder, suffix='.nii.gz') for n in nii_gz: n = remove_trailing_slash(n) <IF_STMT> os.rename(n, n[:-7] + '_0000.nii.gz')",if not n.endswith('_0000.nii.gz'):
"def newstart(self): newstartdatetime = self._newstartdate if not self.checkallday.state: <IF_STMT> tzinfo = self.conf.default.default_timezone else: tzinfo = self.startdt.tzinfo try: newstarttime = self._newstarttime newstartdatetime = datetime.combine(newstartdatetime, newstarttime) newstartdatetime = tzinfo.localize(newstartdatetime) except TypeError: return None return newstartdatetime","if not hasattr(self.startdt, 'tzinfo') or self.startdt.tzinfo is None:"
"def _fetch_all_channels(self, force=False): """"""Fetch all channel feeds from cache or network."""""" channels = self._get_channel_configs(force=force) enabled = self._settings.get(['enabled_channels']) forced = self._settings.get(['forced_channels']) all_channels = {} for key, config in channels.items(): if key not in enabled and key not in forced: continue if 'url' not in config: continue data = self._get_channel_data(key, config, force=force) <IF_STMT> all_channels[key] = data return all_channels",if data is not None:
"def _setup_graph(self): vars = tf.trainable_variables() ops = [] for v in vars: n = v.op.name <IF_STMT> continue logger.info('Clip {}'.format(n)) ops.append(tf.assign(v, tf.clip_by_value(v, -0.01, 0.01))) self._op = tf.group(*ops, name='clip')",if not n.startswith('discrim/'):
"def on_window_state_event(self, widget, event): if event.changed_mask & WindowState.ICONIFIED: <IF_STMT> log.debug('MainWindow is minimized..') component.get('TorrentView').save_state() component.pause(self.child_components) self.is_minimized = True else: log.debug('MainWindow is not minimized..') component.resume(self.child_components) self.is_minimized = False return False",if event.new_window_state & WindowState.ICONIFIED:
"def getJsonData(self, url, decode_from=None, **kwargs): cache_key = md5(url) data = self.getCache(cache_key, url, **kwargs) if data: try: data = data.strip() <IF_STMT> data = data.decode(decode_from) return json.loads(data) except: log.error('Failed to parsing %s: %s', (self.getName(), traceback.format_exc())) return []",if decode_from:
"def init_weights(self): for n, p in self.named_parameters(): if 'bias' in n: torch.nn.init.zeros_(p) <IF_STMT> torch.nn.init.xavier_uniform_(p)",elif 'fc' in n:
"def get_file_language(filename, text=None): """"""Get file language from filename"""""" ext = osp.splitext(filename)[1] if ext.startswith('.'): ext = ext[1:] language = ext if not ext: <IF_STMT> text, _enc = encoding.read(filename) for line in text.splitlines(): if not line.strip(): continue if line.startswith('#!'): shebang = line[2:] if 'python' in shebang: language = 'python' else: break return language",if text is None:
"def readwrite(obj, flags): try: <IF_STMT> obj.handle_read_event() if flags & select.POLLOUT: obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: if e.args[0] not in _DISCONNECTED: obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()",if flags & select.POLLIN:
"def sortPlaces(self, newColumn, newOrder, force=False): profile_id = self.config.currentProfile() if newColumn == 0 and newOrder == Qt.AscendingOrder: <IF_STMT> newColumn, newOrder = (1, Qt.AscendingOrder) self.places.header().setSortIndicator(newColumn, newOrder) self.placesSortLoop[profile_id] = False else: self.placesSortLoop[profile_id] = True self.updatePlaces()",if profile_id in self.placesSortLoop and self.placesSortLoop[profile_id]:
def _result_iter(self): pos = 0 while 1: upper = len(self._result_cache) while pos < upper: yield self._result_cache[pos] pos = pos + 1 if not self._iter: raise StopIteration <IF_STMT> self._fill_cache(),if len(self._result_cache) <= pos:
"def get_field_type(self, name): fkey = (name, self.dummy) target = None op, name = name.split('_', 1) if op in {'delete', 'insert', 'update'}: target = super().get_field_type(name) <IF_STMT> module, edb_name = self.get_module_and_name(name) target = self.edb_schema.get((module, edb_name), None) if target is not None: target = self.convert_edb_to_gql_type(target) self._fields[fkey] = target return target",if target is None:
"def _transaction(self, args=None): cmd = args[0] if args else None if cmd == 'reset': self._clean() return self._resolve() if cmd in ['list', None]: <IF_STMT> out = self.base.output.list_transaction(self.base._transaction) logger.info(out) elif cmd == 'run': try: self.base.do_transaction() except dnf.exceptions.Error as e: logger.error(_('Error:') + ' ' + ucd(e)) else: logger.info(_('Complete!')) self._clean() else: self._help('transaction')",if self.base._transaction:
"def _gather_crash_info(self): super()._gather_crash_info() self._crash_info += [('Commandline args', ' '.join(sys.argv[1:])), ('Open Pages', '\n\n'.join(('\n'.join(e) for e in self._pages))), ('Command history', '\n'.join(self._cmdhist)), ('Objects', self._qobjects)] try: text = 'Log output was disabled.' <IF_STMT> text = log.ram_handler.dump_log() self._crash_info.append(('Debug log', text)) except Exception: self._crash_info.append(('Debug log', traceback.format_exc()))",if log.ram_handler is not None:
"def classifyws(s, tabwidth): raw = effective = 0 for ch in s: if ch == ' ': raw = raw + 1 effective = effective + 1 <IF_STMT> raw = raw + 1 effective = (effective // tabwidth + 1) * tabwidth else: break return (raw, effective)",elif ch == '\t':
"def process(self, node): self.vars = [] for child in node.childNodes: <IF_STMT> child_text = get_xml_text(child) if child_text == '': continue if child.nodeName == 'Real': for val in re.split('[\t ]+', child_text): self.vars.append(1.0 * eval(val)) return self",if child.nodeType == node.ELEMENT_NODE:
"def _format_privilege_data(self, data): for key in ['spcacl']: <IF_STMT> if 'added' in data[key]: data[key]['added'] = parse_priv_to_db(data[key]['added'], self.acl) if 'changed' in data[key]: data[key]['changed'] = parse_priv_to_db(data[key]['changed'], self.acl) if 'deleted' in data[key]: data[key]['deleted'] = parse_priv_to_db(data[key]['deleted'], self.acl)",if key in data and data[key] is not None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_application_key(d.getPrefixedString()) continue if tt == 18: self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = (end, end + size) if cat_dim == -1: part = gaussian[..., beg:end] <IF_STMT> part = gaussian[..., beg:end, :] elif cat_dim == 1: part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)",elif cat_dim == -2:
"def __conform__(self, interface, registry=None, default=None): for providedInterface in self.provided: <IF_STMT> return self.load() if getAdapterFactory(providedInterface, interface, None) is not None: return interface(self.load(), default) return default",if providedInterface.isOrExtends(interface):
"def __init__(self, oid): self.oid = oid self.cmpt = [] fmt = [] for i in oid.split('.'): <IF_STMT> fmt.append('%i') self.cmpt.append(tuple(map(int, i.split('-')))) else: fmt.append(i) self.fmt = '.'.join(fmt)",if '-' in i:
"def build_CallFunc(self, o): children = o.getChildren() callee = self.build(children[0]) args = [] kwargs = {} for child in children[1:]: class_name = child.__class__.__name__ <IF_STMT> continue if class_name == 'Keyword': kwargs.update(self.build(child)) else: args.append(self.build(child)) return callee(*args, **kwargs)",if class_name == 'NoneType':
"def format_raises(self, e, *args, **kw): self.startTest() try: args[0].format(*args[1:], **kw) except e: return True else: <IF_STMT> excName = e.__name__ else: excName = str(e) self.fail('%s not raised' % excName) return False","if hasattr(e, '__name__'):"
"def make_record_paths_absolute(self, record_dict): d = {} for k, v in record_dict.items(): <IF_STMT> if '.' in v: v = os.path.join(self.path, v) d[k] = v return d",if type(v) == str:
"def work(self): while self.active: stat = os.stat(self.filename) <IF_STMT> self.callback(self.last_stat, stat) self.last_stat = stat time.sleep(self.interval)",if self.last_stat is not None and self.last_stat != stat:
"def try_append_extension(self, path): append_setting = self.get_append_extension_setting() if self.settings.get(append_setting, False): if not self.is_copy_original_name(path): _, new_path_extension = os.path.splitext(path) if new_path_extension == '': argument_name = self.get_argument_name() <IF_STMT> _, extension = os.path.splitext(self.view.file_name()) else: _, extension = os.path.splitext(argument_name) path += extension return path",if argument_name is None:
"def _out_of_date(rw_file): """"""Check if a run workflow file points to an older version of manta and needs a refresh."""""" with open(rw_file) as in_handle: for line in in_handle: <IF_STMT> file_version = line.split('/lib/python')[0].split('Cellar/manta/')[-1] if file_version != programs.get_version_manifest('manta'): return True return False",if line.startswith('sys.path.append'):
"def test_model_inference(): x = torch.rand(1, 3, 224, 224) for model_name in encoding.models.pretrained_model_list(): print('Doing: ', model_name) <IF_STMT> continue model = encoding.models.get_model(model_name, pretrained=True) model.eval() y = model(x)",if 'wideresnet' in model_name:
"def _process_frame(self, frame_num, frame_im, callback=None): """"""Adds any cuts detected with the current frame to the cutting list."""""" for detector in self._detector_list: cuts = detector.process_frame(frame_num, frame_im) <IF_STMT> callback(frame_im, frame_num) self._cutting_list += cuts for detector in self._sparse_detector_list: events = detector.process_frame(frame_num, frame_im) if events and callback: callback(frame_im, frame_num) self._event_list += events",if cuts and callback:
"def __saveWork(self, work, results): """"""Stores the resulting last log line to the cache with the proxy key"""""" del work try: <IF_STMT> __cached = self.__cache[results[0]] __cached[self.__TIME] = time.time() __cached[self.__ETA] = results[1] except KeyError as e: pass except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if results:
"def _on_preference_changed(self, client, timestamp, entry, extra): attr = entry.key[entry.key.rindex('/') + 1:] try: valuestruct = self._prefs[attr] except KeyError: pass else: <IF_STMT> newval = getattr(entry.value, 'get_%s' % valuestruct.type)() setattr(self, attr, newval) else: setattr(self, attr, valuestruct.default)",if entry.value != None:
"def open(self, url, new=0, autoraise=1): cmdline = [self.name] + [arg.replace('%s', url) for arg in self.args] try: <IF_STMT> p = subprocess.Popen(cmdline) else: setsid = getattr(os, 'setsid', None) if not setsid: setsid = getattr(os, 'setpgrp', None) p = subprocess.Popen(cmdline, close_fds=True, preexec_fn=setsid) return p.poll() is None except OSError: return False",if sys.platform[:3] == 'win':
"def get_ofs(self, dp_id): if len(self) == 0: raise ValueError('qos sw is not connected.') dps = {} if dp_id == REST_ALL: dps = self else: try: dpid = dpid_lib.str_to_dpid(dp_id) except: raise ValueError('Invalid switchID.') <IF_STMT> dps = {dpid: self[dpid]} else: msg = 'qos sw is not connected. : switchID=%s' % dp_id raise ValueError(msg) return dps",if dpid in self:
"def __init__(self, context, keymap={}): if not ActionHandler._actions: ActionHandler._actions = Actions.get_instance(context) _keymap = {} for k, v in keymap.items(): <IF_STMT> v = {v} _keymap[k] = {op for action in v for op in translate_blenderop(action)} self.__dict__['_keymap'] = _keymap",if type(v) is not set and type(v) is not list:
"def setCounter(self, i): if 0 == i: <IF_STMT> self.setIcon(QtGui.QIcon.fromTheme('scudcloud-attention')) else: self.setIcon(QtGui.QIcon.fromTheme('scudcloud')) elif i > 0 and i < 10: self.setIcon(QtGui.QIcon.fromTheme('scudcloud-attention-' + str(int(i)))) elif i > 9: self.setIcon(QtGui.QIcon.fromTheme('scudcloud-attention-9-plus'))",if True == self.urgent:
"def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() if event is h11.NEED_DATA: break elif isinstance(event, h11.InformationalResponse): continue <IF_STMT> context['h11_response'] = event raise LoopAbort else: raise RuntimeError('Unexpected h11 event {}'.format(event))","elif isinstance(event, h11.Response):"
"def _evoke_request(cls): succeed = False with cls.LOCK: <IF_STMT> resource, request_semaphore = cls.REQUESTING_STACK.pop() node = cls.check_availability(resource) if node is not None: cls.NODE_RESOURCE_MANAGER[node]._request(node, resource) logger.debug('\nEvoking requesting resource {}'.format(resource)) request_semaphore.release() succeed = True else: cls.REQUESTING_STACK.append((resource, request_semaphore)) return if succeed: cls._evoke_request()",if len(cls.REQUESTING_STACK) > 0:
"def _get_related_field(self, field): model_class = self.Meta.model try: related_field = model_class._meta.get_field(field.source) except FieldDoesNotExist: default_postfix = '_set' <IF_STMT> related_field = model_class._meta.get_field(field.source[:-len(default_postfix)]) else: raise if isinstance(related_field, ForeignObjectRel): return (related_field.field, False) return (related_field, True)",if field.source.endswith(default_postfix):
"def find_best_layout_for_subplots(num_subplots): r, c = (1, 1) while r * c < num_subplots: <IF_STMT> c += 1 elif c == r + 2: r += 1 c -= 1 return (r, c)",if c == r + 1 or r == c:
"def __repr__(self): attrs = {} for name, _ in self: try: attr = getattr(self, name) <IF_STMT> attrs[name] = repr(attr) except ValidationError: pass return '{class_name}({fields})'.format(class_name=self.__class__.__name__, fields=', '.join(('{0[0]}={0[1]}'.format(x) for x in sorted(attrs.items()))))",if attr is not None:
"def findsection(self, key): to_return = copy.deepcopy(self) for subsection in to_return: try: value = list(ConfigObj.find_key(to_return[subsection], key))[0] except Exception: value = None <IF_STMT> del to_return[subsection] else: for category in to_return[subsection]: if category != key: del to_return[subsection][category] for key in [k for k, v in to_return.items() if not v]: del to_return[key] return to_return",if not value:
"def _get_streams(self, url, video_id, app_id_ver): for trial_count in range(3): stream_info = self._get_stream_info(url, video_id, app_id_ver, extra_note=' (try %d)' % (trial_count + 1) if trial_count > 0 else '') <IF_STMT> return stream_info[0]['args'][0]['stream'] return []",if 'stream' in stream_info[0]['args'][0]:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.set_path(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def summary(self): """"""Return a string with a pretty-printed summary for the company."""""" if not self: return u'' s = u'Company\n=======\nName: %s\n' % self.get('name', u'') for k in ('distributor', 'production company', 'miscellaneous company', 'special effects company'): d = self.get(k, [])[:5] <IF_STMT> continue s += u'Last movies from this company (%s): %s.\n' % (k, u'; '.join([x.get('long imdb title', u'') for x in d])) return s",if not d:
"def __call__(self, data): keys = set(data.keys) for attr_name in self._attr_names: <IF_STMT> raise Exception('attr_name: {} isn t within keys: {}'.format(attr_name, keys)) for attr_name in self._attr_names: delattr(data, attr_name) return data",if attr_name not in keys and self._strict:
"def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: if isinstance(child, six.string_types): if isinstance(element, six.string_types): <IF_STMT> i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i",if count:
"def produce_etag_headers(self, filename): """"""Produce a dict of curl headers containing etag headers from the download."""""" headers = {} if os.path.exists(filename): self.existing_file_size = os.path.getsize(filename) etag = self.getxattr(self.xattr_etag) last_modified = self.getxattr(self.xattr_last_modified) if etag: headers['If-None-Match'] = etag <IF_STMT> headers['If-Modified-Since'] = last_modified return headers",if last_modified:
"def repack(self): newNsp = Pfs0Stream(self._path[:-4] + '.nsp') for nspF in self.hfs0['secure']: f = newNsp.add(nspF._path, nspF.size) nspF.rewind() i = 0 pageSize = 65536 while True: buf = nspF.read(pageSize) <IF_STMT> break i += len(buf) f.write(buf) newNsp.close()",if len(buf) == 0:
"def assertHasChanged(self, **kwargs): tracker = kwargs.pop('tracker', self.tracker) for field, value in kwargs.items(): <IF_STMT> with self.assertRaises(FieldError): tracker.has_changed(field) else: self.assertEqual(tracker.has_changed(field), value)",if value is None:
def check_engine(engine): if engine == 'auto': if pa is not None: return 'pyarrow' elif fastparquet is not None: return 'fastparquet' else: raise RuntimeError('Please install either pyarrow or fastparquet.') elif engine == 'pyarrow': <IF_STMT> raise RuntimeError('Please install pyarrow fisrt.') return engine elif engine == 'fastparquet': if fastparquet is None: raise RuntimeError('Please install fastparquet first.') return engine else: raise RuntimeError('Unsupported engine {} to read parquet.'.format(engine)),if pa is None:
"def parse_vcs_bundle_file(self, content): for line in content.splitlines(): <IF_STMT> continue match = re.search('^-r\\s*([^ ])?', line) if not match: return (None, None) rev = match.group(1) rest = line[match.end():].strip().split(None, 1)[0] return (rest, rev) return (None, None)",if not line.strip() or line.strip().startswith('#'):
"def __init__(self, parent_instance, *args, **kwargs): self.parent_instance = parent_instance self.pk_field = kwargs.pop('pk_field', False) self.to_field = kwargs.pop('to_field', None) if self.parent_instance is not None: <IF_STMT> kwargs['initial'] = getattr(self.parent_instance, self.to_field) else: kwargs['initial'] = self.parent_instance.pk kwargs['required'] = False kwargs['widget'] = InlineForeignKeyHiddenInput super(InlineForeignKeyField, self).__init__(*args, **kwargs)",if self.to_field:
"def number_multiple_validator(v: 'Number', field: 'ModelField') -> 'Number': field_type: ConstrainedNumber = field.type_ if field_type.multiple_of is not None: mod = float(v) / float(field_type.multiple_of) % 1 <IF_STMT> raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of) return v","if not almost_equal_floats(mod, 0.0) and (not almost_equal_floats(mod, 1.0)):"
"def forward(self, x, edge_index, edge_attr=None): x_old = 0 for i, layer in enumerate(self.hidden_layers): x = self.dropout(x) x = layer(x, edge_index) x = self.norm(x) x = self.relu(x) <IF_STMT> x = x + x_old x_old = x x = self.dropout(x) x = self.out_layer(x, edge_index) return x",if self.skip > 0 and i % self.skip == 0:
"def check_dimensions(nrow, ncol): if nrow is not None: <IF_STMT> warn(""'nrow' must be greater than 0. Your value has been ignored."", PlotnineWarning) nrow = None else: nrow = int(nrow) if ncol is not None: if ncol < 1: warn(""'ncol' must be greater than 0. Your value has been ignored."", PlotnineWarning) ncol = None else: ncol = int(ncol) return (nrow, ncol)",if nrow < 1:
"def logic(): while 1: yield (clock.posedge, reset.negedge) if reset == ACTIVE_LOW: count.next = 0 elif enable: <IF_STMT> count.next = n - 1 else: count.next = count - 1",if count == -n:
"def get_whitelist(self, guild: Optional[discord.Guild]=None) -> Set[int]: async with self._access_lock: ret: Set[int] gid: Optional[int] = guild.id if guild else None <IF_STMT> ret = self._cached_whitelist[gid].copy() else: if gid is not None: ret = set(await self._config.guild_from_id(gid).whitelist()) else: ret = set(await self._config.whitelist()) self._cached_whitelist[gid] = ret.copy() return ret",if gid in self._cached_whitelist:
"def process_response(self, request, response): if getattr(self, 'has_session', False): <IF_STMT> user = '%s (id:%s)' % (request.user.username, request.user.pk) else: user = '(Anonymous)' self.logger.info('Session %s authenticated by %s', request.session.session_key, user) request.session.save = self._save self._save = None self.session = None self.has_session = False","if getattr(request, 'user', None) and request.user.is_authenticated():"
"def cluster(spawnpoints, radius, time_threshold): clusters = [] diameter = 2 * radius for p in spawnpoints: if len(clusters) == 0: clusters.append(Spawncluster(p)) else: c = min(clusters, key=lambda x: cost(p, x, time_threshold)) <IF_STMT> c.append(p) else: c = Spawncluster(p) clusters.append(c) return clusters","if check_cluster(p, c, radius, time_threshold):"
"def get_shape(shape): """"""Convert the shape to correct dtype and vars."""""" ret = [] for dim in shape: <IF_STMT> if libinfo()['INDEX_DEFAULT_I64'] == 'ON': ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm('int32', val)) elif isinstance(dim, tvm.tir.Any): ret.append(te.var('any_dim', 'int32')) else: ret.append(dim) return ret","if isinstance(dim, tvm.tir.IntImm):"
"def run(self): queue = self.queue while True: if not self.running: break callback, requests, fetchTimeout, validityOverride = queue.get() if len(requests) > 0: Price.fetchPrices(requests, fetchTimeout, validityOverride) wx.CallAfter(callback) queue.task_done() for price in requests: callbacks = self.wait.pop(price.typeID, None) <IF_STMT> for callback in callbacks: wx.CallAfter(callback)",if callbacks:
def _load_scopes_(self): if self._model_ is None: tablemap = self.db._adapter.tables(self.query) <IF_STMT> self._model_ = tablemap.popitem()[1]._model_ if self._model_: self._scopes_ = self._model_._instance_()._scopes_,if len(tablemap) == 1:
"def udp_to_tcp(udp_sock, tcp_conn): while True: msg, _ = udp_sock.recvfrom(2 ** 16) log_msg('read_udp', msg) <IF_STMT> return write_tcp(tcp_conn, msg)",if not msg:
"def __get_annotations(self): if not hasattr(self, '_annotations'): self._annotations = _retrieve_annotations(self._adaptor, self._primary_id, self._taxon_id) if self._identifier: self._annotations['gi'] = self._identifier <IF_STMT> self._annotations['data_file_division'] = self._division return self._annotations",if self._division:
def ignore_module(module): result = False for check in ignore_these: <IF_STMT> if check[:-1] in module: result = True elif os.getcwd() + '/' + check + '.py' == module: result = True if result: print_warning('Ignoring module: ' + module) return result,if '/*' in check:
"def find_commands(management_dir): command_dir = os.path.join(management_dir, 'commands') commands = [] try: for f in os.listdir(command_dir): <IF_STMT> continue elif f.endswith('.py') and f[:-3] not in commands: commands.append(f[:-3]) elif f.endswith('.pyc') and f[:-4] not in commands: commands.append(f[:-4]) except OSError: pass return commands",if f.startswith('_'):
"def _add_kid(key, x): if x is None: kids[key] = None el<IF_STMT> x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, '__iter__'): if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x","if type(x) in (type([]), type(())):"
"def classify(self, url, text): for match in self.rules.match(data=text): <IF_STMT> continue self.matches.append((url, match)) if self.discard_url_match(url, match): continue self.handle_match_etags(match) rule = match.rule meta = match.meta tags = ','.join([' '.join(t.split('_')) for t in match.tags]) log.ThugLogging.log_classifier('text', url, rule, tags, meta) for c in self.custom_classifiers: self.custom_classifiers[c](url, text)","if (url, match) in self.matches:"
def recurse(node): for child in node.childNodes: <IF_STMT> continue if child.nodeName.upper() == 'H1': return child if child not in visited: return recurse(child),if child.nodeType != child.ELEMENT_NODE:
"def try_fix_ip_range(self): for i in range(len(self.fake_ip_parts)): <IF_STMT> if i - 1 < 0: raise Exception(""Fake IP's out of range."") self.fake_ip_parts[i - 1] += 1 self.fake_ip_parts[i] = 1",if self.fake_ip_parts[i] > 256:
def run(self): self.thread.start() while self.thread.isRunning(): <IF_STMT> self.update.emit(config.imager_percentage) if not self.thread.isFinished() and config.percentage == 100: config.imager_status_text = '' self.status.emit('Please wait...') time.sleep(0.1) self.update.emit(100) self.update.emit(0) if self.thread.isFinished(): config.status_text = '' self.finished.emit() return,if config.imager_percentage:
"def _get_trading_minutes(self, trading_date): trading_minutes = set() for account_type in self._config.base.accounts: <IF_STMT> trading_minutes = trading_minutes.union(self._get_stock_trading_minutes(trading_date)) elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE: trading_minutes = trading_minutes.union(self._get_future_trading_minutes(trading_date)) return sorted(list(trading_minutes))",if account_type == DEFAULT_ACCOUNT_TYPE.STOCK:
"def lngettext(self, msgid1, msgid2, n): import warnings warnings.warn('lngettext() is deprecated, use ngettext() instead', DeprecationWarning, 2) try: tmsg = self._catalog[msgid1, self.plural(n)] except KeyError: if self._fallback: return self._fallback.lngettext(msgid1, msgid2, n) <IF_STMT> tmsg = msgid1 else: tmsg = msgid2 if self._output_charset: return tmsg.encode(self._output_charset) return tmsg.encode(locale.getpreferredencoding())",if n == 1:
"def check_langs(langs, pairs): messages = [] for src, tgt in pairs: <IF_STMT> messages.append(f'language pair {src}-{tgt} contains languages that are not in the language dictionary') if len(messages) > 0: raise ValueError(' '.join(messages) + f'; langs: {langs}')",if src not in langs or tgt not in langs:
"def to_header(self): """"""Converts the object back into an HTTP header."""""" ranges = [] for begin, end in self.ranges: <IF_STMT> ranges.append(f'{begin}-' if begin >= 0 else str(begin)) else: ranges.append(f'{begin}-{end - 1}') return f""{self.units}={','.join(ranges)}""",if end is None:
"def name(ent, langpref='en'): try: org = ent['organization'] except KeyError: return None for info in ['organization_display_name', 'organization_name', 'organization_url']: try: for item in org[info]: <IF_STMT> return item['text'] except KeyError: pass return None",if item['lang'] == langpref:
"def check_url(value): validate(text, value) parsed = urlparse(value) if not parsed.netloc: raise ValueError(""'{0}' is not a valid URL"".format(value)) for name, schema in attributes.items(): <IF_STMT> raise ValueError(""Invalid URL attribute '{0}'"".format(name)) try: validate(schema, _getattr(parsed, name)) except ValueError as err: raise ValueError(""Unable to validate URL attribute '{0}': {1}"".format(name, err)) return True","if not _hasattr(parsed, name):"
"def stepStarted(self, step): self.currentStep = step for w in self.watchers: receiver = w.stepStarted(self, step) <IF_STMT> if isinstance(receiver, type(())): step.subscribe(receiver[0], receiver[1]) else: step.subscribe(receiver) d = step.waitUntilFinished() d.addCallback(lambda step: step.unsubscribe(receiver)) step.waitUntilFinished().addCallback(self._stepFinished)",if receiver:
"def assert_not_none(obj, msg=None, values=True): """"""Fail the test if given object is None."""""" _msg = 'is None' if obj is None: <IF_STMT> msg = _msg elif values is True: msg = '%s: %s' % (msg, _msg) _report_failure(msg)",if msg is None:
def _parse_date_fmt(): fmt = get_format('DATE_FORMAT') escaped = False for char in fmt: if escaped: escaped = False <IF_STMT> escaped = True elif char in 'Yy': yield 'year' elif char in 'bEFMmNn': yield 'month' elif char in 'dj': yield 'day',elif char == '\\':
"def GetPluginClass(self): if self.plugin_name: plugin_cls = registry.OutputPluginRegistry.PluginClassByName(self.plugin_name) <IF_STMT> logging.warning('Unknown output plugin %s', self.plugin_name) return registry.OutputPluginRegistry.PluginClassByName('UnknownOutputPlugin') return plugin_cls",if plugin_cls is None:
def command(self): config = self.session.config unregister = False self.session.ui.notify(_('Watching logs: Press CTRL-C to return to the CLI')) try: while not mailpile.util.QUITTING and (not config.event_log): time.sleep(1) unregister = config.event_log and config.event_log.ui_watch(self.session.ui) self.session.ui.unblock(force=True) while not mailpile.util.QUITTING: time.sleep(1) except KeyboardInterrupt: pass finally: <IF_STMT> config.event_log.ui_unwatch(self.session.ui) return self._success(_('That was fun!')),if unregister:
"def delete_rule(self, arn): for load_balancer_arn in self.load_balancers: listeners = self.load_balancers.get(load_balancer_arn).listeners.values() for listener in listeners: for rule in listener.rules: <IF_STMT> listener.remove_rule(rule) return",if rule.arn == arn:
"def __dragBegin(self, widget, event): if event.buttons & (event.Buttons.Left | event.Buttons.Middle): GafferUI.Pointer.setCurrent('nodes') <IF_STMT> return next(iter(self.__graphComponents)) else: return Gaffer.StandardSet(self.__graphComponents) return None",if len(self.__graphComponents) == 1:
"def _get_strategy_name(self): frame = sys._getframe() while frame: st = frame.f_locals.get('self') <IF_STMT> return '%s.%s' % (type(st).__module__, type(st).__name__) frame = frame.f_back return ''","if isinstance(st, StrategyBase):"
"def getCommitFromFile(short=True): global _gitdir branch = getBranchFromFile() commit = None if _gitdir and branch: if branch == 'HEAD': commitFile = os.path.join(_gitdir, 'HEAD') else: commitFile = os.path.join(_gitdir, 'refs', 'heads', branch) <IF_STMT> with open(commitFile, 'r', encoding='utf-8') as f: commit = f.readline().strip() if short and commit: return commit[:8] else: return commit",if os.path.isfile(commitFile):
"def _register_aliases_from_pack(self, pack, aliases): registered_count = 0 for alias in aliases: try: LOG.debug('Loading alias from %s.', alias) self._register_action_alias(pack, alias) except Exception as e: <IF_STMT> msg = 'Failed to register alias ""%s"" from pack ""%s"": %s' % (alias, pack, str(e)) raise ValueError(msg) LOG.exception('Unable to register alias: %s', alias) continue else: registered_count += 1 return registered_count",if self._fail_on_failure:
"def pop_many(self, limit=None): if limit is None: limit = DEFAULT_SYNC_OFFLINE_ACTIVITY heartbeats = [] count = 0 while count < limit: heartbeat = self.pop() <IF_STMT> break heartbeats.append(heartbeat) count += 1 if count % HEARTBEATS_PER_REQUEST == 0: yield heartbeats heartbeats = [] if heartbeats: yield heartbeats",if not heartbeat:
"def makeChunkVertices(self, chunk): if chunk.root_tag and 'Level' in chunk.root_tag and ('TileTicks' in chunk.root_tag['Level']): ticks = chunk.root_tag['Level']['TileTicks'] <IF_STMT> self.vertexArrays.append(self._computeVertices([[t[i].value for i in 'xyz'] for t in ticks], (255, 255, 255, 68), chunkPosition=chunk.chunkPosition)) yield",if len(ticks):
"def read_bytes_from_url(url: str, optional=False) -> bytes: if parse_args().print_commands: print_stderr(color_line('=> ', 14) + f'GET {url}') req = request.Request(url) try: response = request.urlopen(req) except URLError as exc: print_error('urllib: ' + str(exc.reason)) <IF_STMT> return b'' if ask_to_continue(_('Do you want to retry?')): return read_bytes_from_url(url, optional=optional) raise SysExit(102) result_bytes = response.read() return result_bytes",if optional:
"def h2i(self, pkt, x): if x is not None: if x <= -180.00000005: warning('Fixed3_7: Input value too negative: %.8f' % x) x = -180.0 <IF_STMT> warning('Fixed3_7: Input value too positive: %.8f' % x) x = 180.0 x = int(round((x + 180.0) * 10000000.0)) return x",elif x >= 180.00000005:
"def replace_incompatible_files(): for filename, version_info in PYTHON_VERSION_REQUIREMENTS.items(): <IF_STMT> continue version = '.'.join((str(v) for v in version_info)) code = INCOMPATIBLE_PYTHON_VERSION_PLACEHOLDER.format(version=version) with open(filename, 'w') as f: f.write(code)",if sys.version_info >= version_info:
"def __eq__(self, other): if self.__class__ != other.__class__: return False for attr in ['bar', 'baz', 'quux']: <IF_STMT> return False elif getattr(self, attr, None) != getattr(other, attr, None): return False return True","if hasattr(self, attr) != hasattr(other, attr):"
"def get_content_length(download): try: meta = download.info() if hasattr(meta, 'getheaders') and hasattr(meta.getheaders, 'Content-Length'): return int(meta.getheaders('Content-Length')[0]) elif hasattr(download, 'getheader') and download.getheader('Content-Length'): return int(download.getheader('Content-Length')) <IF_STMT> return int(meta.getheader('Content-Length')) except Exception: pass return 0","elif hasattr(meta, 'getheader') and meta.getheader('Content-Length'):"
"def set_size(self, size): assert len(size) == 2 width, height = size if width == -1: for button in self._buttons_list: cur_width = button.GetSize()[self.WIDTH] <IF_STMT> width = cur_width if height == -1: for button in self._buttons_list: cur_height = button.GetSize()[self.HEIGHT] if cur_height > height: height = cur_height if self._squared: width = height = width if width > height else height for button in self._buttons_list: button.SetMinSize((width, height))",if cur_width > width:
"def _default_config(self): if sys.platform.startswith('win'): return {'name': 'Command Prompt', 'cmd': 'cmd.exe', 'env': {}} else: if 'SHELL' in os.environ: shell = os.environ['SHELL'] <IF_STMT> cmd = [shell, '-l'] else: cmd = [shell, '-i', '-l'] else: cmd = ['/bin/bash', '-i', '-l'] return {'name': 'Login Shell', 'cmd': cmd, 'env': {}}",if os.path.basename(shell) == 'tcsh':
"def log_sock(s, event_type=None): if sock_silent: pass elif event_type is None: logsocket.sendto(ensure_str(s), (host, port)) <IF_STMT> logsocket.sendto(ensure_str(s), (host, port)) else: pass",elif event_type in show_event:
"def check_eventref_citations(self, obj): if obj: for event_ref in obj.get_event_ref_list(): <IF_STMT> return True event = self.dbstate.db.get_event_from_handle(event_ref.ref) if self.check_event_citations(event): return True return False",if self.check_attribute_citations(event_ref):
"def __exit__(self, exc_type, exc_value, traceback): self.nest -= 1 if self.nest == 0: try: self.con.__exit__(exc_type, exc_value, traceback) self.close() except Exception as exc: <IF_STMT> self.debug.write('EXCEPTION from __exit__: {}'.format(exc)) raise",if self.debug:
"def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] <IF_STMT> collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models",if key not in collected_models:
"def delete(self): """"""Completely shut down pulseaudio client."""""" if self._pa_context is not None: assert _debug('PulseAudioContext.delete') <IF_STMT> pa.pa_context_disconnect(self._pa_context) while self.state is not None and (not self.is_terminated): self.wait() self._disconnect_callbacks() pa.pa_context_unref(self._pa_context) self._pa_context = None",if self.is_ready:
"def _hstack(self, other, prefix=None): """"""Join the columns of the other DataFrame to this one, assuming the ordering is the same"""""" assert len(self) == len(other), 'does not make sense to horizontally stack DataFrames with different lengths' for name in other.get_column_names(): <IF_STMT> new_name = prefix + name else: new_name = name self.add_column(new_name, other.columns[name])",if prefix:
"def smart_linkflags(source, target, env, for_signature): if cplusplus.iscplusplus(source): build_dir = env.subst('$BUILDDIR', target=target, source=source) <IF_STMT> return '-qtempinc=' + os.path.join(build_dir, 'tempinc') return ''",if build_dir:
"def read(self, size): x = len(self.buf) while x < size: raw = self.fileobj.read(self.blocksize) <IF_STMT> break data = self.bz2obj.decompress(raw) self.buf += data x += len(data) buf = self.buf[:size] self.buf = self.buf[size:] self.pos += len(buf) return buf",if not raw:
"def set_ok_verifiability(self, cookie, request): if request.unverifiable and is_third_party(request): if cookie.version > 0 and self.strict_rfc2965_unverifiable: _debug('   third-party RFC 2965 cookie during unverifiable transaction') return False <IF_STMT> _debug('   third-party Netscape cookie during unverifiable transaction') return False return True",elif cookie.version == 0 and self.strict_ns_unverifiable:
"def update_sockets(self, context): bools = [self.min_list, self.max_list, self.size_list] dims = int(self.dimensions[0]) for i in range(3): for j in range(3): out_index = 4 + j + 3 * i hidden = self.outputs[out_index].hide_safe <IF_STMT> if hidden: self.outputs[out_index].hide_safe = False else: self.outputs[out_index].hide_safe = True updateNode(self, context)",if bools[i][j] and j < dims:
"def hash_of_file(path): """"""Return the hash of a downloaded file."""""" with open(path, 'r') as archive: sha = sha256() while True: data = archive.read(2 ** 20) <IF_STMT> break sha.update(data) return encoded_hash(sha)",if not data:
"def _compute_early_outs(self, quotas): for q in quotas: if q.closed and (not self._ignore_closed): self.results[q] = (Quota.AVAILABILITY_ORDERED, 0) elif q.size is None: self.results[q] = (Quota.AVAILABILITY_OK, None) <IF_STMT> self.results[q] = (Quota.AVAILABILITY_GONE, 0)",elif q.size == 0:
"def providers_for_config_string(config_string, netcode): providers = [] for d in config_string.split(): p = provider_for_descriptor_and_netcode(d, netcode) <IF_STMT> providers.append(p) else: warnings.warn(""can't parse provider %s in config string"" % d) return providers",if p:
"def _get_plugin_value(self, feature, actor): for plugin in plugins.all(version=2): handlers = safe_execute(plugin.get_feature_hooks, _with_transaction=False) for handler in handlers or (): rv = handler(feature, actor) <IF_STMT> return rv return None",if rv is not None:
"def test_digit_numeric_consistent(self): count = 0 for i in xrange(65536): c = unichr(i) dec = self.db.digit(c, -1) <IF_STMT> self.assertEqual(dec, self.db.numeric(c)) count += 1 self.assertTrue(count >= 10)",if dec != -1:
"def call(command, title, retry): """"""Run a command-line program and display the result."""""" if Options.rerun_args: command, title, retry = Options.rerun_args Options.rerun_args = None success = call(command, title, retry) <IF_STMT> return False print('') print('$ %s' % ' '.join(command)) failure = subprocess.call(command) if failure and retry: Options.rerun_args = (command, title, retry) return not failure",if not success:
"def handle_custom_actions(self): for _, action in CustomAction.registry.items(): <IF_STMT> continue if action.action not in self.parser.choices: self.parser.add_parser(action.action, help='') action(self.page).add_arguments(self.parser, self)",if action.resource != self.resource:
"def __init__(self, user, *args, **kwargs): self.user = user super(AccountSettingsForm, self).__init__(*args, **kwargs) if self.user.is_managed: for field in ('email', 'name', 'username'): <IF_STMT> self.fields[field] = ReadOnlyTextField(label=self.fields[field].label) del self.fields['new_password'] if self.user.email == self.user.username: del self.fields['username']",if field == 'username' or field in settings.SENTRY_MANAGED_USER_FIELDS:
"def eval(self, code, eval=True, raw=False): self._engine._append_source(code) try: result = self._context.eval(code) except quickjs.JSException as e: raise ProgramError(*e.args) else: <IF_STMT> if raw or not isinstance(result, quickjs.Object): return result elif callable(result) and self.typeof(result) == u'function': return self.Function(self, result) else: return json.loads(result.json())",if eval:
"def get_def_offsets(self, defloc): """"""Get the byte offsets for a definition."""""" defn = self.defs[defloc.def_id] typ = defn.typ if typ == 'Attribute': start, end = self._get_attr_bounds(defn.name, defloc.location) else: start = self.source.get_offset(defloc.location) <IF_STMT> start += DEF_OFFSETS[typ] end = start + len(defn.name) return (start, end)",if typ in DEF_OFFSETS:
"def traverse_before_reduce(operator): """"""Internal traverse function"""""" if isinstance(operator, tvm.te.PlaceholderOp): return if tag.is_injective(operator.tag): sch[operator].compute_inline() for tensor in operator.input_tensors: <IF_STMT> traverse_before_reduce(tensor.op) else: raise RuntimeError('Unsupported operator: %s' % operator.tag) scheduled_ops.append(operator)",if tensor.op not in scheduled_ops:
"def _get_config(key): config = db.session.execute(Configs.__table__.select().where(Configs.key == key)).fetchone() if config and config.value: value = config.value if value and value.isdigit(): return int(value) elif value and isinstance(value, string_types): <IF_STMT> return True elif value.lower() == 'false': return False else: return value return KeyError",if value.lower() == 'true':
"def find_executable(names): for name in names: fpath, fname = os.path.split(name) if fpath: <IF_STMT> return name else: for path in os.environ['PATH'].split(os.pathsep): exe_file = os.path.join(path, name) if is_executable(exe_file): return exe_file return None",if is_executable(name):
"def push(self): advice = self.check() if not self._context['silent']: <IF_STMT> print('No changes to push.') return choice = input('Continue? y/N:') if choice != 'y': print('Aborted on user command') return print('push local changes to remote...') self._publish.syncRemote(self._context['srcroot'], advice)",if not self.hasPendingSync(advice):
"def __init__(self, itemtype, cnf={}, *, master=None, **kw): if not master: <IF_STMT> master = kw['refwindow'] elif 'refwindow' in cnf: master = cnf['refwindow'] else: master = tkinter._default_root if not master: raise RuntimeError('Too early to create display style: no root window') self.tk = master.tk self.stylename = self.tk.call('tixDisplayStyle', itemtype, *self._options(cnf, kw))",if 'refwindow' in kw:
"def __call__(self, x, **kwargs): h = x for layer, argnames, accept_var_args in zip(self.layers, self.argnames, self.accept_var_args): <IF_STMT> layer_kwargs = kwargs else: layer_kwargs = {k: v for k, v in kwargs.items() if k in argnames} h = layer(h, **layer_kwargs) return h",if accept_var_args:
def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): if self.should_save_model(): self.save_model() <IF_STMT> self.save_checkpoint() if self.should_eval_model(): self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result(),if self.should_save_checkpoint():
"def configure_callback(conf): """"""Received configuration information"""""" global ZK_HOSTS for node in conf.children: <IF_STMT> ZK_HOSTS = node.values[0].split(',') else: collectd.warning('zookeeper plugin: Unknown config key: %s.' % node.key) log('Configured with hosts=%s' % ZK_HOSTS)",if node.key == 'Hosts':
"def inner(self, *args, **kwargs): """"""Inner."""""" if not is_internet_available(): LOGGER.debug('\n\n%s', func.__name__) LOGGER.debug('============================') <IF_STMT> LOGGER.debug('"""""" %s """"""', func.__doc__.strip()) LOGGER.debug('----------------------------') LOGGER.debug('Skipping because no Internet connection available.') LOGGER.debug('\n++++++++++++++++++++++++++++') return None result = func(self, *args, **kwargs) return result",if func.__doc__:
"def _shares_in_results(data): shares_in_device, shares_in_subdevice = (False, False) for plugin_name, plugin_result in data.iteritems(): if plugin_result['status'] == 'error': continue if 'device' not in plugin_result: continue <IF_STMT> shares_in_device = True for subdevice in plugin_result['device'].get('subdevices', []): if 'disk_shares' in subdevice: shares_in_subdevice = True break return (shares_in_device, shares_in_subdevice)",if 'disk_shares' in plugin_result['device']:
"def register_auth_provider_blueprints(cls, app, prefix='/auth/login'): app.auth_providers = [] for provider in app.config.get('AUTH_PROVIDERS', ['debug', 'oauth']): <IF_STMT> provider = cls._get_subclass_for(provider.lower())(name=provider, app=app) app.register_blueprint(provider.blueprint, url_prefix='/'.join((prefix, provider.name))) app.auth_providers.append(provider)","if not isinstance(provider, KnowledgeAuthProvider):"
"def getText(self, stuff): if isinstance(stuff, Fighter): active = [x.name for x in stuff.abilities if x.active] <IF_STMT> return 'None' return ', '.join(active)",if len(active) == 0:
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): if item.isUnderCurrentProject(): items.append(item.url('url_production')) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items URL copied') else: sublime.status_message('Item URL copied')",if len(items) > 1:
"def read_boolean(file: BinaryIO, count: int, checkall: bool=False) -> List[bool]: if checkall: all_defined = file.read(1) if all_defined != unhexlify('00'): return [True] * count result = [] b = 0 mask = 0 for i in range(count): <IF_STMT> b = ord(file.read(1)) mask = 128 result.append(b & mask != 0) mask >>= 1 return result",if mask == 0:
"def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split('/', 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: <IF_STMT> comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)",if main in comments:
"def _filter_medias_not_commented(self, media_items): not_commented_medias = [] for media in media_items: if media.get('comment_count', 0) > 0 and media.get('comments'): my_comments = [comment for comment in media['comments'] <IF_STMT>] if my_comments: continue not_commented_medias.append(media) return not_commented_medias",if comment['user_id'] == self.user_id
"def run(url): import os for fpath in [os.path.expanduser('~/Applications/zeal.app'), '/Applications/zeal.app']: <IF_STMT> import subprocess, pipes pid = subprocess.Popen([fpath + '/Contents/MacOS/zeal', '--query={0}'.format(pipes.quote(url))], stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE) return",if os.path.exists(fpath + '/Contents/MacOS/zeal'):
"def get_input_info(exec_info, network): input_dict = collections.OrderedDict() for v in exec_info.data_variable: input_dict[v.variable_name] = [] for v in network.variable: <IF_STMT> shape = v.shape.dim input_dict[v.name] = [x if x > 0 else batch_size for x in shape] return input_dict",if v.name in input_dict:
"def _clean_text(self, text): """"""Performs invalid character removal and whitespace cleanup on text."""""" output = [] char_idx = [] for i, char in enumerate(text): cp = ord(char) if cp == 0 or cp == 65533 or _is_control(char): continue <IF_STMT> output.append(' ') char_idx.append(i) else: output.append(char) char_idx.append(i) return (''.join(output), char_idx)",if _is_whitespace(char):
"def AddVersion(version, ns, versionId='', isLegacy=0, serviceNs=''): if not ns: ns = serviceNs if version not in parentMap: nsMap[version] = ns if len(versionId) > 0: versionMap[ns + '/' + versionId] = version <IF_STMT> versionMap[ns] = version versionIdMap[version] = versionId if not serviceNs: serviceNs = ns serviceNsMap[version] = serviceNs parentMap[version] = set()",if isLegacy or ns is '':
"def set_accessible_async(self, trans, id=None, accessible=False): """"""Set workflow's importable attribute and slug."""""" stored = self.get_stored_workflow(trans, id) importable = accessible in ['True', 'true', 't', 'T'] if stored and stored.importable != importable: <IF_STMT> self._make_item_accessible(trans.sa_session, stored) else: stored.importable = importable trans.sa_session.flush() return",if importable:
"def update(self, val, n=1): if val is not None: self.val = val <IF_STMT> self.sum = type_as(self.sum, val) + val * n self.count = type_as(self.count, n) + n",if n > 0:
"def run(self, root): footnotesDiv = self.footnotes.makeFootnotesDiv(root) if footnotesDiv is not None: result = self.footnotes.findFootnotesPlaceholder(root) <IF_STMT> child, parent, isText = result ind = list(parent).index(child) if isText: parent.remove(child) parent.insert(ind, footnotesDiv) else: parent.insert(ind + 1, footnotesDiv) child.tail = None else: root.append(footnotesDiv)",if result:
def ehp(self): if self.__ehp is None: <IF_STMT> ehp = self.hp else: ehp = self.damagePattern.calculateEhp(self) self.__ehp = ehp return self.__ehp,if self.damagePattern is None:
"def literal(self): if self.peek('""'): lit, lang, dtype = self.eat(r_literal).groups() if lang: lang = lang else: lang = None <IF_STMT> dtype = dtype else: dtype = None if lang and dtype: raise ParseError(""Can't have both a language and a datatype"") lit = unquote(lit) return Literal(lit, lang, dtype) return False",if dtype:
"def _purge(self, queue): """"""Remove all messages from `queue`."""""" count = 0 queue_find = '.' + queue + '.msg' folder = os.listdir(self.data_folder_in) while len(folder) > 0: filename = folder.pop() try: <IF_STMT> continue filename = os.path.join(self.data_folder_in, filename) os.remove(filename) count += 1 except OSError: pass return count",if filename.find(queue_find) < 0:
"def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): if decrypter is None: return False f = open(fname, 'rb') s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: <IF_STMT> if read_only: return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, 'wb') f.write(s) f.close() return True",if decrypter is not None:
"def on_train_epoch_end(self, trainer, pl_module, outputs): epoch = trainer.current_epoch if self.unfreeze_backbone_at_epoch <= epoch: optimizer = trainer.optimizers[0] current_lr = optimizer.param_groups[0]['lr'] backbone_lr = self.previous_backbone_lr <IF_STMT> assert backbone_lr <= current_lr else: assert backbone_lr == current_lr",if epoch < 6:
"def parse_rsync_url(location): """"""Parse a rsync-style URL."""""" if ':' in location and '@' not in location: host, path = location.split(':', 1) user = None elif ':' in location: user_host, path = location.split(':', 1) <IF_STMT> user, host = user_host.rsplit('@', 1) else: user = None host = user_host else: raise ValueError('not a valid rsync-style URL') return (user, host, path)",if '@' in user_host:
"def populate_settings_dict(form, settings): new_settings = {} for key, value in iteritems(settings): try: <IF_STMT> continue else: new_settings[key] = form[key].data except KeyError: pass return new_settings",if value == form[key].data:
"def draw_boxes(image, boxes, scores=None, drop_score=0.5): if scores is None: scores = [1] * len(boxes) for box, score in zip(boxes, scores): <IF_STMT> continue box = np.reshape(np.array(box), [-1, 1, 2]).astype(np.int64) image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2) return image",if score < drop_score:
"def update(self, instance, validated_data): for category, category_data in validated_data.items(): <IF_STMT> continue self.update_validated_settings(category_data) for field_name, field_value in category_data.items(): setattr(getattr(instance, category), field_name, field_value) return instance",if not category_data:
"def insert(self, menuName, position, label, command, underline=None): menu = self.getMenu(menuName) if menu: <IF_STMT> menu.insert(position, 'command', label=label, command=command) else: menu.insert(position, 'command', label=label, command=command, underline=underline)",if underline is None:
"def delete_old_links(): for doc in web.ctx.site.store.values(type='account-link'): expiry_date = datetime.strptime(doc['expires_on'], '%Y-%m-%dT%H:%M:%S.%f') now = datetime.utcnow() key = doc['_key'] <IF_STMT> print('Deleting link %s' % key) del web.ctx.site.store[key] else: print('Retaining link %s' % key)",if expiry_date > now:
"def _object(o: edgedb.Object): ret = {} for attr in dir(o): try: link = o[attr] except (KeyError, TypeError): link = None <IF_STMT> ret[attr] = serialize(link) else: ret[attr] = serialize(getattr(o, attr)) return ret",if link:
"def __init__(self, items): self._format = string.join(map(lambda item: item[0], items), '') self._items = items self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format)) for format, name in self._items: if len(format) == 1: <IF_STMT> val = '\x00' else: val = 0 else: l = int(format[:-1]) val = '\x00' * l self.__dict__[name] = val",if format == 'c':
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') <IF_STMT> fragment = '<i>%s</i>' % fragment if sty.underline: fragment = '<u>%s</u>' % fragment if sty.strikeout: fragment = '<s>%s</s>' % fragment body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.italic:
"def get_from_target(target): domains = set() if isinstance(target, str): <IF_STMT> logger.log('FATAL', 'Use targets parameter for multiple domain names') exit(1) domain = match_main_domain(target) if not domain: return domains domains.add(domain) return domains",if target.endswith('.txt'):
"def iterate(self, prod_, rule_): newProduction = '' for i in range(len(prod_)): step = self.production[i] <IF_STMT> newProduction = newProduction + self.ruleW elif step == 'X': newProduction = newProduction + self.ruleX elif step == 'Y': newProduction = newProduction + self.ruleY elif step == 'Z': newProduction = newProduction + self.ruleZ elif step != 'F': newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction",if step == 'W':
"def cancel_pp(self, nzo_id): """"""Change the status, so that the PP is canceled"""""" for nzo in self.history_queue: <IF_STMT> nzo.abort_direct_unpacker() if nzo.pp_active: nzo.pp_active = False try: self.external_process.kill() logging.info('Killed external process %s', self.external_process.args[0]) except: pass return True return None",if nzo.nzo_id == nzo_id:
"def list_backends(debug): for backend in sorted(backends.getBackendList(), key=lambda backend: backend.identifier): <IF_STMT> print('{:>15} : {} ({})'.format(backend.identifier, backend.__doc__, backend.__name__)) else: print('{:>15} : {}'.format(backend.identifier, backend.__doc__))",if debug:
"def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): <IF_STMT> field_cls = field.document_type if field_cls in inspected: continue if hasattr(field_cls, '_geo_indices'): geo_indices += field_cls._geo_indices(inspected) elif field._geo_index: geo_indices.append(field) return geo_indices","if hasattr(field, 'document_type'):"
"def run_test_family(tests, mode_filter, files, open_func, *make_args): for test_func in tests: if test_func is None: out.write('\n') continue <IF_STMT> continue for s in test_func.file_sizes: name, size = files[size_names[s]] args = tuple((f(name, size) for f in make_args)) run_one_test(name, size, open_func, test_func, *args)",if mode_filter in test_func.file_open_mode:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_application_key(d.getPrefixedString()) continue <IF_STMT> self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def _on_config_changed(self, option: str) -> None: if option in ['zoom.levels', 'zoom.default']: <IF_STMT> factor = float(config.val.zoom.default) / 100 self.set_factor(factor) self._init_neighborlist()",if not self._default_zoom_changed:
"def keyPressEvent(self, event): """"""Add up and down arrow key events to built in functionality."""""" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() <IF_STMT> self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)",if self.completerStrings:
"def maxRange(self): attrs = ('shieldTransferRange', 'powerTransferRange', 'energyDestabilizationRange', 'empFieldRange', 'ecmBurstRange', 'maxRange') for attr in attrs: maxRange = self.getModifiedItemAttr(attr, None) if maxRange is not None: return maxRange if self.charge is not None: delay = self.getModifiedChargeAttr('explosionDelay', None) speed = self.getModifiedChargeAttr('maxVelocity', None) <IF_STMT> return delay / 1000.0 * speed",if delay is not None and speed is not None:
"def decref(self, *keys): for tileable_key, tileable_id in keys: <IF_STMT> continue _graph_key, ids = self._executed_tileables[tileable_key] if tileable_id in ids: ids.remove(tileable_id) if len(ids) != 0: continue self.delete_data(tileable_key)",if tileable_key not in self._executed_tileables:
"def run(self): for obj in bpy.context.scene.objects: if 'modelId' in obj: obj_id = obj['modelId'] if obj_id in self.lights: self._make_lamp_emissive(obj, self.lights[obj_id]) if obj_id in self.windows: self._make_window_emissive(obj) <IF_STMT> self._make_ceiling_emissive(obj)",if obj.name.startswith('Ceiling#'):
"def _create_bucket(self): """"""Create remote S3 bucket if it doesn't exist"""""" resource = boto3.resource('s3') try: resource.meta.client.head_bucket(Bucket=self.bucket) except ClientError as e: error_code = int(e.response['Error']['Code']) <IF_STMT> resource.create_bucket(Bucket=self.bucket) else: raise",if error_code == 404:
"def sort_sizes(size_list): """"""Sorts sizes with extensions. Assumes that size is already in largest unit possible"""""" final_list = [] for suffix in [' B', ' KB', ' MB', ' GB', ' TB']: sub_list = [float(size[:-len(suffix)]) for size in size_list if size.endswith(suffix) and size[:-len(suffix)][-1].isnumeric()] sub_list.sort() final_list += [str(size) + suffix for size in sub_list] <IF_STMT> break return final_list",if len(final_list) == len(size_list):
"def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str): """""" """""" for op in block.ops: for input_name in op.input_arg_names: if input_name == old_name: op._rename_input(old_name, new_name) for output_name in op.output_arg_names: <IF_STMT> op._rename_output(old_name, new_name) block._rename_var(old_name, new_name)",if output_name == old_name:
"def _GetParserChains(self, events): """"""Return a dict with a plugin count given a list of events."""""" parser_chains = {} for event in events: parser_chain = getattr(event, 'parser', None) <IF_STMT> continue if parser_chain in parser_chains: parser_chains[parser_chain] += 1 else: parser_chains[parser_chain] = 1 return parser_chains",if not parser_chain:
def context(self): if self.template is not None: <IF_STMT> return self.template.id if self.template.context: return self.template.context return self.template.getid() return self.unescape_csv(self.mainunit.getcontext()),if self.template.id:
"def _validate_min_max_value(field_name, value, opt): if isinstance(value, (int, float)): <IF_STMT> raise ValueError('Invalid value %s assigned to field %s.\n' % (value, field_name)) elif isinstance(value, str): if len(value) < opt['minValue'] or len(value) > opt['maxValue']: raise ValueError('Invalid value %s assigned to field %s.\n' % (value, field_name))",if value < opt['minValue'] or value > opt['maxValue']:
"def _incr_internal(key, instance=None, tags=None, amount=1): from sentry.app import tsdb if _should_sample(): amount = _sampled_value(amount) <IF_STMT> full_key = '{}.{}'.format(key, instance) else: full_key = key try: tsdb.incr(tsdb.models.internal, full_key, count=amount) except Exception: logger = logging.getLogger('sentry.errors') logger.exception('Unable to incr internal metric')",if instance:
"def get(self, key, default=None, version=None): key = self.make_key(key, version=version) self.validate_key(key) fname = self._key_to_file(key) try: with open(fname, 'rb') as f: exp = pickle.load(f) now = time.time() <IF_STMT> self._delete(fname) else: return pickle.load(f) except (IOError, OSError, EOFError, pickle.PickleError): pass return default",if exp < now:
"def on_execution_scenario(self, cpath, scenario): if isinstance(scenario, dict): self.check_scenario(cpath, scenario) elif isinstance(scenario, str): scenario_name = scenario scenario_path = Path('scenarios', scenario_name) scenario = self.linter.get_config_value(scenario_path, raise_if_not_found=False) <IF_STMT> self.report(ConfigWarning.ERROR, 'undefined-scenario', cpath, ""scenario %r is used but isn't defined"" % scenario_name)",if not scenario:
"def getSubmitKey(request, response): titleId = request.bits[2] titleKey = request.bits[3] try: <IF_STMT> return success(request, response, 'Key successfully added') else: return error(request, response, 'Key validation failed') except LookupError as e: error(request, response, str(e)) except OSError as e: error(request, response, str(e)) except BaseException as e: error(request, response, str(e))","if blockchain.blockchain.suggest(titleId, titleKey):"
"def test_downstream_trials(trial_associated_artifact, trial_obj, sagemaker_session): for i in range(3): time.sleep(10) trials = trial_associated_artifact.downstream_trials(sagemaker_session=sagemaker_session) <IF_STMT> break assert len(trials) == 1 assert trial_obj.trial_name in trials",if len(trials) > 0:
"def get_subfield_asts(context, return_type, field_asts): subfield_asts = DefaultOrderedDict(list) visited_fragment_names = set() for field_ast in field_asts: selection_set = field_ast.selection_set <IF_STMT> subfield_asts = collect_fields(context, return_type, selection_set, subfield_asts, visited_fragment_names) return subfield_asts",if selection_set:
"def _handle_children(self, removed, added): for obj in removed: obj.stop() for obj in added: obj.set(scene=self.scene, parent=self) <IF_STMT> obj.source = self elif is_filter(obj): obj.inputs.append(self) if self.running: try: obj.start() except: exception()","if isinstance(obj, ModuleManager):"
"def __kmp_search(S, W): m = 0 i = 0 T = __kmp_table(W) while m + i < len(S): <IF_STMT> i += 1 if i == len(W): yield m m += i - T[i] i = max(T[i], 0) else: m += i - T[i] i = max(T[i], 0)",if S[m + i] == W[i]:
"def connection(self, commit_on_success=False): with self._lock: <IF_STMT> if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute('PRAGMA synchronous = 0;') yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()",if self._bulk_commit:
def passed(self): for test in self.lints[0]: for template in self.lints[0][test]['results']: results = self.lints[0][test]['results'][template] if results: <IF_STMT> return False return True,if self._is_error(results) or self.strict:
"def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), '127.0.0.255') <IF_STMT> self.assertEqual(str(ip), '127.0.1.0') elif i == 1000: self.assertEqual(str(ip), '127.0.3.233') elif i == 65534: self.assertEqual(str(ip), '127.0.255.255') elif i == 65535: self.assertEqual(str(ip), '127.1.0.0')",elif i == 255:
"def _DecodeUnknownMessages(message, encoded_message, pair_type): """"""Process unknown fields in encoded_message of a message type."""""" field_type = pair_type.value.type new_values = [] all_field_names = [x.name for x in message.all_fields()] for name, value_dict in encoded_message.iteritems(): <IF_STMT> continue value = PyValueToMessage(field_type, value_dict) new_pair = pair_type(key=name, value=value) new_values.append(new_pair) return new_values",if name in all_field_names:
"def test_apply_noise_model(): p = Program(RX(np.pi / 2, 0), RX(np.pi / 2, 1), CZ(0, 1), RX(np.pi / 2, 1)) noise_model = _decoherence_noise_model(_get_program_gates(p)) pnoisy = apply_noise_model(p, noise_model) for i in pnoisy: <IF_STMT> pass elif isinstance(i, Pragma): assert i.command in ['ADD-KRAUS', 'READOUT-POVM'] elif isinstance(i, Gate): assert i.name in NO_NOISE or not i.params","if isinstance(i, DefGate):"
"def i2h(self, pkt, x): if x is not None: if x < 0: warning('Fixed3_7: Internal value too negative: %d' % x) x = 0 <IF_STMT> warning('Fixed3_7: Internal value too positive: %d' % x) x = 3600000000 x = (x - 1800000000) * 1e-07 return x",elif x > 3600000000:
def onClicked(event): shaderConfig = dict() for child in self.shaderDefBox.children: defName = child.shaderDefine enabled = child.isChecked() try: <IF_STMT> mat.addShaderDefine(defName) else: mat.removeShaderDefine(defName) except: pass self.listUniforms(mat) self.listMaterialSettings(self.getSelectedObject()),if enabled:
"def is_mod(self, member: discord.Member) -> bool: """"""Checks if a member is a mod or admin of their guild."""""" try: member_snowflakes = member._roles for snowflake in await self._config.guild(member.guild).admin_role(): <IF_STMT> return True for snowflake in await self._config.guild(member.guild).mod_role(): if member_snowflakes.has(snowflake): return True except AttributeError: pass return False",if member_snowflakes.has(snowflake):
"def _verify_treestore(itr, tree_values): i = 0 while itr: values = tree_values[i] if treestore[itr][0] != values[0]: return False if treestore.iter_children(itr): <IF_STMT> return False itr = treestore.iter_next(itr) i += 1 return True","if not _verify_treestore(treestore.iter_children(itr), values[1]):"
"def _default_config(self): if sys.platform.startswith('win'): return {'name': 'Command Prompt', 'cmd': 'cmd.exe', 'env': {}} else: <IF_STMT> shell = os.environ['SHELL'] if os.path.basename(shell) == 'tcsh': cmd = [shell, '-l'] else: cmd = [shell, '-i', '-l'] else: cmd = ['/bin/bash', '-i', '-l'] return {'name': 'Login Shell', 'cmd': cmd, 'env': {}}",if 'SHELL' in os.environ:
"def _messageHandled(self, resultList): failures = 0 for success, result in resultList: <IF_STMT> failures += 1 log.err(result) if failures: msg = 'Could not send e-mail' resultLen = len(resultList) if resultLen > 1: msg += ' ({} failures out of {} recipients)'.format(failures, resultLen) self.sendCode(550, networkString(msg)) else: self.sendCode(250, b'Delivery in progress')",if not success:
"def to_internal_value(self, data): site = get_current_site() pages_root = reverse('pages-root') ret = [] for path in data: if path.startswith(pages_root): path = path[len(pages_root):] <IF_STMT> path = path[:-1] page = get_page_from_path(site, path) if page: ret.append(page) return ret",if path.endswith('/'):
"def _prune(self): with self.lock: entries = self._list_dir() if len(entries) > self._threshold: now = time.time() try: for i, fpath in enumerate(entries): remove = False f = LockedFile(fpath, 'rb') exp = pickle.load(f.file) f.close() remove = exp <= now or i % 3 == 0 <IF_STMT> self._del_file(fpath) except Exception: pass",if remove:
"def get_ax_arg(uri): if not ax_ns: return u'' prefix = 'openid.' + ax_ns + '.type.' ax_name = None for name in self.request.arguments.keys(): <IF_STMT> part = name[len(prefix):] ax_name = 'openid.' + ax_ns + '.value.' + part break if not ax_name: return u'' return self.get_argument(ax_name, u'')",if self.get_argument(name) == uri and name.startswith(prefix):
"def _generate_expression(self): e = [] for part in PARSE_RE.split(self._format): if not part: continue <IF_STMT> e.append('\\{') elif part == '}}': e.append('\\}') elif part[0] == '{' and part[-1] == '}': e.append(self._handle_field(part)) else: e.append(REGEX_SAFETY.sub(self._regex_replace, part)) return ''.join(e)",elif part == '{{':
"def get_clean_username(user): try: username = force_text(user) except AttributeError: username = 'anonymous' else: <IF_STMT> username = u'{0}... (id={1})'.format(username[:PAGE_USERNAME_MAX_LENGTH - 15], user.pk) return username",if len(username) > PAGE_USERNAME_MAX_LENGTH:
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: try: body = str(request.body, 'utf-8') if isinstance(request.body, bytes) else str(request.body) except TypeError: body = str(request.body) <IF_STMT> request.body = body.replace(old, new) return request",if old in body:
"def get_config_variable(self, name, methods=('env', 'config'), default=None): value = None config_name, envvar_name = self.session_var_map[name] if methods is not None: if 'env' in methods and value is None: value = os.environ.get(envvar_name) <IF_STMT> value = self.config_file_vars.get(config_name) else: value = default return value",if 'config' in methods and value is None:
"def read_subpkgdata_dict(pkg, d): ret = {} subd = read_pkgdatafile(get_subpkgedata_fn(pkg, d)) for var in subd: newvar = var.replace('_' + pkg, '') <IF_STMT> continue ret[newvar] = subd[var] return ret",if newvar == var and var + '_' + pkg in subd:
"def _classify_volume(self, ctxt, volumes): bypass_volumes = [] replica_volumes = [] for v in volumes: volume_type = self._get_volume_replicated_type(ctxt, v) grp = v.group if grp and utils.is_group_a_type(grp, 'consistent_group_replication_enabled'): continue <IF_STMT> replica_volumes.append(v) else: bypass_volumes.append(v) return (bypass_volumes, replica_volumes)","elif volume_type and v.status in ['available', 'in-use']:"
"def _ensure_entity_values(self): entities_values = {entity.name: self._get_entity_values(entity) for entity in self.entities} for intent in self.intents: for utterance in intent.utterances: for chunk in utterance.slot_chunks: <IF_STMT> continue try: chunk.text = next(entities_values[chunk.entity]) except StopIteration: raise DatasetFormatError(""At least one entity value must be provided for entity '%s'"" % chunk.entity) return self",if chunk.text is not None:
"def _consume_msg(self): async for data in self._stream: stream = data.get('ev') if stream: await self._dispatch(data) <IF_STMT> data['ev'] = 'status' await self._dispatch(data) raise ConnectionResetError(f""Polygon terminated connection: ({data.get('message')})"")",elif data.get('status') == 'disconnected':
"def GetHeaderWidth(self): """"""Returns the header window width, in pixels."""""" if not self._headerWidth: count = self.GetColumnCount() for col in range(count): <IF_STMT> continue self._headerWidth += self.GetColumnWidth(col) if self.HasAGWFlag(ULC_FOOTER): self._footerWidth = self._headerWidth return self._headerWidth",if not self.IsColumnShown(col):
"def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): <IF_STMT> self.assertEqual(str(ip), '127.0.0.255') elif i == 255: self.assertEqual(str(ip), '127.0.1.0') elif i == 1000: self.assertEqual(str(ip), '127.0.3.233') elif i == 65534: self.assertEqual(str(ip), '127.0.255.255') elif i == 65535: self.assertEqual(str(ip), '127.1.0.0')",if i == 254:
"def childrenTodo(self, p=None): if p is None: p = self.c.currentPosition() for p in p.children(): <IF_STMT> continue self.setat(p.v, 'priority', 19) self.loadIcons(p)","if self.getat(p.v, 'priority') != 9999:"
"def __init__(self, **kwargs): super(DepthwiseSeparableASPPModule, self).__init__(**kwargs) for i, dilation in enumerate(self.dilations): <IF_STMT> self[i] = DepthwiseSeparableConvModule(self.in_channels, self.channels, 3, dilation=dilation, padding=dilation, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg)",if dilation > 1:
"def test_char(self): for x in range(256): c = System.Char.Parse(chr(x)) self.assertEqual(c, chr(x)) self.assertEqual(chr(x), c) if c == chr(x): pass else: self.assertTrue(False) <IF_STMT> self.assertTrue(False) if chr(x) == c: pass else: self.assertTrue(False) if not chr(x) == c: self.assertTrue(False)",if not c == chr(x):
"def create_model_handler(ns, model_type):  @route(f'/<provider>/{ns}/<model_id>') @use_provider def handle(req, provider, model_id): <IF_STMT> if model_id == 'me': user = getattr(provider, '_user', None) if user is None: raise CmdException(f'log in provider:{provider.identifier} first') return user model = get_model_or_raise(provider, model_type, model_id) return model",if model_type == ModelType.user:
"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_key_: res += prefix + 'key: %s\n' % self.DebugFormatString(self.key_) cnt = 0 for e in self.value_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'value%s: %s\n' % (elm, self.DebugFormatString(e)) cnt += 1 if self.has_partial_: res += prefix + 'partial: %s\n' % self.DebugFormatBool(self.partial_) return res",if printElemNumber:
"def set_value_type_index(self, rows: list, value_type_index: int): for row in rows: label = self.message_type[row] <IF_STMT> label.value_type_index = value_type_index self.protocol_label_updated.emit(label) self.update()",if not label.is_checksum_label:
"def get_model_param(self, job_id, cpn_name, role, party_id): result = None party_id = str(party_id) try: result = self.client.component.output_model(job_id=job_id, role=role, party_id=party_id, component_name=cpn_name) <IF_STMT> raise ValueError(f'job {job_id}, component {cpn_name} has no output model param') return result['data'] except: raise ValueError('Cannot get output model, err msg: ')",if 'data' not in result:
"def validate(self) -> None: if self.query: if not self.sysupgrade: for arg_name in ('aur', 'repo'): <IF_STMT> raise MissingArgument('sysupgrade', arg_name)","if getattr(self, arg_name):"
"def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break <IF_STMT> break choices = parser._subparsers._actions[-1].choices value = getattr(args, 'level_%d' % level) if value is None: parser.print_help() return if not choices: break if isinstance(choices, dict): parser = choices[value] else: return level += 1",if parser._subparsers._actions is None:
"def merge(self, abort=False, message=None): """"""Merge remote branch or reverts the merge."""""" if abort: self.execute(['update', '--clean', '.']) elif self.needs_merge(): if self.needs_ff(): self.execute(['update', '--clean', 'remote(.)']) else: self.configure_merge() try: self.execute(['merge', '-r', 'remote(.)']) except RepositoryException as error: <IF_STMT> return raise self.execute(['commit', '--message', 'Merge'])",if error.retcode == 255:
"def parseArtistIds(cls, page): ids = list() js = demjson.decode(page) if 'error' in js and js['error']: raise PixivException('Error when requesting Fanbox', 9999, page) if 'body' in js and js['body'] is not None: js_body = js['body'] <IF_STMT> js_body = js_body['supportingPlans'] for creator in js_body: ids.append(creator['user']['userId']) return ids",if 'supportingPlans' in js['body']:
"def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super().ignore(other) <IF_STMT> self.expr.ignore(self.ignoreExprs[-1]) else: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if self.expr is not None:
"def execute(self): func = self.func is_batch_func = getattr(func, '_task_batch', False) g['current_task_is_batch'] = is_batch_func g['current_tasks'] = [self] try: <IF_STMT> return func([{'args': self.args, 'kwargs': self.kwargs}]) else: return func(*self.args, **self.kwargs) finally: g['current_task_is_batch'] = None g['current_tasks'] = None",if is_batch_func:
"def fn(value=None): for i in [-1, 0, 1, 2, 3, 4]: if i < 0: continue elif i == 0: yield 0 <IF_STMT> yield 1 i = 0 yield value yield 2 else: try: v = i / value except: v = i yield v",elif i == 1:
"def get_instrumentation_key(url): data = url.split('//')[1] try: uuid.UUID(data) except ValueError: values = data.split('/') <IF_STMT> AppInsightsHelper.log.warning(""Bad format: '%s'"" % url) return AppInsightsHelper._get_instrumentation_key(values[0], values[1]) return data",if len(values) != 2:
"def get_correct(ngrams_ref, ngrams_test, correct, total): for rank in ngrams_test: for chain in ngrams_test[rank]: total[rank] += ngrams_test[rank][chain] <IF_STMT> correct[rank] += min(ngrams_test[rank][chain], ngrams_ref[rank][chain]) return (correct, total)",if chain in ngrams_ref[rank]:
"def _content_type_params__set(self, value_dict): if not value_dict: del self.content_type_params return params = [] for k, v in sorted(value_dict.items()): <IF_STMT> v = '""%s""' % v.replace('""', '\\""') params.append('; %s=%s' % (k, v)) ct = self.headers.pop('Content-Type', '').split(';', 1)[0] ct += ''.join(params) self.headers['Content-Type'] = ct",if not _OK_PARAM_RE.search(v):
"def split_file(self, filename, block_size=2 ** 20): with open(filename, 'rb') as f: file_list = [] while True: data = f.read(block_size) <IF_STMT> break filehash = os.path.join(self.resource_dir, self.__count_hash(data)) filehash = os.path.normpath(filehash) with open(filehash, 'wb') as fwb: fwb.write(data) file_list.append(filehash) return file_list",if not data:
"def _set_live(self, live, _): if live is not None and (not self.live): if isinstance(live, basestring): live = [live] if len(live) == 0: mode = 'Memory' <IF_STMT> mode = live[0] else: raise RuntimeError('--live parameter should specify only one mode.') live_plugin = self.session.plugins.live(mode=mode) live_plugin.live() self.session.register_flush_hook(self, live_plugin.close) return live",elif len(live) == 1:
"def process_percent(token, state, command_line): if not state.is_range_start_line_parsed: <IF_STMT> raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.end.append(token) return (parse_line_ref, command_line)",if command_line.line_range.start:
"def gprv_implicit_orax(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: if op.name == 'REG0' and op_luf(op, 'GPRv_SB'): continue else: return False elif i == 1: <IF_STMT> continue else: return False else: return False return True","if op.name == 'REG1' and op_luf(op, 'OrAX'):"
"def _check_events(self): stack = [] old = self.events[:] for type_, song in self.events: <IF_STMT> stack.append(song) elif type_ == 'ended': self.assertTrue(stack.pop(-1) is song, msg=old) self.assertFalse(stack, msg=old)",if type_ == 'started':
"def _minimal_replacement_cost(self, first, second): first_symbols, second_symbols = (set(), set()) removal_cost, insertion_cost = (0, 0) for a, b in itertools.zip_longest(first, second, fillvalue=None): if a is not None: first_symbols.add(a) <IF_STMT> second_symbols.add(b) removal_cost = max(removal_cost, len(first_symbols - second_symbols)) insertion_cost = max(insertion_cost, len(second_symbols - first_symbols)) return min(removal_cost, insertion_cost)",if b is not None:
"def get_default_backend(self, user_backends): retval = None n_defaults = 0 for name in user_backends: args = user_backends.get(name) <IF_STMT> n_defaults = n_defaults + 1 if retval is None: retval = name return (retval, n_defaults)","if args.get('default', False):"
"def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, 'SIGTTOU'): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) <IF_STMT> signal.signal(signal.SIGTTOU, old_handler)",if old_handler is not None:
"def load_dashboard_module_view(request, pk): result = {'error': False} try: <IF_STMT> raise ValidationError('error') instance = UserDashboardModule.objects.get(pk=pk, user=request.user.pk) module_cls = instance.load_module() module = module_cls(model=instance, context={'request': request}) result['html'] = module.render() except (ValidationError, UserDashboardModule.DoesNotExist): result['error'] = True return JsonResponse(result)",if not user_is_authenticated(request.user) or not request.user.is_staff:
"def _validate_compatible(from_schema, to_schema): if set(from_schema.names) != set(to_schema.names): raise com.IbisInputError('Schemas have different names') for name in from_schema: lt = from_schema[name] rt = to_schema[name] <IF_STMT> raise com.IbisInputError('Cannot safely cast {0!r} to {1!r}'.format(lt, rt)) return",if not lt.castable(rt):
"def load_yaml(self): if 'FUEL_CONFIG' in os.environ: yaml_file = os.environ['FUEL_CONFIG'] else: yaml_file = os.path.expanduser('~/.fuelrc') if os.path.isfile(yaml_file): with open(yaml_file) as f: for key, value in yaml.safe_load(f).items(): <IF_STMT> raise ValueError('Unrecognized config in YAML: {}'.format(key)) self.config[key]['yaml'] = value",if key not in self.config:
"def process(self): if not self.outputs['Polygons'].is_linked: return verts = self.inputs['Vertices'].sv_get() faces = self.inputs['Polygons'].sv_get() if not len(verts) == len(faces): return verts_out = [] polys_out = [] for v_obj, f_obj in zip(verts, faces): res = join_tris(v_obj, f_obj, self) <IF_STMT> return verts_out.append(res[0]) polys_out.append(res[1]) self.outputs['Vertices'].sv_set(verts_out) self.outputs['Polygons'].sv_set(polys_out)",if not res:
"def _set_momentum(self, runner, momentum_groups): for param_group, mom in zip(runner.optimizer.param_groups, momentum_groups): if 'momentum' in param_group.keys(): param_group['momentum'] = mom <IF_STMT> param_group['betas'] = (mom, param_group['betas'][1])",elif 'betas' in param_group.keys():
"def getReceiptInfo(pkgname): """"""Get receipt info from a package"""""" info = [] if hasValidPackageExt(pkgname): display.display_debug2('Examining %s' % pkgname) if os.path.isfile(pkgname): info = getFlatPackageInfo(pkgname) <IF_STMT> info = getBundlePackageInfo(pkgname) elif pkgname.endswith('.dist'): info = parsePkgRefs(pkgname) return info",if os.path.isdir(pkgname):
"def _add_directory_child(self, children, filename): if os.path.isdir(filename): children.append(self._directory_controller(filename)) else: r = self._namespace.get_resource(filename, report_status=False) <IF_STMT> children.append(self._resource_controller(r))",if self._is_valid_resource(r):
"def check_br_addr(self, br): ips = {} cmd = 'ip a show dev %s' % br for line in self.execute(cmd, sudo=True).split('\n'): if line.strip().startswith('inet '): elems = [e.strip() for e in line.strip().split(' ')] ips[4] = elems[1] <IF_STMT> elems = [e.strip() for e in line.strip().split(' ')] ips[6] = elems[1] return ips",elif line.strip().startswith('inet6 '):
"def execute(self, statement, parameters=None): try: <IF_STMT> result = self.real_cursor.execute(statement, parameters) else: result = self.real_cursor.execute(statement) return result except: raise Error(sys.exc_info()[1])",if parameters:
"def isUpdateAvailable(self, localOnly=False): nsp = self.getLatestFile() <IF_STMT> if not nsp: if not self.isUpdate or (self.version and int(self.version) > 0): return True else: return False try: latest = self.lastestVersion(localOnly=localOnly) if latest is None: return False if int(nsp.version) < int(latest): return True except BaseException as e: Print.error('isUpdateAvailable exception %s: %s' % (self.id, str(e))) pass return False",if not nsp:
"def align(size): if size <= 4096: <IF_STMT> return size elif size < 128: return min_ge(range(16, 128 + 1, 16), size) elif size < 512: return min_ge(range(192, 512 + 1, 64), size) else: return min_ge(range(768, 4096 + 1, 256), size) elif size < 4194304: return min_ge(range(4096, 4194304 + 1, 4096), size) else: return min_ge(range(4194304, 536870912 + 1, 4194304), size)",if is_power2(size):
"def __init__(self, transforms): assert isinstance(transforms, collections.abc.Sequence) self.transforms = [] for transform in transforms: <IF_STMT> transform = build_from_cfg(transform, PIPELINES) self.transforms.append(transform) elif callable(transform): self.transforms.append(transform) else: raise TypeError('transform must be callable or a dict')","if isinstance(transform, dict):"
"def branch_name_from_config_file(directory, config_file): ans = None try: with open(config_file, 'rb') as f: for line in f: m = nick_pat.match(line) <IF_STMT> ans = m.group(1).strip().decode(get_preferred_file_contents_encoding(), 'replace') break except Exception: pass return ans or os.path.basename(directory)",if m is not None:
"def do_acquire_write_lock(self, wait): owner_id = self._get_owner_id() while True: if self.client.setnx(self.identifier, owner_id): self.client.pexpire(self.identifier, self.LOCK_EXPIRATION * 1000) return True <IF_STMT> return False time.sleep(0.2)",if not wait:
"def add_files_for_package(sub_package_path, root_package_path, root_package_name): for root, dirs, files in os.walk(sub_package_path): <IF_STMT> dirs.remove('.svn') for f in files: if not f.endswith('.pyc') and (not f.startswith('.')): add(dereference(root + '/' + f), root.replace(root_package_path, root_package_name) + '/' + f)",if '.svn' in dirs:
"def collect_state(object_name, prefix, d): if d[None] is False: return [] result = [] if d[None] is True and prefix is not None: name = v.make_measurement_choice(object_name, prefix) if name in choices: result.append(name) for key in [x for x in list(d.keys()) if x is not None]: <IF_STMT> sub_prefix = key else: sub_prefix = '_'.join((prefix, key)) result += collect_state(object_name, sub_prefix, d[key]) return result",if prefix is None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_num_memcacheg_backends(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def check(dbdef): """"""database version must include required keys"""""" for vnum, vdef in dbdef.items(): missing = set(required) - set(vdef) <IF_STMT> missing -= set(initially_ok) if missing: yield (vnum, missing)",if vnum == min(dbdef):
"def _check(ret): if hasattr(ret, 'value'): ret = ret.value if ret != 0: <IF_STMT> raise USBTimeoutError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret]) else: raise USBError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret]) return ret",if ret == OPENUSB_IO_TIMEOUT:
"def scroll_to(self, x=None, y=None): if x is None or y is None: pos = self.tab.get_scroll_position() x = pos['x'] if x is None else x y = pos['y'] if y is None else y for value, name in [(x, 'x'), (y, 'y')]: <IF_STMT> raise ScriptError({'argument': name, 'message': 'scroll {} coordinate must be a number, got {}'.format(name, repr(value))}) self.tab.set_scroll_position(x, y)","if not isinstance(value, (int, float)):"
"def _validate_secret_list(self, secrets, expected): for secret in secrets: <IF_STMT> expected_secret = expected[secret.name] self._assert_secret_attributes_equal(expected_secret.properties, secret) del expected[secret.name] self.assertEqual(len(expected), 0)",if secret.name in expected.keys():
"def _capture_hub(self, create): <IF_STMT> current_hub = get_hub() if create else get_hub_if_exists() if current_hub is None: return if self.hub is None: self.hub = current_hub",if self.hub is None:
"def _hashable(self): hashes = [self.graph.md5()] for g in self.geometry.values(): if hasattr(g, 'md5'): hashes.append(g.md5()) <IF_STMT> hashes.append(str(hash(g.tostring()))) else: hashes.append(str(hash(g))) hashable = ''.join(sorted(hashes)).encode('utf-8') return hashable","elif hasattr(g, 'tostring'):"
"def load_distribution(args: CommandLineArguments) -> CommandLineArguments: if args.distribution is not None: args.distribution = Distribution[args.distribution] if args.distribution is None or args.release is None: d, r = detect_distribution() <IF_STMT> args.distribution = d if args.distribution == d and d != Distribution.clear and (args.release is None): args.release = r if args.distribution is None: die(""Couldn't detect distribution."") return args",if args.distribution is None:
"def is_different(item, seen): is_diff = True if item not in seen: for value in other: if comparator(iteratee(item), iteratee(value)): is_diff = False break <IF_STMT> seen.append(item) return is_diff",if is_diff:
"def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break if pos > 0 and dn[pos - 1] != '\\': break elif pos > 1 and dn[pos - 1] == '\\': escaped = True for c in dn[pos - 2:0:-1]: <IF_STMT> escaped = not escaped else: break if not escaped: break pos += 1 return pos",if c == '\\':
"def vcf_has_nonfiltered_variants(in_file): if os.path.exists(in_file): with utils.open_gzipsafe(in_file) as in_handle: for line in in_handle: <IF_STMT> parts = line.split('\t') if parts[6] in set(['PASS', '.']): return True return False",if line.strip() and (not line.startswith('#')):
"def clean_vendor(ctx, vendor_dir): remove_all(vendor_dir.glob('*.pyc')) log('Cleaning %s' % vendor_dir) for item in vendor_dir.iterdir(): if item.is_dir(): shutil.rmtree(str(item)) <IF_STMT> item.unlink() else: log('Skipping %s' % item)",elif item.name not in FILE_WHITE_LIST:
"def sel_line(view, s): if mode == modes.INTERNAL_NORMAL: <IF_STMT> if view.line(s.b).size() > 0: eol = view.line(s.b).b begin = view.line(s.b).a begin = utils.next_non_white_space_char(view, begin, white_space=' \t') return R(begin, eol) return s return s",if count == 1:
"def _struct(self, fields): result = {} for field in fields: <IF_STMT> parent = self.instance(field[1]) if isinstance(parent, dict): result.update(parent) elif len(fields) == 1: result = parent else: result[field[0]] = parent else: result[field[0]] = self.instance(field[1]) return result",if field[0] == '__parent':
"def _decode_list(lst): if not PY2: return lst newlist = [] for i in lst: <IF_STMT> i = to_bytes(i) elif isinstance(i, list): i = _decode_list(i) newlist.append(i) return newlist","if isinstance(i, string_types):"
"def _check_arguments(self, arch, state): cc = DEFAULT_CC[arch.name](arch) for i, expected_arg in enumerate(self.arguments): <IF_STMT> continue real_arg = cc.arg(state, i) expected_arg_type, expected_arg_value = expected_arg r = self._compare_arguments(state, expected_arg_type, expected_arg_value, real_arg) if not r: return False return True",if expected_arg is None:
"def _strip_classy_blocks(self, module): for name, child_module in module.named_children(): <IF_STMT> module.add_module(name, child_module.wrapped_module()) self._strip_classy_blocks(child_module)","if isinstance(child_module, ClassyBlock):"
"def test_07_verify_degraded_pool_alert_list_exist_and_get_id(): global alert_id results = GET('/alert/list/') assert results.status_code == 200, results.text assert isinstance(results.json(), list), results.text for line in results.json(): <IF_STMT> alert_id = results.json()[0]['id'] assert results.json()[0]['args']['volume'] == pool_name, results.text assert results.json()[0]['args']['state'] == 'DEGRADED', results.text assert results.json()[0]['level'] == 'CRITICAL', results.text break",if line['source'] == 'VolumeStatus':
"def parseApplicationExtension(parent): yield PascalString8(parent, 'app_name', 'Application name') yield UInt8(parent, 'size') size = parent['size'].value if parent['app_name'].value == 'NETSCAPE2.0' and size == 3: yield Enum(UInt8(parent, 'netscape_code'), NETSCAPE_CODE) <IF_STMT> yield UInt16(parent, 'loop_count') else: yield RawBytes(parent, 'raw', 2) else: yield RawBytes(parent, 'raw', size) yield NullBytes(parent, 'terminator', 1, 'Terminator (0)')",if parent['netscape_code'].value == 1:
def tearDownClass(self): settings.TIME_ZONE = connection.settings_dict['TIME_ZONE'] = self._old_time_zone timezone._localtime = None if TZ_SUPPORT: <IF_STMT> del os.environ['TZ'] else: os.environ['TZ'] = self._old_tz time.tzset(),if self._old_tz is None:
"def __getattr__(self, key): if key in self._raw: val = self._raw[key] if key in ('date',): return pd.Timestamp(val) <IF_STMT> return pd.Timestamp(val).time() elif key in ('session_open', 'session_close'): return pd.Timestamp(val[:2] + ':' + val[-2:]).time() else: return val return super().__getattr__(key)","elif key in ('open', 'close'):"
"def _extract_knob_feature_log(arg): """"""extract knob feature for log items"""""" try: inp, res = arg config = inp.config x = config.get_flatten_feature() <IF_STMT> with inp.target: inp.task.instantiate(config) y = inp.task.flop / np.mean(res.costs) else: y = 0.0 return (x, y) except Exception: return None",if res.error_no == 0:
"def dvipng_hack_alpha(): stdin, stdout = os.popen4('dvipng -version') for line in stdout: <IF_STMT> version = line.split()[-1] mpl.verbose.report('Found dvipng version %s' % version, 'helpful') version = distutils.version.LooseVersion(version) return version < distutils.version.LooseVersion('1.6') raise RuntimeError('Could not obtain dvipng version')",if line.startswith('dvipng '):
"def _get_func_name(self, current_cls: Generic, module_func_dict: dict) -> Optional[str]: mod = current_cls.__module__ + '.' + current_cls.__name__ if mod in module_func_dict: _func_name = module_func_dict[mod] return _func_name elif current_cls.__bases__: for base_class in current_cls.__bases__: base_run_func = self._get_func_name(base_class, module_func_dict) <IF_STMT> return base_run_func else: return None",if base_run_func:
"def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) <IF_STMT> raise IndexError('Index %s out of range (%s elements)' % (key, l)) if key < 0: if key < -l: raise IndexError('Index %s out of range (%s elements)' % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError(self.impl.__class__.__name__ + ' object does not support slicing') else: return self(key)",if key >= l:
"def add_user_functions(self): for udf in user_functions: <IF_STMT> self.conn.create_aggregate(udf.name, udf.param_count, udf.func_or_obj) elif type(udf.func_or_obj) == type(md5): self.conn.create_function(udf.name, udf.param_count, udf.func_or_obj) else: raise Exception('Invalid user function definition %s' % str(udf))",if type(udf.func_or_obj) == type(object):
"def _get_schema_references(self, s): refs = set() if isinstance(s, dict): for k, v in s.items(): if isinstance(v, six.string_types): m = self.__jsonschema_ref_ex.match(v) <IF_STMT> refs.add(m.group(1)) continue elif k in ('oneOf', 'anyOf') and isinstance(v, list): refs.update(*map(self._get_schema_references, v)) refs.update(self._get_schema_references(v)) return refs",if m:
"def create_model_handler(ns, model_type):  @route(f'/<provider>/{ns}/<model_id>') @use_provider def handle(req, provider, model_id): if model_type == ModelType.user: if model_id == 'me': user = getattr(provider, '_user', None) <IF_STMT> raise CmdException(f'log in provider:{provider.identifier} first') return user model = get_model_or_raise(provider, model_type, model_id) return model",if user is None:
"def stream_read_bz2(ifh, ofh): """"""Uncompress bz2 compressed *ifh* into *ofh*"""""" decompressor = bz2.BZ2Decompressor() while True: buf = ifh.read(BUFSIZE) if not buf: break buf = decompressor.decompress(buf) <IF_STMT> ofh.write(buf) if decompressor.unused_data or ifh.read(1) != b'': raise CorruptedObjectError('Data after end of bz2 stream')",if buf:
"def copy_layer(layer, keep_bias=True, name_template=None, weights=None, reuse_symbolic_tensors=True, **kwargs): config = layer.get_config() if name_template is None: config['name'] = None else: config['name'] = name_template % config['name'] if keep_bias is False and config.get('use_bias', False): config['use_bias'] = False <IF_STMT> if reuse_symbolic_tensors: weights = layer.weights[:-1] else: weights = layer.get_weights()[:-1] return get_layer_from_config(layer, config, weights=weights, **kwargs)",if weights is None:
"def do_status(self, directory, path): with self._repo(directory) as repo: <IF_STMT> path = os.path.join(directory, path) statuses = repo.status(include=path, all=True) for status, paths in statuses: if paths: return self.statuses[status][0] return None else: resulting_status = 0 for status, paths in repo.status(all=True): if paths: resulting_status |= self.statuses[status][1] return self.repo_statuses_str[resulting_status]",if path:
"def close(self): if self.changed: save = EasyDialogs.AskYesNoCancel('Save window ""%s"" before closing?' % self.name, 1) if save > 0: self.menu_save() <IF_STMT> return if self.parent.active == self: self.parent.active = None self.parent.updatemenubar() del self.ted self.do_postclose()",elif save < 0:
"def _Return(self, t): self._fill('return ') if t.value: <IF_STMT> text = ', '.join([name.name for name in t.value.asList()]) self._write(text) else: self._dispatch(t.value) if not self._do_indent: self._write('; ')","if isinstance(t.value, Tuple):"
"def __init__(self, itemtype, cnf={}, *, master=None, **kw): if not master: if 'refwindow' in kw: master = kw['refwindow'] <IF_STMT> master = cnf['refwindow'] else: master = tkinter._default_root if not master: raise RuntimeError('Too early to create display style: no root window') self.tk = master.tk self.stylename = self.tk.call('tixDisplayStyle', itemtype, *self._options(cnf, kw))",elif 'refwindow' in cnf:
"def _load_items(self, splits): """"""Load individual image indices from splits."""""" ids = list() for name in splits: root = os.path.join(self._root, 'VisDrone2019-DET-' + name) images_dir = self._images_dir.format(root) images = [f[:-4] for f in os.listdir(images_dir) <IF_STMT>] ids += [(root, line.strip()) for line in images] return ids","if os.path.isfile(os.path.join(images_dir, f)) and f[-3:] == 'jpg'"
"def _gen_langs_in_db(self): for d in os.listdir(join(self.base_dir, 'db')): <IF_STMT> continue lang_path = join(self.base_dir, 'db', d, 'lang') if not exists(lang_path): log.warn(""unexpected lang-zone db dir without 'lang' file: `%s' (skipping)"" % dirname(lang_path)) continue fin = open(lang_path, 'r') try: lang = fin.read().strip() finally: fin.close() yield lang",if d in self._non_lang_db_dirs:
"def handler_click_link(self, link): if link.startswith('[['): link = link[2:-2] self.notify_observers('click:notelink', link) el<IF_STMT> os.startfile(link) elif platform.system().lower() == 'darwin': subprocess.call(('open', link)) else: subprocess.call(('xdg-open', link))",if platform.system().lower() == 'windows':
"def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None if isinstance(o, dict): name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, '__dict__', None) is o: o = r break <IF_STMT> name = web.dictfind(o, self.obj) if not isinstance(name, six.string_types): name = None d.append(Object(o, name)) return d","elif isinstance(o, dict):"
"def parse_preference(path): """"""parse android's shared preference xml"""""" storage = {} read = open(path) for line in read: line = line.strip() <IF_STMT> index = line.find('""', 14) key = line[14:index] value = line[index + 2:-9] storage[key] = value read.close() return storage","if line.startswith('<string name=""'):"
"def __getExpectedSampleOffsets(self, tileOrigin, area1, area2): ts = GafferImage.ImagePlug.tileSize() data = [] for y in range(tileOrigin.y, tileOrigin.y + ts): for x in range(tileOrigin.x, tileOrigin.x + ts): pixel = imath.V2i(x, y) data.append(data[-1] if data else 0) if GafferImage.BufferAlgo.contains(area1, pixel): data[-1] += 1 <IF_STMT> data[-1] += 1 return IECore.IntVectorData(data)","if GafferImage.BufferAlgo.contains(area2, pixel):"
"def test_doc_attributes(self): print_test_name('TEST DOC ATTRIBUTES') correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) <IF_STMT> correct += 1 if original_schema.type == 'record': for f in original_schema.fields: if f.doc is None: self.fail(""Failed to preserve 'doc' in fields: "" + example.schema_string) self.assertEqual(correct, len(DOC_EXAMPLES))",if original_schema.doc is not None:
"def enter(self, node, key, parent, path, ancestors): for i, visitor in enumerate(self.visitors): if not self.skipping[i]: result = visitor.enter(node, key, parent, path, ancestors) if result is False: self.skipping[i] = node <IF_STMT> self.skipping[i] = BREAK elif result is not None: return result",elif result is BREAK:
"def new_user_two_factor(): user = Journalist.query.get(request.args['uid']) if request.method == 'POST': token = request.form['token'] <IF_STMT> flash(gettext('Token in two-factor authentication accepted for user {user}.').format(user=user.username), 'notification') return redirect(url_for('admin.index')) else: flash(gettext('Could not verify token in two-factor authentication.'), 'error') return render_template('admin_new_user_two_factor.html', user=user)",if user.verify_token(token):
"def _check_locations(self, locations, available_locations): for location in locations: <IF_STMT> self.log.warning('List of supported locations for you is: %s', sorted(available_locations.keys())) raise TaurusConfigError('Invalid location requested: %s' % location)",if location not in available_locations:
"def find_best_layout_for_subplots(num_subplots): r, c = (1, 1) while r * c < num_subplots: if c == r + 1 or r == c: c += 1 <IF_STMT> r += 1 c -= 1 return (r, c)",elif c == r + 2:
"def check_env(env): for name, val in env.items(): if not isinstance(name, six.string_types): raise ValueError('non-string env name %r' % name) <IF_STMT> raise ValueError(""non-string env value for '%s': %r"" % (name, val))","if not isinstance(val, six.string_types):"
"def _indexes(self): indexes = [] names = ('index', 'columns') for ax in range(self.input.ndim): index = names[ax] val = getattr(self, index) <IF_STMT> indexes.append(val) else: indexes.append(slice(None)) return indexes",if val is not None:
"def gen(): for _ in range(256): if seq: yield self.tb.dut.i.eq(seq.pop(0)) i = (yield self.tb.dut.i) if (yield self.tb.dut.n): self.assertEqual(i, 0) else: o = (yield self.tb.dut.o) <IF_STMT> self.assertEqual(i & 1 << o - 1, 0) self.assertGreaterEqual(i, 1 << o) yield",if o > 0:
"def early_version(self, argv): if '--version' in argv: <IF_STMT> from flower.utils import bugreport print(bugreport(), file=self.stdout) print(__version__, file=self.stdout) super(FlowerCommand, self).early_version(argv)",if '--debug' in argv:
"def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: if not d.enabled: continue if key_len > d.longest_key: continue value = d.get(key) <IF_STMT> for f in filters: if f(key, value): return None return value",if value:
def get_lang3(lang): try: if len(lang) == 2: ret_value = get(part1=lang).part3 <IF_STMT> ret_value = lang else: ret_value = '' except KeyError: ret_value = lang return ret_value,elif len(lang) == 3:
"def get_config_settings(): config = {} for plugin in extension_loader.MANAGER.plugins: fn_name = plugin.name function = plugin.plugin if hasattr(function, '_takes_config'): fn_module = importlib.import_module(function.__module__) <IF_STMT> config[fn_name] = fn_module.gen_config(function._takes_config) return yaml.safe_dump(config, default_flow_style=False)","if hasattr(fn_module, 'gen_config'):"
"def _import_pathname(self, pathname, fqname): if _os_path_isdir(pathname): result = self._import_pathname(_os_path_join(pathname, '__init__'), fqname) <IF_STMT> values = result[2] values['__pkgdir__'] = pathname values['__path__'] = [pathname] return (1, result[1], values) return None for suffix, importFunc in self.suffixes: filename = pathname + suffix try: finfo = _os_stat(filename) except OSError: pass else: return importFunc(filename, finfo, fqname) return None",if result:
def __iter__(self): with self._guard: for dp in self.ds: shp = dp[self.idx].shape holder = self.holder[shp] holder.append(dp) <IF_STMT> yield BatchData.aggregate_batch(holder) del holder[:],if len(holder) == self.batch_size:
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: if module is None: module = self.name if section is None: section = 'all_sections' <IF_STMT> s_name = f['s_name'] if source is None: source = os.path.abspath(os.path.join(f['root'], f['fn'])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning('Tried to add data source for {}, but was missing fields data'.format(self.name))",if s_name is None:
"def forward(self, seq, adj, sparse=False): seq_fts = self.fc(seq) if len(seq_fts.shape) > 2: <IF_STMT> out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0) else: out = torch.bmm(adj, seq_fts) elif sparse: out = torch.spmm(adj, torch.squeeze(seq_fts, 0)) else: out = torch.mm(adj, seq_fts) if self.bias is not None: out += self.bias return self.act(out)",if sparse:
"def stat(self, path): """"""Get attributes of a file or directory, following symlinks"""""" try: return SFTPAttrs.from_local(super().stat(path)) except OSError as exc: <IF_STMT> raise SFTPError(FX_PERMISSION_DENIED, exc.strerror) else: raise SFTPError(FX_FAILURE, exc.strerror)",if exc.errno == errno.EACCES:
"def _run_eagerly(*inputs): with context.eager_mode(): constants = [_wrap_as_constant(value, tensor_spec) for value, tensor_spec in zip(inputs, input_signature)] output = fn(*constants) <IF_STMT> return output._make([tensor.numpy() for tensor in output]) if isinstance(output, (tuple, list)): return [tensor.numpy() for tensor in output] else: return output.numpy()","if hasattr(output, '_make'):"
"def do_draw(self, data): if cu.biased_coin(data, self.__p): return data.draw(self) + data.draw(self) else: n = data.draw_bits(16) << 16 | data.draw_bits(16) <IF_STMT> return (POISON,) else: return (None,)",if n == MAX_INT:
"def object_matches_a_check(obj, checks): """"""Does the object match *any* of the given checks from the ""only_cache_matching"" list?"""""" for check in checks: <IF_STMT> if check(obj): return True else: try: for field, value in check.items(): if not getattr(obj, field) == value: break else: return True except AttributeError: logger.error('Invalid filter for model %s, %s', obj.__class__, check) raise return False",if callable(check):
"def handle_edge(self, src_id, dst_id, attrs): try: pos = attrs['pos'] except KeyError: return points = self.parse_edge_pos(pos) shapes = [] for attr in ('_draw_', '_ldraw_', '_hdraw_', '_tdraw_', '_hldraw_', '_tldraw_'): <IF_STMT> parser = XDotAttrParser(self, attrs[attr]) shapes.extend(parser.parse()) if shapes: src = self.node_by_name[src_id] dst = self.node_by_name[dst_id] self.edges.append(elements.Edge(src, dst, points, shapes, attrs.get('tooltip')))",if attr in attrs:
"def get_available_data_asset_names(self): known_assets = [] if not os.path.isdir(self.base_directory): return {'names': [(asset, 'path') for asset in known_assets]} for data_asset_name in self.asset_globs.keys(): batch_paths = self._get_data_asset_paths(data_asset_name=data_asset_name) <IF_STMT> known_assets.append(data_asset_name) return {'names': [(asset, 'path') for asset in known_assets]}",if len(batch_paths) > 0 and data_asset_name not in known_assets:
"def _maintain_pool(self): waiting = self._docker_interface.services_waiting_by_constraints() active = self._docker_interface.nodes_active_by_constraints() for constraints, needed_dict in self._state.slots_needed(waiting, active).items(): services = needed_dict['services'] nodes = needed_dict['nodes'] slots_needed = needed_dict['slots_needed'] <IF_STMT> self._spawn_nodes(constraints, services, slots_needed) elif slots_needed < 0: self._destroy_nodes(constraints, nodes, slots_needed)",if slots_needed > 0:
def retention_validator(ns): if ns.backup_retention is not None: val = ns.backup_retention <IF_STMT> raise CLIError('incorrect usage: --backup-retention. Range is 7 to 35 days.'),if not 7 <= int(val) <= 35:
"def write(path, data, kind='OTHER', dohex=0): asserttype1(data) kind = string.upper(kind) try: os.remove(path) except os.error: pass err = 1 try: if kind == 'LWFN': writelwfn(path, data) <IF_STMT> writepfb(path, data) else: writeother(path, data, dohex) err = 0 finally: if err and (not DEBUG): try: os.remove(path) except os.error: pass",elif kind == 'PFB':
"def __init__(self, zone, poll_interval=1): self.zone = zone self.poll_interval = poll_interval self.queue_client = QueueClient(zone) self.shards = [] for database in config['DATABASE_HOSTS']: <IF_STMT> shard_ids = [shard['ID'] for shard in database['SHARDS']] self.shards.extend((shard_id for shard_id in shard_ids if shard_id in engine_manager.engines))",if database.get('ZONE') == self.zone:
"def _postprocess_message(self, msg): if msg['type'] != 'param': return event_dim = msg['kwargs'].get('event_dim') if event_dim is None: return for frame in msg['cond_indep_stack']: <IF_STMT> value = msg['value'] event_dim += value.unconstrained().dim() - value.dim() value.unconstrained()._pyro_dct_dim = frame.dim - event_dim return",if frame.name == self.name:
def RemoveIdleHandler(self): if self.idleHandlerSet: debug('Idle handler reset\n') <IF_STMT> debug('Error deleting idle handler\n') self.idleHandlerSet = 0,if win32ui.GetApp().DeleteIdleHandler(self.QueueIdleHandler) == 0:
"def folder_is_public(self, folder): for sub_folder in folder.folders: if not self.folder_is_public(sub_folder): return False for library_dataset in folder.datasets: ldda = library_dataset.library_dataset_dataset_association <IF_STMT> return False return True",if ldda and ldda.dataset and (not self.dataset_is_public(ldda.dataset)):
"def _error_check(self, command_response): error = command_response.get('error') if error: command = command_response.get('command') <IF_STMT> raise NXAPICommandError(command, error['data']['msg']) else: raise NXAPICommandError(command, 'Invalid command.')",if 'data' in error:
"def find_idx_impl(arr, idx): chunks = parallel_chunks(len(arr)) new_arr = [List.empty_list(types.int64) for i in range(len(chunks))] for i in prange(len(chunks)): chunk = chunks[i] for j in range(chunk.start, chunk.stop): <IF_STMT> new_arr[i].append(j) return new_arr",if arr[j] == idx:
"def _l2bytes(l): try: <IF_STMT> return bytes(l) raise NameError except NameError: return ''.join(map(chr, l))",if bytes is not str:
"def decode(self): while True: clock_pin, data_pin = self.wait({0: 'f'}) self.handle_bits(data_pin) <IF_STMT> clock_pin, data_pin = self.wait({0: 'r'}) self.handle_bits(data_pin)",if self.bitcount == 11:
"def letterrange(first, last, charset): for k in range(len(last)): for x in product(*[chain(charset)] * (k + 1)): result = ''.join(x) if first: <IF_STMT> continue else: first = None yield result if result == last: return",if first != result:
"def run(self): while not self._stop: for i in range(0, self._interval): time.sleep(1) if self._stop: self.__logger.debug('%s - ping thread stopped' % self.name) return ping = PingIqProtocolEntity() self._layer.waitPong(ping.getId()) <IF_STMT> self._layer.sendIq(ping)",if not self._stop:
"def __init__(self): self.converters = dict() for p in dir(self): attr = getattr(self, p) <IF_STMT> for p in attr._converter_for: self.converters[p] = attr","if hasattr(attr, '_converter_for'):"
def consume(self): if not self.inputState.guessing: c = self.LA(1) if self.caseSensitive: self.append(c) else: c = self.inputState.input.LA(1) self.append(c) <IF_STMT> self.tab() else: self.inputState.column += 1 self.inputState.input.consume(),if c and c in '\t':
"def _is_target_pattern_matched(self, pattern, targets): for target in targets: try: search_result = re.search(pattern, target) except: logger.warning(f'Illegal regular match in mock data!\n {traceback.format_exc()}') return False <IF_STMT> return False return True",if not search_result:
"def forwards(self, orm): from sentry.models import ProjectKey for project in orm['sentry.Project'].objects.all(): <IF_STMT> continue orm['sentry.ProjectKey'].objects.create(project=project, public_key=ProjectKey.generate_api_key(), secret_key=ProjectKey.generate_api_key())","if orm['sentry.ProjectKey'].objects.filter(project=project, user=None).exists():"
"def prepare_content_length(self, body): if hasattr(body, 'seek') and hasattr(body, 'tell'): curr_pos = body.tell() body.seek(0, 2) end_pos = body.tell() self.headers['Content-Length'] = builtin_str(max(0, end_pos - curr_pos)) body.seek(curr_pos, 0) elif body is not None: l = super_len(body) <IF_STMT> self.headers['Content-Length'] = builtin_str(l) elif self.method not in ('GET', 'HEAD') and self.headers.get('Content-Length') is None: self.headers['Content-Length'] = '0'",if l:
"def listdir(path='.'): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] <IF_STMT> good = fname != b'.' and fname == b'..' else: good = fname != '.' and fname != '..' if good: if not is_bytes: fname = fsdecode(fname) res.append(fname) return res",if is_bytes:
"def _validate_mappings(self): for m in self.mapping.mapping_rules: for policy_id in m.policy_ids: <IF_STMT> raise ReferencedObjectNotFoundError(reference_id=policy_id, reference_type='policy') for w in m.whitelist_ids: if w not in self.whitelists: raise ReferencedObjectNotFoundError(reference_id=w, reference_type='whitelist')",if policy_id not in self.policies:
"def sendall(self, data): len_data = len(data) os_write = os.write fileno = self._fileno try: total_sent = os_write(fileno, data) except OSError as e: <IF_STMT> raise IOError(*e.args) total_sent = 0 while total_sent < len_data: self._trampoline(self, write=True) try: total_sent += os_write(fileno, data[total_sent:]) except OSError as e: if get_errno(e) != errno.EAGAIN: raise IOError(*e.args)",if get_errno(e) != errno.EAGAIN:
"def dr_relation(self, C, trans, nullable): state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: <IF_STMT> a = p.prod[p.lr_index + 1] if a in self.grammar.Terminals: if a not in terms: terms.append(a) if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append('$end') return terms",if p.lr_index < p.len - 1:
"def canonical_standard_headers(self, headers): interesting_headers = ['content-md5', 'content-type', 'date'] hoi = [] if 'Date' in headers: del headers['Date'] headers['Date'] = self._get_date() for ih in interesting_headers: found = False for key in headers: lk = key.lower() <IF_STMT> hoi.append(headers[key].strip()) found = True if not found: hoi.append('') return '\n'.join(hoi)",if headers[key] is not None and lk == ih:
"def _fatal_error(self, exc, message='Fatal error on pipe transport'): try: if isinstance(exc, OSError): <IF_STMT> logger.debug('%r: %s', self, message, exc_info=True) else: self._loop.call_exception_handler({'message': message, 'exception': exc, 'transport': self, 'protocol': self._protocol}) finally: self._force_close(exc)",if self._loop.get_debug():
"def match_empty(self, el): """"""Check if element is empty (if requested)."""""" is_empty = True for child in self.get_children(el, tags=False): <IF_STMT> is_empty = False break elif self.is_content_string(child) and RE_NOT_EMPTY.search(child): is_empty = False break return is_empty",if self.is_tag(child):
"def _sortNodes(self, nodes, sortBy, sortDir, force=False): if force or self._sortedBy != sortBy or self._sortDir != sortDir: log.debug('KPFTree::_sortNodes()') <IF_STMT> nodes.sort(lambda a, b: compareNodeFolder(a, b, sortBy, sortDir)) else: nodes.sort(lambda a, b: compareNode(a, b, sortBy)) self._sortDir = sortDir self._sortedBy = sortBy else: log.debug('KPFTree::_sortNodes:: already sorted')",if sortDir != 0:
"def log(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app['websockets'].add(web_socket) try: async for msg in web_socket: <IF_STMT> if msg.data == 'close': await web_socket.close() elif msg.type == WSMsgType.ERROR: print('web socket connection closed with exception %s' % web_socket.exception()) finally: self.app['websockets'].remove(web_socket) return web_socket",if msg.type == WSMsgType.TEXT:
"def analyze_items(items, category_id, agg_data): for item in items: if not agg_data['cat_asp'].get(category_id, None): agg_data['cat_asp'][category_id] = [] agg_data['cat_asp'][category_id].append(float(item.sellingStatus.currentPrice.value)) <IF_STMT> agg_data['watch_count'] += int(item.listingInfo.watchCount) if getattr(item, 'postalCode', None): agg_data['postal_code'] = item.postalCode","if getattr(item.listingInfo, 'watchCount', None):"
"def __init__(self, filename, metadata_name, metadata_column, message='Value for metadata not found.', line_startswith=None, split='\t'): self.metadata_name = metadata_name self.message = message self.valid_values = [] for line in open(filename): <IF_STMT> fields = line.split(split) if metadata_column < len(fields): self.valid_values.append(fields[metadata_column].strip())",if line_startswith is None or line.startswith(line_startswith):
"def iter_flat(self): for f in self.layout: e = getattr(self, f[0]) if isinstance(e, Signal): <IF_STMT> yield (e, f[2]) else: yield (e, DIR_NONE) elif isinstance(e, Record): yield from e.iter_flat() else: raise TypeError",if len(f) == 3:
"def shell(self, cmd): if self._debug: logger.log(cmd) if is_sequence(cmd): cmd = ''.join(cmd) if self._log: <IF_STMT> cmd = ""(%s) 2>&1 | tee '%s'"" % (cmd, self._log) else: cmd = ""(%s) >> '%s' 2>&1"" % (cmd, self._log) returncode = subprocess.call(cmd, shell=True, cwd=self._cwd) if returncode: raise ShellCommandException('%s: failed to `%s`' % (returncode, cmd))",if self._verbose:
"def _to_sentences(self, lines): text = '' sentence_objects = [] for line in lines: if isinstance(line, Sentence): <IF_STMT> sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) sentence_objects.append(line) text = '' else: text += ' ' + line text = text.strip() if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) return sentence_objects",if text:
"def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: if hasattr(field, 'attname'): <IF_STMT> continue elif field.attname.endswith('ptr_id'): continue if getattr(field, 'editable', True): fds.add(field.attname) return fds",if field.attname == 'id':
"def get_router_id(path, local_bgp_id): path_source = path.source if path_source is None: return local_bgp_id else: originator_id = path.get_pattr(BGP_ATTR_TYPE_ORIGINATOR_ID) <IF_STMT> return originator_id.value return path_source.protocol.recv_open_msg.bgp_identifier",if originator_id:
"def visit_SelectionSetNode(self, node): elements = [] for sel in node.selections: if not self._should_include(sel.directives): continue spec = self.visit(sel) <IF_STMT> elements.append(spec) elements = self.combine_field_results(elements) return elements",if spec is not None:
"def update_groups_of_conv(self): for op in self.ops(): <IF_STMT> op.set_attr('groups', op.inputs('Filter')[0].shape()[0])",if op.type() == 'depthwise_conv2d' or op.type() == 'depthwise_conv2d_grad':
"def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int): self.constraint_states = [] for constraint_tensor in batch_constraints: if self.representation == 'ordered': constraint_state = OrderedConstraintState.create(constraint_tensor) <IF_STMT> constraint_state = UnorderedConstraintState.create(constraint_tensor) self.constraint_states.append([constraint_state for i in range(beam_size)])",elif self.representation == 'unordered':
def startInputThread(self): global input try: input = raw_input except NameError: pass while True: cmd = self._queuedCmds.pop(0) <IF_STMT> else input(self.getPrompt()).strip() wait = self.execCmd(cmd) if wait: self.acceptingInput = False self.blockingQueue.get(True) self.acceptingInput = True,if len(self._queuedCmds)
"def apply_list(self, expr, rules, evaluation): """"""ReplaceRepeated[expr_, rules_]"""""" try: rules, ret = create_rules(rules, expr, 'ReplaceRepeated', evaluation) except PatternError: evaluation.message('Replace', 'reps', rules) return None if ret: return rules while True: evaluation.check_stopped() result, applied = expr.apply_rules(rules, evaluation) if applied: result = result.evaluate(evaluation) <IF_STMT> expr = result else: break return result",if applied and (not result.same(expr)):
"def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs): if not dnn_available(ctx_name): return ins = [] for n in inputs: n = as_gpuarray_variable(n, ctx_name) <IF_STMT> return ins.append(n.dimshuffle(0, 'x', 1, 'x')) out = GpuDnnSoftmaxGrad('accurate', 'instance')(gpu_contiguous(ins[0]), gpu_contiguous(ins[1])) return [out.dimshuffle(0, 2)]",if n.ndim != 2:
"def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): if hasattr(field, 'document_type'): field_cls = field.document_type if field_cls in inspected: continue <IF_STMT> geo_indices += field_cls._geo_indices(inspected) elif field._geo_index: geo_indices.append(field) return geo_indices","if hasattr(field_cls, '_geo_indices'):"
"def get_skip_list(self, handle): todo = [handle] skip = [handle] while todo: handle = todo.pop() for child in self.dbstate.db.find_backlink_handles(handle, ['Place']): <IF_STMT> todo.append(child[1]) skip.append(child[1]) return skip",if child[1] not in skip:
"def convertstore(self, inputstore, includefuzzy=False): """"""converts a file to .lang format"""""" thetargetfile = lang.LangStore(mark_active=self.mark_active) for pounit in inputstore.units: <IF_STMT> continue newunit = thetargetfile.addsourceunit(pounit.source) if includefuzzy or not pounit.isfuzzy(): newunit.settarget(pounit.target) else: newunit.settarget('') if pounit.getnotes('developer'): newunit.addnote(pounit.getnotes('developer'), 'developer') return thetargetfile",if pounit.isheader() or not pounit.istranslatable():
"def api_read(self): files = [] files.append('/bin/netcat') files.append('/etc/alternative/netcat') files.append('/bin/nc') installed = False support = False path = None for _file in files: file_content = self.shell.read(_file) <IF_STMT> installed = True path = _file if '-e filename' in file_content: support = True break result = {'netcat_installed': installed, 'supports_shell_bind': support, 'path': path} return result",if file_content:
"def _create_waiter(self, func_name): if self._waiter is not None: <IF_STMT> if not self._waiter.done(): raise RuntimeError('%s() called while connection is being cancelled' % func_name) else: raise RuntimeError('%s() called while another coroutine is already waiting for incoming data' % func_name) self._waiter = create_future(self._loop) return self._waiter",if self._cancelling:
"def calculate(self): addr_space = utils.load_as(self._config) for mod in modules.lsmod(addr_space): <IF_STMT> continue for offset, password in self.scan_module(addr_space, mod.DllBase, self._config.MIN_LENGTH): yield (offset, password)",if str(mod.BaseDllName).lower() != 'truecrypt.sys':
"def on_touch_up(self, touch): try: if not self.h_picker_touch: return <IF_STMT> if touch.grab_current is not self: if self.picker == 'hours': self.picker = 'minutes' except AttributeError: pass super().on_touch_up(touch)",if not self.animating:
"def handle(self, *args, **options): dry_run = options.get('dry_run', False) state = options.get('state', None) if not dry_run: script_utils.add_file_logger(logger, __file__) with transaction.atomic(): add_reviews_notification_setting(notification_type=options['notification'], state=state) <IF_STMT> raise RuntimeError('Dry run, transaction rolled back.')",if dry_run:
"def __call__(self, es, params): ops = 0 indices = mandatory(params, 'indices', self) only_if_exists = params.get('only-if-exists', False) request_params = params.get('request-params', {}) for index_name in indices: <IF_STMT> es.indices.delete(index=index_name, params=request_params) ops += 1 elif only_if_exists and es.indices.exists(index=index_name): self.logger.info('Index [%s] already exists. Deleting it.', index_name) es.indices.delete(index=index_name, params=request_params) ops += 1 return (ops, 'ops')",if not only_if_exists:
"def find_first_of_filetype(content, filterfiltype, attr='name'): """"""Find the first of the file type."""""" filename = '' for _filename in content: if isinstance(_filename, str): <IF_STMT> filename = _filename break elif getattr(_filename, attr).endswith(f'.{filterfiltype}'): filename = getattr(_filename, attr) break return filename",if _filename.endswith(f'.{filterfiltype}'):
"def join(s, *p): path = s for t in p: if not s or isabs(t): path = t continue if t[:1] == ':': t = t[1:] <IF_STMT> path = ':' + path if path[-1:] != ':': path = path + ':' path = path + t return path",if ':' not in path:
"def cell_double_clicked(self, row, column): if column == 3: archive_name = self.selected_archive_name() if not archive_name: return mount_point = self.mount_points.get(archive_name) <IF_STMT> QDesktopServices.openUrl(QtCore.QUrl(f'file:///{mount_point}'))",if mount_point is not None:
"def parseLink(line): parts = line.split() optional = parts[0] == 'Link*:' assert optional or parts[0] == 'Link:' attrs = {} for attr in parts[1:]: k, v = attr.split('=', 1) <IF_STMT> attr_optional = 1 k = k[:-1] else: attr_optional = 0 attrs[k] = (attr_optional, v) return (optional, attrs)",if k[-1] == '*':
"def should_wait(self, offer_hash: str): with self._lock: <IF_STMT> if self._offer_hash != offer_hash: logger.debug('already processing another offer (%s vs %s)', self._offer_hash, offer_hash) return True if self._started == self._wtct_num_subtasks: logger.info('all subtasks for `%s` have been started', self._offer_hash) return True return False",if self._offer_hash is not None:
"def list_urls(self): for idx, job in enumerate(self.urlwatcher.jobs): if self.urlwatch_config.verbose: print('%d: %s' % (idx + 1, repr(job))) else: pretty_name = job.pretty_name() location = job.get_location() <IF_STMT> print('%d: %s ( %s )' % (idx + 1, pretty_name, location)) else: print('%d: %s' % (idx + 1, pretty_name)) return 0",if pretty_name != location:
"def _encode_realm(self, realm): <IF_STMT> realm = self.default_realm if realm is None: raise TypeError('you must specify a realm explicitly, or set the default_realm attribute') return self._encode_field(realm, 'realm')",if realm is None:
"def set(sensor_spec: dict, **kwargs): for key, value in kwargs.items(): if key == 'position': sensor_spec['transform'] = SensorSpecs.get_position(value) elif key == 'attachment_type': sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value] <IF_STMT> sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]",elif key == 'color_converter':
"def _check_arguments(self, arch, state): cc = DEFAULT_CC[arch.name](arch) for i, expected_arg in enumerate(self.arguments): if expected_arg is None: continue real_arg = cc.arg(state, i) expected_arg_type, expected_arg_value = expected_arg r = self._compare_arguments(state, expected_arg_type, expected_arg_value, real_arg) <IF_STMT> return False return True",if not r:
"def all_projects(): if not REPODIR: return excludes = set(['tempest', 'requirements']) for name in PROJECTS: name = name.strip() short_name = name.split('/')[-1] try: with open(os.path.join(REPODIR, short_name, 'setup.py'), 'rt') as f: <IF_STMT> continue except IOError: continue if short_name in excludes: continue yield (short_name, dict(name=name, short_name=short_name))",if 'pbr' not in f.read():
"def get_converter(self, key, default=None): """"""Gets a converter for the given key."""""" if key in self._vars: return self._vars[key].convert for k, var in self._vars.items(): <IF_STMT> continue if k.match(key) is not None: converter = var.convert self._vars[key] = var break else: converter = self._get_default_converter(default=default) return converter","if isinstance(k, str):"
"def get_artist(self, name): artist = self.artists.get(name) if not artist: if self.use_db: try: artist = q(m.Artist).filter_by(name=name).one() except NoResultFound: pass <IF_STMT> self.add_artist(artist) return artist",if artist and self.ram_cache:
"def move(self, x, y): offset = self.h width = max((len(val.get()) for val in self.values)) for i, label in enumerate(self.labels): <IF_STMT> label.place(x=x, y=y + offset) label.config(width=width, bg=self.fg if self.selected == i else self.bg) offset += self.h + self.pady * 2 else: label.place(x=9999, y=9999) return",if self.values[i].get() != '':
"def visit_Assign(self, node): if len(node.targets) == 1: <IF_STMT> plugPath = self.__plugPath(self.__path(node.targets[0])) if plugPath: self.plugWrites.add(plugPath) self.visit(node.value)","if isinstance(node.targets[0], ast.Subscript):"
"def _minimal_replacement_cost(self, first, second): first_symbols, second_symbols = (set(), set()) removal_cost, insertion_cost = (0, 0) for a, b in itertools.zip_longest(first, second, fillvalue=None): <IF_STMT> first_symbols.add(a) if b is not None: second_symbols.add(b) removal_cost = max(removal_cost, len(first_symbols - second_symbols)) insertion_cost = max(insertion_cost, len(second_symbols - first_symbols)) return min(removal_cost, insertion_cost)",if a is not None:
"def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: if system.NUMBER_KEY in letters: stroke = stroke.replace(system.NUMBER_KEY, '') m = _IMPLICIT_NUMBER_RX.search(stroke) <IF_STMT> start = m.start(2) return stroke[:start] + '-' + stroke[start:] if '-' in letters: if stroke.endswith('-'): stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace('-', '') return stroke",if m is not None:
"def serve_json(self, args=None): request = current.request response = current.response response.headers['Content-Type'] = 'application/json; charset=utf-8' if not args: args = request.args d = dict(request.vars) if args and args[0] in self.json_procedures: s = self.call_service_function(self.json_procedures[args[0]], *args[1:], **d) <IF_STMT> s = s.as_list() return response.json(s) self.error()","if hasattr(s, 'as_list'):"
"def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): if line == '': continue elif line.startswith('@highlight'): next_is_highlight = True <IF_STMT> highlights.append(line) else: article_lines.append(line) article = ' '.join(article_lines) abstract = ' '.join(highlights) return (article, abstract)",elif next_is_highlight:
"def _get_commands(): proc = Popen(['react-native', '--help'], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() <IF_STMT> continue if 'Commands:' in line: should_yield = True continue if should_yield: yield line.split(' ')[0]",if not line:
"def _wait_for_state(self, server_id, state, retries=50): for i in (0, retries): server = self.ex_get_server(server_id) <IF_STMT> return sleep(5) if i == retries: raise Exception('Retries count reached')",if server.extra['status']['state'] == state:
def add_letter(inner_letter): if inner_letter in alphabet: wordTrans.append(alphabet[inner_letter]) else: l2 = stringTools.stripAccents(inner_letter) <IF_STMT> raise KeyError('Cannot translate ' + inner_letter) wordTrans.append(alphabet['^']) wordTrans.append(alphabet[l2]),if l2 == inner_letter:
"def _parse_message(data): try: jsonrpc_message = json.loads(data, encoding='utf-8') if jsonrpc_message.get('jsonrpc') != '2.0': raise InvalidRequest() del jsonrpc_message['jsonrpc'] <IF_STMT> return Response(**jsonrpc_message) else: return Request(**jsonrpc_message) except json.JSONDecodeError: raise ParseError() except TypeError: raise InvalidRequest()",if 'result' in jsonrpc_message.keys() or 'error' in jsonrpc_message.keys():
"def get_buildings_in_range(self): buildings = self.settlement.buildings for building in buildings: <IF_STMT> continue if distances.distance_rect_rect(self.position, building.position) <= self.radius: yield building",if building is self:
"def get_tab_title(self, uuid=None): """"""Return the title of a parent tab of a given terminal"""""" maker = Factory() terminal = self.terminator.find_terminal_by_uuid(uuid) window = terminal.get_toplevel() root_widget = window.get_children()[0] if maker.isinstance(root_widget, 'Notebook'): for tab_child in root_widget.get_children(): terms = [tab_child] <IF_STMT> terms = enumerate_descendants(tab_child)[1] if terminal in terms: return root_widget.get_tab_label(tab_child).get_label()","if not maker.isinstance(terms[0], 'Terminal'):"
def is_valid_origin(origin): if not settings.SENTRY_ALLOW_ORIGIN: return False if settings.SENTRY_ALLOW_ORIGIN == '*': return True if not origin: return False origin = origin.lower() for value in settings.SENTRY_ALLOW_ORIGIN: <IF_STMT> if value.lower() == origin: return True elif value.match(origin): return True return False,"if isinstance(value, string_types):"
def addr_func(ctx): nodes = ctx.xpathEval(base_xpath + '/ip') nodes = nodes or [] ret = [] for node in nodes: addr = node.prop('address') pref = node.prop('prefix') if not addr: continue <IF_STMT> addr += '/%s' % pref ret.append(addr) return ret,if pref:
"def _select_delete(self, select, args, row_index=0, arg_index=0): count = 0 delete = 'DELETE FROM Cache WHERE rowid IN (%s)' try: while True: with self._transact() as (sql, cleanup): rows = sql(select, args).fetchall() <IF_STMT> break count += len(rows) sql(delete % ','.join((str(row[0]) for row in rows))) for row in rows: args[arg_index] = row[row_index] cleanup(row[-1]) except Timeout: raise Timeout(count) return count",if not rows:
"def _set_checkpointer(self, train_config): if train_config['checkpoint']: checkpoint_config = train_config['checkpoint_config'] checkpoint_metric = checkpoint_config['checkpoint_metric'] <IF_STMT> checkpoint_config['checkpoint_metric'] = f'valid/{checkpoint_metric}' self.checkpointer = Checkpointer(checkpoint_config, verbose=self.config['verbose']) else: self.checkpointer = None",if checkpoint_metric.count('/') == 0:
def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split('.') test_ver = test_version.split('.') if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: if runtime_ver[1] > test_ver[1]: return True <IF_STMT> if runtime_ver[2] > test_ver[2]: return True return False,elif runtime_ver[1] == test_ver[1]:
"def generate_scraper_test(class_name, host_name): with open('templates/test_scraper.py') as source: code = source.read() program = ast.parse(code) state = GenerateTestScraperState(class_name, host_name, code) for node in ast.walk(program): <IF_STMT> break output = f'tests/test_{class_name.lower()}.py' with open(output, 'w') as target: target.write(state.result())",if not state.step(node):
"def _init_fetches(self): futures = [] for node_id, request in six.iteritems(self._create_fetch_requests()): <IF_STMT> log.debug('Sending FetchRequest to node %s', node_id) future = self._client.send(node_id, request) future.add_callback(self._handle_fetch_response, request) future.add_errback(log.error, 'Fetch to node %s failed: %s', node_id) futures.append(future) self._fetch_futures.extend(futures) self._clean_done_fetch_futures() return futures",if self._client.ready(node_id):
"def discover(cls, path, **kwargs): if kwargs.pop('collection', None) is not None: raise TypeError('collection argument must not be given.') path = expand_path(path) try: collections = os.listdir(path) except OSError as e: if e.errno != errno.ENOENT: raise else: for collection in collections: collection_path = os.path.join(path, collection) <IF_STMT> continue args = dict(collection=collection, path=collection_path, **kwargs) yield args",if not cls._validate_collection(collection_path):
"def writefile(filename, now): with open(os.path.join('src/teensy/' + filename)) as fileopen, open(os.path.join(core.userconfigpath, 'reports/teensy_{0}.ino'.format(now)), 'w') as filewrite: for line in fileopen: match = re.search('IPADDR', line) <IF_STMT> line = line.replace('IPADDR', ipaddr) match = re.search('12,12,12,12', line) if match: ipaddr_replace = ipaddr.replace('.', ',', 4) line = line.replace('12,12,12,12', ipaddr_replace) filewrite.write(line)",if match:
"def get_added_files(diff): """"""hacky approach to extract added files from github diff output"""""" prefix = '+++ b/' lastline = None for line in diff.splitlines(): line = line.strip() <IF_STMT> yield line[len(prefix):] lastline = line",if line.startswith(prefix) and lastline and (lastline == '--- /dev/null'):
def bpe_decode(tokens: List[str]) -> List[str]: words = [] pieces: List[str] = [] for t in tokens: <IF_STMT> pieces.append(t[:-2]) else: words.append(''.join(pieces + [t])) pieces = [] if len(pieces) > 0: words.append(''.join(pieces)) return words,if t.endswith(DecodeMixin.bpe_cont_str):
"def _maybe_encrypt(self, data): gpgr = self.config.prefs.gpg_recipient tokeys = [gpgr] if gpgr not in (None, '', '!CREATE', '!PASSWORD') else None if self.config.get_master_key(): with EncryptingStreamer(self.config.get_master_key(), delimited=True) as es: es.write(data) es.finish() return es.save(None) elif tokeys: stat, edata = GnuPG(self.config, event=GetThreadEvent()).encrypt(data, tokeys=tokeys) <IF_STMT> return edata return data",if stat == 0:
"def faces_uvs_list(self) -> List[torch.Tensor]: if self._faces_uvs_list is None: <IF_STMT> self._faces_uvs_list = [torch.empty((0, 3), dtype=torch.float32, device=self.device)] * self._N else: self._faces_uvs_list = padded_to_list(self._faces_uvs_padded, split_size=self._num_faces_per_mesh) return self._faces_uvs_list",if self.isempty():
"def handle_resource_click(self, widget, event): if event.getButton() == fife.MouseEvent.LEFT: self.show_resource_menu(widget.parent, widget.parent.parent) elif event.getButton() == fife.MouseEvent.RIGHT: <IF_STMT> self.hide_resource_menu() else: self.add_resource(slot=widget.parent, res_id=0, entry=widget.parent.parent)",if self.resource_menu_shown:
"def update_device(self, device): for bridge in self.bridges: if bridge.device == device: <IF_STMT> bridge.device.ip = device.ip bridge.device.port = device.port logger.info('Updated device ""{}"" - New settings: {}:{}'.format(device.label, device.ip, device.port)) self.update() self.share_bridges() break",if bridge.device.ip != device.ip or bridge.device.port != device.port:
"def endElement(self, name, value, connection): if name == 'OptionGroupName': self.name = value elif name == 'EngineName': self.engine_name = value elif name == 'MajorEngineVersion': self.major_engine_version = value elif name == 'OptionGroupDescription': self.description = value elif name == 'AllowsVpcAndNonVpcInstanceMemberships': <IF_STMT> self.allow_both_vpc_and_nonvpc = True else: self.allow_both_vpc_and_nonvpc = False elif name == 'VpcId': self.vpc_id = value else: setattr(self, name, value)",if value.lower() == 'true':
"def log_items(self, interface, action, media, items): if not items: return for item in items: if not item: continue log.info('[%s:%s](%s) %r (%r)', interface, action, media, item.get('title'), item.get('year')) <IF_STMT> self.log_episodes(item)",if media == 'shows':
"def _copy_files(self, files, src, dest, message=''): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) if message: print('{}: {}'.format(message, destpath)) <IF_STMT> destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) elif os.path.exists(destpath): os.remove(destpath)",if os.path.exists(srcpath):
"def disconnect(self, endpoint=None): if endpoint is not None: conn = self.connections_endpoints.pop(endpoint, None) <IF_STMT> self.connections.pop(conn.get_socket_object(), None) conn.close() else: for _, conn in self.connections_endpoints.items(): conn.close() self.connections_endpoints = {} self.connections = {}",if conn:
"def cisco_inventory(raw): for match in INVENTORY_RE.finditer(raw): d = match.groupdict() <IF_STMT> d['sn'] = None d['descr'] = d['descr'].strip('""') d['name'] = d['name'].strip('""') yield d",if d['sn'] in SERIAL_BLACKLIST:
"def _dispatchBubblingEvent(self, tag, evtType, evtObject): for node in tag.parents: if node is None: break if not node._listeners: continue <IF_STMT> continue capture_listeners, bubbling_listeners = self._get_listeners(node, evtType) for c in bubbling_listeners: evtObject.currentTarget = node._node self.do_dispatch(c, evtObject)",if evtObject._stoppedPropagation:
"def got_shares(self, shares): if self.check_reneging: <IF_STMT> self.finished_d.errback(unittest.FailTest('The node was told by the share finder that it is destined to remain hungry, then was given another share.')) return self.got += len(shares) log.msg('yyy 3 %s.got_shares(%s) got: %s' % (self, shares, self.got)) if self.got == 3: self.finished_d.callback(True)",if self._no_more_shares:
"def get_class_obj_(self, node, default_class=None): class_obj1 = default_class if 'xsi' in node.nsmap: classname = node.get('{%s}type' % node.nsmap['xsi']) if classname is not None: names = classname.split(':') <IF_STMT> classname = names[1] class_obj2 = globals().get(classname) if class_obj2 is not None: class_obj1 = class_obj2 return class_obj1",if len(names) == 2:
"def update(self, mapping, update_only=False): for name in mapping: <IF_STMT> if hasattr(self[name], 'update'): self[name].update(mapping[name], update_only) continue self.field(name, mapping[name]) if update_only: for name in mapping._meta: if name not in self._meta: self._meta[name] = mapping._meta[name] else: self._meta.update(mapping._meta)",if update_only and name in self:
"def configure(self): super(Command, self).configure() if self.needs_config and (not self.resolver): <IF_STMT> self.add_option('config', 'c', InputOption.VALUE_REQUIRED, 'The config file path')",if not self._check_config():
"def is_metric(cls, key_type, comparator): if key_type == cls._METRIC_IDENTIFIER: <IF_STMT> raise MlflowException(""Invalid comparator '%s' not one of '%s"" % (comparator, cls.VALID_METRIC_COMPARATORS), error_code=INVALID_PARAMETER_VALUE) return True return False",if comparator not in cls.VALID_METRIC_COMPARATORS:
"def get_full_qualified_name(self, node: Element) -> str: if node.get('reftype') == 'option': progname = node.get('std:program') command = ws_re.split(node.get('reftarget')) if progname: command.insert(0, progname) option = command.pop() <IF_STMT> return '.'.join(['-'.join(command), option]) else: return None else: return None",if command:
"def log_unsupported(logger, message, dictionary): if len(dictionary) < 1: return logger.info(message, len(dictionary)) for service in dictionary.keys(): <IF_STMT> logger.info('Ignoring service: %s' % service) continue logger.warn('Unsupported service: %s' % service, extra={'event': {'module': __name__, 'name': 'unsupported_service', 'key': service}})",if service is None or service in IGNORED_SERVICES:
"def encode_password(pw): """"""Encode password in hexadecimal if needed"""""" enc = False if pw: encPW = __PW_PREFIX for c in pw: cnum = ord(c) if c == '#' or cnum < 33 or cnum > 126: enc = True encPW += '%2x' % cnum <IF_STMT> return encPW return pw",if enc:
"def matrix_min_and_max(matrix): _min = None _max = None for row in matrix: for el in row: val = el if _min is None or val < _min: _min = val <IF_STMT> _max = val return (_min, _max)",if _max is None or val > _max:
"def __init__(self, content=None, parent=None): Transformable.__init__(self, content, parent) self._items = [] for element in content: <IF_STMT> continue tag = element.tag[len(namespace):] if tag == 'g': item = Group(element, self) elif tag == 'path': item = Path(element, self) else: log.warn('Unhandled SVG tag (%s)' % tag) continue self._items.append(item)",if not element.tag.startswith(namespace):
def reset_appid(self): with self.lock: self.working_appid_list = list() for appid in self.config.GAE_APPIDS: <IF_STMT> self.config.GAE_APPIDS.remove(appid) continue self.working_appid_list.append(appid) self.not_exist_appids = [] self.out_of_quota_appids = [] self.last_reset_time = time.time(),if not appid:
"def find_widget(self, pos): for widget in self.subwidgets[::-1]: if widget.visible: r = widget.rect <IF_STMT> return widget.find_widget(subtract(pos, r.topleft)) return self",if r.collidepoint(pos):
"def _get_names(dirs): """"""Get alphabet and label names, union across all dirs."""""" alphabets = set() label_names = {} for d in dirs: for example in _walk_omniglot_dir(d): alphabet, alphabet_char_id, label, _, _ = example alphabets.add(alphabet) label_name = '%s_%d' % (alphabet, alphabet_char_id) <IF_STMT> assert label_names[label] == label_name else: label_names[label] = label_name label_names = [label_names[k] for k in sorted(label_names)] return (alphabets, label_names)",if label in label_names:
"def model(): with pyro.plate_stack('plates', shape): with pyro.plate('particles', 200000): <IF_STMT> pyro.sample('x', dist.Normal(loc, scale)) else: pyro.sample('x', dist.StudentT(10.0, loc, scale))",if 'dist_type' == 'Normal':
"def set_note_pinned(self, key, pinned): n = self.notes[key] old_pinned = utils.note_pinned(n) if pinned != old_pinned: if 'systemtags' not in n: n['systemtags'] = [] systemtags = n['systemtags'] <IF_STMT> systemtags.append('pinned') else: systemtags.remove('pinned') n['modifydate'] = time.time() self.notify_observers('change:note-status', events.NoteStatusChangedEvent(what='modifydate', key=key))",if pinned:
"def __init__(self, name, contents): self.name = name self.all_entries = [] self.attr = [] self.child = [] self.seq_child = [] for entry in contents: clean_entry = entry.rstrip('*') self.all_entries.append(clean_entry) if entry.endswith('**'): self.seq_child.append(clean_entry) <IF_STMT> self.child.append(clean_entry) else: self.attr.append(entry)",elif entry.endswith('*'):
"def testToFileBinary(self): z = dns.zone.from_file(here('example'), 'example') try: f = open(here('example3-binary.out'), 'wb') z.to_file(f) f.close() ok = filecmp.cmp(here('example3-binary.out'), here('example3.good')) finally: <IF_STMT> os.unlink(here('example3-binary.out')) self.failUnless(ok)",if not _keep_output:
"def test_collect_gradients_with_allreduce_failure_case(self): worker = self._workers[1] train_db, _ = get_mnist_dataset(self._batch_size) for step, (x, y) in enumerate(train_db): if step == 0: worker._run_model_call_before_training(x) <IF_STMT> break self.assertEqual(worker._calculate_grads_and_report_with_allreduce(None), False, 'Should fail when no data is received')",if step == self._test_steps:
"def clean(self): data = self.cleaned_data number, ccv = (data.get('number'), data.get('ccv')) if number and ccv: <IF_STMT> raise forms.ValidationError(_('American Express cards use a 4 digit security code')) return data",if bankcards.is_amex(number) and len(ccv) != 4:
"def _gen_GreaterEqual(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): <IF_STMT> result.append(self.builder.fcmp_ordered('>=', lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed('>=', lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)",if ret_type == real_type:
"def console_get(context, console_id, instance_id=None): query = model_query(context, models.Console, read_deleted='yes').filter_by(id=console_id).options(joinedload('pool')) if instance_id is not None: query = query.filter_by(instance_id=instance_id) result = query.first() if not result: <IF_STMT> raise exception.ConsoleNotFoundForInstance(console_id=console_id, instance_id=instance_id) else: raise exception.ConsoleNotFound(console_id=console_id) return result",if instance_id:
"def publish(): pub = await aioredis.create_redis(('localhost', 6379)) while not tsk.done(): while True: subs = await pub.pubsub_numsub('channel:1') <IF_STMT> break await asyncio.sleep(0, loop=loop) for msg in ['one', 'two', 'three']: await pub.publish('channel:1', msg) await pub.publish('channel:1', STOPWORD) pub.close() await pub.wait_closed()",if subs[b'channel:1'] == 1:
"def read(self, size=None): if not size: size = self._size contents = BytesIO() while True: blocks = GzipFile.read(self, size) <IF_STMT> contents.flush() break contents.write(blocks) return contents.getvalue() else: return GzipFile.read(self, size)",if not blocks:
"def i2repr(self, pkt, x): if type(x) is list or type(x) is tuple: return repr(x) if self.multi: r = [] else: r = '' i = 0 while x: <IF_STMT> if self.multi: r += [self.names[i]] else: r += self.names[i] i += 1 x >>= 1 if self.multi: r = '+'.join(r) return r",if x & 1:
"def _run(self): while not self.stopped: with self.lock: started = time.time() try: self.bus.send(self.message) except Exception as exc: log.exception(exc) break <IF_STMT> break delay = self.period - (time.time() - started) time.sleep(max(0.0, delay))",if self.end_time is not None and time.time() >= self.end_time:
"def currentLevel(self): currentStr = '' for stackType, stackValue in self.stackVals: if stackType == 'dict': <IF_STMT> currentStr += ""['"" + stackValue + ""']"" else: currentStr += '[' + str(stackValue) + ']' elif stackType == 'listLike': currentStr += '[' + str(stackValue) + ']' elif stackType == 'getattr': currentStr += "".__getattribute__('"" + stackValue + ""')"" else: raise Exception(f'Cannot get attribute of type {stackType}') return currentStr","if isinstance(stackValue, str):"
"def restoreParent(self): if self.sid.isRoot: return with self.suspendMouseButtonNavigation(): confirm, opt = self.confirmRestore((self.path,)) <IF_STMT> return if opt['delete'] and (not self.confirmDelete(warnRoot=self.path == '/')): return rd = RestoreDialog(self, self.sid, self.path, **opt) rd.exec()",if not confirm:
"def keep_vocab_item(word, count, min_count, trim_rule=None): default_res = count >= min_count if trim_rule is None: return default_res else: rule_res = trim_rule(word, count, min_count) <IF_STMT> return True elif rule_res == RULE_DISCARD: return False else: return default_res",if rule_res == RULE_KEEP:
"def _get_cuda_device(*args): for arg in args: if type(arg) is not bool and isinstance(arg, _integer_types): check_cuda_available() return Device(arg) <IF_STMT> if arg.device is None: continue return arg.device if available and isinstance(arg, Device): return arg return DummyDevice","if isinstance(arg, ndarray):"
"def __init__(self, filename, metadata_name, metadata_column, message='Value for metadata not found.', line_startswith=None, split='\t'): self.metadata_name = metadata_name self.message = message self.valid_values = [] for line in open(filename): if line_startswith is None or line.startswith(line_startswith): fields = line.split(split) <IF_STMT> self.valid_values.append(fields[metadata_column].strip())",if metadata_column < len(fields):
"def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) <IF_STMT> start = index elif char in BRACKETS: if not stack: return (-1, -1) if stack.pop() != BRACKETS[char]: return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)",if start == -1:
"def _on_message(self, msg: str) -> None: obj = json.loads(msg) _id = obj.get('id') if _id and _id in self._callbacks: callback = self._callbacks.pop(_id) <IF_STMT> error = obj['error'] msg = error.get('message') data = error.get('data') callback.set_exception(NetworkError(f'Protocol Error: {msg} {data}')) else: result = obj.get('result') callback.set_result(result) else: self.emit(obj.get('method'), obj.get('params'))",if 'error' in obj:
"def _get_containers_with_state(self, container_names, select_random, *container_states): containers = self._get_all_containers() candidates = dict(((c.name, c) for c in containers if c.status in container_states)) if select_random and candidates: return [random.choice(list(candidates.values()))] if container_names is None: return list(candidates.values()) found = [] for name in container_names: container = candidates.get(name) <IF_STMT> raise BlockadeError('Container %s is not found or not any of %s' % (name, container_states)) found.append(container) return found",if not container:
"def __eq__(self, other): if isinstance(other, WeakMethod): <IF_STMT> return False if self.instance is None: return other.instance is None else: return self.instance() == other.instance() elif callable(other): return self == WeakMethod(other) else: return False",if self.function != other.function:
"def last_bottle_hash(): """"""Fetch the bottle do ... end from the latest brew formula"""""" resp = requests.get(HOMEBREW_FORMULAR_LATEST) resp.raise_for_status() lines = resp.text.split('\n') look_for_end = False start = 0 end = 0 for idx, content in enumerate(lines): if look_for_end: <IF_STMT> end = idx break elif 'bottle do' in content: start = idx look_for_end = True return '\n'.join(lines[start:end + 1])",if 'end' in content:
"def wrapper(fn): if debug_run_test_calls: ret = str(fn(*args, *kwargs)) print('TEST: %s()' % fn.__name__) <IF_STMT> print('  arg:', args) if kwargs: print('  kwa:', kwargs) print('  ret:', ret) return fn",if args:
"def parse_socket_line(line): lsp = line.strip().split() if not len(lsp) in {3, 5}: print(line, 'is malformed') return UNPARSABLE else: socket_type = sock_dict.get(lsp[2]) socket_name = lsp[1] <IF_STMT> return UNPARSABLE elif len(lsp) == 3: return (socket_type, socket_name, None, None) else: default = processed(lsp[3]) nested = processed(lsp[4]) return (socket_type, socket_name, default, nested)",if not socket_type:
"def release(self): me, lock_count = self.__begin() try: if me is None: return self._count = count = self._count - 1 <IF_STMT> self._owner = None self._block.release() finally: self.__end(me, lock_count)",if not count:
"def Traverse(self): """"""A generator for _IMAGE_RESOURCE_DATA_ENTRY under this node."""""" for entry in self: <IF_STMT> for subentry in entry.Entry.Traverse(): yield subentry else: yield entry.OffsetToData.dereference()",if entry.ChildIsEntry:
"def getInstances_WithSource(self, instancesAmount, sourceObject, scenes): if sourceObject is None: self.removeAllObjects() return [] else: sourceHash = hash(sourceObject) <IF_STMT> if lastSourceHashes[self.identifier] != sourceHash: self.removeAllObjects() lastSourceHashes[self.identifier] = sourceHash return self.getInstances_Base(instancesAmount, sourceObject, scenes)",if self.identifier in lastSourceHashes:
"def used_pos(): pos_along_edges = [] for e in edges: A, B = (pos[e[0]], pos[e[1]]) <IF_STMT> for i in range(A[1], B[1], np.sign(B[1] - A[1])): pos_along_edges.append((A[0], i)) else: for i in range(A[0], B[0], np.sign(B[0] - A[0])): pos_along_edges.append((i, A[1])) return list(pos.values()) + pos_along_edges",if A[0] == B[0]:
"def __init__(self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None): if builtin and isinstance(builtin, (str, unicode)): builtin = os.path.basename(builtin) for ignore in ('.py', '.pyo', '.pyc'): <IF_STMT> builtin = builtin[:-len(ignore)] if builtin not in self.LOADED: self.LOADED.append(builtin) self.loading_plugin = plugin_name self.loading_builtin = plugin_name and builtin self.builtin = builtin self.deprecated = deprecated self.session = session self.config = config self.manifests = []",if builtin.endswith(ignore):
def input(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.input_ is None: self.input_ = InputSettings() finally: self.lazy_init_lock_.release() return self.input_,if self.input_ is None:
"def _shares_in_results(data): shares_in_device, shares_in_subdevice = (False, False) for plugin_name, plugin_result in data.iteritems(): <IF_STMT> continue if 'device' not in plugin_result: continue if 'disk_shares' in plugin_result['device']: shares_in_device = True for subdevice in plugin_result['device'].get('subdevices', []): if 'disk_shares' in subdevice: shares_in_subdevice = True break return (shares_in_device, shares_in_subdevice)",if plugin_result['status'] == 'error':
"def m2i(self, pkt, x): res = [] while x: cur = [] while x and x[0] != 0: l = x[0] cur.append(x[1:l + 1]) x = x[l + 1:] res.append(b'.'.join(cur)) <IF_STMT> x = x[1:] return res",if x and x[0] == 0:
"def generate_idempotent_uuid(params, model, **kwargs): for name in model.idempotent_members: <IF_STMT> params[name] = str(uuid.uuid4()) logger.debug(""injecting idempotency token (%s) into param '%s'."" % (params[name], name))",if name not in params:
"def __init__(self, name, signatures, kind, vm): super().__init__(name, vm) assert signatures self.kind = kind self.bound_class = BoundPyTDFunction self.signatures = signatures self._signature_cache = {} self._return_types = {sig.pytd_sig.return_type for sig in signatures} for sig in signatures: for param in sig.pytd_sig.params: <IF_STMT> self._has_mutable = True break else: self._has_mutable = False for sig in signatures: sig.function = self sig.name = self.name",if param.mutated_type is not None:
def sub_dict(d): r = {} for k in d: if type(d[k]) in prims: r[k] = d[k] <IF_STMT> r[k] = sub_list(d[k]) elif type(d[k]) is dict: r[k] = sub_dict(d[k]) else: print('Unknown Type: {}'.format(type(d[k]))) return r,elif type(d[k]) is list:
"def listAdd(): cpe = request.args.get('cpe') cpeType = request.args.get('type') lst = request.args.get('list') if cpe and cpeType and lst: status = 'added_to_list' <IF_STMT> else 'already_exists_in_list' returnList = db.getWhitelist() if lst == 'whitelist' else db.getBlacklist() return jsonify({'status': status, 'rules': returnList, 'listType': lst.title()}) else: return jsonify({'status': 'could_not_add_to_list'})","if addCPEToList(cpe, lst, cpeType)"
"def _integrate_fixed_trajectory(self, h, T, step, relax): """"""Generates a solution trajectory of fixed length."""""" solution = np.hstack((self.t, self.y)) while self.successful(): self.integrate(self.t + h, step, relax) current_step = np.hstack((self.t, self.y)) solution = np.vstack((solution, current_step)) if h > 0 and self.t >= T: break <IF_STMT> break else: continue return solution",elif h < 0 and self.t <= T:
"def transform(self, X): if self.preprocessor is None: raise NotImplementedError() with warnings.catch_warnings(): warnings.filterwarnings('error') X_new = self.preprocessor.transform(X) <IF_STMT> raise ValueError('KernelPCA removed all features!') return X_new",if X_new.shape[1] == 0:
"def playerData(s): """"""Returns a list of tuples of original string and dict of values"""""" p = [] i = 0 while True: match = re_input.match(s, pos=i) <IF_STMT> return p else: d = match.groupdict() if d['args'] is not None: d['degree'], d['kwargs'] = getArgs(d['args']) else: d['degree'], d['kwargs'] = ('', {}) del d['args'] p.append((match.group().strip(), d)) i = match.end() return",if match is None:
def extract_deps(file): deps = set() for line in open(file).readlines(): line = line.strip() if line.startswith('import') or line.startswith('from'): words = line.split() <IF_STMT> deps.add(words[1]) return deps,if words[0] == 'import' or (words[0] == 'from' and words[2] == 'import'):
"def _remove_optional_none_type_hints(self, type_hints, defaults): for arg in defaults: <IF_STMT> type_ = type_hints[arg] if self._is_union(type_): types = type_.__args__ if len(types) == 2 and types[1] is type(None): type_hints[arg] = types[0]",if defaults[arg] is None and arg in type_hints:
"def _gaf10iterator(handle): for inline in handle: <IF_STMT> continue inrec = inline.rstrip('\n').split('\t') if len(inrec) == 1: continue inrec[3] = inrec[3].split('|') inrec[5] = inrec[5].split('|') inrec[7] = inrec[7].split('|') inrec[10] = inrec[10].split('|') inrec[12] = inrec[12].split('|') yield dict(zip(GAF10FIELDS, inrec))",if inline[0] == '!':
"def cvePluginInfo(self, cve, **args): cveInfo = [] for plugin in self.getWebPlugins(): try: data = plugin.cvePluginInfo(cve, **args) <IF_STMT> cveInfo.append(data) except Exception as e: print('[!] Plugin %s failed on fetching CVE plugin info!' % plugin.getName()) print('[!]  -> %s' % e) return cveInfo",if type(data) == dict and 'title' in data and ('data' in data):
"def testLastContainerMarker(self): for format in [None, 'json', 'xml']: containers = self.env.account.containers({'format': format}) self.assertEquals(len(containers), len(self.env.containers)) self.assert_status(200) containers = self.env.account.containers(parms={'format': format, 'marker': containers[-1]}) self.assertEquals(len(containers), 0) <IF_STMT> self.assert_status(204) else: self.assert_status(200)",if format is None:
"def _make_input_layers(self, rebuild=False): for name, layer in self.layer_map.items(): layer.left_in_edges = len(layer.in_edges) if len(layer.in_edges) == 0: <IF_STMT> if not layer.get_attr('scope'): self.input_layers.append(name) else: self.input_layers.append(name)",if rebuild:
"def widget_attrs(self, widget): attrs = super(IntegerField, self).widget_attrs(widget) if isinstance(widget, NumberInput): <IF_STMT> attrs['min'] = self.min_value if self.max_value is not None: attrs['max'] = self.max_value return attrs",if self.min_value is not None:
"def _get_outfile(self): outfile = self.inputs.transformed_file if not isdefined(outfile): if self.inputs.inverse is True: <IF_STMT> src = 'orig.mgz' else: src = self.inputs.target_file else: src = self.inputs.source_file outfile = fname_presuffix(src, newpath=os.getcwd(), suffix='_warped') return outfile",if self.inputs.fs_target is True:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <IF_STMT> self.set_num_memcacheg_backends(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
def try_to_find_osquery(self): extention = '' if platform.system() == 'Windows': extention = '.exe' try: return resources.get_resource('osqueryi' + extention) except IOError as e: if platform.system() == 'Windows': result = 'c:\\ProgramData\\osquery\\osqueryi.exe' <IF_STMT> return result else: return spawn.find_executable('osqueryi') raise e,"if os.access(result, os.R_OK):"
"def keyPressEvent(self, event): """"""Add up and down arrow key events to built in functionality."""""" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: <IF_STMT> self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)",if keyPressed == Constants.UP_KEY:
"def find_parent_for_new_to(self, pos): """"""Figure out the parent object for something at 'pos'."""""" for children in self._editable_children: <IF_STMT> return children.find_parent_for_new_to(pos) if children._start == pos and pos == children._end: return children.find_parent_for_new_to(pos) return self",if children._start <= pos < children._end:
def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) if self._shuffle: <IF_STMT> random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret if self._mode == 'test': break,if self._n_gpus > 1:
"def to_multidevice(batch_iter, num_trainer): """"""to_multidevice"""""" batch_dict = [] for batch in batch_iter(): batch_dict.append(batch) <IF_STMT> yield batch_dict batch_dict = [] if len(batch_dict) > 0: log.warning(""The batch (%s) can't fill all device (%s)which will be discarded."" % (len(batch_dict), num_trainer))",if len(batch_dict) == num_trainer:
"def get_word_parens_range(self, offset, opening='(', closing=')'): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): if self.code[index] == opening: open_count += 1 if self.code[index] == closing: open_count -= 1 <IF_STMT> return (start_parens, index + 1) index += 1 return (start_parens, index)",if open_count == 0:
"def getNodeBySunid(self, sunid): """"""Return a node from its sunid."""""" <IF_STMT> return self._sunidDict[sunid] if self.db_handle: self.getDomainFromSQL(sunid=sunid) if sunid in self._sunidDict: return self._sunidDict[sunid] else: return None",if sunid in self._sunidDict:
"def get_cabal_in_dir(cabal_dir): """"""Return .cabal file for cabal directory"""""" for entry in os.listdir(cabal_dir): <IF_STMT> project_name = os.path.splitext(entry)[0] return (project_name, os.path.join(cabal_dir, entry)) return (None, None)",if entry.endswith('.cabal'):
"def authenticate(self, username, password): emails = ContactValue.objects.filter(value=username, field__field_type='email', contact__trash=False, contact__related_user__isnull=False) for email in emails: try: user = email.contact.related_user.user.user <IF_STMT> return user except: pass return None",if user.check_password(password):
"def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): <IF_STMT> continue elif line.startswith('@highlight'): next_is_highlight = True elif next_is_highlight: highlights.append(line) else: article_lines.append(line) article = ' '.join(article_lines) abstract = ' '.join(highlights) return (article, abstract)",if line == '':
def find_token(self): found = False while not found: while self.data[self.index] in ' \t': self.index += 1 <IF_STMT> while self.data[self.index] != '\n': self.index += 1 if self.data[self.index] == '\n': self.index += 1 else: found = True,if self.data[self.index] == '#':
"def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f['f'].splitlines(): cols = line.rstrip().split('\t') if cols[0] == '#bamPEFragmentSize': continue elif cols[0] == 'Size': continue else: s_name = self.clean_s_name(cols[2].rstrip().split('/')[-1], f['root']) <IF_STMT> d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d",if s_name != lastsample:
"def get_user_home(): if is_win(): <IF_STMT> output = subprocess.Popen(['cygpath', '-m', os.path.expanduser('~')], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].rstrip() return output else: return os.environ['USERPROFILE'] else: return os.path.expanduser('~')",if sys.platform == 'cygwin':
def _grouping_intervals(grouping): last_interval = None for interval in grouping: if interval == CHAR_MAX: return <IF_STMT> if last_interval is None: raise ValueError('invalid grouping') while True: yield last_interval yield interval last_interval = interval,if interval == 0:
def remove_duplicates(model): for struct in model.structs: fields = [] names = [] for field in struct.fields: <IF_STMT> names.append(field.name) fields.append(field) struct.fields = fields,if field.name not in names:
"def set_multi(self, value): del self[atype] for addr in value: if not isinstance(addr, Address): if atype != 'all': addr['type'] = atype <IF_STMT> addr['type'] = addr['atype'] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)",elif 'atype' in addr and 'type' not in addr:
"def import_directives(): files_list = os.listdir(os.path.dirname(__file__)) for directive_file in files_list: <IF_STMT> continue __import__('gixy.directives.' + os.path.splitext(directive_file)[0], None, None, [''])",if not directive_file.endswith('.py') or directive_file.startswith('_'):
"def _get_all_tasks(): proc = Popen(['yarn', '--help'], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() <IF_STMT> should_yield = True continue if should_yield and '- ' in line: yield line.split(' ')[-1]",if 'Commands:' in line:
"def _waitFakenetStopped(self, timeoutsec=None): retval = False while True: if self._confirmFakenetStopped(): retval = True break time.sleep(1) <IF_STMT> timeoutsec -= 1 if timeoutsec <= 0: break return retval",if timeoutsec is not None:
"def parse_compare_fail(string, rex=re.compile('^(?P<field>min|max|mean|median|stddev|iqr):((?P<percentage>[0-9]?[0-9])%|(?P<difference>[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?))$')): m = rex.match(string) if m: g = m.groupdict() <IF_STMT> return PercentageRegressionCheck(g['field'], int(g['percentage'])) elif g['difference']: return DifferenceRegressionCheck(g['field'], float(g['difference'])) raise argparse.ArgumentTypeError('Could not parse value: %r.' % string)",if g['percentage']:
"def get_converter(self, key, default=None): """"""Gets a converter for the given key."""""" if key in self._vars: return self._vars[key].convert for k, var in self._vars.items(): if isinstance(k, str): continue <IF_STMT> converter = var.convert self._vars[key] = var break else: converter = self._get_default_converter(default=default) return converter",if k.match(key) is not None:
"def get_model_params(problem_type: str, hyperparameters): penalty = hyperparameters.get('penalty', L2) handle_text = hyperparameters.get('handle_text', IGNORE) if problem_type == REGRESSION: if penalty == L2: model_class = Ridge <IF_STMT> model_class = Lasso else: logger.warning('Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2'.format(penalty)) penalty = L2 model_class = Ridge else: model_class = LogisticRegression return (model_class, penalty, handle_text)",elif penalty == L1:
"def __init__(self, content=None, parent=None): Transformable.__init__(self, content, parent) self._items = [] for element in content: if not element.tag.startswith(namespace): continue tag = element.tag[len(namespace):] <IF_STMT> item = Group(element, self) elif tag == 'path': item = Path(element, self) else: log.warn('Unhandled SVG tag (%s)' % tag) continue self._items.append(item)",if tag == 'g':
"def f_context(args: argparse.Namespace): choice = args.choice ctx = utils.get_context() if choice is None: <IF_STMT> group = ctx.stem print(f""{group}: {' '.join(utils.get_groups()[group])}"") else: print('Context is not set') elif choice == 'none': ctx and ctx.unlink() else: fname = Path(common.get_config_dir()) / (choice + '.context') if ctx: ctx.rename(fname) else: open(fname, 'w').close()",if ctx:
"def check_checksum(self): """"""fix media checksums"""""" self.progress.set_pass(_('Updating checksums on media'), len(self.db.get_media_handles())) for objectid in self.db.get_media_handles(): self.progress.step() obj = self.db.get_media_from_handle(objectid) full_path = media_path_full(self.db, obj.get_path()) new_checksum = create_checksum(full_path) <IF_STMT> logging.info('checksum: updating ' + obj.gramps_id) obj.checksum = new_checksum self.db.commit_media(obj, self.trans)",if new_checksum != obj.checksum:
"def get_default_backend(self, user_backends): retval = None n_defaults = 0 for name in user_backends: args = user_backends.get(name) if args.get('default', False): n_defaults = n_defaults + 1 <IF_STMT> retval = name return (retval, n_defaults)",if retval is None:
"def on_mqtt_packet_received(self, *args, **kwargs): packet = kwargs.get('packet') if packet: packet_size = packet.bytes_length self._stats[STAT_BYTES_RECEIVED] += packet_size self._stats[STAT_MSG_RECEIVED] += 1 <IF_STMT> self._stats[STAT_PUBLISH_RECEIVED] += 1",if packet.fixed_header.packet_type == PUBLISH:
"def func(self): if self.schema: d = {} for key in self._schema_keys: d[key] = getattr(self, key) <IF_STMT> akeys = set(self._data.keys()) - set(d.keys()) for akey in akeys: d[akey] = self._data[akey] return d else: return None",if self._data:
"def endElement(self, name, value, connection): if name == 'vpcId': self.vpc_id = value elif name == 'value': if value == 'true': value = True else: value = False <IF_STMT> self.enable_dns_hostnames = value elif self._current_attr == 'enableDnsSupport': self.enable_dns_support = value",if self._current_attr == 'enableDnsHostnames':
"def keyPressEvent(self, event): if event.key() in (Qt.Key_Right, Qt.Key_Left): direction = 1 <IF_STMT> direction = -1 if event.modifiers() == Qt.ShiftModifier: print('shift') direction *= 10 self.timeline.setValue(self.timeline.value() + direction) else: super(VideoPlayerWidget, self).keyPressEvent(event)",if event.key() == Qt.Key_Left:
"def find_config(pipeline_config_path: Union[str, Path]) -> Path: if not Path(pipeline_config_path).is_file(): configs = [c for c in Path(__file__).parent.parent.parent.glob(f'configs/**/{pipeline_config_path}.json') if str(c.with_suffix('')).endswith(pipeline_config_path)] <IF_STMT> log.info(f""Interpreting '{pipeline_config_path}' as '{configs[0]}'"") pipeline_config_path = configs[0] return Path(pipeline_config_path)",if configs:
"def list_translations(dirname): if not os.path.isdir(dirname): return [] result = [] for entry in scandir(dirname): locale_dir = os.path.join(entry.path, 'LC_MESSAGES') if not os.path.isdir(locale_dir): continue <IF_STMT> result.append(Locale.parse(entry.name)) return result","if any(filter(lambda x: x.name.endswith('.mo'), scandir(locale_dir))):"
"def writeTo(self, writable): chunkStart = 0 fileSize = blob.properties.content_length while chunkStart < fileSize: chunkEnd = chunkStart + outer_self._maxAzureBlockBytes - 1 buf = container.get_blob_to_bytes(blob_name=str(jobStoreFileID), start_range=chunkStart, end_range=chunkEnd).content <IF_STMT> buf = encryption.decrypt(buf, outer_self.keyPath) writable.write(buf) chunkStart = chunkEnd + 1",if encrypted:
"def get_extractor(name): for extractor in ALL_EXTRACTORS: <IF_STMT> module = import_module('anime_downloader.extractors.{}'.format(extractor['modulename'])) return getattr(module, extractor['class'])",if extractor['regex'] in name.lower():
"def updateSize(self): if self.size is not None: return height = 0 width = 0 for row in range(self.layout.rowCount()): row_height = 0 col_witdh = 0 for col in range(self.layout.columnCount()): item = self.layout.itemAt(row, col) <IF_STMT> col_witdh += item.width() + 3 row_height = max(row_height, item.height()) width = max(width, col_witdh) height += row_height self.setGeometry(0, 0, width, height) return",if item:
"def close_group(self): """"""Closes a grouping for previous filters"""""" if self._filters: <IF_STMT> raise RuntimeError('Not enough open groups to close.') if isinstance(self._filters[-1], ChainOperator): flt_sentence = self._filters[-2] else: flt_sentence = self._filters[-1] flt_sentence[1] = flt_sentence[1] + ')' self._close_group_flag.append(False) else: raise RuntimeError(""No filters present. Can't close a group"") return self",if len(self._open_group_flag) < len(self._close_group_flag) + 1:
def test_name_conflicts(): ev = fake_event() ev2 = fake_event() ev2.participants[0]['email'] = None ev2.participants[0]['status'] = 'yes' merged_participants = ev._partial_participants_merge(ev2) assert len(merged_participants) == 2 for participant in merged_participants: <IF_STMT> assert participant['status'] == 'yes' else: assert participant['name'] == 'Ronald Zubar',if participant['email'] is None:
"def set_idle(view, idle): vid = view.id() current_idle = vid in State['idle_views'] if idle != current_idle: <IF_STMT> State['idle_views'].add(vid) else: State['idle_views'].discard(vid) toggle_demoted_regions(view, idle)",if idle:
"def _deserialize(self, value, attr, data, **kwargs): if isinstance(value, str): return [value, 0, 0] if isinstance(value, list) and len(value) == 3: condition = isinstance(value[0], str) and isinstance(value[1], int) and isinstance(value[1], int) <IF_STMT> return value raise ValidationError('This field expects a str or a list of [str, int, int].')",if condition:
"def _struct(self, fields): result = {} for field in fields: if field[0] == '__parent': parent = self.instance(field[1]) if isinstance(parent, dict): result.update(parent) <IF_STMT> result = parent else: result[field[0]] = parent else: result[field[0]] = self.instance(field[1]) return result",elif len(fields) == 1:
"def validate(self): if 'accounts' in self.data and self.data['accounts'] == 'matched': found = False for f in self.manager.iter_filters(): if isinstance(f, AmiCrossAccountFilter): found = True break <IF_STMT> raise PolicyValidationError('policy:%s filter:%s with matched requires cross-account filter' % (self.manager.ctx.policy.name, self.type))",if not found:
"def add_rule6(self, rule): if self.cleared: return self._lock.acquire() try: self._other6.append(rule) <IF_STMT> self._insert_iptables_rule(rule, ipv6=True) finally: self._lock.release()","if not self._exists_iptables_rule(rule, ipv6=True):"
"def load_grammar(self, *args): """"""Load a grammar from a pickle file"""""" filename = askopenfilename(filetypes=self.GRAMMAR_FILE_TYPES, defaultextension='.cfg') if not filename: return try: <IF_STMT> with open(filename, 'rb') as infile: grammar = pickle.load(infile) else: with open(filename, 'r') as infile: grammar = CFG.fromstring(infile.read()) self.set_grammar(grammar) except Exception as e: tkinter.messagebox.showerror('Error Loading Grammar', 'Unable to open file: %r' % filename)",if filename.endswith('.pickle'):
"def _join_printed_types(self, types): """"""Pretty-print the union of the printed types."""""" types = sorted(set(types)) if len(types) == 1: return next(iter(types)) elif types: <IF_STMT> types.remove('None') return 'Optional[%s]' % self._join_printed_types(types) else: return 'Union[%s]' % ', '.join(types) else: return 'nothing'",if 'None' in types:
"def __init__(self, **kwargs): for key, val in kwargs.items(): field = getattr(self.__class__, key, None) <IF_STMT> raise TypeError('Field %r returned from raw SQL query does not have a column defined in the model' % key) setattr(self, field.get_attname() or key, field.to_python(val))",if field is None:
"def get_transaction_execution_results(self, batch_signature): with self._condition: batch_status = self._batch_statuses.get(batch_signature) <IF_STMT> return None annotated_batch = self._batch_by_id.get(batch_signature) if annotated_batch is None: return None results = [] for txn in annotated_batch.batch.transactions: result = self._txn_results.get(txn.header_signature) if result is not None: results.append(result) return results",if batch_status is None:
def _check_params(self) -> None: if self.augmentation and self.ratio <= 0: raise ValueError('The augmentation ratio must be positive.') if self.clip_values is not None: <IF_STMT> raise ValueError('`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.') if np.array(self.clip_values[0] >= self.clip_values[1]).any(): raise ValueError('Invalid `clip_values`: min >= max.'),if len(self.clip_values) != 2:
def ping_all(): for l in _all_listeners.values(): count = l.receiver.count() if count: for dev in l.receiver: dev.ping() l._status_changed(dev) count -= 1 <IF_STMT> break,if not count:
"def on_btOK_clicked(self, *a): """"""Handler for OK button"""""" if self.ac_callback is not None: self._set_title() if self._mode == ActionEditor.AEC_MENUITEM: self.ac_callback(self.id, self) else: a = self.generate_modifiers(self._action, self._selected_component.NAME == 'custom') self.ac_callback(self.id, a) self.ac_callback = None <IF_STMT> self._selected_component.on_ok(a) self.close()",if self._selected_component:
"def apply_ssl(self, request): if self.ssl_protocol: <IF_STMT> self.sslconf.setProtocol(self.ssl_protocol) QSslConfiguration.setDefaultConfiguration(self.sslconf) request.setSslConfiguration(self.sslconf) return request",if self.sslconf.protocol() != self.ssl_protocol:
"def _iter_process_args(mapping, pid, max_depth): """"""Iterator to traverse up the tree, yielding each process's argument list."""""" for _ in range(max_depth): try: proc = mapping[pid] except KeyError: break <IF_STMT> yield proc.args pid = proc.ppid",if proc.args:
"def store_data(self, store_loc, **kwargs): """"""Put arrays to store"""""" g = self.store.create_group(store_loc) for k, v in kwargs.items(): if type(v) == list: <IF_STMT> if type(v[0]) is np.str_ or type(v[0]) is str: v = [a.encode('utf8') for a in v] g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)",if len(v) != 0:
def add_system_info_creds_to_config(creds): for user in creds: ConfigService.creds_add_username(creds[user]['username']) if 'password' in creds[user] and creds[user]['password']: ConfigService.creds_add_password(creds[user]['password']) if 'lm_hash' in creds[user] and creds[user]['lm_hash']: ConfigService.creds_add_lm_hash(creds[user]['lm_hash']) <IF_STMT> ConfigService.creds_add_ntlm_hash(creds[user]['ntlm_hash']),if 'ntlm_hash' in creds[user] and creds[user]['ntlm_hash']:
"def _format_arg(self, name, spec, value): if name == 'title': <IF_STMT> return '--title' elif isinstance(value, str): return '--title --title_text %s' % (value,) else: raise ValueError('Unknown value for ""title"" argument: ' + str(value)) return super(Pik, self)._format_arg(name, spec, value)","if isinstance(value, bool) and value:"
"def handle_friend(self): tokens, last = self._get_var_tokens_up_to(False, '(', ';') if last.name == '(': tokens.append(last) self._add_back_tokens(tokens) token = self._get_next_token() while token.name in ('inline', 'typename', '::'): token = self._get_next_token() result = self._generate_one(token) else: <IF_STMT> tokens = tokens[1:] result = self.converter.to_type(tokens)[0] assert result return Friend(result.start, result.end, result, self.namespace_stack)",if tokens[0].name == 'class':
"def list_subtitles(self, video, languages): season = None episodes = [] if isinstance(video, Episode): titles = [video.series] + video.alternative_series season = video.season episodes = video.episodes else: titles = [video.title] + video.alternative_titles for title in titles: subtitles = [s for l in languages for s in self.query(l, title, season=season, episodes=episodes, year=video.year)] <IF_STMT> return subtitles return []",if subtitles:
"def on_write_needed(self, nbytes, underflow): if underflow: self._handle_underflow() else: self._write_to_stream(nbytes) if self._events: <IF_STMT> self._time_sync_operation.delete() self._time_sync_operation = None if self._time_sync_operation is None: assert _debug('PulseAudioPlayer: trigger timing info update') self._time_sync_operation = self.stream.update_timing_info(self._process_events)",if self._time_sync_operation is not None and self._time_sync_operation.is_done:
def _set_account_info(self): with session_scope(self.account_id) as db_session: account = db_session.query(ImapAccount).get(self.account_id) self.sync_state = account.sync_state self.provider = account.provider self.provider_info = account.provider_info self.email_address = account.email_address self.auth_handler = account.auth_handler <IF_STMT> self.client_cls = GmailCrispinClient else: self.client_cls = CrispinClient,if account.provider == 'gmail':
"def make_timesheet_records(): employees = get_timesheet_based_salary_slip_employee() for e in employees: ts = make_timesheet(e.employee, simulate=True, billable=1, activity_type=get_random('Activity Type'), company=frappe.flags.company) frappe.db.commit() rand = random.random() <IF_STMT> make_salary_slip_for_timesheet(ts.name) rand = random.random() if rand >= 0.2: make_sales_invoice_for_timesheet(ts.name)",if rand >= 0.3:
"def free(self, addr, ban=0): with self.lock: <IF_STMT> self.ban.append({'addr': addr, 'counter': ban}) else: base, bit, is_allocated = self.locate(addr) if len(self.addr_map) <= base: raise KeyError('address is not allocated') if self.addr_map[base] & 1 << bit: raise KeyError('address is not allocated') self.allocated -= 1 self.addr_map[base] ^= 1 << bit",if ban != 0:
"def flush_log(self): try: while len(self.log_buffer) > 0: level, message = self.log_buffer.pop(0) <IF_STMT> self._display_log(message, level) except IndexError: pass",if level <= self.log_level:
def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args['user'] = mysql_user if mysql_pwd: args['passwd'] = mysql_pwd if mysql_host: args['host'] = mysql_host <IF_STMT> args['port'] = mysql_port if mysql_socket: args['unix_socket'] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception('Cannot interface with MySQL server: %s' % e),if mysql_port:
"def get_middleware_resolvers(middlewares): for middleware in middlewares: <IF_STMT> yield middleware if not hasattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION): continue yield getattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION)",if inspect.isfunction(middleware):
def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) <IF_STMT> if self._n_gpus > 1: random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret if self._mode == 'test': break,if self._shuffle:
"def extract_cookies(self, response, request): """"""Extract cookies from response, where allowable given the request."""""" _debug('extract_cookies: %s', response.info()) self._cookies_lock.acquire() try: self._policy._now = self._now = int(time.time()) for cookie in self.make_cookies(response, request): <IF_STMT> _debug(' setting cookie: %s', cookie) self.set_cookie(cookie) finally: self._cookies_lock.release()","if self._policy.set_ok(cookie, request):"
"def _gen_filename(self, name): if name == 'in_average': avg_subject = str(self.inputs.hemisphere) + '.EC_average' avg_directory = os.path.join(self.inputs.subjects_dir, avg_subject) <IF_STMT> fs_home = os.path.abspath(os.environ.get('FREESURFER_HOME')) return avg_subject elif name == 'out_file': return self._list_outputs()[name] else: return None",if not os.path.isdir(avg_directory):
"def decorated_view(*args, **kwargs): h = {} mechanisms = [(method, login_mechanisms.get(method)) for method in auth_methods] for method, mechanism in mechanisms: if mechanism and mechanism(): return fn(*args, **kwargs) <IF_STMT> r = _security.default_http_auth_realm h['WWW-Authenticate'] = 'Basic realm=""%s""' % r if _security._unauthorized_callback: return _security._unauthorized_callback() else: return _get_unauthorized_response(headers=h)",elif method == 'basic':
"def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False for pattern in S3Sync.exclude_files: if fnmatch.fnmatch(file, pattern): exclude = True break <IF_STMT> full_path = root + '/' + file if include_checksums: checksum = self._hash_file(full_path) else: checksum = '' file_list[relpath + file] = [full_path, checksum] return file_list",if not exclude:
"def attr(**kw): kw = kw.items() kw.sort() parts = [] for name, value in kw: if value is None: continue <IF_STMT> name = name[:-1] parts.append('%s=""%s""' % (html_quote(name), html_quote(value))) return html(' '.join(parts))",if name.endswith('_'):
"def create(self): if not self.created: self.created = True cmd = self._mode <IF_STMT> cmd = u'' vim.command((u':%snoremap %s %s' % (cmd, str(self), self.command)).encode(u'utf-8'))",if cmd == MODE_ALL:
"def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): <IF_STMT> if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield (index, token, value)",if token is Name:
"def _merge_colormaps(kwargs): """"""Merge colormaps listed in kwargs."""""" from trollimage.colormap import Colormap full_cmap = None palette = kwargs['palettes'] if isinstance(palette, Colormap): full_cmap = palette else: for itm in palette: cmap = create_colormap(itm) cmap.set_range(itm['min_value'], itm['max_value']) <IF_STMT> full_cmap = cmap else: full_cmap = full_cmap + cmap return full_cmap",if full_cmap is None:
"def from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True): key_tag = tok.get_uint16() algorithm = tok.get_uint8() digest_type = tok.get_uint8() chunks = [] while 1: t = tok.get().unescape() if t.is_eol_or_eof(): break <IF_STMT> raise dns.exception.SyntaxError chunks.append(t.value) digest = ''.join(chunks) digest = digest.decode('hex_codec') return cls(rdclass, rdtype, key_tag, algorithm, digest_type, digest)",if not t.is_identifier():
"def connect_reader_to_writer(reader, writer): BUF_SIZE = 8192 try: while True: data = await reader.read(BUF_SIZE) <IF_STMT> if not writer.transport.is_closing(): writer.write_eof() await writer.drain() return writer.write(data) await writer.drain() except (OSError, asyncio.IncompleteReadError) as e: pass",if not data:
"def _get_cuda_device(*args): for arg in args: if type(arg) is not bool and isinstance(arg, _integer_types): check_cuda_available() return Device(arg) if isinstance(arg, ndarray): if arg.device is None: continue return arg.device <IF_STMT> return arg return DummyDevice","if available and isinstance(arg, Device):"
"def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == ';': return i <IF_STMT> i = g.skip_string(s, i) elif g.match(s, i, '//'): i = g.skip_to_end_of_line(s, i) elif g.match(s, i, '/*'): i = g.skip_block_comment(s, i) else: i += 1 return i","elif c == ""'"" or c == '""':"
"def build_CallFunc(self, o): children = o.getChildren() callee = self.build(children[0]) args = [] kwargs = {} for child in children[1:]: class_name = child.__class__.__name__ if class_name == 'NoneType': continue <IF_STMT> kwargs.update(self.build(child)) else: args.append(self.build(child)) return callee(*args, **kwargs)",if class_name == 'Keyword':
"def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]: ret: Dict[str, List[str]] = {} for contract in slither.contracts: cst_functions = [_get_name(f) for f in contract.functions_entry_points if _is_constant(f)] cst_functions += [v.function_name for v in contract.state_variables <IF_STMT>] if cst_functions: ret[contract.name] = cst_functions return ret",if v.visibility in ['public']
"def acquire_read_lock(self, wait=True): state = self.state if state.writing: raise LockError('lock is in writing state') if state.reentrantcount == 0: x = self.do_acquire_read_lock(wait) <IF_STMT> state.reentrantcount += 1 state.reading = True return x elif state.reading: state.reentrantcount += 1 return True",if wait or x:
"def get_optional_nargs(self, name): for n, kwargs in self.conf['optional_args']: <IF_STMT> if 'action' in kwargs: action = kwargs['action'] if action in ('store_true', 'store_false'): return 0 break return 1",if name == n:
"def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [lnk for lnk in rule.link_extractor.extract_links(response) <IF_STMT>] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) request = self._build_request(n, link) yield rule._process_request(request, response)",if lnk not in seen
"def process_module(name, module, parent): if parent: modules[parent]['items'].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) if get_module_type(name) == 'py3status': module['.group'] = parent for k, v in list(module.items()): <IF_STMT> process_onclick(k, v, name) del module[k] if isinstance(v, ModuleDefinition): module['items'] = [] return module",if k.startswith('on_click'):
"def _mysql_version_validator(version, sku_info, tier): if version: versions = get_mysql_versions(sku_info, tier) <IF_STMT> raise CLIError('Incorrect value for --version. Allowed values : {}'.format(versions))",if version not in versions:
"def do_blocking_test(self, block_func, block_args, trigger_func, trigger_args): thread = _TriggerThread(trigger_func, trigger_args) thread.start() try: self.result = block_func(*block_args) <IF_STMT> self.fail(""blocking function '%r' appeared not to block"" % block_func) return self.result finally: thread.join(10) if thread.is_alive(): self.fail(""trigger function '%r' appeared to not return"" % trigger_func)",if not thread.startedEvent.is_set():
"def _fatal_error(self, exc, message='Fatal error on pipe transport'): try: <IF_STMT> if self._loop.get_debug(): logger.debug('%r: %s', self, message, exc_info=True) else: self._loop.call_exception_handler({'message': message, 'exception': exc, 'transport': self, 'protocol': self._protocol}) finally: self._force_close(exc)","if isinstance(exc, OSError):"
"def run_test_family(tests, mode_filter, files, open_func, *make_args): for test_func in tests: <IF_STMT> out.write('\n') continue if mode_filter in test_func.file_open_mode: continue for s in test_func.file_sizes: name, size = files[size_names[s]] args = tuple((f(name, size) for f in make_args)) run_one_test(name, size, open_func, test_func, *args)",if test_func is None:
"def py__get__(self, obj): names = self.get_function_slot_names('__get__') if names: <IF_STMT> return self.execute_function_slots(names, obj, obj.class_context) else: none_obj = compiled.create(self.evaluator, None) return self.execute_function_slots(names, none_obj, obj) else: return ContextSet(self)","if isinstance(obj, AbstractInstanceContext):"
"def _options_fcheck(self, name, xflags, table): for entry in table: if entry.name is None: break <IF_STMT> raise XTablesError('%s: --%s must be specified' % (name, entry.name)) if not xflags & 1 << entry.id: continue",if entry.flags & XTOPT_MAND and (not xflags & 1 << entry.id):
"def _consumer_healthy(self): abnormal_num = 0 for w in self._consumers: if not w.is_alive() and w.id not in self._consumer_endsig: abnormal_num += 1 <IF_STMT> errmsg = 'consumer[{}] exit abnormally with exitcode[{}]'.format(w.pid, w.exitcode) else: errmsg = 'consumer[{}] exit abnormally'.format(w.ident) logger.warn(errmsg) if abnormal_num > 0: logger.warn('{} consumers have exited abnormally!!!'.format(abnormal_num)) return abnormal_num == 0",if self._use_process:
"def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): <IF_STMT> continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return (groups, words)",if not word:
"def _validate_callbacks(cls, callbacks): for callback in callbacks: if not isinstance(callback, Callback): <IF_STMT> raise TypeError('Make sure to instantiate the callbacks.') raise TypeError('Only accepts a `callbacks` instance.')","if issubclass(callback, Callback):"
"def convert_errors(from_, to, msg=None): exc = None try: yield None except from_ as e: exc = e if exc: info = '%s: %s' % (exc.__class__.__name__, str(exc)) <IF_STMT> info = '%s: %s' % (msg, info) raise to(info)",if msg:
"def delete_loan(loan_key, loan=None): <IF_STMT> loan = web.ctx.site.store.get(loan_key) if not loan: raise Exception('Could not find store record for %s', loan_key) loan.delete()",if not loan:
"def last_action_for(self, agent_id: AgentID=_DUMMY_AGENT_ID) -> EnvActionType: """"""Returns the last action for the specified agent, or zeros."""""" if agent_id in self._agent_to_last_action: return flatten_to_single_ndarray(self._agent_to_last_action[agent_id]) else: policy = self._policies[self.policy_for(agent_id)] flat = flatten_to_single_ndarray(policy.action_space.sample()) <IF_STMT> return np.zeros_like(flat, dtype=policy.action_space.dtype) return np.zeros_like(flat)","if hasattr(policy.action_space, 'dtype'):"
"def on_leave(self, original_node: CSTNodeT, updated_node: CSTNodeT) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]: if isinstance(updated_node, cst.Import): for alias in updated_node.names: name = alias.name if isinstance(name, cst.Name) and name.value == 'b': return cst.RemoveFromParent() elif isinstance(updated_node, cst.ImportFrom): module = updated_node.module <IF_STMT> return cst.RemoveFromParent() return updated_node","if isinstance(module, cst.Name) and module.value == 'e':"
"def sortkey(self, r, prog=None): ret = [] for col, reverse in self._ordering: <IF_STMT> col = self.column(col) val = col.getTypedValue(r) ret.append(Reversor(val) if reverse else val) if prog: prog.addProgress(1) return ret","if isinstance(col, str):"
"def down_button_clicked(self, obj): ref = self.get_selected() if ref and ref[1] is not None: pos = self.find_index(ref) <IF_STMT> self._move_down(pos, ref[1]) elif ref and ref[1] is None: self._move_down_group(ref[0])",if pos[1] >= 0 and pos[1] < len(self.get_data()[pos[0]]) - 1:
"def maybe_swap_for_shadow_path(self, path: str) -> str: if not self.shadow_map: return path path = normpath(path, self.options) previously_checked = path in self.shadow_equivalence_map if not previously_checked: for source, shadow in self.shadow_map.items(): <IF_STMT> self.shadow_equivalence_map[path] = shadow break else: self.shadow_equivalence_map[path] = None shadow_file = self.shadow_equivalence_map.get(path) return shadow_file if shadow_file else path","if self.fscache.samefile(path, source):"
"def _add_kid(key, x): if x is None: kids[key] = None elif type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 <IF_STMT> if hasattr(x, '__iter__'): if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x","elif isinstance(x, TVTKBase):"
"def find_zone_id(domain, client=None): paginator = client.get_paginator('list_hosted_zones') zones = [] for page in paginator.paginate(): for zone in page['HostedZones']: <IF_STMT> if not zone['Config']['PrivateZone']: zones.append((zone['Name'], zone['Id'])) if not zones: raise ValueError('Unable to find a Route53 hosted zone for {}'.format(domain)) return zones[0][1]",if domain.endswith(zone['Name']) or (domain + '.').endswith(zone['Name']):
"def render(self, context): for condition, nodelist in self.conditions_nodelists: if condition is not None: try: match = condition.eval(context) except VariableDoesNotExist: match = None else: match = True <IF_STMT> return nodelist.render(context) return ''",if match:
"def init_weight(self): if self.pretrained is not None: load_entire_model(self, self.pretrained) else: for sublayer in self.sublayers(): <IF_STMT> kaiming_normal_init(sublayer.weight) elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)): kaiming_normal_init(sublayer.weight)","if isinstance(sublayer, nn.Conv2D):"
"def _next_empty_row(view, pt): r = utils.row_at(view, pt) while True: r += 1 pt = view.text_point(r, 0) if utils.row_at(view, pt) == utils.last_row(view): return (view.size(), True) <IF_STMT> return (pt, False)",if view.line(pt).empty():
"def __init__(self, parent, name, max_size=None, description=None): Field.__init__(self, parent, name, size=0, description=description) value = 0 addr = self.absolute_address while max_size is None or self._size < max_size: byte = parent.stream.readBits(addr, 8, LITTLE_ENDIAN) value += byte self._size += 8 <IF_STMT> break addr += 8 self.createValue = lambda: value",if byte != 255:
"def xdir(obj, return_values=False): for attr in dir(obj): if attr[:2] != '__' and attr[-2:] != '__': <IF_STMT> yield (attr, getattr(obj, attr)) else: yield attr",if return_values:
"def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): if value == ChangeType.REMOVED: if name in doc_map: deletes.append(name) <IF_STMT> if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)",elif name in doc_map:
"def endElement(self, name): if self._is_active is True: <IF_STMT> self._is_active = False self._tag_level = None if _callable(self._callback): self._callback(self._record) self._record = None elif self._level == self._tag_level + 1: if name != 'xref': self._record[name] = ''.join(self._tag_payload) self._tag_payload = None self._tag_feeding = False self._level -= 1",if name == 'record' and self._tag_level == self._level:
"def init_worker(status_queue: multiprocessing.SimpleQueue, param_queue: multiprocessing.SimpleQueue, result_queue: multiprocessing.SimpleQueue) -> None: global result global coverage_run random.seed() result = ChannelingTestResult(result_queue) if not param_queue.empty(): server_addr = param_queue.get() <IF_STMT> os.environ['EDGEDB_TEST_CLUSTER_ADDR'] = json.dumps(server_addr) coverage_run = devmode.CoverageConfig.start_coverage_if_requested() status_queue.put(True)",if server_addr is not None:
"def wait(uuid: str, kind: str, max_retries: int): """"""Delete an s3 subpath."""""" from polyaxon import settings from polyaxon.agents.spawners.spawner import Spawner spawner = Spawner(namespace=settings.CLIENT_CONFIG.namespace, in_cluster=True) retry = 1 while retry < max_retries: try: k8s_operation = spawner.get(run_uuid=uuid, run_kind=kind) except: k8s_operation = None <IF_STMT> retry += 1 time.sleep(retry) else: return sys.exit(1)",if k8s_operation:
def _get_data_fields(): global supported_kinds ret = [] for data in supported_kinds: msg = ifinfmsg.ifinfo.data_map.get(data) if msg is not None: <IF_STMT> ret += [msg.nla2name(i[0]) for i in msg.nla_map] else: ret += [ifinfmsg.nla2name(i[0]) for i in msg.nla_map] return ret,"if getattr(msg, 'prefix', None) is not None:"
def loop_check(self): in_loop = [] for node in self.nodes: node.dfs_loop_status = 'DFS_UNCHECKED' for node in self.nodes: <IF_STMT> self.dfs_loop_search(node) if node.dfs_loop_status == 'DFS_LOOP_INSIDE': in_loop.append(node) for node in self.nodes: del node.dfs_loop_status return in_loop,if node.dfs_loop_status == 'DFS_UNCHECKED':
"def _find_config(args, app_desc): path = os.path.join(args.galaxy_root, app_desc.destination) if not os.path.exists(path): path = None for possible_ini_config_rel in app_desc.config_paths: possible_ini_config = os.path.join(args.galaxy_root, possible_ini_config_rel) <IF_STMT> path = possible_ini_config if path is None: _warn(USING_SAMPLE_MESSAGE % path) path = os.path.join(args.galaxy_root, app_desc.sample_destination) return path",if os.path.exists(possible_ini_config):
"def parseArgs(self, argv): if sys.version_info < (3, 4): if '-R' in argv: argv.remove('-R') self.refleak = True <IF_STMT> argv.remove('-m') self.multiprocess = True super(NumbaTestProgram, self).parseArgs(argv) if self.verbosity <= 0: self.buffer = True",if '-m' in argv:
"def filter_custom_selected_callback(indices, old, new): logger.info('filter custom callback') filter_label.text = 'Please Wait...' global all_topics, apply_filter if new != [-1]: apply_filter = True selected_topics = [filter_custom_table_source.data['topics'][x] for x in new] for i, line in enumerate(all_topics): <IF_STMT> all_topics[i][2] = '1' else: all_topics[i][2] = '0' filter_label.text = ''",if line[0] in selected_topics:
"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): if name not in skip: name = '__%s__' % name <IF_STMT> res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in list(self.unops.items()): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)","if hasattr(a, name):"
"def reader_matches(self, text): text = text[1:] matches = [] for p in self.reader_path: for k in p.keys(): <IF_STMT> if k.startswith(text): matches.append('#{}'.format(k)) return matches","if isinstance(k, string_types):"
"def load_templates(templates: List[JobTemplateConfig]) -> None: handlers = {TemplateSubmitHandler: build_template_func} for handler in handlers: for name in dir(handler): <IF_STMT> continue delattr(handler, name) for template in templates: setattr(handler, template.name, handlers[handler](template))",if name.startswith('_'):
"def scan_resource_conf(self, conf): if 'properties' in conf: if 'supportsHttpsTrafficOnly' in conf['properties']: if str(conf['properties']['supportsHttpsTrafficOnly']).lower() == 'true': return CheckResult.PASSED else: return CheckResult.FAILED if 'apiVersion' in conf: year = int(conf['apiVersion'][0:4]) <IF_STMT> return CheckResult.FAILED else: return CheckResult.PASSED return CheckResult.FAILED",if year < 2019:
"def gather_failed_tests(output): if output.upper() == 'NONE': return [] gatherer = GatherFailedTests() tests_or_tasks = 'tests or tasks' try: suite = ExecutionResult(output, include_keywords=False).suite suite.visit(gatherer) tests_or_tasks = 'tests' if not suite.rpa else 'tasks' <IF_STMT> raise DataError('All %s passed.' % tests_or_tasks) except: raise DataError(""Collecting failed %s from '%s' failed: %s"" % (tests_or_tasks, output, get_error_message())) return gatherer.tests",if not gatherer.tests:
"def ds_leak(): print('Testing vlens for dataset r/w') print('-----------------------------') with h5py.File(FNAME, 'w') as f: ds = f.create_dataset('dset', (1000,), dtype=dt) for idx in range(500): <IF_STMT> print_memory() ds[...] = data ds[...]",if idx % 100 == 0:
"def extract_geth_traces(input, batch_size, output, max_workers): """"""Extracts geth traces from JSON lines file."""""" with smart_open(input, 'r') as geth_traces_file: <IF_STMT> traces_iterable = (json.loads(line) for line in geth_traces_file) else: traces_iterable = (trace for trace in csv.DictReader(geth_traces_file)) job = ExtractGethTracesJob(traces_iterable=traces_iterable, batch_size=batch_size, max_workers=max_workers, item_exporter=traces_item_exporter(output)) job.run()",if input.endswith('.json'):
"def save_project_as(): if PROJECT().last_save_path != None: open_dir = os.path.dirname(PROJECT().last_save_path) <IF_STMT> open_dir = expanduser('~') else: open_dir = expanduser('~') dialogs.save_project_as_dialog(_save_as_dialog_callback, PROJECT().name, open_dir)",if open_dir.startswith(userfolders.get_cache_dir()) == True:
def _skip_to_next_iteration_group(self): while True: <IF_STMT> pass elif self._tgtkey is self._marker: break elif not self._tgtkey == self._currkey: break newvalue = next(self._iterator) if self._keyfunc is None: newkey = newvalue else: newkey = self._keyfunc(newvalue) self._currkey = newkey self._currvalue = newvalue,if self._currkey is self._marker:
"def extractNames(self, names): offset = names['offset'].value for header in names.array('header'): key = header['nameID'].value foffset = offset + header['offset'].value field = names.getFieldByAddress(foffset * 8) if not field or not isString(field): continue value = field.value if key not in self.NAMEID_TO_ATTR: continue key = self.NAMEID_TO_ATTR[key] <IF_STMT> value = value[8:] setattr(self, key, value)",if key == 'version' and value.startswith(u'Version '):
"def visit_BoolOp(self, node): for i, value in enumerate(node.values): <IF_STMT> self.visit(value) else: self.visit(value) self.visit(node.op)",if i == len(node.values) - 1:
"def list_sparkline_type_id_values(date_range_sparkline, correlation_type, type_id, key_id): sparklines_value = [] for date_day in date_range_sparkline: nb_seen_this_day = r_serv_metadata.hget('{}:{}:{}'.format(correlation_type, type_id, date_day), key_id) <IF_STMT> nb_seen_this_day = 0 sparklines_value.append(int(nb_seen_this_day)) return sparklines_value",if nb_seen_this_day is None:
"def find_nameless_urls(self, conf): nameless = [] patterns = self.get_patterns(conf) for u in patterns: if self.has_patterns(u): nameless.extend(self.find_nameless_urls(u)) el<IF_STMT> nameless.append(u) return nameless",if u.name is None:
"def find_zone_id(domain, client=None): paginator = client.get_paginator('list_hosted_zones') zones = [] for page in paginator.paginate(): for zone in page['HostedZones']: if domain.endswith(zone['Name']) or (domain + '.').endswith(zone['Name']): <IF_STMT> zones.append((zone['Name'], zone['Id'])) if not zones: raise ValueError('Unable to find a Route53 hosted zone for {}'.format(domain)) return zones[0][1]",if not zone['Config']['PrivateZone']:
"def _lookup_reference(self, reference): if not reference.startswith('#/'): return path = reference[2:].split('/') pointer = self.swagger for component in path: <IF_STMT> raise IndexError(""Can't find location by reference %r at part %r"" % (reference, component)) pointer = pointer[component] self.log.debug('Found by reference %r: %r', reference, pointer) return pointer",if component not in pointer:
"def read_line_from_file(ff): line = b'' while True: vv = ff.read_data(1)[0] if vv.symbolic: break ct = bytes(chr(vv.args[0]), 'utf-8') <IF_STMT> break line += ct return line",if ct == b'\n':
"def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker='sphere'): """"""Show N random gaussian distributed points using a scatter plot."""""" import ipyvolume as ipv rng = np.random.RandomState(seed) x, y, z = rng.normal(size=(3, N)) if draw: if color: mesh = ipv.scatter(x, y, z, marker=marker, color=color) else: mesh = ipv.scatter(x, y, z, marker=marker) <IF_STMT> ipv.show() return mesh else: return (x, y, z)",if show:
"def test_read_only_directory(self): with _inside_empty_temp_dir(): oldmode = mode = os.stat(tempfile.tempdir).st_mode mode &= ~(stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH) os.chmod(tempfile.tempdir, mode) try: <IF_STMT> self.skipTest(""can't set the directory read-only"") with self.assertRaises(PermissionError): self.make_temp() self.assertEqual(os.listdir(tempfile.tempdir), []) finally: os.chmod(tempfile.tempdir, oldmode)","if os.access(tempfile.tempdir, os.W_OK):"
"def is_checked_sls_template(template): if template.__contains__('provider'): if isinstance(template['provider'], dict_node): if template['provider'].get('name').lower() not in SUPPORTED_PROVIDERS: return False <IF_STMT> if template['provider'] not in SUPPORTED_PROVIDERS: return False return True return False","if isinstance(template['provider'], str_node):"
"def detail(self, req): resp_backup = super(BackupsController, self).detail(req) context = req.environ['cinder.context'] req_version = req.api_version_request if req_version.matches(mv.BACKUP_PROJECT): <IF_STMT> for bak in resp_backup['backups']: self._add_backup_project_attribute(req, bak) if req_version.matches(mv.BACKUP_PROJECT_USER_ID): if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False): for bak in resp_backup['backups']: self._add_backup_user_attribute(req, bak) return resp_backup","if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False):"
"def genConditional(self): for i in range(3): x = 0 try: <IF_STMT> continue x = 1 finally: for j in range(x, x + 2): yield j",if i == 2:
def _cacheAffectedBones(self): self._affectedBones = [] for f_idx in range(self.nFrames): frameData = self.getAtFramePos(f_idx) self._affectedBones.append([]) for b_idx in range(self.nBones): <IF_STMT> self._affectedBones[f_idx].append(b_idx),if not isRest(frameData[b_idx]):
"def load_metrics(self, filename, config_dict): if 'metrics' in config_dict: metrics = config_dict['metrics'] <IF_STMT> error(""c['metrics'] must be a dictionary"") else: self.metrics = metrics","if not isinstance(metrics, dict):"
"def _decode_list_response(response: Iterable[Any], decode: bool) -> Any: if decode is True: new_response = [] for val in response: <IF_STMT> val = val.decode('utf-8') new_response.append(val) return new_response return response","if isinstance(val, bytes):"
"def _np_convert_in_place(d): """"""Convert any jax devicearray leaves to numpy arrays in place."""""" if isinstance(d, dict): for k, v in d.items(): <IF_STMT> d[k] = np.array(v) elif isinstance(v, dict): _np_convert_in_place(v) elif isinstance(d, jax.xla.DeviceArray): return np.array(d) return d","if isinstance(v, jax.xla.DeviceArray):"
"def reader(): with tarfile.open(filename, mode='r') as f: names = (each_item.name for each_item in f if sub_name in each_item.name) while True: for name in names: <IF_STMT> batch = pickle.load(f.extractfile(name)) else: batch = pickle.load(f.extractfile(name), encoding='bytes') for item in read_batch(batch): yield item if not cycle: break",if six.PY2:
"def _Determine_Do(self): self.applicable = 1 method = 'moz-src' method_arg = None for opt, optarg in self.chosenOptions: if opt == '--moz-src': method = 'moz-src' <IF_STMT> method = 'moz-objdir' method_arg = optarg if method == 'moz-src': self.value = self._get_mozilla_objdir() elif method == 'moz-objdir': self.value = self._use_mozilla_objdir(method_arg) else: raise black.configure.ConfigureError('bogus method: %r' % method) self.determined = 1",elif opt == '--moz-objdir':
"def close_all(map=None, ignore_all=False): if map is None: map = socket_map for x in list(map.values()): try: x.close() except OSError as x: if x.args[0] == EBADF: pass <IF_STMT> raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()",elif not ignore_all:
"def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None): del debug_context for attribute_name, attribute in six.iteritems(self._attributes): attribute_value = attribute.to_xml_string(prefix_root) <IF_STMT> xml_element.set(attribute_name, self.full_identifier) elif attribute_value is None: continue else: xml_element.set(attribute_name, attribute_value)",if attribute_name == self._spec.identifier and attribute_value is None:
"def parse(s): """"""Parse the output below to create a new StopWatch."""""" stopwatch = StopWatch() for line in s.splitlines(): if line.strip(): parts = line.split(None) name = parts[0] <IF_STMT> rest = (float(v) for v in parts[2:]) stopwatch.times[parts[0]].merge(Stat.build(*rest)) return stopwatch",if name != '%':
"def reverse_adjust_line_according_to_hunks(self, hunks, line): for hunk in reversed(hunks): head_start = hunk.head_start saved_start = hunk.saved_start if hunk.saved_length == 0: saved_start += 1 <IF_STMT> saved_start -= 1 head_end = head_start + hunk.head_length saved_end = saved_start + hunk.saved_length if saved_end <= line: return head_end + line - saved_end elif saved_start <= line: return head_start return line",elif hunk.head_length == 0:
"def add(self, *args): self._digest = None llt = Hasher.list_like_types for arg in args: t = type(arg) <IF_STMT> self._hasher.update(bytes(f'{llt[t]} {len(arg)}', 'utf8')) self.add(*arg) else: self._hasher.update(bytes(str(arg), 'utf8'))",if t in llt:
"def filter(self, qs, value): if value: if value.start is not None and value.stop is not None: value = (value.start, value.stop) <IF_STMT> self.lookup_expr = 'startswith' value = value.start elif value.stop is not None: self.lookup_expr = 'endswith' value = value.stop return super().filter(qs, value)",elif value.start is not None:
"def _getResourceData(self, jid, dataname): """"""Return specific jid's resource representation in internal format. Used internally."""""" if jid.find('/') + 1: jid, resource = jid.split('/', 1) if self._data[jid]['resources'].has_key(resource): return self._data[jid]['resources'][resource][dataname] elif self._data[jid]['resources'].keys(): lastpri = -129 for r in self._data[jid]['resources'].keys(): <IF_STMT> resource, lastpri = (r, int(self._data[jid]['resources'][r]['priority'])) return self._data[jid]['resources'][resource][dataname]",if int(self._data[jid]['resources'][r]['priority']) > lastpri:
"def OnGetText(self, node_id): try: ea, rows = self[node_id] <IF_STMT> colour = self.colours[ea] else: colour = 16777215 ret = [] for row in rows: ret.append(row[2]) label = '\n'.join(ret) return (label, colour) except: print('GraphViewer.OnGetText:', sys.exc_info()[1]) return ('ERROR', 0)",if ea in self.colours:
"def _apply_scales(array, scales, dtype): """"""Apply scales to the array."""""" new_array = np.empty(array.shape, dtype) for i in array.dtype.names: try: new_array[i] = array[i] * scales[i] except TypeError: <IF_STMT> new_array[i] = array[i] else: raise return new_array",if np.all(scales[i] == 1):
"def run(self): self.running = True while self.running: errCode, bytes, key, overlapped = GetQueuedCompletionStatus(self.io_req_port, INFINITE) if key == ISAPI_SHUTDOWN and overlapped is None: break dispatcher = self.extension.dispatch_map.get(key) <IF_STMT> raise RuntimeError(""Bad request '%s'"" % (key,)) dispatcher(errCode, bytes, key, overlapped)",if dispatcher is None:
"def on_task_filter(self, task, config): if task.options.learn: log.info('Plugin limit_new is disabled with --learn') return amount = config for index, entry in enumerate(task.accepted): <IF_STMT> log.verbose('Allowed %s (%s)' % (entry['title'], entry['url'])) else: entry.reject('limit exceeded') plugin.get('backlog', self).add_backlog(task, entry) log.debug('Rejected: %s Allowed: %s' % (len(task.accepted[amount:]), len(task.accepted[:amount])))",if index < amount:
"def initialize_pairs(self): self._defined_pairs['WHITE_BLACK'] = (0, curses.COLOR_WHITE, curses.COLOR_BLACK) for cp in self.__class__._colors_to_define: <IF_STMT> continue self.initalize_pair(cp[0], cp[1], cp[2])",if cp[0] == 'WHITE_BLACK':
"def get_story_task_body(payload: Dict[str, Any], action: str) -> str: primary_action = get_action_with_primary_id(payload) kwargs = {'task_description': primary_action['description'], 'action': action} for a in payload['actions']: <IF_STMT> kwargs['name_template'] = STORY_NAME_TEMPLATE.format(name=a['name'], app_url=a['app_url']) return STORY_TASK_TEMPLATE.format(**kwargs)",if a['entity_type'] == 'story':
"def _key_remap(key, keys, item): elements_list = [] for r_item in item.get(key, []): element = {} for r_outkey, r_inkey in six.iteritems(keys): <IF_STMT> element[r_outkey] = r_item.get(r_inkey) elements_list.append(element) return elements_list",if r_inkey in r_item:
"def fix_identities(self, uniq=None): """"""Make pattern-tree tips point to same object if they are equal."""""" if not hasattr(self, 'children'): return self uniq = list(set(self.flat())) if uniq is None else uniq for i, c in enumerate(self.children): <IF_STMT> assert c in uniq self.children[i] = uniq[uniq.index(c)] else: c.fix_identities(uniq)","if not hasattr(c, 'children'):"
"def _apply_main_args(main_args, exec_args): i = 0 while i < len(exec_args): <IF_STMT> exec_args[i:i + 1] = main_args i += len(main_args) i += 1",if exec_args[i] == '${main_args}':
"def _clean_text(self, text): """"""Performs invalid character removal and whitespace cleanup on text."""""" output = [] char_idx = [] for i, char in enumerate(text): cp = ord(char) <IF_STMT> continue if _is_whitespace(char): output.append(' ') char_idx.append(i) else: output.append(char) char_idx.append(i) return (''.join(output), char_idx)",if cp == 0 or cp == 65533 or _is_control(char):
"def upgrade_state_dict_named(self, state_dict, name): prefix = name + '.' if name != '' else '' for k, v in state_dict.items(): if k.endswith(prefix + 'weight'): <IF_STMT> state_dict[k] = v.squeeze(1)",if v.dim() == 3 and v.size(1) == 1:
"def fetch_with_retry(self): for i in range(self.max_retries): try: self.is_truncated, self.next_marker = self._fetch() except ServerError as e: <IF_STMT> raise if i == self.max_retries - 1: raise else: return",if e.status // 100 != 5:
"def hg_hook(ui, repo, node=None, **kwargs): """"""Run pylama after mercurial commit."""""" seen = set() paths = [] if len(repo): for rev in range(repo[node], len(repo)): for file_ in repo[rev].files(): file_ = op.join(repo.root, file_) <IF_STMT> continue seen.add(file_) paths.append(file_) options = parse_options() setup_logger(options) if paths: process_paths(options, candidates=paths)",if file_ in seen or not op.exists(file_):
"def test_playlist_items(self): playlists = self.spotify.user_playlists(self.username, limit=5) self.assertTrue('items' in playlists) for playlist in playlists['items']: <IF_STMT> continue pid = playlist['id'] results = self.spotify.playlist_items(pid) self.assertEqual(len(results['items']), 0)",if playlist['uri'] != self.new_playlist_uri:
"def update_execute_option_setting(css_selector_of_option_status, css_selector_of_option): retry = 3 check_status = self.driver.find_element_by_css_selector(css_selector_of_option_status) if 'visibility-hidden' not in check_status.get_attribute('class'): while retry > 0: self.find_by_css_selector(css_selector_of_option).click() time.sleep(0.2) <IF_STMT> break else: retry -= 1",if 'visibility-hidden' in check_status.get_attribute('class'):
"def _validate_config(self): for item in ['to', 'cc', 'bcc']: if item in self.app.config.keys(self._meta.config_section): value = self.app.config.get(self._meta.config_section, item) <IF_STMT> value_list = value.split(',') value_list = [x.strip() for x in value_list] self.app.config.set(self._meta.config_section, item, value_list)",if type(value) is str:
"def cell_func(combo, render, model, iter_, *args): value = model.get_value(iter_) if value is None: text = escape(_('System Default')) else: <IF_STMT> value = u'en' text = ""%s <span weight='light'>(%s)</span>"" % (escape(value), escape(iso639.translate(value.split('_', 1)[0]))) render.set_property('markup', text)",if value == u'C':
"def _get_all_tasks(): proc = Popen(['yarn', '--help'], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if 'Commands:' in line: should_yield = True continue <IF_STMT> yield line.split(' ')[-1]",if should_yield and '- ' in line:
"def _staged_model_references(self, load_relationships=False): for name, field in self._fields.items(): if isinstance(field, BaseRelationship): try: if load_relationships: value = getattr(self, name) else: value = self.data_store.get(name, ('staged', 'committed')) except (AttributeError, KeyError, PathResolutionError): continue if value is None: continue <IF_STMT> value = [value] for related in value: related_name = field.related_name yield (related, related_name)","if not isinstance(value, ModelCollection):"
"def get_all_fix_names(fixer_pkg, remove_prefix=True): """"""Return a sorted list of all available fix names in the given package."""""" pkg = __import__(fixer_pkg, [], [], ['*']) fixer_dir = os.path.dirname(pkg.__file__) fix_names = [] for name in sorted(os.listdir(fixer_dir)): if name.startswith('fix_') and name.endswith('.py'): <IF_STMT> name = name[4:] fix_names.append(name[:-3]) return fix_names",if remove_prefix:
"def extract_info_to_dest(self, info, dest): """"""Extracts the given info to a directory and checks the file size."""""" self.zip_file.extract(info, dest) dest = os.path.join(dest, info.filename) if not os.path.isdir(dest): size = os.stat(dest)[stat.ST_SIZE] <IF_STMT> log.error('Extraction error, uncompressed size: %s, %s not %s' % (self.source, size, info.file_size)) raise forms.ValidationError(gettext('Invalid archive.'))",if size != info.file_size:
"def _close_brackets(self, fragment): stack = [] for char in fragment: <IF_STMT> stack.append(char) elif char in self._PARENS.values(): if stack and self._PARENS[stack[-1]] == char: stack.pop() else: return '' return ''.join((self._PARENS[paren] for paren in reversed(stack)))",if char in self._PARENS.keys():
"def __call__(self, input_tensors, shape): if self.order in 'KA': if any((t.order == TensorOrder.C_ORDER for t in input_tensors)): order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER el<IF_STMT> order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)",if self.order == 'C':
def __iter__(self): iteration = self.start_iter while iteration <= self.num_iterations: <IF_STMT> self.batch_sampler.sampler.set_epoch(iteration) for batch in self.batch_sampler: iteration += 1 if iteration > self.num_iterations: break yield batch,"if hasattr(self.batch_sampler.sampler, 'set_epoch'):"
def all_pairs_shortest_path(adjacency_matrix): new_array = copy.deepcopy(adjacency_matrix) for k in range(len(new_array)): for i in range(len(new_array)): for j in range(len(new_array)): <IF_STMT> new_array[i][j] = new_array[i][k] + new_array[k][j] return new_array,if new_array[i][j] > new_array[i][k] + new_array[k][j]:
"def cancel_pp(self, nzo_id): """"""Change the status, so that the PP is canceled"""""" for nzo in self.history_queue: if nzo.nzo_id == nzo_id: nzo.abort_direct_unpacker() <IF_STMT> nzo.pp_active = False try: self.external_process.kill() logging.info('Killed external process %s', self.external_process.args[0]) except: pass return True return None",if nzo.pp_active:
"def cvPreprocess(): import cv2 imgarr_orig = [] image_ext_list = ['.jpg', '.png', '.JPEG', '.jpeg', '.PNG', '.JPG'] for file in onlyfiles: fimg = imgroot + file if any([x in image_ext_list for x in fimg]): print(fimg + ' is not an image file') continue img1 = cv2.imread(fimg) <IF_STMT> print('ERROR opening ', fimg) continue img1 = cv2.resize(img1, (896, 896)) imgarr_orig.append(img1) return imgarr_orig",if img1 is None:
"def substituteargs(self, pattern, replacement, old): new = [] for k in range(len(replacement)): item = replacement[k] newitem = [item[0], item[1], item[2]] for i in range(3): if item[i] == '*': newitem[i] = old[k][i] <IF_STMT> index = int(item[i][1:]) - 1 newitem[i] = old[index][i] new.append(tuple(newitem)) return new",elif item[i][:1] == '$':
"def process(self, profile): contributors = self.createContributors(profile) for contributor in contributors: <IF_STMT> reasons = self.createExecSqlNodeReason(contributor, profile) else: reasons = self.createExecNodeReason(contributor, profile) contributor.reason = reasons return contributors",if contributor.type == 'SQLOperator':
"def showImage(filename): osName = platform.system() if osName == 'Windows': subprocess.Popen([filename], shell=True) elif osName == 'Linux': LINUX_DISPLAY_COMMAND = ('xdg-open', 'display', 'gvfs-open', 'shotwell') commands = list(filter(HasCommand, LINUX_DISPLAY_COMMAND)) <IF_STMT> subprocess.Popen([commands[0], filename]) else: raise elif osName == 'Darwin': subprocess.Popen(['open', filename]) else: raise Exception('other system')",if commands:
"def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) <IF_STMT> if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail('FATAL: bad directory %s in environment variable %s' % (dir, envvar))",if os.path.isdir(dir):
"def add(self, state): if state.key in self: <IF_STMT> raise sa_exc.InvalidRequestError(""Can't attach instance %s; another instance with key %s is already present in this session."" % (orm_util.state_str(state), state.key)) return False else: self._dict[state.key] = state.obj() self._manage_incoming_state(state) return True",if attributes.instance_state(self._dict[state.key]) is not state:
"def request(self, stream=None, tty=None, demux=None): assert stream is not None and tty is not None and (demux is not None) with APIClient(base_url=self.address, version=DEFAULT_DOCKER_API_VERSION) as client: <IF_STMT> url = client._url('/tty') else: url = client._url('/no-tty') resp = client._post(url, stream=True) return client._read_from_socket(resp, stream=stream, tty=tty, demux=demux)",if tty:
"def select(model, path, iter_, paths_): paths, first = paths_ value = model.get_value(iter_) if value is None: return not bool(paths) value = normalize_path(value) if value in paths: self.get_child().get_selection().select_path(path) paths.remove(value) <IF_STMT> self.get_child().set_cursor(path) first.append(path.copy()) else: for fpath in paths: if fpath.startswith(value): self.get_child().expand_row(path, False) return not bool(paths)",if not first:
"def _validate(self, qobj): for experiment in qobj.experiments: <IF_STMT> logger.warning(""no measurements in circuit '%s', classical register will remain all zeros."", experiment.header.name)",if 'measure' not in [op.name for op in experiment.instructions]:
"def exitval_from_opts(options, project): exit_value_from = options.get('--exit-code-from') if exit_value_from: if not options.get('--abort-on-container-exit'): log.warning('using --exit-code-from implies --abort-on-container-exit') options['--abort-on-container-exit'] = True <IF_STMT> log.error('No service named ""%s"" was found in your compose file.', exit_value_from) sys.exit(2) return exit_value_from",if exit_value_from not in [s.name for s in project.get_services()]:
"def __call__(self, tokens, reader): first_return = False for token in tokens: <IF_STMT> reader.context.current_function.exit_count = 1 first_return = True if token == 'return': if first_return: first_return = False else: reader.context.current_function.exit_count += 1 yield token","if not hasattr(reader.context.current_function, 'exit_count'):"
"def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: if len(spec) == 2: event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec if register_type is handlers.REGISTER_FIRST: self._events.register_first(event_name, handler) <IF_STMT> self._events.register_last(event_name, handler)",elif register_type is handlers.REGISTER_LAST:
def test_sql(self): with self.get_temp() as temp: railroad = to_railroad(simpleSQL) assert len(railroad) == 7 temp.write(railroad_to_html(railroad)) <IF_STMT> print('sql: ' + temp.name),if self.railroad_debug():
"def resources_to_link(self, resources): if isinstance(self.Bucket, dict) and 'Ref' in self.Bucket: bucket_id = self.Bucket['Ref'] if not isinstance(bucket_id, string_types): raise InvalidEventException(self.relative_id, ""'Ref' value in S3 events is not a valid string."") <IF_STMT> return {'bucket': resources[bucket_id], 'bucket_id': bucket_id} raise InvalidEventException(self.relative_id, 'S3 events must reference an S3 bucket in the same template.')",if bucket_id in resources:
"def list_target_unit_files(self, *modules): """"""show all the target units and the enabled status"""""" result = {} enabled = {} for unit in _all_common_targets: result[unit] = None enabled[unit] = 'static' if unit in _all_common_enabled: enabled[unit] = 'enabled' <IF_STMT> enabled[unit] = 'enabled' return [(unit, enabled[unit]) for unit in sorted(result)]",if unit in _all_common_disabled:
"def teardown_network_port(self): """"""tearDown for Network and Port table"""""" networks = self.quantum.get_all_networks('t1') for net in networks: netid = net['net-id'] name = net['net-name'] <IF_STMT> ports = self.quantum.get_all_ports(netid) for por in ports: self.quantum.delete_port(netid, por['port-id']) self.quantum.delete_network(netid)",if 'net' in name:
"def findConfigFiles(self, cfg_args): """"""Find available config files"""""" filenames = cfg_args.config[:] proj_opts = ('unittest.cfg', 'nose2.cfg') for fn in proj_opts: <IF_STMT> fn = os.path.abspath(os.path.join(cfg_args.top_level_directory, fn)) filenames.append(fn) if cfg_args.user_config: user_opts = ('~/.unittest.cfg', '~/.nose2.cfg') for fn in user_opts: filenames.append(os.path.expanduser(fn)) return filenames",if cfg_args.top_level_directory:
"def make_aware(value): if settings.USE_TZ: <IF_STMT> value = timezone.make_aware(value, timezone.utc) default_tz = timezone.get_default_timezone() value = timezone.localtime(value, default_tz) return value",if timezone.is_naive(value):
"def update(id): """"""Update a post if the current user is the author."""""" post = get_post(id) if request.method == 'POST': title = request.form['title'] body = request.form['body'] error = None if not title: error = 'Title is required.' <IF_STMT> flash(error) else: post.title = title post.body = body db.session.commit() return redirect(url_for('blog.index')) return render_template('blog/update.html', post=post)",if error is not None:
"def copyfileobj(src, dest, length=512): if hasattr(src, 'readinto'): buf = bytearray(length) while True: sz = src.readinto(buf) if not sz: break if sz == length: dest.write(buf) else: b = memoryview(buf)[:sz] dest.write(b) else: while True: buf = src.read(length) <IF_STMT> break dest.write(buf)",if not buf:
"def imgFileProcessingTick(output): if isinstance(output, tuple): workerOutput.append(output) workerPool.terminate() else: for page in output: if page is not None: options.imgMetadata[page[0]] = page[1] options.imgOld.append(page[2]) if GUI: GUI.progressBarTick.emit('tick') <IF_STMT> workerPool.terminate()",if not GUI.conversionAlive:
"def process_word(word): if word.parent == 'remapping': raise UDError('There is a cycle in a sentence') if word.parent is None: head = int(word.columns[HEAD]) if head > len(ud.words) - sentence_start: raise UDError(""HEAD '{}' points outside of the sentence"".format(word.columns[HEAD])) <IF_STMT> parent = ud.words[sentence_start + head - 1] word.parent = 'remapping' process_word(parent) word.parent = parent",if head:
def validate_export(namespace): destination = namespace.destination if destination == 'file': <IF_STMT> raise CLIError('usage error: --path PATH --format FORMAT') elif destination == 'appconfig': if namespace.dest_name is None and namespace.dest_connection_string is None: raise CLIError('usage error: --config-name NAME | --connection-string STR') elif destination == 'appservice': if namespace.appservice_account is None: raise CLIError('usage error: --appservice-account NAME_OR_ID'),if namespace.path is None or namespace.format_ is None:
"def get_change_set_status(context, stack_name, change_set_name): try: response = retry_boto_call(context.client.describe_change_set, ChangeSetName=change_set_name, StackName=stack_name) except ClientError as e: <IF_STMT> return None else: raise e return response['Status']",if e.response['Error']['Code'] == 'ChangeSetNotFound':
"def predict(self, predict_data): assert self.predict_fn is not None with Timer() as t: <IF_STMT> self.predictions = self.predict_fn(predict_data) else: self.predictions = self.predict_fn(predict_data, concatenate_outputs=False) if not self.batch_benchmark: self.predictions = np.concatenate(self.predictions) return t.interval",if self.batch_benchmark:
"def __str__(self): s = '(' + str(self[0]) s += ', ' if isinstance(self[1], Tensor): if self[1].name and self[1].name is not None: s += self[1].name else: s += 'tensor-' + hex(id(self[1])) else: s += str(self[1]) s += ', ' if isinstance(self[2], Tensor): <IF_STMT> s += self[2].name else: s += 'tensor-' + hex(id(self[2])) else: s += str(self[2]) s += ')' return s",if self[2].name and self[2].name is not None:
"def get_local_cache(self, past, data, from_file, temp_id): """"""parse individual cached geometry if there is any"""""" cache = [] if self.accumulative: <IF_STMT> cache = past[temp_id] if not from_file and len(data) > 0: cache = data.get(temp_id, []) return cache",if from_file and len(past) > 0:
def get_mappings(index): mappings = {} from kitsune.search.models import get_mapping_types for cls in get_mapping_types(): group = cls.get_index_group() <IF_STMT> mappings[cls.get_mapping_type_name()] = cls.get_mapping() return mappings,if index == write_index(group) or index == read_index(group):
"def find_first_of_filetype(content, filterfiltype, attr='name'): """"""Find the first of the file type."""""" filename = '' for _filename in content: <IF_STMT> if _filename.endswith(f'.{filterfiltype}'): filename = _filename break elif getattr(_filename, attr).endswith(f'.{filterfiltype}'): filename = getattr(_filename, attr) break return filename","if isinstance(_filename, str):"
"def _timer(duetime: typing.AbsoluteOrRelativeTime, period: Optional[typing.RelativeTime]=None, scheduler: Optional[typing.Scheduler]=None) -> Observable: if isinstance(duetime, datetime): <IF_STMT> return observable_timer_date(duetime, scheduler) else: return observable_timer_duetime_and_period(duetime, period, scheduler) if period is None: return observable_timer_timespan(duetime, scheduler) return observable_timer_timespan_and_period(duetime, period, scheduler)",if period is None:
"def __getattribute__(self, attrname): result = object.__getattribute__(self, attrname) <IF_STMT> try: self._read_info(attrname) except Exception as e: logging.warning(""An error '%s' was raised while decoding '%s'"", e, repr(self.path)) result = object.__getattribute__(self, attrname) if result is NOT_SET: result = self.INITIAL_INFO[attrname] return result",if result is NOT_SET:
"def on_btOK_clicked(self, *a): """"""Handler for OK button"""""" if self.ac_callback is not None: self._set_title() <IF_STMT> self.ac_callback(self.id, self) else: a = self.generate_modifiers(self._action, self._selected_component.NAME == 'custom') self.ac_callback(self.id, a) self.ac_callback = None if self._selected_component: self._selected_component.on_ok(a) self.close()",if self._mode == ActionEditor.AEC_MENUITEM:
"def execute(): if frappe.db.get_value('Company', {'country': 'India'}, 'name'): address_template = frappe.db.get_value('Address Template', 'India', 'template') <IF_STMT> set_up_address_templates(default_country='India')",if not address_template or 'gstin' not in address_template:
"def is_ncname(name): first = name[0] if first == '_' or category(first) in NAME_START_CATEGORIES: for i in xrange(1, len(name)): c = name[i] if not category(c) in NAME_CATEGORIES: <IF_STMT> continue return 0 return 1 else: return 0",if c in ALLOWED_NAME_CHARS:
"def _get_sonnet_version(): with open('sonnet/__init__.py') as fp: for line in fp: <IF_STMT> g = {} exec(line, g) return g['__version__'] raise ValueError('`__version__` not defined in `sonnet/__init__.py`')",if line.startswith('__version__'):
def disjoined(self): gridscope = GridScope(globals=self.globals) for key in self.user_added: value = self[key] <IF_STMT> grid = vaex.utils.disjoined(value) gridscope[key] = grid else: gridscope[key] = value return gridscope,"if isinstance(value, np.ndarray):"
def _maybe_uncompress(self): if not self._decompressed: compression_type = self.compression_type <IF_STMT> data = memoryview(self._buffer)[self._pos:] if compression_type == self.CODEC_GZIP: uncompressed = gzip_decode(data) if compression_type == self.CODEC_SNAPPY: uncompressed = snappy_decode(data.tobytes()) if compression_type == self.CODEC_LZ4: uncompressed = lz4_decode(data.tobytes()) self._buffer = bytearray(uncompressed) self._pos = 0 self._decompressed = True,if compression_type != self.CODEC_NONE:
"def read_chat_forever(reader, pub_socket): line = reader.readline() who = 'someone' while line: print('Chat:', line.strip()) <IF_STMT> who = line.split(':')[-1].strip() try: pub_socket.send_pyobj((who, line)) except socket.error as e: if e[0] != 32: raise line = reader.readline() print('Participant left chat.')",if line.startswith('name:'):
"def items(self, section=None): section = section if section is not None else Settings.DEFAULT_SECTION result = {'section': section} try: <IF_STMT> for option in self._global_settings.options(section): result[option] = self._global_settings.get(section, option) if section in self._local_settings.sections(): for option in self._local_settings.options(section): result[option] = self._local_settings.get(section, option) except configparser.InterpolationSyntaxError: core.termwarn('Unable to parse settings file') return result",if section in self._global_settings.sections():
"def before_train(self, program): """"""doc"""""" if self.summary_record: if self.summary_record.scalar: self.s_name, self.s_tolog = zip(*self.summary_record.scalar) else: self.s_name, self.s_tolog = ([], []) <IF_STMT> self.h_name, self.h_tolog = zip(*self.summary_record.histogram) else: self.h_name, self.h_tolog = ([], [])",if self.summary_record.histogram:
"def _s3_init(self): """"""Initialize s3 bucket."""""" try: bucket_exists = (yield self._bucket_exists()) <IF_STMT> LOGGER.warning('Will attempt to create bucket') yield self._create_bucket() except botocore.exceptions.NoCredentialsError: LOGGER.error('You must set ""s3.accessKeyId"" and ""s3.secretAccessKey"", or ""s3.profile"" in your Streamlit configuration.') raise errors.S3NoCredentials",if not bucket_exists:
"def id2unit(self, id): items = [] for v, k in zip(id, self._id2unit.keys()): <IF_STMT> continue if self.keyed: items.append('{}={}'.format(k, self._id2unit[k][v])) else: items.append(self._id2unit[k][v]) res = self.sep.join(items) if res == '': res = '_' return res",if v == EMPTY_ID:
"def forward(model: TransformerListener, docs, is_train): if is_train: model.verify_inputs(docs) return (model._outputs, model.backprop_and_clear) else: <IF_STMT> outputs = [] elif any((doc._.trf_data is None for doc in docs)): width = model.get_dim('nO') outputs = [TransformerData.zeros(len(doc), width, xp=model.ops.xp) for doc in docs] else: outputs = [doc._.trf_data for doc in docs] return (outputs, lambda d_data: [])",if len(docs) == 0:
"def get_plugin_dir(shooting_dir): DIRNAME = 'lunapark' parent = os.path.abspath(os.path.join(shooting_dir, os.pardir)) if os.path.basename(parent) == DIRNAME: return parent else: plugin_dir = os.path.join(parent, DIRNAME) <IF_STMT> os.makedirs(plugin_dir) return plugin_dir",if not os.path.exists(plugin_dir):
"def _get_plugin(self, name, lang=None, check=False): if lang is None: lang = self.get_lang() if name not in self.plugin_attrib_map: return None plugin_class = self.plugin_attrib_map[name] if plugin_class.is_extension: <IF_STMT> return self.plugins[name, None] else: return None if check else self.init_plugin(name, lang) elif (name, lang) in self.plugins: return self.plugins[name, lang] else: return None if check else self.init_plugin(name, lang)","if (name, None) in self.plugins:"
"def globs_relative_to_buildroot(self): buildroot = get_buildroot() globs = [] for bundle in self.bundles: fileset = bundle.fileset if fileset is None: continue <IF_STMT> globs += bundle.fileset.filespec['globs'] else: globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()] super_globs = super().globs_relative_to_buildroot() if super_globs: globs += super_globs['globs'] return {'globs': globs}","elif hasattr(fileset, 'filespec'):"
"def running_jobs(self, exit_on_error=True): """"""Initialize multiprocessing."""""" with self.handling_exceptions(): <IF_STMT> from concurrent.futures import ProcessPoolExecutor try: with ProcessPoolExecutor(self.jobs) as self.executor: yield finally: self.executor = None else: yield if exit_on_error: self.exit_on_error()",if self.using_jobs:
"def _get_all_checkpoint_paths(self) -> List[str]: """"""Returns all the checkpoint paths managed by the instance."""""" if tf.io.gfile.exists(self._root_dir): root_dir_entries = tf.io.gfile.listdir(self._root_dir) return [os.path.join(self._root_dir, e) for e in root_dir_entries <IF_STMT>] else: return []",if e.startswith(self._prefix)
"def test_tag_priority(self): for tag in _low_priority_D_TAG: val = ENUM_D_TAG[tag] if _DESCR_D_TAG[val] == tag: for tag2 in ENUM_D_TAG: <IF_STMT> continue self.assertNotEqual(ENUM_D_TAG[tag2], val)",if tag2 == tag:
"def cycle(self, forward=True): if self.cycle_list: if forward is True: self.cycle_list.rotate(-1) <IF_STMT> self.cycle_list.rotate(1) self.move_to_obj(self.cycle_list[0])",elif forward is False:
"def __init__(self): self.keyring = None if not haveKeyring: return try: self.keyring = gnomekeyring.get_default_keyring_sync() <IF_STMT> self.keyring = 'default' try: gnomekeyring.create_sync(self.keyring, None) except gnomekeyring.AlreadyExistsError: pass except: logging.exception('Error determining keyring') self.keyring = None",if self.keyring == None:
"def _coerce_trials_data(data, path): if not isinstance(data, list): <IF_STMT> raise BatchFileError(path, 'invalid data type for trials: expected list or dict, got %s' % type(data).__name__) data = [data] for item in data: if not isinstance(item, dict): raise BatchFileError(path, 'invalid data type for trial %r: expected dict' % item) return data","if not isinstance(data, dict):"
def update(self): if self.openfilename is not None: try: current_mtime = os.stat(self.openfilename).st_mtime except OSError: return True <IF_STMT> self.last_mtime = current_mtime self.reload() return True,if current_mtime != self.last_mtime:
"def _wrap_new_compiler(*args, **kwargs): try: return func(*args, **kwargs) except errors.DistutilsPlatformError: <IF_STMT> CCompiler = _UnixCCompiler else: CCompiler = _MSVCCompiler return CCompiler(None, kwargs['dry_run'], kwargs['force'])",if not sys.platform == 'win32':
"def _run_eagerly(*inputs): with context.eager_mode(): constants = [_wrap_as_constant(value, tensor_spec) for value, tensor_spec in zip(inputs, input_signature)] output = fn(*constants) if hasattr(output, '_make'): return output._make([tensor.numpy() for tensor in output]) <IF_STMT> return [tensor.numpy() for tensor in output] else: return output.numpy()","if isinstance(output, (tuple, list)):"
"def _on_event_MetadataAnalysisFinished(self, event, data): with self._selectedFileMutex: <IF_STMT> self._setJobData(self._selectedFile['filename'], self._selectedFile['filesize'], self._selectedFile['sd'], self._selectedFile['user'])",if self._selectedFile:
"def env_asset_url_default(endpoint, values): """"""Create asset URLs dependent on the current env"""""" if endpoint == 'views.themes': path = values.get('path', '') static_asset = path.endswith('.js') or path.endswith('.css') direct_access = '.dev' in path or '.min' in path <IF_STMT> env = values.get('env', current_app.env) mode = '.dev' if env == 'development' else '.min' base, ext = os.path.splitext(path) values['path'] = base + mode + ext",if static_asset and (not direct_access):
"def __init__(self, inStr): """"""Initialize the class."""""" inStr = inStr.strip() if len(inStr) != 1 and len(inStr) != 2: raise ValueError('PosAlign: length not 2 chars' + inStr) if inStr == '..': self.aa = '-' self.gap = 1 else: self.gap = 0 self.aa = inStr[0] <IF_STMT> self.aa = 'C' if len(inStr) == 2: self.ss = inStr[1].upper() else: self.ss = '0'",if self.aa == self.aa.lower():
"def iter_ReassignParameters(self, inputNode, variables, nodeByID): for node in inputNode.getReassignParameterNodes(nodeByID): yield from iterNodeCommentLines(node) yield from iterInputConversionLines(node, variables) socket = node.inputs[0] if socket.isUnlinked and socket.isCopyable(): expression = getCopyExpression(socket, variables) else: expression = variables[socket] <IF_STMT> conditionPrefix = '' else: conditionPrefix = 'if {}: '.format(variables[node.conditionSocket]) yield '{}{} = {}'.format(conditionPrefix, variables[node.linkedParameterSocket], expression)",if node.conditionSocket is None:
"def init_weight(self): if self.pretrained is not None: load_entire_model(self, self.pretrained) else: for sublayer in self.sublayers(): if isinstance(sublayer, nn.Conv2D): kaiming_normal_init(sublayer.weight) <IF_STMT> kaiming_normal_init(sublayer.weight)","elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)):"
def logic(): while 1: <IF_STMT> yield reset.posedge for i in range(20): yield clock.posedge if enable: count.next = i j = 1 while j < 25: if enable: yield clock.posedge yield clock.posedge count.next = 2 * j j += 1,if reset == ACTIVE_LOW:
"def clean_log_messages(result_data): for idx in range(len(result_data['executePlan']['stepEvents'])): message = result_data['executePlan']['stepEvents'][idx].get('message') <IF_STMT> result_data['executePlan']['stepEvents'][idx]['message'] = re.sub('(\\d+(\\.\\d+)?)', '{N}', message) return result_data",if message is not None:
"def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: if section == NAME: return to_qvariant('Name') elif section == VERSION: return to_qvariant('Version') <IF_STMT> return to_qvariant('Action') elif section == DESCRIPTION: return to_qvariant('Description') return to_qvariant()",elif section == ACTION:
"def _gather_infos(self): if self._prev_state is not None: for attr in self._tracked_infos: self.state[attr] = self.state.get(attr) or self._prev_state.get(attr) for info in ['score', 'moves']: <IF_STMT> self.state[info] = int(self.state[info].strip()) self.state['won'] = '*** The End ***' in self.state['feedback'] self.state['lost'] = '*** You lost! ***' in self.state['feedback']",if self.state[info] is not None and type(self.state[info]) is not int:
"def calc_parity(sig, kind): if kind in ('zero', 'none'): return C(0, 1) elif kind == 'one': return C(1, 1) else: bits, _ = value_bits_sign(sig) even_parity = sum([sig[b] for b in range(bits)]) & 1 <IF_STMT> return ~even_parity elif kind == 'even': return even_parity else: assert False",if kind == 'odd':
"def tool(self, **kwds): process_definition = kwds.get('process_definition', None) if process_definition is None: raw_process_reference = kwds.get('raw_process_reference', None) <IF_STMT> raw_process_reference = self.raw_process_reference(kwds['path']) process_definition = self.process_definition(raw_process_reference) tool = load_tool.make_tool(process_definition.uri, process_definition.loading_context) return tool",if raw_process_reference is None:
def context(self): if self.template is not None: if self.template.id: return self.template.id <IF_STMT> return self.template.context return self.template.getid() return self.unescape_csv(self.mainunit.getcontext()),if self.template.context:
"def test_six_thread_safety(): _reload_six() with patch('botocore.vendored.six.moves.__class__.__setattr__', wraps=_wrapped_setattr): threads = [] for i in range(2): t = _ExampleThread() threads.append(t) t.start() while threads: t = threads.pop() t.join() <IF_STMT> six.reraise(*t.exc_info)",if t.exc_info:
"def _handle_js_events(self, change): if self.js_events: if self.eventHandlers: for event in self.js_events: event_name = event['name'] <IF_STMT> self.eventHandlers[event_name](event['detail']) self.js_events = []",if event_name in self.eventHandlers:
"def single_discriminator(x, filters=128, kernel_size=8, strides=4, pure_mean=False): """"""A simple single-layer convolutional discriminator."""""" with tf.variable_scope('discriminator'): net = layers().Conv2D(filters, kernel_size, strides=strides, padding='SAME', name='conv1')(x) <IF_STMT> net = tf.reduce_mean(net, [1, 2]) else: net = mean_with_attention(net, 'mean_with_attention') return net",if pure_mean:
"def find_path(self, from_location, to_location): end = to_location f_node = self.mh.get_node(from_location) self.on.append(f_node) self.o.append(f_node.lid) next_node = f_node counter = 0 while next_node is not None: <IF_STMT> break finish = self._handle_node(next_node, end) if finish: return self._trace_path(finish) next_node = self._get_best_open_node() counter += 1 return None",if counter > 10000:
"def format_var_dict(dct, indent=4, max_width=80): lines = [] pre = ' ' * indent for key, value in dct.items(): line = pre + key + ' = ' + repr(value) <IF_STMT> line = line[:max_width - 3] + '...' try: value_len = len(value) except: pass else: line += '\n' + pre + 'len(%s) = %s' % (key, value_len) lines.append(line) return '\n'.join(lines)",if len(line) > max_width:
"def _recursive_name_seach(self, layer_names, layer, pre_name, depth): for name, module in layer.named_children(): nname = pre_name + '_' + name if pre_name != '' else name <IF_STMT> if self._wrap_layer_check(module, name, nname): layer_names.append(nname) if self.depth is None or depth <= self.depth: if len(list(layer.named_children())) > 0: self._recursive_name_seach(layer_names, module, nname, depth + 1) return layer_names",if depth == self.depth or self.depth is None:
"def finished_at(self): f = self.metadata_get(['State', 'FinishedAt']) if f: f = f[:26] <IF_STMT> return DINOSAUR_TIME finished_at = datetime.datetime.strptime(f, ISO_DATETIME_PARSE_STRING) return finished_at",if f == '0001-01-01T00:00:00Z':
"def write_bool(self, bool): if self._bool_fid and self._bool_fid > self._last_fid and (self._bool_fid - self._last_fid <= 15): <IF_STMT> ctype = CompactType.TRUE else: ctype = CompactType.FALSE self._write_field_header(ctype, self._bool_fid) elif bool: self.write_byte(CompactType.TRUE) else: self.write_byte(CompactType.FALSE)",if bool:
"def update(self, topLeft, bottomRight): if self._updating: return if self._index: if topLeft.row() <= self._index.row() <= bottomRight.row(): self.updateText() elif self._indexes: update = False for i in self._indexes: if topLeft.row() <= i.row() <= bottomRight.row(): update = True <IF_STMT> self.updateText()",if update:
"def _preprocess_add_items(self, items): """"""Split the items into two lists of path strings and BaseEntries."""""" paths = [] entries = [] for item in items: <IF_STMT> paths.append(self._to_relative_path(item)) elif isinstance(item, (Blob, Submodule)): entries.append(BaseIndexEntry.from_blob(item)) elif isinstance(item, BaseIndexEntry): entries.append(item) else: raise TypeError('Invalid Type: %r' % item) return (paths, entries)","if isinstance(item, string_types):"
def ping_all(): for l in _all_listeners.values(): count = l.receiver.count() <IF_STMT> for dev in l.receiver: dev.ping() l._status_changed(dev) count -= 1 if not count: break,if count:
"def stage_node_dot(g, stage): """"""Create a stage node."""""" with g.subgraph(name='cluster_' + stage['id']) as subgraph: subgraph.attr(label=stage['name']) <IF_STMT> for itervar in stage['all_itervars']: iv_type = itervar['itervar_type'] itervar_node_dot(subgraph, itervar, iv_type, itervar['index']) for rel in stage['relations']: node_id = rel['id'] itervar_relation_dot(subgraph, rel, node_id) else: subgraph.node(stage['name'] + '_placeholder', style='invis')",if stage['all_itervars']:
"def run() -> None: nonlocal state, timeout while True: if timeout > 0.0: disposed.wait(timeout) <IF_STMT> return time: datetime = self.now state = action(state) timeout = seconds - (self.now - time).total_seconds()",if disposed.is_set():
"def increment(s): if not s: return '1' for sequence in (string.digits, string.lowercase, string.uppercase): lastc = s[-1] if lastc in sequence: i = sequence.index(lastc) + 1 <IF_STMT> if len(s) == 1: s = sequence[0] * 2 if s == '00': s = '10' else: s = increment(s[:-1]) + sequence[0] else: s = s[:-1] + sequence[i] return s return s",if i >= len(sequence):
"def Import(self, patch, force): if not patch.get('file'): <IF_STMT> raise PatchError('Patch file must be specified in patch import.') else: patch['file'] = bb.fetch2.localpath(patch['remote'], self.d) for param in PatchSet.defaults: if not patch.get(param): patch[param] = PatchSet.defaults[param] if patch.get('remote'): patch['file'] = self.d.expand(bb.fetch2.localpath(patch['remote'], self.d)) patch['filemd5'] = bb.utils.md5_file(patch['file'])",if not patch.get('remote'):
"def _setReadyState(self, state: str) -> None: if state != self.__readyState: self.__log_debug('- %s -> %s', self.__readyState, state) self.__readyState = state <IF_STMT> self.emit('open') elif state == 'closed': self.emit('close') self.remove_all_listeners()",if state == 'open':
def count_brokers(self): self.nb_brokers = 0 for broker in self.brokers: if not broker.spare: self.nb_brokers += 1 for realm in self.higher_realms: for broker in realm.brokers: <IF_STMT> self.nb_brokers += 1,if not broker.spare and broker.manage_sub_realms:
def _refresh(self): self.uiProfileSelectComboBox.clear() self.uiProfileSelectComboBox.addItem('default') try: <IF_STMT> for profile in sorted(os.listdir(self.profiles_path)): if not profile.startswith('.'): self.uiProfileSelectComboBox.addItem(profile) except OSError: pass,if os.path.exists(self.profiles_path):
"def run(self): for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'Dataset' and v['task_type'] == 'Communication': try: params = json.loads(v['task_type_parameters']) except json.JSONDecodeError: pass else: if len(params) == 1: params.extend(['stub', 'fifo_io']) v['task_type_parameters'] = json.dumps(params) return self.objs",if k.startswith('_'):
"def _listen(self, consumer_id: str) -> AsyncIterable[Any]: try: while True: if self._listening: async for msg in self._listen_to_queue(consumer_id): <IF_STMT> yield msg await asyncio.sleep(0.5) else: async for msg in self._listen_to_ws(): yield msg except asyncio.CancelledError: pass except Exception as e: raise e",if msg is not None:
"def recv(self, bufsiz, flags=0): d = self._sock.recv(bufsiz, flags) if self.replace_pattern and b' HTTP/1.1\r\n' in d: line_end = d.find(b'\r\n') req_line = d[:line_end] words = req_line.split() <IF_STMT> method, url, http_version = words url = url.replace(self.replace_pattern[0], self.replace_pattern[1]) d = b'%s %s %s' % (method, url, http_version) + d[line_end:] return d",if len(words) == 3:
"def Import(self, patch, force): if not patch.get('file'): if not patch.get('remote'): raise PatchError('Patch file must be specified in patch import.') else: patch['file'] = bb.fetch2.localpath(patch['remote'], self.d) for param in PatchSet.defaults: <IF_STMT> patch[param] = PatchSet.defaults[param] if patch.get('remote'): patch['file'] = self.d.expand(bb.fetch2.localpath(patch['remote'], self.d)) patch['filemd5'] = bb.utils.md5_file(patch['file'])",if not patch.get(param):
"def delete(post_id): blogging_engine = _get_blogging_engine(current_app) storage = blogging_engine.storage post = storage.get_post_by_id(post_id) if post is not None and current_user.get_id() == post['user_id']: success = storage.delete_post(post_id) <IF_STMT> flash('Your post was successfully deleted', 'info') else: flash('Something went wrong while deleting your post', 'warning') else: flash('You do not have the rights to delete this post', 'warning') return redirect(url_for('blog_app.index'))",if success:
"def update_schema_configs(state, schema): RegistrationSchema = state.get_model('osf', 'registrationschema') for rs in RegistrationSchema.objects.all(): if rs.schema.get('description', False): rs.description = rs.schema['description'] <IF_STMT> rs.config = rs.schema['config'] rs.save()","if rs.schema.get('config', False):"
"def set_payload(self, value): del self['payload'] if isinstance(value, ElementBase): <IF_STMT> self.init_plugin(value.plugin_attrib, existing_xml=value.xml) self.xml.append(value.xml) else: self.xml.append(value)",if value.tag_name() in self.plugin_tag_map:
"def getCellPropertyNames_aux(self, col_id): if col_id == 'name': if self.image_icon == 'places_busy': return ['places_busy'] baseName = self.image_icon <IF_STMT> return [baseName + '_open'] else: return [baseName + '_closed'] return []",if self.isOpen:
"def one_xmm_reg_imm8(ii): i, j, n = (0, 0, 0) for op in _gen_opnds(ii): if op_reg(op) and op_xmm(op): n += 1 <IF_STMT> i += 1 elif op_imm8_2(op): j += 1 else: return False return n == 1 and i == 1 and (j <= 1)",elif op_imm8(op):
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) <IF_STMT> self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if i == self._skip - 2:
"def assertNodeSequenceEqual(self, seq1: Sequence[cst.CSTNode], seq2: Sequence[cst.CSTNode], msg: Optional[str]=None) -> None: suffix = '' if msg is None else f'\n{msg}' if len(seq1) != len(seq2): raise AssertionError(f'\n{seq1!r}\nis not deeply equal to \n{seq2!r}{suffix}') for node1, node2 in zip(seq1, seq2): <IF_STMT> raise AssertionError(f'\n{seq1!r}\nis not deeply equal to \n{seq2!r}{suffix}')",if not node1.deep_equals(node2):
"def close(self): if self._file_writer is not None: <IF_STMT> flat_result = flatten_dict(self.last_result, delimiter='/') scrubbed_result = {k: value for k, value in flat_result.items() if isinstance(value, tuple(VALID_SUMMARY_TYPES))} self._try_log_hparams(scrubbed_result) self._file_writer.close()",if self.trial and self.trial.evaluated_params and self.last_result:
"def check_space(arr, task_id): for a in arr: if a.startswith('hadoop jar'): found = False for x in shlex.split(a): if task_id in x: found = True <IF_STMT> raise AssertionError",if not found:
"def is_valid_block(self): """"""check wheter the block is valid in the current position"""""" for i in range(self.block.x): for j in range(self.block.x): <IF_STMT> if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True","if self.block.get(i, j):"
"def undo_block_stop(self): if self.undoblock.bump_depth(-1) == 0: cmd = self.undoblock self.undoblock = 0 <IF_STMT> if len(cmd) == 1: cmd = cmd.getcmd(0) self.addcmd(cmd, 0)",if len(cmd) > 0:
"def __(task: pipelines.Task): if not acl.current_user_has_permission(views.acl_resource): return bootstrap.card(header_left='Commands', body=acl.inline_permission_denied_message()) else: commands_card = bootstrap.card(header_left='Commands', fixed_header_height=True, sections=[_render_command(command) for command in task.commands]) <IF_STMT> return [bootstrap.card(header_left=f'Max retries: {task.max_retries}'), commands_card] else: return commands_card",if task.max_retries:
"def closeEvent(self, e=None): """"""Save settings and remove registered logging handler"""""" if self.editor.isModified(): if self.wants_save(): <IF_STMT> e.accept() else: e.ignore() else: e.accept() else: e.accept()",if self.save():
"def _merge(self, a, b, path=None): """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge"""""" if path is None: path = [] for key in b: <IF_STMT> if isinstance(a[key], dict) and isinstance(b[key], dict): self._merge(a[key], b[key], path + [str(key)]) elif a[key] == b[key]: pass else: raise Exception('Conflict at %s' % '.'.join(path + [str(key)])) else: a[key] = b[key] return a",if key in a:
"def _flags_helper(conf, atom, new_flags, test=False): try: new_flags = __salt__['portage_config.get_missing_flags'](conf, atom, new_flags) except Exception: import traceback return {'result': False, 'comment': traceback.format_exc()} if new_flags: old_flags = __salt__['portage_config.get_flags_from_package_conf'](conf, atom) <IF_STMT> __salt__['portage_config.append_to_package_conf'](conf, atom, new_flags) return {'result': True, 'changes': {'old': old_flags, 'new': new_flags}} return {'result': None}",if not test:
"def _confirm_deps(self, trans): if [pkgs for pkgs in trans.dependencies if pkgs]: dia = AptConfirmDialog(trans, parent=self.parent) res = dia.run() dia.hide() if res != Gtk.ResponseType.OK: log.debug('Response is: %s' % res) <IF_STMT> log.debug('Finish_handler...') self.finish_handler(trans, 0, self.data) return self._run_transaction(trans)",if self.finish_handler:
def get_supported_extensions(self): for item in self.get_subclasses(): instance = item() <IF_STMT> for ext in instance.supports_extensions: self.extractors.update({instance.cls_name: instance}) try: self.extractors_by_extension[ext].append(instance) except KeyError: self.extractors_by_extension[ext] = [instance],if instance.check():
"def find_module(self, fullname, path=None): localname = fullname.split('.')[-1] name, ext = os.path.splitext(localname) try: fobj, filename, typeinfo = imp.find_module(name, path) except ImportError: logger.info('Dcode Searching: %s (%s)', name, path) pymod = self.proxy.getPythonModule(fullname, path) <IF_STMT> logger.info('Dcode Loaded: %s', fullname) return DcodeLoader(*pymod)",if pymod:
def run(self): try: self.server_sock = self._create_socket_and_bind() self.port = self.server_sock.getsockname()[1] self.ready_event.set() self._handle_requests() <IF_STMT> self.wait_to_close_event.wait(self.WAIT_EVENT_TIMEOUT) finally: self.ready_event.set() self._close_server_sock_ignore_errors() self.stop_event.set(),if self.wait_to_close_event:
"def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: <IF_STMT> self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute('PRAGMA synchronous = 0;') yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()",if self._pending_connection is None:
"def getReceiptInfo(pkgname): """"""Get receipt info from a package"""""" info = [] if hasValidPackageExt(pkgname): display.display_debug2('Examining %s' % pkgname) <IF_STMT> info = getFlatPackageInfo(pkgname) if os.path.isdir(pkgname): info = getBundlePackageInfo(pkgname) elif pkgname.endswith('.dist'): info = parsePkgRefs(pkgname) return info",if os.path.isfile(pkgname):
"def test_gen_speed(gen_func): cur_time = time.time() for idx, _ in enumerate(gen_func()): log.info('iter %s: %s s' % (idx, time.time() - cur_time)) cur_time = time.time() <IF_STMT> break",if idx == 100:
"def __init__(self, *args, **kwargs): if not quickjs_available: msg = 'No supported QuickJS package found on custom python environment!' <IF_STMT> msg += ' Please install python package quickjs or use ChakraJSEngine.' elif external_interpreter: msg += ' Please install python package quickjs or use ExternalJSEngine.' else: msg += ' Please install python package quickjs.' raise RuntimeError(msg) self._context = self.Context(self) InternalJSEngine.__init__(self, *args, **kwargs)",if chakra_available:
"def _draw_nodes(self, cr, bounding, highlight_items): highlight_nodes = [] for element in highlight_items: if isinstance(element, Edge): highlight_nodes.append(element.src) highlight_nodes.append(element.dst) else: highlight_nodes.append(element) for node in self.nodes: <IF_STMT> node._draw(cr, highlight=node in highlight_nodes, bounding=bounding)",if bounding is None or node._intersects(bounding):
"def upgrade(): bind = op.get_bind() session = db.Session(bind=bind) for slc in session.query(Slice).filter(Slice.viz_type.like('deck_%')): params = json.loads(slc.params) <IF_STMT> params['spatial'] = {'lonCol': params.get('longitude'), 'latCol': params.get('latitude'), 'type': 'latlong'} del params['latitude'] del params['longitude'] slc.params = json.dumps(params) session.merge(slc) session.commit() session.close()",if params.get('latitude'):
"def list_completers(): """"""List the active completers"""""" o = 'Registered Completer Functions: \n' _comp = xsh_session.completers ml = max((len(i) for i in _comp), default=0) _strs = [] for c in _comp: <IF_STMT> doc = 'No description provided' else: doc = ' '.join(_comp[c].__doc__.split()) doc = justify(doc, 80, ml + 3) _strs.append('{: >{}} : {}'.format(c, ml, doc)) return o + '\n'.join(_strs) + '\n'",if _comp[c].__doc__ is None:
"def test_numeric_literals(self):  @udf(BigIntVal(FunctionContext, SmallIntVal)) def fn(context, a): if a is None: return 1729 <IF_STMT> return None elif a < 10: return a + 5 else: return a * 2",elif a < 0:
"def get_normal_sample(in_file): """"""Retrieve normal sample if normal/turmor"""""" with utils.open_gzipsafe(in_file) as in_handle: for line in in_handle: <IF_STMT> parts = line.strip().split('Original=')[1][:-1] return parts",if line.startswith('##PEDIGREE'):
"def generate_html_index(index_file, outdir): data = parse_index_file(index_file) data = ((d[0], d[1]) for d in data) for i, chunk in enumerate(web.group(data, 1000)): back = '..' index = t_html_layout(t_html_sitemap(back, chunk)) path = outdir + '/%02d/%05d.html' % (i / 1000, i) write(path, web.safestr(index)) for f in os.listdir(outdir): path = os.path.join(outdir, f) <IF_STMT> dirindex(path) dirindex(outdir, back='.')",if os.path.isdir(path):
"def _aggregate_metadata_attribute(self, attr, agg_func=np.max, default_value=0, from_type_metadata=True): attr_values = [] for a in self.appliances: <IF_STMT> attr_value = a.type.get(attr) else: attr_value = a.metadata.get(attr) if attr_value is not None: attr_values.append(attr_value) if len(attr_values) == 0: return default_value else: return agg_func(attr_values)",if from_type_metadata:
"def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext if hasattr(names, '__contains__'): if 'gettext' in names: __builtin__.__dict__['gettext'] = __builtin__.__dict__['_'] if 'ngettext' in names: __builtin__.__dict__['ngettext'] = unicode and self.ungettext or self.ngettext <IF_STMT> __builtin__.__dict__['lgettext'] = self.lgettext if 'lngettext' in names: __builtin__.__dict__['lngettext'] = self.lngettext",if 'lgettext' in names:
def logic(): while 1: if reset == ACTIVE_LOW: yield reset.posedge for i in range(20): yield clock.posedge <IF_STMT> count.next = i j = 1 while j < 25: if enable: yield clock.posedge yield clock.posedge count.next = 2 * j j += 1,if enable:
"def multi_device(reader, dev_count): if dev_count == 1: for batch in reader: yield batch else: batches = [] for batch in reader: batches.append(batch) <IF_STMT> yield batches batches = []",if len(batches) == dev_count:
"def lockfile_from_pipfile(cls, pipfile_path): from .pipfile import Pipfile if os.path.isfile(pipfile_path): <IF_STMT> pipfile_path = os.path.abspath(pipfile_path) pipfile = Pipfile.load(os.path.dirname(pipfile_path)) return plette.lockfiles.Lockfile.with_meta_from(pipfile._pipfile) raise PipfileNotFound(pipfile_path)",if not os.path.isabs(pipfile_path):
"def _resolve_result(self, f=None): try: if f: results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) <IF_STMT> r = results[0] if isinstance(r, Exception): raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)",if self._single_result:
"def config_update(self, *updates): filename = os.path.join(self.path, '.git', 'config') with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) if value is None: config.remove_option(section, key) continue if old == value: continue except (NoSectionError, NoOptionError): pass <IF_STMT> config.set_value(section, key, value)",if value is not None:
"def process_percent(token, state, command_line): if not state.is_range_start_line_parsed: if command_line.line_range.start: raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.start.append(token) else: <IF_STMT> raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.end.append(token) return (parse_line_ref, command_line)",if command_line.line_range.end:
"def Flatten(self, metadata, value_to_flatten): if metadata: self.metadata = metadata for desc in value_to_flatten.type_infos: <IF_STMT> continue if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name): setattr(self, desc.name, getattr(value_to_flatten, desc.name))",if desc.name == 'metadata':
"def create_model(model, args, is_train): """"""Create model, include basic model, googlenet model and mixup model"""""" data_loader, data = utility.create_data_loader(is_train, args) if args.model == 'GoogLeNet': loss_out = _googlenet_model(data, model, args, is_train) el<IF_STMT> loss_out = _mixup_model(data, model, args, is_train) else: loss_out = _basic_model(data, model, args, is_train) return (data_loader, loss_out)",if args.use_mixup and is_train:
"def __init__(self, store): if store.context_aware: self.contexts = list(store.contexts()) self.default_context = store.default_context.identifier <IF_STMT> self.contexts.append(store.default_context) else: self.contexts = [store] self.default_context = None super(TrigSerializer, self).__init__(store)",if store.default_context:
def validate_import_depth(namespace): depth = namespace.depth if depth is not None: try: depth = int(depth) <IF_STMT> raise CLIError('Depth should be at least 1.') except ValueError: raise CLIError('Depth is not a number.'),if depth < 1:
"def __sync(self): """"""Skip reader to the block boundary."""""" pad_length = BLOCK_SIZE - self.__reader.tell() % BLOCK_SIZE if pad_length and pad_length != BLOCK_SIZE: data = self.__reader.read(pad_length) <IF_STMT> raise EOFError('Read %d bytes instead of %d' % (len(data), pad_length))",if len(data) != pad_length:
"def _split_long_text(text, idx, size): splited_text = text.split() if len(splited_text) > 25: <IF_STMT> first = '' else: first = ' '.join(splited_text[:10]) if idx != 0 and idx == size - 1: last = '' else: last = ' '.join(splited_text[-10:]) return '{}(...){}'.format(first, last) return text",if idx == 0:
"def download_label_map(out_dir): log.info('Downloading ScanNet ' + RELEASE_NAME + ' label mapping file...') files = [LABEL_MAP_FILE] for file in files: url = BASE_URL + RELEASE_TASKS + '/' + file localpath = os.path.join(out_dir, file) localdir = os.path.dirname(localpath) <IF_STMT> os.makedirs(localdir) download_file(url, localpath) log.info('Downloaded ScanNet ' + RELEASE_NAME + ' label mapping file.')",if not os.path.isdir(localdir):
"def get_related_ids(self, resources): vpc_ids = [vpc['VpcId'] for vpc in resources] vpc_igw_ids = set() for igw in self.manager.get_resource_manager('internet-gateway').resources(): for attachment in igw['Attachments']: <IF_STMT> vpc_igw_ids.add(igw['InternetGatewayId']) return vpc_igw_ids","if attachment.get('VpcId', '') in vpc_ids:"
"def visit_Assign(self, node): """"""Handle visiting an assignment statement."""""" ups = set() for targ in node.targets: if isinstance(targ, (Tuple, List)): ups.update((leftmostname(elt) for elt in targ.elts)) elif isinstance(targ, BinOp): newnode = self.try_subproc_toks(node) <IF_STMT> ups.add(leftmostname(targ)) else: return newnode else: ups.add(leftmostname(targ)) self.ctxupdate(ups) return node",if newnode is node:
"def evex_mask_dest_reg_only(ii): i, m, xyz = (0, 0, 0) for op in _gen_opnds(ii): if op_mask_reg(op): m += 1 elif op_xmm(op) or op_ymm(op) or op_zmm(op): xyz += 1 <IF_STMT> i += 1 else: return False return m == 1 and xyz > 0 and (i <= 1)",elif op_imm8(op):
"def get_pynames(self, parameters): result = [None] * max(len(parameters), len(self.args)) for index, arg in enumerate(self.args): <IF_STMT> result[parameters.index(arg.arg)] = self._evaluate(arg.value) else: result[index] = self._evaluate(arg) return result","if isinstance(arg, ast.keyword) and arg.arg in parameters:"
"def _discovery_modules(self) -> List[str]: modules: List[str] = [] autodiscover = self.conf.autodiscover if autodiscover: <IF_STMT> if self.conf.origin is None: raise ImproperlyConfigured(E_NEED_ORIGIN) elif callable(autodiscover): modules.extend(cast(Callable[[], Iterator[str]], autodiscover)()) else: modules.extend(autodiscover) if self.conf.origin: modules.append(self.conf.origin) return modules","if isinstance(autodiscover, bool):"
"def _lock(self, files, type): for i in count(0): lockfile = os.path.join(self._lockdir, '{}.{}.lock'.format(i, type)) <IF_STMT> self._lockfile[type] = lockfile with open(lockfile, 'w') as lock: print(*files, sep='\n', file=lock) return",if not os.path.exists(lockfile):
"def _init_inheritable_dicts_(cls): if cls.__bases__ != (object,): return for attr in cls._inheritable_dict_attrs_: if isinstance(attr, tuple): attr_name, default = attr else: attr_name, default = (attr, {}) <IF_STMT> raise SyntaxError('{} is not a dictionary'.format(attr_name)) setattr(cls, attr_name, default)","if not isinstance(default, dict):"
"def _validate_name(self, name): if isinstance(name, str): name = dns.name.from_text(name, None) elif not isinstance(name, dns.name.Name): raise KeyError('name parameter must be convertible to a DNS name') if name.is_absolute(): if not name.is_subdomain(self.origin): raise KeyError('name parameter must be a subdomain of the zone origin') <IF_STMT> name = name.relativize(self.origin) return name",if self.relativize:
"def hard_update(self, cache, size_change, pins_gates): """"""replace verts, rads and vel (in NumPy)"""""" verts, rads, vel, react = cache if len(verts) == self.v_len: <IF_STMT> unpinned = self.params['unpinned'] self.verts[unpinned] = verts[unpinned] else: self.verts = verts self.vel = vel if not size_change: self.rads = rads",if pins_gates[0] and pins_gates[1]:
"def enable(self): """"""enable the patch."""""" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return if not self.PY2: raise IncompatiblePatch('Python 2 not supported!') if pyv == 3: if self.PY3 == SKIP: return <IF_STMT> raise IncompatiblePatch('Python 3 not supported!') self.pre_enable() self.do_enable() self.enabled = True",if not self.PY3:
def on_project_dialog_finished(self): if self.sender().committed: <IF_STMT> self.close_project() self.project_manager.from_dialog(self.sender()) else: self.project_manager.project_updated.emit(),if self.sender().new_project:
"def filter_database(db, user, filter_name): """"""Returns a list of person handles"""""" filt = MatchesFilter([filter_name]) filt.requestprepare(db, user) if user: user.begin_progress(_('Finding relationship paths'), _('Retrieving all sub-filter matches'), db.get_number_of_people()) matches = [] for handle in db.iter_person_handles(): person = db.get_person_from_handle(handle) <IF_STMT> matches.append(handle) if user: user.step_progress() if user: user.end_progress() filt.requestreset() return matches","if filt.apply(db, person):"
"def add(self, key, val): if key is None: g.trace('TypeDict: None is not a valid key', g.callers()) return self._checkKeyType(key) self._checkValType(val) if self.isList: aList = self.d.get(key, []) <IF_STMT> aList.append(val) self.d[key] = aList else: self.d[key] = val",if val not in aList:
"def show_help(ctx, param, value): if value and (not ctx.resilient_parsing): <IF_STMT> echo(format_help(ctx.get_help())) else: echo(ctx.get_help(), color=ctx.color) ctx.exit()",if not ctx.invoked_subcommand:
"def wav_to_spec(wav_audio, hparams): """"""Transforms the contents of a wav file into a series of spectrograms."""""" if hparams.spec_type == 'raw': spec = _wav_to_framed_samples(wav_audio, hparams) else: <IF_STMT> spec = _wav_to_cqt(wav_audio, hparams) elif hparams.spec_type == 'mel': spec = _wav_to_mel(wav_audio, hparams) else: raise ValueError('Invalid spec_type: {}'.format(hparams.spec_type)) if hparams.spec_log_amplitude: spec = librosa.power_to_db(spec) return spec",if hparams.spec_type == 'cqt':
"def __bytes__(self) -> bytes: payload = pack('!LL', self.ssrc, self.media_ssrc) if self.lost: pid = self.lost[0] blp = 0 for p in self.lost[1:]: d = p - pid - 1 <IF_STMT> blp |= 1 << d else: payload += pack('!HH', pid, blp) pid = p blp = 0 payload += pack('!HH', pid, blp) return pack_rtcp_packet(RTCP_RTPFB, self.fmt, payload)",if d < 16:
"def run() -> None: nonlocal state, timeout while True: <IF_STMT> disposed.wait(timeout) if disposed.is_set(): return time: datetime = self.now state = action(state) timeout = seconds - (self.now - time).total_seconds()",if timeout > 0.0:
"def _get_host(self, array, connector, remote=False): """"""Return dict describing existing Purity host object or None."""""" if remote and array.get_rest_version() in SYNC_REPLICATION_REQUIRED_API_VERSIONS: hosts = array.list_hosts(remote=True) else: hosts = array.list_hosts() matching_hosts = [] for host in hosts: for wwn in connector['wwpns']: <IF_STMT> matching_hosts.append(host) break return matching_hosts",if wwn.lower() in str(host['wwn']).lower():
"def validate_moment(self, moment: 'cirq.Moment'): super().validate_moment(moment) for op in moment.operations: <IF_STMT> for other in moment.operations: if other is not op and self._check_if_exp11_operation_interacts(cast(ops.GateOperation, op), cast(ops.GateOperation, other)): raise ValueError('Adjacent Exp11 operations: {}.'.format(moment))","if isinstance(op.gate, ops.CZPowGate):"
"def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): <IF_STMT> continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models",if keys is not None and key not in keys:
"def test_all(self): expected = [] blacklist = {'executable', 'nobody_uid', 'test'} for name in dir(server): if name.startswith('_') or name in blacklist: continue module_object = getattr(server, name) <IF_STMT> expected.append(name) self.assertCountEqual(server.__all__, expected)","if getattr(module_object, '__module__', None) == 'http.server':"
"def _adjust_input(self): for i in range(len(self.block.ops)): current_op = self.block.ops[i] for input_arg in current_op.input_arg_names: <IF_STMT> current_op._rename_input(input_arg, self.input_map[input_arg])",if input_arg in self.input_map:
"def __getitem__(self, cls): try: return dict.__getitem__(self, cls) except KeyError as e: <IF_STMT> cls = cls.__class__ for b in reversed(cls.__bases__): try: retval = self[b] self[cls] = retval return retval except KeyError: pass raise e","if not hasattr(cls, '__bases__'):"
"def before_read(self, parser, section, option, value): try: json_value = srsly.json_loads(value) <IF_STMT> value = json_value except Exception: pass return super().before_read(parser, section, option, value)","if isinstance(json_value, str) and json_value not in JSON_EXCEPTIONS:"
"def insert_files(self, urls, pos): """"""Not only images"""""" image_extensions = ['.png', '.jpg', '.bmp', '.gif'] for url in urls: if url.scheme() == 'file': path = url.path() ext = os.path.splitext(path)[1] <IF_STMT> self._insert_image_from_path(path) else: self.parent.resource_edit.add_attach(path)",if os.path.exists(path) and ext in image_extensions:
"def p_constant(self, p): """"""constant : PP_NUMBER"""""" value = p[1].rstrip('LlUu') try: <IF_STMT> value = int(value[2:], 16) elif value[0] == '0': value = int(value, 8) else: value = int(value) except ValueError: value = value.rstrip('eEfF') try: value = float(value) except ValueError: value = 0 p[0] = ConstantExpressionNode(value)",if value[:2] == '0x':
"def _decode_pattern_list(data): rv = [] contains_dict = False for item in data: if isinstance(item, list): item = _decode_pattern_list(item) <IF_STMT> item = _decode_pattern_dict(item) contains_dict = True rv.append(item) if not contains_dict: rv = sorted(rv) return rv","elif isinstance(item, dict):"
"def value(self, mode): v = super(mn_armt, self).value(mode) if mode == 'l': out = [] for x in v: <IF_STMT> out.append(x[::-1]) elif len(x) == 4: out.append(x[:2][::-1] + x[2:4][::-1]) return out elif mode == 'b': return [x for x in v] else: raise NotImplementedError('bad attrib')",if len(x) == 2:
def _press_fire(self): fire_action = 1 if self.is_atari_env and self.env.unwrapped.get_action_meanings()[fire_action] == 'FIRE': self.current_ale_lives = self.env.unwrapped.ale.lives() self.step(fire_action) <IF_STMT> self.reset_internal_state(),if self.done:
"def update_fid_err_log(self, fid_err): """"""add an entry to the fid_err log"""""" self.fid_err_log.append(fid_err) if self.write_to_file: <IF_STMT> mode = 'w' else: mode = 'a' f = open(self.fid_err_file, mode) f.write('{}\n'.format(fid_err)) f.close()",if len(self.fid_err_log) == 1:
"def _name(self, sender, short=True, full_email=False): words = re.sub('[""<>]', '', sender).split() nomail = [w for w in words if not '@' in w] if nomail: if short: if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES: return nomail[1] return nomail[0] return ' '.join(nomail) elif words: <IF_STMT> return words[0].split('@', 1)[0] return words[0] return '(nobody)'",if not full_email:
"def zrx_order_to_json(order: Optional[ZeroExOrder]) -> Optional[Dict[str, any]]: if order is None: return None retval: Dict[str, any] = {} for key, value in order.items(): <IF_STMT> retval[key] = value else: retval[f'__binary__{key}'] = base64.b64encode(value).decode('utf8') return retval","if not isinstance(value, bytes):"
"def _get_outfile(self): outfile = self.inputs.transformed_file if not isdefined(outfile): <IF_STMT> if self.inputs.fs_target is True: src = 'orig.mgz' else: src = self.inputs.target_file else: src = self.inputs.source_file outfile = fname_presuffix(src, newpath=os.getcwd(), suffix='_warped') return outfile",if self.inputs.inverse is True:
"def close(self): if self.changed: save = EasyDialogs.AskYesNoCancel('Save window ""%s"" before closing?' % self.name, 1) <IF_STMT> self.menu_save() elif save < 0: return if self.parent.active == self: self.parent.active = None self.parent.updatemenubar() del self.ted self.do_postclose()",if save > 0:
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward <IF_STMT> break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if done:
"def __isub__(self, other): """"""In-place subtraction of a matrix or scalar."""""" if isinstance(other, Matrix): <IF_STMT> raise ValueError('matrix shapes do not match') for row_a, row_b in izip(self._data, other): for i in xrange(len(row_a)): row_a[i] -= row_b[i] else: for row in self._data: for i in xrange(len(row)): row[i] -= other return self",if self.shape != other.shape:
"def check(self, count, count_v, enable, clock, reset, n): expect = 0 yield reset.posedge self.assertEqual(count, expect) self.assertEqual(count, count_v) while 1: yield clock.posedge if enable: <IF_STMT> expect = n - 1 else: expect -= 1 yield delay(1) self.assertEqual(count, expect) self.assertEqual(count, count_v)",if expect == -n:
"def getmod(self, nm): mod = None for thing in self.path: if isinstance(thing, basestring): owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) if owner: mod = owner.getmod(nm) else: mod = thing.getmod(nm) <IF_STMT> break return mod",if mod:
"def get_file_language(filename, text=None): """"""Get file language from filename"""""" ext = osp.splitext(filename)[1] if ext.startswith('.'): ext = ext[1:] language = ext if not ext: if text is None: text, _enc = encoding.read(filename) for line in text.splitlines(): if not line.strip(): continue <IF_STMT> shebang = line[2:] if 'python' in shebang: language = 'python' else: break return language",if line.startswith('#!'):
"def do_status(self, directory, path): with self._repo(directory) as repo: if path: path = os.path.join(directory, path) statuses = repo.status(include=path, all=True) for status, paths in statuses: <IF_STMT> return self.statuses[status][0] return None else: resulting_status = 0 for status, paths in repo.status(all=True): if paths: resulting_status |= self.statuses[status][1] return self.repo_statuses_str[resulting_status]",if paths:
def _kill(proc): if proc is None: return if proc.stdout is not None: proc.stdout.close() if proc.stderr is not None: proc.stderr.close() <IF_STMT> try: proc.terminate() except: if proc.returncode is None: try: proc.kill() except: pass,if proc.returncode is None:
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if isinstance(rv, flask.Response): try: result = etag <IF_STMT> result = result(rv) if result: rv.set_etag(result) except Exception: logging.getLogger(__name__).exception('Error while calculating the etag value for response {!r}'.format(rv)) return rv",if callable(result):
def _list_shape_iter(shape): last_shape = _void for item in shape: if item is Ellipsis: <IF_STMT> raise ValueError('invalid shape spec: Ellipsis cannot be thefirst element') while True: yield last_shape last_shape = item yield item,if last_shape is _void:
def delete_oidc_session_tokens(session): if session: <IF_STMT> del session['oidc_access_token'] if 'oidc_id_token' in session: del session['oidc_id_token'] if 'oidc_id_token_expiration' in session: del session['oidc_id_token_expiration'] if 'oidc_login_next' in session: del session['oidc_login_next'] if 'oidc_refresh_token' in session: del session['oidc_refresh_token'] if 'oidc_state' in session: del session['oidc_state'],if 'oidc_access_token' in session:
"def calc_parity(sig, kind): if kind in ('zero', 'none'): return C(0, 1) elif kind == 'one': return C(1, 1) else: bits, _ = value_bits_sign(sig) even_parity = sum([sig[b] for b in range(bits)]) & 1 if kind == 'odd': return ~even_parity <IF_STMT> return even_parity else: assert False",elif kind == 'even':
"def parse_cookies(cookies_headers): parsed = {} for cookie in cookies_headers: cookie = cookie.split(';') for c in cookie: name, value = c.split('=', 1) name = name.strip() value = value.strip() <IF_STMT> continue parsed[name] = value return parsed",if name.lower() in _SPECIAL_COOKIE_NAMES:
"def search_rotate(array, val): low, high = (0, len(array) - 1) while low <= high: mid = (low + high) // 2 if val == array[mid]: return mid <IF_STMT> if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 elif array[mid] <= val <= array[high]: low = mid + 1 else: high = mid - 1 return -1",if array[low] <= array[mid]:
"def _get_instance_attribute(self, attr, default=None, defaults=None, incl_metadata=False): if self.instance is None or not hasattr(self.instance, attr): <IF_STMT> return self.parsed_metadata[attr] elif defaults is not None: for value in defaults: if callable(value): value = value() if value is not None: return value return default return getattr(self.instance, attr)",if incl_metadata and attr in self.parsed_metadata:
"def _handle_rate_limit(self, exception: RedditAPIException) -> Optional[Union[int, float]]: for item in exception.items: <IF_STMT> amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) if 'minute' in amount_search.group(2): seconds *= 60 if seconds <= int(self.config.ratelimit_seconds): sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None",if item.error_type == 'RATELIMIT':
"def _split_values(self, value): if not self.allowed_values: return ('',) try: r = re.compile(self.allowed_values) except: print(self.allowed_values, file=sys.stderr) raise s = str(value) i = 0 vals = [] while True: m = r.search(s[i:]) if m is None: break vals.append(m.group()) delimiter = s[i:i + m.start()] <IF_STMT> self.delimiter = delimiter i += m.end() return tuple(vals)",if self.delimiter is None and delimiter != '':
"def render(self, mode='none'): """"""Renders the environment via matplotlib."""""" if mode == 'log': self.logger.info('Performance: ' + str(self._portfolio.performance)) elif mode == 'chart': <IF_STMT> raise NotImplementedError() self.viewer.render(self.clock.step - 1, self._portfolio.performance, self._broker.trades)",if self.viewer is None:
"def load_vocabulary(vocab_file): with open(vocab_file, 'r') as f: vocabulary = [] for line in f: line = line.strip() <IF_STMT> line = line.split(' ')[0] vocabulary.append(line) return vocabulary",if ' ' in line:
"def test_confirm_extension_is_yml(self): files_with_incorrect_extensions = [] for file in self.yield_next_rule_file_path(self.path_to_rules): file_name_and_extension = os.path.splitext(file) if len(file_name_and_extension) == 2: extension = file_name_and_extension[1] <IF_STMT> files_with_incorrect_extensions.append(file) self.assertEqual(files_with_incorrect_extensions, [], Fore.RED + 'There are rule files with extensions other than .yml')",if extension != '.yml':
"def diff_from_indeces(self, indeces): rgroups = [] with self._lock: for i in indeces: rgroup = self.events[i] <IF_STMT> rgroups.append(rgroup) return '\n'.join((rgroup.diff for rgroup in rgroups))","if isinstance(rgroup, findlib2.ReplaceHitGroup):"
"def deep_update(config, override_config): for k, v in override_config.items(): if isinstance(v, Mapping): k_config = config.get(k, {}) <IF_STMT> v_config = deep_update(k_config, v) config[k] = v_config else: config[k] = v else: config[k] = override_config[k] return config","if isinstance(k_config, Mapping):"
"def GetBoundingBoxMin(self): """"""Get the minimum bounding box."""""" x1, y1 = (10000, 10000) x2, y2 = (-10000, -10000) for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] <IF_STMT> y1 = point[1] if point[0] > x2: x2 = point[0] if point[1] > y2: y2 = point[1] return (x2 - x1, y2 - y1)",if point[1] < y1:
"def insertChars(self, chars): tc = self.editBoxes[self.ind].textCursor() if tc.hasSelection(): selection = tc.selectedText() <IF_STMT> if len(selection) > 2 * len(chars): selection = selection[len(chars):-len(chars)] tc.insertText(selection) else: tc.insertText(chars + tc.selectedText() + chars) else: tc.insertText(chars)",if selection.startswith(chars) and selection.endswith(chars):
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') if sty.italic: fragment = '<i>%s</i>' % fragment if sty.underline: fragment = '<u>%s</u>' % fragment <IF_STMT> fragment = '<s>%s</s>' % fragment body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.strikeout:
def mFEBRUARY(self): try: _type = FEBRUARY _channel = DEFAULT_CHANNEL pass self.match('feb') alt14 = 2 LA14_0 = self.input.LA(1) <IF_STMT> alt14 = 1 if alt14 == 1: pass self.match('ruary') self._state.type = _type self._state.channel = _channel finally: pass,if LA14_0 == 114:
"def test_calendar(self): subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit) widgets = subreddit.widgets with self.use_cassette('TestSubredditWidgets.fetch_widgets'): calendar = None for widget in widgets.sidebar: <IF_STMT> calendar = widget break assert isinstance(calendar, Calendar) assert calendar == calendar assert calendar.id == calendar assert calendar in widgets.sidebar assert isinstance(calendar.configuration, dict) assert hasattr(calendar, 'requiresSync') assert subreddit == calendar.subreddit","if isinstance(widget, Calendar):"
def count(num): cnt = 0 for i in range(num): try: if i % 2: raise ValueError <IF_STMT> raise ArithmeticError('1') except Exception as e: cnt += 1 return cnt,if i % 3:
"def pop(self): """"""Pop a nonterminal.  (Internal)"""""" popdfa, popstate, popnode = self.stack.pop() newnode = self.convert(self.grammar, popnode) if newnode is not None: <IF_STMT> dfa, state, node = self.stack[-1] node[-1].append(newnode) else: self.rootnode = newnode try: self.rootnode.used_names = self.used_names except AttributeError: pass",if self.stack:
"def handle_custom_actions(self): for _, action in CustomAction.registry.items(): if action.resource != self.resource: continue <IF_STMT> self.parser.add_parser(action.action, help='') action(self.page).add_arguments(self.parser, self)",if action.action not in self.parser.choices:
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url, timeout=1).json().get('config', {}) if 'Version' in resp: meta['nomad_version'] = resp.get('Version') if 'Region' in resp: meta['nomad_region'] = resp.get('Region') <IF_STMT> meta['nomad_datacenter'] = resp.get('Datacenter') except Exception as ex: self.log.debug('Error getting Nomad version: %s' % str(ex)) return meta",if 'Datacenter' in resp:
"def _source_tuple(af, address, port): if address or port: if address is None: <IF_STMT> address = '0.0.0.0' elif af == socket.AF_INET6: address = '::' else: raise NotImplementedError(f'unknown address family {af}') return (address, port) else: return None",if af == socket.AF_INET:
"def _evoke_request(cls): succeed = False with cls.LOCK: if len(cls.REQUESTING_STACK) > 0: resource, request_semaphore = cls.REQUESTING_STACK.pop() node = cls.check_availability(resource) <IF_STMT> cls.NODE_RESOURCE_MANAGER[node]._request(node, resource) logger.debug('\nEvoking requesting resource {}'.format(resource)) request_semaphore.release() succeed = True else: cls.REQUESTING_STACK.append((resource, request_semaphore)) return if succeed: cls._evoke_request()",if node is not None:
"def update_all_rhos(instances, scenario_tree, rho_value=None, rho_scale=None): assert not (rho_value is not None and rho_scale is not None) for stage in scenario_tree._stages[:-1]: for tree_node in stage._tree_nodes: for scenario in tree_node._scenarios: rho = scenario._rho[tree_node._name] for variable_id in tree_node._variable_ids: <IF_STMT> rho[variable_id] = rho_value else: rho[variable_id] *= rho_scale",if rho_value is not None:
"def configured_request_log_handlers(config, prefix='query_log', default_logger=None): """"""Returns configured query loggers as defined in the `config`."""""" handlers = [] for section in config.sections(): <IF_STMT> options = dict(config.items(section)) type_ = options.pop('type') if type_ == 'default': logger = default_logger or get_logger() handler = ext.request_log_handler('default', logger) else: handler = ext.request_log_handler(type_, **options) handlers.append(handler) return handlers",if section.startswith(prefix):
"def eval_dummy_genomes_ctrnn_bad(genomes, config): for genome_id, genome in genomes: net = neat.ctrnn.CTRNN.create(genome, config, 0.01) net.advance([0.5, 0.5, 0.5], 0.01, 0.05) <IF_STMT> genome.fitness = 0.0 else: net.reset() genome.fitness = 1.0",if genome_id <= 150:
"def housenumber(self): if self.street: expression = '\\d+' pattern = re.compile(expression) match = pattern.search(self.street, re.UNICODE) <IF_STMT> return match.group(0)",if match:
"def func(): end_received = False while True: for idx, q in enumerate(self._local_out_queues): data = q.get() q.task_done() if isinstance(data, EndSignal): end_received = True <IF_STMT> continue self._out_queue.put(data) if end_received: break",if idx > 0:
"def spin(): """"""Wheeeee!"""""" state = 0 states = random.choice(spinners.spinners) while True: prefix = '[%s] ' % _spinner_style(states[state]) spinner_handle.update(prefix) state = (state + 1) % len(states) <IF_STMT> break",if stop.wait(0.1):
"def _format_ip_address(container_group): """"""Format IP address."""""" ip_address = container_group.get('ipAddress') if ip_address: ports = ip_address['ports'] or [] <IF_STMT> for container in container_group.get('containers'): ports += container.get('ports') ports = ','.join((str(p['port']) for p in ports)) return '{0}:{1}'.format(ip_address.get('ip'), ports) return None",if ip_address['type'] == 'Private':
"def check(self, count, count_v, enable, clock, reset, n): expect = 0 yield reset.posedge self.assertEqual(count, expect) self.assertEqual(count, count_v) while 1: yield clock.posedge <IF_STMT> if expect == -n: expect = n - 1 else: expect -= 1 yield delay(1) self.assertEqual(count, expect) self.assertEqual(count, count_v)",if enable:
"def _to_str(self, tokens: List[int]) -> str: pos = next((idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1) if pos != -1: tokens = tokens[:pos] vocab_map = self.vocab.id_to_token_map_py words = [vocab_map[t] for t in tokens] if self.encoding is not None and self.perform_decode: if self.encoding == 'bpe': words = self.bpe_decode(words) <IF_STMT> words = self.spm_decode(words) sentence = ' '.join(words) return sentence",elif self.encoding == 'spm':
"def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False for pattern in S3Sync.exclude_files: if fnmatch.fnmatch(file, pattern): exclude = True break if not exclude: full_path = root + '/' + file <IF_STMT> checksum = self._hash_file(full_path) else: checksum = '' file_list[relpath + file] = [full_path, checksum] return file_list",if include_checksums:
"def render(self, context): if self.user is None: entries = LogEntry.objects.all() else: user_id = self.user <IF_STMT> user_id = context[self.user].pk entries = LogEntry.objects.filter(user__pk=user_id) context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)] return ''",if not user_id.isdigit():
"def pin_data_keys(self, session_id, data_keys, token, devices=None): if not devices: devices = functools.reduce(operator.or_, self._manager_ref.get_data_locations(session_id, data_keys), set()) else: devices = self._normalize_devices(devices) pinned = set() for dev in devices: handler = self.get_storage_handler(dev) <IF_STMT> continue keys = handler.pin_data_keys(session_id, data_keys, token) pinned.update(keys) return list(pinned)","if not getattr(handler, '_spillable', False):"
"def resolve(self, value: Optional[T]) -> T: v: Optional[Any] = value if value is None: t = os.environ.get(self.envvar) if self.type is bool and t: v = t in ['true', 'True', '1', 'yes'] elif self.type is str and t: v = t <IF_STMT> v = ast.literal_eval(t) if t is not None else None if v is None: v = self.default return v",elif t:
"def remove(self, *objs): val = getattr(instance, rel_field.rel.get_related_field().attname) for obj in objs: <IF_STMT> setattr(obj, rel_field.name, None) obj.save() else: raise rel_field.rel.to.DoesNotExist('%r is not related to %r.' % (obj, instance))","if getattr(obj, rel_field.attname) == val:"
"def generate_segment_memory(chart_type, race_configs, environment): structures = [] for race_config in race_configs: if 'segment_memory' in race_config.charts: title = chart_type.format_title(environment, race_config.track, es_license=race_config.es_license, suffix='%s-segment-memory' % race_config.label) chart = chart_type.segment_memory(title, environment, race_config) <IF_STMT> structures.append(chart) return structures",if chart:
"def comment_multiline(self, text, delimiter_end, delimiter_start, style): """"""Process the beggining and end of a multiline comment."""""" startIndex = 0 if self.previousBlockState() != 1: startIndex = delimiter_start.indexIn(text) while startIndex >= 0: endIndex = delimiter_end.indexIn(text, startIndex) commentLength = 0 <IF_STMT> self.setCurrentBlockState(1) commentLength = len(text) - startIndex else: commentLength = endIndex - startIndex + delimiter_end.matchedLength() self.setFormat(startIndex, commentLength, style) startIndex = delimiter_start.indexIn(text, startIndex + commentLength)",if endIndex == -1:
def getLatestFile(self): highestNsp = None highestNsx = None for nsp in self.getFiles(): try: if nsp.path.endswith('.nsx'): if not highestNsx or int(nsp.version) > int(highestNsx.version): highestNsx = nsp el<IF_STMT> highestNsp = nsp except BaseException: pass return highestNsp or highestNsx,if not highestNsp or int(nsp.version) > int(highestNsp.version):
"def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) <IF_STMT> continue elif isinstance(id, collections.Hashable): if id not in dict or not dict[id].is_alive(): d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()",if id is None:
"def _build_pcf(named_sc, named_pc): r = '' for sig, pins, others, resname in named_sc: <IF_STMT> for bit, pin in enumerate(pins): r += 'set_io {}[{}] {}\n'.format(sig, bit, pin) else: r += 'set_io {} {}\n'.format(sig, pins[0]) if named_pc: r += '\n' + '\n\n'.join(named_pc) return r",if len(pins) > 1:
"def __init__(self, profile, report_dir=None, timestamp=None): self.report_dir = report_dir if report_dir else DEFAULT_REPORT_DIR self.profile = profile.replace('/', '_').replace('\\', '_') self.current_time = datetime.datetime.now(dateutil.tz.tzlocal()) if timestamp != False: self.timestamp = self.current_time.strftime('%Y-%m-%d_%Hh%M%z') <IF_STMT> else timestamp",if not timestamp
"def _convert_params_to_v3(params): for k, v in OLD_TO_NEW_PARAMS.items(): if k in params: msg = Message.WARN_PARAMS_NOT_SUPPORTED % (k, v) warnings.warn(msg, DeprecationWarning) <IF_STMT> params[v] = params.pop(k)",if v not in params:
"def rollup_logical(counter, lookup, logical_keys): logical = Counter() for k, v in counter.items(): <IF_STMT> logical['unknown', k] = v continue linfo = lookup[k] lkey = tuple((linfo.get(lk, 'unknown') for lk in logical_keys)) logical[lkey] += v return logical",if k not in lookup:
"def assert_summary_equals(self, records, tag, step, value): for record in records[1:]: if record.summary.value[0].tag != tag: continue <IF_STMT> continue self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor)) return self.fail('Could not find record for tag {} and step {}'.format(tag, step))",if record.step != step:
"def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]): names = [] for type_ in types: if isinstance(type_, StrawberryUnion): return type_.name <IF_STMT> name = capitalize_first(type_._type_definition.name) else: name = capitalize_first(type_.__name__) names.append(name) return ''.join(names)","elif hasattr(type_, '_type_definition'):"
"def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f['f'].splitlines(): cols = line.rstrip().split('\t') <IF_STMT> continue elif cols[0] == 'Size': continue else: s_name = self.clean_s_name(cols[2].rstrip().split('/')[-1], f['root']) if s_name != lastsample: d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d",if cols[0] == '#bamPEFragmentSize':
"def read_output(meteor_output_path, n_repeats): n_combinations = math.factorial(n_repeats) / (math.factorial(2) * math.factorial(n_repeats - 2)) raw_scores = [] average_scores = [] for line in open(meteor_output_path): if not line.startswith('Segment '): continue score = float(line.strip().split('\t')[1]) raw_scores.append(score) <IF_STMT> average_scores.append(sum(raw_scores) / n_combinations) raw_scores = [] os.remove(meteor_output_path) return average_scores",if len(raw_scores) == n_combinations:
"def get_new_pids(self): if not self.need_poll(): return for process in psutil.process_iter(): info = process.as_dict(['create_time', 'pid', 'name', 'exe']) pid = info['pid'] if pid not in self.pids or self.pids[pid] == info['create_time']: for name in self.names: <IF_STMT> yield pid self.pids[pid] = info['create_time']",if name.match(info['name']) or name.match(info['exe']):
"def _Attribute(self, node): if not isinstance(node.ctx, ast.Store): scope = self.scope.get_inner_scope_for_line(node.lineno) pyname = evaluate.eval_node(scope, node.value) <IF_STMT> if node.attr not in pyname.get_object(): self._add_error(node, 'Unresolved attribute') ast.walk(node.value, self)",if pyname is not None and pyname.get_object() != pyobjects.get_unknown():
def _init_neighbor(neighbor): families = neighbor.families() for change in neighbor.changes: if change.nlri.family() in families: neighbor.rib.outgoing.add_to_rib_watchdog(change) for message in messages: if message.family() in families: <IF_STMT> neighbor.asm[message.family()] = message else: neighbor.messages.append(message) self.neighbors[neighbor.name()] = neighbor,if message.name == 'ASM':
"def date_match(self, date1, date2): if date1.is_empty() or date2.is_empty(): return 0 if date1.is_equal(date2): return 1 if date1.is_compound() or date2.is_compound(): return self.range_compare(date1, date2) if date1.get_year() == date2.get_year(): <IF_STMT> return 0.75 if not date1.get_month_valid() or not date2.get_month_valid(): return 0.75 else: return -1 else: return -1",if date1.get_month() == date2.get_month():
"def del_var_history(self, var, f=None, line=None): """"""If file f and line are not given, the entire history of var is deleted"""""" if var in self.variables: <IF_STMT> self.variables[var] = [x for x in self.variables[var] if x['file'] != f and x['line'] != line] else: self.variables[var] = []",if f and line:
"def test_certs(self): self.assertTrue(len(self.regions) > 0) for region in self.regions: special_access_required = False for snippet in ('gov', 'cn-'): if snippet in region.name: special_access_required = True break try: c = region.connect() self.sample_service_call(c) except: <IF_STMT> raise",if not special_access_required:
"def convert_encoder_layer(opus_dict, layer_prefix: str, converter: dict): sd = {} for k in opus_dict: <IF_STMT> continue stripped = remove_prefix(k, layer_prefix) v = opus_dict[k].T sd[converter[stripped]] = torch.tensor(v).squeeze() return sd",if not k.startswith(layer_prefix):
"def test_sequence(self, sequence): for test in sequence: <IF_STMT> test, kwargs = test else: kwargs = {} self.do_check(test, **kwargs) if test == ExpectedError: return False return True","if isinstance(test, tuple):"
"def make_table(grid): max_cols = [max(out) for out in map(list, zip(*[[len(item) for item in row] for row in grid]))] rst = table_div(max_cols, 1) for i, row in enumerate(grid): header_flag = False <IF_STMT> header_flag = True rst += normalize_row(row, max_cols) rst += table_div(max_cols, header_flag) return rst",if i == 0 or i == len(grid) - 1:
"def test_float_overflow(self): import sys big_int = int(sys.float_info.max) * 2 for t in float_types + [c_longdouble]: self.assertRaises(OverflowError, t, big_int) if hasattr(t, '__ctype_be__'): self.assertRaises(OverflowError, t.__ctype_be__, big_int) <IF_STMT> self.assertRaises(OverflowError, t.__ctype_le__, big_int)","if hasattr(t, '__ctype_le__'):"
"def _process_folder(config, folder, cache, output): if not os.path.isdir(folder): raise ConanException(""No such directory: '%s'"" % str(folder)) if config.source_folder: folder = os.path.join(folder, config.source_folder) for root, dirs, files in walk(folder): dirs[:] = [d for d in dirs if d != '.git'] <IF_STMT> continue for f in files: _process_file(root, f, config, cache, output, folder)",if '.git' in root:
"def setChanged(self, c, changed): dw = c.frame.top i = self.indexOf(dw) if i < 0: return s = self.tabText(i) s = g.u(s) if len(s) > 2: <IF_STMT> if not s.startswith('* '): title = '* ' + s self.setTabText(i, title) elif s.startswith('* '): title = s[2:] self.setTabText(i, title)",if changed:
def dump_metrics(self): metrics = self._registry.dump_metrics() for metric in metrics.itervalues(): if metric.get('count') == 0: <IF_STMT> metric['min'] = 0.0 if 'max' in metric: metric['max'] = 0.0 return metrics,if 'min' in metric:
"def ref_max_pooling_3d(x, kernel, stride, ignore_border, pad): y = [] for xx in x.reshape((-1,) + x.shape[-4:]): <IF_STMT> xx = xx[np.newaxis] y += [refs.pooling_3d(xx, 'max', kernel, stride, pad, ignore_border)[np.newaxis]] y = np.vstack(y) if x.ndim == 3: y = np.squeeze(y, 1) return y.reshape(x.shape[:-4] + y.shape[1:])",if xx.ndim == 3:
def reader_(): with open(file_list) as flist: lines = [line.strip() for line in flist] <IF_STMT> random.shuffle(lines) for line in lines: file_path = line.strip() yield [file_path],if shuffle:
"def _sql_like_to_regex(pattern, escape): cur_i = 0 pattern_length = len(pattern) while cur_i < pattern_length: nxt_i = cur_i + 1 cur = pattern[cur_i] nxt = pattern[nxt_i] if nxt_i < pattern_length else None skip = 1 if nxt is not None and escape is not None and (cur == escape): yield nxt skip = 2 elif cur == '%': yield '.*' <IF_STMT> yield '.' else: yield cur cur_i += skip",elif cur == '_':
"def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker='sphere'): """"""Show N random gaussian distributed points using a scatter plot."""""" import ipyvolume as ipv rng = np.random.RandomState(seed) x, y, z = rng.normal(size=(3, N)) if draw: <IF_STMT> mesh = ipv.scatter(x, y, z, marker=marker, color=color) else: mesh = ipv.scatter(x, y, z, marker=marker) if show: ipv.show() return mesh else: return (x, y, z)",if color:
"def _delete_keys(bucket, keys): for name in keys: while True: try: k = boto.s3.connection.Key(bucket, name) bucket.delete_key(k) except boto.exception.S3ResponseError as e: <IF_STMT> break raise else: break",if e.status == 404:
"def detect(self): hardware = self.middleware.call_sync('failover.hardware') if hardware == 'ECHOSTREAM': proc = subprocess.check_output('/usr/sbin/pciconf -lv | grep ""card=0xa01f8086 chip=0x10d38086""', shell=True, encoding='utf8') <IF_STMT> return [proc.split('@')[0]] if hardware in ('ECHOWARP', 'PUMA'): return ['ntb0'] if hardware == 'BHYVE': return ['vtnet1'] if hardware == 'SBB': return ['ix0'] if hardware == 'ULTIMATE': return ['igb1'] return []",if proc:
"def check_config(param): fileopen = open('/etc/setoolkit/set.config', 'r') for line in fileopen: line = line.rstrip() if line.startswith(param) != '#': <IF_STMT> line = line.rstrip() line = line.replace('""', '') line = line.replace(""'"", '') line = line.split('=', 1) return line[1]",if line.startswith(param):
"def put(self, s): """"""Put string s to self.outputFile. All output eventually comes here."""""" if s: self.putCount += 1 <IF_STMT> s = g.toEncodedString(s, self.leo_file_encoding, reportErrors=True) self.outputFile.write(s)",if not g.isPython3:
"def get_system_prop_font(self): """"""Look up the system font"""""" if self.system_prop_font is not None: return self.system_prop_font elif 'org.gnome.desktop.interface' not in Gio.Settings.list_schemas(): return else: gsettings = Gio.Settings.new('org.gnome.desktop.interface') value = gsettings.get_value('font-name') <IF_STMT> self.system_prop_font = value.get_string() else: self.system_prop_font = 'Sans 10' return self.system_prop_font",if value:
"def _setoct(self, octstring): """"""Reset the bitstring to have the value given in octstring."""""" octstring = tidy_input_string(octstring) octstring = octstring.replace('0o', '') binlist = [] for i in octstring: try: <IF_STMT> raise ValueError binlist.append(OCT_TO_BITS[int(i)]) except ValueError: raise CreationError(""Invalid symbol '{0}' in oct initialiser."", i) self._setbin_unsafe(''.join(binlist))",if not 0 <= int(i) < 8:
"def group(self, resources): groups = {} for r in resources: v = self._value_to_sort(self.group_by, r) vstr = str(v) <IF_STMT> groups[vstr] = {'sortkey': v, 'resources': []} groups[vstr]['resources'].append(r) return groups",if vstr not in groups:
"def rd(line_number, row, col, key, default=None): """"""Return Row data by column name"""""" if key in col: if col[key] >= len(row): LOG.warning(""missing '%s, on line %d"" % (key, line_number)) return default retval = row[col[key]].strip() <IF_STMT> return default else: return retval else: return default",if retval == '':
"def _run(self): while True: tup = self._pop() <IF_STMT> return method_name, kwargs, msg = tup try: super(SerializedInvoker, self).invoke(method_name, kwargs, msg) except mitogen.core.CallError: e = sys.exc_info()[1] LOG.warning('%r: call error: %s: %s', self, msg, e) msg.reply(e) except Exception: LOG.exception('%r: while invoking %s()', self, method_name) msg.reply(mitogen.core.Message.dead())",if tup is None:
"def raises(except_cls, message=None): try: yield success = False except except_cls as e: <IF_STMT> assert re.search(message, compat.text_type(e), re.UNICODE), '%r !~ %s' % (message, e) print(compat.text_type(e).encode('utf-8')) success = True assert success, 'Callable did not raise an exception'",if message:
"def buttonClicked(self, button): role = self.buttonBox.buttonRole(button) if role == QDialogButtonBox.ResetRole: current_tab = self.tabwidget.currentWidget() section_to_update = Sections.ALL if current_tab is self.page_general: section_to_update = Sections.GENERAL <IF_STMT> section_to_update = Sections.DISPLAY self.resetToDefaults(section_to_update)",if current_tab is self.page_display:
"def make_range_list(*values): ranges = [] for v in values: <IF_STMT> val_node = plural.value_node(v) ranges.append((val_node, val_node)) else: assert isinstance(v, tuple) ranges.append((plural.value_node(v[0]), plural.value_node(v[1]))) return plural.range_list_node(ranges)","if isinstance(v, int):"
def __in_comment(self): if self.highlighter: current_color = self.__get_current_color() comment_color = self.highlighter.get_color_name('comment') <IF_STMT> return True else: return False else: return False,if current_color == comment_color:
"def __str__(self): """"""Constructs to variable list output used in cron jobs"""""" ret = [] for key, value in self.items(): if self.previous: if self.previous.all().get(key, None) == value: continue <IF_STMT> value = '""%s""' % value ret.append('%s=%s' % (key, unicode(value))) ret.append('') return '\n'.join(ret)",if ' ' in unicode(value) or value == '':
"def _on_config_changed(changed_name: str) -> None: """"""Call config_changed hooks if the config changed."""""" for mod_info in _module_infos: if mod_info.skip_hooks: continue for option, hook in mod_info.config_changed_hooks: if option is None: hook() else: cfilter = config.change_filter(option) cfilter.validate() <IF_STMT> hook()",if cfilter.check_match(changed_name):
"def __init__(self, transcripts, vocab=None, unknown=None, *args, **kwargs): """"""Creates a new raw transcript source."""""" super().__init__(*args, **kwargs) self.transcripts = transcripts self.indices = numpy.arange(len(self)) self.vocab = self.make_vocab(vocab) if unknown is None: self.unknown = self.unknown_index = None else: self.unknown_index = self.vocab.get(unknown) <IF_STMT> raise ValueError('The ""unknown"" vocabulary word must be part of the vocabulary itself.') self.unknown = unknown",if self.unknown_index is None:
"def load_info(cls, path, reset_paths=False, load_model_if_required=True): load_path = path + cls.trainer_info_name try: return load_pkl.load(path=load_path) except: <IF_STMT> trainer = cls.load(path=path, reset_paths=reset_paths) return trainer.get_info() else: raise",if load_model_if_required:
"def createActions(actions, target): for name, shortcut, icon, desc, func in actions: action = QAction(target) <IF_STMT> action.setIcon(icon) if shortcut: action.setShortcut(shortcut) action.setText(desc) action.triggered.connect(func) setattr(target, name, action)",if icon:
"def load_user_logins(self, key, dates, timestamps, size_threshold=None): date_bucket = {} for user_data in self.fetch_user_table(): <IF_STMT> continue dt = datetime.fromtimestamp(user_data[6] / 1000) dt = dt.date().isoformat() date_bucket[dt] = date_bucket.get(dt, 0) + 1 datapoints = [] for dt, ts in zip(dates, timestamps): count = date_bucket.get(dt, 0) datapoints.append((count, ts)) return {'target': key, 'datapoints': datapoints}",if size_threshold is not None and user_data[1] < size_threshold:
def apply_batch(it): batch = [] for item in it: <IF_STMT> yield item else: batch.append(item) if len(batch) >= n: yield batch batch = [] if batch: yield batch,"if isinstance(item, _NextValueNotReady):"
"def convert_tomlkit_table(section): if isinstance(section, tomlkit.items.Table): body = section.value._body else: body = section._body for key, value in body: if not key: continue <IF_STMT> table = tomlkit.inline_table() table.update(value.value) section[key.key] = table","if hasattr(value, 'keys') and (not isinstance(value, tomlkit.items.InlineTable)):"
"def _do_ssl_handshake(self): try: self.socket.do_handshake() except ssl.SSLError as err: if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE): return <IF_STMT> return self.handle_close() raise except OSError as err: if err.args[0] == errno.ECONNABORTED: return self.handle_close() else: self._ssl_accepting = False",elif err.args[0] == ssl.SSL_ERROR_EOF:
"def get_filechanges(repo, revision, parents, mleft): """"""Given some repository and revision, find all changed/deleted files."""""" l, c, r = ([], [], []) for p in parents: <IF_STMT> continue mright = revsymbol(repo, b'%d' % p).manifest() l, c, r = split_dict(mleft, mright, l, c, r) l.sort() c.sort() r.sort() return (l, c, r)",if p < 0:
"def close_share(self, share_name): c = await run([SMBCmd.SMBCONTROL.value, 'smbd', 'close-share', share_name], check=False) if c.returncode != 0: <IF_STMT> return self.logger.warn('Failed to close smb share [%s]: [%s]', share_name, c.stderr.decode().strip())","if ""Can't find pid"" in c.stderr.decode():"
"def execute(self, context): if self.tree_name: ng = bpy.data.node_groups.get(self.tree_name) <IF_STMT> apply_theme(ng) else: return {'CANCELLED'} else: apply_theme() return {'FINISHED'}",if ng:
"def apply(self, db, object): if not self.source_handle: <IF_STMT> return len(object.get_all_citation_lists()) == 0 else: return False else: for citation_handle in object.get_all_citation_lists(): citation = db.get_citation_from_handle(citation_handle) if citation.get_reference_handle() == self.source_handle: return True return False",if self.nosource:
"def get_data_dir(): """"""Get the directory path for flit user data files."""""" home = os.path.realpath(os.path.expanduser('~')) if sys.platform == 'darwin': d = Path(home, 'Library') elif os.name == 'nt': appdata = os.environ.get('APPDATA', None) <IF_STMT> d = Path(appdata) else: d = Path(home, 'AppData', 'Roaming') else: xdg = os.environ.get('XDG_DATA_HOME', None) d = Path(xdg) if xdg else Path(home, '.local/share') return d / 'flit'",if appdata:
"def wait_for_service(name, timeout=200): start = time.time() while True: status = win32serviceutil.QueryServiceStatus(name) if status[1] == win32service.SERVICE_STOPPED: break <IF_STMT> raise TimeoutError('Timeout waiting for service') time.sleep(0.3)",if time.time() - start > timeout:
"def get_selection(self): if self.uistate['selection'] == 'all': return AllPages(self.notebook) else: path = self.uistate['selected_page'] <IF_STMT> return SubPages(self.notebook, path) else: return SinglePage(self.notebook, path)",if self.uistate['selection_recursive']:
"def test_repeated_edges(self): graph_size = 20 for _ in range(20): graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=True) edges = [(e.start, e.end) for e in graph.iterate_edges()] has_repeated_edges = len(edges) > len(set(edges)) <IF_STMT> break self.assertTrue(has_repeated_edges) for _ in range(10): graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=False) edges = list(graph.iterate_edges()) self.assertEqual(len(edges), len(set(edges)))",if has_repeated_edges:
"def cs(self): """"""ConfigSpace representation of this search space."""""" cs = CS.ConfigurationSpace() for k, v in self.kwvars.items(): <IF_STMT> _add_cs(cs, v.cs, k) elif isinstance(v, Space): hp = v.get_hp(name=k) _add_hp(cs, hp) else: _rm_hp(cs, k) return cs","if isinstance(v, NestedSpace):"
"def packet_handler(Packet): global add_new_line if Packet.haslayer(ICMP): Data = Packet.getlayer(ICMP).getlayer(Raw) exfiltrated_data = Data.load[int(exfiltration_length):].replace(exfiltration_length * '\n', '\n') <IF_STMT> add_new_line = False sys.stdout.write(exfiltrated_data) sys.stdout.flush()",if exfiltrated_data.endswith('\n'):
"def acquire(self, *, wait=False): if not wait and self.value <= 0: return False while self.value <= 0: future = self.loop.create_future() self._waiters.append(future) try: await future except: future.cancel() <IF_STMT> self.wake_up() raise self.value -= 1 return True",if self.value > 0 and (not future.cancelled()):
"def handle_events(self, events): for event in events: <IF_STMT> self.recording ^= True if not self.recording: self.save() else: logger.info('ScreenRecorder started') break return events",if event == WindowEvent.SCREEN_RECORDING_TOGGLE:
"def _register_for_operations(config, session, service_name): for key in config: <IF_STMT> continue handler = retryhandler.create_retry_handler(config, key) unique_id = 'retry-config-%s-%s' % (service_name, key) session.register('needs-retry.%s.%s' % (service_name, key), handler, unique_id=unique_id)",if key == '__default__':
"def showTicks(self, show=True): for tick in self.ticks.keys(): if show: tick.show() orig = getattr(self, '_allowAdd_backup', None) <IF_STMT> self.allowAdd = orig else: self._allowAdd_backup = self.allowAdd self.allowAdd = False tick.hide()",if orig:
"def _has_cycle(self, node, visited, visit_stack): self.last_visited_node = node self.path.append(node) visited[node] = True visit_stack[node] = True for neighbor in self.graph[node]: if not visited[neighbor]: <IF_STMT> return True elif visit_stack[neighbor]: self.path.append(neighbor) return True self.path.remove(node) visit_stack[node] = False return False","if self._has_cycle(neighbor, visited, visit_stack):"
"def get_project_list(exclude_default=False): """"""get_project_list - get list of all projects"""""" projects_path = __project__.get_projects_path() project_list = [] if os.path.exists(projects_path): for project in os.listdir(projects_path): project_path = os.path.join(projects_path, project) <IF_STMT> project_list.append(project) if exclude_default: pass else: project_list.append('default') return sorted(project_list)",if os.path.isdir(project_path):
"def split(self, chunksize): modulus_map = {4: 256, 5: 10, 8: 100} chunks, ip = self.preprocess(chunksize) ret = '' for i in range(len(chunks)): ip_part = compat_str(ip[i] % modulus_map[chunksize]) if i < 4 else '' <IF_STMT> ret += ip_part + chunks[i] else: ret += chunks[i] + ip_part self.target = ret",if chunksize == 8:
"def DepsToModules(deps, prefix, suffix): modules = [] for filepath in deps: filename = os.path.basename(filepath) <IF_STMT> modules.append(filename[len(prefix):-len(suffix)]) return modules",if filename.startswith(prefix) and filename.endswith(suffix):
"def listdir(path): path = path.rstrip('/') + '/' dir_set, file_set = (set(), set()) for p in files.keys(): <IF_STMT> continue parts = p[len(path):].split('/') if len(parts) == 1: file_set.add(parts[0]) else: dir_set.add(parts[0]) return (sorted(dir_set), sorted(file_set))",if not p.startswith(path):
"def read_series(rec): found = [] for tag in ('440', '490', '830'): fields = rec.get_fields(tag) <IF_STMT> continue for f in fields: this = [] for k, v in f.get_subfields(['a', 'v']): if k == 'v' and v: this.append(v) continue v = v.rstrip('.,; ') if v: this.append(v) if this: found += [' -- '.join(this)] return found",if not fields:
"def find_nameless_urls(self, conf): nameless = [] patterns = self.get_patterns(conf) for u in patterns: <IF_STMT> nameless.extend(self.find_nameless_urls(u)) elif u.name is None: nameless.append(u) return nameless",if self.has_patterns(u):
"def update_billing_status(self, update_modified=True): updated_pr = [self.name] for d in self.get('items'): <IF_STMT> updated_pr += update_billed_amount_based_on_po(d.purchase_order_item, update_modified) for pr in set(updated_pr): pr_doc = self if pr == self.name else frappe.get_doc('Purchase Receipt', pr) update_billing_percentage(pr_doc, update_modified=update_modified) self.load_from_db()",if d.purchase_order_item:
"def _get_version(): with open('haiku/__init__.py') as fp: for line in fp: <IF_STMT> g = {} exec(line, g) return g['__version__'] raise ValueError('`__version__` not defined in `haiku/__init__.py`')",if line.startswith('__version__'):
"def GetSelected(self): if self.GetStyleL('style') & self.Style.LBS_MULTIPLESEL: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) if result: return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) <IF_STMT> return result",if result != LB_ERR:
"def __init__(self, column_names, column_types, **kwargs): super().__init__(**kwargs) self.column_names = column_names self.column_types = column_types encoding = [] for column_name in self.column_names: column_type = self.column_types[column_name] <IF_STMT> encoding.append(keras_layers.INT) else: encoding.append(keras_layers.NONE) self.layer = keras_layers.MultiCategoryEncoding(encoding)",if column_type == analysers.CATEGORICAL:
"def rotate(cls, axis, theta): """"""Prepare a quaternion that represents a rotation on a given axis."""""" if isinstance(axis, str): <IF_STMT> axis = V.X elif axis in ('y', 'Y'): axis = V.Y elif axis in ('z', 'Z'): axis = V.Z axis = axis.normalize() s = math.sin(theta / 2.0) c = math.cos(theta / 2.0) return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)","if axis in ('x', 'X'):"
"def log(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app['websockets'].add(web_socket) try: async for msg in web_socket: if msg.type == WSMsgType.TEXT: <IF_STMT> await web_socket.close() elif msg.type == WSMsgType.ERROR: print('web socket connection closed with exception %s' % web_socket.exception()) finally: self.app['websockets'].remove(web_socket) return web_socket",if msg.data == 'close':
"def test_loc_is_stochastic_parameter(self): param = iap.Laplace(iap.Choice([-100, 100]), 1) seen = [0, 0] for _ in sm.xrange(1000): samples = param.draw_samples((100,)) exp = np.mean(samples) <IF_STMT> seen[0] += 1 elif 100 - 10 < exp < 100 + 10: seen[1] += 1 else: assert False assert 500 - 100 < seen[0] < 500 + 100 assert 500 - 100 < seen[1] < 500 + 100",if -100 - 10 < exp < -100 + 10:
"def cli_setup(args=None): """"""future api for setup env by cli"""""" if not args: <IF_STMT> print('no cmdline args') return False args = sys.argv print(args) ap = argparse.ArgumentParser() if '--report' in args: from airtest.report.report import main as report_main ap = report_parser(ap) args = ap.parse_args(args) report_main(args) exit(0) else: ap = runner_parser(ap) args = ap.parse_args(args) setup_by_args(args) return True",if len(sys.argv) < 2:
"def validate_attributes(cls, cleaned_data): errors = {} for field in ['product_attributes', 'variant_attributes']: attributes = cleaned_data.get(field) if not attributes: continue not_valid_attributes = [graphene.Node.to_global_id('Attribute', attr.pk) for attr in attributes if attr.type != AttributeType.PRODUCT_TYPE] <IF_STMT> errors[field] = ValidationError('Only Product type attributes are allowed.', code=ProductErrorCode.INVALID.value, params={'attributes': not_valid_attributes}) if errors: raise ValidationError(errors)",if not_valid_attributes:
"def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == 'conv': if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) <IF_STMT> x = self.norm(x) elif layer == 'act' and activate and self.with_activation: x = self.activate(x) return x",elif layer == 'norm' and norm and self.with_norm:
"def _FunctionDef(self, node): _ScopeVisitor._FunctionDef(self, node) if len(node.args.args) > 0: first = node.args.args[0] <IF_STMT> new_visitor = _ClassInitVisitor(self, first.id) for child in ast.get_child_nodes(node): ast.walk(child, new_visitor)","if isinstance(first, ast.Name):"
"def result(self): """"""Gets the formatted string result."""""" if self.__group.isChecked(): if self.__moreThan.isChecked(): return 'gt%d' % self.__min.value() <IF_STMT> return 'lt%d' % self.__max.value() if self.__range.isChecked(): return '%d-%d' % (self.__min.value(), self.__max.value()) return ''",if self.__lessThan.isChecked():
"def hash_of_file(path): """"""Return the hash of a downloaded file."""""" with open(path, 'rb') as archive: sha = sha256() while True: data = archive.read(2 ** 20) <IF_STMT> break sha.update(data) return encoded_hash(sha)",if not data:
"def read_boolean(file: BinaryIO, count: int, checkall: bool=False) -> List[bool]: if checkall: all_defined = file.read(1) <IF_STMT> return [True] * count result = [] b = 0 mask = 0 for i in range(count): if mask == 0: b = ord(file.read(1)) mask = 128 result.append(b & mask != 0) mask >>= 1 return result",if all_defined != unhexlify('00'):
"def start_prompt(self): """"""Start the interpreter."""""" logger.show('Coconut Interpreter:') logger.show(""(type 'exit()' or press Ctrl-D to end)"") self.start_running() while self.running: try: code = self.get_input() <IF_STMT> compiled = self.handle_input(code) if compiled: self.execute(compiled, use_eval=None) except KeyboardInterrupt: printerr('\nKeyboardInterrupt')",if code:
"def _wrap_lineanchors(self, inner): s = self.lineanchors i = self.linenostart - 1 for t, line in inner: <IF_STMT> i += 1 yield (1, '<a name=""%s-%d""></a>' % (s, i) + line) else: yield (0, line)",if t:
"def __UpdateQueryHistory(self, query): clone = datastore_pb.Query() clone.CopyFrom(query) clone.clear_hint() clone.clear_limit() clone.clear_offset() clone.clear_count() if clone in self.__query_history: self.__query_history[clone] += 1 else: self.__query_history[clone] = 1 <IF_STMT> self.__query_ci_history.add(datastore_index.CompositeIndexForQuery(clone))",if clone.app() == self._app_id:
"def call(self, trajectory: traj.Trajectory): if not self._batch_size: <IF_STMT> self._batch_size = 1 else: assert trajectory.step_type.ndim == 1 self._batch_size = trajectory.step_type.shape[0] self.reset() if trajectory.step_type.ndim == 0: trajectory = nest_utils.batch_nested_array(trajectory) self._batched_call(trajectory)",if trajectory.step_type.ndim == 0:
"def steps(self): """""""""""" for step_id in range(self.micro_batches): cmds = [LoadMicroBatch(buffer_id=0), ForwardPass(buffer_id=0), BackwardPass(buffer_id=0)] <IF_STMT> cmds.extend([ReduceGrads(), OptimizerStep()]) yield cmds",if step_id == self.micro_batches - 1:
"def resolve_project(self, workspace, project_name): if isinstance(project_name, (int, float)): project_id = int(project_name) self.log.debug('Treating project name as ID: %s', project_id) project = workspace.projects(ident=project_id).first() <IF_STMT> raise TaurusConfigError('BlazeMeter project not found by ID: %s' % project_id) elif project_name: project = workspace.projects(name=project_name).first() else: project = None if not project: project = self._create_project_or_use_default(workspace, project_name) return project",if not project:
"def __reader(self, collector, source): while True: data = os.read(source.fileno(), 65536) self.__lock.acquire() collector.append(data) self.__lock.release() <IF_STMT> source.close() break return",if data == '':
"def add(self, undoinfo, msg=None): if not undoinfo: return if msg is not None: if isinstance(undoinfo[0], str): undoinfo = (msg,) + undoinfo[1:] <IF_STMT> undoinfo = (msg,) + undoinfo else: undoinfo = (msg, undoinfo) f = 1 else: f = int(isinstance(undoinfo[0], str)) assert isinstance(undoinfo, list) or callable(undoinfo[f]) or isinstance(undoinfo[f], list) self.undoList.append(undoinfo) del self.redoList[:]","elif isinstance(undoinfo, tuple):"
"def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) if c['tries'] == 0 or c['tries'] is None: if c['data'] not in history: history[c['data']] = c['timestamp'] <IF_STMT> break return history",if len(history) >= count:
"def __str__(self): from sqlalchemy.sql import util details = [SQLAlchemyError.__str__(self)] if self.statement: details.append('[SQL: %r]' % self.statement) <IF_STMT> params_repr = util._repr_params(self.params, 10) details.append('[parameters: %r]' % params_repr) return ' '.join(['(%s)' % det for det in self.detail] + details)",if self.params:
"def _consume_msg(self): async for data in self._stream: stream = data.get('ev') <IF_STMT> await self._dispatch(data) elif data.get('status') == 'disconnected': data['ev'] = 'status' await self._dispatch(data) raise ConnectionResetError(f""Polygon terminated connection: ({data.get('message')})"")",if stream:
"def nan2none(l): for idx, val in enumerate(l): if isinstance(val, Sequence): l[idx] = nan2none(l[idx]) <IF_STMT> l[idx] = None return l",elif isnum(val) and math.isnan(val):
"def _make_binary_stream(s, encoding): try: if _py3k: if isinstance(s, str): s = s.encode(encoding) el<IF_STMT> s = s.encode(encoding) from io import BytesIO rv = BytesIO(s) except ImportError: rv = StringIO(s) return rv",if type(s) is not str:
"def __set__(self, instance, value): try: value = int(value) <IF_STMT> self.display_value = str(value) self.value = value else: raise PocsuiteValidationException('Invalid option. Port value should be between 0 and 65536.') except ValueError: raise PocsuiteValidationException(""Invalid option. Cannot cast '{}' to integer."".format(value))",if 0 <= value <= 65535:
"def addVaXref(self, va, parent=None): if parent is None: parent = self xtova, ok = QInputDialog.getText(parent, 'Enter...', 'Make Code Xref 0x%x -> ' % va) if ok: try: val = self.vw.parseExpression(str(xtova)) <IF_STMT> self.vw.addXref(va, val, REF_CODE) else: self.vw.vprint('Invalid Expression: %s   (%s)' % (xtova, val)) except Exception as e: self.vw.vprint(repr(e))",if self.vw.isValidPointer(val):
"def ArrayBuffer(): a = arguments[0] if isinstance(a, PyJsNumber): length = a.to_uint32() <IF_STMT> raise MakeError('RangeError', 'Invalid array length') temp = Js(bytearray([0] * length)) return temp return Js(bytearray([0]))",if length != a.value:
"def _update_positions(nodes, line_offset, last_leaf): for node in nodes: try: children = node.children except AttributeError: node.line += line_offset <IF_STMT> raise _PositionUpdatingFinished else: _update_positions(children, line_offset, last_leaf)",if node is last_leaf:
"def class_has_method(self, curr_node, the_text): try: class_node = self.containers[VAR_KIND_CLASS][-1] for c in class_node.children: <IF_STMT> return True except: pass return False","if isinstance(c, MethodNode) and c.name == the_text:"
"def _fm(map_id): for i in range(num_key): for j in range(num_value_per_key): <IF_STMT> yield (i, j) else: yield ((map_id, i), j)",if dup_key:
"def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] if len(rulesList[ruleIndex]) == maxLength: args = rulesList[ruleIndex][-1] if maxLength == 4: shouldRunMethod, method, isCorrect = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) <IF_STMT> shouldRunMethod, method = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking",elif maxLength == 3:
def select(result): for elem in result: parent = elem.getparent() if parent is None: continue try: elems = list(parent.iterchildren(elem.tag)) <IF_STMT> yield elem except IndexError: pass,if elems[index] is elem:
"def get_kwarg_or_param(request, kwargs, key): value = None try: value = kwargs[key] except KeyError: <IF_STMT> value = request.GET.get(key) elif request.method == 'POST': value = request.POST.get(key) return value",if request.method == 'GET':
"def __imul__(self, other): if isinstance(other, str): other = Matrix(other) if isinstance(other, Matrix): if self.start is not None: self.start *= other <IF_STMT> self.control1 *= other if self.control2 is not None: self.control2 *= other if self.end is not None: self.end *= other return self",if self.control1 is not None:
def _parse_date_fmt(): fmt = get_format('DATE_FORMAT') escaped = False for char in fmt: if escaped: escaped = False elif char == '\\': escaped = True <IF_STMT> yield 'year' elif char in 'bEFMmNn': yield 'month' elif char in 'dj': yield 'day',elif char in 'Yy':
def filter_forms(forms): result = [] seen = set() for form in forms: <IF_STMT> if pos in self._lemma_pos_offset_map[form]: if form not in seen: result.append(form) seen.add(form) return result,if form in self._lemma_pos_offset_map:
"def calculate(self): """"""Enumerate processes by scanning for _EPROCESS."""""" result = set() psscan = self.session.plugins.psscan() pslist = self.session.plugins.pslist() for row in psscan.collect(): physical_eprocess = row['offset_p'] <IF_STMT> eprocess = pslist.virtual_process_from_physical_offset(physical_eprocess) else: eprocess = physical_eprocess if eprocess != None: result.add(eprocess.obj_offset) self.session.logging.debug('Listed %s processes using PSScan', len(result)) return result",if physical_eprocess.obj_vm == self.session.physical_address_space:
"def _build_kwargs_string(cls, expectation): kwargs = [] for k, v in expectation['kwargs'].items(): <IF_STMT> kwargs.insert(0, ""{}='{}'"".format(k, v)) elif isinstance(v, str): kwargs.append(""{}='{}'"".format(k, v)) else: kwargs.append('{}={}'.format(k, v)) return ', '.join(kwargs)",if k == 'column':
"def prec3_expr(self, arg_type): pass self.prec4_expr(arg_type) while True: <IF_STMT> pass pass self.match(POWER) op = struct.pack('B', ptgPower) self.prec4_expr(arg_type) self.rpn += op else: break",if self.LA(1) == POWER:
"def evaluate(analysis, rule): try: if isinstance(rule, MetaRule): result = _evaluate_meta_rule(analysis, rule) elif isinstance(rule, SingleRule): result = _evaluate_single_rule(analysis, rule) <IF_STMT> result = _evaluate_sub_path_rule(analysis, rule) else: raise TypeError('rule must be of one in types [SingleRule, MetaRule, SubPathRule]') return result except KeyError: return False","elif isinstance(rule, SubPathRule):"
"def create_log_file(d, logname): logpath = d.getVar('LOG_DIR') bb.utils.mkdirhier(logpath) logfn, logsuffix = os.path.splitext(logname) logfile = os.path.join(logpath, '%s.%s%s' % (logfn, d.getVar('DATETIME'), logsuffix)) if not os.path.exists(logfile): slogfile = os.path.join(logpath, logname) <IF_STMT> os.remove(slogfile) open(logfile, 'w+').close() os.symlink(logfile, slogfile) d.setVar('LOG_FILE', logfile) return logfile",if os.path.exists(slogfile):
"def init_eventlog(self): """"""Set up the event logging system."""""" self.eventlog = EventLog(parent=self) for dirname, _, files in os.walk(os.path.join(here, 'event-schemas')): for file in files: <IF_STMT> continue self.eventlog.register_schema_file(os.path.join(dirname, file))",if not file.endswith('.yaml'):
"def resize(self, limit, force=False, ignore_errors=False, reset=False): prev_limit = self._limit if (self._dirty and 0 < limit < self._limit) and (not ignore_errors): <IF_STMT> raise RuntimeError(""Can't shrink pool when in use: was={0} now={1}"".format(self._limit, limit)) reset = True self._limit = limit if reset: try: self.force_close_all() except Exception: pass self.setup() if limit < prev_limit: self._shrink_down(collect=limit > 0)",if not force:
"def accept_request(self, request): if self.restriction_type == BaseViewRestriction.PASSWORD: passed_restrictions = request.session.get(self.passed_view_restrictions_session_key, []) if self.id not in passed_restrictions: return False elif self.restriction_type == BaseViewRestriction.LOGIN: if not request.user.is_authenticated: return False elif self.restriction_type == BaseViewRestriction.GROUPS: <IF_STMT> current_user_groups = request.user.groups.all() if not any((group in current_user_groups for group in self.groups.all())): return False return True",if not request.user.is_superuser:
"def getLatestXci(self, version=None): highest = None for nsp in self.getFiles(): try: if nsp.path.endswith('.xci'): if version is not None and nsp.version == version: return nsp <IF_STMT> highest = nsp except BaseException: pass return highest",if not highest or int(nsp.version) > int(highest.version):
"def evaluate(self, x, y, z): vertex = Vector((x, y, z)) nearest, normal, idx, distance = self.bvh.find_nearest(vertex) if self.use_normal: if self.signed_normal: sign = (v - nearest).dot(normal) sign = copysign(1, sign) else: sign = 1 return sign * np.array(normal) else: dv = np.array(nearest - vertex) <IF_STMT> norm = np.linalg.norm(dv) len = self.falloff(norm) dv = len * dv return dv else: return dv",if self.falloff is not None:
"def to_py(self, value: _StrUnset) -> _StrUnsetNone: self._basic_py_validation(value, str) if isinstance(value, usertypes.Unset): return value elif not value: return None value = os.path.expandvars(value) value = os.path.expanduser(value) try: if not os.path.isdir(value): raise configexc.ValidationError(value, 'must be a valid directory!') <IF_STMT> raise configexc.ValidationError(value, 'must be an absolute path!') except UnicodeEncodeError as e: raise configexc.ValidationError(value, e) return value",if not os.path.isabs(value):
"def validate_load_balancer_sku(namespace): """"""Validates the load balancer sku string."""""" if namespace.load_balancer_sku is not None: <IF_STMT> return if namespace.load_balancer_sku.lower() != 'basic' and namespace.load_balancer_sku.lower() != 'standard': raise CLIError('--load-balancer-sku can only be standard or basic')",if namespace.load_balancer_sku == '':
"def _getLocalSpineType(self): if self._spineType is not None: return self._spineType else: for thisEvent in self.eventList: m1 = re.match('\\*\\*(.*)', thisEvent.contents) <IF_STMT> self._spineType = m1.group(1) return self._spineType return None",if m1:
def set_selected_device(self): current_devices = self.get_current_devices() if self.device in current_devices.values(): return for device_name in current_devices.values(): <IF_STMT> self.parent.py3.log(f'device {self.device} detected as {device_name}') self.device = device_name break,if self.device in device_name:
"def write(self, buff): if not self.handle: raise TTransportException(type=TTransportException.NOT_OPEN, message='Transport not open') sent = 0 have = len(buff) while sent < have: plus = self.handle.send(buff) <IF_STMT> raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket sent 0 bytes') sent += plus buff = buff[plus:]",if plus == 0:
"def get_named_key_value(self, rule, match, key_name): if key_name in rule: try: key_value = lookup_es_key(match, rule[key_name]) <IF_STMT> key_value = str(key_value) except KeyError: key_value = '_missing' else: key_value = None return key_value",if key_value is not None:
"def __iter__(self): protocol = self.protocol source = write_source_from_arg(self.source) with source.open('wb') as f: it = iter(self.table) hdr = next(it) <IF_STMT> pickle.dump(hdr, f, protocol) yield tuple(hdr) for row in it: pickle.dump(row, f, protocol) yield tuple(row)",if self.write_header:
"def abs__file__(): """"""Set all module' __file__ attribute to an absolute path"""""" for m in sys.modules.values(): <IF_STMT> continue try: m.__file__ = os.path.abspath(m.__file__) except (AttributeError, OSError): pass","if hasattr(m, '__loader__'):"
"def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() if now - when_pressed > self._debounce_time: if GPIO.input(self._channel) == self._expected: if not pressed: pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) el<IF_STMT> pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)",if pressed:
"def get_run_cmd(submission_dir): """"""Get the language of a submission"""""" with CD(submission_dir): if os.path.exists('run.sh'): with open('run.sh') as f: for line in f: <IF_STMT> return line.rstrip('\r\n')",if line[0] != '#':
"def client_read(self, path, **kwargs): """"""Retrieve a value from a etcd key."""""" try: res = self.client.read(path, timeout=kwargs.get('timeout', DEFAULT_TIMEOUT), recursive=kwargs.get('recursive') or kwargs.get('all', False)) <IF_STMT> modified_indices = (res.modifiedIndex,) + tuple((leaf.modifiedIndex for leaf in res.leaves)) return max(modified_indices) else: return res.value except EtcdKeyNotFound: raise KeyNotFound('The key %s was not found in etcd' % path) except TimeoutError as e: raise e","if kwargs.get('watch', False):"
"def populate_wrapper(klass, wrapping): for meth, how in klass._wrap_methods.items(): <IF_STMT> continue func = getattr(wrapping, meth) wrapper = make_wrapper(func, how) setattr(klass, meth, wrapper)","if not hasattr(wrapping, meth):"
"def _copy_files(self, files, src, dest, message=''): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) if message: print('{}: {}'.format(message, destpath)) if os.path.exists(srcpath): destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) <IF_STMT> os.remove(destpath)",elif os.path.exists(destpath):
"def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: if 'master' in node['flags']: cursor = '0' while cursor != 0: pieces = [cursor] if match is not None: pieces.extend(['MATCH', match]) <IF_STMT> pieces.extend(['COUNT', count]) response = await self.execute_command_on_nodes([node], 'SCAN', *pieces) cursor, data = list(response.values())[0] for item in data: yield item",if count is not None:
"def restart(cls, request, server_name): with cls._servername_to_shell_server_lock: <IF_STMT> servr = cls._servername_to_shell_server[server_name] servr.restart()",if server_name in cls._servername_to_shell_server:
"def human_waiting_on(self): if self.waiting_on is None: return 'N/A' things = [] for cluster, queue in self.waiting_on.items(): queue_length = len(queue) <IF_STMT> continue elif queue_length == 1: things.append(f'`{cluster}`: `{queue[0].get_instance()}`') else: things.append(f'`{cluster}`: {len(queue)} instances') return ', '.join(things)",if queue_length == 0:
"def psea(pname): """"""Parse PSEA output file."""""" fname = run_psea(pname) start = 0 ss = '' with open(fname) as fp: for l in fp: if l[0:6] == '>p-sea': start = 1 continue <IF_STMT> continue if l[0] == '\n': break ss = ss + l[0:-1] return ss",if not start:
"def encrypt_system_info_ssh_keys(ssh_info): for idx, user in enumerate(ssh_info): for field in ['public_key', 'private_key', 'known_hosts']: <IF_STMT> ssh_info[idx][field] = encryptor.enc(ssh_info[idx][field])",if ssh_info[idx][field]:
"def get_shape(shape): """"""Convert the shape to correct dtype and vars."""""" ret = [] for dim in shape: if isinstance(dim, tvm.tir.IntImm): if libinfo()['INDEX_DEFAULT_I64'] == 'ON': ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm('int32', val)) <IF_STMT> ret.append(te.var('any_dim', 'int32')) else: ret.append(dim) return ret","elif isinstance(dim, tvm.tir.Any):"
"def unpack(sources): temp_dir = tempfile.mkdtemp('-scratchdir', 'unpacker-') for package, content in sources.items(): filepath = package.split('/') dirpath = os.sep.join(filepath[:-1]) packagedir = os.path.join(temp_dir, dirpath) <IF_STMT> os.makedirs(packagedir) mod = open(os.path.join(packagedir, filepath[-1]), 'wb') try: mod.write(base64.b64decode(content)) finally: mod.close() return temp_dir",if not os.path.isdir(packagedir):
"def set_torrent_path(self, torrent_id, path): try: if not self.connect(): return False self.client.core.set_torrent_move_completed_path(torrent_id, path).get() self.client.core.set_torrent_move_completed(torrent_id, 1).get() except Exception: return False finally: <IF_STMT> self.disconnect() return True",if self.client:
"def _get_specs(self, link, source, target): for src_spec, code in link.code.items(): src_specs = src_spec.split('.') if src_spec.startswith('event:'): src_spec = (None, src_spec) <IF_STMT> src_spec = ('.'.join(src_specs[:-1]), src_specs[-1]) else: src_prop = src_specs[0] if isinstance(source, Reactive): src_prop = source._rename.get(src_prop, src_prop) src_spec = (None, src_prop) return [(src_spec, (None, None), code)]",elif len(src_specs) > 1:
"def deserialize(self, meth, content_type, body): meth_deserializers = getattr(meth, 'wsgi_deserializers', {}) try: mtype = _MEDIA_TYPE_MAP.get(content_type, content_type) <IF_STMT> deserializer = meth_deserializers[mtype] else: deserializer = self.default_deserializers[mtype] except (KeyError, TypeError): raise exception.InvalidContentType(content_type=content_type) return deserializer().deserialize(body)",if mtype in meth_deserializers:
"def object_inspect(self, oname, detail_level=0): """"""Get object info about oname"""""" with self.builtin_trap: info = self._object_find(oname) <IF_STMT> return self.inspector.info(info.obj, oname, info=info, detail_level=detail_level) else: return oinspect.object_info(name=oname, found=False)",if info.found:
"def wrapper(*args, **kargs): for key, value in vkargs.items(): <IF_STMT> abort(403, 'Missing parameter: %s' % key) try: kargs[key] = value(kargs[key]) except ValueError: abort(403, 'Wrong parameter format for: %s' % key) return func(*args, **kargs)",if key not in kargs:
"def _append_fragment(self, ctx, frag_content): try: ctx['dest_stream'].write(frag_content) ctx['dest_stream'].flush() finally: if self.__do_ytdl_file(ctx): self._write_ytdl_file(ctx) <IF_STMT> os.remove(encodeFilename(ctx['fragment_filename_sanitized'])) del ctx['fragment_filename_sanitized']","if not self.params.get('keep_fragments', False):"
"def override_args_required_option(argument_table, args, session, **kwargs): need_to_override = False if len(args) == 1 and args[0] == 'help' else True if need_to_override: parsed_configs = configutils.get_configs(session) for arg_name in argument_table.keys(): <IF_STMT> argument_table[arg_name].required = False","if arg_name.replace('-', '_') in parsed_configs:"
"def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) <IF_STMT> return 1 else: i += child._count(element, count=count) if not count and i: return i return i",elif element in child:
"def teardown_class(cls): collections = cls.discovery.list_collections(cls.environment_id).get_result()['collections'] for collection in collections: <IF_STMT> print('Deleting the temporary collection') cls.discovery.delete_collection(cls.environment_id, cls.collection_id) break",if collection['name'] == cls.collection_name:
"def _shares_in_results(data): shares_in_device, shares_in_subdevice = (False, False) for plugin_name, plugin_result in data.iteritems(): if plugin_result['status'] == 'error': continue <IF_STMT> continue if 'disk_shares' in plugin_result['device']: shares_in_device = True for subdevice in plugin_result['device'].get('subdevices', []): if 'disk_shares' in subdevice: shares_in_subdevice = True break return (shares_in_device, shares_in_subdevice)",if 'device' not in plugin_result:
"def accept_request(self, request): if self.restriction_type == BaseViewRestriction.PASSWORD: passed_restrictions = request.session.get(self.passed_view_restrictions_session_key, []) <IF_STMT> return False elif self.restriction_type == BaseViewRestriction.LOGIN: if not request.user.is_authenticated: return False elif self.restriction_type == BaseViewRestriction.GROUPS: if not request.user.is_superuser: current_user_groups = request.user.groups.all() if not any((group in current_user_groups for group in self.groups.all())): return False return True",if self.id not in passed_restrictions:
"def __setitem__(self, index, item): try: start, stop, step = (index.start, index.stop, index.step) except AttributeError: index = operator.index(index) else: <IF_STMT> self.lists[0][index] = item else: tmp = list(self) tmp[index] = item self.lists[:] = [tmp] self._balance_list(0) return list_idx, rel_idx = self._translate_index(index) if list_idx is None: raise IndexError() self.lists[list_idx][rel_idx] = item",if len(self.lists) == 1:
"def random_permutation_equality_groups(n_groups, n_perms_per_group, n_items, prob): fingerprints = set() for _ in range(n_groups): perms = random_equal_permutations(n_perms_per_group, n_items, prob) perm = perms[0] fingerprint = tuple((perm.get(i, i) for i in range(n_items))) <IF_STMT> yield perms fingerprints.add(fingerprint)",if fingerprint not in fingerprints:
"def get_proper_pip(): if not venv_active(): default_pip = os.environ.get('_DEFAULT_PIP_', None) <IF_STMT> return default_pip elif not ON_WINDOWS: return 'pip3' return 'pip'",if default_pip:
"def close(self, checkcount=False): self.mutex.acquire() try: if checkcount: self.openers -= 1 <IF_STMT> self.do_close() else: if self.openers > 0: self.do_close() self.openers = 0 finally: self.mutex.release()",if self.openers == 0:
"def _lxml_default_loader(href, parse, encoding=None, parser=None): if parse == 'xml': data = etree.parse(href, parser).getroot() else: <IF_STMT> f = urlopen(href) else: f = open(href, 'rb') data = f.read() f.close() if not encoding: encoding = 'utf-8' data = data.decode(encoding) return data",if '://' in href:
"def Save(self): eventHandler = self._window.GetEventHandler() isAGWAui = isinstance(eventHandler, AUI.AuiManager) if not isAGWAui: return True if self._manager.GetManagerStyle() & PM_SAVE_RESTORE_AUI_PERSPECTIVES: perspective = eventHandler.SavePerspective() <IF_STMT> name = PERSIST_AGW_AUI_PERSPECTIVE else: name = PERSIST_AUI_PERSPECTIVE self._pObject.SaveValue(name, perspective) return True",if isAGWAui:
"def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: if isinstance(arg_type, ScalarArg): result.append(arg_type.dtype) elif isinstance(arg_type, VectorArg): result.append(None) <IF_STMT> result.append(np.int64) else: raise RuntimeError('arg type not understood: %s' % type(arg_type)) return result",if arg_type.with_offset:
"def perform_secure_deletion_of_temporary_files(self): for f in os.listdir(self.state.settings.tmp_path): path = os.path.join(self.state.settings.tmp_path, f) timestamp = datetime.fromtimestamp(os.path.getmtime(path)) <IF_STMT> overwrite_and_remove(path)","if is_expired(timestamp, days=1):"
"def set_torrent_ratio(self, torrent_ids, ratio): try: <IF_STMT> return False self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get() self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get() except Exception as err: return False finally: if self.client: self.disconnect() return True",if not self.connect():
"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('MySQL backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value.replace(microsecond=0))",if settings.USE_TZ:
"def remote_run_capture_all(login, cmd, log=None): """"""Run the remote command and return the (retval, stdout, stderr) result."""""" if sys.platform == 'win32': <IF_STMT> login = '%s@%s' % (getpass.getuser(), login) cmd = 'plink -A -batch %s ""%s""' % (login, cmd) else: cmd = 'ssh -A -o BatchMode=yes %s ""%s""' % (login, cmd) __run_log(logstream, ""running '%s'"", cmd) p = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = p.communicate() status = p.returncode return (status, stdout, stderr)",if '@' not in login:
"def parseLeftHandSideExpressionAllowCall(): marker = None expr = None args = None property = None marker = createLocationMarker() expr = parseNewExpression() if matchKeyword('new') else parsePrimaryExpression() while (match('.') or match('[')) or match('('): <IF_STMT> args = parseArguments() expr = delegate.createCallExpression(expr, args) elif match('['): property = parseComputedMember() expr = delegate.createMemberExpression('[', expr, property) else: property = parseNonComputedMember() expr = delegate.createMemberExpression('.', expr, property) if marker: marker.end() marker.apply(expr) return expr",if match('('):
"def getImageId(self, stuff): if not isinstance(stuff, Module): return -1 if stuff.charge is None: return -1 else: iconFile = stuff.charge.iconID if stuff.charge.iconID else '' <IF_STMT> return self.fittingView.imageList.GetImageIndex(iconFile, 'icons') else: return -1",if iconFile:
"def instance_reader(): for epoch_index in range(epoch): if shuffle: if shuffle_seed is not None: np.random.seed(shuffle_seed) np.random.shuffle(examples) <IF_STMT> self.current_train_epoch = epoch_index for index, example in enumerate(examples): if phase == 'train': self.current_train_example = index + 1 feature = self.convert_example(index, example, self.get_labels(), self.max_seq_len, self.tokenizer) instance = self.generate_instance(feature) yield instance",if phase == 'train':
"def i2h(self, pkt, x): if x is not None: <IF_STMT> warning('Fixed3_6: Internal value too negative: %d' % x) x = 0 elif x > 999999999: warning('Fixed3_6: Internal value too positive: %d' % x) x = 999999999 x = x * 1e-06 return x",if x < 0:
"def _is_section_header(self) -> bool: section, underline = self._line_iter.peek(2) section = section.lower() if section in self._sections and isinstance(underline, str): return bool(_numpy_section_regex.match(underline)) elif self._directive_sections: <IF_STMT> for directive_section in self._directive_sections: if section.startswith(directive_section): return True return False",if _directive_regex.match(section):
def _parse_date_fmt(): fmt = get_format('DATE_FORMAT') escaped = False for char in fmt: if escaped: escaped = False elif char == '\\': escaped = True elif char in 'Yy': yield 'year' elif char in 'bEFMmNn': yield 'month' <IF_STMT> yield 'day',elif char in 'dj':
"def _wait_port_open(port, max_wait=60): print(f'Waiting for port {port}') start = time.time() while True: try: socket.create_connection(('localhost', port), timeout=1) except OSError: <IF_STMT> raise time.sleep(1) else: return",if time.time() - start > max_wait:
"def _list(self): data_sources = self.mkt_contract.functions.getAllProviders().call() data = [] for index, data_source in enumerate(data_sources): if index > 0: <IF_STMT> data.append(dict(dataset=self.to_text(data_source))) return pd.DataFrame(data)",if 'test' not in Web3.toText(data_source).lower():
"def log_start(self, prefix, msg): with self._log_lock: if self._last_log_prefix != prefix: <IF_STMT> self._log_file.write('\n') self._log_file.write(prefix) self._log_file.write(msg) self._last_log_prefix = prefix",if self._last_log_prefix is not None:
"def _split_string_to_tokens(text): """"""Splits text to a list of string tokens."""""" if not text: return [] ret = [] token_start = 0 is_alnum = [c in _ALPHANUMERIC_CHAR_SET for c in text] for pos in xrange(1, len(text)): if is_alnum[pos] != is_alnum[pos - 1]: token = text[token_start:pos] <IF_STMT> ret.append(token) token_start = pos final_token = text[token_start:] ret.append(final_token) return ret",if token != u' ' or token_start == 0:
"def _install_groups(self, grp_specs): try: self.base.env_group_install(grp_specs, tuple(self.base.conf.group_package_types), strict=self.base.conf.strict) except dnf.exceptions.Error: <IF_STMT> raise",if self.base.conf.strict:
def _idx2token(idxs): for idx in idxs: if idx < self.tgt_vocab_size: token = self.tgt_vocab([[idx]])[0][0] <IF_STMT> break yield token else: yield self.kb_keys[idx - self.tgt_vocab_size],if token == self.eos_token:
"def increment(s): if not s: return '1' for sequence in (string.digits, string.lowercase, string.uppercase): lastc = s[-1] if lastc in sequence: i = sequence.index(lastc) + 1 if i >= len(sequence): <IF_STMT> s = sequence[0] * 2 if s == '00': s = '10' else: s = increment(s[:-1]) + sequence[0] else: s = s[:-1] + sequence[i] return s return s",if len(s) == 1:
"def main(): import sys, getopt try: opts, args = getopt.getopt(sys.argv[1:], 'ho:', ['help', 'output=']) except getopt.GetoptError as err: usage() sys.exit(1) output = None for o, a in opts: if o in ('-h', '--help'): usage() sys.exit() <IF_STMT> output = a else: usage() sys.exit(1) if not args: usage() sys.exit(1) concat_flv(args, output)","elif o in ('-o', '--output'):"
def binaryFindInDocument(): hi = len(self.headings) lo = 0 while lo < hi: mid = (lo + hi) // 2 h = self.headings[mid] <IF_STMT> lo = mid + 1 elif h.start > position: hi = mid else: return binaryFindHeading(h),if h.end_of_last_child < position:
"def on_key_press(self, *events): for ev in events: <IF_STMT> ivar = 'minibufferWidget' if self.name == 'minibuffer' else self.name self.root.do_key(ev, ivar)",if self.should_be_leo_key(ev):
"def _make_dataset(data_dir): data_dir = os.path.expanduser(data_dir) if not os.path.isdir(data_dir): raise '{} should be a dir'.format(data_dir) images = [] for root, _, fnames in sorted(os.walk(data_dir, followlinks=True)): for fname in sorted(fnames): file_path = os.path.join(root, fname) <IF_STMT> images.append(file_path) return images",if _is_valid_file(file_path):
"def release(provider, connection, cache=None): if cache is not None: db_session = cache.db_session <IF_STMT> try: cursor = connection.cursor() sql = 'SET foreign_key_checks = 1' if core.local.debug: log_orm(sql) cursor.execute(sql) except: provider.pool.drop(connection) raise DBAPIProvider.release(provider, connection, cache)",if db_session is not None and db_session.ddl and cache.saved_fk_state:
"def get_pfunctions(self): p_functions = [] for name, item in self.pdict.items(): if name[:2] != 'p_': continue <IF_STMT> continue if isinstance(item, (types.FunctionType, types.MethodType)): line = func_code(item).co_firstlineno file = func_code(item).co_filename p_functions.append((line, file, name, item.__doc__)) p_functions.sort() self.pfuncs = p_functions",if name == 'p_error':
"def get_output_sizes(self): sizes = [] output_paths = self.get_output_fnames() for outfile in [unicodify(o) for o in output_paths]: <IF_STMT> sizes.append((outfile, os.stat(outfile).st_size)) else: sizes.append((outfile, 0)) return sizes",if os.path.exists(outfile):
"def normalize_crlf(tree): for elem in tree.getiterator(): <IF_STMT> elem.text = elem.text.replace('\r\n', '\n') if elem.tail: elem.tail = elem.tail.replace('\r\n', '\n')",if elem.text:
"def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: <IF_STMT> if self.process_name_expr_decorator(decorator, o): is_abstract = True elif isinstance(decorator, MemberExpr): if self.process_member_expr_decorator(decorator, o): is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)","if isinstance(decorator, NameExpr):"
"def formatweekday(self, day, width): with TimeEncoding(self.locale) as encoding: <IF_STMT> names = day_name else: names = day_abbr name = names[day] if encoding is not None: name = name.decode(encoding) return name[:width].center(width)",if width >= 9:
def autocommitter(): while True: try: <IF_STMT> break if self._auto_commit_enable: self._auto_commit() self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000) except ReferenceError: break except Exception: self._worker_exception = sys.exc_info() break log.debug('Autocommitter thread exiting'),if not self._running:
"def pseudo_raw_input(self, prompt): """"""copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout"""""" if self.use_rawinput: try: line = raw_input(prompt) except EOFError: line = 'EOF' else: self.stdout.write(prompt) self.stdout.flush() line = self.stdin.readline() if not len(line): line = 'EOF' el<IF_STMT> line = line[:-1] return line",if line[-1] == '\n':
"def get_suggestion(self, suggestion): if suggestion is None: return suggestion counter = 0 results = [] for feature in self._features: if feature in self._discrete_features: result, counter = self._get_discrete_suggestion(feature=feature, suggestion=suggestion, counter=counter) results.append(result) <IF_STMT> result, counter = self._get_categorical_suggestion(feature=feature, suggestion=suggestion, counter=counter) results.append(result) else: results.append(suggestion[counter]) counter = counter + 1 return dict(zip(self._features, results))",elif feature in self._categorical_features:
def gen_raw_options(modelines): for m in modelines: opt = m.partition(':')[2].strip() <IF_STMT> for subopt in (s for s in opt.split(MULTIOPT_SEP)): yield subopt else: yield opt,if MULTIOPT_SEP in opt:
"def _parse_chunked(self, data): body = [] trailers = {} n = 0 lines = data.split(b'\r\n') while True: size, chunk = lines[n:n + 2] size = int(size, 16) if size == 0: n += 1 break self.assertEqual(size, len(chunk)) body.append(chunk) n += 2 <IF_STMT> break return b''.join(body)",if n > len(lines):
"def join(s, *p): path = s for t in p: <IF_STMT> path = t continue if t[:1] == ':': t = t[1:] if ':' not in path: path = ':' + path if path[-1:] != ':': path = path + ':' path = path + t return path",if not s or isabs(t):
"def validate_route_filter(cmd, namespace): from msrestazure.tools import is_valid_resource_id, resource_id if namespace.route_filter: <IF_STMT> namespace.route_filter = resource_id(subscription=get_subscription_id(cmd.cli_ctx), resource_group=namespace.resource_group_name, namespace='Microsoft.Network', type='routeFilters', name=namespace.route_filter)",if not is_valid_resource_id(namespace.route_filter):
"def expanded_output(self): """"""Iterate over output files while dynamic output is expanded."""""" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion = self.expand_dynamic(f_) <IF_STMT> yield f_ for f, _ in expansion: file_to_yield = IOFile(f, self.rule) file_to_yield.clone_flags(f_) yield file_to_yield else: yield f",if not expansion:
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') if sty.italic: fragment = '<i>%s</i>' % fragment if sty.underline: fragment = '<u>%s</u>' % fragment if sty.strikeout: fragment = '<s>%s</s>' % fragment <IF_STMT> raise ContentNotUsable body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.drawing:
"def decref(self, key, count=1): with self._lock: slot = self._dict[key] <IF_STMT> del self._dict[key] else: slot[1] -= count self._dict[key] = slot",if slot[1] < count:
"def stale_rec(node, nodes): if node.abspath() in node.ctx.env[Build.CFG_FILES]: return if getattr(node, 'children', []): for x in node.children.values(): if x.name != 'c4che': stale_rec(x, nodes) else: for ext in DYNAMIC_EXT: <IF_STMT> break else: if not node in nodes: if can_delete(node): Logs.warn('Removing stale file -> %r', node) node.delete()",if node.name.endswith(ext):
"def _do_ssl_handshake(self): try: self.socket.do_handshake() except ssl.SSLError as err: if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE): return elif err.args[0] == ssl.SSL_ERROR_EOF: return self.handle_close() raise except OSError as err: <IF_STMT> return self.handle_close() else: self._ssl_accepting = False",if err.args[0] == errno.ECONNABORTED:
"def test_full_hd_tv(self): cur_test = 'full_hd_tv' cur_qual = common.Quality.FULLHDTV for name, tests in iteritems(self.test_cases): for test in tests: <IF_STMT> self.assertEqual(cur_qual, common.Quality.name_quality(test)) else: self.assertNotEqual(cur_qual, common.Quality.name_quality(test))",if name == cur_test:
"def debug_tree(tree): l = [] for elt in tree: if isinstance(elt, int): l.append(_names.get(elt, elt)) <IF_STMT> l.append(elt) else: l.append(debug_tree(elt)) return l","elif isinstance(elt, str):"
"def get_all_missing_headers(self): for chunk_height, expected_hash in reversed(list(self.checkpoints.items())): if chunk_height in self.known_missing_checkpointed_chunks: continue <IF_STMT> self.known_missing_checkpointed_chunks.add(chunk_height) return self.known_missing_checkpointed_chunks","if self.chunk_hash(chunk_height, 1000) != expected_hash:"
"def get_byname(userId, documentName, session=None): if not session: session = db.Session ret = {} result = session.query(LegacyArchiveDocument).filter_by(userId=userId, documentName=documentName).first() if result: obj = dict(((key, value) for key, value in vars(result).items() <IF_STMT>)) ret = obj return ret",if not key.startswith('_')
"def cb(ipdb, msg, action): if action == 'RTM_NEWLINK' and msg.get_attr('IFLA_IFNAME', '') in (ifP1, ifP2): obj = ipdb.interfaces[msg['index']] <IF_STMT> ipdb.interfaces[ifM].add_port(obj) try: ipdb.interfaces[ifM].commit() except Exception: pass",if obj not in ipdb.interfaces[ifM]:
"def reorder_encoder_rules(self, nts): """"""reorder rules so that any rules with ENCODER_PREFERRED is first"""""" for nt in nts.values(): first_rules = [] rest_of_the_rules = [] for r in nt.rules: <IF_STMT> first_rules.append(r) else: rest_of_the_rules.append(r) nt.rules = first_rules + rest_of_the_rules",if r.conditions.contains('ENCODER_PREFERRED'):
"def update_url(self, s, keywords): pc = self w = pc.ensure_text_widget() pc.show() if 1: w.setPlainText('') else: url = pc.get_url(s, '@url') <IF_STMT> w.setPlainText('@url %s' % url) else: w.setPlainText('@url: no url given')",if url:
"def _update_engines(self, engines): """"""Update our engines dict and _ids from a dict of the form: {id:uuid}."""""" for k, v in iteritems(engines): eid = int(k) <IF_STMT> self._ids.append(eid) self._engines[eid] = v self._ids = sorted(self._ids) if sorted(self._engines.keys()) != list(range(len(self._engines))) and self._task_scheme == 'pure' and self._task_socket: self._stop_scheduling_tasks()",if eid not in self._engines:
def test_delete_chat_thread(self): async with self.chat_client: await self._create_thread() await self.chat_client.delete_chat_thread(self.thread_id) <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():
"def _to_protobuf_matrix(matrix, p_matrix, transformation=None): for row in matrix: p_row = p_matrix.rows.add() for cell in row: value = cell <IF_STMT> value = transformation(value) p_row.cells.append(value)",if transformation:
"def apply(self, db, family): if self.rtype: <IF_STMT> if self.regex[0].search(str(family.get_relationship())) is None: return False elif self.rtype != family.get_relationship(): return False return True",if self.rtype.is_custom() and self.use_regex:
"def get_somatic_variantcallers(items): """"""Retrieve all variant callers for somatic calling, handling somatic/germline."""""" out = [] for data in items: vcs = dd.get_variantcaller(data) <IF_STMT> vcs = vcs['somatic'] if not isinstance(vcs, (list, tuple)): vcs = [vcs] out += vcs return set(vcs)","if isinstance(vcs, dict) and 'somatic' in vcs:"
"def balancer_list_members(self, balancer): lb = self._get_balancer_model(balancer.id) members = [] vs = self._locate_service_group(lb, balancer.port) if vs: <IF_STMT> srvgrp = vs['serviceGroups'][0] members = [self._to_member(srv, balancer) for srv in srvgrp['services']] return members",if vs['serviceGroups']:
"def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split('] ')[1] + '.' except IndexError: error_msg = str(err_msg.args[0]) + '.' <IF_STMT> if settings.VERBOSITY_LEVEL < 2: print(settings.FAIL_STATUS) elif settings.VERBOSITY_LEVEL < 1: print('') print(settings.print_critical_msg(error_msg)) raise SystemExit()",if settings.INIT_TEST == True:
"def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): <IF_STMT> self.library_dirs.append(dir) elif fatal: fail('FATAL: bad directory %s in environment variable %s' % (dir, envvar))",if not dir in self.library_dirs:
"def check_placement_group_index(placement_group: PlacementGroup, bundle_index: int): assert placement_group is not None if placement_group.id.is_nil(): <IF_STMT> raise ValueError('If placement group is not set, the value of bundle index must be -1.') elif bundle_index >= placement_group.bundle_count or bundle_index < -1: raise ValueError(f'placement group bundle index {bundle_index} is invalid. Valid placement group indexes: 0-{placement_group.bundle_count}')",if bundle_index != -1:
"def incoming(): while True: m = ws.receive() if m is not None: m = str(m) print((m, len(m))) <IF_STMT> ws.close() break else: break print(('Connection closed!',))",if len(m) == 35:
"def walk_tree(root: Element, processor: Callable[[Element], Optional[_T]], stop_after_first: bool=False) -> List[_T]: results = [] queue = deque([root]) while queue: currElement = queue.popleft() for child in currElement: if child: queue.append(child) result = processor(child) if result is not None: results.append(result) <IF_STMT> return results return results",if stop_after_first:
"def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: <IF_STMT> return item if self._tree.ItemHasChildren(item): result = self._find_node_with_predicate(item, predicate) if result: return result item, cookie = self._tree.GetNextChild(node, cookie) return None",if predicate(item):
"def traverse_coords(coords, dst_coords): for p in coords: <IF_STMT> lst = [] traverse_coords(p, lst) dst_coords.append(lst) else: x, y = (p[0], p[1]) d = (x + (y - b) * m) / (1 + m * m) x2 = 2 * d - x y2 = 2 * d * m - y + 2 * b dst_coords.append((x2, y2)) return dst_coords",if type(p[0]) is list:
"def normalize_replies(self, x): xs = x.split('\n') xs2 = [] for x in xs: <IF_STMT> x = x[len('your persona: '):] x = normalize_reply(x) x = 'your persona: ' + x else: x = normalize_reply(x) xs2.append(x) return '\n'.join(xs2)",if 'your persona:' in x:
"def run_unittest(*classes): suite = unittest.TestSuite() for c in classes: <IF_STMT> c = __import__(c) for name in dir(c): obj = getattr(c, name) if isinstance(obj, type) and issubclass(obj, unittest.TestCase): suite.addTest(obj) else: suite.addTest(c) runner = unittest.TestRunner() result = runner.run(suite)","if isinstance(c, str):"
"def bprop_naive(self, error, permute=False): for dst in range(self.ofmsize): rflinks = self.links[dst] A = error[:, self.ofmlocs[dst]] B = self.weights <IF_STMT> inds = np.random.permutation(A.shape[1]) np.dot(A[:, inds], B[inds, :], self.bpropbuf) else: np.dot(A, B, self.bpropbuf) self.berror[:, rflinks] += self.bpropbuf",if permute:
"def rewrite_order_lookup_key(model, lookup_key): try: <IF_STMT> return '-' + rewrite_lookup_key(model, lookup_key[1:]) else: return rewrite_lookup_key(model, lookup_key) except AttributeError: return lookup_key",if lookup_key.startswith('-'):
"def test_default_configuration(self): transformations = [] for i in range(2): transformation, original = self._test_helper(RescalingChoice, dataset='boston') self.assertAlmostEqual(np.mean(transformation), 0, places=5) self.assertAlmostEqual(np.std(transformation), 1, places=5) self.assertFalse((original == transformation).all()) transformations.append(transformation) <IF_STMT> self.assertTrue((transformations[-1] == transformations[-2]).all())",if len(transformations) > 1:
"def test_get_filter_text(self): with realized(self.b): <IF_STMT> self.assertEqual(self.b.get_filter_text(), u'') self.assertTrue(isinstance(self.b.get_filter_text(), str)) self.b.filter_text(u'foo') self.assertEqual(self.b.get_filter_text(), u'foo') self.assertTrue(isinstance(self.b.get_filter_text(), str))",if self.b.can_filter_text():
"def _namelist(instance): namelist, namedict, classlist = ([], {}, [instance.__class__]) for c in classlist: for b in c.__bases__: classlist.append(b) for name in c.__dict__.keys(): <IF_STMT> namelist.append(name) namedict[name] = 1 return namelist",if not namedict.has_key(name):
"def resolve_cloudtrail_payload(self, payload): sources = self.data.get('sources', []) events = [] for e in self.data.get('events'): if not isinstance(e, dict): events.append(e) event_info = CloudWatchEvents.get(e) <IF_STMT> continue else: event_info = e events.append(e['event']) sources.append(event_info['source']) payload['detail'] = {'eventSource': list(set(sources)), 'eventName': events}",if event_info is None:
"def __setitem__(self, aset, c): if isinstance(aset, tuple): <IF_STMT> row = self.rownames.index(aset[0]) else: row = aset[0] if isinstance(aset[1], str): column = self.colnames.index(aset[1]) else: column = aset[1] self.cell_value(row, column, c) else: Matrix.__setitem__(self, aset, c)","if isinstance(aset[0], str):"
"def test_retrieve_robots_token_permission(username, is_admin, with_permissions, app, client): with client_with_identity(username, client) as cl: params = {'orgname': 'buynlarge', 'token': 'true'} <IF_STMT> params['permissions'] = 'true' result = conduct_api_call(cl, OrgRobotList, 'GET', params, None) assert result.json['robots'] for robot in result.json['robots']: assert (robot.get('token') is not None) == is_admin assert (robot.get('repositories') is not None) == (is_admin and with_permissions)",if with_permissions:
"def _analyze_ast(contents): try: return ast.literal_eval(contents) except SyntaxError: pass try: contents = re.sub(re.compile('/\\*.*?\\*/', re.DOTALL), '', contents) contents = re.sub(re.compile('#.*?\\n'), '', contents) match = re.match('^([^{]+)', contents) <IF_STMT> contents = contents.replace(match.group(1), '') return ast.literal_eval(contents) except SyntaxError: pass return False",if match:
"def bulk_disable_accounts(account_names): """"""Bulk disable accounts"""""" for account_name in account_names: account = Account.query.filter(Account.name == account_name).first() <IF_STMT> app.logger.debug('Disabling account %s', account.name) account.active = False db.session.add(account) db.session.commit() db.session.close()",if account:
"def _add_agent_rewards(self, reward_dict: Dict[AgentID, float]) -> None: for agent_id, reward in reward_dict.items(): <IF_STMT> self.agent_rewards[agent_id, self.policy_for(agent_id)] += reward self.total_reward += reward self._agent_reward_history[agent_id].append(reward)",if reward is not None:
"def wrapper(strategy, backend, pipeline_index, *args, **kwargs): current_partial = partial_prepare(strategy, backend, pipeline_index, *args, **kwargs) out = func(*args, strategy=strategy, backend=backend, pipeline_index=pipeline_index, current_partial=current_partial, **kwargs) or {} if not isinstance(out, dict): strategy.storage.partial.store(current_partial) <IF_STMT> strategy.session_set(PARTIAL_TOKEN_SESSION_NAME, current_partial.token) return out",if save_to_session:
def restore_text(self): if self.source_is_console(): cb = self._last_console_cb else: cb = self._last_editor_cb if cb is None: <IF_STMT> self.plain_text.clear() else: self.rich_text.clear() else: func = cb[0] args = cb[1:] func(*args) if get_meth_class_inst(func) is self.rich_text: self.switch_to_rich_text() else: self.switch_to_plain_text(),if self.is_plain_text_mode():
"def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: <IF_STMT> group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return (groups, words)",if previous == word:
"def pendingcalls_thread(self, context): try: self.pendingcalls_submit(context.l, context.n) finally: with context.lock: context.nFinished += 1 nFinished = context.nFinished if False and support.verbose: print('finished threads: ', nFinished) <IF_STMT> context.event.set()",if nFinished == context.nThreads:
"def __getattr__(self, item: str) -> Any: if hasattr(MissingPandasLikeRolling, item): property_or_func = getattr(MissingPandasLikeRolling, item) <IF_STMT> return property_or_func.fget(self) else: return partial(property_or_func, self) raise AttributeError(item)","if isinstance(property_or_func, property):"
"def _csv(self, match=None, dump=None): if dump is None: dump = self._dump(match) for record in dump: row = [] for field in record: if isinstance(field, int): row.append('%i' % field) <IF_STMT> row.append('') else: row.append(""'%s'"" % field) yield ','.join(row)",elif field is None:
def get_default_dict(section_definition): section_key = section_definition.get('key') if section_key == 'global': section_key += '_' if 'cluster' == section_key: section_key += '_sit' <IF_STMT> else '_hit' default_dict = DefaultDict[section_key].value return default_dict,if section_definition.get('cluster_model') == ClusterModel.SIT.name
"def scan_resource_conf(self, conf): subscription = re.compile('\\/|\\/subscriptions\\/[\\w\\d-]+$|\\[subscription\\(\\).id\\]') if 'properties' in conf: <IF_STMT> if any((re.match(subscription, scope) for scope in conf['properties']['assignableScopes'])): if 'permissions' in conf['properties']: if conf['properties']['permissions']: for permission in conf['properties']['permissions']: if 'actions' in permission and '*' in permission['actions']: return CheckResult.FAILED return CheckResult.PASSED",if 'assignableScopes' in conf['properties']:
"def hard_update(self, cache, size_change, pins_gates): """"""replace verts, rads and vel (in NumPy)"""""" verts, rads, vel, react = cache if len(verts) == self.v_len: if pins_gates[0] and pins_gates[1]: unpinned = self.params['unpinned'] self.verts[unpinned] = verts[unpinned] else: self.verts = verts self.vel = vel <IF_STMT> self.rads = rads",if not size_change:
"def run(self): if self.check(): path = '/../../../../../../../../../../../..{}'.format(self.filename) response = self.http_request(method='GET', path=path) if response is None: return <IF_STMT> print_success('Success! File: %s' % self.filename) print_info(response.text) else: print_error('Exploit failed') else: print_error('Device seems to be not vulnerable')",if response.status_code == 200 and response.text:
"def write_text(self, text): """"""Writes re-indented text into the buffer."""""" should_indent = False rows = [] for row in text.split('\n'): if should_indent: row = '{}'.format(row) if '\x08' in row: row = row.replace('\x08', '', 1) should_indent = True <IF_STMT> should_indent = False rows.append(row) self.write('{}\n'.format('\n'.join(rows)))",elif not len(row.strip()):
"def default_logger(): """"""A logger used to output seed information to nosetests logs."""""" logger = logging.getLogger(__name__) if not len(logger.handlers): handler = logging.StreamHandler(sys.stderr) handler.setFormatter(logging.Formatter('[%(levelname)s] %(message)s')) logger.addHandler(handler) <IF_STMT> logger.setLevel(logging.INFO) return logger",if logger.getEffectiveLevel() == logging.NOTSET:
"def while1_test(a, b, c): while 1: if a != 2: if b: a = 3 b = 0 <IF_STMT> c = 0 else: a += b + c break return (a, b, c)",elif c:
"def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split('\n'): line = line.strip() if not line or line.startswith('#') or '.' not in line: continue if ' # ' in line: reason = line.split(' # ')[1].split()[0].lower() <IF_STMT> continue retval[line.split(' # ')[0]] = (__info__, __reference__) return retval",if reason == 'scanning':
"def create_order(order, shopify_settings, old_order_sync=False, company=None): so = create_sales_order(order, shopify_settings, company) if so: <IF_STMT> create_sales_invoice(order, shopify_settings, so, old_order_sync=old_order_sync) if order.get('fulfillments') and (not old_order_sync): create_delivery_note(order, shopify_settings, so)",if order.get('financial_status') == 'paid':
"def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) if key >= l: raise IndexError('Index %s out of range (%s elements)' % (key, l)) <IF_STMT> if key < -l: raise IndexError('Index %s out of range (%s elements)' % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError(self.impl.__class__.__name__ + ' object does not support slicing') else: return self(key)",if key < 0:
"def load_checks(path=None, subpkg=''): """"""Dynamically import all check modules for the side effect of registering checks."""""" if path is None: path = os.path.dirname(__file__) modules = [] for name in os.listdir(path): if os.path.isdir(os.path.join(path, name)): modules = modules + load_checks(os.path.join(path, name), subpkg + '.' + name) continue <IF_STMT> modules.append(import_module(__package__ + subpkg + '.' + name[:-3])) return modules",if name.endswith('.py') and name not in LOADER_EXCLUDES:
"def _remove_temporary_files(self, temporary_files): """"""Internal function for cleaning temporary files"""""" for file_object in temporary_files: file_name = file_object.name file_object.close() if os.path.exists(file_name): os.remove(file_name) arff_file_name = file_name + '.arff' <IF_STMT> os.remove(arff_file_name)",if os.path.exists(arff_file_name):
"def search_rotate(array, val): low, high = (0, len(array) - 1) while low <= high: mid = (low + high) // 2 if val == array[mid]: return mid if array[low] <= array[mid]: if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 el<IF_STMT> low = mid + 1 else: high = mid - 1 return -1",if array[mid] <= val <= array[high]:
"def match_file(self, file, tff_format): match = tff_format.search(file.filename.replace('\\', '/')) if match: result = {} for name, value in match.groupdict().items(): value = value.strip() if name in self.numeric_tags: value = value.lstrip('0') <IF_STMT> value = value.replace('_', ' ') result[name] = value return result else: return {}",if self.ui.replace_underscores.isChecked():
"def exclude_pkgs(self, pkgs): name = 'excludepkgs' if pkgs is not None and pkgs != []: <IF_STMT> self._set_value(name, pkgs, dnf.conf.PRIO_COMMANDLINE) else: logger.warning(_('Unknown configuration option: %s = %s'), ucd(name), ucd(pkgs))",if self._has_option(name):
"def button_press_cb(self, tdw, event): if self.zone in (_EditZone.CREATE_AXIS, _EditZone.DELETE_AXIS): button = event.button <IF_STMT> self._click_info = (button, self.zone) return False return super(SymmetryEditMode, self).button_press_cb(tdw, event)",if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS:
"def declare_var(self, type_name: Union[str, Tuple[str, str]], *, var_name: str='', var_name_prefix: str='v', shared: bool=False) -> str: if shared: <IF_STMT> var_name = var_name_prefix if var_name not in self.shared_vars: self.declarations.append((var_name, type_name)) self.shared_vars.add(var_name) else: if not var_name: var_name = self.get_var_name(var_name_prefix) self.declarations.append((var_name, type_name)) return var_name",if not var_name:
"def get_module_map(module, module_path): """"""Map true modules to exported name"""""" if not module_is_public(module): return {} m = {} for symbol_name in dir(module): if symbol_name.startswith('_'): continue symbol = getattr(module, symbol_name) symbol_path = '%s.%s' % (module_path, symbol_name) m[symbol] = symbol_path <IF_STMT> m.update(get_module_map(symbol, symbol_path)) return m",if inspect.ismodule(symbol):
"def build_properties(self): self.properties = set() if self.module.partial_scan == True: for prop in self.COMMON_PROPERTIES: self.properties.add(chr(prop)) else: for pb in range(0, 9): for lp in range(0, 5): for lc in range(0, 5): prop = self.build_property(pb, lp, lc) <IF_STMT> self.properties.add(chr(prop))",if prop is not None:
"def getFileIdFromAlternateLink(altLink): loc = altLink.find('/d/') <IF_STMT> fileId = altLink[loc + 3:] loc = fileId.find('/') if loc != -1: return fileId[:loc] else: loc = altLink.find('/folderview?id=') if loc > 0: fileId = altLink[loc + 15:] loc = fileId.find('&') if loc != -1: return fileId[:loc] controlflow.system_error_exit(2, f'{altLink} is not a valid Drive File alternateLink')",if loc > 0:
"def _coerce_trials_data(data, path): if not isinstance(data, list): if not isinstance(data, dict): raise BatchFileError(path, 'invalid data type for trials: expected list or dict, got %s' % type(data).__name__) data = [data] for item in data: <IF_STMT> raise BatchFileError(path, 'invalid data type for trial %r: expected dict' % item) return data","if not isinstance(item, dict):"
"def remove(self, *objs): val = getattr(self.instance, attname) for obj in objs: <IF_STMT> setattr(obj, rel_field.name, None) obj.save() else: raise rel_field.rel.to.DoesNotExist('%r is not related to %r.' % (obj, self.instance))","if getattr(obj, rel_field.attname) == val:"
"def run(self): try: <IF_STMT> self.shell = os.name == 'nt' if self.working_dir != '': os.chdir(self.working_dir) proc = subprocess.Popen(self.command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=self.shell, env=self.env) output = codecs.decode(proc.communicate()[0]) self.on_done(output) except subprocess.CalledProcessError as e: self.on_done(e.returncode, error=True) except OSError as e: self.on_done(e.message, error=True)",if not self.shell:
"def filter_testsuite(suite, matcher, level=None): """"""Returns a flattened list of test cases that match the given matcher."""""" if not isinstance(suite, unittest.TestSuite): raise TypeError('not a TestSuite', suite) results = [] for test in suite._tests: if level is not None and getattr(test, 'level', 0) > level: continue <IF_STMT> testname = test.id() if matcher(testname): results.append(test) else: filtered = filter_testsuite(test, matcher, level) results.extend(filtered) return results","if isinstance(test, unittest.TestCase):"
"def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args): triggered = False for i in self._touchable_widgets: if i.collide_point(touch.x, touch.y): triggered = True if touch_event == 'down': i.on_touch_down(touch) elif touch_event == 'move': i.on_touch_move(touch, *args) <IF_STMT> i.on_touch_up(touch) return triggered",elif touch_event == 'up':
"def add_attributes(attributes, all_base64): lines = [] oc_attr = None for attr in attributes: if attr.lower() == 'objectclass': for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) oc_attr = attr break for attr in attributes: <IF_STMT> for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) return lines",if attr != oc_attr and attr in attributes:
"def split_quality(quality): anyQualities = [] bestQualities = [] for curQual in Quality.qualityStrings.keys(): <IF_STMT> anyQualities.append(curQual) if curQual << 16 & quality: bestQualities.append(curQual) return (sorted(anyQualities), sorted(bestQualities))",if curQual & quality:
"def check(dbdef): """"""database version must include required keys"""""" for vnum, vdef in dbdef.items(): missing = set(required) - set(vdef) if vnum == min(dbdef): missing -= set(initially_ok) <IF_STMT> yield (vnum, missing)",if missing:
"def teardown_func(): try: yield finally: 'tear down test fixtures' cache = os.path.join(here, 'data', 'cache.db') <IF_STMT> os.remove(cache)",if os.path.exists(cache):
"def getCachedArt(albumid): from headphones import cache c = cache.Cache() artwork_path = c.get_artwork_from_cache(AlbumID=albumid) if not artwork_path: return if artwork_path.startswith('http://'): artwork = request.request_content(artwork_path, timeout=20) <IF_STMT> logger.warn('Unable to open url: %s', artwork_path) return else: with open(artwork_path, 'r') as fp: return fp.read()",if not artwork:
"def delete_volume(self, name, reraise=False): try: self.k8s_api.delete_persistent_volume(name=name, body=client.V1DeleteOptions(api_version=constants.K8S_API_VERSION_V1)) logger.debug('Volume `{}` Deleted'.format(name)) except ApiException as e: <IF_STMT> raise PolyaxonK8SError('Connection error: %s' % e) from e else: logger.debug('Volume `{}` was not found'.format(name))",if reraise:
"def _hashable(self): hashes = [self.graph.md5()] for g in self.geometry.values(): <IF_STMT> hashes.append(g.md5()) elif hasattr(g, 'tostring'): hashes.append(str(hash(g.tostring()))) else: hashes.append(str(hash(g))) hashable = ''.join(sorted(hashes)).encode('utf-8') return hashable","if hasattr(g, 'md5'):"
"def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) <IF_STMT> if c['data'] not in history: history[c['data']] = c['timestamp'] if len(history) >= count: break return history",if c['tries'] == 0 or c['tries'] is None:
"def renderable_events(self, date, hour): """"""Returns the number of renderable events"""""" renderable_events = [] for event in self.events: <IF_STMT> renderable_events.append(event) if hour: for current in renderable_events: for event in self.events: if event not in renderable_events: for hour in range(self.start_hour, self.end_hour): if current.covers(date, hour) and event.covers(date, hour): renderable_events.append(event) break return renderable_events","if event.covers(date, hour):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def _parseConfigFile(self, iniPath, createConfig=True): parser = SafeConfigParserUnicode(strict=False) if not os.path.isfile(iniPath): if createConfig: open(iniPath, 'w').close() else: return parser.readfp(codecs.open(iniPath, 'r', 'utf_8_sig')) for section, options in list(self._iniStructure.items()): <IF_STMT> for option in options: if parser.has_option(section, option): self._config[option] = parser.get(section, option)",if parser.has_section(section):
"def get_block_id_at_height(store, height, descendant_id): if height is None: return None while True: block = store._load_block(descendant_id) <IF_STMT> return descendant_id descendant_id = block['search_id' if util.get_search_height(block['height']) >= height else 'prev_id']",if block['height'] == height:
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): if readies[idx] < min_counts[idx]: all_satisfy = False readies[idx] = count_fun(selector) break if all_satisfy: break <IF_STMT> raise TimeoutError('Wait cluster start timeout') time.sleep(1)",if timeout and timeout + start_time < time.time():
"def waitForNodes(self, expected, comparison=None, tag_filters={}): MAX_ITER = 50 for i in range(MAX_ITER): n = len(self.provider.non_terminated_nodes(tag_filters)) if comparison is None: comparison = self.assertEqual try: comparison(n, expected) return except Exception: <IF_STMT> raise time.sleep(0.1)",if i == MAX_ITER - 1:
"def _api_snapshot_delete(self, drbd_rsc_name, snap_name): with lin_drv(self.default_uri) as lin: <IF_STMT> lin.connect() snap_reply = lin.snapshot_delete(rsc_name=drbd_rsc_name, snapshot_name=snap_name) return snap_reply",if not lin.connected:
"def response(resp): results = [] search_results = loads(resp.text) if not search_results.get('query', {}).get('search'): return [] for result in search_results['query']['search']: <IF_STMT> continue url = base_url.format(language=resp.search_params['language']) + 'wiki/' + quote(result['title'].replace(' ', '_').encode('utf-8')) results.append({'url': url, 'title': result['title'], 'content': ''}) return results","if result.get('snippet', '').startswith('#REDIRECT'):"
"def getBody(self, path): if path == '': return 'This server has ' + str(self.__fileProvider.count()) + ' files.' else: downloadCounts = self.__fileProvider.get(path).downloadCount <IF_STMT> return str(downloadCounts[path]) else: return '0'",if path in downloadCounts:
"def parse_entrypoints(self, content: str, root=None) -> RootDependency: if root is None: root = RootDependency() entrypoints = [] group = 'console_scripts' for line in content.split('\n'): line = line.strip() if not line or line[0] in '#;': continue <IF_STMT> group = line[1:-1] else: entrypoints.append(EntryPoint.parse(text=line, group=group)) root.entrypoints = tuple(entrypoints) return root",if line[0] == '[' and line[-1] == ']':
"def _validate_callbacks(cls, callbacks): for callback in callbacks: <IF_STMT> if issubclass(callback, Callback): raise TypeError('Make sure to instantiate the callbacks.') raise TypeError('Only accepts a `callbacks` instance.')","if not isinstance(callback, Callback):"
"def detab(self, text): """"""Remove a tab from the front of each line of the given text."""""" newtext = [] lines = text.split('\n') for line in lines: <IF_STMT> newtext.append(line[markdown.TAB_LENGTH:]) elif not line.strip(): newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",if line.startswith(' ' * markdown.TAB_LENGTH):
"def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return <IF_STMT> if time_now - self.last_check_time < 3: return elif time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()",if fail or self.network_stat != 'OK':
"def wrapper(*args, **kwargs): if is_profiling_enabled(section): global _profile_nesting profile = get_global_profile() _profile_nesting += 1 <IF_STMT> profile.enable() result = func(*args, **kwargs) _profile_nesting -= 1 if _profile_nesting == 0: profile.disable() return result else: return func(*args, **kwargs)",if _profile_nesting == 1:
"def get_sequence_type_str(x: Sequence[Any]) -> str: container_type = type(x).__name__ if not x: <IF_STMT> return '[]' else: return container_type + '([])' elem_type = get_type_str(x[0]) if container_type == 'list': if len(x) == 1: return '[' + elem_type + ']' else: return '[' + elem_type + ', ...]' elif len(x) == 1: return f'{container_type}([{elem_type}])' else: return f'{container_type}([{elem_type}, ...])'",if container_type == 'list':
"def attempts(self): if not hasattr(self, '_attempts'): task_attempts = self.job.api.task_attempts(self.job.id, self.id)['taskAttempts'] <IF_STMT> self._attempts = [Attempt(self, attempt) for attempt in task_attempts['taskAttempt']] else: self._attempts = [] return self._attempts",if task_attempts:
"def __call__(self, message, keyname): if keyname in self.keyring: key = self.keyring[keyname] if isinstance(key, Key) and key.algorithm == GSS_TSIG: <IF_STMT> GSSTSigAdapter.parse_tkey_and_step(key, message, keyname) return key else: return None",if message:
"def location_dec(str): head = int(str[0]) str = str[1:] rows = head cols = int(len(str) / rows) + 1 out = '' full_row = len(str) % head for c in range(cols): for r in range(rows): <IF_STMT> continue if r < full_row: char = str[r * cols + c] else: char = str[cols * full_row + (r - full_row) * (cols - 1) + c] out += char return parse.unquote(out).replace('^', '0')",if c == cols - 1 and r >= full_row:
def request(self): if 'Cookie' in self._req._headers: c = self._req._headers['Cookie'].split('; ') <IF_STMT> return cookies.cookie({x[0]: x[2] for x in [x.partition('=') for x in c]}) return cookies.cookie({}),if c[0]:
"def bulk_enable_accounts(account_names): """"""Bulk enable accounts"""""" for account_name in account_names: account = Account.query.filter(Account.name == account_name).first() <IF_STMT> app.logger.debug('Enabling account %s', account.name) account.active = True db.session.add(account) db.session.commit() db.session.close()",if account:
"def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(""can't specify timeout for non-blocking acquire"") rc = False endtime = None self._cond.acquire() while self._value == 0: if not blocking: break if timeout is not None: <IF_STMT> endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc",if endtime is None:
"def _sorted_layers(self, structure, top_layer_id): """"""Return the image layers sorted"""""" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) if 'json' not in structure['repolayers'][next_layer]: break <IF_STMT> break next_layer = structure['repolayers'][next_layer]['json']['parent'] if not next_layer: break return sorted_layers",if 'parent' not in structure['repolayers'][next_layer]['json']:
"def on_change(self, data): for window in sublime.windows(): for view in window.views(): <IF_STMT> file_name = view.file_name() if view.settings().get('tp_ammo', False): self.update(view) elif file_name and file_name.endswith(global_settings('ammo_file_extension', '.ammo')): self.update(view)","if view.get_status('inactive') and view.settings().get('tp_append', False):"
"def _maintain_pool(self): waiting = self._docker_interface.services_waiting_by_constraints() active = self._docker_interface.nodes_active_by_constraints() for constraints, needed_dict in self._state.slots_needed(waiting, active).items(): services = needed_dict['services'] nodes = needed_dict['nodes'] slots_needed = needed_dict['slots_needed'] if slots_needed > 0: self._spawn_nodes(constraints, services, slots_needed) <IF_STMT> self._destroy_nodes(constraints, nodes, slots_needed)",elif slots_needed < 0:
"def _update_vhosts_addrs_ssl(self, vhosts): """"""Update a list of raw parsed vhosts to include global address sslishness"""""" addr_to_ssl = self._build_addr_to_ssl() for vhost in vhosts: for addr in vhost.addrs: addr.ssl = addr_to_ssl[addr.normalized_tuple()] <IF_STMT> vhost.ssl = True",if addr.ssl:
def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name <IF_STMT> filename = {} filename[file.name] = {'is_include_file': True} if file.file_type != common_type: if type(filename) == str: filename = {} filename[file.name] = {'file_type': file.file_type} files.append(filename) return files,if file.is_include_file == True:
"def _get_resource_group_name_of_staticsite(client, static_site_name): static_sites = client.list() for static_site in static_sites: <IF_STMT> resource_group = _parse_resource_group_from_arm_id(static_site.id) if resource_group: return resource_group raise CLIError(""Static site was '{}' not found in subscription."".format(static_site_name))",if static_site.name.lower() == static_site_name.lower():
"def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: <IF_STMT> return if fail or self.network_stat != 'OK': if time_now - self.last_check_time < 3: return elif time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()",if self._checking_num > 0:
"def _gen(): for i in dataset(): if isinstance(i, tuple) or isinstance(i, list): <IF_STMT> yield i elif fn(i) is True: yield i",if fn(*i) is True:
"def _merge_dict(d1, d2): for k, v2 in d2.items(): v1 = d1.get(k) if v1 is None: raise Exception('{} is not recognized by client_config'.format(k)) <IF_STMT> _merge_dict(v1, v2) else: d1[k] = v2 return d1","if isinstance(v1, Mapping) and isinstance(v2, Mapping):"
"def OnRelease(self, evt): if self.isDrag: parent = self.GetParent() DrawSash(parent, self.px, self.py, self.side) self.ReleaseMouse() self.isDrag = False <IF_STMT> parent.AddLeaf(MV_VER, self.py) else: parent.AddLeaf(MV_HOR, self.px) else: evt.Skip()",if self.side == MV_HOR:
"def check_zookeeper_metrics(): response = get_metrics_prom(dcos_api_session, dcos_api_session.masters[0]) for family in text_string_to_metric_families(response.text): for sample in family.samples: <IF_STMT> assert sample[1]['dcos_component_name'] == 'ZooKeeper' return raise Exception('Expected ZooKeeper zookeeper_avg_latency metric not found')",if sample[0] == 'zookeeper_avg_latency':
"def scan_patterns(self, kind): """"""Parse the config section into a list of patterns, preserving order."""""" d = self.scan_d(kind) aList = [] seen = set() for key in d: value = d.get(key) <IF_STMT> g.trace('duplicate key', key) else: seen.add(key) aList.append(self.msf.Pattern(key, value)) return aList",if key in seen:
def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] if ch == '{': return True elif ch == '(': openParen += 1 elif ch == ')': <IF_STMT> return False openParen -= 1 elif ch == ';' or ch == '}': return False i += 1 return False,if openParen == 0:
"def append(self, child): if child not in (None, self): tag = child_tag(self._tag) if tag: <IF_STMT> if child.tag != tag: child = Html(tag, child) elif not child.startswith('<%s' % tag): child = Html(tag, child) super().append(child)","if isinstance(child, Html):"
"def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == 'conv': <IF_STMT> x = self.padding_layer(x) x = self.conv(x) elif layer == 'norm' and norm and self.with_norm: x = self.norm(x) elif layer == 'act' and activate and self.with_activation: x = self.activate(x) return x",if self.with_explicit_padding:
"def get_tasks(self): for task in asyncio.all_tasks(loop=self.middleware.loop): formatted = None frame = None frames = [] for frame in task.get_stack(): cur_frame = get_frame_details(frame, self.logger) <IF_STMT> frames.append(cur_frame) if frame: formatted = traceback.format_stack(frame) yield {'stack': formatted, 'frames': frames}",if cur_frame:
"def _read_row_from_packet(self, packet): row = [] for encoding, converter in self.converters: try: data = packet.read_length_coded_string() except IndexError: break if data is not None: if encoding is not None: data = data.decode(encoding) <IF_STMT> print('DEBUG: DATA = ', data) if converter is not None: data = converter(data) row.append(data) return tuple(row)",if DEBUG:
"def get_child(self, name): if self.isdir: try: return self.data[name] except: if not self.case_sensitive: for childname, child in list(self.data.items()): <IF_STMT> return child raise",if childname.lower() == name.lower():
"def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: if strip: line = line.strip() skip = False <IF_STMT> skip = line.isspace() or not line if not skip: yield line",if skip_blanks:
"def atleast_3d(*arys): if len(arys) == 1: arr = array(arys[0]) if ndim(arr) == 0: arr = expand_dims(arr, axis=(0, 1, 2)) <IF_STMT> arr = expand_dims(arr, axis=(0, 2)) elif ndim(arr) == 2: arr = expand_dims(arr, axis=2) return arr else: return [atleast_3d(arr) for arr in arys]",elif ndim(arr) == 1:
"def scan_resource_conf(self, conf): os_profile = conf.get('os_profile') if os_profile: os_profile = os_profile[0] custom_data = os_profile.get('custom_data') if custom_data: custom_data = custom_data[0] if isinstance(custom_data, str): <IF_STMT> return CheckResult.FAILED return CheckResult.PASSED",if string_has_secrets(custom_data):
"def __call__(self, trainer): observation = trainer.observation if self.key in observation: loss = observation[self.key] <IF_STMT> self.min_loss = loss self.best_model = trainer.updater.epoch src = '%s.%d' % (self.prefix, self.best_model) dest = os.path.join(trainer.out, '%s.%s' % (self.prefix, self.suffix)) if os.path.lexists(dest): os.remove(dest) os.symlink(src, dest) logging.info('best model is ' + src)",if self.best_model == -1 or loss < self.min_loss:
"def dump_prefs(self): ret = '' for pref in self.prefs: <IF_STMT> value = str(self.prefs[pref].value) elif type(self.prefs[pref].value) == bool: value = 'true' if self.prefs[pref].value == True else 'false' else: value = '""%s""' % self.prefs[pref].value ret += pref + ': ' + value + ' (' + self.prefs[pref].anon_source + ')\n' return ret",if type(self.prefs[pref].value) == int:
"def translate_isinstance(builder: IRBuilder, expr: CallExpr, callee: RefExpr) -> Optional[Value]: if len(expr.args) == 2 and expr.arg_kinds == [ARG_POS, ARG_POS] and isinstance(expr.args[1], (RefExpr, TupleExpr)): irs = builder.flatten_classes(expr.args[1]) <IF_STMT> return builder.builder.isinstance_helper(builder.accept(expr.args[0]), irs, expr.line) return None",if irs is not None:
"def autoname(self): naming_method = frappe.db.get_value('HR Settings', None, 'emp_created_by') if not naming_method: throw(_('Please setup Employee Naming System in Human Resource > HR Settings')) el<IF_STMT> set_name_by_naming_series(self) elif naming_method == 'Employee Number': self.name = self.employee_number elif naming_method == 'Full Name': self.set_employee_name() self.name = self.employee_name self.employee = self.name",if naming_method == 'Naming Series':
"def search_expr(sheet, expr, reverse=False): for i in rotateRange(len(sheet.rows), sheet.cursorRowIndex, reverse=reverse): try: <IF_STMT> sheet.cursorRowIndex = i return except Exception as e: vd.exceptionCaught(e) vd.fail(f'no {sheet.rowtype} where {expr}')","if sheet.evalExpr(expr, sheet.rows[i]):"
"def _targets(self, urls, querystring): for input, output in urls: response = self.client.get(u'/1/%s' % input, follow=True) if output == 404: eq_(404, response.status_code) <IF_STMT> chain = [u[0] for u in response.redirect_chain] assert output in chain else: r = response.redirect_chain r.reverse() final = urlparse(r[0][0]) eq_(output, final.path) eq_(querystring, final.query)",elif output.startswith('http'):
"def get_local_cache(self, past, data, from_file, temp_id): """"""parse individual cached geometry if there is any"""""" cache = [] if self.accumulative: if from_file and len(past) > 0: cache = past[temp_id] <IF_STMT> cache = data.get(temp_id, []) return cache",if not from_file and len(data) > 0:
"def _parse_abbrev_table(self): """"""Parse the abbrev table from the stream"""""" map = {} self.stream.seek(self.offset) while True: decl_code = struct_parse(struct=self.structs.Dwarf_uleb128(''), stream=self.stream) <IF_STMT> break declaration = struct_parse(struct=self.structs.Dwarf_abbrev_declaration, stream=self.stream) map[decl_code] = AbbrevDecl(decl_code, declaration) return map",if decl_code == 0:
def mFRIDAY(self): try: _type = FRIDAY _channel = DEFAULT_CHANNEL pass self.match('fri') alt10 = 2 LA10_0 = self.input.LA(1) if LA10_0 == 100: alt10 = 1 <IF_STMT> pass self.match('day') self._state.type = _type self._state.channel = _channel finally: pass,if alt10 == 1:
"def __getattr__(self, key): from mongokit.schema_document import i18n if key in self: if isinstance(self[key], i18n): <IF_STMT> return self[key].get(self._doc._fallback_lang) return self[key][self._doc._current_lang] return self[key]",if self._doc._current_lang not in self[key]:
"def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) if not value: continue <IF_STMT> value = HIDE_LIST elif key == FEATS: value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append('%s=%s' % (key, value)) return '%s(%s)' % (record.__class__.__name__, ', '.join(parts))","if isinstance(value, list):"
"def pre_validate(self, form): if self.data: values = list((c[0] for c in self.choices)) for d in self.data: <IF_STMT> raise ValueError(self.gettext(u""'%(value)s' is not a valid choice for this field"") % dict(value=d))",if d not in values:
"def _sql_like_to_regex(pattern, escape): cur_i = 0 pattern_length = len(pattern) while cur_i < pattern_length: nxt_i = cur_i + 1 cur = pattern[cur_i] nxt = pattern[nxt_i] if nxt_i < pattern_length else None skip = 1 if nxt is not None and escape is not None and (cur == escape): yield nxt skip = 2 <IF_STMT> yield '.*' elif cur == '_': yield '.' else: yield cur cur_i += skip",elif cur == '%':
"def find_caller(stack): """"""Finds info about first non-sqlalchemy call in stack"""""" for frame in stack: module = inspect.getmodule(frame[0]) <IF_STMT> continue if module.__name__.startswith('sqlalchemy'): continue return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),) log.warning('Transaction from unknown origin') return (None, None, None, None)","if not hasattr(module, '__name__'):"
"def _get_normal_median_depth(normal_counts): depths = [] with open(normal_counts) as in_handle: header = None for line in in_handle: if header is None and (not line.startswith('@')): header = line.strip().split() <IF_STMT> n_vals = dict(zip(header, line.strip().split())) depths.append(int(n_vals['REF_COUNT']) + int(n_vals['ALT_COUNT'])) return np.median(depths)",elif header:
"def get_pool(self, *args, **kw): key = self._serialize(*args, **kw) try: return self.pools[key] except KeyError: self._create_pool_mutex.acquire() try: <IF_STMT> kw.pop('sa_pool_key', None) pool = self.poolclass(lambda: self.module.connect(*args, **kw), **self.kw) self.pools[key] = pool return pool else: return self.pools[key] finally: self._create_pool_mutex.release()",if key not in self.pools:
"def add(self, field, value, boost=None): match = {'value': value} if boost: <IF_STMT> match['boost'] = boost else: match['boost'] = float(boost) self._values[field] = match return self._values[field] = value","if isinstance(boost, (float, int)):"
"def get_shape(shape): """"""Convert the shape to correct dtype and vars."""""" ret = [] for dim in shape: if isinstance(dim, tvm.tir.IntImm): <IF_STMT> ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm('int32', val)) elif isinstance(dim, tvm.tir.Any): ret.append(te.var('any_dim', 'int32')) else: ret.append(dim) return ret",if libinfo()['INDEX_DEFAULT_I64'] == 'ON':
"def _find_icacls_exe(): if os.name == 'nt': paths = [os.path.expandvars('%windir%\\{0}').format(subdir) for subdir in ('system32', 'SysWOW64')] for path in paths: icacls_path = next(iter((fn for fn in os.listdir(path) if fn.lower() == 'icacls.exe')), None) <IF_STMT> icacls_path = os.path.join(path, icacls_path) return icacls_path return None",if icacls_path is not None:
def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split('.') test_ver = test_version.split('.') if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: if runtime_ver[1] > test_ver[1]: return True elif runtime_ver[1] == test_ver[1]: <IF_STMT> return True return False,if runtime_ver[2] > test_ver[2]:
"def get_ready_conn(self, host): conn = None self._lock.acquire() try: if host in self._hostmap: for c in self._hostmap[host]: <IF_STMT> self._readymap[c] = 0 conn = c break finally: self._lock.release() return conn",if self._readymap[c]:
"def to_svc_hst_distinct_lists(ref, tab): r = {'hosts': [], 'services': []} for e in tab: cls = e.__class__ <IF_STMT> name = e.get_dbg_name() r['services'].append(name) else: name = e.get_dbg_name() r['hosts'].append(name) return r",if cls.my_type == 'service':
"def playerData(s): """"""Returns a list of tuples of original string and dict of values"""""" p = [] i = 0 while True: match = re_input.match(s, pos=i) if match is None: return p else: d = match.groupdict() <IF_STMT> d['degree'], d['kwargs'] = getArgs(d['args']) else: d['degree'], d['kwargs'] = ('', {}) del d['args'] p.append((match.group().strip(), d)) i = match.end() return",if d['args'] is not None:
"def _params_for_TXT(self, record): for value in record.values: field_type = 'TXT' <IF_STMT> field_type = 'DKIM' value = value.replace('\\;', ';') yield {'target': value, 'subDomain': record.name, 'ttl': record.ttl, 'fieldType': field_type}",if self._is_valid_dkim(value):
"def create(self, values): conn = self.get_connection() object_classes = self.structural_classes + [self.object_class] attrs = [('objectClass', object_classes)] for k, v in values.iteritems(): if k == 'id' or k in self.attribute_ignore: continue <IF_STMT> attr_type = self.attribute_mapping.get(k, k) attrs.append((attr_type, [v])) if 'groupOfNames' in object_classes and self.use_dumb_member: attrs.append(('member', [self.DUMB_MEMBER_DN])) conn.add_s(self._id_to_dn(values['id']), attrs) return values",if v is not None:
"def get_new_unlinked_nodes(before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict): affected_nodes = [] for node_id, socket in zip(before_inputted_nodes, before_input_sockets): if not socket in input_sockets: if node_id in nodes_dict: <IF_STMT> affected_nodes.append(node_id) return affected_nodes",if not node_id in affected_nodes:
"def show_panel(panel_id): for position in _positions_names: pos_panel_ids = _get_position_panels(position) if len(pos_panel_ids) == 0: continue if len(pos_panel_ids) == 1: continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) <IF_STMT> notebook.set_current_page(i)",if notebook_page == panel_widget:
"def merge(self, abort=False, message=None): """"""Merge remote branch or reverts the merge."""""" if abort: self.execute(['update', '--clean', '.']) elif self.needs_merge(): <IF_STMT> self.execute(['update', '--clean', 'remote(.)']) else: self.configure_merge() try: self.execute(['merge', '-r', 'remote(.)']) except RepositoryException as error: if error.retcode == 255: return raise self.execute(['commit', '--message', 'Merge'])",if self.needs_ff():
"def runButtons(action): global sqlUpdate if action == 'Clear': app.text(LABS['run'], replace=True) app.message(LABS['run'], '', bg='grey') log('SQL cleared') elif action == 'Run': app.message(LABS['run'], '') sql = app.text(LABS['run']).strip() <IF_STMT> runSql(sql) else: app.message(LABS['run'], '', bg='grey') app.text(LABS['run'], focus=True)",if len(sql) > 0:
"def receive_loop(self): while not self._stoped: try: rd, _, _ = select.select([self.teredo_sock], [], [], 0.5) <IF_STMT> self.receive_ra_packet() except Exception as e: logger.exception('receive procedure fail once: %r', e) pass",if rd and (not self._stoped):
"def add_items(self, model, objs): search_fields = model.get_search_fields() if not search_fields: return indexers = [ObjectIndexer(obj, self.backend) for obj in objs] if indexers: content_type_pk = get_content_type_pk(model) update_method = self.add_items_upsert <IF_STMT> else self.add_items_update_then_create update_method(content_type_pk, indexers)",if self._enable_upsert
"def __init__(self, service: RestClient, **k_args: Dict[str, str]): self.path: str = None self.httpMethod: str = None self.service: RestClient = service self.__dict__.update(k_args) self.path_args: List[str] = [] self.query_args: List[str] = [] if hasattr(self, 'parameters'): for key, value in self.parameters.items(): <IF_STMT> self.path_args.append(key) else: self.query_args.append(key)",if value['location'] == 'path':
"def insertion_unsort(str, extended): """"""3.2 Insertion unsort coding"""""" oldchar = 128 result = [] oldindex = -1 for c in extended: index = pos = -1 char = ord(c) curlen = selective_len(str, char) delta = (curlen + 1) * (char - oldchar) while 1: index, pos = selective_find(str, c, index, pos) <IF_STMT> break delta += index - oldindex result.append(delta - 1) oldindex = index delta = 0 oldchar = char return result",if index == -1:
"def get_sorted_entry(field, bookid): if field == 'title' or field == 'authors': book = calibre_db.get_filtered_book(bookid) if book: <IF_STMT> return json.dumps({'sort': book.sort}) elif field == 'authors': return json.dumps({'author_sort': book.author_sort}) return ''",if field == 'title':
"def _convert_tstamp(out): if 'timestamp' in out: f = float(out['timestamp']) out['timestamp'] = datetime.fromtimestamp(f / 1000) else: for ticker, data in out.items(): <IF_STMT> f = float(data['timestamp']) data['timestamp'] = datetime.fromtimestamp(f / 1000) out[ticker] = data return out",if 'timestamp' in data:
"def write_urls(self, person): """"""Write URL and EMAIL properties of a VCard."""""" url_list = person.get_url_list() for url in url_list: href = url.get_path() <IF_STMT> if url.get_type() == UrlType(UrlType.EMAIL): if href.startswith('mailto:'): href = href[len('mailto:'):] self.writeln('EMAIL:%s' % self.esc(href)) else: self.writeln('URL:%s' % self.esc(href))",if href:
"def get_range(min, max): if max < min: min, max = (max, min) elif min == max: <IF_STMT> min, max = (2 * min, 0) elif min > 0: min, max = (0, 2 * min) else: min, max = (-1, 1) return (min, max)",if min < 0:
"def __init__(self, mapping=None): if isinstance(mapping, MultiDict): dict.__init__(self, ((k, l[:]) for k, l in mapping.iterlists())) elif isinstance(mapping, dict): tmp = {} for key, value in mapping.iteritems(): <IF_STMT> value = list(value) else: value = [value] tmp[key] = value dict.__init__(self, tmp) else: tmp = {} for key, value in mapping or (): tmp.setdefault(key, []).append(value) dict.__init__(self, tmp)","if isinstance(value, (tuple, list)):"
"def modified_precision(candidate, references, n): candidate_ngrams = list(ngrams(candidate, n)) if len(candidate_ngrams) == 0: return 0 c_words = set(candidate_ngrams) for word in c_words: count_w = candidate_ngrams.count(word) + 1 count_max = 0 for reference in references: reference_ngrams = list(ngrams(reference, n)) count = reference_ngrams.count(word) + 1 <IF_STMT> count_max = count return min(count_w, count_max) / (len(candidate) + len(c_words))",if count > count_max:
"def reverse_adjust_line_according_to_hunks(self, hunks, line): for hunk in reversed(hunks): head_start = hunk.head_start saved_start = hunk.saved_start if hunk.saved_length == 0: saved_start += 1 elif hunk.head_length == 0: saved_start -= 1 head_end = head_start + hunk.head_length saved_end = saved_start + hunk.saved_length if saved_end <= line: return head_end + line - saved_end <IF_STMT> return head_start return line",elif saved_start <= line:
"def indent_xml(elem, level=0): """"""Do our pretty printing and make Matt very happy."""""" i = '\n' + level * '  ' if elem: if not elem.text or not elem.text.strip(): elem.text = i + '  ' <IF_STMT> elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i elif level and (not elem.tail or not elem.tail.strip()): elem.tail = i",if not elem.tail or not elem.tail.strip():
"def test_infer_shape_matrix(self): x = theano.tensor.matrix() for op in self.ops: <IF_STMT> continue if op.return_index: f = op(x)[2] else: f = op(x)[1] self._compile_and_check([x], [f], [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)], self.op_class)",if not op.return_inverse:
"def drop_lists(value): out = {} for key, val in value.items(): val = val[0] if isinstance(key, bytes): key = str(key, 'utf-8') <IF_STMT> val = str(val, 'utf-8') out[key] = val return out","if isinstance(val, bytes):"
"def malloc(self, size): assert 0 <= size < sys.maxsize if os.getpid() != self._lastpid: self.__init__() with self._lock: self._free_pending_blocks() size = self._roundup(max(size, 1), self._alignment) arena, start, stop = self._malloc(size) new_stop = start + size <IF_STMT> self._free((arena, new_stop, stop)) block = (arena, start, new_stop) self._allocated_blocks.add(block) return block",if new_stop < stop:
"def ContinueStatement(self, label, **kwargs): if label is None: self.emit('JUMP', self.implicit_continues[-1]) else: label = label.get('name') <IF_STMT> raise MakeError('SyntaxError', ""Undefined label '%s'"" % label) else: self.emit('JUMP', self.declared_continue_labels[label])",if label not in self.declared_continue_labels:
"def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: token, = tokens <IF_STMT> if token.lower_value in ('decimal', 'disc'): if token.lower_value not in counter_style: return token.value elif token.lower_value != 'none': return token.value",if token.type == 'ident':
"def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: if key == KDTREE_KEY: continue item = data[key] <IF_STMT> item = item[self._indices].clone() elif torch.is_tensor(item): item = item.clone() setattr(new_data, key, item) return new_data",if torch.is_tensor(item) and num_points == item.shape[0]:
"def HandleEvent(self, event): e_id = event.GetId() if e_id in self.handlers: handler = self.handlers[e_id] try: <IF_STMT> return handler(event) except RuntimeError: self.RemoveHandlerForID(e_id) else: event.Skip() return False",if handler:
"def try_append_extension(self, path): append_setting = self.get_append_extension_setting() if self.settings.get(append_setting, False): if not self.is_copy_original_name(path): _, new_path_extension = os.path.splitext(path) <IF_STMT> argument_name = self.get_argument_name() if argument_name is None: _, extension = os.path.splitext(self.view.file_name()) else: _, extension = os.path.splitext(argument_name) path += extension return path",if new_path_extension == '':
"def _get_namespace(self, gl_client, gl_namespace, lazy=False): try: <IF_STMT> return gl_client.groups.get(gl_namespace.attributes['id'], lazy=lazy) if gl_namespace.attributes['kind'] == 'user': return gl_client.users.get(gl_client.user.attributes['id'], lazy=lazy) return gl_client.users.get(gl_namespace.attributes['id'], lazy=lazy) except gitlab.GitlabGetError: return None",if gl_namespace.attributes['kind'] == 'group':
"def removeReadOnly(self, files): for filepath in files: <IF_STMT> os.chmod(filepath, stat.S_IWRITE | os.stat(filepath).st_mode)",if os.path.isfile(filepath):
"def initiate_all_local_variables_instances(nodes, local_variables_instances, all_local_variables_instances): for node in nodes: <IF_STMT> new_var = LocalIRVariable(node.variable_declaration) if new_var.name in all_local_variables_instances: new_var.index = all_local_variables_instances[new_var.name].index + 1 local_variables_instances[node.variable_declaration.name] = new_var all_local_variables_instances[node.variable_declaration.name] = new_var",if node.variable_declaration:
"def find_comment(line): """"""Finds the index of a comment # and returns None if not found"""""" instring, instring_char = (False, '') for i, char in enumerate(line): <IF_STMT> if instring: if char == instring_char: instring = False instring_char = '' else: instring = True instring_char = char elif char == '#': if not instring: return i return None","if char in ('""', ""'""):"
"def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None: with _create_scoped_session(self.scoped_session, True) as session: study = models.StudyModel.find_or_raise_by_id(study_id, session) attribute = models.StudySystemAttributeModel.find_by_study_and_key(study, key, session) <IF_STMT> attribute = models.StudySystemAttributeModel(study_id=study_id, key=key, value_json=json.dumps(value)) session.add(attribute) else: attribute.value_json = json.dumps(value)",if attribute is None:
"def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) if sChild.declaration and sChild.docname == docname: sChild.declaration = None sChild.docname = None sChild.line = None <IF_STMT> sChild.siblingAbove.siblingBelow = sChild.siblingBelow if sChild.siblingBelow is not None: sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None",if sChild.siblingAbove is not None:
"def test_sum_values_list_group_by(self): ret = await Book.annotate(sum=Sum('rating')).group_by('author_id').values_list('author_id', 'sum') for item in ret: author_id = item[0] sum_ = item[1] <IF_STMT> self.assertEqual(sum_, 45.0) elif author_id == self.a2.pk: self.assertEqual(sum_, 10.0)",if author_id == self.a1.pk:
"def save_claims_for_resolve(self, claim_infos): to_save = {} for info in claim_infos: if 'value' in info: <IF_STMT> to_save[info['claim_id']] = info else: for key in ('certificate', 'claim'): if info.get(key, {}).get('value'): to_save[info[key]['claim_id']] = info[key] return self.save_claims(to_save.values())",if info['value']:
"def utcoffset(self, dt): if not dst_only: dt_n = dt.replace(tzinfo=None) <IF_STMT> return timedelta(hours=-1) return timedelta(hours=0)","if dt_start <= dt_n < dt_end and getattr(dt_n, 'fold', 0):"
"def find_comment(line): """"""Finds the index of a comment # and returns None if not found"""""" instring, instring_char = (False, '') for i, char in enumerate(line): if char in ('""', ""'""): <IF_STMT> if char == instring_char: instring = False instring_char = '' else: instring = True instring_char = char elif char == '#': if not instring: return i return None",if instring:
"def __subclasshook__(cls, C): if cls is Coroutine: mro = get_mro(C) for method in ('__await__', 'send', 'throw', 'close'): for base in mro: <IF_STMT> break else: return NotImplemented return True return NotImplemented",if method in base.__dict__:
"def GetFile(cls, session, sig, mode='r'): sig = sig[:cls.HASH_LEN] while len(sig) > 0: fn = cls.SaveFile(session, sig) try: if os.path.exists(fn): return (open(fn, mode), sig) except (IOError, OSError): pass <IF_STMT> sig = sig[:-1] elif 'r' in mode: return (None, sig) else: return (open(fn, mode), sig) return (None, None)",if len(sig) > 1:
"def _store_pickle_output(self, pickle_output): if pickle_output: if self.output_options.output is None: self.error(""Can't use without --output"", 'pickle-output') <IF_STMT> self.error('Must specify %s file for --output' % load_pytd.PICKLE_EXT, 'pickle-output') self.output_options.pickle_output = pickle_output",elif not load_pytd.is_pickle(self.output_options.output):
"def the_func(*args, **kwargs): try: controller = args[0] version = controller.version return func(*args, **kwargs) except Exception as e: <IF_STMT> quantum_error_class = quantum_error_dict[version] raise quantum_error_class(e) raise",if errors is not None and type(e) in errors:
"def publish_create(cls, payload): try: <IF_STMT> thread = eventlet.spawn(workflows.get_engine().process, payload) cls.threads.append(thread) except Exception: traceback.print_exc() print(payload)","if isinstance(payload, wf_ex_db.WorkflowExecutionDB):"
"def get_suggestion(self, buffer: 'Buffer', document: Document) -> Optional[Suggestion]: history = buffer.history text = document.text.rsplit('\n', 1)[-1] if text.strip(): for string in reversed(list(history.get_strings())): for line in reversed(string.splitlines()): <IF_STMT> return Suggestion(line[len(text):]) return None",if line.startswith(text):
"def _get_parameter_scope(param, cmd_list): if not cmd_list: return 'N/A (NOT FOUND)' test_list = cmd_list[0].split(' ') while len(test_list) > 0: test_entry = ' '.join(test_list) all_match = True for entry in cmd_list[1:]: <IF_STMT> all_match = False break if not all_match: test_list.pop() else: return test_entry return '_ROOT_'",if test_entry not in entry:
"def __call__(self, params): for param in params: <IF_STMT> optim = self.optim_objs[param] else: optim = torch.optim.Adam([param], **self.optim_args) self.optim_objs[param] = optim optim.step()",if param in self.optim_objs:
"def filter_database(db, user, filter_name): """"""Returns a list of person handles"""""" filt = MatchesFilter([filter_name]) filt.requestprepare(db, user) <IF_STMT> user.begin_progress(_('Finding relationship paths'), _('Retrieving all sub-filter matches'), db.get_number_of_people()) matches = [] for handle in db.iter_person_handles(): person = db.get_person_from_handle(handle) if filt.apply(db, person): matches.append(handle) if user: user.step_progress() if user: user.end_progress() filt.requestreset() return matches",if user:
"def get_independence_days(self, year): """"""returns a possibly empty list of (date, holiday_name) tuples"""""" days = [] if year > 2004: actual_date = date(year, 5, 4) days = [(actual_date, 'Restoration of Independence Day')] <IF_STMT> days += [(self.find_following_working_day(actual_date), 'Restoration of Independence Observed')] return days",if actual_date.weekday() in self.get_weekend_days():
"def on_mode_paused(result, mode, *args): from deluge.ui.console.widgets.popup import PopupsHandler if isinstance(mode, PopupsHandler): <IF_STMT> log.error('Mode ""%s"" still has popups available after being paused. Ensure all popups are removed on pause!', mode.popup.title)",if mode.popup is not None:
def step(self): if not self.fully_grown: <IF_STMT> self.fully_grown = True self.countdown = self.model.grass_regrowth_time else: self.countdown -= 1,if self.countdown <= 0:
"def getOnlineBuilders(self): all_workers = (yield self.master.data.get(('workers',))) online_builderids = set() for worker in all_workers: connected = worker['connected_to'] <IF_STMT> continue builders = worker['configured_on'] builderids = [builder['builderid'] for builder in builders] online_builderids.update(builderids) defer.returnValue(list(online_builderids))",if not connected:
"def _latest_major(alternatives): max_major = -1 for a in alternatives: <IF_STMT> major, _, _, _ = components(a, strict=False) max_major = max(major, max_major) return max_major","if is_version_identifier(a, strict=False):"
"def getVar(self, name): value = self.tinfoil.run_command('dataStoreConnectorFindVar', self.dsindex, name) overrides = None if isinstance(value, dict): <IF_STMT> value['_content'] = self.tinfoil._reconvert_type(value['_content'], value['_connector_origtype']) del value['_connector_origtype'] if '_connector_overrides' in value: overrides = value['_connector_overrides'] del value['_connector_overrides'] return (value, overrides)",if '_connector_origtype' in value:
"def initAbbrev(self): k = self c = k.c d = c.config.getAbbrevDict() if d: for key in d: commandName = d.get(key) <IF_STMT> pass else: self.initOneAbbrev(commandName, key)",if commandName.startswith('press-') and commandName.endswith('-button'):
def restore_text(self): if self.source_is_console(): cb = self._last_console_cb else: cb = self._last_editor_cb if cb is None: if self.is_plain_text_mode(): self.plain_text.clear() else: self.rich_text.clear() else: func = cb[0] args = cb[1:] func(*args) <IF_STMT> self.switch_to_rich_text() else: self.switch_to_plain_text(),if get_meth_class_inst(func) is self.rich_text:
def get_test_layer(): layers = get_bb_var('BBLAYERS').split() testlayer = None for l in layers: <IF_STMT> l = os.path.expanduser(l) if '/meta-selftest' in l and os.path.isdir(l): testlayer = l break return testlayer,if '~' in l:
"def __parse_query(self, model, iter_, data): f, b = (self.__filter, self.__bg_filter) if f is None and b is None: return True else: album = model.get_album(iter_) if album is None: return True <IF_STMT> return f(album) elif f is None: return b(album) else: return b(album) and f(album)",elif b is None:
"def iter(iterable, sentinel=None): if sentinel is None: i = getattr(iterable, '__iter__', None) if i is not None: return i() i = getattr(iterable, '__getitem__', None) if i is not None: return _iter_getitem(iterable) <IF_STMT> return list(iterable).__iter__() raise TypeError('object is not iterable') if callable(iterable): return _iter_callable(iterable, sentinel) raise TypeError('iter(v, w): v must be callable')",if JS('@{{iterable}} instanceof Array'):
def run(self): next(self.coro) try: while True: with self.abort_lock: <IF_STMT> return msg = self.in_queue.get() if msg is POISON: break with self.abort_lock: if self.abort_flag: return self.coro.send(msg) except: self.abort_all(sys.exc_info()) return,if self.abort_flag:
"def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]): names = [] for type_ in types: <IF_STMT> return type_.name elif hasattr(type_, '_type_definition'): name = capitalize_first(type_._type_definition.name) else: name = capitalize_first(type_.__name__) names.append(name) return ''.join(names)","if isinstance(type_, StrawberryUnion):"
"def _get_user_from_email(group, email): from sentry.models import User for user in User.objects.filter(email__iexact=email): context = access.from_user(user=user, organization=group.organization) <IF_STMT> logger.warning('User %r does not have access to group %r', user, group) continue return user",if not context.has_team(group.project.team):
"def _make_binary_stream(s, encoding): try: if _py3k: <IF_STMT> s = s.encode(encoding) elif type(s) is not str: s = s.encode(encoding) from io import BytesIO rv = BytesIO(s) except ImportError: rv = StringIO(s) return rv","if isinstance(s, str):"
"def error_messages(file_list, files_removed): if files_removed is None: return for remove_this, reason in files_removed: if file_list is not None: file_list.remove(remove_this) if reason == 0: print(' REMOVED : (' + str(remove_this) + ')   is not PNG file format') <IF_STMT> print(' REMOVED : (' + str(remove_this) + ')   already exists') elif reason == 2: print(' REMOVED : (' + str(remove_this) + ')   file unreadable')",elif reason == 1:
"def _eyeAvailable(*args, **kwargs): try: r = pylink.getEyeLink().eyeAvailable() <IF_STMT> return EyeTrackerConstants.getName(EyeTrackerConstants.LEFT_EYE) elif r == 1: return EyeTrackerConstants.getName(EyeTrackerConstants.RIGHT_EYE) elif r == 2: return EyeTrackerConstants.getName(EyeTrackerConstants.BINOCULAR) else: return EyeTrackerConstants.UNDEFINED except Exception as e: printExceptionDetailsToStdErr()",if r == 0:
"def ignore_callback_errors(self, ignore): EventEmitter.ignore_callback_errors.fset(self, ignore) for emitter in self._emitters.values(): <IF_STMT> emitter.ignore_callback_errors = ignore elif isinstance(emitter, EmitterGroup): emitter.ignore_callback_errors_all(ignore)","if isinstance(emitter, EventEmitter):"
"def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: <IF_STMT> continue if type(node) is CodeNode and BaseNode.test_empty_node(node.node): continue if BaseNode.test_empty_node(node): continue return False return True",if node is None:
"def _confirm_deps(self, trans): if [pkgs for pkgs in trans.dependencies if pkgs]: dia = AptConfirmDialog(trans, parent=self.parent) res = dia.run() dia.hide() <IF_STMT> log.debug('Response is: %s' % res) if self.finish_handler: log.debug('Finish_handler...') self.finish_handler(trans, 0, self.data) return self._run_transaction(trans)",if res != Gtk.ResponseType.OK:
"def ascii85decode(data): n = b = 0 out = '' for c in data: if '!' <= c and c <= 'u': n += 1 b = b * 85 + (ord(c) - 33) <IF_STMT> out += struct.pack('>L', b) n = b = 0 elif c == 'z': assert n == 0 out += '\x00\x00\x00\x00' elif c == '~': if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack('>L', b)[:n - 1] break return out",if n == 5:
"def calculateModifiedAttributes(self, fit, runTime, forceProjected=False): if self.item: for effect in self.item.effects.values(): <IF_STMT> effect.handler(fit, self, ('module',), None, effect=effect)",if effect.runTime == runTime and effect.activeByDefault:
"def loadHandler(self, human, values, strict): if values[0] == 'pose': poseFile = values[1] poseFile = getpath.thoroughFindFile(poseFile, self.paths) if not os.path.isfile(poseFile): <IF_STMT> raise RuntimeError('Could not load pose %s, file does not exist.' % poseFile) log.warning('Could not load pose %s, file does not exist.', poseFile) else: self.loadPose(poseFile) return",if strict:
"def get_outdated_docs(self) -> Iterator[str]: for docname in self.env.found_docs: <IF_STMT> yield docname continue targetname = path.join(self.outdir, docname + self.out_suffix) try: targetmtime = path.getmtime(targetname) except Exception: targetmtime = 0 try: srcmtime = path.getmtime(self.env.doc2path(docname)) if srcmtime > targetmtime: yield docname except OSError: pass",if docname not in self.env.all_docs:
"def __init__(self, items=()): _dictEntries = [] for name, value in items: <IF_STMT> for item in name: _dictEntries.append((item, value)) else: _dictEntries.append((name, value)) dict.__init__(self, _dictEntries) assert len(self) == len(_dictEntries) self.default = None","if isinstance(name, (list, tuple, frozenset, set)):"
"def ping_task(): try: if self._protocol.peer_manager.peer_is_good(peer): <IF_STMT> self._protocol.add_peer(peer) return await self._protocol.get_rpc_peer(peer).ping() except (asyncio.TimeoutError, RemoteException): pass",if peer not in self._protocol.routing_table.get_peers():
def get_resolved_dependencies(self): dependencies = [] for dependency in self.envconfig.deps: if dependency.indexserver is None: package = resolve_package(package_spec=dependency.name) <IF_STMT> dependency = dependency.__class__(package) dependencies.append(dependency) return dependencies,if package != dependency.name:
"def main(msg: func.QueueMessage, dashboard: func.Out[str]) -> None: body = msg.get_body() logging.info('heartbeat: %s', body) raw = json.loads(body) try: entry = TaskHeartbeatEntry.parse_obj(raw) task = Task.get_by_task_id(entry.task_id) <IF_STMT> logging.error(task) return if task: task.heartbeat = datetime.utcnow() task.save() except ValidationError: logging.error('invalid task heartbeat: %s', raw) events = get_events() if events: dashboard.set(events)","if isinstance(task, Error):"
"def testTlsServerServeForeverTwice(self): """"""Call on serve_forever() twice should result in a runtime error"""""" with patch.object(ssl.SSLContext, 'load_cert_chain') as mock_method: server = (yield from StartTlsServer(context=self.context, address=('127.0.0.1', 0), loop=self.loop)) <IF_STMT> server_task = asyncio.create_task(server.serve_forever()) else: server_task = asyncio.ensure_future(server.serve_forever()) yield from server.serving with self.assertRaises(RuntimeError): yield from server.serve_forever() server.server_close()","if PYTHON_VERSION >= (3, 7):"
"def getInstances_WithSource(self, instancesAmount, sourceObject, scenes): if sourceObject is None: self.removeAllObjects() return [] else: sourceHash = hash(sourceObject) if self.identifier in lastSourceHashes: <IF_STMT> self.removeAllObjects() lastSourceHashes[self.identifier] = sourceHash return self.getInstances_Base(instancesAmount, sourceObject, scenes)",if lastSourceHashes[self.identifier] != sourceHash:
"def get_row(self, binary=False, columns=None, raw=None, prep_stmt=None): """"""Get the next rows returned by the MySQL server"""""" try: rows, eof = self.get_rows(count=1, binary=binary, columns=columns, raw=raw, prep_stmt=prep_stmt) <IF_STMT> return (rows[0], eof) return (None, eof) except IndexError: return (None, None)",if rows:
"def try_adjust_widgets(self): if hasattr(self.parent, 'adjust_widgets'): self.parent.adjust_widgets() if hasattr(self.parent, 'parentApp'): if hasattr(self.parent.parentApp, '_internal_adjust_widgets'): self.parent.parentApp._internal_adjust_widgets() <IF_STMT> self.parent.parentApp.adjust_widgets()","if hasattr(self.parent.parentApp, 'adjust_widgets'):"
def parseStatementList(): list__py__ = [] statement = None while index < length: <IF_STMT> break statement = parseSourceElement() if ('undefined' if not 'statement' in locals() else typeof(statement)) == 'undefined': break list__py__.append(statement) return list__py__,if match('}'):
"def forward(self, Z): losses = [] context = self.context_cnn(Z) targets = self.target_cnn(Z) _, _, h, w = Z.shape preds = self.pred_cnn(context) for steps_to_ignore in range(h - 1): for i in range(steps_to_ignore + 1, h): loss = self.compute_loss_h(targets, preds, i) <IF_STMT> losses.append(loss) loss = torch.stack(losses).sum() return loss",if not torch.isnan(loss):
"def __run(self, command): sys.stdout, self.stdout = (self.stdout, sys.stdout) sys.stderr, self.stderr = (self.stderr, sys.stderr) try: try: r = eval(command, self.namespace, self.namespace) <IF_STMT> print_(repr(r)) except SyntaxError: exec(command, self.namespace) except: if hasattr(sys, 'last_type') and sys.last_type == SystemExit: self.destroy() else: traceback.print_exc() sys.stdout, self.stdout = (self.stdout, sys.stdout) sys.stderr, self.stderr = (self.stderr, sys.stderr)",if r is not None:
"def prune(self): file = self.file if self.remain == 0: read_pos = file.tell() file.seek(0, 2) sz = file.tell() file.seek(read_pos) if sz == 0: return nf = self.newfile() while True: data = file.read(COPY_BYTES) <IF_STMT> break nf.write(data) self.file = nf",if not data:
"def reduce_inode(self, f, init): for x in range(0, len(self._array), 2): key_or_none = self._array[x] val_or_node = self._array[x + 1] <IF_STMT> init = val_or_node.reduce_inode(f, init) else: init = f.invoke([init, rt.map_entry(key_or_none, val_or_node)]) if rt.reduced_QMARK_(init): return init return init",if key_or_none is None and val_or_node is not None:
"def gen_topython_helper(cw): cw.enter_block('private static BaseException/*!*/ ToPythonHelper(System.Exception clrException)') allExceps = get_all_exceps([], exceptionHierarchy) allExceps.sort(cmp=compare_exceptions) for x in allExceps: <IF_STMT> cw.writeline('#if !SILVERLIGHT') cw.writeline('if (clrException is %s) return %s;' % (x.ExceptionMappingName, x.MakeNewException())) if not x.silverlightSupported: cw.writeline('#endif') cw.writeline('return new BaseException(Exception);') cw.exit_block()",if not x.silverlightSupported:
"def file_versions(self, path): """"""Returns all commits where given file was modified"""""" versions = [] commits_info = self.commit_info() seen_shas = set() for commit in commits_info: try: files = self.get_commit_files(commit['sha'], paths=[path]) file_path, file_data = files.items()[0] except IndexError: continue file_sha = file_data['sha'] <IF_STMT> continue else: seen_shas.add(file_sha) commit['file'] = file_data versions.append(file_data) return versions",if file_sha in seen_shas:
"def _append_fragment(self, ctx, frag_content): try: ctx['dest_stream'].write(frag_content) ctx['dest_stream'].flush() finally: <IF_STMT> self._write_ytdl_file(ctx) if not self.params.get('keep_fragments', False): os.remove(encodeFilename(ctx['fragment_filename_sanitized'])) del ctx['fragment_filename_sanitized']",if self.__do_ytdl_file(ctx):
"def gen_segs(glyph): bzs = glyph_to_bzs(glyph) for sp in bzs: bks = segment_sp(sp) for i in range(len(bks)): bk0, bk1 = (bks[i], bks[(i + 1) % len(bks)]) <IF_STMT> segstr = seg_to_string(sp, bk0, bk1) fn = seg_fn(segstr) file(fn, 'w').write(segstr)",if bk1 != (bk0 + 1) % len(sp) or len(sp[bk0]) != 2:
"def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) if not match and parent_path != '': <IF_STMT> match = pattern.match(os.path.sep.join(parent_path_dirs[:len(pattern.dirs)])) if match: matched = not negative return matched",if len(pattern.dirs) <= len(parent_path_dirs):
"def __repr__(self): text = '{}('.format(self.__class__.__name__) n = len(self) for i in range(n): <IF_STMT> if i > 0: text = text + ', ' text = text + '{}={}'.format(fields[i], str(self[i])) text = text + ')' return text",if self[i] != None:
"def difference_matrix(samples, debug=True): """"""Calculate the difference matrix for the given set of samples."""""" diff_matrix = {} for x in samples: if debug: print('Calculating difference matrix for %s' % x) <IF_STMT> diff_matrix[x] = {} for y in samples: if samples[x] != samples[y]: d = difference(samples[x], samples[y]) diff_matrix[x][y] = d else: diff_matrix[x][y] = 0 return diff_matrix",if x not in diff_matrix:
"def load_config(self): try: with open(CONFIG_PATH) as f: y = yaml.safe_load(f) for key, value in y.items(): <IF_STMT> setattr(self, key.upper(), value) except IOError: logger.warning(f'No config file found at {CONFIG_PATH}, using defaults.\nSet the CONFIG_PATH environment variable to point to a config file to override.')","if hasattr(self, key.upper()) and (not os.getenv(key.upper())):"
"def checkout_branch(self, branch): if branch in self.remote_branches: sickrage.app.log.debug('Branch checkout: ' + self._find_installed_version() + '->' + branch) if not self.install_requirements(self.current_branch): return False <IF_STMT> self.reset() self.fetch() __, __, exit_status = self._git_cmd(self._git_path, 'checkout -f ' + branch) if exit_status == 0: return True return False",if sickrage.app.config.git_reset:
"def upload(youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None): body_keys = ','.join(body.keys()) media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True) videos = youtube_resource.videos() request = videos.insert(part=body_keys, body=body, media_body=media) while 1: status, response = request.next_chunk() if response: <IF_STMT> return response['id'] else: raise KeyError(""Response has no 'id' field"") elif status and progress_callback: progress_callback(status.total_size, status.resumable_progress)",if 'id' in response:
def execute(self): with self._guard_sigpipe(): try: targets = self.get_targets() if self.act_transitively else self.context.target_roots for value in self.console_output(targets) or tuple(): self._outstream.write(value.encode()) self._outstream.write(self._console_separator.encode()) finally: self._outstream.flush() <IF_STMT> self._outstream.close(),if self.get_options().output_file:
"def declare_var(self, type_name: Union[str, Tuple[str, str]], *, var_name: str='', var_name_prefix: str='v', shared: bool=False) -> str: if shared: if not var_name: var_name = var_name_prefix <IF_STMT> self.declarations.append((var_name, type_name)) self.shared_vars.add(var_name) else: if not var_name: var_name = self.get_var_name(var_name_prefix) self.declarations.append((var_name, type_name)) return var_name",if var_name not in self.shared_vars:
"def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: token, = tokens if token.type == 'ident': if token.lower_value in ('decimal', 'disc'): <IF_STMT> return token.value elif token.lower_value != 'none': return token.value",if token.lower_value not in counter_style:
"def __init__(self, appName=''): dlgappcore.AppDialog.__init__(self, win32ui.IDD_GENERAL_STATUS) self.timerAppName = appName self.argOff = 0 if len(self.timerAppName) == 0: <IF_STMT> self.timerAppName = sys.argv[1] self.argOff = 1",if len(sys.argv) > 1 and sys.argv[1][0] != '/':
"def tearDownClass(cls): for conn in settings.HAYSTACK_CONNECTIONS.values(): <IF_STMT> continue if 'STORAGE' in conn and conn['STORAGE'] != 'file': continue if os.path.exists(conn['PATH']): shutil.rmtree(conn['PATH']) super(WhooshTestCase, cls).tearDownClass()",if conn['ENGINE'] != 'haystack.backends.whoosh_backend.WhooshEngine':
"def forward(self, x): if self.ffn_type in (1, 2): x0 = self.wx0(x) if self.ffn_type == 1: x1 = x <IF_STMT> x1 = self.wx1(x) out = self.output(x0 * x1) out = self.dropout(out) out = self.LayerNorm(out + x) return out",elif self.ffn_type == 2:
"def __call__(self, data, **params): p = param.ParamOverrides(self, params) if isinstance(data, (HoloMap, NdOverlay)): ranges = {d.name: data.range(d) for d in data.dimensions()} data = data.clone({k: GridMatrix(self._process(p, v, ranges)) for k, v in data.items()}) data = Collator(data, merge_type=type(data))() <IF_STMT> data = data.map(lambda x: x.overlay(p.overlay_dims), (HoloMap,)) return data elif isinstance(data, Element): data = self._process(p, data) return GridMatrix(data)",if p.overlay_dims:
"def _update_model(self, events, msg, root, model, doc, comm=None): msg = dict(msg) if self._rename['objects'] in msg: old = events['objects'].old msg[self._rename['objects']] = self._get_objects(model, old, doc, root, comm) with hold(doc): super(Panel, self)._update_model(events, msg, root, model, doc, comm) from ..io import state ref = root.ref['id'] <IF_STMT> state._views[ref][0]._preprocess(root)",if ref in state._views:
"def reset_two_factor_hotp(): otp_secret = request.form.get('otp_secret', None) if otp_secret: <IF_STMT> return render_template('account_edit_hotp_secret.html') g.user.set_hotp_secret(otp_secret) db.session.commit() return redirect(url_for('account.new_two_factor')) else: return render_template('account_edit_hotp_secret.html')","if not validate_hotp_secret(g.user, otp_secret):"
"def ETA(self): if self.done: prefix = 'Done' t = self.elapsed else: prefix = 'ETA ' <IF_STMT> t = -1 elif self.elapsed == 0 or self.cur == self.min: t = 0 else: t = float(self.max - self.min) t /= self.cur - self.min t = (t - 1) * self.elapsed return '%s: %s' % (prefix, self.format_duration(t))",if self.max is None:
"def add_property(self, key, value): with self.secure() as config: keys = key.split('.') for i, key in enumerate(keys): <IF_STMT> config[key] = table() if i == len(keys) - 1: config[key] = value break config = config[key]",if key not in config and i < len(keys) - 1:
"def validate_against_domain(cls, ensemble: Optional['PolicyEnsemble'], domain: Optional[Domain]) -> None: if ensemble is None: return for p in ensemble.policies: <IF_STMT> continue if domain is None or p.deny_suggestion_intent_name not in domain.intents: raise InvalidDomain(""The intent '{0}' must be present in the domain file to use TwoStageFallbackPolicy. Either include the intent '{0}' in your domain or exclude the TwoStageFallbackPolicy from your policy configuration"".format(p.deny_suggestion_intent_name))","if not isinstance(p, TwoStageFallbackPolicy):"
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = [] kwspaces = self.kwspaces striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for idx, obj in enumerate(self.data): <IF_STMT> sub_config = _strip_config_space(config, prefix=str(idx)) ret.append(obj.sample(**sub_config)) elif isinstance(obj, SimpleSpace): ret.append(config[str(idx)]) else: ret.append(obj) return ret","if isinstance(obj, NestedSpace):"
"def init_weights(self): for module in self.decoder.modules(): <IF_STMT> module.weight.data.normal_(mean=0.0, std=0.02) elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() for p in self.generator.parameters(): if p.dim() > 1: xavier_uniform_(p) else: p.data.zero_()","if isinstance(module, (nn.Linear, nn.Embedding)):"
"def backfill_first_message_id(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None: Stream = apps.get_model('zerver', 'Stream') Message = apps.get_model('zerver', 'Message') for stream in Stream.objects.all(): first_message = Message.objects.filter(recipient__type_id=stream.id, recipient__type=2).first() <IF_STMT> continue stream.first_message_id = first_message.id stream.save()",if first_message is None:
"def commandComplete(self, cmd): if self.property: if cmd.didFail(): return result = self.observer.getStdout() <IF_STMT> result = result.strip() propname = self.property self.setProperty(propname, result, 'SetPropertyFromCommand Step') self.property_changes[propname] = result else: new_props = self.extract_fn(cmd.rc, self.observer.getStdout(), self.observer.getStderr()) for k, v in iteritems(new_props): self.setProperty(k, v, 'SetPropertyFromCommand Step') self.property_changes = new_props",if self.strip:
"def part(p, imaginary): s = 'j' if imaginary else '' try: if math.isinf(p): if p < 0: return '-1e1000' + s return '1e1000' + s <IF_STMT> return '(1e1000%s-1e1000%s)' % (s, s) except OverflowError: pass return repr(p) + s",if math.isnan(p):
"def _user_has_perm(user, perm, obj): anon = user.is_anonymous() for backend in auth.get_backends(): if not anon or backend.supports_anonymous_user: if hasattr(backend, 'has_perm'): <IF_STMT> if backend.supports_object_permissions and backend.has_perm(user, perm, obj): return True elif backend.has_perm(user, perm): return True return False",if obj is not None:
"def check_backslashes(payload): if payload.count('\\') >= 15: <IF_STMT> if menu.options.tamper: menu.options.tamper = menu.options.tamper + ',backslashes' else: menu.options.tamper = 'backslashes' from src.core.tamper import backslashes payload = backslashes.tamper(payload)",if not settings.TAMPER_SCRIPTS['backslashes']:
"def _check_model(cls): errors = [] if cls._meta.proxy: <IF_STMT> errors.append(checks.Error(""Proxy model '%s' contains model fields."" % cls.__name__, id='models.E017')) return errors",if cls._meta.local_fields or cls._meta.local_many_to_many:
"def _format_column_list(self, data): if 'columns' in data: for c in data['columns']: if 'attacl' in c: c['attacl'] = parse_priv_to_db(c['attacl'], self.column_acl) <IF_STMT> c['cltype'], c['hasSqrBracket'] = column_utils.type_formatter(c['cltype'])",if 'cltype' in c:
"def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]: ret: Dict[str, List[str]] = {} for contract in slither.contracts: cst_functions = [_get_name(f) for f in contract.functions_entry_points if _is_constant(f)] cst_functions += [v.function_name for v in contract.state_variables if v.visibility in ['public']] <IF_STMT> ret[contract.name] = cst_functions return ret",if cst_functions:
"def safe_zip(*args): """"""Like zip, but ensures arguments are of same length"""""" base = len(args[0]) for i, arg in enumerate(args[1:]): <IF_STMT> raise ValueError('Argument 0 has length %d but argument %d has length %d' % (base, i + 1, len(arg))) return zip(*args)",if len(arg) != base:
"def readMemory(self, va, size): ret = b'' while size: pageva = va & self.pagemask pageoff = va - pageva chunksize = min(self.pagesize - pageoff, size) page = self.pagecache.get(pageva) <IF_STMT> page = self.mem.readMemory(pageva, self.pagesize) self.pagecache[pageva] = page ret += page[pageoff:pageoff + chunksize] va += chunksize size -= chunksize return ret",if page is None:
"def horizontal_neighbors_iter(self, ordered=True): n_horizontal_edges_per_y = self.x_dimension - (self.x_dimension <= 2 or not self.periodic) for x in range(n_horizontal_edges_per_y): for y in range(self.y_dimension): i = self.to_site_index((x, y)) j = self.to_site_index(((x + 1) % self.x_dimension, y)) yield (i, j) <IF_STMT> yield (j, i)",if ordered:
"def apply_ordering(self, query): ordering = request.args.get('ordering') or '' if ordering: desc, column = (ordering.startswith('-'), ordering.lstrip('-')) <IF_STMT> field = self.model._meta.fields[column] query = query.order_by(field.asc() if not desc else field.desc()) return query",if column in self.model._meta.fields:
"def check_hashes(self, string): for hash in self.hashes.copy(): ctext, hash = self.check_hash(hash, string) <IF_STMT> yield (ctext, hash) self.found.add(hash) self.hashes.remove(hash)",if ctext is not None:
"def undo_block_stop(self): if self.undoblock.bump_depth(-1) == 0: cmd = self.undoblock self.undoblock = 0 if len(cmd) > 0: <IF_STMT> cmd = cmd.getcmd(0) self.addcmd(cmd, 0)",if len(cmd) == 1:
"def create_model_handler(ns, model_type):  @route(f'/<provider>/{ns}/<model_id>') @use_provider def handle(req, provider, model_id): if model_type == ModelType.user: <IF_STMT> user = getattr(provider, '_user', None) if user is None: raise CmdException(f'log in provider:{provider.identifier} first') return user model = get_model_or_raise(provider, model_type, model_id) return model",if model_id == 'me':
"def _remove_optional_none_type_hints(self, type_hints, defaults): for arg in defaults: if defaults[arg] is None and arg in type_hints: type_ = type_hints[arg] <IF_STMT> types = type_.__args__ if len(types) == 2 and types[1] is type(None): type_hints[arg] = types[0]",if self._is_union(type_):
"def set_billing_hours_and_amount(self): if not self.project: for timesheet in self.timesheets: ts_doc = frappe.get_doc('Timesheet', timesheet.time_sheet) <IF_STMT> timesheet.billing_hours = ts_doc.total_billable_hours if not timesheet.billing_amount and ts_doc.total_billable_amount: timesheet.billing_amount = ts_doc.total_billable_amount",if not timesheet.billing_hours and ts_doc.total_billable_hours:
"def _real_len(self, s): s_len = 0 in_esc = False prev = ' ' for c in replace_all({'\x00+': '', '\x00-': '', '\x00^': '', '\x01': '', '\t': ' '}, s): if in_esc: if c == 'm': in_esc = False el<IF_STMT> in_esc = True s_len -= 1 else: s_len += self._display_len(c) prev = c return s_len",if c == '[' and prev == '\x1b':
"def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: if predicate(item): return item if self._tree.ItemHasChildren(item): result = self._find_node_with_predicate(item, predicate) <IF_STMT> return result item, cookie = self._tree.GetNextChild(node, cookie) return None",if result:
"def main(): parser = optparse.OptionParser() options, argv = parser.parse_args() counts = defaultdict(int) for line in fileinput.input(argv): try: tweet = json.loads(line) except: continue if 'retweeted_status' not in tweet: continue rt = tweet['retweeted_status'] id = rt['id_str'] count = rt['retweet_count'] <IF_STMT> counts[id] = count for id in sorted(counts, key=counts.get, reverse=True): print('{},{}'.format(id, counts[id]))",if count > counts[id]:
"def to_get_select_object_meta(meta_param): if meta_param is not None and SelectParameters.Json_Type in meta_param: <IF_STMT> raise SelectOperationClientError(""Json_Type can only be 'LINES' for creating meta"", '') else: return to_get_select_json_object_meta(meta_param) else: return to_get_select_csv_object_meta(meta_param)",if meta_param[SelectParameters.Json_Type] != SelectJsonTypes.LINES:
"def check_if_match(self, value, index, flags=0): pattern = self.get_pattern(index) if value: if _is_iterable(value): <IF_STMT> return True else: if isinstance(value, (int, long)): value = str(value) return bool(re.search(pattern, value, flags)) return False","if any([bool(re.search(pattern, x, flags)) for x in value]):"
"def assemble(self, multi_model_placement: Dict[Model, PhysicalDevice]) -> Tuple[Node, PhysicalDevice]: for node in self.origin_nodes: <IF_STMT> new_node = Node(node.original_graph, node.id, f'M_{node.original_graph.model.model_id}_{node.name}', node.operation) return (new_node, multi_model_placement[node.original_graph.model]) raise ValueError(f'DedupInputNode {self.name} does not contain nodes from multi_model')",if node.original_graph.model in multi_model_placement:
"def doc_generator(self, imdb_dir, dataset, include_label=False): dirs = [(os.path.join(imdb_dir, dataset, 'pos'), True), (os.path.join(imdb_dir, dataset, 'neg'), False)] for d, label in dirs: for filename in os.listdir(d): with tf.gfile.Open(os.path.join(d, filename)) as imdb_f: doc = imdb_f.read().strip() <IF_STMT> yield (doc, label) else: yield doc",if include_label:
"def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: if node is None: continue <IF_STMT> continue if BaseNode.test_empty_node(node): continue return False return True",if type(node) is CodeNode and BaseNode.test_empty_node(node.node):
"def rewrite_imports(package_dir, vendored_libs, vendor_dir): for item in package_dir.iterdir(): if item.is_dir(): rewrite_imports(item, vendored_libs, vendor_dir) <IF_STMT> rewrite_file_imports(item, vendored_libs, vendor_dir)",elif item.name.endswith('.py'):
def _validate_zone(self): availability_zone = self.availability_zone if availability_zone: zone = self.ec2.get_zone(availability_zone) if not zone: raise exception.ClusterValidationError('availability_zone = %s does not exist' % availability_zone) <IF_STMT> log.warn('The availability_zone = %s ' % zone + 'is not available at this time') return True,if zone.state != 'available':
"def addnoise(line): noise = fillers ratio = len(line) // len(noise) res = '' while line and noise: <IF_STMT> c, line = (line[0], line[1:]) else: c, noise = (noise[0], noise[1:]) res += c return res + noise + line",if len(line) // len(noise) > ratio:
"def cwr1(iterable, r): """"""Pure python version shown in the docs"""""" pool = tuple(iterable) n = len(pool) if not n and r: return indices = [0] * r yield tuple((pool[i] for i in indices)) while 1: for i in reversed(range(r)): <IF_STMT> break else: return indices[i:] = [indices[i] + 1] * (r - i) yield tuple((pool[i] for i in indices))",if indices[i] != n - 1:
"def subscribe(self, params) -> bool: emit_data = {'method': 'eth_subscribe', 'params': params} nonce = await self._send(emit_data) raw_message = await self._client.recv() if raw_message is not None: resp = ujson.loads(raw_message) <IF_STMT> self._node_address = resp.get('result') return True return False","if resp.get('id', None) == nonce:"
"def _(node): for __ in dir(node): <IF_STMT> candidate = getattr(node, __) if isinstance(candidate, str): if '\\' in candidate: try: re.compile(candidate) except: errMsg = ""smoke test failed at compiling '%s'"" % candidate logger.error(errMsg) raise else: _(candidate)",if not __.startswith('_'):
"def get_field_values(self, fields): field_values = [] for field in fields: if field == 'title': value = self.get_title_display() <IF_STMT> try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = '' elif field == 'salutation': value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values",elif field == 'country':
"def __str__(self): s = '' for k, v in self._members.items(): if isinstance(v.get('type'), list): s += k + ' : ' + ';'.join(getattr(self, item)) + '\n' <IF_STMT> s += k + ' : ' + getattr(self, k) + '\n' return s","elif isinstance(v.get('type'), str):"
"def _merge(self, a, b, path=None): """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge"""""" if path is None: path = [] for key in b: if key in a: if isinstance(a[key], dict) and isinstance(b[key], dict): self._merge(a[key], b[key], path + [str(key)]) <IF_STMT> pass else: raise Exception('Conflict at %s' % '.'.join(path + [str(key)])) else: a[key] = b[key] return a",elif a[key] == b[key]:
"def get_child_nodes(node): if isinstance(node, _ast.Module): return node.body result = [] if node._fields is not None: for name in node._fields: child = getattr(node, name) if isinstance(child, list): for entry in child: if isinstance(entry, _ast.AST): result.append(entry) <IF_STMT> result.append(child) return result","if isinstance(child, _ast.AST):"
def _handle_enter(self) -> None: if self.multiple_selection: val = self.values[self._selected_index][0] <IF_STMT> self.current_values.remove(val) else: self.current_values.append(val) else: self.current_value = self.values[self._selected_index][0],if val in self.current_values:
"def close_all(map=None, ignore_all=False): if map is None: map = socket_map for x in list(map.values()): try: x.close() except OSError as x: <IF_STMT> pass elif not ignore_all: raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()",if x.args[0] == EBADF:
"def _get_spawn_property(self, constraints, constraint_name, services): if services: if constraint_name == IMAGE_CONSTRAINT: return services[0].image elif constraint_name == CPUS_CONSTRAINT: return services[0].cpus for constraint in constraints: <IF_STMT> return constraint.value return None",if constraint.name == constraint_name:
"def _handle_children(self, removed, added): for obj in removed: obj.stop() for obj in added: obj.set(scene=self.scene, parent=self) if isinstance(obj, ModuleManager): obj.source = self <IF_STMT> obj.inputs.append(self) if self.running: try: obj.start() except: exception()",elif is_filter(obj):
"def _get_cols_width(self, values): width = 14 for row in values: for header in self.headers: header_len = len(header) if header_len > width: width = header_len value_len = len(unicode(row.get(header, ''))) <IF_STMT> width = value_len width += 2 return width",if value_len > width:
"def crawl(self, *args, **kwargs): assert not self.crawling, 'Crawling already taking place' self.crawling = True try: self.spider = self._create_spider(*args, **kwargs) self.engine = self._create_engine() <IF_STMT> start_requests = iter(self.spider.start_requests()) else: start_requests = () yield self.engine.open_spider(self.spider, start_requests) yield defer.maybeDeferred(self.engine.start) except Exception: self.crawling = False raise",if self.start_requests:
"def _copy_files(self, files, src, dest, message=''): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) <IF_STMT> print('{}: {}'.format(message, destpath)) if os.path.exists(srcpath): destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) elif os.path.exists(destpath): os.remove(destpath)",if message:
def describe_tags(self): resource_arns = self._get_multi_param('ResourceArns.member') resources = [] for arn in resource_arns: <IF_STMT> resource = self.elbv2_backend.target_groups.get(arn) if not resource: raise TargetGroupNotFoundError() elif ':loadbalancer' in arn: resource = self.elbv2_backend.load_balancers.get(arn) if not resource: raise LoadBalancerNotFoundError() else: raise LoadBalancerNotFoundError() resources.append(resource) template = self.response_template(DESCRIBE_TAGS_TEMPLATE) return template.render(resources=resources),if ':targetgroup' in arn:
def iterator(): try: while True: yield from pullparser.read_events() data = source.read(16 * 1024) <IF_STMT> break pullparser.feed(data) root = pullparser._close_and_return_root() yield from pullparser.read_events() it.root = root finally: if close_source: source.close(),if not data:
"def __repr__(self): data = '' for c in self.children: data += c.shortrepr() <IF_STMT> data = data[:56] + ' ...' break if self['names']: return '<%s ""%s"": %s>' % (self.__class__.__name__, '; '.join([ensure_str(n) for n in self['names']]), data) else: return '<%s: %s>' % (self.__class__.__name__, data)",if len(data) > 60:
"def __exit__(self, exc_type, exc_value, traceback): template_rendered.disconnect(self.on_template_render) if exc_type is not None: return if not self.test(): message = self.message() <IF_STMT> message += ' No template was rendered.' else: message += ' Following templates were rendered: %s' % ', '.join(self.rendered_template_names) self.test_case.fail(message)",if len(self.rendered_templates) == 0:
"def _match(self, byte_chunk): quote_character = None data = byte_chunk.nhtml open_angle_bracket = data.rfind('<') if open_angle_bracket <= data.rfind('>'): return False for s in data[open_angle_bracket + 1:]: if s in ATTR_DELIMITERS: if quote_character and s == quote_character: quote_character = None continue <IF_STMT> quote_character = s continue if quote_character == self.quote_character: return True return False",elif not quote_character:
"def recent_events(self, events): try: frame = self.get_frame() except EndofVideoFileError: logger.info('Video has ended.') self.notify_all({'subject': 'file_source.video_finished', 'source_path': self.source_path}) self.play = False else: self._recent_frame = frame events['frame'] = frame <IF_STMT> self.wait(frame)",if self.timed_playback:
"def _prune(self): if self.over_threshold(): now = time.time() for idx, (key, (expires, _)) in enumerate(self._cache.items()): <IF_STMT> with self._mutex: self._cache.pop(key, None)",if expires is not None and expires <= now or idx % 3 == 0:
"def dict_path(d, path): if not isinstance(path, (list, tuple)): raise ValueError() for keys in path: if type(keys) is not list: keys = [keys] value = None for key in keys: <IF_STMT> continue value = d[key] if value is None: value = {} for key in keys: d[key] = value d = value return d",if key not in d:
"def span_tokenize(self, string): if self.__tokenizer == 'nltk': raw_tokens = nltk.word_tokenize(string) <IF_STMT> matched = [m.group() for m in re.finditer('``|\'{2}|\\""', string)] tokens = [matched.pop(0) if tok in ['""', '``', ""''""] else tok for tok in raw_tokens] else: tokens = raw_tokens spans = align_tokens(tokens, string) return spans","if '""' in string or ""''"" in string:"
"def literal(self): if self.peek('""'): lit, lang, dtype = self.eat(r_literal).groups() if lang: lang = lang else: lang = None if dtype: dtype = dtype else: dtype = None <IF_STMT> raise ParseError(""Can't have both a language and a datatype"") lit = unquote(lit) return Literal(lit, lang, dtype) return False",if lang and dtype:
"def get(): result = [] for b in self.key_bindings: if len(keys) < len(b.keys): match = True for i, j in zip(b.keys, keys): <IF_STMT> match = False break if match: result.append(b) return result",if i != j and i != Keys.Any:
"def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] if len(rulesList[ruleIndex]) == maxLength: args = rulesList[ruleIndex][-1] <IF_STMT> shouldRunMethod, method, isCorrect = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) elif maxLength == 3: shouldRunMethod, method = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking",if maxLength == 4:
def parents_in_pipfile(self): if not self._parents_in_pipfile: self._parents_in_pipfile = [p for p in self.flattened_parents <IF_STMT>] return self._parents_in_pipfile,if p.normalized_name in self.pipfile_packages
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_content(d.getPrefixedString()) continue <IF_STMT> self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue if tt == 32: self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def base64_encode_image_mapper(self, tag, url): if tag == 'img': <IF_STMT> image_data = base64.b64encode(self.kp_images[url]) image_mimetype = mimetypes.guess_type(url)[0] if image_mimetype is not None: return 'data:{};base64, '.format(image_mimetype) + image_data.decode('utf-8') return None",if url in self.kp_images:
"def get_args_from_ref_args(handler, ref_args): args = [] for ref_arg in ref_args: <IF_STMT> temp = handler.create_from_numpy(ref_arg) args.append(temp) else: args.append(ref_arg) return args",if type(ref_arg) is ref.array_type:
"def _get_cols_width(self, values): width = 14 for row in values: for header in self.headers: header_len = len(header) <IF_STMT> width = header_len value_len = len(unicode(row.get(header, ''))) if value_len > width: width = value_len width += 2 return width",if header_len > width:
"def OnLeaveWindow(self, event): if self.start_drag and (not self.dragging): self.dragging = False self.start_drag = False self.dragged_tab = None self.drag_trigger = self.drag_trail <IF_STMT> self.ReleaseMouse() if self.preview_wnd: self.preview_wnd.Show(False) del self.preview_wnd self.preview_wnd = None event.Skip()",if self.HasCapture():
"def _checkPid(self, pid): retval = False if self.settings.windows: PROCESS_TERMINATE = 1 p = ctypes.windll.kernel32.OpenProcess(PROCESS_TERMINATE, 0, pid) retval = p != 0 <IF_STMT> ctypes.windll.kernel32.CloseHandle(p) else: try: os.kill(pid, 0) except OSError: pass else: retval = True return retval",if p:
"def concat_index_value(index_values, store_data=False): result = pd.Index([]) if not isinstance(index_values, (list, tuple)): index_values = [index_values] for index_value in index_values: <IF_STMT> result = result.append(index_value) else: result = result.append(index_value.to_pandas()) return parse_index(result, store_data=store_data)","if isinstance(index_value, pd.Index):"
"def apply(self, db, family): if self.rtype: if self.rtype.is_custom() and self.use_regex: <IF_STMT> return False elif self.rtype != family.get_relationship(): return False return True",if self.regex[0].search(str(family.get_relationship())) is None:
"def get_child_nodes(node): if isinstance(node, _ast.Module): return node.body result = [] if node._fields is not None: for name in node._fields: child = getattr(node, name) if isinstance(child, list): for entry in child: <IF_STMT> result.append(entry) if isinstance(child, _ast.AST): result.append(child) return result","if isinstance(entry, _ast.AST):"
"def output(self): """"""Transform self into a list of (name, value) tuples."""""" header_list = [] for k, v in self.items(): if isinstance(k, unicodestr): k = self.encode(k) if not isinstance(v, basestring): v = str(v) <IF_STMT> v = self.encode(v) k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list","if isinstance(v, unicodestr):"
def check_valid_emoji_name(emoji_name: str) -> None: if emoji_name: <IF_STMT> return raise JsonableError(_('Invalid characters in emoji name')) raise JsonableError(_('Emoji name is missing')),"if re.match('^[0-9a-z.\\-_]+(?<![.\\-_])$', emoji_name):"
"def cache_subscriptions(self, region: str): async with self.regional_subscriptions_cache_locks.setdefault(region, asyncio.Lock()): <IF_STMT> return self.subscriptions_cache[region] = await AWSFacadeUtils.get_all_pages('sns', region, self.session, 'list_subscriptions', 'Subscriptions') for subscription in self.subscriptions_cache[region]: topic_arn = subscription.pop('TopicArn') subscription['topic_name'] = topic_arn.split(':')[-1]",if region in self.subscriptions_cache:
"def AdjustArg(arg, break_chars, argv_out): end_indices = [] state = ST_Begin for i, c in enumerate(arg): ch = CH_Break if c in break_chars else CH_Other state, emit_span = _TRANSITIONS[state, ch] <IF_STMT> end_indices.append(i) end_indices.append(len(arg)) begin = 0 for end in end_indices: argv_out.append(arg[begin:end]) begin = end",if emit_span:
"def load_model(self, model_name: str, path: str=None, model_type=None) -> AbstractModel: if isinstance(model_name, AbstractModel): return model_name if model_name in self.models.keys(): return self.models[model_name] else: if path is None: path = self.get_model_attribute(model=model_name, attribute='path') <IF_STMT> model_type = self.get_model_attribute(model=model_name, attribute='type') return model_type.load(path=path, reset_paths=self.reset_paths)",if model_type is None:
"def find_config(pipeline_config_path: Union[str, Path]) -> Path: if not Path(pipeline_config_path).is_file(): configs = [c for c in Path(__file__).parent.parent.parent.glob(f'configs/**/{pipeline_config_path}.json') <IF_STMT>] if configs: log.info(f""Interpreting '{pipeline_config_path}' as '{configs[0]}'"") pipeline_config_path = configs[0] return Path(pipeline_config_path)",if str(c.with_suffix('')).endswith(pipeline_config_path)
"def __init__(self, bounds, channel_axis, preprocess=None): assert len(bounds) == 2 assert channel_axis in [0, 1, 2, 3] self._bounds = bounds self._channel_axis = channel_axis if preprocess is not None: sub, div = np.array(preprocess) if not np.any(sub): sub = 0 <IF_STMT> div = 1 assert div is None or np.all(div) self._preprocess = (sub, div) else: self._preprocess = (0, 1)",if np.all(div == 1):
"def iter_imports(path): """"""Yield imports in *path*"""""" for node in ast.parse(open(path, 'rb').read()).body: if isinstance(node, ast.ImportFrom): <IF_STMT> prefix = () else: prefix = tuple(node.module.split('.')) for snode in node.names: yield (node.level, prefix + (snode.name,)) elif isinstance(node, ast.Import): for node in node.names: yield (0, tuple(node.name.split('.')))",if node.module is None:
"def __init__(self, spec=None, add_book=True, xl=None, visible=None): if spec is not None: warn('spec is ignored on Windows.') if xl is None: self._xl = COMRetryObjectWrapper(DispatchEx('Excel.Application')) <IF_STMT> self._xl.Workbooks.Add() self._hwnd = None elif isinstance(xl, int): self._xl = None self._hwnd = xl else: self._xl = xl self._hwnd = None",if add_book:
"def _find_split(): """"""Find the first = sign to split on (that isn't in [brackets])"""""" key = [] value = [] brackets = False chars = list(expression) while chars: c = chars.pop(0) if c == '=' and (not brackets): value = chars break <IF_STMT> brackets = True key += c elif c == ']' and brackets: brackets = False key += c else: key += c return (''.join(key), ''.join(value))",elif c == '[':
"def _ApplySizeLimit(regions: Iterable[rdf_memory.ProcessMemoryRegion], size_limit: int) -> List[rdf_memory.ProcessMemoryRegion]: """"""Truncates regions so that the total size stays in size_limit."""""" total_size = 0 regions_in_limit = [] for region in regions: <IF_STMT> break region.dumped_size = min(region.size, size_limit - total_size) regions_in_limit.append(region) total_size += region.dumped_size return regions_in_limit",if total_size >= size_limit:
"def _get_matched_files(input_path): """"""Returns all files that matches the input_path."""""" input_patterns = input_path.strip().split(',') all_matched_files = [] for input_pattern in input_patterns: input_pattern = input_pattern.strip() <IF_STMT> continue matched_files = tf.io.gfile.glob(input_pattern) if not matched_files: raise ValueError('%s does not match any files.' % input_pattern) else: all_matched_files.extend(matched_files) return sorted(all_matched_files)",if not input_pattern:
"def _add_kid(key, x): if x is None: kids[key] = None elif type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] <IF_STMT> kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, '__iter__'): if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x",if x1:
"def _read_info(self, field): fs.File._read_info(self, field) if field == 'dimensions': self.dimensions = self._plat_get_dimensions() <IF_STMT> self.dimensions = (self.dimensions[1], self.dimensions[0]) elif field == 'exif_timestamp': self.exif_timestamp = self._get_exif_timestamp()","if self._get_orientation() in {5, 6, 7, 8}:"
"def process_timeline(self, info): children = info.get('_children', []) if not children: return False for entry in children: state = TIMELINE_STATES.get(entry.get('state')) <IF_STMT> continue self.emit('%s.timeline.%s' % (self.name, state), entry) return True",if not state:
"def from_chx(self): if self.array is not None: device = backend.get_device_from_array(self.array) else: device = self._initial_device if device.xp is chainerx: backend_name = device.device.backend.name if backend_name == 'native': self._initial_device = backend.CpuDevice() <IF_STMT> self._initial_device = backend.GpuDevice.from_device_id(device.device.index) super(Parameter, self)._from_chx(allow_unchaining=True)",elif backend_name == 'cuda':
"def get_title_extensions(self, title=None): extensions = [] for extension in self.title_extensions: <IF_STMT> extensions.extend(list(extension.objects.filter(extended_object=title))) else: extensions.extend(list(extension.objects.all())) return extensions",if title:
"def tag(vs, push=False): """"""Make the tagged release commit"""""" patch_version(vs, repo_root) with cd(repo_root): run('git commit -a -m ""release {}""'.format(vs)) run('git tag -a -m ""release {0}"" {0}'.format(vs)) <IF_STMT> run('git push') run('git push --tags')",if push:
"def parse_bismark_report(self, report, regexes): """"""Search a bismark report with a set of regexes"""""" parsed_data = {} for k, r in regexes.items(): r_search = re.search(r, report, re.MULTILINE) <IF_STMT> try: parsed_data[k] = float(r_search.group(1)) except ValueError: parsed_data[k] = r_search.group(1) if len(parsed_data) == 0: return None return parsed_data",if r_search:
"def _scroll_delete(dirname, max_num_checkpoints=3): dirs = os.listdir(dirname) serial_map = {} for serial in dirs: serial_num = _get_dir_serial(serial) serial_map[serial_num] = serial if len(list(serial_map.keys())) <= max_num_checkpoints: return serials = list(serial_map.keys()) serials.sort(reverse=True) serials = serials[max_num_checkpoints:] for serial in serials: cur_dir = _get_serial_dir(dirname, serial) try: shutil.rmtree(cur_dir) except OSError as err: <IF_STMT> raise err",if err.errno != errno.ENOENT:
"def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: <IF_STMT> continue if key_len > d.longest_key: continue value = d.get(key) if value: for f in filters: if f(key, value): return None return value",if not d.enabled:
"def get_preset(self, unit): for line in self._lines: m = re.match('(enable|disable)\\s+(\\S+)', line) <IF_STMT> status, pattern = (m.group(1), m.group(2)) if fnmatch.fnmatchcase(unit, pattern): logg.debug('%s %s => %s [%s]', status, pattern, unit, self.filename()) return status return None",if m:
"def gen_cpu_name(cpu): if cpu == 'simple': return event_download.get_cpustr() for j in known_cpus: <IF_STMT> if isinstance(j[1][0], tuple): return 'GenuineIntel-6-%02X-%d' % j[1][0] else: return 'GenuineIntel-6-%02X' % j[1][0] assert False",if cpu == j[0]:
"def allow_request(self, request, view): if settings.API_THROTTLING: request_allowed = super(GranularUserRateThrottle, self).allow_request(request, view) <IF_STMT> user = getattr(request, 'user', None) if user and request.user.is_authenticated: log.info('User %s throttled for scope %s', request.user, self.scope) ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user) return request_allowed else: return True",if not request_allowed:
"def __getitem__(self, tagSet): try: return self.__presentTypes[tagSet] except KeyError: <IF_STMT> raise KeyError() elif tagSet in self.__skipTypes: raise error.PyAsn1Error('Key in negative map') else: return self.__defaultType",if self.__defaultType is None:
"def _media(self): sup_cls = super(cls, self) try: base = sup_cls.media except AttributeError: base = Media() definition = getattr(cls, 'Media', None) if definition: extend = getattr(definition, 'extend', True) if extend: <IF_STMT> m = base else: m = Media() for medium in extend: m = m + base[medium] return m + Media(definition) else: return Media(definition) else: return base",if extend == True:
"def ascii85decode(data): n = b = 0 out = '' for c in data: if '!' <= c and c <= 'u': n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack('>L', b) n = b = 0 elif c == 'z': assert n == 0 out += '\x00\x00\x00\x00' <IF_STMT> if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack('>L', b)[:n - 1] break return out",elif c == '~':
"def get_max_shape(data): if isinstance(data, dict): max = 0 val = None for k, v in data.items(): tmp = reduce(lambda x, y: x * y, v.shape) <IF_STMT> val = v.shape max = tmp return val else: return data[0].shape",if tmp > max:
"def _subscribe_core(self, observer: typing.Observer, scheduler: Optional[typing.Scheduler]=None) -> typing.Disposable: with self.lock: self.check_disposed() <IF_STMT> self.observers.append(observer) return InnerSubscription(self, observer) ex = self.exception has_value = self.has_value value = self.value if ex: observer.on_error(ex) elif has_value: observer.on_next(value) observer.on_completed() else: observer.on_completed() return Disposable()",if not self.is_stopped:
"def ratio(self, outevent, inevent): assert outevent not in self assert inevent in self for function in compat_itervalues(self.functions): assert outevent not in function assert inevent in function function[outevent] = ratio(function[inevent], self[inevent]) for call in compat_itervalues(function.calls): assert outevent not in call <IF_STMT> call[outevent] = ratio(call[inevent], self[inevent]) self[outevent] = 1.0",if inevent in call:
"def _format_changelog(self, changelog): """"""Format the changelog correctly and convert it to a list of strings"""""" if not changelog: return changelog new_changelog = [] for line in changelog.strip().split('\n'): line = line.strip() if line[0] == '*': new_changelog.extend(['', line]) <IF_STMT> new_changelog.append(line) else: new_changelog.append('  ' + line) if not new_changelog[0]: del new_changelog[0] return new_changelog",elif line[0] == '-':
"def _set_base64md5(self, value): if value: <IF_STMT> value = value.decode('utf-8') self.local_hashes['md5'] = binascii.a2b_base64(value) elif 'md5' in self.local_hashes: del self.local_hashes['md5']","if not isinstance(value, six.string_types):"
"def setGeometry(self, rect): """"""Set the window geometry, but only once when using the qttabs gui."""""" if g.app.qt_use_tabs: m = self.leo_master assert self.leo_master <IF_STMT> m.leo_geom_inited = True self.leo_master.setGeometry(rect) QtWidgets.QMainWindow.setGeometry(self, rect) else: QtWidgets.QMainWindow.setGeometry(self, rect)","if not hasattr(m, 'leo_geom_inited'):"
"def _get_extension_suppressions(mod_loaders): res = [] for m in mod_loaders: suppressions = getattr(m, 'suppress_extension', None) if suppressions: suppressions = suppressions if isinstance(suppressions, list) else [suppressions] for sup in suppressions: <IF_STMT> res.append(sup) return res","if isinstance(sup, ModExtensionSuppress):"
"def _check_positional(results): positional = None for name, char in results: if positional is None: positional = name is None el<IF_STMT> raise TranslationError('format string mixes positional and named placeholders') return bool(positional)",if (name is None) != positional:
"def ascii85decode(data): n = b = 0 out = '' for c in data: if '!' <= c and c <= 'u': n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack('>L', b) n = b = 0 <IF_STMT> assert n == 0 out += '\x00\x00\x00\x00' elif c == '~': if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack('>L', b)[:n - 1] break return out",elif c == 'z':
"def __getattr__(self, name): assert skip_checks or name != 'aval' try: attr = getattr(self.aval, name) except KeyError as err: raise AttributeError('{} has no attribute {}'.format(self.__class__.__name__, name)) from err else: t = type(attr) <IF_STMT> return attr.fget(self) elif t is aval_method: return types.MethodType(attr.fun, self) else: return attr",if t is aval_property:
"def build_vocab(self, filename): EOS = '</eos>' vocab_dict = {} ids = 0 vocab_dict[EOS] = ids ids += 1 with open(filename, 'r') as f: for line in f.readlines(): for w in line.strip().split(): <IF_STMT> vocab_dict[w] = ids ids += 1 self.vocab_size = ids return vocab_dict",if w not in vocab_dict:
"def eval_dummy_genomes_iznn(genomes, config): for genome_id, genome in genomes: net = neat.iznn.IZNN.create(genome, config) if genome_id < 10: net.reset() genome.fitness = 0.0 <IF_STMT> genome.fitness = 0.5 else: genome.fitness = 1.0",elif genome_id <= 150:
"def _add_csrf(self, without_csrf, explicit_csrf=None): parts = urlparse(without_csrf) query = parse_qs(parts[4]) with self.app.session_transaction() as sess: <IF_STMT> query[CSRF_TOKEN_KEY] = explicit_csrf else: sess[CSRF_TOKEN_KEY] = 'something' query[CSRF_TOKEN_KEY] = sess[CSRF_TOKEN_KEY] return urlunparse(list(parts[0:4]) + [urlencode(query)] + list(parts[5:]))",if explicit_csrf is not None:
"def test_confirm_extension_is_yml(self): files_with_incorrect_extensions = [] for file in self.yield_next_rule_file_path(self.path_to_rules): file_name_and_extension = os.path.splitext(file) <IF_STMT> extension = file_name_and_extension[1] if extension != '.yml': files_with_incorrect_extensions.append(file) self.assertEqual(files_with_incorrect_extensions, [], Fore.RED + 'There are rule files with extensions other than .yml')",if len(file_name_and_extension) == 2:
"def _handle_eof(self, m): self.lock.acquire() try: if not self.eof_received: self.eof_received = True self.in_buffer.close() self.in_stderr_buffer.close() <IF_STMT> self._pipe.set_forever() finally: self.lock.release() self._log(DEBUG, 'EOF received ({})'.format(self._name))",if self._pipe is not None:
"def do_close(self): if self.flags is not None and (self.flags == 'c' or self.flags == 'w'): <IF_STMT> insert = self.table.insert() self.bind.execute(insert, namespace=self.namespace, data=self.hash, accessed=datetime.now(), created=datetime.now()) self._is_new = False else: update = self.table.update(self.table.c.namespace == self.namespace) self.bind.execute(update, data=self.hash, accessed=datetime.now()) self.flags = None",if self._is_new:
"def __init__(self, sh_cmd, title=None, env=None, d=None): self.command = d and d.getVar('OE_TERMINAL_CUSTOMCMD') if self.command: <IF_STMT> self.command += ' {command}' Terminal.__init__(self, sh_cmd, title, env, d) logger.warn('Custom terminal was started.') else: logger.debug(1, 'No custom terminal (OE_TERMINAL_CUSTOMCMD) set') raise UnsupportedTerminal('OE_TERMINAL_CUSTOMCMD not set')",if not '{command}' in self.command:
"def __code_color(self, code): if code in self.last_dist.keys(): <IF_STMT> return self.screen.markup.GREEN elif int(code) == 314: return self.screen.markup.MAGENTA else: return self.screen.markup.RED else: return ''",if int(code) == 0:
"def _calc_benchmark_stat(self, f): timer = Timer() i = 0 while True: f() i += 1 if i >= self.min_run: _, elapsed = timer.lap() <IF_STMT> break return BenchmarkStat(elapsed / i, i)",if elapsed > self.min_time:
"def _get_user_call_site(): import traceback stack = traceback.extract_stack(sys._getframe()) for i in range(1, len(stack)): callee_path = stack[i][STACK_FILE_NAME] <IF_STMT> caller_path = stack[i - 1][STACK_FILE_NAME] caller_lineno = stack[i - 1][STACK_LINE_NUM] dpark_func_name = stack[i][STACK_FUNC_NAME] user_call_site = '%s:%d ' % (caller_path, caller_lineno) return (dpark_func_name, user_call_site) return ('<func>', ' <root>')",if src_dir == os.path.dirname(os.path.abspath(callee_path)):
"def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) <IF_STMT> continue if isinstance(value, list): value = HIDE_LIST elif key == FEATS: value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append('%s=%s' % (key, value)) return '%s(%s)' % (record.__class__.__name__, ', '.join(parts))",if not value:
"def get_tools(self, found_files): self.configured_by = {} runners = [] for tool_name in self.tools_to_run: tool = tools.TOOLS[tool_name]() config_result = tool.configure(self, found_files) <IF_STMT> configured_by = None messages = [] else: configured_by, messages = config_result if messages is None: messages = [] self.configured_by[tool_name] = configured_by self.messages += messages runners.append(tool) return runners",if config_result is None:
def erase_previous(self): if self.prev: length = len(self.prev) <IF_STMT> length = length - 1 self.write(' ' * length + '\r') self.prev = '',"if self.prev[-1] in ('\n', '\r'):"
"def __demo_mode_pause_if_active(self, tiny=False): if self.demo_mode: wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT <IF_STMT> wait_time = float(self.demo_sleep) if not tiny: time.sleep(wait_time) else: time.sleep(wait_time / 3.4) elif self.slow_mode: self.__slow_mode_pause_if_active()",if self.demo_sleep:
"def pack_remaining_length(remaining_length): s = '' while True: byte = remaining_length % 128 remaining_length = remaining_length // 128 if remaining_length > 0: byte = byte | 128 s = s + struct.pack('!B', byte) <IF_STMT> return s",if remaining_length == 0:
"def _get_definitions(self, schema, query): results, error = self.run_query(query, None) if error is not None: raise Exception('Failed getting schema.') results = json_loads(results) for row in results['rows']: if row['TABLE_SCHEMA'] != 'public': table_name = '{}.{}'.format(row['TABLE_SCHEMA'], row['TABLE_NAME']) else: table_name = row['TABLE_NAME'] <IF_STMT> schema[table_name] = {'name': table_name, 'columns': []} schema[table_name]['columns'].append(row['COLUMN_NAME'])",if table_name not in schema:
def _parsed_config_to_dict(config): config_dict = {} for section in config.keys(): <IF_STMT> continue config_dict[section] = {} for option in config[section].keys(): config_dict[section][option] = config[section][option] return config_dict,if section == 'DEFAULT':
"def escape_string(self, value): value = EscapedString.promote(value) value = value.expanduser() result = '' for is_literal, txt in value.strings: if is_literal: txt = pipes.quote(txt) <IF_STMT> txt = ""'%s'"" % txt else: txt = txt.replace('\\', '\\\\') txt = txt.replace('""', '\\""') txt = '""%s""' % txt result += txt return result","if not txt.startswith(""'""):"
"def sendMessage(self, text, meta=None): if self.account.client is None: raise locals.OfflineError for line in text.split('\n'): <IF_STMT> self.account.client.ctcpMakeQuery(self.name, [('ACTION', line)]) else: self.account.client.msg(self.name, line) return succeed(text)","if meta and meta.get('style', None) == 'emote':"
def clean_email(self): email = self.cleaned_data.get('email') if self.instance.id: <IF_STMT> if not User.objects.filter(email=self.cleaned_data.get('email')).exists(): return self.cleaned_data.get('email') raise forms.ValidationError('Email already exists') else: return self.cleaned_data.get('email') else: if not User.objects.filter(email=self.cleaned_data.get('email')).exists(): return self.cleaned_data.get('email') raise forms.ValidationError('User already exists with this email'),if self.instance.email != email:
"def render_checks(cr, size, nchecks): """"""Render a checquerboard pattern to a cairo surface"""""" cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_1) cr.paint() cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_2) for i in xrange(0, nchecks): for j in xrange(0, nchecks): <IF_STMT> continue cr.rectangle(i * size, j * size, size, size) cr.fill()",if (i + j) % 2 == 0:
"def seek(self, timestamp, log=True): """"""Seek to a particular timestamp in the movie."""""" if self.status in [PLAYING, PAUSED]: player = self._player <IF_STMT> player.set_time(int(timestamp * 1000.0)) self._vlc_clock.reset(timestamp) if self.status == PAUSED: self._pause_time = timestamp if log: logAttrib(self, log, 'seek', timestamp)",if player and player.is_seekable():
"def class_results_to_node(key, elements): title = attributetabletitle(key, key) ul = nodes.bullet_list('') for element in elements: ref = nodes.reference('', '', *[nodes.Text(element.label)], internal=True, refuri='#' + element.fullname, anchorname='') para = addnodes.compact_paragraph('', '', ref) <IF_STMT> ul.append(attributetable_item('', element.badge, para)) else: ul.append(attributetable_item('', para)) return attributetablecolumn('', title, ul)",if element.badge is not None:
"def parse_function(self, l): bracket = l.find('(') fname = l[8:bracket] if self.properties: if self.properties[0] == 'propget': self.props[fname] = 1 self.propget[fname] = 1 <IF_STMT> self.props[fname] = 1 self.propput[fname] = 1 else: self.functions[fname] = 1 self.properties = None",elif self.properties[0] == 'propput':
"def _slurp_from_queue(self, task_id, accept, limit=1000, no_ack=False): with self.app.pool.acquire_channel(block=True) as (_, channel): binding = self._create_binding(task_id)(channel) binding.declare() for _ in range(limit): msg = binding.get(accept=accept, no_ack=no_ack) <IF_STMT> break yield msg else: raise self.BacklogLimitExceeded(task_id)",if not msg:
"def analyse_text(text): if re.search('^\\s*model\\s*\\{', text, re.M): if re.search('^\\s*data\\s*\\{', text, re.M): return 0.9 <IF_STMT> return 0.9 else: return 0.3 else: return 0","elif re.search('^\\s*var', text, re.M):"
"def wait_for_step(self, error_buffer=None, timeout=None): with self.cv: start = time.time() while True: <IF_STMT> return elif timeout is not None and time.time() - start > timeout: raise error.Error('No rewards received in {}s'.format(timeout)) if error_buffer: error_buffer.check() self.cv.wait(timeout=0.5)",if self.count != 0:
"def TestDictAgainst(dict, check): for key, value in check.iteritems(): <IF_STMT> raise error(""Indexing for '%s' gave the incorrect value - %s/%s"" % (repr(key), repr(dict[key]), repr(check[key])))",if dict(key) != value:
"def callback(username, password, msg): self.add_channel() if hasattr(self, '_closed') and (not self._closed): self.attempted_logins += 1 <IF_STMT> msg += ' Disconnecting.' self.respond('530 ' + msg) self.close_when_done() else: self.respond('530 ' + msg) self.log(""USER '%s' failed login."" % username) self.on_login_failed(username, password)",if self.attempted_logins >= self.max_login_attempts:
"def handle_disconnect(self): """"""Socket gets disconnected"""""" try: self.serial.rts = False self.serial.dtr = False finally: self.serial.apply_settings(self.serial_settings_backup) self.rfc2217 = None self.buffer_ser2net = bytearray() <IF_STMT> self.socket.close() self.socket = None if self.log is not None: self.log.warning('{}: Disconnected'.format(self.device))",if self.socket is not None:
"def select_invitation_id_for_network(invitations, networkid, status=None): invitationsfornetwork = [] for invitation in invitations: <IF_STMT> if status is None or invitation['Status'] == status: invitationsfornetwork.append(invitation['InvitationId']) return invitationsfornetwork",if invitation['NetworkSummary']['Id'] == networkid:
"def fit(self, refstring, subpipes): if not isinstance(subpipes, list): subpipes = [subpipes] for subpipe in subpipes: <IF_STMT> substring = subpipe.transform(None) else: substring = subpipe self._scores.append((self.base_aligner.fit_transform(refstring, substring, get_score=True), subpipe)) return self","if hasattr(subpipe, 'transform'):"
"def build_priorities(self, _iter, priorities): while _iter is not None: <IF_STMT> self.build_priorities(self.files_treestore.iter_children(_iter), priorities) elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep): priorities[self.files_treestore.get_value(_iter, 3)] = self.files_treestore.get_value(_iter, 0) _iter = self.files_treestore.iter_next(_iter) return priorities",if self.files_treestore.iter_has_child(_iter):
"def __init__(self, fileobj, info): pages = [] complete = False while not complete: page = OggPage(fileobj) <IF_STMT> pages.append(page) complete = page.complete or len(page.packets) > 1 packets = OggPage.to_packets(pages) if not packets: raise error('Missing metadata packet') data = packets[0][7:] super(OggTheoraCommentDict, self).__init__(data, framing=False) self._padding = len(data) - self._size",if page.serial == info.serial:
"def _run_interface(self, runtime): mel_icas = [] for item in self.inputs.mel_icas_in: <IF_STMT> mel_icas.append(item) if len(mel_icas) == 0: raise Exception('%s did not find any hand_labels_noise.txt files in the following directories: %s' % (self.__class__.__name__, mel_icas)) return runtime","if os.path.exists(os.path.join(item, 'hand_labels_noise.txt')):"
"def download_file(url, file): try: xlog.info('download %s to %s', url, file) req = opener.open(url) CHUNK = 16 * 1024 with open(file, 'wb') as fp: while True: chunk = req.read(CHUNK) <IF_STMT> break fp.write(chunk) return True except: xlog.info('download %s to %s fail', url, file) return False",if not chunk:
"def check_sales_order_on_hold_or_close(self, ref_fieldname): for d in self.get('items'): if d.get(ref_fieldname): status = frappe.db.get_value('Sales Order', d.get(ref_fieldname), 'status') <IF_STMT> frappe.throw(_('Sales Order {0} is {1}').format(d.get(ref_fieldname), status))","if status in ('Closed', 'On Hold'):"
"def iterstack(sources, missing, trim, pad): its = [iter(t) for t in sources] hdrs = [next(it) for it in its] hdr = hdrs[0] n = len(hdr) yield tuple(hdr) for it in its: for row in it: outrow = tuple(row) <IF_STMT> outrow = outrow[:n] if pad and len(outrow) < n: outrow += (missing,) * (n - len(outrow)) yield outrow",if trim:
"def __call__(self, response_headers): rates = get_rates_from_response_headers(response_headers) if rates: time.sleep(self._get_wait_time(rates.short_usage, rates.long_usage, get_seconds_until_next_quarter(), get_seconds_until_next_day())) <IF_STMT> self.short_limit = rates.short_limit self.long_limit = rates.long_limit",if not self.force_limits:
"def main(self): self.model.clear() self.callman.unregister_all() active_handle = self.get_active('Place') if active_handle: active = self.dbstate.db.get_place_from_handle(active_handle) <IF_STMT> self.display_place(active, None, [active_handle], DateRange()) else: self.set_has_data(False) else: self.set_has_data(False)",if active:
"def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full if node is None: node = '' <IF_STMT> ifrom = '' if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True",if ifrom is None:
"def append_to(project_url, destination): url = ('%smagic/%s' % (project_url, destination)).replace('\\', '/') response = urllib2.urlopen(url) if response.getcode() == 200: with open(destination, 'r') as dest: lines = ''.join(dest.readlines()) content = response.read() <IF_STMT> print_out('IGNORED', destination) return with open(destination, 'a') as dest: dest.write(content) print_out('APPEND', destination)",if content in lines:
"def close(self, invalidate=False): self.session.transaction = self._parent if self._parent is None: for connection, transaction, autoclose in set(self._connections.values()): if invalidate: connection.invalidate() if autoclose: connection.close() else: transaction.close() self._state = CLOSED self.session.dispatch.after_transaction_end(self.session, self) if self._parent is None: <IF_STMT> self.session.begin() self.session = None self._connections = None",if not self.session.autocommit:
"def list_local_packages(path): """"""Lists all local packages below a path that could be installed."""""" rv = [] try: for filename in os.listdir(path): <IF_STMT> rv.append('@' + filename) except OSError: pass return rv","if os.path.isfile(os.path.join(path, filename, 'setup.py')):"
"def walk_dir(templates, dest, filter=None): l = [] for root, folders, files in os.walk(templates): for filename in files: <IF_STMT> continue relative_dir = '.{0}'.format(os.path.split(os.path.join(root, filename).replace(templates, ''))[0]) l.append((os.path.join(root, filename), os.path.join(dest, relative_dir))) return l",if filename.endswith('.pyc') or (filter and filename not in filter):
"def selectItemHelper(self, item, scroll): if self.frame.lockout: return w = self.treeWidget if item and item.IsOk(): self.frame.lockout = True try: w.SelectItem(item) <IF_STMT> w.ScrollTo(item) finally: self.frame.lockout = False",if scroll:
"def validate_external(self, field): if hasattr(self, 'forum'): <IF_STMT> raise ValidationError(_('You cannot convert a forum that contains topics into an external link.'))",if self.forum.topics.count() > 0:
"def add_help(self): """"""Attach help functions for each of the parsed token handlers."""""" for attrname, func in list(shell.BQLShell.__dict__.items()): <IF_STMT> continue command_name = attrname[3:] setattr(self.__class__, 'help_{}'.format(command_name.lower()), lambda _, fun=func: print(textwrap.dedent(fun.__doc__).strip(), file=self.outfile))",if attrname[:3] != 'on_':
"def createFields(self): yield UInt8(self, 'tag') yield UInt24(self, 'size', 'Content size') yield UInt24(self, 'timestamp', 'Timestamp in millisecond') yield NullBytes(self, 'reserved', 4) size = self['size'].value if size: <IF_STMT> for field in self.parser(self, size): yield field else: yield RawBytes(self, 'content', size)",if self.parser:
"def migrate_model_field_data(Model): queryset = Model.objects.all().order_by('pk') for batch_pks in queryset_in_batches(queryset): instances = [] batch = Model.objects.filter(pk__in=batch_pks) for instance in batch: <IF_STMT> instance.content_json = parse_to_editorjs(instance.content_json) instances.append(instance) Model.objects.bulk_update(instances, ['content_json'])",if instance.content_json:
"def _add_account(cfg, which): username = self._get_account(cfg) if username and (username == only or only is None) and (cfg.auth_type == 'password'): <IF_STMT> accounts[username][which] = cfg.host else: fingerprint = self._user_fingerprint(username) accounts[username] = {which: cfg.host, 'username': username, 'policy': self._get_policy(fingerprint)} if accounts[username]['policy'] is None: del accounts[username]['policy']",if username in accounts:
"def update_msg_tags(self, msg_idx_pos, msg_info): tags = set(self.get_tags(msg_info=msg_info)) with self._lock: for tid in set(self.TAGS.keys()) - tags: self.TAGS[tid] -= set([msg_idx_pos]) for tid in tags: <IF_STMT> self.TAGS[tid] = set() self.TAGS[tid].add(msg_idx_pos)",if tid not in self.TAGS:
"def close(self, reason='protocol closed, reason unspecified'): if self.connection: self.logger.debug(reason, self.connection.session()) self.connection.close() self.connection = None self.peer.stats['down'] = self.peer.stats.get('down', 0) + 1 try: <IF_STMT> self.peer.reactor.processes.down(self.peer.neighbor, reason) except ProcessError: self.logger.debug('could not send notification of neighbor close to API', self.connection.session())",if self.peer.neighbor.api['neighbor-changes']:
"def check_objects_exist(self, compare_id, raise_exc=True): for uid in convert_compare_id_to_list(compare_id): <IF_STMT> if raise_exc: raise FactCompareException('{} not found in database'.format(uid)) return True return False",if not self.existence_quick_check(uid):
"def on_double_click(self, event): path = self.get_selected_path() if path: kind = self.get_selected_kind() <IF_STMT> self.focus_into(path) else: self.log_frame.load_log(path) return 'break'",if kind == 'dir':
"def resolve_cloudtrail_payload(self, payload): sources = self.data.get('sources', []) events = [] for e in self.data.get('events'): <IF_STMT> events.append(e) event_info = CloudWatchEvents.get(e) if event_info is None: continue else: event_info = e events.append(e['event']) sources.append(event_info['source']) payload['detail'] = {'eventSource': list(set(sources)), 'eventName': events}","if not isinstance(e, dict):"
"def load_graph_session_from_ckpt(ckpt_path, sess_config, print_op=False): """"""load graph and session from checkpoint file"""""" graph = tf.Graph() with graph.as_default(): sess = get_session(sess_config) with sess.as_default(): saver = tf.train.import_meta_graph('{}.meta'.format(ckpt_path)) saver.restore(sess, ckpt_path) <IF_STMT> print_ops(graph, prefix='load_graph_session_from_ckpt') return (graph, sess)",if print_op:
"def _parseConfigFile(self, iniPath, createConfig=True): parser = SafeConfigParserUnicode(strict=False) if not os.path.isfile(iniPath): if createConfig: open(iniPath, 'w').close() else: return parser.readfp(codecs.open(iniPath, 'r', 'utf_8_sig')) for section, options in list(self._iniStructure.items()): if parser.has_section(section): for option in options: <IF_STMT> self._config[option] = parser.get(section, option)","if parser.has_option(section, option):"
def parse(self): while 1: l = self.f.readline() if not l: return l = l.strip() if l.startswith('['): self.parse_uuid(l) <IF_STMT> self.parse_interface(l) elif l.startswith('coclass'): self.parse_coclass(l),elif l.startswith('interface') or l.startswith('dispinterface'):
"def encode(self): if not isinstance(self.expr, m2_expr.ExprInt): return False if not test_set_sf(self.parent, self.expr.size): return False value = int(self.expr) if value < 1 << self.l: self.parent.shift.value = 0 else: if value & 4095: return False value >>= 12 <IF_STMT> return False self.parent.shift.value = 1 self.value = value return True",if value >= 1 << self.l:
"def _func_runner(self): _locals.thread = self try: self._final_result = self.target(*self.args, **self.kwargs) self._final_exc = None except BaseException as e: self._final_result = None self._final_exc = e <IF_STMT> log.warning('Unexpected exception in cancelled async thread', exc_info=True) finally: self._request.set_result(None)","if not isinstance(e, errors.CancelledError):"
"def _set_dialect(self, value): if value is None: self._dialect = mac_eui48 el<IF_STMT> self._dialect = value else: raise TypeError('custom dialects should subclass mac_eui48!')","if hasattr(value, 'word_size') and hasattr(value, 'word_fmt'):"
"def fixup_namespace_packages(path_item, parent=None): """"""Ensure that previously-declared namespace packages include path_item"""""" imp.acquire_lock() try: for package in _namespace_packages.get(parent, ()): subpath = _handle_ns(package, path_item) <IF_STMT> fixup_namespace_packages(subpath, package) finally: imp.release_lock()",if subpath:
"def close_file_descriptor(self, fd): """"""Attempt to close a file descriptor."""""" start_timer = time.time() error = '' while True: try: fd.close() break except OSError as e: log.debug('Error closing file descriptor: %s' % str(e)) time.sleep(0.5) current_wait_time = time.time() - start_timer <IF_STMT> error = 'Error closing file descriptor: %s' % str(e) break return error",if current_wait_time >= 600:
"def p_constant(self, p): """"""constant : PP_NUMBER"""""" value = p[1].rstrip('LlUu') try: if value[:2] == '0x': value = int(value[2:], 16) <IF_STMT> value = int(value, 8) else: value = int(value) except ValueError: value = value.rstrip('eEfF') try: value = float(value) except ValueError: value = 0 p[0] = ConstantExpressionNode(value)",elif value[0] == '0':
"def set_add_delete_state(self): """"""Toggle the state for the help list buttons based on list entries."""""" if self.helplist.size() < 1: self.button_helplist_edit.state(('disabled',)) self.button_helplist_remove.state(('disabled',)) el<IF_STMT> self.button_helplist_edit.state(('!disabled',)) self.button_helplist_remove.state(('!disabled',)) else: self.button_helplist_edit.state(('disabled',)) self.button_helplist_remove.state(('disabled',))",if self.helplist.curselection():
def _erase_status(): CodeintelHandler.status_lock.acquire() try: <IF_STMT> view.erase_status(lid) CodeintelHandler.status_msg[lid][1] = None if lid in CodeintelHandler.status_lineno: del CodeintelHandler.status_lineno[lid] finally: CodeintelHandler.status_lock.release(),"if msg == CodeintelHandler.status_msg.get(lid, [None, None, 0])[1]:"
"def PARSE_TWO_PARAMS(x, y): """"""used to convert different possible x/y params to a tuple"""""" if y is not None: return (x, y) el<IF_STMT> return (x[0], x[1]) else: if isinstance(x, UNIVERSAL_STRING): x = x.strip() if ',' in x: return [int(w.strip()) for w in x.split(',')] return (x, x)","if isinstance(x, (list, tuple)):"
"def cancel_spot_fleet_requests(self, spot_fleet_request_ids, terminate_instances): spot_requests = [] for spot_fleet_request_id in spot_fleet_request_ids: spot_fleet = self.spot_fleet_requests[spot_fleet_request_id] <IF_STMT> spot_fleet.target_capacity = 0 spot_fleet.terminate_instances() spot_requests.append(spot_fleet) del self.spot_fleet_requests[spot_fleet_request_id] return spot_requests",if terminate_instances:
"def pop(self, key, default=_MISSING): with self._lock: try: ret = super(LRI, self).pop(key) except KeyError: <IF_STMT> raise ret = default else: self._remove_from_ll(key) return ret",if default is _MISSING:
"def _remove_optional_none_type_hints(self, type_hints, defaults): for arg in defaults: if defaults[arg] is None and arg in type_hints: type_ = type_hints[arg] if self._is_union(type_): types = type_.__args__ <IF_STMT> type_hints[arg] = types[0]",if len(types) == 2 and types[1] is type(None):
"def reader(self, myself): ok = True line = '' while True: line = sys.stdin.readline().strip() if ok: if not line: ok = False continue <IF_STMT> break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)",elif not line:
"def checkout_branch(self, branch): if branch in self.remote_branches: sickrage.app.log.debug('Branch checkout: ' + self._find_installed_version() + '->' + branch) <IF_STMT> return False if sickrage.app.config.git_reset: self.reset() self.fetch() __, __, exit_status = self._git_cmd(self._git_path, 'checkout -f ' + branch) if exit_status == 0: return True return False",if not self.install_requirements(self.current_branch):
"def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): if ok_node(nodes[i]): node = nodes[i] if isinstance(node, ast.Starred): <IF_STMT> return node.value else: return None else: return nodes[i] return None",if ok_node(node.value):
"def restart(): """"""Restart application."""""" popen_list = [sys.executable, app.MY_FULLNAME] if not app.NO_RESTART: popen_list += app.MY_ARGS <IF_STMT> popen_list += ['--nolaunch'] logger.info('Restarting Medusa with {options}', options=popen_list) logging.shutdown() print(popen_list) subprocess.Popen(popen_list, cwd=os.getcwd())",if '--nolaunch' not in popen_list:
"def StopBackgroundWorkload(self): """"""Stop the background workoad."""""" for workload in background_workload.BACKGROUND_WORKLOADS: if workload.IsEnabled(self): <IF_STMT> raise NotImplementedError() workload.Stop(self)",if self.OS_TYPE in workload.EXCLUDED_OS_TYPES:
"def __init__(self, token): self._convert_to_ascii = False self._find = None if token.search is None: return flags = 0 self._match_this_many = 1 if token.options: if 'g' in token.options: self._match_this_many = 0 <IF_STMT> flags |= re.IGNORECASE if 'a' in token.options: self._convert_to_ascii = True self._find = re.compile(token.search, flags | re.DOTALL) self._replace = _CleverReplace(token.replace)",if 'i' in token.options:
"def _draw_nodes(self, cr, bounding, highlight_items): highlight_nodes = [] for element in highlight_items: <IF_STMT> highlight_nodes.append(element.src) highlight_nodes.append(element.dst) else: highlight_nodes.append(element) for node in self.nodes: if bounding is None or node._intersects(bounding): node._draw(cr, highlight=node in highlight_nodes, bounding=bounding)","if isinstance(element, Edge):"
"def _removeCachedRFInfo(self, cache_key, path, removeChildPaths): log.debug('_removeCachedRFInfo: cache_key %r, path %r', cache_key, path) if self._cachedFiles.has_key(cache_key): cache = self._cachedFiles[cache_key] if cache.has_key(path): del cache[path] <IF_STMT> from remotefilelib import addslash dirPath = addslash(path) for keypath in cache.keys(): if keypath.startswith(dirPath): del cache[keypath]",if removeChildPaths:
"def write_row(xf, worksheet, row, row_idx, max_column): attrs = {'r': '%d' % row_idx, 'spans': '1:%d' % max_column} dims = worksheet.row_dimensions if row_idx in dims: row_dimension = dims[row_idx] attrs.update(dict(row_dimension)) with xf.element('row', attrs): for col, cell in row: <IF_STMT> continue el = write_cell(xf, worksheet, cell, cell.has_style)",if cell._value is None and (not cell.has_style) and (not cell._comment):
"def reset_feature_range(data, column_max_value, column_min_value, scale_column_idx): _data = copy.deepcopy(data) for i in scale_column_idx: value = _data.features[i] <IF_STMT> _data.features[i] = column_max_value[i] elif value < column_min_value[i]: _data.features[i] = column_min_value[i] return _data",if value > column_max_value[i]:
"def test_listing_all_frameworks_and_check_frameworks_by_order(self): """"""List all frameworks and check if frameworks appear by order"""""" result = subprocess.check_output(self.command_as_list([UMAKE, '--list'])) previous_framework = None for element in result.split(b'\n'): if element.startswith(b'\t'): current_framework = element[:element.find(b':')] <IF_STMT> self.assertTrue(previous_framework < current_framework) previous_framework = current_framework else: previous_framework = None",if previous_framework:
"def merge(module_name, tree1, tree2): for child in tree2.node: <IF_STMT> replaceFunction(tree1, child.name, child) elif isinstance(child, ast.Assign): replaceAssign(tree1, child.nodes[0].name, child) elif isinstance(child, ast.Class): replaceClassMethods(tree1, child.name, child) else: raise TranslationError('Do not know how to merge %s' % child, child, module_name) return tree1","if isinstance(child, ast.Function):"
def _filter_supported_drivers(): global supported_drivers with Env() as gdalenv: ogrdrv_names = gdalenv.drivers().keys() supported_drivers_copy = supported_drivers.copy() for drv in supported_drivers.keys(): <IF_STMT> del supported_drivers_copy[drv] supported_drivers = supported_drivers_copy,if drv not in ogrdrv_names:
"def serialize(self, cassette_dict): for interaction in cassette_dict['interactions']: response = interaction['response'] headers = response['headers'] <IF_STMT> rg, size, filename = self._parse_headers(headers) content = response['body']['string'] if rg[0] == 0 and rg[1] + 1 == size: with open(join(self.directory, filename), 'wb') as f: f.write(content) del response['body']['string'] return self.base_serializer.serialize(cassette_dict)",if 'Content-Range' in headers and 'Content-Disposition' in headers:
"def verify_software_token(self, access_token, user_code): for user_pool in self.user_pools.values(): if access_token in user_pool.access_tokens: _, username = user_pool.access_tokens[access_token] user = user_pool.users.get(username) <IF_STMT> raise UserNotFoundError(username) user.token_verified = True return {'Status': 'SUCCESS'} else: raise NotAuthorizedError(access_token)",if not user:
"def __fixdict(self, dict): for key in dict.keys(): <IF_STMT> tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = (getattr(self, key), end) elif key[:4] == 'end_': tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = (start, getattr(self, key))",if key[:6] == 'start_':
"def generate_playlist(sourcefile): """"""Generate a playlist from video titles in sourcefile"""""" if '--description' in sourcefile or '-d' in sourcefile: description_generator(sourcefile) return expanded_sourcefile = path.expanduser(sourcefile) if not check_sourcefile(expanded_sourcefile): g.message = util.F('mkp empty') % expanded_sourcefile else: queries = read_sourcefile(expanded_sourcefile) g.message = util.F('mkp parsed') % (len(queries), sourcefile) <IF_STMT> create_playlist(queries) g.message = util.F('pl help') g.content = content.playlists_display()",if queries:
"def flush(self): for record in self._unique_ordered_records: record.message = self._format_string.format(message=record.message, count=self._message_to_count[record.message]) <IF_STMT> dispatch = record.dispatcher.call_handlers else: dispatch = dispatch_record dispatch(record) self.clear()",if record.dispatcher is not None:
"def __init__(self, name, contents): self.name = name self.all_entries = [] self.attr = [] self.child = [] self.seq_child = [] for entry in contents: clean_entry = entry.rstrip('*') self.all_entries.append(clean_entry) <IF_STMT> self.seq_child.append(clean_entry) elif entry.endswith('*'): self.child.append(clean_entry) else: self.attr.append(entry)",if entry.endswith('**'):
"def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: if node is None: continue if type(node) is CodeNode and BaseNode.test_empty_node(node.node): continue <IF_STMT> continue return False return True",if BaseNode.test_empty_node(node):
"def test_deprecated_format_string(obj, fmt_str, should_raise_warning): if sys.version_info[0] == 3 and sys.version_info[1] >= 4: <IF_STMT> self.assertRaises(TypeError, format, obj, fmt_str) else: try: format(obj, fmt_str) except TypeError: self.fail('object.__format__ raised TypeError unexpectedly') else: with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always', DeprecationWarning) format(obj, fmt_str)",if should_raise_warning:
"def get_queryset(self): if self.queryset is not None: return self.queryset._clone() elif self.model is not None: qs = self.model._default_manager <IF_STMT> access_class = access_registry[self.model] if access_class.select_related: qs = qs.select_related(*access_class.select_related) if access_class.prefetch_related: qs = qs.prefetch_related(*access_class.prefetch_related) return qs else: return super(GenericAPIView, self).get_queryset()",if self.model in access_registry:
"def ping_task(): try: <IF_STMT> if peer not in self._protocol.routing_table.get_peers(): self._protocol.add_peer(peer) return await self._protocol.get_rpc_peer(peer).ping() except (asyncio.TimeoutError, RemoteException): pass",if self._protocol.peer_manager.peer_is_good(peer):
"def _validate_usage(schema_argument, variable_used): if isinstance(schema_argument.gql_type, GraphQLNonNull) and (not isinstance(variable_used.type, NonNullTypeNode)): has_variable_a_df = not isinstance(variable_used.default_value, (NullValueNode, type(None))) has_argument_a_df = schema_argument.default_value is not None <IF_STMT> return False return _validate_type_compatibility(variable_used.type, schema_argument.gql_type.gql_type) return _validate_type_compatibility(variable_used.type, schema_argument.gql_type)",if not has_variable_a_df and (not has_argument_a_df):
"def _add_kid(key, x): if x is None: kids[key] = None elif type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): <IF_STMT> if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x","if hasattr(x, '__iter__'):"
"def postCreate(node, menu): with node.scriptNode().context(): <IF_STMT> cropFormat = node['in']['format'].getValue() else: cropFormat = GafferImage.FormatPlug.getDefaultFormat(node.scriptNode().context()) node['area'].setValue(cropFormat.getDisplayWindow())",if node['in'].getInput():
"def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: <IF_STMT> stroke = stroke.replace(system.NUMBER_KEY, '') m = _IMPLICIT_NUMBER_RX.search(stroke) if m is not None: start = m.start(2) return stroke[:start] + '-' + stroke[start:] if '-' in letters: if stroke.endswith('-'): stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace('-', '') return stroke",if system.NUMBER_KEY in letters:
"def vim_k(self): """"""Cursor up N lines."""""" if self.is_text_wrapper(self.w): for z in range(self.n1 * self.n): <IF_STMT> self.do('previous-line-extend-selection') else: self.do('previous-line') self.done() elif self.in_tree(self.w): self.do('goto-prev-visible') self.done() else: self.quit()",if self.state == 'visual':
"def parseTime(timeStr): regex = re.compile(constants.PARSE_TIME_REGEX) parts = regex.match(timeStr) if not parts: return parts = parts.groupdict() time_params = {} for name, param in parts.items(): if param: <IF_STMT> time_params['microseconds'] = int(param) * 1000 else: time_params[name] = int(param) return datetime.timedelta(**time_params).total_seconds()",if name == 'miliseconds':
"def update(self, other=None, **kwargs): if other is not None: if hasattr(other, 'items'): other = other.items() for key, value in other: <IF_STMT> raise TensorforceError.value(name='NestedDict.update', argument='key', value=key, condition='specified twice') self[key] = value for key, value in kwargs.items(): self[key] = value",if key in kwargs:
"def to_string(self, ostream=None, verbose=None, precedence=0): """"""Print this expression"""""" if ostream is None: ostream = sys.stdout _verbose = pyomo.core.base.expr_common.TO_STRING_VERBOSE if verbose is None else verbose ostream.write(self.cname() + '( ') first = True for arg in self._args: if first: first = False <IF_STMT> ostream.write(' , ') else: ostream.write(', ') arg.to_string(ostream=ostream, precedence=self._precedence(), verbose=verbose) ostream.write(' )')",elif _verbose:
"def apply_gradient_for_batch(inputs, labels, weights, loss): with tf.GradientTape() as tape: outputs = self.model(inputs, training=True) if isinstance(outputs, tf.Tensor): outputs = [outputs] <IF_STMT> outputs = [outputs[i] for i in self._loss_outputs] batch_loss = loss(outputs, labels, weights) if variables is None: vars = self.model.trainable_variables else: vars = variables grads = tape.gradient(batch_loss, vars) self._tf_optimizer.apply_gradients(zip(grads, vars)) self._global_step.assign_add(1) return batch_loss",if self._loss_outputs is not None:
"def check_all(self, strict=False): """"""run sanity check on all keys, issue warning if out of sync"""""" same = self._is_same_value for path, (orig, expected) in iteritems(self._state): <IF_STMT> continue msg = 'another library has patched resource: %r' % path if strict: raise RuntimeError(msg) else: warn(msg, PasslibRuntimeWarning)","if same(self._get_path(path), expected):"
"def setup_child(self, child): child.parent = self if self.document: child.document = self.document if child.source is None: child.source = self.document.current_source <IF_STMT> child.line = self.document.current_line",if child.line is None:
"def shift_expr(self, nodelist): node = self.com_node(nodelist[0]) for i in range(2, len(nodelist), 2): right = self.com_node(nodelist[i]) if nodelist[i - 1][0] == token.LEFTSHIFT: node = LeftShift([node, right], lineno=nodelist[1][2]) <IF_STMT> node = RightShift([node, right], lineno=nodelist[1][2]) else: raise ValueError('unexpected token: %s' % nodelist[i - 1][0]) return node",elif nodelist[i - 1][0] == token.RIGHTSHIFT:
"def styleRow(self, row, selected): if row != -1: <IF_STMT> self.getRowFormatter().addStyleName(row, 'midpanel-SelectedRow') else: self.getRowFormatter().removeStyleName(row, 'midpanel-SelectedRow')",if selected:
"def __call__(self, img): img = self.topil(img) ops = random.choices(self.augment_list, k=self.n) for op, minval, maxval in ops: <IF_STMT> continue val = float(self.m) / 30 * float(maxval - minval) + minval img = op(img, val) return img","if random.random() > random.uniform(0.2, 0.8):"
"def run(self, **inputs): if self.inputs.copy_inputs: self.inputs.subjects_dir = os.getcwd() <IF_STMT> inputs['subjects_dir'] = self.inputs.subjects_dir copy2subjdir(self, self.inputs.surface, 'surf') copy2subjdir(self, self.inputs.curvfile1, 'surf') copy2subjdir(self, self.inputs.curvfile2, 'surf') return super(CurvatureStats, self).run(**inputs)",if 'subjects_dir' in inputs:
"def get_func_name(obj): if inspect.ismethod(obj): match = RE_BOUND_METHOD.match(repr(obj)) if match: cls = match.group('class') <IF_STMT> return match.group('name') return '%s.%s' % (match.group('class'), match.group('name')) return None",if not cls:
"def local_path_export(at_start=True, env_cmd=None): """"""Retrieve paths to local install, also including environment paths if env_cmd included."""""" paths = [get_bcbio_bin()] if env_cmd: env_path = os.path.dirname(get_program_python(env_cmd)) <IF_STMT> paths.insert(0, env_path) if at_start: return 'export PATH=%s:""$PATH"" && ' % ':'.join(paths) else: return 'export PATH=""$PATH"":%s && ' % ':'.join(paths)",if env_path not in paths:
"def copystat(src, dst): """"""Copy all stat info (mode bits, atime, mtime, flags) from src to dst"""""" st = os.stat(src) mode = stat.S_IMODE(st.st_mode) if hasattr(os, 'utime'): os.utime(dst, (st.st_atime, st.st_mtime)) if hasattr(os, 'chmod'): os.chmod(dst, mode) if hasattr(os, 'chflags') and hasattr(st, 'st_flags'): try: os.chflags(dst, st.st_flags) except OSError as why: <IF_STMT> raise","if not hasattr(errno, 'EOPNOTSUPP') or why.errno != errno.EOPNOTSUPP:"
"def _asdict(self, *, to_string: bool=False) -> dict: res = [] for key in self._keys: value = getattr(self, key) if isinstance(value, Struct): value = value._asdict(to_string=to_string) <IF_STMT> value = str(value) res.append((key, value)) return dict(res)",elif to_string:
"def _SI(size, K=1024, i='i'): """"""Return size as SI string."""""" if 1 < K <= size: f = float(size) for si in iter('KMGPTE'): f /= K <IF_STMT> return ' or %.1f %s%sB' % (f, si, i) return ''",if f < K:
"def _flatten(*args): arglist = [] for arg in args: <IF_STMT> if arg.vhdl_code is not None: arglist.append(arg.vhdl_code) continue else: arg = arg.subs if id(arg) in _userCodeMap['vhdl']: arglist.append(_userCodeMap['vhdl'][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist","if isinstance(arg, _Block):"
"def new_token(self): data = '{{""username"": ""{}"", ""password"": ""{}""}}'.format(self.username, self.password) try: resp = requests.post('https://api.zoomeye.org/user/login', data=data) <IF_STMT> content = resp.json() self.token = content['access_token'] self.headers = {'Authorization': 'JWT %s' % self.token} return True except Exception as ex: logger.error(str(ex)) return False",if resp.status_code != 401 and 'access_token' in resp.json():
"def finalize_computation(self, transaction: SignedTransactionAPI, computation: ComputationAPI) -> ComputationAPI: computation = super().finalize_computation(transaction, computation) touched_accounts = collect_touched_accounts(computation) for account in touched_accounts: should_delete = self.vm_state.account_exists(account) and self.vm_state.account_is_empty(account) <IF_STMT> self.vm_state.logger.debug2('CLEARING EMPTY ACCOUNT: %s', encode_hex(account)) self.vm_state.delete_account(account) return computation",if should_delete:
"def send_messages(self, text, user_ids): broken_items = [] if not user_ids: self.logger.info('User must be at least one.') return broken_items self.logger.info('Going to send %d messages.' % len(user_ids)) for user in tqdm(user_ids): <IF_STMT> self.error_delay() broken_items = user_ids[user_ids.index(user):] break return broken_items","if not self.send_message(text, user):"
def editable_cpp_info(self): if self._layout_file: <IF_STMT> return EditableLayout(self._layout_file) else: raise ConanException('Layout file not found: %s' % self._layout_file),if os.path.isfile(self._layout_file):
"def to_python(self, value): if isinstance(value, list) and len(value) == 2 and isinstance(value[0], str): filename, payload = value try: payload = base64.b64decode(payload) except TypeError: pass else: <IF_STMT> self.storage.delete(filename) self.storage.save(filename, ContentFile(payload)) return filename return value",if self.storage.exists(filename):
"def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) <IF_STMT> self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning('Configuration file {} does not exist.'.format(value)) elif isinstance(value, type(None)): pass else: raise ValueError('Cannot interpret {}'.format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)","elif isinstance(value, types.ModuleType):"
"def __getitem__(self, item: str) -> Any: try: return self.data[item] except KeyError: for g in self.extended_groups(): try: r = g.data[item] return r except KeyError: continue r = self.defaults.data.get(item) <IF_STMT> return r raise",if r is not None:
"def _parse_arguments(self, handler_method): spec = DynamicArgumentParser().parse(self._argspec, self.longname) if not self._supports_kwargs: if spec.kwargs: raise DataError(""Too few '%s' method parameters for **kwargs support."" % self._run_keyword_method_name) <IF_STMT> raise DataError(""Too few '%s' method parameters for keyword-only arguments support."" % self._run_keyword_method_name) spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name) return spec",if spec.kwonlyargs:
"def test_orphans_match(self): """"""api handles last three chars match query"""""" response = self.client.get('%s?q=%s' % (self.api_link, self.user.username[-3:])) self.assertEqual(response.status_code, 200) response_json = response.json() self.assertIn('users', [p['id'] for p in response_json]) for provider in response_json: <IF_STMT> results = provider['results']['results'] self.assertEqual(len(results), 1) self.assertEqual(results[0]['id'], self.user.id)",if provider['id'] == 'users':
"def test_costs_1D_noisy_names(signal_bkps_1D_noisy, cost_name): signal, bkps = signal_bkps_1D_noisy cost = cost_factory(cost_name) cost.fit(signal) cost.fit(signal.flatten()) cost.error(0, 100) cost.error(100, signal.shape[0]) cost.error(10, 50) cost.sum_of_costs(bkps) with pytest.raises(NotEnoughPoints): <IF_STMT> cost.min_size = 4 cost.error(1, 2) else: cost.error(1, 2)",if cost_name == 'cosine':
"def _delete_access_key(self, params): sys.stdout.write('Deleting the IAM user access keys... ') list_access_keys = self.iam.get_paginator('list_access_keys') try: for response in list_access_keys.paginate(UserName=params.user_name): for access_key in response['AccessKeyMetadata']: self.iam.delete_access_key(UserName=params.user_name, AccessKeyId=access_key['AccessKeyId']) except ClientError as e: <IF_STMT> raise e sys.stdout.write('DONE\n')","if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':"
"def run_pending(self, now=None): """"""Runs the command if scheduled"""""" now = now or datetime.now() if self.is_enabled(): if self.last_run is None: self.last_run = now next_time = self.schedule(self.last_run).get_next() <IF_STMT> self.last_run = now return self.run() return -1",if next_time < now:
"def parse_row(cls, doc_row): row = {} for field_name, field in FIELD_MAP.items(): <IF_STMT> field_value = doc_row[field[1]] else: field_value = '' if len(field) >= 3 and callable(field[2]): field_value = field[2](field_value) row[field_name] = field_value return row",if len(doc_row) > field[1]:
"def list(self, items, columns=4, width=80): items = list(sorted(items)) colw = width // columns rows = (len(items) + columns - 1) // columns for row in range(rows): for col in range(columns): i = col * rows + row if i < len(items): self.output.write(items[i]) <IF_STMT> self.output.write(' ' + ' ' * (colw - 1 - len(items[i]))) self.output.write('\n')",if col < columns - 1:
"def _on_message(self, storage, data): if '_meta' in data and 'session_id' in data['_meta']: self.session_id = data['_meta']['session_id'] if is_blacklisted(data.get('url', '')): blacklist_error(data, self) return command = data['_command'] command = self._handlers.get(command, command) with data_store_context(): commands = Commands(data, self, storage) result = getattr(commands, command, lambda: None)() if result: result.setdefault('_command', data.get('_callback', command)) <IF_STMT> result['id'] = data['_meta']['id'] return result",if '_meta' in data and 'id' in data['_meta']:
"def get_model_params(problem_type: str, hyperparameters): penalty = hyperparameters.get('penalty', L2) handle_text = hyperparameters.get('handle_text', IGNORE) if problem_type == REGRESSION: <IF_STMT> model_class = Ridge elif penalty == L1: model_class = Lasso else: logger.warning('Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2'.format(penalty)) penalty = L2 model_class = Ridge else: model_class = LogisticRegression return (model_class, penalty, handle_text)",if penalty == L2:
"def get_queryset(self): if self.queryset is not None: return self.queryset._clone() elif self.model is not None: qs = self.model._default_manager if self.model in access_registry: access_class = access_registry[self.model] if access_class.select_related: qs = qs.select_related(*access_class.select_related) <IF_STMT> qs = qs.prefetch_related(*access_class.prefetch_related) return qs else: return super(GenericAPIView, self).get_queryset()",if access_class.prefetch_related:
"def map_package(shutit_pexpect_session, package, install_type): """"""If package mapping exists, then return it, else return package."""""" if package in PACKAGE_MAP.keys(): for itype in PACKAGE_MAP[package].keys(): if itype == install_type: ret = PACKAGE_MAP[package][install_type] if isinstance(ret, str): return ret <IF_STMT> ret(shutit_pexpect_session) return '' return package",if callable(ret):
"def find_missing_cache_files(self, modules: Dict[str, str], manager: build.BuildManager) -> Set[str]: ignore_errors = True missing = {} for id, path in modules.items(): meta = build.find_cache_meta(id, path, manager) <IF_STMT> missing[id] = path return set(missing.values())","if not build.validate_meta(meta, id, path, ignore_errors, manager):"
"def parse_percent_formats(data, tree): percent_formats = data.setdefault('percent_formats', {}) for elem in tree.findall('.//percentFormats/percentFormatLength'): type = elem.attrib.get('type') <IF_STMT> continue pattern = text_type(elem.findtext('percentFormat/pattern')) percent_formats[type] = numbers.parse_pattern(pattern)","if _should_skip_elem(elem, type, percent_formats):"
"def nan2none(l): for idx, val in enumerate(l): <IF_STMT> l[idx] = nan2none(l[idx]) elif isnum(val) and math.isnan(val): l[idx] = None return l","if isinstance(val, Sequence):"
"def process(self, message: Message, **kwargs: Any) -> None: for attribute in DENSE_FEATURIZABLE_ATTRIBUTES: <IF_STMT> message.set(SPACY_DOCS[attribute], self.doc_for_text(message.get(attribute)))",if message.get(attribute):
"def accessSlice(self, node): self.visit(node.value) node.obj = self.getObj(node.value) self.access = _access.INPUT lower, upper = (node.slice.lower, node.slice.upper) if lower: self.visit(lower) <IF_STMT> self.visit(upper) if isinstance(node.obj, intbv): if self.kind == _kind.DECLARATION: self.require(lower, 'Expected leftmost index') leftind = self.getVal(lower) if upper: rightind = self.getVal(upper) else: rightind = 0 node.obj = node.obj[leftind:rightind]",if upper:
"def forg(x, prec=3): if prec == 3: <IF_STMT> return '%9.3g' % x else: return '%9.3f' % x elif prec == 4: if abs(x) >= 10000.0 or abs(x) < 0.0001: return '%10.4g' % x else: return '%10.4f' % x else: raise ValueError('`prec` argument must be either 3 or 4, not {prec}'.format(prec=prec))",if abs(x) >= 10000.0 or abs(x) < 0.0001:
"def pseudo_raw_input(self, prompt): """"""copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout"""""" if self.use_rawinput: try: line = raw_input(prompt) except EOFError: line = 'EOF' else: self.stdout.write(prompt) self.stdout.flush() line = self.stdin.readline() <IF_STMT> line = 'EOF' elif line[-1] == '\n': line = line[:-1] return line",if not len(line):
"def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break if pos > 0 and dn[pos - 1] != '\\': break <IF_STMT> escaped = True for c in dn[pos - 2:0:-1]: if c == '\\': escaped = not escaped else: break if not escaped: break pos += 1 return pos",elif pos > 1 and dn[pos - 1] == '\\':
"def update_user(username): permission = UserAdminPermission(username) if permission.can(): update_request = request.get_json() <IF_STMT> logger.debug('Updating user password') model.user.change_password(get_authenticated_user(), update_request['password']) return jsonify({'username': get_authenticated_user().username, 'email': get_authenticated_user().email}) abort(403)",if 'password' in update_request:
"def pages(self): if hasattr(self, '_pages'): return self._pages doctop = 0 pp = self.pages_to_parse self._pages = [] for i, page in enumerate(PDFPage.create_pages(self.doc)): page_number = i + 1 <IF_STMT> continue p = Page(self, page, page_number=page_number, initial_doctop=doctop) self._pages.append(p) doctop += p.height return self._pages",if pp is not None and page_number not in pp:
"def image_size(img_data, pure_python=False): try: <IF_STMT> return imgsize.get_size(PeekableStringIO(img_data)) if Image is not None and (not pure_python): return Image.open(cStringIO.StringIO(img_data)).size except (ValueError, imgsize.UnknownSize): pass return None",if imgsize is not None:
"def email_csv_query(request, query_id): if request.is_ajax(): email = request.POST.get('email', None) <IF_STMT> execute_query.delay(query_id, email) return HttpResponse(content={'message': 'message was sent successfully'}) return HttpResponse(status=403)",if email:
"def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs['groups'] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs['groups'] if 'group_name' in group: mykwargs['source_security_group_name'] = group['group_name'] <IF_STMT> mykwargs['source_security_group_owner_id'] = group['user_id'] if 'group_id' in group: mykwargs['source_security_group_id'] = group['group_id'] groups_args_split.append(mykwargs) return groups_args_split",if 'user_id' in group:
"def get_subnet_groups(self, region: str, vpc: str): try: await self._cache_subnet_groups(region) return [subnet_group for subnet_group in self._subnet_groups_cache[region] <IF_STMT>] except Exception as e: print_exception(f'Failed to get RDS subnet groups: {e}') return []",if subnet_group['VpcId'] == vpc
def on_state_update(self) -> None: if self.road: self.lane_index = self.road.network.get_closest_lane_index(self.position) self.lane = self.road.network.get_lane(self.lane_index) <IF_STMT> self.history.appendleft(self.create_from(self)),if self.road.record_history:
"def delete_old_post_save(sender, instance, raw, created, update_fields, using, **kwargs): """"""Post_save on all models with file fields, deletes old files"""""" if raw or created: return for field_name, new_file in cache.fields_for_model_instance(instance): if update_fields is None or field_name in update_fields: old_file = cache.get_field_attr(instance, field_name) <IF_STMT> delete_file(instance, field_name, old_file, using) cache.make_cleanup_cache(instance)",if old_file != new_file:
"def i2h(self, pkt, x): if x is not None: if x < 0: warning('Fixed3_6: Internal value too negative: %d' % x) x = 0 <IF_STMT> warning('Fixed3_6: Internal value too positive: %d' % x) x = 999999999 x = x * 1e-06 return x",elif x > 999999999:
"def quick_main(self): if self.actions.pressed('cancel'): self.previs_timer.stop() return 'main' if self.actions.mousemove_stop: self.hovering_edge, _ = self.rfcontext.accel_nearest2D_edge(max_dist=options['action dist']) <IF_STMT> self.hovering_edge = None if self.hovering_edge and self.rfcontext.actions.pressed('quick insert'): return self.insert_edge_loop_strip()",if self.hovering_edge and (not self.hovering_edge.is_valid):
def check_status(self) -> None: join_requested = False while not join_requested: status_response = self._interface.communicate_status(check_stop_req=True) if status_response and status_response.run_should_stop: <IF_STMT> thread.interrupt_main() return join_requested = self._join_event.wait(self._polling_interval),if not wandb.agents.pyagent.is_running():
"def listed(output, pool): for line in output.splitlines(): name, mountpoint, refquota = line.split(b'\t') name = name[len(pool) + 1:] <IF_STMT> refquota = int(refquota.decode('ascii')) if refquota == 0: refquota = None yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)",if name:
"def defined_properties(cls, aliases=True, properties=True, rels=True): from .relationship_manager import RelationshipDefinition props = {} for baseclass in reversed(cls.__mro__): props.update(dict(((name, property) for name, property in vars(baseclass).items() <IF_STMT> or (properties and isinstance(property, Property) and (not isinstance(property, AliasProperty))) or (rels and isinstance(property, RelationshipDefinition))))) return props","if aliases and isinstance(property, AliasProperty)"
"def _mock_manager(self, *args, **kwargs): if kwargs and 'normalize' not in kwargs: device_params = kwargs['device_params'] device_handler = make_device_handler(device_params) session = SSHSession(device_handler) return Manager(session, device_handler) if args: if args[0].tag == 'request-pfe-execute': file_name = args[0].findtext('command').replace(' ', '_') return self._read_file(file_name + '.xml') <IF_STMT> file_name = args[0].text.replace(' ', '_') return self._read_file(file_name + '.xml')",elif args[0].tag == 'command':
"def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != 'OK': if time_now - self.last_check_time < 3: return el<IF_STMT> return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()",if time_now - self.last_check_time < 10:
def delete(self): if not self.force and (not self.exists()): return [] cmd = ['delete'] if self.filename: cmd.append('--filename=' + self.filename) else: if not self.resource: self.module.fail_json(msg='resource required to delete without filename') cmd.append(self.resource) if self.name: cmd.append(self.name) if self.label: cmd.append('--selector=' + self.label) <IF_STMT> cmd.append('--all') if self.force: cmd.append('--ignore-not-found') return self._execute(cmd),if self.all:
"def load(self): """"""load a custom filter"""""" try: <IF_STMT> parser = make_parser() parser.setContentHandler(FilterParser(self)) with open(self.file, 'r', encoding='utf8') as the_file: parser.parse(the_file) except (IOError, OSError): print('IO/OSError in _filterlist.py') except SAXParseException: print('Parser error')",if os.path.isfile(self.file):
"def exitFullscreen(self, container=None): """"""turns off fullscreen mode for the specified window"""""" if container is None or isinstance(container, UNIVERSAL_STRING): try: container = self.widgetManager.get(WIDGET_NAMES.SubWindow, container) except: container = self._getTopLevel() if container.isFullscreen: container.isFullscreen = False container.attributes('-fullscreen', False) <IF_STMT> container.unbind('<Escape>', container.escapeBindId) with PauseLogger(): self._doTitleBar() return True else: return False",if container.escapeBindId is not None:
"def __get__(self, instance: Any, owner: Type) -> Any: if instance is None: return self field = self.field instance_dict = instance.__dict__ to_python = self._to_python value = instance_dict[field] if self.lazy_coercion and to_python is not None: evaluated_fields: Set[str] evaluated_fields = instance.__evaluated_fields__ <IF_STMT> if value is not None or self.required: value = instance_dict[field] = to_python(value) evaluated_fields.add(field) return value",if field not in evaluated_fields:
def ip_list(_): ips = [] for ip in _.split(' '): if not ip: continue <IF_STMT> ips.append(IP.create(ip)) else: raise TypeError('ip %s is invalid' % ip) return ips,elif isip(ip):
"def _parse_fields(line, legacy=False): """"""Removes ' ' from fields line and returns fields as a list (columns)."""""" line = line.rstrip('\n') if legacy: fields = line.split(',') else: line = line.split('# Fields: ')[1] fields = line.split(', ') columns = [] for field in fields: <IF_STMT> raise BLAST7FormatError('Unrecognized field (%r). Supported fields: %r' % (field, set(column_converter.keys()))) columns.append(column_converter[field]) return columns",if field not in column_converter:
"def _resolve_plugin_path(path): if not os.path.isabs(path): p = os.path.normpath(os.path.join(sublime.packages_path(), 'User', path)) <IF_STMT> p = os.path.normpath(os.path.join(sublime.packages_path(), 'LaTeXTools', path)) return p return path",if not os.path.exists(p):
"def _deep_copy_dict(source, dest): for key, value in source.items(): <IF_STMT> dest[key] = {} TqApi._deep_copy_dict(value, dest[key]) else: dest[key] = value","if isinstance(value, Entity):"
"def encode(self): if not isinstance(self.expr, m2_expr.ExprInt): return False if not test_set_sf(self.parent, self.expr.size): return False value = int(self.expr) if value < 1 << self.l: self.parent.shift.value = 0 else: <IF_STMT> return False value >>= 12 if value >= 1 << self.l: return False self.parent.shift.value = 1 self.value = value return True",if value & 4095:
"def test_read_audio_properties(self): mediafile = self._mediafile_fixture('full') for key, value in self.audio_properties.items(): <IF_STMT> self.assertAlmostEqual(getattr(mediafile, key), value, delta=0.1) else: self.assertEqual(getattr(mediafile, key), value)","if isinstance(value, float):"
"def get_all_fix_names(fixer_pkg, remove_prefix=True): """"""Return a sorted list of all available fix names in the given package."""""" pkg = __import__(fixer_pkg, [], [], ['*']) fixer_dir = os.path.dirname(pkg.__file__) fix_names = [] for name in sorted(os.listdir(fixer_dir)): <IF_STMT> if remove_prefix: name = name[4:] fix_names.append(name[:-3]) return fix_names",if name.startswith('fix_') and name.endswith('.py'):
"def _get_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None): arg = None if len(args) > arg_no: arg = args[arg_no] elif arg_name in kws: arg = kws[arg_name] if arg is None: if default is not None: return default <IF_STMT> err_msg = ""{} requires '{}' argument"".format(f_name, arg_name) raise ValueError(err_msg) return arg",if err_msg is None:
"def get_satellite_list(self, daemon_type=''): res = {} for t in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver', 'broker']: <IF_STMT> continue satellite_list = [] res[t] = satellite_list daemon_name_attr = t + '_name' daemons = self.app.get_daemons(t) for dae in daemons: if hasattr(dae, daemon_name_attr): satellite_list.append(getattr(dae, daemon_name_attr)) return res",if daemon_type and daemon_type != t:
"def do_upload(file: Path, metadata: Metadata, repo_name=None): """"""Upload a file to an index server."""""" repo = get_repository(repo_name) upload_file(file, metadata, repo) if repo['is_warehouse']: domain = urlparse(repo['url']).netloc <IF_STMT> domain = domain[7:] log.info('Package is at https://%s/project/%s/', domain, metadata.name) else: log.info('Package is at %s/%s', repo['url'], metadata.name)",if domain.startswith('upload.'):
"def __next__(self): for res in self._execution_context: for item in res: for operator in self._local_aggregators: if isinstance(item, dict) and item: operator.aggregate(item['item']) <IF_STMT> operator.aggregate(item) if self._results is None: self._results = [] for operator in self._local_aggregators: self._results.append(operator.get_result()) if self._result_index < len(self._results): res = self._results[self._result_index] self._result_index += 1 return res raise StopIteration","elif isinstance(item, numbers.Number):"
"def __iter__(self): yield (pd.Timestamp.utcnow(), SESSION_START) while True: current_time = pd.Timestamp.utcnow() current_minute = current_time.floor('1 min') <IF_STMT> break if self._last_emit is None or current_minute > self._last_emit: log.debug('emitting minutely bar: {}'.format(current_minute)) self._last_emit = current_minute yield (current_minute, BAR) else: sleep(1) yield (current_minute, SESSION_END)",if self.end is not None and current_minute >= self.end:
"def _escape_attrib(text): try: <IF_STMT> text = text.replace('&', '&amp;') if '<' in text: text = text.replace('<', '&lt;') if '>' in text: text = text.replace('>', '&gt;') if '""' in text: text = text.replace('""', '&quot;') if '\n' in text: text = text.replace('\n', '&#10;') return text except (TypeError, AttributeError): _raise_serialization_error(text)",if '&' in text:
"def _read_row_from_packet(self, packet): row = [] for encoding, converter in self.converters: try: data = packet.read_length_coded_string() except IndexError: break if data is not None: if encoding is not None: data = data.decode(encoding) if DEBUG: print('DEBUG: DATA = ', data) <IF_STMT> data = converter(data) row.append(data) return tuple(row)",if converter is not None:
"def dumpMenuTree(self, aList, level=0, path=''): for z in aList: kind, val, val2 = z <IF_STMT> name = self.getName(val, val2) g.es_print('%s %s (%s) [%s]' % ('' * (level + 0), val, val2, path + '/' + name)) else: name = self.getName(kind.replace('@menu ', '')) g.es_print('%s %s... [%s]' % ('' * level, kind, path + '/' + name)) self.dumpMenuTree(val, level + 1, path=path + '/' + name)",if kind == '@item':
"def startElement(self, name, attrs, connection): if name == 'SecurityGroups': return self.security_groups elif name == 'ClassicLinkVPCSecurityGroups': return self.classic_link_vpc_security_groups elif name == 'BlockDeviceMappings': <IF_STMT> self.block_device_mappings = BDM() else: self.block_device_mappings = ResultSet([('member', BlockDeviceMapping)]) return self.block_device_mappings elif name == 'InstanceMonitoring': self.instance_monitoring = InstanceMonitoring(self) return self.instance_monitoring",if self.use_block_device_types:
"def __get_dev_and_disk(topology): rv = [] for values in topology.values(): values = values.copy() while values: value = values.pop() <IF_STMT> rv.append((value['path'].replace('/dev/', ''), value['disk'])) values += value.get('children') or [] return rv",if value['type'] == 'DISK':
"def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = (key.fileobj, key.data) if mask & selectors.EVENT_READ and reader is not None: <IF_STMT> self.remove_reader(fileobj) else: self._add_callback(reader) if mask & selectors.EVENT_WRITE and writer is not None: if writer._cancelled: self.remove_writer(fileobj) else: self._add_callback(writer)",if reader._cancelled:
"def colourLabels(self): if self.showAttr and self.hasAttr: self.canvas.itemconfigure(self.attrId, fill=self.fgColour) try: <IF_STMT> self.label.config(background=self.bgColour, fg=self.fgColour) else: self.label.config(background=self.bgHColour, fg=self.fgHColour) except: pass",if not self.selected:
"def validate_char_lengths(self): for field in self._meta.get_fields(): <IF_STMT> if isinstance(getattr(self, field.name), basestring) and len(getattr(self, field.name)) > field.max_length: raise Exception('Role %s value exceeeds max length of %s.' % (field.name, field.max_length))",if not field.is_relation and field.get_internal_type() == 'CharField':
"def _render_lang_List(self, element): with self.buffer.foldable_lines(): self.buffer.write('[', style=self.styles.bracket) item_count = len(element.items) <IF_STMT> with self.buffer.indent(): for idx, item in enumerate(element.items): self._render(item) if idx < item_count - 1: self.buffer.write(',') self.buffer.mark_line_break() if element.trimmed: self.buffer.write('...') self.buffer.write(']', style=self.styles.bracket)",if item_count:
"def do_dialog(): """"""Post dialog and handle user interaction until quit"""""" my_dlg = Dlg.GetNewDialog(ID_MAIN, -1) while 1: n = Dlg.ModalDialog(None) if n == ITEM_LOOKUP_BUTTON: tp, h, rect = my_dlg.GetDialogItem(ITEM_LOOKUP_ENTRY) txt = Dlg.GetDialogItemText(h) tp, h, rect = my_dlg.GetDialogItem(ITEM_RESULT) Dlg.SetDialogItemText(h, dnslookup(txt)) <IF_STMT> break",elif n == ITEM_QUIT_BUTTON:
"def _extract_more_comments(tree): """"""Return a list of MoreComments objects removed from tree."""""" more_comments = [] queue = [(None, x) for x in tree] while len(queue) > 0: parent, comm = queue.pop(0) <IF_STMT> heappush(more_comments, comm) if parent: parent.replies.remove(comm) else: tree.remove(comm) else: for item in comm.replies: queue.append((comm, item)) return more_comments","if isinstance(comm, MoreComments):"
"def run(self): while True: self.finished.wait(self.interval) <IF_STMT> return try: self.function(*self.args, **self.kwargs) except Exception: if self.bus: self.bus.log('Error in perpetual timer thread function %r.' % self.function, level=40, traceback=True) raise",if self.finished.isSet():
"def emit_classattribs(self, typebld): if hasattr(self, '_clrclassattribs'): for attrib_info in self._clrclassattribs: <IF_STMT> ci = clr.GetClrType(attrib_info).GetConstructor(()) cab = CustomAttributeBuilder(ci, ()) elif isinstance(attrib_info, CustomAttributeDecorator): cab = attrib_info.GetBuilder() else: make_decorator = attrib_info() cab = make_decorator.GetBuilder() typebld.SetCustomAttribute(cab)","if isinstance(attrib_info, type):"
"def wrapper(fn): if debug_run_test_calls: ret = str(fn(*args, *kwargs)) print('TEST: %s()' % fn.__name__) if args: print('  arg:', args) <IF_STMT> print('  kwa:', kwargs) print('  ret:', ret) return fn",if kwargs:
"def _prune(self): with self.lock: entries = self._list_dir() <IF_STMT> now = time.time() try: for i, fpath in enumerate(entries): remove = False f = LockedFile(fpath, 'rb') exp = pickle.load(f.file) f.close() remove = exp <= now or i % 3 == 0 if remove: self._del_file(fpath) except Exception: pass",if len(entries) > self._threshold:
"def delete_if_forked(ghrequest): FORKED = False query = '/user/repos' r = utils.query_request(query) for repo in r.json(): <IF_STMT> if ghrequest.target_repo_fullname in repo['description']: FORKED = True url = f""/repos/{repo['full_name']}"" utils.query_request(url, method='DELETE') return FORKED",if repo['description']:
"def _feed_data_to_buffered_proto(proto, data): data_len = len(data) while data_len: buf = proto.get_buffer(data_len) buf_len = len(buf) <IF_STMT> raise RuntimeError('get_buffer() returned an empty buffer') if buf_len >= data_len: buf[:data_len] = data proto.buffer_updated(data_len) return else: buf[:buf_len] = data[:buf_len] proto.buffer_updated(buf_len) data = data[buf_len:] data_len = len(data)",if not buf_len:
"def _plugin_get_requirements(self, requirements_iter): plugin_requirements = {'platform': [], 'python': [], 'network': [], 'native': []} for requirement in requirements_iter: key = requirement[0] values = requirement[1] if isinstance(values, str) or isinstance(values, bool): values = [values] <IF_STMT> plugin_requirements[key].extend(values) else: warning('{}={}: No supported requirement'.format(key, values)) return plugin_requirements",if key in plugin_requirements:
"def setCurrentModelIndexes(self, indexes): self._indexes = [] self._index = None for i in indexes: if i.isValid(): <IF_STMT> i = i.sibling(i.row(), self._column) self._indexes.append(i) self.updateItems() self.updateSelectedItem()",if i.column() != self._column:
"def _publish(self, data): retry = True while True: try: <IF_STMT> self._redis_connect() return self.redis.publish(self.channel, pickle.dumps(data)) except redis.exceptions.ConnectionError: if retry: logger.error('Cannot publish to redis... retrying') retry = False else: logger.error('Cannot publish to redis... giving up') break",if not retry:
"def write_pad_and_flush(self, data, pad=' '): if self.encryptor and (data or self.encode_buffer): <IF_STMT> remainder = len(self.encode_buffer) + len(data) remainder %= self.encode_batches padding = self.encode_batches - remainder data += pad * padding self.write(data) self.flush()",if self.encode_batches:
def dump_metrics(self): metrics = self._registry.dump_metrics() for metric in metrics.itervalues(): <IF_STMT> if 'min' in metric: metric['min'] = 0.0 if 'max' in metric: metric['max'] = 0.0 return metrics,if metric.get('count') == 0:
"def demo(): d = StatusProgressDialog('A Demo', 'Doing something...') import win32api for i in range(100): if i == 50: d.SetText('Getting there...') <IF_STMT> d.SetText('Nearly done...') win32api.Sleep(20) d.Tick() d.Close()",if i == 90:
"def get_file_contents(app_name: str, app_version: str, file_path: str): full_path = f'{app_name}/{app_version}/{file_path}' success, contents = await MinioApi.get_file(app_name, app_version, file_path) if success: return contents el<IF_STMT> raise DoesNotExistException('read', 'file', full_path) else: raise InvalidInputException('read', 'file', full_path, errors={'error': contents})",if contents is None:
"def _sashMark(self, event): self._sashIndex = -1 try: self._sashIndex, which = self.paneframe.identify(event.x, event.y) <IF_STMT> self._sashx = [self.paneframe.sash_coord(i)[0] for i in range(len(self._lists) - 1)] self._sashdx = self._sashx[self._sashIndex] - event.x self._sashDrag(event) else: self._sashIndex = -1 except: return return 'break'",if which == 'sash':
def emptyTree(self): for child in self: childObj = child.getObject() del childObj[NameObject('/Parent')] <IF_STMT> del childObj[NameObject('/Next')] if NameObject('/Prev') in childObj: del childObj[NameObject('/Prev')] if NameObject('/Count') in self: del self[NameObject('/Count')] if NameObject('/First') in self: del self[NameObject('/First')] if NameObject('/Last') in self: del self[NameObject('/Last')],if NameObject('/Next') in childObj:
"def contractIfNotCurrent(c, p, leaveOpen): if p == leaveOpen or not p.isAncestorOf(leaveOpen): p.contract() for child in p.children(): <IF_STMT> contractIfNotCurrent(c, child, leaveOpen) else: for p2 in child.self_and_subtree(): p2.contract()",if child != leaveOpen and child.isAncestorOf(leaveOpen):
"def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = (end, end + size) if cat_dim == -1: part = gaussian[..., beg:end] elif cat_dim == -2: part = gaussian[..., beg:end, :] <IF_STMT> part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)",elif cat_dim == 1:
"def _remove_timeout(self, key): if key in self.waiting: request, callback, timeout_handle = self.waiting[key] <IF_STMT> self.io_loop.remove_timeout(timeout_handle) del self.waiting[key]",if timeout_handle is not None:
"def gyro(self, mapper, *pyr): for i in (0, 1, 2): axis = self.axes[i] <IF_STMT> mapper.gamepad.axisEvent(axis, AxisAction.clamp_axis(axis, pyr[i] * self.speed[i] * -10)) mapper.syn_list.add(mapper.gamepad)",if axis in Axes or type(axis) == int:
"def check_enums_ATLAS_MACHTYPE(lines): for i, mach_type in enumerate(ATLAS_MACHTYPE): got = lines.pop(0).strip() expect = ""{0} = '{1}'"".format(i, mach_type) <IF_STMT> raise RuntimeError('ATLAS_MACHTYPE mismatch at position ' + str(i) + ': got >>' + got + '<<, expected >>' + expect + '<<')",if got != expect:
"def readArgs(self, node): res = {} for c in self.getChildrenOf(node): val = c.getAttribute('val') if val in self.modules: res[str(c.nodeName)] = self.modules[val] <IF_STMT> res[str(c.nodeName)] = self.mothers[val] elif val != '': res[str(c.nodeName)] = eval(val) return res",elif val in self.mothers:
"def submit_events(self, events): headers = {'Content-Type': 'application/json'} event_chunk_size = self.event_chunk_size for chunk in chunks(events, event_chunk_size): payload = {'apiKey': self.api_key, 'events': {'api': chunk}, 'uuid': get_uuid(), 'internalHostname': get_hostname()} params = {} <IF_STMT> params['api_key'] = self.api_key url = '%s/intake?%s' % (self.api_host, urlencode(params)) self.submit_http(url, json.dumps(payload), headers)",if self.api_key:
"def rewrite_urls_mygpo(self): rewritten_urls = self.mygpo_client.get_rewritten_urls() changed = False for rewritten_url in rewritten_urls: <IF_STMT> continue for channel in self.channels: if channel.url == rewritten_url.old_url: logger.info('Updating URL of %s to %s', channel, rewritten_url.new_url) channel.url = rewritten_url.new_url channel.save() changed = True break if changed: util.idle_add(self.update_episode_list_model)",if not rewritten_url.new_url:
"def validate_hostname(hostname): if hostname is None or len(hostname) == 0: return (False, 'Empty hostname or domain is not allowed') fields = hostname.split('.') for field in fields: if not field: return (False, 'Empty hostname or domain is not allowed') <IF_STMT> return (False, ""Hostname or domain should not start or end with '-'"") machinename = fields[0] if len(machinename) > 64 or not machinename[0].isalpha(): return (False, 'Hostname should start with alpha char and <= 64 chars') return (True, None)",if field[0] == '-' or field[-1] == '-':
"def apply_to(cls, lexer): lexer.setFont(Font().load()) for name, font in cls.__dict__.items(): <IF_STMT> continue if hasattr(lexer, name): style_num = getattr(lexer, name) lexer.setColor(QColor(font.color), style_num) lexer.setEolFill(True, style_num) lexer.setPaper(QColor(font.paper), style_num) lexer.setFont(font.load(), style_num)","if not isinstance(font, Font):"
"def dr_relation(self, C, trans, nullable): state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: if p.lr_index < p.len - 1: a = p.prod[p.lr_index + 1] <IF_STMT> if a not in terms: terms.append(a) if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append('$end') return terms",if a in self.grammar.Terminals:
"def process_module(name, module, parent): if parent: modules[parent]['items'].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) <IF_STMT> module['.group'] = parent for k, v in list(module.items()): if k.startswith('on_click'): process_onclick(k, v, name) del module[k] if isinstance(v, ModuleDefinition): module['items'] = [] return module",if get_module_type(name) == 'py3status':
"def GetQualifiedWsdlName(type): with _lazyLock: wsdlNSAndName = _wsdlNameMap.get(type) <IF_STMT> return wsdlNSAndName elif issubclass(type, list): ns = GetWsdlNamespace(type.Item._version) return (ns, 'ArrayOf' + Capitalize(type.Item._wsdlName)) else: ns = GetWsdlNamespace(type._version) return (ns, type._wsdlName)",if wsdlNSAndName:
"def assert_tensors_equal(sess, t1, t2, n): """"""Compute tensors `n` times and ensure that they are equal."""""" for _ in range(n): v1, v2 = sess.run([t1, t2]) <IF_STMT> return False if not np.all(v1 == v2): return False return True",if v1.shape != v2.shape:
"def _lxml_default_loader(href, parse, encoding=None, parser=None): if parse == 'xml': data = etree.parse(href, parser).getroot() else: if '://' in href: f = urlopen(href) else: f = open(href, 'rb') data = f.read() f.close() <IF_STMT> encoding = 'utf-8' data = data.decode(encoding) return data",if not encoding:
"def range_f(begin, end, step): seq = [] while True: <IF_STMT> break if step < 0 and begin < end: break seq.append(begin) begin = begin + step return seq",if step > 0 and begin > end:
"def _get_seccomp_whitelist(self): whitelist = [False] * MAX_SYSCALL_NUMBER index = _SYSCALL_INDICIES[NATIVE_ABI] for i in range(SYSCALL_COUNT): <IF_STMT> continue handler = self._security.get(i, DISALLOW) for call in translator[i][index]: if call is None: continue if isinstance(handler, int): whitelist[call] = handler == ALLOW return whitelist","if i in (sys_exit, sys_exit_group):"
"def add_custom_versions(versions): """"""create custom versions strings"""""" versions_dict = {} for tech, version in versions.items(): if '-' in version: version = version.split('-')[0] <IF_STMT> version = version[1:] versions_dict[tech + '_numeric'] = version.split('+')[0] versions_dict[tech + '_short'] = '{}.{}'.format(*version.split('.')) return versions_dict",if version.startswith('v'):
"def detab(self, text): """"""Remove a tab from the front of each line of the given text."""""" newtext = [] lines = text.split('\n') for line in lines: <IF_STMT> newtext.append(line[self.tab_length:]) elif not line.strip(): newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",if line.startswith(' ' * self.tab_length):
def ignore_module(module): result = False for check in ignore_these: if '/*' in check: <IF_STMT> result = True elif os.getcwd() + '/' + check + '.py' == module: result = True if result: print_warning('Ignoring module: ' + module) return result,if check[:-1] in module:
def load_previous_values(self): ReportOptions.load_previous_values(self) for optname in self.options_dict: menu_option = self.menu.get_option_by_name(optname) <IF_STMT> menu_option.set_value(self.options_dict[optname]),if menu_option:
"def dequeue(self): with self.db(commit=True) as curs: curs.execute('select id, data from task where queue = ? order by priority desc, id limit 1', (self.name,)) result = curs.fetchone() <IF_STMT> tid, data = result curs.execute('delete from task where id = ?', (tid,)) if curs.rowcount == 1: return to_bytes(data)",if result is not None:
"def _collect_sublayers_attr(self, attr): if attr not in ['trainable_weights', 'nontrainable_weights']: raise ValueError(""Only support to collect some certain attributes of nested layers,e.g. 'trainable_weights', 'nontrainable_weights', but got {}"".format(attr)) if self._layers is None: return [] nested = [] for layer in self._layers: value = getattr(layer, attr) <IF_STMT> nested.extend(value) return nested",if value is not None:
"def DeleteTab(self, tab): tab_renderer = self.tabs[tab] was_selected = tab_renderer.GetSelected() self.tabs.remove(tab_renderer) if tab_renderer: del tab_renderer if was_selected and self.GetTabsCount() > 0: <IF_STMT> self.tabs[self.GetTabsCount() - 1].SetSelected(True) else: self.tabs[tab].SetSelected(True) self.AdjustTabsSize() self.Refresh()",if tab > self.GetTabsCount() - 1:
"def _show_warnings(self): if self._warnings_handled: return self._warnings_handled = True if self._result and (self._result.has_next or not self._result.warning_count): return ws = self._get_db().show_warnings() if ws is None: return for w in ws: msg = w[-1] <IF_STMT> if isinstance(msg, unicode): msg = msg.encode('utf-8', 'replace') warnings.warn(err.Warning(*w[1:3]), stacklevel=4)",if PY2:
"def fetch(): retval = {} content = retrieve_content(__url__) if __check__ not in content: content = retrieve_content(__backup__) if __check__ in content: for line in content.split('\n'): line = line.strip() <IF_STMT> continue retval[line] = (__info__, __reference__) return retval",if not line or line.startswith('#') or '.' not in line:
"def findUserByAttr(self, identifier, attr_type, attr_data): for uid in self.users_info: attrs = self.users_info[uid] for attr in attrs: <IF_STMT> return defer.succeed(uid) uid = self.nextId() self.db.insertTestData([User(uid=uid, identifier=identifier)]) self.db.insertTestData([UserInfo(uid=uid, attr_type=attr_type, attr_data=attr_data)]) return defer.succeed(uid)",if attr_type == attr['attr_type'] and attr_data == attr['attr_data']:
"def order_note_added_event(*, order: Order, user: UserType, message: str) -> OrderEvent: kwargs = {} if user is not None and (not user.is_anonymous): <IF_STMT> account_events.customer_added_to_note_order_event(user=user, order=order, message=message) kwargs['user'] = user return OrderEvent.objects.create(order=order, type=OrderEvents.NOTE_ADDED, parameters={'message': message}, **kwargs)",if order.user is not None and order.user.pk == user.pk:
"def __str__(self): if self.team: <IF_STMT> return '(%s, %s, Q%d, %d and %d) %s' % (self.team, self.data['yrdln'], self.time.qtr, self.down, self.yards_togo, self.desc) else: return '(%s, %s, Q%d) %s' % (self.team, self.data['yrdln'], self.time.qtr, self.desc) return self.desc",if self.down != 0:
"def write(self, stream): self.write1(stream) i = 0 n = 0 for name, offset, value, bsize in self.variables: stream.write(self.body[i:offset]) <IF_STMT> write_uint(stream, value) elif bsize == 8: write_ulong(stream, value) else: raise NotImplementedError() n += offset - i + bsize i = offset + bsize stream.write(self.body[i:]) n += len(self.body) - i assert n == len(self.body)",if bsize == 4:
"def __setattr__(self, attr, val): if hasattr(self, attr): old = getattr(self, attr) <IF_STMT> if isinstance(val, Setting): raise ValueError('Attempting to reassign setting %s with %s' % (old, val)) log.warn('Setting attr %s via __setattr__ instead of set()!', attr) return old.set(val) log.debug('Setting {%s => %s}' % (attr, val)) return object.__setattr__(self, attr, val)","if isinstance(old, Setting):"
"def setup_release_cwd_hook(prompter, history, completer, bindings, **kw): if ON_WINDOWS and (not ON_CYGWIN) and (not ON_MSYS): prompter.prompt = _cwd_release_wrapper(prompter.prompt) <IF_STMT> completer.completer.complete = _cwd_restore_wrapper(completer.completer.complete)",if completer.completer:
"def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): <IF_STMT> if key in org_dict: if not isinstance(org_dict[key], dict): raise ValueError('Mismatch between org_dict and upd_dict at node {}'.format(key)) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value","if isinstance(value, dict):"
"def check_sum(self, x, gpu=False): total = 0 for i in range(5): t = numpy.array([i], dtype=numpy.int32) <IF_STMT> t = cuda.to_gpu(t) loss = self.link(chainer.Variable(x), chainer.Variable(t)).data self.assertEqual(loss.dtype, self.dtype) self.assertEqual(loss.shape, ()) total += numpy.exp(-cuda.to_cpu(loss)) self.assertAlmostEqual(1.0, float(total), **self.check_sum_options)",if gpu:
"def find_node_by_link(node_group, to_node, inp): for link in node_group.links: if link.to_node == to_node and link.to_socket == inp: <IF_STMT> return find_node_by_link(node_group, link.from_node, link.from_node.inputs[0]) return link.from_node",if link.from_node.bl_idname == 'NodeReroute':
def _gen_opnds(ii): for op in ii.parsed_operands: <IF_STMT> continue if op.visibility == 'SUPPRESSED': continue if op.name == 'BCAST': continue yield op,"if op.lookupfn_name in ['MASK1', 'MASKNOT0']:"
"def contains_trained_model(self): if not hasattr(self, '_contains_trained_model'): for f in self._files: <IF_STMT> self._contains_trained_model = True self._model_name = f return self._contains_trained_model self._contains_trained_model = False return self._contains_trained_model else: return self._contains_trained_model",if '.pt' in f:
"def _call(self, name, *args, **kwargs): data = self._get_data(name, *args, **kwargs) is_ascii = self._encoding == 'ascii' body = json.dumps(data, ensure_ascii=is_ascii).encode(self._encoding) resp = await self._http.post(self._url, data=body) if self._full_response: return resp else: content = resp.json() if resp.is_error: <IF_STMT> resp.raise_for_status() return self.loads(content)",if 'error' not in content:
"def get_classif_name(classifier_config, usepytorch): if not usepytorch: modelname = 'sklearn-LogReg' else: nhid = classifier_config['nhid'] optim = 'adam' if 'optim' not in classifier_config else classifier_config['optim'] bs = 64 <IF_STMT> else classifier_config['batch_size'] modelname = 'pytorch-MLP-nhid%s-%s-bs%s' % (nhid, optim, bs) return modelname",if 'batch_size' not in classifier_config
"def on_fill(self, order: Order, exchange: 'Exchange', trade: 'Trade'): if trade.order_id in self._executed and trade not in self._trades: self._trades[trade.order_id] = self._trades.get(trade.order_id, []) self._trades[trade.order_id] += [trade] if order.is_complete(): next_order = order.complete(exchange) <IF_STMT> self.submit(next_order)",if next_order:
"def _create_examples(cls, lines, set_type): examples = [] for i, line in enumerate(lines): <IF_STMT> continue segments = line.strip().split('\t') idx, text_a, text_b, label = segments examples.append(Example(guid='%s-%s' % (set_type, idx), text_a=text_a, text_b=text_b, label=label)) return examples",if i == 0:
def split_path_info(path): path = path.strip('/') clean = [] for segment in path.split('/'): if not segment or segment == '.': continue elif segment == '..': <IF_STMT> del clean[-1] else: clean.append(segment) return tuple(clean),if clean:
"def _mock_manager(self, *args, **kwargs): if kwargs and 'normalize' not in kwargs: device_params = kwargs['device_params'] device_handler = make_device_handler(device_params) session = SSHSession(device_handler) return Manager(session, device_handler) if args: <IF_STMT> file_name = args[0].findtext('command').replace(' ', '_') return self._read_file(file_name + '.xml') elif args[0].tag == 'command': file_name = args[0].text.replace(' ', '_') return self._read_file(file_name + '.xml')",if args[0].tag == 'request-pfe-execute':
"def update_loan_status(self, cancel=0): if cancel: loan_status = frappe.get_value('Loan', self.loan, 'status') <IF_STMT> frappe.db.set_value('Loan', self.loan, 'status', 'Loan Closure Requested') else: pledged_qty = 0 current_pledges = get_pledged_security_qty(self.loan) for security, qty in iteritems(current_pledges): pledged_qty += qty if not pledged_qty: frappe.db.set_value('Loan', self.loan, 'status', 'Closed')",if loan_status == 'Closed':
"def _wrapped_view(request, *args, **kwargs): if flag_name.startswith('!'): active = not flag_is_active(request, flag_name[1:]) else: active = flag_is_active(request, flag_name) if not active: response_to_redirect_to = get_response_to_redirect(redirect_to, *args, **kwargs) <IF_STMT> return response_to_redirect_to else: raise Http404 return view(request, *args, **kwargs)",if response_to_redirect_to:
"def process_stroke_filter(stroke, min_distance=1.0, max_distance=2.0): """"""filter stroke to pts that are at least min_distance apart"""""" nstroke = stroke[:1] for p in stroke[1:]: v = p - nstroke[-1] l = v.length <IF_STMT> continue d = v / l while l > 0: q = nstroke[-1] + d * min(l, max_distance) nstroke.append(q) l -= max_distance return nstroke",if l < min_distance:
"def _fix_break_node(self, node: Node): end_node = self._find_end_loop(node, [], 0) <IF_STMT> end_node = self._find_end_loop(node, [], -1) if not end_node: raise ParsingError('Break in no-loop context {}'.format(node.function)) for son in node.sons: son.remove_father(node) node.set_sons([end_node]) end_node.add_father(node)",if not end_node:
"def _Append(cls, session, word, mail_ids, compact=True): super(GlobalPostingList, cls)._Append(session, word, mail_ids, compact=compact) with GLOBAL_GPL_LOCK: global GLOBAL_GPL sig = cls.WordSig(word, session.config) <IF_STMT> GLOBAL_GPL = {} if sig not in GLOBAL_GPL: GLOBAL_GPL[sig] = set() for mail_id in mail_ids: GLOBAL_GPL[sig].add(mail_id)",if GLOBAL_GPL is None:
"def __saveComment(self): """"""Saves the new or selected comment"""""" if self.__btnSave.text() == SAVE_NEW: self.__addComment(self.__textSubject.text(), self.__textMessage.toPlainText()) self.refreshComments() el<IF_STMT> comment = self.__treeSubjects.currentItem().getInstance() comment.setSubject(str(self.__textSubject.text())) comment.setMessage(str(self.__textMessage.toPlainText())) self.__treeSubjects.currentItem().getInstance().save() self.refreshComments()",if self.__treeSubjects.currentItem():
"def verify_random_objects(): resources = [Node, Registration, QuickFilesNode] for resource in resources: for i in range(1, 10): random_resource = _get_random_object(resource) <IF_STMT> _verify_contributor_perms(random_resource)",if random_resource:
"def apply_gradient_modifiers(self): for layer_name, views in self.gradient_modifiers.items(): for view_name, gradient_mods in views.items(): for gm in gradient_mods: gm.rnd.set_seed(self.rnd.generate_seed()) <IF_STMT> gm(self.handler, self.buffer[layer_name].parameters[view_name], self.buffer[layer_name].gradients[view_name]) else: gm(self.handler, self.buffer[layer_name].gradients[view_name])","if isinstance(gm, GradientModifier):"
"def _split_auth_string(auth_string): """"""split a digest auth string into individual key=value strings"""""" prev = None for item in auth_string.split(','): try: <IF_STMT> prev = '%s,%s' % (prev, item) continue except AttributeError: if prev == None: prev = item continue else: raise StopIteration yield prev.strip() prev = item yield prev.strip() raise StopIteration","if prev.count('""') == 1:"
"def checkUnchangedIvars(obj, d, exceptions=None): if not exceptions: exceptions = [] ok = True for key in d: <IF_STMT> if getattr(obj, key) != d.get(key): g.trace('changed ivar: %s old: %s new: %s' % (key, repr(d.get(key)), repr(getattr(obj, key)))) ok = False return ok",if key not in exceptions:
def checkChildren(item): for c in item.children(): _id = c.data(Outline.ID.value) <IF_STMT> c.getUniqueID() checkChildren(c),if not _id or _id == '0':
"def main(): if len(sys.argv) > 1: g = globals().copy() r = g['test_' + sys.argv[1]]() <IF_STMT> for func_and_args in r: func, args = (func_and_args[0], func_and_args[1:]) func(*args) else: run_all()",if r is not None:
"def _create_entities(parsed_entities: Dict[Text, Union[Text, List[Text]]], sidx: int, eidx: int) -> List[Dict[Text, Any]]: entities = [] for k, vs in parsed_entities.items(): <IF_STMT> vs = [vs] for value in vs: entities.append({'entity': k, 'start': sidx, 'end': eidx, 'value': value}) return entities","if not isinstance(vs, list):"
"def _group_stacks(stacks: Stacks) -> List[dict]: stacks_by_client: dict = {} for stack in stacks: client = stack.client <IF_STMT> stacks_by_client[client] = {'Client': client, 'Stacks': []} stacks_by_client[client]['Stacks'].append(stack) return [stacks_by_client[r] for r in stacks_by_client]",if client not in stacks_by_client:
"def append(self, labels): if isinstance(labels, list): for label in labels: if not label in self.__menuLabels: self.__menuLabels.append(label) self.__enabledLabels.append(label) el<IF_STMT> self.__menuLabels.append(labels) self.__enabledLabels.append(labels)",if not labels in self.__menuLabels:
"def _json_to_flat_metrics(self, prefix, data): for key, value in data.items(): <IF_STMT> for k, v in self._json_to_flat_metrics('%s.%s' % (prefix, key), value): yield (k, v) else: try: int(value) except ValueError: value = None finally: yield ('%s.%s' % (prefix, key), value)","if isinstance(value, dict):"
"def _rename(src, dst): src = to_unicode(src, sys.getfilesystemencoding()) dst = to_unicode(dst, sys.getfilesystemencoding()) if _rename_atomic(src, dst): return True retry = 0 rv = False while not rv and retry < 100: rv = _MoveFileEx(src, dst, _MOVEFILE_REPLACE_EXISTING | _MOVEFILE_WRITE_THROUGH) <IF_STMT> time.sleep(0.001) retry += 1 return rv",if not rv:
"def expect_stream_start(self): if isinstance(self.event, StreamStartEvent): <IF_STMT> self.encoding = self.event.encoding self.write_stream_start() self.state = self.expect_first_document_start else: raise EmitterError('expected StreamStartEvent, but got %s' % self.event)","if self.event.encoding and (not getattr(self.stream, 'encoding', None)):"
"def _doWait(self): doit = True while doit: self.setMeta('SignalInfo', None) self.setMeta('PendingSignal', None) event = self.platformWait() self.running = False self.platformProcessEvent(event) doit = self.shouldRunAgain() <IF_STMT> self._doRun()",if doit:
"def get_source(self, environment, template): if self._sep in template: prefix, name = template.split(self._sep, 1) <IF_STMT> raise TemplateNotFound(template) return self._mapping[prefix].get_source(environment, name) return self._default.get_source(environment, template)",if prefix not in self._mapping:
def find_child_processes_that_send_spans(pants_result_stderr): child_processes = set() for line in pants_result_stderr.split('\n'): <IF_STMT> i = line.rindex(':') child_process_pid = line[i + 1:] child_processes.add(int(child_process_pid)) return child_processes,if 'Sending spans to Zipkin server from pid:' in line:
"def list_dependencies_modules(self, *modules): """"""[UNIT]... show the dependency tree"" """""" found_all = True units = [] for module in modules: matched = self.match_units([module]) if not matched: logg.error(""no such service '%s'"", module) found_all = False continue for unit in matched: <IF_STMT> units += [unit] return self.list_dependencies_units(units)",if unit not in units:
"def getCommitFromFile(short=True): global _gitdir branch = getBranchFromFile() commit = None if _gitdir and branch: <IF_STMT> commitFile = os.path.join(_gitdir, 'HEAD') else: commitFile = os.path.join(_gitdir, 'refs', 'heads', branch) if os.path.isfile(commitFile): with open(commitFile, 'r', encoding='utf-8') as f: commit = f.readline().strip() if short and commit: return commit[:8] else: return commit",if branch == 'HEAD':
"def _node_for(pvector_like, i): if 0 <= i < pvector_like._count: <IF_STMT> return pvector_like._tail node = pvector_like._root for level in range(pvector_like._shift, 0, -SHIFT): node = node[i >> level & BIT_MASK] return node raise IndexError('Index out of range: %s' % (i,))",if i >= pvector_like._tail_offset:
def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args['user'] = mysql_user if mysql_pwd: args['passwd'] = mysql_pwd <IF_STMT> args['host'] = mysql_host if mysql_port: args['port'] = mysql_port if mysql_socket: args['unix_socket'] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception('Cannot interface with MySQL server: %s' % e),if mysql_host:
"def flatten(self, d, parent_key='', sep='.'): items = [] for k, v in d.items(): new_key = parent_key + sep + k if parent_key else k <IF_STMT> items.extend(self.flatten(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items)","if isinstance(v, MutableMapping):"
"def get_item(type_, preference): items = {} for item in playlist.findall('./info/%s/item' % type_): lang, label = (xpath_text(item, 'lg', default=None), xpath_text(item, 'label', default=None)) <IF_STMT> items[lang] = label.strip() for p in preference: if items.get(p): return items[p]",if lang and label:
"def test_lxml(): try: from lxml.etree import LXML_VERSION, __version__ <IF_STMT> return (True, __version__) else: return (False, __version__) except ImportError: return (None, None)","if LXML_VERSION >= (2, 1, 4, 0):"
"def send(self, data, flags=0, timeout=timeout_default): if timeout is timeout_default: timeout = self.timeout try: return self._sock.send(data, flags) except error as ex: <IF_STMT> raise sys.exc_clear() self._wait(self._write_event) try: return self._sock.send(data, flags) except error as ex2: if ex2.args[0] == EWOULDBLOCK: return 0 raise",if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0:
def blob_from_lang(self): self.acquire_lock() try: <IF_STMT> try: self._load_buf_data_once() except NotFoundInDatabase: self.release_lock() try: self.scan() finally: self.acquire_lock() self._load_buf_data_once(True) return self._blob_from_lang_cache finally: self.release_lock(),if self._blob_from_lang_cache is None:
"def processElem(elem, keyList): for k, v in elem.items(): prefix = '.'.join(keyList) <IF_STMT> k = makeSane(k) self.publish('%s.%s' % (prefix, k), v)",if k not in self.IGNORE_ELEMENTS and self.NUMVAL_MATCH.match(v):
"def __conform__(self, interface, registry=None, default=None): for providedInterface in self.provided: if providedInterface.isOrExtends(interface): return self.load() <IF_STMT> return interface(self.load(), default) return default","if getAdapterFactory(providedInterface, interface, None) is not None:"
"def restrict(points): result = [] for p in points: <IF_STMT> result.append(p) else: loc, normal, index, distance = bvh.find_nearest(p) if loc is not None: result.append(tuple(loc)) return result","if point_inside_mesh(bvh, p):"
def __iter__(self): buffer = [b''] for chunk in self.stream(decode_content=True): if b'\n' in chunk: chunk = chunk.split(b'\n') yield (b''.join(buffer) + chunk[0] + b'\n') for x in chunk[1:-1]: yield (x + b'\n') <IF_STMT> buffer = [chunk[-1]] else: buffer = [] else: buffer.append(chunk) if buffer: yield b''.join(buffer),if chunk[-1]:
"def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) if sChild.declaration and sChild.docname == docname: sChild.declaration = None sChild.docname = None sChild.line = None if sChild.siblingAbove is not None: sChild.siblingAbove.siblingBelow = sChild.siblingBelow <IF_STMT> sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None",if sChild.siblingBelow is not None:
"def _get_current_weight(self, policy, fw): weights = policy.get_weights() if fw == 'torch': <IF_STMT> return weights['_hidden_layers.0._model.0.weight'][0][0] else: return weights['policy_model.action_0._model.0.weight'][0][0] key = 0 if fw in ['tf2', 'tfe'] else list(weights.keys())[0] return weights[key][0][0]",if '_hidden_layers.0._model.0.weight' in weights:
"def add_unit(self, name, value, aliases=tuple(), **modifiers): """"""Add unit to the registry."""""" if not isinstance(value, self.Quantity): value = self.Quantity(value, **modifiers) self._UNITS[name] = value for ndx, alias in enumerate(aliases): <IF_STMT> logger.warn('Alias cannot contain a space ' + alias) self._UNITS.add_alias(alias.strip(), name, not ndx)",if ' ' in alias:
"def keyPressEvent(self, event): """"""Add up and down arrow key events to built in functionality."""""" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) <IF_STMT> self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)",elif keyPressed == Constants.TAB_KEY and self.completerStrings:
"def _add_bookmark_breakpoint(self): """"""Add a bookmark or breakpoint to the current file in the editor."""""" editorWidget = self.ide.mainContainer.get_actual_editor() if editorWidget and editorWidget.hasFocus(): <IF_STMT> editorWidget._sidebarWidget.set_bookmark(editorWidget.textCursor().blockNumber()) elif self.ide.mainContainer.actualTab.navigator.operation == 2: editorWidget._sidebarWidget.set_breakpoint(editorWidget.textCursor().blockNumber())",if self.ide.mainContainer.actualTab.navigator.operation == 1:
"def list_generator(pages, num_results): result = [] page = list(next(pages)) result += page while True: if not pages.continuation_token: break if num_results is not None: <IF_STMT> break page = list(next(pages)) result += page return result",if num_results == len(result):
"def _print_handles(self, text, handle_list): for handle in handle_list: source, citation = self.get_source_or_citation(handle, False) _LOG.debug('\n\n\n') if source: _LOG.debug('---- %s -- source %s' % (text, source.get_title())) <IF_STMT> _LOG.debug('---- %s -- citation %s' % (text, citation.get_page())) else: _LOG.debug('---- %s -- handle %s' % (text, handle))",elif citation:
"def _parse_whois(self, txt): asn, desc = (None, b'') for l in txt.splitlines(): if not asn and l.startswith(b'origin:'): asn = l[7:].strip().decode('utf-8') if l.startswith(b'descr:'): <IF_STMT> desc += b'\\n' desc += l[6:].strip() if asn is not None and desc.strip(): desc = desc.strip().decode('utf-8') break return (asn, desc)",if desc:
"def build(opt): dpath = os.path.join(opt['datapath'], 'multiwoz_v20') version = '1.0' if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def _global_pool2d_shape_func(data_shape, height_axis, width_axis): out = output_tensor((data_shape.shape[0],), 'int64') for i in const_range(out.shape[0]): <IF_STMT> out[i] = int64(1) else: out[i] = data_shape[i] return out",if i == height_axis or i == width_axis:
"def post_mortem(t=None): <IF_STMT> t = sys.exc_info()[2] if t is None: raise ValueError('A valid traceback must be passed if no exception is being handled') p = Pdb() p.reset() p.interaction(None, t)",if t is None:
"def clear(self, purge=False, delete_dataset=True): self.deleted = True if self.dataset: <IF_STMT> self.dataset.deleted = True if purge: self.dataset.purged = True if purge and self.dataset.deleted: self.purged = True try: os.unlink(self.file_name) except Exception as e: log.error('Failed to purge associated file ({}) from disk: {}'.format(self.file_name, unicodify(e)))",if delete_dataset:
"def scan_resource_conf(self, conf): if 'properties' in conf: <IF_STMT> if str(conf['properties']['supportsHttpsTrafficOnly']).lower() == 'true': return CheckResult.PASSED else: return CheckResult.FAILED if 'apiVersion' in conf: year = int(conf['apiVersion'][0:4]) if year < 2019: return CheckResult.FAILED else: return CheckResult.PASSED return CheckResult.FAILED",if 'supportsHttpsTrafficOnly' in conf['properties']:
"def connect(self): while True: errno = self.sock.connect_ex(self.addr) if not errno: break elif errno == EINPROGRESS: break <IF_STMT> self.create_connection(self.failover_interval) return else: raise ValueError('Unexpected socket errno: %d' % errno) self.event_loop.watch_file(self.sock.fileno(), self.handle)",elif errno == ENOENT:
"def _get_commands(): proc = Popen(['react-native', '--help'], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if not line: continue if 'Commands:' in line: should_yield = True continue <IF_STMT> yield line.split(' ')[0]",if should_yield:
"def getintdict(self, section): try: return dict(((key, int(value)) for key, value in self.items(section) <IF_STMT>)) except NoSectionError: return {}","if key not in {k for k, _ in self.items('DEFAULT')}"
"def _gen_opnds(ii): for op in ii.parsed_operands: if op.lookupfn_name in ['MASK1', 'MASKNOT0']: continue <IF_STMT> continue if op.name == 'BCAST': continue yield op",if op.visibility == 'SUPPRESSED':
"def do_definition(tag): w.end_para() macro('.TP') w.started = True split = 0 pre = [] post = [] for typ, text in _bitlist(tag): if split: post.append((typ, text)) <IF_STMT> split = 1 post.append((typ, text.lstrip()[2:].lstrip())) else: pre.append((typ, text)) _boldline(pre) w.write(_text(post)) w.started = False",elif text.lstrip().startswith(': '):
"def EvalInScriptedSection(self, codeBlock, globals, locals=None): if locals is None: locals = globals assert not codeBlock.beenExecuted, 'This code block should not have been executed' codeBlock.beenExecuted = 1 self.BeginScriptedSection() try: try: return self._EvalInScriptedSection(codeBlock.codeObject, globals, locals) finally: <IF_STMT> self.debugManager.OnLeaveScript() self.EndScriptedSection() except: self.HandleException(codeBlock)",if self.debugManager:
"def OSError__str__(self): if self.filename: <IF_STMT> return '[Errno %s] %s: %s -> %s' % (self.errno, self.strerror, self.filename, self.filename2) else: return '[Errno %s] %s: %s' % (self.errno, self.strerror, self.filename) if self.errno and self.strerror: return '[Errno %s] %s' % (self.errno, self.strerror) return BaseException.__str__(self)",if self.filename2:
"def save(self, *args, **kwargs): if not self.identifier: charset = list('ABCDEFGHJKLMNPQRSTUVWXYZ3789') while True: code = get_random_string(length=8, allowed_chars=charset) <IF_STMT> self.identifier = code break super().save(*args, **kwargs) if self.event: self.event.cache.clear()","if not Question.objects.filter(event=self.event, identifier=code).exists():"
"def malloc(self, size): assert 0 <= size < sys.maxint if os.getpid() != self._lastpid: self.__init__() self._lock.acquire() try: size = self._roundup(max(size, 1), self._alignment) arena, start, stop = self._malloc(size) new_stop = start + size <IF_STMT> self._free((arena, new_stop, stop)) block = (arena, start, new_stop) self._allocated_blocks.add(block) return block finally: self._lock.release()",if new_stop < stop:
"def commit(cache): assert cache.is_alive try: if cache.modified: cache.flush() <IF_STMT> assert cache.connection is not None cache.database.provider.commit(cache.connection, cache) cache.for_update.clear() cache.query_results.clear() cache.max_id_cache.clear() cache.immediate = True except: cache.rollback() raise",if cache.in_transaction:
"def __get_tasks(cls, task_ids=None, project_name=None, task_name=None, **kwargs): if task_ids: <IF_STMT> task_ids = [task_ids] return [cls(private=cls.__create_protection, task_id=task_id, log_to_backend=False) for task_id in task_ids] return [cls(private=cls.__create_protection, task_id=task.id, log_to_backend=False) for task in cls._query_tasks(project_name=project_name, task_name=task_name, **kwargs)]","if isinstance(task_ids, six.string_types):"
"def _VarRefOrWord(node, dynamic_arith): with tagswitch(node) as case: <IF_STMT> return True elif case(arith_expr_e.Word): if dynamic_arith: return True return False",if case(arith_expr_e.VarRef):
"def fit(self, data_instances, suffix): if self.statics_obj is None: self.statics_obj = MultivariateStatisticalSummary(data_instances) quantile_points = self.statics_obj.get_quantile_point(self.percentile) for col_name in self.selection_properties.select_col_names: quantile_value = quantile_points.get(col_name) <IF_STMT> self.selection_properties.add_left_col_name(col_name) self.selection_properties.add_feature_value(col_name, quantile_value) self._keep_one_feature(pick_high=True) return self",if quantile_value < self.upper_threshold:
"def predict_dict(self, words): """"""Predict a list of expansions given words."""""" expansions = [] for w in words: if w in self.expansion_dict: expansions += [self.expansion_dict[w]] <IF_STMT> expansions += [self.expansion_dict[w.lower()]] else: expansions += [w] return expansions",elif w.lower() in self.expansion_dict:
"def connect(self, host, port, ssl, helo, starttls, timeout): if ssl == '0': <IF_STMT> port = 25 fp = SMTP(timeout=int(timeout)) else: if not port: port = 465 fp = SMTP_SSL(timeout=int(timeout)) resp = fp.connect(host, int(port)) if helo: cmd, name = helo.split(' ', 1) if cmd.lower() == 'ehlo': resp = fp.ehlo(name) else: resp = fp.helo(name) if not starttls == '0': resp = fp.starttls() return TCP_Connection(fp, resp)",if not port:
"def _init_from_text(self, text): parts = text.split('; ') for part in parts: key, val = part.split('=') if key == 'CLONE': <IF_STMT> self.is_image = True self.image = val[6:] setattr(self, key.lower(), val)",if val[:5] == 'IMAGE':
"def to_laid_out_tensor(self): if not self._reduced: self._reduced = self.mesh_impl.allreduce(self.laid_out_input, self.mesh_axes, 'SUM') <IF_STMT> self._add_counter_fn() return self._reduced",if self._add_counter_fn:
"def platformGetThreads(self): ret = {} self._sendPkt('qfThreadInfo') tbytes = self._recvPkt() while tbytes.startswith('m'): <IF_STMT> for bval in tbytes[1:].split(','): ret[int(bval, 16)] = 0 else: ret[int(tbytes[1:], 16)] = 0 self._sendPkt('qsThreadInfo') tbytes = self._recvPkt() return ret","if tbytes.find(','):"
"def _generate_patterns(self, intent, intent_utterances, entity_placeholders): unique_patterns = set() patterns = [] stop_words = self._get_intent_stop_words(intent) for utterance in intent_utterances: pattern = self._utterance_to_pattern(utterance, stop_words, entity_placeholders) <IF_STMT> unique_patterns.add(pattern) patterns.append(pattern) return patterns",if pattern not in unique_patterns:
def generator(): try: _resp_data = DataHelper.flow2origin(self.flow['response']) or '' length = len(_resp_data) size = self.response_chunk_size bandwidth = config.bandwidth <IF_STMT> sleep_time = self.response_chunk_size / (bandwidth * 1024) else: sleep_time = 0 for i in range(int(length / size) + 1): time.sleep(sleep_time) self.server_resp_time = time.time() yield _resp_data[i * size:(i + 1) * size] finally: self.update_client_resp_time(),if bandwidth > 0:
"def generateMapItemListNode(self, key, value): itemslist = list() for item in value: <IF_STMT> itemslist.append('%s = %s' % (key, self.generateValueNode(item, key))) else: itemslist.append('%s' % self.generateValueNode(item)) return '(' + ' OR '.join(itemslist) + ')'",if key in self.allowedFieldsList:
"def _underscore_dict(dictionary): new_dictionary = {} for key, value in dictionary.items(): <IF_STMT> value = _underscore_dict(value) if isinstance(key, str): key = underscore(key) new_dictionary[key] = value return new_dictionary","if isinstance(value, dict):"
"def offsetToRva(self, offset): if self.inmem: return offset for s in self.sections: sbase = s.PointerToRawData <IF_STMT> ssize = s.VirtualSize else: ssize = max(s.SizeOfRawData, s.VirtualSize) if sbase <= offset and offset < sbase + ssize: return offset - s.PointerToRawData + s.VirtualAddress return 0",if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva():
"def func(): end_received = False while True: for idx, q in enumerate(self._local_out_queues): data = q.get() q.task_done() if isinstance(data, EndSignal): end_received = True if idx > 0: continue self._out_queue.put(data) <IF_STMT> break",if end_received:
def unwrap_assert_methods() -> None: for patcher in _mock_module_patches: try: patcher.stop() except RuntimeError as e: <IF_STMT> pass else: raise _mock_module_patches[:] = [] _mock_module_originals.clear(),if str(e) == 'stop called on unstarted patcher':
"def run(self): queue = self.queue while True: if not self.running: break callback, requests, fetchTimeout, validityOverride = queue.get() <IF_STMT> Price.fetchPrices(requests, fetchTimeout, validityOverride) wx.CallAfter(callback) queue.task_done() for price in requests: callbacks = self.wait.pop(price.typeID, None) if callbacks: for callback in callbacks: wx.CallAfter(callback)",if len(requests) > 0:
"def loadGCodeData(self, dataStream): if self._printing: return False self._lineCount = 0 for line in dataStream: <IF_STMT> line = line[:line.index(';')] line = line.strip() if len(line) < 1: continue self._lineCount += 1 self._doCallback() return True",if ';' in line:
"def _prepare_work_root(self): if os.path.exists(self.work_root): for f in os.listdir(self.work_root): <IF_STMT> shutil.rmtree(os.path.join(self.work_root, f)) else: os.remove(os.path.join(self.work_root, f)) else: os.makedirs(self.work_root)","if os.path.isdir(os.path.join(self.work_root, f)):"
"def _parse(self): for factory in self._sub_factories(): <IF_STMT> node, self.token_pos = factory(**self._initializer_args())._parse_with_pos() return node self.raise_unexpected_token()",if factory.is_possible_start(self.get_next_token()):
"def run(self): try: if not self.shell: self.shell = os.name == 'nt' <IF_STMT> os.chdir(self.working_dir) proc = subprocess.Popen(self.command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=self.shell, env=self.env) output = codecs.decode(proc.communicate()[0]) self.on_done(output) except subprocess.CalledProcessError as e: self.on_done(e.returncode, error=True) except OSError as e: self.on_done(e.message, error=True)",if self.working_dir != '':
"def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: return True <IF_STMT> return False elif name in self.get_attr(cls, '__annotations__', {}): return False elif isinstance(obj, ObjectMember) and obj.class_ is cls: return False return False",elif name in cls.__dict__:
"def _connect(s, address): try: s.connect(address) except socket.error: ty, v = sys.exc_info()[:2] if hasattr(v, 'errno'): v_err = v.errno else: v_err = v[0] <IF_STMT> raise v","if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]:"
"def _send_file(self, conn, path): """"""Method for a file PUT coro"""""" while True: chunk = conn.queue.get() <IF_STMT> try: with ChunkWriteTimeout(self.app.node_timeout): conn.send(chunk) except (Exception, ChunkWriteTimeout): conn.failed = True self.exception_occurred(conn.node, _('Object'), _('Trying to write to %s') % path) conn.queue.task_done()",if not conn.failed:
"def get_http_auth(self, name): auth = self._config.get('http-basic.{}'.format(name)) if not auth: username = self._config.get('http-basic.{}.username'.format(name)) password = self._config.get('http-basic.{}.password'.format(name)) <IF_STMT> return None else: username, password = (auth['username'], auth.get('password')) if password is None: password = self.keyring.get_password(name, username) return {'username': username, 'password': password}",if not username and (not password):
"def _do_analyze(self, action_ref, rule_links=None, processed=None, depth=0): if processed is None: processed = set() if rule_links is None: rule_links = [] processed.add(action_ref) for rule_link in self._rules.get(action_ref, []): rule_links.append((depth, rule_link)) <IF_STMT> continue self._do_analyze(rule_link._dest_action_ref, rule_links=rule_links, processed=processed, depth=depth + 1) return rule_links",if rule_link._dest_action_ref in processed:
"def _mock_manager_nfx(self, *args, **kwargs): if args: if args[0].tag == 'command': raise RpcError() <IF_STMT> return True else: return self._read_file('sw_info_nfx_' + args[0].tag + '.xml')",elif args[0].tag == 'get-software-information' and args[0].find('./*') is None:
"def test_url_invalid_set(): for line in URL_INVALID_TESTS.split('\n'): line = line.strip() <IF_STMT> continue match = COMMENT.match(line) if match: continue mbox = address.parse(line, strict=True) assert_equal(mbox, None)",if line == '':
"def _monitor_thread_function(main_process_pid): while True: logger.debug('Monitor thread monitoring pid: %d', main_process_pid) main_process_alive = any([process.pid for process in process_iter() if process.pid == main_process_pid]) <IF_STMT> logger.debug('Main process with pid %d is dead. Killing worker', main_process_pid) os._exit(0) sleep(1)",if not main_process_alive:
"def OnInsertCells(self, event=None): if self._counter == 1: <IF_STMT> self._counter = 0 self._icells = None return else: self._counter = 1 self._icells = (self.selection.topleft, self.selection.bottomright) self._execute(InsertCells(self.selection.topleft, self.selection.bottomright)) self._resize_grid() self._skip_except_on_mac(event)","if self._icells == (self.selection.topleft, self.selection.bottomright):"
"def get_scripts(): """"""Get custom npm scripts."""""" proc = Popen(['npm', 'run-script'], stdout=PIPE) should_yeild = False for line in proc.stdout.readlines(): line = line.decode() if 'available via `npm run-script`:' in line: should_yeild = True continue <IF_STMT> yield line.strip().split(' ')[0]","if should_yeild and re.match('^  [^ ]+', line):"
"def get_netloc(url): """"""Get Domain."""""" try: domain = '' parse_uri = urlparse(url) if not parse_uri.scheme: url = '//' + url parse_uri = urlparse(url) domain = '{uri.netloc}'.format(uri=parse_uri) <IF_STMT> return domain except Exception: logger.exception('[ERROR] Extracting Domain form URL')",if verify_domain(domain):
"def initiate_all_local_variables_instances(nodes, local_variables_instances, all_local_variables_instances): for node in nodes: if node.variable_declaration: new_var = LocalIRVariable(node.variable_declaration) <IF_STMT> new_var.index = all_local_variables_instances[new_var.name].index + 1 local_variables_instances[node.variable_declaration.name] = new_var all_local_variables_instances[node.variable_declaration.name] = new_var",if new_var.name in all_local_variables_instances:
"def _disconnect(self, sync): if self._connection: <IF_STMT> try: self._connection.send_all() self._connection.fetch_all() except (WorkspaceError, ServiceUnavailable): pass if self._connection: self._connection.in_use = False self._connection = None self._connection_access_mode = None",if sync:
"def init(self): """"""Initialize a booster from the database and validate"""""" self.__item = None if self.itemID: self.__item = eos.db.getItem(self.itemID) <IF_STMT> pyfalog.error('Item (id: {0}) does not exist', self.itemID) return if self.isInvalid: pyfalog.error('Item (id: {0}) is not a Booster', self.itemID) return self.build()",if self.__item is None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_limit(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _match_greater_than_or_equal(search_base, attribute, value, candidates): matches = list() for entry in candidates: dn = entry.get('dn') if not dn.endswith(search_base): continue value_from_directory = entry.get('attributes').get(attribute) <IF_STMT> entry['type'] = 'searchResEntry' matches.append(entry) return matches",if str(value_from_directory) >= str(value):
"def list_target_unit_files(self, *modules): """"""show all the target units and the enabled status"""""" result = {} enabled = {} for unit in _all_common_targets: result[unit] = None enabled[unit] = 'static' <IF_STMT> enabled[unit] = 'enabled' if unit in _all_common_disabled: enabled[unit] = 'enabled' return [(unit, enabled[unit]) for unit in sorted(result)]",if unit in _all_common_enabled:
"def handle_data(self, data): if self.in_span or self.in_div: <IF_STMT> self.no_user = True elif data == 'Invalid password': self.bad_pw = True elif data == 'User with that email already exists': self.already_exists = True",if data == 'No such user (please note that login is case sensitive)':
"def walk_tree(root: Element, processor: Callable[[Element], Optional[_T]], stop_after_first: bool=False) -> List[_T]: results = [] queue = deque([root]) while queue: currElement = queue.popleft() for child in currElement: <IF_STMT> queue.append(child) result = processor(child) if result is not None: results.append(result) if stop_after_first: return results return results",if child:
"def characters(self, ch): if self._inside_fuzzable: modified_value = self._fuzzed_parameters[self._fuzzable_index][1] if isinstance(modified_value, DataToken): modified_value = modified_value.get_value() <IF_STMT> enc_val = base64.b64encode(modified_value) else: enc_val = cgi.escape(modified_value).encode('ascii', 'xmlcharrefreplace') self.fuzzed_xml_string += enc_val else: self.fuzzed_xml_string += ch",if self._fuzzed_parameters[self._fuzzable_index][0] == 'base64':
def when_the_task_has_started(context): for _ in range(120): app = context.marathon_clients.current[0].get_app(APP_ID) happy_count = app.tasks_running <IF_STMT> return time.sleep(0.5) raise Exception('timed out waiting for task to start'),if happy_count >= 3:
"def _sock_send(self, msg): try: if isinstance(msg, str): msg = msg.encode('ascii') <IF_STMT> msg = msg + b'|#' + self.dogstatsd_tags.encode('ascii') if self.sock: self.sock.send(msg) except Exception: Logger.warning(self, 'Error sending message to statsd', exc_info=True)",if self.dogstatsd_tags:
"def __init__(self, constraints=None, preferences=None, platforms=None, maxreplicas=None): if constraints is not None: self['Constraints'] = constraints if preferences is not None: self['Preferences'] = [] for pref in preferences: <IF_STMT> pref = PlacementPreference(*pref) self['Preferences'].append(pref) if maxreplicas is not None: self['MaxReplicas'] = maxreplicas if platforms: self['Platforms'] = [] for plat in platforms: self['Platforms'].append({'Architecture': plat[0], 'OS': plat[1]})","if isinstance(pref, tuple):"
def start(self): if not self._active: self._active = True <IF_STMT> self.exit = threading.Event() self.thread = threading.Thread(target=self.check) self.thread.daemon = True self.thread.start(),if self.thread is None:
"def on_player_state_changed(self, state): if state == State.playing: self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[1]) self._toggle_player_action.setIcon(QIcon.fromTheme('media-pause')) self._toggle_player_action.setEnabled(True) else: self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[0]) self._toggle_player_action.setIcon(QIcon.fromTheme('media-play')) <IF_STMT> self._toggle_player_action.setEnabled(False) else: self._toggle_player_action.setEnabled(True)",if state == State.stopped:
"def __init__(self, el): self.elements = list(el) parameters = {} tokens = [] token_quote = '@' for key, value in el.attrib.items(): if key == 'token_quote': token_quote = value if key == 'tokens': for token in value.split(','): tokens.append((token, REQUIRED_PARAMETER)) <IF_STMT> token = key[len('token_'):] tokens.append((token, value)) for name, default in tokens: parameters[name] = (token_quote, default) self.parameters = parameters",elif key.startswith('token_'):
"def create(self): if self.mode == 'INDICES': self.newInput('Integer List', 'Indices', 'indices') self.newOutput('Polygon Indices', 'Polygon Indices', 'polygonIndices') elif self.mode == 'VERTEX_AMOUNT': <IF_STMT> self.newInput('Integer List', 'Vertex Amounts', 'vertexAmounts') self.newOutput('Polygon Indices List', 'Polygon Indices List', 'polygonIndicesList') else: self.newInput('Integer', 'Vertex Amount', 'vertexAmount', value=3, minValue=3) self.newOutput('Polygon Indices', 'Polygon Indices', 'polygonIndices')",if self.useList:
def _chroot_pids(chroot): pids = [] for root in glob.glob('/proc/[0-9]*/root'): try: link = os.path.realpath(root) <IF_STMT> pids.append(int(os.path.basename(os.path.dirname(root)))) except OSError: pass return pids,if link.startswith(chroot):
"def to_word_end(view, s): if mode == modes.NORMAL: pt = word_end_reverse(view, s.b, count) return sublime.Region(pt) elif mode in (modes.VISUAL, modes.VISUAL_BLOCK): if s.a < s.b: pt = word_end_reverse(view, s.b - 1, count) <IF_STMT> return sublime.Region(s.a, pt + 1) return sublime.Region(s.a + 1, pt) pt = word_end_reverse(view, s.b, count) return sublime.Region(s.a, pt) return s",if pt > s.a:
"def torch_sparse_Tensor(coords, feats, size=None): if size is None: <IF_STMT> return torch.sparse.DoubleTensor(coords, feats) elif feats.dtype == torch.float32: return torch.sparse.FloatTensor(coords, feats) else: raise ValueError('Feature type not supported.') elif feats.dtype == torch.float64: return torch.sparse.DoubleTensor(coords, feats, size) elif feats.dtype == torch.float32: return torch.sparse.FloatTensor(coords, feats, size) else: raise ValueError('Feature type not supported.')",if feats.dtype == torch.float64:
"def detab(self, text): """"""Remove a tab from the front of each line of the given text."""""" newtext = [] lines = text.split('\n') for line in lines: if line.startswith(' ' * markdown.TAB_LENGTH): newtext.append(line[markdown.TAB_LENGTH:]) <IF_STMT> newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",elif not line.strip():
"def iter_input(input, filename, parser, line_by_line): if isinstance(input, basestring): with open(input, 'rb') as f: for tree in iter_input(f, filename, parser, line_by_line): yield tree else: try: if line_by_line: for line in input: <IF_STMT> yield et.ElementTree(et.fromstring(line, parser)) else: yield et.parse(input, parser) except IOError: e = sys.exc_info()[1] error('parsing %r failed: %s: %s', filename, e.__class__.__name__, e)",if line:
"def find_xsubpp(): for var in ('privlib', 'vendorlib'): xsubpp = cfg_lst('$Config{%s}/ExtUtils/xsubpp$Config{exe_ext}' % var) <IF_STMT> return xsubpp return self.find_program('xsubpp')",if xsubpp and os.path.isfile(xsubpp[0]):
"def apply_list(self, expr, rules, evaluation): """"""ReplaceRepeated[expr_, rules_]"""""" try: rules, ret = create_rules(rules, expr, 'ReplaceRepeated', evaluation) except PatternError: evaluation.message('Replace', 'reps', rules) return None if ret: return rules while True: evaluation.check_stopped() result, applied = expr.apply_rules(rules, evaluation) <IF_STMT> result = result.evaluate(evaluation) if applied and (not result.same(expr)): expr = result else: break return result",if applied:
"def __init__(self, lambda_val: Optional[Union[torch.Tensor, Tuple[float, float]]]=None, same_on_batch: bool=False, p: float=1.0) -> None: super(RandomMixUp, self).__init__(p=1.0, p_batch=p, same_on_batch=same_on_batch) if lambda_val is None: self.lambda_val = torch.tensor([0, 1.0]) else: self.lambda_val = cast(torch.Tensor, lambda_val) <IF_STMT> else torch.tensor(lambda_val)","if isinstance(lambda_val, torch.Tensor)"
"def run_sync(self): count = 0 while count < self.args.num_messages: batch = self.receiver.receive_messages(max_message_count=self.args.num_messages - count, max_wait_time=self.args.max_wait_time or None) <IF_STMT> for msg in batch: self.receiver.complete_message(msg) count += len(batch)",if self.args.peeklock:
"def ns_to_timespec(self, nsec): """"""Transforms nanoseconds to a timespec."""""" ts = self.timespec() if not nsec: ts.tv_sec = 0 ts.tv_nsec = 0 else: ts.tv_sec, rem = divmod(nsec, timespec.NSEC_PER_SEC) <IF_STMT> ts.tv_sec -= 1 rem += timespec.NSEC_PER_SEC ts.tv_nsec = rem return ts",if rem < 0:
"def fixFunctionDocTag(funcnode): doctext = funcnode.get('doc') if doctext: if funcnode.attrib['name'] == 'eval': funcnode.attrib['doc'] = doctext.replace('ECMAScript', 'JavaScript') sp = doctext.rsplit('Return Type: ', 1) <IF_STMT> funcnode.attrib['doc'] = sp[0].rstrip() returnType = standardizeJSType(sp[1].split(None, 1)[0]) addCixReturns(funcnode, returnType) return returnType return None",if len(sp) == 2:
def check_engine(engine): if engine == 'auto': <IF_STMT> return 'pyarrow' elif fastparquet is not None: return 'fastparquet' else: raise RuntimeError('Please install either pyarrow or fastparquet.') elif engine == 'pyarrow': if pa is None: raise RuntimeError('Please install pyarrow fisrt.') return engine elif engine == 'fastparquet': if fastparquet is None: raise RuntimeError('Please install fastparquet first.') return engine else: raise RuntimeError('Unsupported engine {} to read parquet.'.format(engine)),if pa is not None:
"def addInt(self, intval, width, nodeinfo): node = self.basenode for sh in range(width - 1, -1, -1): choice = intval >> sh & 1 <IF_STMT> node[choice] = [None, None, None] node = node[choice] node[2] = nodeinfo",if node[choice] is None:
"def add_cand(cands): cands = [cand.creator for cand in cands if cand.creator is not None] for x in cands: if x in seen_set: continue order = 1 <IF_STMT> order = -len(seen_set) heapq.heappush(cand_funcs, (order, -x.rank, -len(seen_set), x)) seen_set.add(x)",if fan_out[x] == 1 and len(cands) == 1:
"def indentSelection(self, howFar=4): startLineNum = self.LineFromPosition(self.GetSelectionStart()) endLineNum = self.LineFromPosition(self.GetSelectionEnd()) self.BeginUndoAction() for lineN in range(startLineNum, endLineNum + 1): newIndent = self.GetLineIndentation(lineN) + howFar <IF_STMT> newIndent = 0 self.SetLineIndentation(lineN, newIndent) self.EndUndoAction()",if newIndent < 0:
"def request(self, host, handler, request_body, verbose=False): for i in (0, 1): try: return self.single_request(host, handler, request_body, verbose) except socket.error as e: <IF_STMT> raise except http_client.BadStatusLine: if i: raise","if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE):"
"def update_data(self, change): self.mark.x = self.state.x_centers y0 = self.state.grid <IF_STMT> y0 = y0 / np.sum(y0) if self.state.grid_sliced is not None: y1 = self.state.grid_sliced if self.normalize: y1 = y1 / np.sum(y1) self.mark.y = np.array([y0, y1]) self.mark.colors = [C0, C1] self.mark.type = 'grouped' else: self.mark.y = y0 self.mark.colors = [C0]",if self.normalize:
"def visit_body(self, nodes): new_nodes = [] count = 0 for node in nodes: rewriter = IfExpRewriter(count) possibly_transformed_node = rewriter.visit(node) <IF_STMT> new_nodes.extend(rewriter.assignments) count += len(rewriter.assignments) new_nodes.append(possibly_transformed_node) return new_nodes",if rewriter.assignments:
"def byteRegOffset(self, val, prefixes=0): if prefixes & PREFIX_REX: val |= e_i386.RMETA_LOW8 el<IF_STMT> val |= e_i386.RMETA_LOW8 else: val |= e_i386.RMETA_HIGH8 val -= 4 return val",if val < 4:
"def gprv_immv(ii): for i, op in enumerate(_gen_opnds(ii)): <IF_STMT> if op.name == 'REG0' and op_luf_start(op, 'GPRv'): continue else: return False elif i == 1: if op_immv(op): continue else: return False else: return False return True",if i == 0:
"def normalize(self): self.pairs.sort() i = 1 while i < len(self.pairs): alo, ahi = self.pairs[i - 1] blo, bhi = self.pairs[i] <IF_STMT> self.pairs[i - 1:i + 1] = [(alo, max(ahi, bhi))] else: i = i + 1",if ahi >= blo - 1:
"def __substitute_composite_key(self, key, composite_file, dataset=None): if composite_file.substitute_name_with_metadata: <IF_STMT> meta_value = str(dataset.metadata.get(composite_file.substitute_name_with_metadata)) else: meta_value = self.spec[composite_file.substitute_name_with_metadata].default return key % meta_value return key",if dataset:
"def cb(definition): if len(definition.strip()) == 0: <IF_STMT> dialog = wx.MessageDialog(self, _('Do you want to erase the macro?'), style=wx.YES_NO | wx.YES_DEFAULT | wx.ICON_QUESTION) if dialog.ShowModal() == wx.ID_YES: self.delete_macro(macro_name) return self.log(_('Cancelled.')) return self.cur_macro_name = macro_name self.cur_macro_def = definition self.end_macro()",if old_macro_definition != '':
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) <IF_STMT> try: body = str(request.body, 'utf-8') if isinstance(request.body, bytes) else str(request.body) except TypeError: body = str(request.body) if old in body: request.body = body.replace(old, new) return request",if is_text_payload(request) and request.body:
"def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None): progress = Progress(len(meshes), None) fp.write('\n  <library_controllers>\n') for mIdx, mesh in enumerate(meshes): subprog = Progress()(0, 0.5) <IF_STMT> writeSkinController(fp, human, mesh, skel, config) subprog(0.5, 1) if shapes is not None: writeMorphController(fp, mesh, shapes[mIdx], config) progress.step() fp.write('  </library_controllers>\n')",if skel:
def checkpoint(): if checkpoint_asserts: self.assert_integrity_idxs_take() <IF_STMT> toposort(self.idxs_memo[node]) if node in self.take_memo: for take in self.take_memo[node]: toposort(take),if node in self.idxs_memo:
"def __virtual__(): try: global __salt__ <IF_STMT> __salt__ = salt.loader.minion_mods(__opts__) return True except Exception as e: log.error('Could not load __salt__: %s', e) return False",if not __salt__:
"def annotate_disk_for_smart(middleware, devices, disk): args = await get_smartctl_args(middleware, devices, disk) if args: <IF_STMT> args.extend(['-a']) args.extend(['-d', 'removable']) return (disk, dict(smartctl_args=args))",if await ensure_smart_enabled(args):
"def make_connection(self, host): h, eh, kwargs = self.get_host_info(host) if not kwargs: kwargs = {} kwargs['timeout'] = self.timeout if _ver_info == (2, 6): result = HTTPS(host, None, **kwargs) else: <IF_STMT> self._extra_headers = eh self._connection = (host, httplib.HTTPSConnection(h, None, **kwargs)) result = self._connection[1] return result",if not self._connection or host != self._connection[0]:
"def get_base_types(self, cdef: ClassDef) -> List[str]: """"""Get list of base classes for a class."""""" base_types = [] for base in cdef.base_type_exprs: if isinstance(base, NameExpr): if base.name != 'object': base_types.append(base.name) <IF_STMT> modname = get_qualified_name(base.expr) base_types.append('%s.%s' % (modname, base.name)) elif isinstance(base, IndexExpr): p = AliasPrinter(self) base_types.append(base.accept(p)) return base_types","elif isinstance(base, MemberExpr):"
"def add_entry(self, entry): version = entry.as_python if version: _ = self.versions[version.version_tuple] paths = {p.path for p in self.versions.get(version.version_tuple, [])} <IF_STMT> self.versions[version.version_tuple].append(entry)",if entry.path not in paths:
def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args['user'] = mysql_user if mysql_pwd: args['passwd'] = mysql_pwd if mysql_host: args['host'] = mysql_host if mysql_port: args['port'] = mysql_port <IF_STMT> args['unix_socket'] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception('Cannot interface with MySQL server: %s' % e),if mysql_socket:
"def findsection(self, key): to_return = copy.deepcopy(self) for subsection in to_return: try: value = list(ConfigObj.find_key(to_return[subsection], key))[0] except Exception: value = None if not value: del to_return[subsection] else: for category in to_return[subsection]: <IF_STMT> del to_return[subsection][category] for key in [k for k, v in to_return.items() if not v]: del to_return[key] return to_return",if category != key:
"def get_ready_conn(self, host): conn = None self._lock.acquire() try: <IF_STMT> for c in self._hostmap[host]: if self._readymap[c]: self._readymap[c] = 0 conn = c break finally: self._lock.release() return conn",if host in self._hostmap:
"def assign_set_scope(ir_set: irast.Set, scope: Optional[irast.ScopeTreeNode], *, ctx: context.ContextLevel) -> irast.Set: if scope is None: ir_set.path_scope_id = None else: <IF_STMT> scope.unique_id = ctx.scope_id_ctr.nextval() ir_set.path_scope_id = scope.unique_id if scope.find_child(ir_set.path_id): raise RuntimeError('scoped set must not contain itself') return ir_set",if scope.unique_id is None:
"def _flatten(*args): arglist = [] for arg in args: if isinstance(arg, _Block): if arg.vhdl_code is not None: arglist.append(arg.vhdl_code) continue else: arg = arg.subs <IF_STMT> arglist.append(_userCodeMap['vhdl'][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist",if id(arg) in _userCodeMap['vhdl']:
"def _prepare_expected(data, lags, trim='front'): t, k = data.shape expected = np.zeros((t + lags, (lags + 1) * k)) for col in range(k): for i in range(lags + 1): <IF_STMT> expected[i:-lags + i, (lags + 1) * col + i] = data[:, col] else: expected[i:, (lags + 1) * col + i] = data[:, col] if trim == 'front': expected = expected[:-lags] return expected",if i < lags:
"def test_class_based_views_inherit_from_acl_gateway_class(self): for urlpattern in self.urlpatterns_to_test: callback_name = urlpattern.callback.__name__ module_name = urlpattern.callback.__module__ if (callback_name, module_name) in self.excluded_callbacks: continue imported_module = __import__(module_name, fromlist=[callback_name]) found_callback = getattr(imported_module, callback_name) <IF_STMT> continue msg = ""Class '{}' does not inherit from 'ACLGateway' class."".format(found_callback) self.assertTrue(issubclass(found_callback, ACLGateway), msg)",if not inspect.isclass(found_callback):
"def generateMapItemTypedNode(self, key, value): if type(value) == SigmaRegularExpressionModifier: regex = str(value) <IF_STMT> regex = '.*' + regex if not (regex.endswith('$') or regex.endswith('.*')): regex = regex + '.*' return '%s MATCHES %s' % (self.cleanKey(key), self.generateValueNode(regex)) else: raise NotImplementedError(""Type modifier '{}' is not supported by backend"".format(value.identifier))",if not (regex.startswith('^') or regex.startswith('.*')):
"def __str__(self): _outicalfile = self._icalfile for unit in self.units: for location in unit.getlocations(): match = re.match('\\[(?P<uid>.+)\\](?P<property>.+)', location) for component in self._icalfile.components(): if component.name != 'VEVENT': continue <IF_STMT> continue for property in component.getChildren(): if property.name == match.groupdict()['property']: property.value = unit.target if _outicalfile: return str(_outicalfile.serialize()) else: return ''",if component.uid.value != match.groupdict()['uid']:
"def __init__(self, items): self._format = string.join(map(lambda item: item[0], items), '') self._items = items self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format)) for format, name in self._items: <IF_STMT> if format == 'c': val = '\x00' else: val = 0 else: l = int(format[:-1]) val = '\x00' * l self.__dict__[name] = val",if len(format) == 1:
"def __init__(self, learners, names=None): self.learners = learners for i, learner in enumerate(learners): self.update_set_reward(learner) learner.accumulated_rewards = [] learner.known_states = [] learner.temperatures = [] <IF_STMT> learner.name = 'Learner %d' % i else: learner.name = names[i]",if names is None:
"def __init__(self, *args, **kwargs): self.default_currency = kwargs.pop('default_currency', None) super().__init__(*args, **kwargs) for idx, validator in enumerate(self.validators): <IF_STMT> self.validators[idx] = MinMoneyValidator(self.min_value) elif isinstance(validator, MaxValueValidator): self.validators[idx] = MaxMoneyValidator(self.max_value)","if isinstance(validator, MinValueValidator):"
"def add_line_taxes(self, lines): for line in lines: <IF_STMT> continue for index, line_tax in enumerate(line.source_line.taxes, 1): line.taxes.create(tax=line_tax.tax, name=line_tax.name, amount_value=line_tax.amount.value, base_amount_value=line_tax.base_amount.value, ordering=index)",if not line.source_line:
"def linesub(match): line = match.group() for token in TOKEN_RE.findall(line): if token in names: targets = names[token] fdist.inc(token) <IF_STMT> log.warning('%s is ambiguous: %s' % (token, ', '.join((str(v.canonical_name) for v in names[token])))) line += INDEXTERM % token return line",if len(targets) > 1:
"def ask(self) -> Dict[str, Any]: params = {} param_values = self._optimizer.ask() for (name, distribution), value in zip(sorted(self._search_space.items()), param_values): if isinstance(distribution, distributions.DiscreteUniformDistribution): value = value * distribution.q + distribution.low <IF_STMT> value = value * distribution.step + distribution.low if isinstance(distribution, distributions.IntLogUniformDistribution): value = int(np.round(value)) value = min(max(value, distribution.low), distribution.high) params[name] = value return params","if isinstance(distribution, distributions.IntUniformDistribution):"
def fetcher(): while True: try: <IF_STMT> break self.fetch() self._cluster.handler.sleep(0.01) except ReferenceError: break except Exception: self._worker_exception = sys.exc_info() break try: self.cleanup() except ReferenceError as e: log.debug('Attempt to cleanup consumer failed with ReferenceError') log.debug('Fetcher thread exiting'),if not self._running:
"def write_text(self, text): """"""Writes re-indented text into the buffer."""""" should_indent = False rows = [] for row in text.split('\n'): if should_indent: row = '{}'.format(row) <IF_STMT> row = row.replace('\x08', '', 1) should_indent = True elif not len(row.strip()): should_indent = False rows.append(row) self.write('{}\n'.format('\n'.join(rows)))",if '\x08' in row:
"def test_kafka_consumer(self): self.send_messages(0, range(0, 100)) self.send_messages(1, range(100, 200)) consumer = self.kafka_consumer(auto_offset_reset='earliest') n = 0 messages = {0: set(), 1: set()} for m in consumer: logging.debug('Consumed message %s' % repr(m)) n += 1 messages[m.partition].add(m.offset) <IF_STMT> break self.assertEqual(len(messages[0]), 100) self.assertEqual(len(messages[1]), 100)",if n >= 200:
"def get_command(scaffolding, command_path): path, _, command_name = command_path.rpartition('.') if path not in scaffolding: raise KeyError('Ingredient for command ""%s"" not found.' % command_path) if command_name in scaffolding[path].commands: return scaffolding[path].commands[command_name] el<IF_STMT> raise KeyError('Command ""%s"" not found in ingredient ""%s""' % (command_name, path)) else: raise KeyError('Command ""%s"" not found' % command_name)",if path:
"def build_extension(self, ext): ext._convert_pyx_sources_to_lang() _compiler = self.compiler try: <IF_STMT> self.compiler = self.shlib_compiler _build_ext.build_extension(self, ext) if ext._needs_stub: cmd = self.get_finalized_command('build_py').build_lib self.write_stub(cmd, ext) finally: self.compiler = _compiler","if isinstance(ext, Library):"
"def _send_payload(self, payload): req = eventlet_urllib2.Request(self._url, headers=payload[1]) try: <IF_STMT> response = eventlet_urllib2.urlopen(req, payload[0]).read() else: response = eventlet_urllib2.urlopen(req, payload[0], self.timeout).read() return response except Exception as err: return err","if sys.version_info < (2, 6):"
"def get_access_token(self, callback): if not self.is_authorized(): callback(None) else: access_token = config.persist['oauth_access_token'] access_token_expires = config.persist['oauth_access_token_expires'] <IF_STMT> callback(access_token) else: self.forget_access_token() self.refresh_access_token(callback)",if access_token and time.time() < access_token_expires:
"def mark_first_parents(event): """"""Mark the node and all its parents."""""" c = event.get('c') if not c: return changed = [] for parent in c.p.self_and_parents(): <IF_STMT> parent.v.setMarked() parent.setAllAncestorAtFileNodesDirty() changed.append(parent.copy()) if changed: c.setChanged() c.redraw() return changed",if not parent.isMarked():
"def normalize_reg_path(self, path): new = path if path: roots = ('\\registry\\machine\\', 'hklm\\') for r in roots: <IF_STMT> new = 'HKEY_LOCAL_MACHINE\\' + path[len(r):] return new return path",if path.lower().startswith(r):
"def extract_labels(filename, one_hot=False): """"""Extract the labels into a 1D uint8 numpy array [index]."""""" print('Extracting', filename) with gzip.open(filename) as bytestream: magic = _read32(bytestream) if magic != 2049: raise ValueError('Invalid magic number %d in MNIST label file: %s' % (magic, filename)) num_items = _read32(bytestream) buf = bytestream.read(num_items) labels = numpy.frombuffer(buf, dtype=numpy.uint8) <IF_STMT> return dense_to_one_hot(labels) return labels",if one_hot:
"def on_change(self, data): for window in sublime.windows(): for view in window.views(): if view.get_status('inactive') and view.settings().get('tp_append', False): file_name = view.file_name() <IF_STMT> self.update(view) elif file_name and file_name.endswith(global_settings('ammo_file_extension', '.ammo')): self.update(view)","if view.settings().get('tp_ammo', False):"
"def list(self, items, columns=4, width=80): items = list(sorted(items)) colw = width // columns rows = (len(items) + columns - 1) // columns for row in range(rows): for col in range(columns): i = col * rows + row <IF_STMT> self.output.write(items[i]) if col < columns - 1: self.output.write(' ' + ' ' * (colw - 1 - len(items[i]))) self.output.write('\n')",if i < len(items):
"def test_dynamic_section_solaris(self): """"""Verify that we can parse relocations from the .dynamic section"""""" test_dir = os.path.join('test', 'testfiles_for_unittests') with open(os.path.join(test_dir, 'exe_solaris32_cc.elf'), 'rb') as f: elff = ELFFile(f) for sect in elff.iter_sections(): <IF_STMT> relos = sect.get_relocation_tables() self.assertEqual(set(relos), {'JMPREL', 'REL'})","if isinstance(sect, DynamicSection):"
"def close(self, checkcount=False): self.mutex.acquire() try: if checkcount: self.openers -= 1 if self.openers == 0: self.do_close() else: <IF_STMT> self.do_close() self.openers = 0 finally: self.mutex.release()",if self.openers > 0:
def subcommand_table(self): if self._subcommand_table is None: <IF_STMT> self._topic_tag_db = TopicTagDB() self._topic_tag_db.load_json_index() self._subcommand_table = self._create_subcommand_table() return self._subcommand_table,if self._topic_tag_db is None:
"def layer_init(self): for layer in self.cnn: if isinstance(layer, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal_(layer.weight, nn.init.calculate_gain('relu')) <IF_STMT> nn.init.constant_(layer.bias, val=0)",if layer.bias is not None:
"def _append_modifier(code, modifier): if modifier == 'euro': if '.' not in code: return code + '.ISO8859-15' _, _, encoding = code.partition('.') if encoding in ('ISO8859-15', 'UTF-8'): return code <IF_STMT> return _replace_encoding(code, 'ISO8859-15') return code + '@' + modifier",if encoding == 'ISO8859-1':
"def set_mean(self, mean): if mean is not None: <IF_STMT> mean = mean[:, np.newaxis, np.newaxis] elif self.is_color: assert len(mean.shape) == 3 self.mean = mean",if mean.ndim == 1:
"def _set_state(self, value): if self._pwm: try: value = int(value * self._connection.get_PWM_range(self._number)) <IF_STMT> self._connection.set_PWM_dutycycle(self._number, value) except pigpio.error: raise PinInvalidState('invalid state ""%s"" for pin %r' % (value, self)) elif self.function == 'input': raise PinSetInput('cannot set state of pin %r' % self) else: self._connection.write(self._number, bool(value))",if value != self._connection.get_PWM_dutycycle(self._number):
"def do_stop(self): logger.info('[%s] Stopping all workers', self.name) for w in self.workers.values(): try: w.terminate() w.join(timeout=1) except (AttributeError, AssertionError): pass if self.http_daemon: if self.brok_interface: self.http_daemon.unregister(self.brok_interface) <IF_STMT> self.http_daemon.unregister(self.scheduler_interface) super(Satellite, self).do_stop()",if self.scheduler_interface:
"def iter_input(input, filename, parser, line_by_line): if isinstance(input, basestring): with open(input, 'rb') as f: for tree in iter_input(f, filename, parser, line_by_line): yield tree else: try: <IF_STMT> for line in input: if line: yield et.ElementTree(et.fromstring(line, parser)) else: yield et.parse(input, parser) except IOError: e = sys.exc_info()[1] error('parsing %r failed: %s: %s', filename, e.__class__.__name__, e)",if line_by_line:
"def debug_print(data: json): try: print('[+] ---Debug info---') for i, v in data.items(): if i == 'outline': print('[+]  -', i, ':', len(v), 'characters') continue <IF_STMT> continue print('[+]  -', '%-11s' % i, ':', v) print('[+] ---Debug info---') except: pass",if i == 'actor_photo' or i == 'year':
def deliver_event(self): while True: client = self._client() <IF_STMT> return diff = self._due - client.loop.time() if diff <= 0: await client._dispatch_event(self._event) return del client await asyncio.sleep(diff),if client is None:
"def pluginload(bot, event, *args): """"""loads a previously unloaded plugin, requires plugins. prefix"""""" if args: module_path = args[0] try: <IF_STMT> message = '<b><pre>{}</pre>: loaded</b>'.format(module_path) else: message = '<b><pre>{}</pre>: failed</b>'.format(module_path) except RuntimeError as e: message = '<b><pre>{}</pre>: <pre>{}</pre></b>'.format(module_path, str(e)) else: message = '<b>module path required</b>' yield from bot.coro_send_message(event.conv_id, message)","if plugins.load(bot, module_path):"
"def validate_prompt_lb(hostname): hostname = validate_prompt_hostname(hostname) for host in hosts: <IF_STMT> raise click.BadParameter('Cannot re-use ""%s"" as a load balancer, please specify a separate host' % hostname) return hostname",if host.connect_to == hostname and (host.is_master() or host.is_node()):
"def alter_inventory(session, resource, amount): """"""Alters the inventory of each settlement."""""" from horizons.component.storagecomponent import StorageComponent for settlement in session.world.settlements: <IF_STMT> settlement.warehouse.get_component(StorageComponent).inventory.alter(resource, amount)",if settlement.owner == session.world.player and settlement.warehouse:
"def _(value): if kb.customInjectionMark in (value or ''): <IF_STMT> value = value.replace(kb.customInjectionMark, '') else: value = re.sub('\\w*%s' % re.escape(kb.customInjectionMark), payload, value) return value",if payload is None:
"def __call__(self, target): if 'weights' not in target.temp: return True targets = target.temp['weights'] for cname in target.children: if cname in targets: c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) if deviation > self.tolerance: return True if 'cash' in target.temp: cash_deviation = abs((target.capital - targets.value) / targets.value - target.temp['cash']) <IF_STMT> return True return False",if cash_deviation > self.tolerance:
"def splitroot(self, part, sep=sep): if part and part[0] == sep: stripped_part = part.lstrip(sep) <IF_STMT> return ('', sep * 2, stripped_part) else: return ('', sep, stripped_part) else: return ('', '', part)",if len(part) - len(stripped_part) == 2:
"def _Determine_Do(self): self.applicable = 1 method = 'moz-src' method_arg = None for opt, optarg in self.chosenOptions: <IF_STMT> method = 'moz-src' elif opt == '--moz-objdir': method = 'moz-objdir' method_arg = optarg if method == 'moz-src': self.value = self._get_mozilla_objdir() elif method == 'moz-objdir': self.value = self._use_mozilla_objdir(method_arg) else: raise black.configure.ConfigureError('bogus method: %r' % method) self.determined = 1",if opt == '--moz-src':
"def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: return True elif name in cls.__dict__: return False <IF_STMT> return False elif isinstance(obj, ObjectMember) and obj.class_ is cls: return False return False","elif name in self.get_attr(cls, '__annotations__', {}):"
"def _remove_all_greasemonkey_scripts(self): page_scripts = self._widget.page().scripts() for script in page_scripts.toList(): <IF_STMT> log.greasemonkey.debug('Removing script: {}'.format(script.name())) removed = page_scripts.remove(script) assert removed, script.name()",if script.name().startswith('GM-'):
"def merge_intervals(intervals): """"""Merge intervals in the form of a list."""""" if intervals is None: return None intervals.sort(key=lambda i: i[0]) out = [intervals.pop(0)] for i in intervals: <IF_STMT> out[-1][-1] = max(out[-1][-1], i[-1]) else: out.append(i) return out",if out[-1][-1] >= i[0]:
"def __setattr__(self, key, val): self.__dict__[key] = val self.__dict__[key.upper()] = val levels = key.split('.') last_level = len(levels) - 1 pointer = self._pointer if len(levels) > 1: for i, l in enumerate(levels): <IF_STMT> setattr(getattr(self, l), '.'.join(levels[i:]), val) if l == last_level: pointer[l] = val else: pointer = pointer[l]","if hasattr(self, l) and isinstance(getattr(self, l), Config):"
"def get_menu_title(self): handle = self.obj.get_handle() if handle: who = get_participant_from_event(self.db, handle) desc = self.obj.get_description() event_name = self.obj.get_type() <IF_STMT> event_name = '%s - %s' % (event_name, desc) if who: event_name = '%s - %s' % (event_name, who) dialog_title = _('Event: %s') % event_name else: dialog_title = _('New Event') return dialog_title",if desc:
"def perform_initialization(m): if isinstance(m, self.initialize_layers): <IF_STMT> initialization_method(m.weight.data, **initialization_kwargs) if m.bias is not None and self.initialize_bias != 'No' and (initialization_method_bias is not None): try: initialization_method_bias(m.bias.data, **initialization_kwargs_bias) except ValueError: pass",if initialization_method is not None:
"def forward(self, inputs): x = inputs['image'] out = self.conv0(x) out = self.downsample0(out) blocks = [] for i, conv_block_i in enumerate(self.darknet_conv_block_list): out = conv_block_i(out) if i == self.freeze_at: out.stop_gradient = True <IF_STMT> blocks.append(out) if i < self.num_stages - 1: out = self.downsample_list[i](out) return blocks",if i in self.return_idx:
"def _urlvars__set(self, value): environ = self.environ if 'wsgiorg.routing_args' in environ: environ['wsgiorg.routing_args'] = (environ['wsgiorg.routing_args'][0], value) <IF_STMT> del environ['paste.urlvars'] elif 'paste.urlvars' in environ: environ['paste.urlvars'] = value else: environ['wsgiorg.routing_args'] = ((), value)",if 'paste.urlvars' in environ:
"def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == 'conv': if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == 'norm' and norm and self.with_norm: x = self.norm(x) <IF_STMT> x = self.activate(x) return x",elif layer == 'act' and activate and self.with_activation:
"def add(self, entry): if not self._find_entry(entry, filters=False): show = self.add_show(entry) <IF_STMT> self._shows = None log.verbose('Successfully added show %s to Sonarr', show['title']) else: log.debug('entry %s already exists in Sonarr list', entry)",if show:
"def __eq__(self, other): if not isinstance(other, Result): return False equal = self.info == other.info equal &= self.stats == other.stats equal &= self.trajectories == other.trajectories for k in self.np_arrays: if k not in other.np_arrays: equal &= False break <IF_STMT> break equal &= all([np.array_equal(self.np_arrays[k], other.np_arrays[k])]) return equal",if not equal:
"def handle_server_api(output, kwargs): """"""Special handler for API-call 'set_config' [servers]"""""" name = kwargs.get('keyword') if not name: name = kwargs.get('name') if name: server = config.get_config('servers', name) <IF_STMT> server.set_dict(kwargs) old_name = name else: config.ConfigServer(name, kwargs) old_name = None sabnzbd.Downloader.update_server(old_name, name) return name",if server:
"def extractNames(self, names): offset = names['offset'].value for header in names.array('header'): key = header['nameID'].value foffset = offset + header['offset'].value field = names.getFieldByAddress(foffset * 8) <IF_STMT> continue value = field.value if key not in self.NAMEID_TO_ATTR: continue key = self.NAMEID_TO_ATTR[key] if key == 'version' and value.startswith(u'Version '): value = value[8:] setattr(self, key, value)",if not field or not isString(field):
"def api_read(self): files = [] files.append('/bin/netcat') files.append('/etc/alternative/netcat') files.append('/bin/nc') installed = False support = False path = None for _file in files: file_content = self.shell.read(_file) if file_content: installed = True path = _file <IF_STMT> support = True break result = {'netcat_installed': installed, 'supports_shell_bind': support, 'path': path} return result",if '-e filename' in file_content:
"def _get_iscsi_portal(self, netspace): for netpsace_interface in netspace.get_ips(): <IF_STMT> port = netspace.get_properties().iscsi_tcp_port return '%s:%s' % (netpsace_interface.ip_address, port) msg = _('No available interfaces in iSCSI network space %s') % netspace.get_name() raise exception.VolumeDriverException(message=msg)",if netpsace_interface.enabled:
def show(self): if len(self.figures.keys()) == 0: return if not SETTINGS.plot_split: if SETTINGS.plot_backend.lower() == 'qt4agg': self.tabbed_qt4_window() elif SETTINGS.plot_backend.lower() == 'qt5agg': self.tabbed_qt5_window() <IF_STMT> self.tabbed_tk_window() else: plt.show() else: plt.show(),elif SETTINGS.plot_backend.lower() == 'tkagg':
"def _update_decommissioned_icon(self): """"""Add or remove decommissioned icon."""""" if not self.instance.has_status_icon: return if self.is_active() is not self.__active: self.__active = not self.__active <IF_STMT> RemoveStatusIcon.broadcast(self, self.instance, DecommissionedStatus) else: self._add_status_icon(DecommissionedStatus(self.instance))",if self.__active:
"def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: <IF_STMT> if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i","if isinstance(child, six.string_types):"
"def test_read_lazy(self): want = b'x' * 100 telnet = test_telnet([want]) self.assertEqual(b'', telnet.read_lazy()) data = b'' while True: try: read_data = telnet.read_lazy() data += read_data <IF_STMT> telnet.fill_rawq() except EOFError: break self.assertTrue(want.startswith(data)) self.assertEqual(data, want)",if not read_data:
"def getprefs(path=PREFSFILENAME): if not os.path.exists(path): f = open(path, 'w') f.write(default_prefs) f.close() f = open(path) lines = f.readlines() prefs = {} for line in lines: <IF_STMT> line = line[:-1] try: name, value = re.split(':', line, 1) prefs[string.strip(name)] = eval(value) except: pass return prefs",if line[-1:] == '\n':
"def connect(self): while True: errno = self.sock.connect_ex(self.addr) <IF_STMT> break elif errno == EINPROGRESS: break elif errno == ENOENT: self.create_connection(self.failover_interval) return else: raise ValueError('Unexpected socket errno: %d' % errno) self.event_loop.watch_file(self.sock.fileno(), self.handle)",if not errno:
"def set_enabled_addons(file_path, addons, comment=None): with codecs.open(file_path, 'w', 'utf-8') as f: <IF_STMT> f.write('# %s\n\n' % comment) for addon in addons: f.write('%s\n' % addon)",if comment:
"def check_interfaceinNetWorkManager(self, interface): """"""check if interface is already in file config"""""" mac = Refactor.get_interface_mac(interface) if mac != None: if mac in open(self.mn_path, 'r').read(): return True <IF_STMT> return True return False","if interface in open(self.mn_path, 'r').read():"
"def spaceless(writer, node): original = writer.spaceless writer.spaceless = True writer.warn('entering spaceless mode with different semantics', node) nodelist = list(node.nodelist) if nodelist: <IF_STMT> nodelist[0] = TextNode(nodelist[0].s.lstrip()) if isinstance(nodelist[-1], TextNode): nodelist[-1] = TextNode(nodelist[-1].s.rstrip()) writer.body(nodelist) writer.spaceless = original","if isinstance(nodelist[0], TextNode):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <IF_STMT> self.set_queue_name(d.getPrefixedString()) continue if tt == 24: self.set_pause(d.getBoolean()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def group_re(self): """"""Return a regexp pattern with named groups"""""" out = '' for token, data in self.tokens(): if token == 'TXT': out += re.escape(data) <IF_STMT> out += '(?P<%s>%s)' % (data[1], data[0]) elif token == 'ANON': out += '(?:%s)' % data return out",elif token == 'VAR':
"def wrap_in(input): if isinstance(input, SymbolicInput): return input elif isinstance(input, gof.Variable): return SymbolicInput(input) elif isinstance(input, (list, tuple)): <IF_STMT> return SymbolicInput(input[0], update=input[1]) else: raise TypeError('Expected two elements in the list or tuple.', input) else: raise TypeError('Unknown input type: %s (%s), expected Variable instance', type(input), input)",if len(input) == 2:
"def _remove_event(self, event): i = bisect.bisect(self._eventq, event) while i > 0: i -= 1 e = self._eventq[i] <IF_STMT> raise exception.EventNotFound(event) elif id(e) == id(event): self._eventq.pop(i) return raise exception.EventNotFound(event)",if e.timestamp != event.timestamp:
"def cron_starter(*args: Any) -> None: _tz = self.conf.timezone if timezone is None else timezone while not self.should_stop: await self.sleep(cron.secs_for_next(cron_format, _tz)) if not self.should_stop: should_run = not on_leader or self.is_leader() <IF_STMT> with self.trace(shortlabel(fun), trace_enabled=traced): await fun(*args)",if should_run:
"def _find_boundary(self): ct_info = tuple((x.strip() for x in self.content_type.split(';'))) mimetype = ct_info[0] if mimetype.split('/')[0].lower() != 'multipart': raise NonMultipartContentTypeException(""Unexpected mimetype in content-type: '{0}'"".format(mimetype)) for item in ct_info[1:]: attr, value = _split_on_find(item, '=') <IF_STMT> self.boundary = encode_with(value.strip('""'), self.encoding)",if attr.lower() == 'boundary':
"def get_kwarg_or_param(request, kwargs, key): value = None try: value = kwargs[key] except KeyError: if request.method == 'GET': value = request.GET.get(key) <IF_STMT> value = request.POST.get(key) return value",elif request.method == 'POST':
"def _gather_async_results(self, result: Result, source: typing.Any) -> None: try: context = result['context'] context['is_refresh'] = False context['vars'] = self._vim.vars async_candidates = source.gather_candidates(context) context['vars'] = None result['is_async'] = context['is_async'] <IF_STMT> return context['candidates'] += convert2candidates(async_candidates) except Exception as exc: self._handle_source_exception(source, exc)",if async_candidates is None:
"def _check_session(self, session, action): if session is None: <IF_STMT> key = self[0].key else: key = self.key raise ValueError(f'Tileable object {key} must be executed first before {action}')","if isinstance(self, tuple):"
"def update(self, dict=None, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if dict is not None: <IF_STMT> dict = type({})(dict) for key, o in dict.items(): d[key] = KeyedRef(o, self._remove, key) if len(kwargs): self.update(kwargs)","if not hasattr(dict, 'items'):"
def get_sigma(self): if self.wants_automatic_sigma.value: if self.method == M_CANNY: return 1.0 <IF_STMT> return 2.0 else: raise NotImplementedError('Automatic sigma not supported for method %s.' % self.method.value) else: return self.sigma.value,elif self.method == M_LOG:
"def forward(self, x, activate=True, norm=True): for layer in self.order: <IF_STMT> if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == 'norm' and norm and self.with_norm: x = self.norm(x) elif layer == 'act' and activate and self.with_activation: x = self.activate(x) return x",if layer == 'conv':
def _grouping_intervals(grouping): last_interval = None for interval in grouping: <IF_STMT> return if interval == 0: if last_interval is None: raise ValueError('invalid grouping') while True: yield last_interval yield interval last_interval = interval,if interval == CHAR_MAX:
"def iterRelativeExportCFiles(basepath): for root, dirs, files in os.walk(basepath, topdown=True): for directory in dirs: <IF_STMT> dirs.remove(directory) for filename in files: if not isExportCFileIgnored(filename): fullpath = os.path.join(root, filename) yield os.path.relpath(fullpath, basepath)",if isAddonDirectoryIgnored(directory):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.add_application_key(d.getPrefixedString()) continue if tt == 18: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue <IF_STMT> self.set_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def _get_future_trading_minutes(self, trading_date): trading_minutes = set() universe = self._get_universe() for order_book_id in universe: <IF_STMT> continue trading_minutes.update(self._env.data_proxy.get_trading_minutes_for(order_book_id, trading_date)) return set([convert_int_to_datetime(minute) for minute in trading_minutes])",if self._env.get_account_type(order_book_id) == DEFAULT_ACCOUNT_TYPE.STOCK:
"def helper(chunk: Any) -> Any: nonlocal counter if not isinstance(chunk, dict): return chunk if len(chunk) <= 2: return chunk id = hash(str(chunk)) if id in cache: return cache[id] else: cache[id] = {'.id': counter} chunk['.cache_id'] = counter counter += 1 for name in sorted(chunk.keys()): value = chunk[name] <IF_STMT> chunk[name] = [helper(child) for child in value] elif isinstance(value, dict): chunk[name] = helper(value) return chunk","if isinstance(value, list):"
"def _render_lang_List(self, element): with self.buffer.foldable_lines(): self.buffer.write('[', style=self.styles.bracket) item_count = len(element.items) if item_count: with self.buffer.indent(): for idx, item in enumerate(element.items): self._render(item) if idx < item_count - 1: self.buffer.write(',') self.buffer.mark_line_break() <IF_STMT> self.buffer.write('...') self.buffer.write(']', style=self.styles.bracket)",if element.trimmed:
"def test_parse_query_params_matchable_field(self): query_params = {'filter[string_field][contains]': 'foo', 'filter[string_field][icontains]': 'bar'} fields = self.view.parse_query_params(query_params) for key, field_name in fields.items(): if field_name['string_field']['op'] == 'contains': assert_equal(field_name['string_field']['value'], 'foo') <IF_STMT> assert_equal(field_name['string_field']['value'], 'bar') else: self.fail()",elif field_name['string_field']['op'] == 'icontains':
"def receive(debug=debug): if should_shutdown and should_shutdown(): debug('worker got sentinel -- exiting') raise SystemExit(EX_OK) try: ready, req = _receive(1.0) <IF_STMT> return None except (EOFError, IOError) as exc: if get_errno(exc) == errno.EINTR: return None debug('worker got %s -- exiting', type(exc).__name__) raise SystemExit(EX_FAILURE) if req is None: debug('worker got sentinel -- exiting') raise SystemExit(EX_FAILURE) return req",if not ready:
"def test_all(self): raw = [r for r in self.map._revision_map.values() if r is not None] revs = [rev for rev in self.map.iterate_revisions('heads', 'base')] eq_(set(raw), set(revs)) for idx, rev in enumerate(revs): ancestors = set(self.map._get_ancestor_nodes([rev])).difference([rev]) descendants = set(self.map._get_descendant_nodes([rev])).difference([rev]) assert not ancestors.intersection(descendants) remaining = set(revs[idx + 1:]) <IF_STMT> assert remaining.intersection(ancestors)",if remaining:
"def is_issue(self, node): first = node.children[0] if first.type == 'string' and self._normalizer.version >= (3, 0): first_is_bytes = self._is_bytes_literal(first) for string in node.children[1:]: <IF_STMT> return True",if first_is_bytes != self._is_bytes_literal(string):
"def elements(registry): """"""Given a resource registry return sorted de-aliased values."""""" seen = {} for k, v in registry.items(): <IF_STMT> continue if v in seen: continue else: seen[ElementSchema.name(v)] = v return [seen[k] for k in sorted(seen)]","if k in ('and', 'or', 'not'):"
"def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: if isinstance(part, list): part = make_pattern(part) elif wtree[0] != '': for c in part: <IF_STMT> raise GlobError() subpattern.append(part) return ''.join(subpattern)",if c in special_chars:
"def check_if_list_contain_duplicates(item: list, depth: int) -> None: try: if len(item) != len(set(item)): print(Fore.RED + 'Rule {} has duplicate filters'.format(file)) files_with_duplicate_filters.append(file) except: for sub_item in item: <IF_STMT> check_list_or_recurse_on_dict(sub_item, depth + 1)",if type(sub_item) == dict and depth <= MAX_DEPTH:
"def PrintHighlighted(self, out): from doctools import make_help pos = self.start_pos for line_end in Lines(self.s, self.start_pos, self.end_pos): line = self.s[pos:line_end] html_line = make_help.HighlightLine(self.lang, line) <IF_STMT> out.PrintUntil(pos) out.Print(html_line) out.SkipTo(line_end) pos = line_end",if html_line is not None:
"def closeEvent(self, e=None): """"""Save settings and remove registered logging handler"""""" if self.editor.isModified(): <IF_STMT> if self.save(): e.accept() else: e.ignore() else: e.accept() else: e.accept()",if self.wants_save():
"def readlines(self, hint=None): if self.chunked_input: lines = [] for line in iter(self.readline, b''): lines.append(line) if hint and hint > 0: hint -= len(line) <IF_STMT> break return lines else: return self._do_read(self.rfile.readlines, hint)",if hint <= 0:
"def test_prod(self): with gpytorch.settings.fast_computations(covar_root_decomposition=False): lazy_tensor = self.create_lazy_tensor() evaluated = self.evaluate_lazy_tensor(lazy_tensor) <IF_STMT> self.assertAllClose(lazy_tensor.prod(-3).evaluate(), evaluated.prod(-3), **self.tolerances['prod']) if lazy_tensor.ndimension() > 3: self.assertAllClose(lazy_tensor.prod(-4).evaluate(), evaluated.prod(-4), **self.tolerances['prod'])",if lazy_tensor.ndimension() > 2:
"def make_module_translation_map(names: List[str]) -> Dict[str, str]: num_instances = {} for name in names: for suffix in candidate_suffixes(name): num_instances[suffix] = num_instances.get(suffix, 0) + 1 result = {} for name in names: for suffix in candidate_suffixes(name): <IF_STMT> result[name] = suffix break else: assert False, names return result",if num_instances[suffix] == 1:
"def output(self): """"""Transform self into a list of (name, value) tuples."""""" header_list = [] for k, v in self.items(): if isinstance(k, unicodestr): k = self.encode(k) <IF_STMT> v = str(v) if isinstance(v, unicodestr): v = self.encode(v) k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list","if not isinstance(v, basestring):"
"def get_errors(self, attacked_text, use_cache=False): text = attacked_text.text if use_cache: <IF_STMT> self.grammar_error_cache[text] = len(self.lang_tool.check(text)) return self.grammar_error_cache[text] else: return len(self.lang_tool.check(text))",if text not in self.grammar_error_cache:
"def gen(): for _ in range(256): if seq: yield self.tb.dut.i.eq(seq.pop(0)) i = (yield self.tb.dut.i) <IF_STMT> self.assertEqual(i, 0) else: o = (yield self.tb.dut.o) if o > 0: self.assertEqual(i & 1 << o - 1, 0) self.assertGreaterEqual(i, 1 << o) yield",if (yield self.tb.dut.n):
"def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: <IF_STMT> event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec if register_type is handlers.REGISTER_FIRST: self._events.register_first(event_name, handler) elif register_type is handlers.REGISTER_LAST: self._events.register_last(event_name, handler)",if len(spec) == 2:
"def is_checked_sls_template(template): if template.__contains__('provider'): <IF_STMT> if template['provider'].get('name').lower() not in SUPPORTED_PROVIDERS: return False if isinstance(template['provider'], str_node): if template['provider'] not in SUPPORTED_PROVIDERS: return False return True return False","if isinstance(template['provider'], dict_node):"
"def decode_body(self, response): if response is None: return response if six.PY2: return response if response.body: <IF_STMT> response._body = response.body.decode('utf-8') else: response._body = salt.ext.tornado.escape.native_str(response.body) return response",if response.headers.get('Content-Type') == 'application/json':
def get_active_project_path(): window = sublime.active_window() folders = window.folders() if len(folders) == 1: return folders[0] else: active_view = window.active_view() active_file_name = active_view.file_name() if active_view else None if not active_file_name: return folders[0] if len(folders) else os.path.expanduser('~') for folder in folders: <IF_STMT> return folder return os.path.dirname(active_file_name),if active_file_name.startswith(folder):
"def pop(self, *a): lists = self.lists if len(lists) == 1 and (not a): return self.lists[0].pop() index = a and a[0] if index == () or index is None or index == -1: ret = lists[-1].pop() if len(lists) > 1 and (not lists[-1]): lists.pop() else: list_idx, rel_idx = self._translate_index(index) <IF_STMT> raise IndexError() ret = lists[list_idx].pop(rel_idx) self._balance_list(list_idx) return ret",if list_idx is None:
"def setup(self, gen): Node.setup(self, gen) try: self.target = gen.rules[self.name] <IF_STMT> self.accepts_epsilon = self.target.accepts_epsilon gen.changed() except KeyError: (print >> sys.stderr, 'Error: no rule <%s>' % self.name) self.target = self",if self.accepts_epsilon != self.target.accepts_epsilon:
"def match(self, userargs): if len(self.args) != len(userargs): return False for pattern, arg in zip(self.args, userargs): try: <IF_STMT> break except re.error: return False else: return True return False","if not re.match(pattern + '$', arg):"
"def broadcast(self, msg, eid): for s in self.subs: if type(self.subs[s].eid) is list: if eid in self.subs[s].eid: self.subs[s].write_message(msg) el<IF_STMT> self.subs[s].write_message(msg)",if self.subs[s].eid == eid:
"def apply_transformation(self, ti: TransformationInput) -> Transformation: fragments = ti.fragments if fragments and fragment_list_to_text(fragments).startswith(' '): t = (self.style, self.get_char()) fragments = explode_text_fragments(fragments) for i in range(len(fragments)): <IF_STMT> fragments[i] = t else: break return Transformation(fragments)",if fragments[i][1] == ' ':
"def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): <IF_STMT> values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: if value is None: continue if not isinstance(key, bytes): key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield (url_quote_plus(key) + '=' + url_quote_plus(value))","if not isinstance(values, list):"
"def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]: rootSymbol = self.data['root_symbol'] for symbol in rootSymbol.get_all_symbols(): <IF_STMT> continue assert symbol.docname fullNestedName = symbol.get_full_nested_name() name = str(fullNestedName).lstrip('.') dispname = fullNestedName.get_display_string().lstrip('.') objectType = symbol.declaration.objectType docname = symbol.docname newestId = symbol.declaration.get_newest_id() yield (name, dispname, objectType, docname, newestId, 1)",if symbol.declaration is None:
"def _delete_duplicates(l, keep_last): """"""Delete duplicates from a sequence, keeping the first or last."""""" seen = {} result = [] if keep_last: l.reverse() for i in l: try: <IF_STMT> result.append(i) seen[i] = 1 except TypeError: result.append(i) if keep_last: result.reverse() return result",if i not in seen:
"def combine_logs(audit_logs, statement_text_logs): for audit_transaction in audit_logs: for audit_query in audit_logs[audit_transaction]: matching_statement_text_logs = statement_text_logs.get(hash(audit_query)) <IF_STMT> statement_text_log = matching_statement_text_logs.pop() if statement_text_log: if statement_text_log.start_time: audit_query.start_time = statement_text_log.start_time if statement_text_log.end_time: audit_query.end_time = statement_text_log.end_time",if matching_statement_text_logs:
"def free(self, addr, ban=0): with self.lock: if ban != 0: self.ban.append({'addr': addr, 'counter': ban}) else: base, bit, is_allocated = self.locate(addr) if len(self.addr_map) <= base: raise KeyError('address is not allocated') <IF_STMT> raise KeyError('address is not allocated') self.allocated -= 1 self.addr_map[base] ^= 1 << bit",if self.addr_map[base] & 1 << bit:
"def _assertParseMethod(test, code_str, method, expect_success=True): arena, c_parser = InitCommandParser(code_str) m = getattr(c_parser, method) node = m() if node: ast_lib.PrettyPrint(node) if not expect_success: test.fail('Expected %r to fail ' % code_str) else: err = c_parser.Error() print(err) ui.PrintErrorStack(err, arena, sys.stdout) <IF_STMT> test.fail('%r failed' % code_str) return node",if expect_success:
"def _gen(): while True: try: loop_val = it.next() except StopIteration: break self.mem.SetValue(lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly) <IF_STMT> b = self.EvalExpr(comp.cond) else: b = True if b: item = self.EvalExpr(node.elt) yield item",if comp.cond:
"def _build_default_obj_recursive(self, _properties, res): """"""takes disparate and nested default keys, and builds up a default object"""""" for key, prop in _properties.items(): <IF_STMT> res[key] = copy(prop['default']) elif prop.get('type') == 'object' and 'properties' in prop: res.setdefault(key, {}) res[key] = self._build_default_obj_recursive(prop['properties'], res[key]) return res",if 'default' in prop and key not in res:
"def mean(self): """"""Compute the mean of the value_field in the window."""""" if len(self.data) > 0: datasum = 0 datalen = 0 for dat in self.data: <IF_STMT> datasum += dat[1] datalen += 1 if datalen > 0: return datasum / float(datalen) return None else: return None",if 'placeholder' not in dat[0]:
"def addNames(self, import_names, node_names): for names in node_names: <IF_STMT> name = names elif names[1] is None: name = names[0] else: name = names[1] import_names[name] = True","if isinstance(names, basestring):"
"def set(sensor_spec: dict, **kwargs): for key, value in kwargs.items(): <IF_STMT> sensor_spec['transform'] = SensorSpecs.get_position(value) elif key == 'attachment_type': sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value] elif key == 'color_converter': sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]",if key == 'position':
"def delete_session(self): cookie = self.headers.get(HTTP_HEADER.COOKIE) if cookie: match = re.search('%s=(.+)' % SESSION_COOKIE_NAME, cookie) if match: session = match.group(1) <IF_STMT> del SESSIONS[session]",if session in SESSIONS:
"def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str): """""" """""" for op in block.ops: for input_name in op.input_arg_names: <IF_STMT> op._rename_input(old_name, new_name) for output_name in op.output_arg_names: if output_name == old_name: op._rename_output(old_name, new_name) block._rename_var(old_name, new_name)",if input_name == old_name:
"def updateParticle(part, best, phi1, phi2): u1 = numpy.random.uniform(0, phi1, len(part)) u2 = numpy.random.uniform(0, phi2, len(part)) v_u1 = u1 * (part.best - part) v_u2 = u2 * (best - part) part.speed += v_u1 + v_u2 for i, speed in enumerate(part.speed): <IF_STMT> part.speed[i] = math.copysign(part.smin, speed) elif abs(speed) > part.smax: part.speed[i] = math.copysign(part.smax, speed) part += part.speed",if abs(speed) < part.smin:
"def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(""can't specify timeout for non-blocking acquire"") rc = False endtime = None self._cond.acquire() while self._value == 0: if not blocking: break <IF_STMT> if endtime is None: endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc",if timeout is not None:
"def test_ESPnetDataset_text_float(text_float): dataset = IterableESPnetDataset(path_name_type_list=[(text_float, 'data8', 'text_float')], preprocess=preprocess) for key, data in dataset: <IF_STMT> assert all(data['data8'] == np.array([1.4, 3.4], dtype=np.float32)) if key == 'b': assert all(data['data8'] == np.array([0.9, 9.3], dtype=np.float32))",if key == 'a':
"def __eq__(self, other): if isinstance(other, OrderedDict): <IF_STMT> return False for p, q in zip(list(self.items()), list(other.items())): if p != q: return False return True return dict.__eq__(self, other)",if len(self) != len(other):
"def exec_command(command, cwd=None, stdout=None, env=None): """"""Returns True in the command was executed successfully"""""" try: command_list = command if isinstance(command, list) else command.split() env_vars = os.environ.copy() <IF_STMT> env_vars.update(env) subprocess.check_call(command_list, stdout=stdout, cwd=cwd, env=env_vars) return True except subprocess.CalledProcessError as err: print(err, file=sys.stderr) return False",if env:
"def _get_lun_details(self, lun_id): """"""Given the ID of a LUN, get the details about that LUN"""""" server = self.client.service res = server.LunListInfoIterStart(ObjectNameOrId=lun_id) tag = res.Tag try: res = server.LunListInfoIterNext(Tag=tag, Maximum=1) <IF_STMT> return res.Luns.LunInfo[0] finally: server.LunListInfoIterEnd(Tag=tag)","if hasattr(res, 'Luns') and res.Luns.LunInfo:"
"def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = (key.fileobj, key.data) if mask & selectors.EVENT_READ and reader is not None: if reader._cancelled: self.remove_reader(fileobj) else: self._add_callback(reader) <IF_STMT> if writer._cancelled: self.remove_writer(fileobj) else: self._add_callback(writer)",if mask & selectors.EVENT_WRITE and writer is not None:
"def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]: """"""Let the user process the docstrings before adding them."""""" for docstringlines in docstrings: if self.env.app: self.env.app.emit('autodoc-process-docstring', self.objtype, self.fullname, self.object, self.options, docstringlines) <IF_STMT> docstringlines.append('') yield from docstringlines",if docstringlines and docstringlines[-1] != '':
"def vectorize(self, doc, vocab, char_vocab): words = np.asarray([vocab[w.lower()] if w.lower() in vocab else 1 for w in doc]).reshape(1, -1) sentence_chars = [] for w in doc: word_chars = [] for c in w: <IF_STMT> _cid = char_vocab[c] else: _cid = 1 word_chars.append(_cid) sentence_chars.append(word_chars) sentence_chars = np.expand_dims(pad_sentences(sentence_chars, self.model.word_length), axis=0) return (words, sentence_chars)",if c in char_vocab:
"def runtestenv(venv, config, redirect=False): if venv.status == 0 and config.option.notest: venv.status = 'skipped tests' else: <IF_STMT> return config.pluginmanager.hook.tox_runtest_pre(venv=venv) if venv.status == 0: config.pluginmanager.hook.tox_runtest(venv=venv, redirect=redirect) config.pluginmanager.hook.tox_runtest_post(venv=venv)",if venv.status:
"def _import_config_module(self, name): try: self.find_module(name) except NotAPackage: <IF_STMT> reraise(NotAPackage, NotAPackage(CONFIG_WITH_SUFFIX.format(module=name, suggest=name[:-3])), sys.exc_info()[2]) reraise(NotAPackage, NotAPackage(CONFIG_INVALID_NAME.format(module=name)), sys.exc_info()[2]) else: return self.import_from_cwd(name)",if name.endswith('.py'):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_format(d.getVarInt32()) continue if tt == 18: self.set_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 8:
"def get(self, request, *args, **kwargs): self.sidebar_forms = [] for form_id, (plugin, Form) in self.get_sidebar_form_classes().items(): <IF_STMT> form = Form(self.article, self.request.user) setattr(form, 'form_id', form_id) else: form = None self.sidebar.append((plugin, form)) return super().get(request, *args, **kwargs)",if Form:
"def check_click(self): if not isinstance(self, SwiDebugView): return cursor = self.sel()[0].a index = 0 click_regions = self.get_regions('swi_log_clicks') for callback in click_regions: if cursor > callback.a and cursor < callback.b: <IF_STMT> callback = self.callbacks[index] callback['callback'](*callback['args']) index += 1",if index < len(self.callbacks):
"def get_sock(port): sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) while True: try: _port = port or random.randint(1025, 5000) print(('try bind local port:', _port)) sock.bind(('0.0.0.0', _port)) return sock except socket.error as e: <IF_STMT> print('bind local port %d fail: %r' % (_port, e)) return if e.args[0] == errno.EADDRINUSE: pass",if port:
"def ParsePlacemark(self, node): ret = Placemark() for child in node.childNodes: if child.nodeName == 'name': ret.name = self.ExtractText(child) <IF_STMT> ret.coordinates = self.ExtractCoordinates(child) return ret",if child.nodeName == 'Point' or child.nodeName == 'LineString':
"def _load_library(self): if self.library is not None: <IF_STMT> name, mod_path = self.library else: name = mod_path = self.library try: module = importlib.import_module(mod_path) except ImportError: raise ValueError(""Couldn't load %s password algorithm library"" % name) return module raise ValueError(""Hasher '%s' doesn't specify a library attribute"" % self.__class__)","if isinstance(self.library, (tuple, list)):"
"def check(self): for r in self.results: <IF_STMT> assert r.backend.name == self.target.path.k8s, (r.backend.name, self.target.path.k8s) assert r.backend.request.headers['x-envoy-original-path'][0] in (f'/{self.name}/', f'/{self.name}-nested/')",if r.backend:
"def eval(self, code, eval=True, raw=False): self._engine._append_source(code) try: result = self._context.eval(code) except quickjs.JSException as e: raise ProgramError(*e.args) else: if eval: if raw or not isinstance(result, quickjs.Object): return result <IF_STMT> return self.Function(self, result) else: return json.loads(result.json())",elif callable(result) and self.typeof(result) == u'function':
"def __truediv__(self, val): if isinstance(val, Vector3): <IF_STMT> raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr) else: if val is 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val) return Vector3.build_from_gdobj(gd_obj)",if val.x == 0 or val.y == 0 or val.z == 0:
"def set_peek(self, dataset, is_multi_byte=False): if not dataset.dataset.purged: dataset.peek = data.get_file_peek(dataset.file_name) <IF_STMT> dataset.blurb = '%s sequences' % util.commaify(str(dataset.metadata.sequences)) else: dataset.blurb = nice_size(dataset.get_size()) else: dataset.peek = 'file does not exist' dataset.blurb = 'file purged from disk'",if dataset.metadata.sequences:
"def _get_plugin_src_dirs(base_dir): plug_in_base_path = Path(get_src_dir(), base_dir) plugin_dirs = get_dirs_in_dir(str(plug_in_base_path)) plugins = [] for plugin_path in plugin_dirs: plugin_code_dir = Path(plugin_path, 'code') <IF_STMT> plugins.append(str(plugin_code_dir)) else: logging.warning('Plugin has no code directory: {}'.format(plugin_path)) return plugins",if plugin_code_dir.is_dir():
"def _format_privilege_data(self, data): for key in ['spcacl']: if key in data and data[key] is not None: if 'added' in data[key]: data[key]['added'] = parse_priv_to_db(data[key]['added'], self.acl) if 'changed' in data[key]: data[key]['changed'] = parse_priv_to_db(data[key]['changed'], self.acl) <IF_STMT> data[key]['deleted'] = parse_priv_to_db(data[key]['deleted'], self.acl)",if 'deleted' in data[key]:
"def __init__(self, methodName='runTest'): unittest.TestCase.__init__(self, methodName) test_dir = dirname(dirname(__file__)) self._dir = normpath(join(test_dir, 'stuff/charsets/www.kostis.net/charsets')) self._enc = {} names = os.listdir(self._dir) for name in names: <IF_STMT> continue enc = name.split('.')[0] if decoderAvailable(enc): self._enc[enc] = name","if not os.path.isfile(os.path.join(self._dir, name)):"
"def get_actions_on_list(self, actions, modelview_name): res_actions = dict() for action_key in actions: action = actions[action_key] <IF_STMT> res_actions[action_key] = action return res_actions","if self.is_item_visible(action.name, modelview_name) and action.multiple:"
"def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != 'OK': <IF_STMT> return elif time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()",if time_now - self.last_check_time < 3:
"def write(self, root): """"""Write all the *descendants* of an .dart node."""""" root_level = root.level() for p in root.subtree(): indent = p.level() - root_level self.put('%s %s' % ('*' * indent, p.h)) for s in p.b.splitlines(False): <IF_STMT> self.put(s) root.setVisited() return True",if not g.isDirective(s):
"def characters(self, ch): if self.Text_tag: <IF_STMT> self.Summary_ch += ch elif self.Attack_Prerequisite_tag: self.Attack_Prerequisite_ch += ch elif self.Solution_or_Mitigation_tag: self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch",if self.Summary_tag:
"def _handle_function(self, addr): if self.arch.name == 'X86': try: b = self._project.loader.memory.load(addr, 4) except KeyError: return except TypeError: return <IF_STMT> ebx_offset = self.arch.registers['ebx'][0] self.state.store_register(ebx_offset, 4, self.block.addr + self.block.size)",if b == b'\x8b\x1c$\xc3':
"def safe_makedir(dname): """"""Make a directory if it doesn't exist, handling concurrent race conditions."""""" if not dname: return dname num_tries = 0 max_tries = 5 while not os.path.exists(dname): try: os.makedirs(dname) except OSError: <IF_STMT> raise num_tries += 1 time.sleep(2) return dname",if num_tries > max_tries:
"def _setup_data(self, path): with PathManager.open(path) as data_file: <IF_STMT> line = data_file.readline() line = line[:line.rfind('{')] line = line[:line.rfind(',')] + ']' self.data = json.loads(line) else: self.data = json.load(data_file)",if 'extra' in path and 'train' in path:
"def _end_delimiter(state, token): py = state['pymode'] s = token.string l, c = token.start if len(py) > 1: mode, orig, match, pos = py.pop() <IF_STMT> e = '""{}"" at {} ends ""{}"" at {} (expected ""{}"")' return e.format(s, (l, c), orig, pos, match) else: return 'Unmatched ""{}"" at line {}, column {}'.format(s, l, c)",if s != match:
"def onLeftDoubleClick(self, event): row, _ = self.HitTest(event.Position) if row != -1: col = self.getColumn(event.Position) <IF_STMT> try: booster = self.boosters[row] except IndexError: return self.removeBoosters([booster])",if col != self.getColIndex(State):
"def get_instance_userdata(version='latest', sep=None, url='http://169.254.169.254', timeout=None, num_retries=5): ud_url = _build_instance_metadata_url(url, version, 'user-data') user_data = retry_url(ud_url, retry_on_404=False, num_retries=num_retries, timeout=timeout) if user_data: <IF_STMT> l = user_data.split(sep) user_data = {} for nvpair in l: t = nvpair.split('=') user_data[t[0].strip()] = t[1].strip() return user_data",if sep:
def parts(self): klass = self.__class__ this = list() for token in self: if token.startswith_fws(): <IF_STMT> yield (this[0] if len(this) == 1 else klass(this)) this.clear() end_ws = token.pop_trailing_ws() this.append(token) if end_ws: yield klass(this) this = [end_ws] if this: yield (this[0] if len(this) == 1 else klass(this)),if this:
"def run(self): while True: self._trigger.wait() self._trigger.clear() <IF_STMT> break for url in self.urls: logger.info('Pinging for problem update: %s', url) try: with closing(urlopen(url, data='')) as f: f.read() except Exception: logger.exception('Failed to ping for problem update: %s', url)",if self._terminate:
"def _get_trading_minutes(self, trading_date): trading_minutes = set() for account_type in self._config.base.accounts: if account_type == DEFAULT_ACCOUNT_TYPE.STOCK: trading_minutes = trading_minutes.union(self._get_stock_trading_minutes(trading_date)) <IF_STMT> trading_minutes = trading_minutes.union(self._get_future_trading_minutes(trading_date)) return sorted(list(trading_minutes))",elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE:
"def make_tree(self, node): if node is self.root: node.code = '' children = [] for bit in '01': next_code = node.code + bit <IF_STMT> child = Node(char=self.codes[next_code]) else: child = Node() child.code = next_code children.append(child) node.add(children) for child in children: if not child.is_leaf: self.make_tree(child)",if next_code in self.codes:
"def _merge(self, a, b, path=None): """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge"""""" if path is None: path = [] for key in b: if key in a: <IF_STMT> self._merge(a[key], b[key], path + [str(key)]) elif a[key] == b[key]: pass else: raise Exception('Conflict at %s' % '.'.join(path + [str(key)])) else: a[key] = b[key] return a","if isinstance(a[key], dict) and isinstance(b[key], dict):"
"def _append_value(generator, val=None): for example in generator: example = list(example) <IF_STMT> for key, value in val.items(): example[key] = np.append(example[key], value, -1) yield tuple(example)",if val is not None:
"def run(self): to_delete = set() for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'SubmissionFormatElement': to_delete.add(k) if v['_class'] == 'Task': v['submission_format'] = list((self.objs[k]['filename'] for k in v.get('submission_format', list()))) for k in to_delete: del self.objs[k] return self.objs",if k.startswith('_'):
"def service_destroy(context, service_id): session = get_session() with session.begin(): service_ref = service_get(context, service_id, session=session) service_ref.delete(session=session) <IF_STMT> for c in service_ref.compute_node: c.delete(session=session)",if service_ref.topic == 'compute' and service_ref.compute_node:
"def wiki(self, query): res = [] for entry in g.current_wiki.get_index(): name = filename_to_cname(entry['name']) name = re.sub('//+', '/', name) <IF_STMT> page = g.current_wiki.get_page(name) if page: res.append(dict(name=name, content=page.data)) return res","if set(query.split()).intersection(name.replace('/', '-').split('-')):"
"def numericalize(self, arr, device=None): if isinstance(arr[0][0], list): tmp = [super(BABI20Field, self).numericalize(x, device=device).data for x in arr] arr = torch.stack(tmp) <IF_STMT> arr = arr.contiguous() return arr else: return super(BABI20Field, self).numericalize(arr, device=device)",if self.sequential:
def validate_and_handle(self): valid = self.validate(set_cursor=True) if valid: if self.accept_handler: keep_text = self.accept_handler(self) else: keep_text = False <IF_STMT> self.reset(),if not keep_text:
"def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: <IF_STMT> return to_qvariant('Name') elif section == VERSION: return to_qvariant('Version') elif section == ACTION: return to_qvariant('Action') elif section == DESCRIPTION: return to_qvariant('Description') return to_qvariant()",if section == NAME:
"def replace(self, state): if state.key in self._dict: try: existing = self._dict[state.key] except KeyError: pass else: <IF_STMT> self._manage_removed_state(existing) else: return self._dict[state.key] = state self._manage_incoming_state(state)",if existing is not state:
"def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: <IF_STMT> line = line.strip() skip = False if skip_blanks: skip = line.isspace() or not line if not skip: yield line",if strip:
"def _get_workers_with_max_size(worker_to_size): """"""Get workers with maximal size"""""" max_workers = set() max_size = 0 for w, size in worker_to_size.items(): <IF_STMT> max_size = size max_workers = {w} elif size == max_size: max_workers.add(w) max_workers.difference_update([None]) return (max_size, list(max_workers))",if size > max_size:
def parse(self): while 1: l = self.f.readline() if not l: return l = l.strip() if l.startswith('['): self.parse_uuid(l) elif l.startswith('interface') or l.startswith('dispinterface'): self.parse_interface(l) <IF_STMT> self.parse_coclass(l),elif l.startswith('coclass'):
"def check_source_unit(self, source, unit): """"""Check source string."""""" rules = [FLAG_RULES[flag] for flag in unit.all_flags if flag in FLAG_RULES] if not rules: return False found = set() for regexp, is_position_based in rules: for match in regexp.finditer(source[0]): if is_position_based(match[1]): found.add((match.start(0), match.end(0))) <IF_STMT> return True return False",if len(found) >= 2:
"def parse_exprlist(self): list = [] while TRUE: self.reader.skip_white() c = self.reader.peek() <IF_STMT> break node = self.parse_expr() viml_add(list, node) return list","if c != '""' and self.ends_excmds(c):"
"def can_see_ban_details(request, profile): if request.user.is_authenticated: <IF_STMT> from .bans import get_user_ban return bool(get_user_ban(profile, request.cache_versions)) return False return False",if request.user_acl['can_see_ban_details']:
"def mouse_move(self, ips, x, y, btn, **key): if ips.roi == None: return lim = 5.0 / key['canvas'].get_scale() if btn == None: self.cursor = wx.CURSOR_CROSS <IF_STMT> self.cursor = wx.CURSOR_HAND elif btn == 1: if self.curobj: ips.roi.draged(self.odx, self.ody, x, y, ips.cur, self.curobj) ips.update() self.odx, self.ody = (x, y)","if ips.roi.snap(x, y, ips.cur, lim) != None:"
"def evex_mask_dest_reg_only(ii): i, m, xyz = (0, 0, 0) for op in _gen_opnds(ii): if op_mask_reg(op): m += 1 <IF_STMT> xyz += 1 elif op_imm8(op): i += 1 else: return False return m == 1 and xyz > 0 and (i <= 1)",elif op_xmm(op) or op_ymm(op) or op_zmm(op):
"def encode_datetime(self, dt, state): fmt = self.options.datetime_format is_iso = not fmt or fmt == 'iso' if is_iso: <IF_STMT> fmt = '%Y-%m-%dT%H:%M:%S%z' else: fmt = '%Y-%m-%dT%H:%M:%S.%f%z' s = dt.strftime(fmt) if is_iso and s.endswith('-00:00') or s.endswith('+00:00'): s = s[:-6] + 'Z' self.encode_string(s, state)",if dt.microsecond == 0:
"def main(config): with PathManager.open(config['infile'], 'r') as fin, PathManager.open(config['outfile'], 'w') as fout: for line in fin.readlines(): <IF_STMT> continue first_space = line.index(' ') first_tab = line.index('\t') candidate = line[first_space + 1:first_tab] fout.write(candidate + '\n')",if 'persona' in line:
"def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) if not value: continue if isinstance(value, list): value = HIDE_LIST <IF_STMT> value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append('%s=%s' % (key, value)) return '%s(%s)' % (record.__class__.__name__, ', '.join(parts))",elif key == FEATS:
"def make_chain(word): which = 1 while True: songs = find_songs_that_start_with_word(word) <IF_STMT> song = random.choice(songs) print(which, song['name'] + ' by ' + song['artists'][0]['name']) which += 1 word = song['name'].lower().split()[-1] else: break",if len(songs) > 0:
"def set_break(self, filename, lineno, temporary=False, cond=None, funcname=None): if isinstance(funcname, str): <IF_STMT> globals_ = globals() else: module = importlib.import_module(filename[:-3]) globals_ = module.__dict__ func = eval(funcname, globals_) code = func.__code__ filename = code.co_filename lineno = code.co_firstlineno funcname = code.co_name res = super(Bdb, self).set_break(filename, lineno, temporary=temporary, cond=cond, funcname=funcname) if isinstance(res, str): raise BdbError(res) return res",if filename == __file__:
"def __init__(self, shapefile=None, shapeType=POINT, autoBalance=1): self.autoBalance = autoBalance if not shapefile: Writer.__init__(self, shapeType) elif is_string(shapefile): base = os.path.splitext(shapefile)[0] <IF_STMT> r = Reader(base) Writer.__init__(self, r.shapeType) self._shapes = r.shapes() self.fields = r.fields self.records = r.records()",if os.path.isfile('%s.shp' % base):
"def test_env_not_set(self): with mock.patch.dict('os.environ'): <IF_STMT> del os.environ[self.env_name] self.assertEqual(helper.get_xdg_env(self.env_name, self.default), self.default)",if self.env_name in os.environ:
"def selection_only(self): selection_only = False sel = self.sel() if (self.context == 'selection' or self.context == 'both') and len(sel): if len(sel) > 1: selection_only = True elif self.threshold and (not sel[0].empty()): text = self.view.substr(sel[0]) match = re.search(self.threshold, text) <IF_STMT> selection_only = True else: selection_only = False return selection_only",if match:
"def __call__(self, rule, param): p, g = (param.data, param.grad) if p is None or g is None: return with chainer.using_device(param.device): xp = param.device.xp sign = xp.sign(p) <IF_STMT> kernel = cuda.elementwise('T s, T decay', 'T g', 'g += decay * s', 'lasso') kernel(sign, self.rate, g) else: g += self.rate * sign",if xp is cuda.cupy:
"def map_packages(shutit_pexpect_session, package_str, install_type): res = '' for package in package_str.split(): map_package_res = map_package(shutit_pexpect_session, package, install_type) <IF_STMT> return res res += ' ' + map_package_res return res",if map_package_res == '':
"def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) elif op_luf_start(op, 'GPRv'): types.append('v') elif op_luf_start(op, 'GPRz'): types.append('z') <IF_STMT> types.append('y') else: die('Unhandled op type {}'.format(op)) return types","elif op_luf_start(op, 'GPRy'):"
"def _process_archive(self, archive_stream, subtitle): for file_name in archive_stream.namelist(): <IF_STMT> logger.info('Found subtitle file %r', file_name) subtitle.content = fix_line_ending(archive_stream.read(file_name)) if subtitle.is_valid(): return","if file_name.lower().endswith(('.srt', '.sub')):"
"def truncate(self, size=None): with self._lock: if size is None: size = self.tell() with self.fs.openbin(self.path) as f: data = f.read(size) with self.fs.openbin(self.path, 'w') as f: f.write(data) <IF_STMT> f.write(b'\x00' * (size - len(data))) return size",if len(data) < size:
def wakeup(self): try: <IF_STMT> self.wm_withdraw() self.wm_deiconify() self.tkraise() self.focused_widget.focus_set() except TclError: pass,if self.wm_state() == 'iconic':
"def locus_parser(self): line = self.stream.readline() while line != '': line = line.rstrip() match = re.match(' Locus: (.+)', line) <IF_STMT> locus = match.group(1) alleles, table = _read_allele_freq_table(self.stream) return (locus, alleles, table) line = self.stream.readline() self.done = True raise StopIteration",if match is not None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_content(d.getPrefixedString()) continue if tt == 18: self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue <IF_STMT> self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 32:
"def concat_kernel_sources(self): func_sources = OrderedDict() for kernel in self.kernels: for func_name, source in kernel.func_sources.items(): <IF_STMT> assert func_sources[func_name] == source else: func_sources[func_name] = source self.generate_top_source() self.generate_exec_source() self.generate_init_source() combined_source = ''.join(self.header_sources.values()) + '\n'.join(func_sources.values()) + ''.join(self.footer_sources.values()) return combined_source",if func_name in func_sources:
"def parseUnderindentTag(self, s): tag = self.underindentEscapeString s2 = s[len(tag):] i = 0 while i < len(s2) and s2[i].isdigit(): i += 1 if i > 0: n = int(s2[:i]) <IF_STMT> i += 1 return (n, s2[i:]) else: return (0, s)",if i < len(s2) and s2[i] == '.':
"def load(self, data): ckey = None for key, val in _rx_cookie.findall(data): if key.lower() in _c_keys: if ckey: self[ckey][key] = _unquote(val) <IF_STMT> continue else: self[key] = _unquote(val) ckey = key",elif key[0] == '$':
"def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data['given'] for case in test_data['cases']: <IF_STMT> test_type = 'result' elif 'error' in case: test_type = 'error' elif 'bench' in case: test_type = 'bench' else: raise RuntimeError('Unknown test type: %s' % json.dumps(case)) yield (given, test_type, case)",if 'result' in case:
def delete(self): if not self.force and (not self.exists()): return [] cmd = ['delete'] if self.filename: cmd.append('--filename=' + self.filename) else: <IF_STMT> self.module.fail_json(msg='resource required to delete without filename') cmd.append(self.resource) if self.name: cmd.append(self.name) if self.label: cmd.append('--selector=' + self.label) if self.all: cmd.append('--all') if self.force: cmd.append('--ignore-not-found') return self._execute(cmd),if not self.resource:
"def validate_latex_theme_options(app: Sphinx, config: Config) -> None: for key in list(config.latex_theme_options): <IF_STMT> msg = __('Unknown theme option: latex_theme_options[%r], ignored.') logger.warning(msg % (key,)) config.latex_theme_options.pop(key)",if key not in Theme.UPDATABLE_KEYS:
"def connectionLost(self, reason): <IF_STMT> self.log.info(""WampRawSocketProtocol: connection lost: reason = '{0}'"".format(reason)) try: wasClean = isinstance(reason.value, ConnectionDone) self._session.onClose(wasClean) except Exception as e: if self.factory.debug: self.log.info('WampRawSocketProtocol: ApplicationSession.onClose raised ({0})'.format(e)) self._session = None",if self.factory.debug:
"def parse(filename): dead_links = [] with open(filename, 'r') as file_: for line in file_.readlines(): res = reference_line.search(line) <IF_STMT> if not exists(res.group(1)): dead_links.append(res.group(1)) return dead_links",if res:
"def is_speaker_at_session(self, session_id): try: session = Session.query.filter(Session.speakers.any(Speaker.user_id == self.id)).filter(Session.id == session_id).one() <IF_STMT> return True else: return False except MultipleResultsFound: return False except NoResultFound: return False",if session:
def _validate_deployment_name(namespace): if namespace.deployment_name is None: template_filename = None <IF_STMT> template_filename = namespace.template_file if namespace.template_uri and urlparse(namespace.template_uri).scheme: template_filename = urlsplit(namespace.template_uri).path if template_filename: template_filename = os.path.basename(template_filename) namespace.deployment_name = os.path.splitext(template_filename)[0] else: namespace.deployment_name = 'deployment1',if namespace.template_file and os.path.isfile(namespace.template_file):
"def mro(cls): if self.ready: if cls.__name__ == 'B1': B2.__bases__ = (B1,) <IF_STMT> B1.__bases__ = (B2,) return type.mro(cls)",if cls.__name__ == 'B2':
"def mark_shard_complete(): try: marker.refresh_from_db() except DeferIterationMarker.DoesNotExist: logger.warning('TaskMarker with ID: %s has vanished, cancelling task', marker_id) return marker.shards_complete += 1 marker.save() if marker.shards_complete == marker.shard_count: <IF_STMT> marker.delete() defer(finalize, *args, _transactional=True, _queue=task_queue_name(), **kwargs)",if marker.delete_on_completion:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_public_certificate_list().TryMerge(tmp) continue <IF_STMT> self.set_max_client_cache_time_in_second(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
"def check_free(self, payload): for m in self.free_list: args = m.split(',', 1) for arg in args: k, v = arg.split('=', 1) <IF_STMT> break else: return True return False",if payload[k] != v:
def getInnerText(element): text = '' child = element.firstChild while child: <IF_STMT> text += getInnerText(child) elif child.nodeValue: text += child.nodeValue child = child.nextSibling return text,if child.nodeType == 1:
"def get_complete_http(self): finished = [] c = self.connection.cursor() rows = c.execute('SELECT * FROM http WHERE complete=1').fetchall() for row in rows: o = pickle.loads(row['object']) uadat = c.execute('SELECT * FROM ua WHERE parent_id=?', (o.id,)).fetchall() for ua in uadat: uao = pickle.loads(ua['object']) <IF_STMT> o.add_ua_data(uao) finished.append(o) c.close() return finished",if uao is not None and uao.source_code is not None and o.source_code:
"def get_tools(self, found_files): self.configured_by = {} runners = [] for tool_name in self.tools_to_run: tool = tools.TOOLS[tool_name]() config_result = tool.configure(self, found_files) if config_result is None: configured_by = None messages = [] else: configured_by, messages = config_result <IF_STMT> messages = [] self.configured_by[tool_name] = configured_by self.messages += messages runners.append(tool) return runners",if messages is None:
"def _yield_batches(self, keys): while self._shuffling_buffer.can_retrieve(): post_shuffled_row = self._shuffling_buffer.retrieve() if not isinstance(post_shuffled_row, dict): post_shuffled_row = dict(zip(keys, post_shuffled_row)) self._batch_acc.append(post_shuffled_row) <IF_STMT> yield self.collate_fn(self._batch_acc) self._batch_acc = []",if len(self._batch_acc) == self.batch_size:
"def action_open_file_filtered_dialog(self, widget): try: fname = self.main_window.open_file_dialog(title='Open file with Toga', multiselect=False, file_types=['doc', 'txt']) <IF_STMT> self.label.text = 'File to open:' + fname else: self.label.text = 'No file selected!' except ValueError: self.label.text = 'Open file dialog was canceled'",if fname is not None:
"def validate_vars(env): """"""Validate the PCH and PCHSTOP construction variables."""""" if 'PCH' in env and env['PCH']: <IF_STMT> raise SCons.Errors.UserError('The PCHSTOP construction must be defined if PCH is defined.') if not SCons.Util.is_String(env['PCHSTOP']): raise SCons.Errors.UserError('The PCHSTOP construction variable must be a string: %r' % env['PCHSTOP'])",if 'PCHSTOP' not in env:
"def page_func(page_num): playlist = self._call_api('product/playlist', show_id, {'playListId': playlist_id, 'pageNumber': page_num, 'pageSize': 30, 'sorts': [{'order': 'DESC', 'type': 'SORTDATE'}]}) for product in playlist.get('productList', {}).get('products', []): product_url = product.get('productUrl', []).get('url') <IF_STMT> continue yield self.url_result(product_url, 'Shahid', str_or_none(product.get('id')), product.get('title'))",if not product_url:
"def forward(self, x): for rproj, conv in zip(self.residual_proj, self.conv_layers): residual = x x = conv(x) if self.skip_connections: <IF_STMT> residual = rproj(residual) x = (x + residual) * self.residual_scale return x",if rproj is not None:
"def _make_results_dir(self): """"""Makes directory for saving eqa-cnn-pretrain eval results."""""" for s_type in ['rgb', 'seg', 'depth']: dir_name = self.config.RESULTS_DIR.format(split='val', type=s_type) <IF_STMT> os.makedirs(dir_name)",if not os.path.isdir(dir_name):
"def ignore_callback_errors(self, ignore): EventEmitter.ignore_callback_errors.fset(self, ignore) for emitter in self._emitters.values(): if isinstance(emitter, EventEmitter): emitter.ignore_callback_errors = ignore <IF_STMT> emitter.ignore_callback_errors_all(ignore)","elif isinstance(emitter, EmitterGroup):"
"def cron_starter(*args: Any) -> None: _tz = self.conf.timezone if timezone is None else timezone while not self.should_stop: await self.sleep(cron.secs_for_next(cron_format, _tz)) <IF_STMT> should_run = not on_leader or self.is_leader() if should_run: with self.trace(shortlabel(fun), trace_enabled=traced): await fun(*args)",if not self.should_stop:
def rotateafter(self): if self.i != self.previ: i = self.parent.l.GetSelection() <IF_STMT> self.parent.models[self.parent.l.GetString(i)].rot -= 5 * (self.i - self.previ) self.previ = self.i self.Refresh(),if i != wx.NOT_FOUND:
"def select(model, path, iter_, paths_): paths, first = paths_ value = model.get_value(iter_) if value is None: return not bool(paths) value = normalize_path(value) if value in paths: self.get_child().get_selection().select_path(path) paths.remove(value) if not first: self.get_child().set_cursor(path) first.append(path.copy()) else: for fpath in paths: <IF_STMT> self.get_child().expand_row(path, False) return not bool(paths)",if fpath.startswith(value):
"def read_logs_file(logs_path) -> List[V1Log]: if not os.path.exists(logs_path): return [] async with aiofiles.open(logs_path, mode='r') as f: contents = await f.read() if contents: <IF_STMT> return V1Logs.read_csv(contents).logs logs = V1Logs.read(contents) return logs.logs return []",if '.plx' in logs_path:
"def adjust_sockets(self): variables = self.get_variables() for key in self.inputs.keys(): <IF_STMT> self.debug('Input {} not in variables {}, remove it'.format(key, str(variables))) self.inputs.remove(self.inputs[key]) for v in variables: if v not in self.inputs: self.debug('Variable {} not in inputs {}, add it'.format(v, str(self.inputs.keys()))) self.inputs.new('SvStringsSocket', v)",if key not in variables and key not in ['Field']:
def run(self): while self.running: cmd = self.cmds.get() <IF_STMT> break elif cmd == 'clear': dead_tasks = [] for task in self.tasks: if task.status == Task.FINISH or task.status == Task.ERROR: dead_tasks.append(task) for dead_task in dead_tasks: self.tasks.remove(dead_task),if cmd == 'stop':
"def process(self, node): self.vars = [] for child in node.childNodes: if child.nodeType == node.ELEMENT_NODE: child_text = get_xml_text(child) if child_text == '': continue <IF_STMT> for val in re.split('[\t ]+', child_text): self.vars.append(1.0 * eval(val)) return self",if child.nodeName == 'Real':
"def drain(self, fd): """"""Make `fd` unreadable."""""" while True: try: if not os.read(fd, 4096): return except OSError: e = sys.exc_info()[1] <IF_STMT> return raise",if e.args[0] == errno.EAGAIN:
"def parse(s): """"""Parse the output below to create a new StopWatch."""""" stopwatch = StopWatch() for line in s.splitlines(): <IF_STMT> parts = line.split(None) name = parts[0] if name != '%': rest = (float(v) for v in parts[2:]) stopwatch.times[parts[0]].merge(Stat.build(*rest)) return stopwatch",if line.strip():
"def delete(identifier, filenames=None, **kwargs): item = get_item(identifier) if filenames: <IF_STMT> filenames = [filenames] for f in item.iter_files(): if f.name not in filenames: continue f.delete(**kwargs)","if not isinstance(filenames, (set, list)):"
"def _get_absolute_timeout(self, timeout): if timeout is Timeout.DEFAULT_TIMEOUT: return 5 if isinstance(timeout, Timeout): <IF_STMT> warnings.warn('URLFetch does not support granular timeout settings, reverting to total timeout.', AppEnginePlatformWarning) return timeout.total return timeout",if timeout.read is not timeout.connect:
"def _add_annotation_to_imports(self, annotation: cst.Attribute) -> Union[cst.Name, cst.Attribute]: key = get_full_name_for_node(annotation.value) if key is not None: if key in self.existing_imports: return annotation import_name = get_full_name_for_node(annotation.attr) <IF_STMT> AddImportsVisitor.add_needed_import(self.context, key, import_name) return annotation.attr",if import_name is not None:
"def unique_definitions(cls, defns): """"""Takes a collection of defns and returns the unique list of defns."""""" unique_defns = [] for defn in defns: for unique_defn in unique_defns: <IF_STMT> break else: unique_defns.append(defn) return unique_defns",if unique_defn.path == defn.path and unique_defn == defn:
"def store_data(self, store_loc, **kwargs): """"""Put arrays to store"""""" g = self.store.create_group(store_loc) for k, v in kwargs.items(): <IF_STMT> if len(v) != 0: if type(v[0]) is np.str_ or type(v[0]) is str: v = [a.encode('utf8') for a in v] g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)",if type(v) == list:
"def connect_to_uri(self, uri, autoconnect=None, do_start=True): try: conn = self._check_conn(uri) if not conn: conn = self.add_conn(uri) <IF_STMT> conn.set_autoconnect(bool(autoconnect)) self.show_manager() if do_start: conn.open() return conn except Exception: logging.exception('Error connecting to %s', uri) return None",if autoconnect is not None:
def fn(n): while n < 3: if n < 0: yield 'less than zero' <IF_STMT> yield 'zero' elif n == 1: yield 'one' else: yield 'more than one' n += 1,elif n == 0:
"def closeEvent(self, e): self.common.log('MainWindow', 'closeEvent') if self.tabs.are_tabs_active(): self.common.log('MainWindow', 'closeEvent, opening warning dialog') self.close_dialog.exec_() <IF_STMT> self.system_tray.hide() e.accept() else: e.ignore() return self.system_tray.hide() e.accept()",if self.close_dialog.clickedButton() == self.close_dialog.accept_button:
"def _stop_child_activities(self, name=None): """"""Stop all child activities spawn by this activity."""""" for child_name, child in list(self._child_activity_map.items()): <IF_STMT> continue LOG.debug('%s: Stopping child activity %s ', self.name, child_name) if child.started: child.stop() self._child_activity_map.pop(child_name, None)",if name is not None and name != child_name:
"def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) <IF_STMT> fail('FATAL: bad directory %s in environment variable %s' % (dir, envvar))",elif fatal:
"def _serialize_list(array, previous): array = array or [] previous = previous or [] params = {} for i, v in enumerate(array): previous_item = previous[i] if len(previous) > i else None <IF_STMT> params[str(i)] = v.serialize(previous_item) else: params[str(i)] = _compute_diff(v, previous_item) return params","if hasattr(v, 'serialize'):"
"def list_bucket(self, prefix='', delimiter='', headers=None, all_versions=False): self._check_bucket_uri('list_bucket') bucket = self.get_bucket(headers=headers) if all_versions: return (v for v in bucket.list_versions(prefix=prefix, delimiter=delimiter, headers=headers) <IF_STMT>) else: return bucket.list(prefix=prefix, delimiter=delimiter, headers=headers)","if not isinstance(v, DeleteMarker)"
"def writeattr(stream, text): countdouble = text.count('""') if countdouble: countsingle = text.count(""'"") <IF_STMT> entities = {'""': '&quot;'} quote = '""' else: entities = {""'"": '&apos;'} quote = ""'"" else: entities = {} quote = '""' stream.write(quote) writetext(stream, text, entities) stream.write(quote)",if countdouble <= countsingle:
"def __gt__(self, other): if not isinstance(other, self.__class__): other = self.__class__(other) for part, value in self.parts: other_value = other[part] if part in LETTERS: cmp = self._cmp_part(value or 'z', other_value or 'z') else: cmp = self._cmp_part(value, other_value) <IF_STMT> continue else: return cmp == 1 return False",if cmp == 0:
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: if isinstance(t1, ptr_class) and isinstance(t2, ptr_class): return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) <IF_STMT> return t1 elif isinstance(t2, ptr_class): return t2 else: return ptr_class(BottomType()) return n_cls()","if isinstance(t1, ptr_class):"
"def __init__(self, items=None): super().__init__() self.include_dirs = [] self._add_member('src_files', FileList, 'C source files for VPI library') self._add_member('include_files', FileList, 'C include files for VPI library') self._add_member('libs', StringList, 'External libraries linked with the VPI library') if items: self.load_dict(items) <IF_STMT> self.include_dirs += unique_dirs(self.include_files) self.export_files = self.src_files + self.include_files",if self.include_files:
"def __init__(self, parent_element): if parent_element.items(): self.update(dict(parent_element.items())) for element in parent_element: if len(element) > 0: if element.tag == element[0].tag: aDict = ListParser(element) else: aDict = DictParser(element) <IF_STMT> aDict.update(dict(element.items())) self.update({element.tag: aDict}) elif element.items(): self.update({element.tag: dict(element.items())}) else: self.update({element.tag: element.text})",if element.items():
"def _shares_in_results(data): shares_in_device, shares_in_subdevice = (False, False) for plugin_name, plugin_result in data.iteritems(): if plugin_result['status'] == 'error': continue if 'device' not in plugin_result: continue if 'disk_shares' in plugin_result['device']: shares_in_device = True for subdevice in plugin_result['device'].get('subdevices', []): <IF_STMT> shares_in_subdevice = True break return (shares_in_device, shares_in_subdevice)",if 'disk_shares' in subdevice:
"def decorator(self, command, *args, **kwargs): if required_keys: missing_keys = diff_keys(required_keys, command) <IF_STMT> raise InvalidCommand('Command missing %s of required keys %s' % (missing_keys, required_keys)) return func(self, command, *args, **kwargs)",if missing_keys:
"def xml(self): out = ['<spreadsheet>'] for (x, y), cell in self.cells.items(): <IF_STMT> cellxml = cell.xml() else: cellxml = '<value>%s</value>' % escape(cell) out.append('<cell row=""%s"" col=""%s"">\n  %s\n</cell>' % (y, x, cellxml)) out.append('</spreadsheet>') return '\n'.join(out)","if hasattr(cell, 'xml'):"
"def speed_tester_d(self, uid): if uid not in self._speed_tester_d: <IF_STMT> self._speed_tester_d[uid] = SpeedTester(self._config.get('speed_limit_per_user', 0)) else: self._speed_tester_d[uid] = SpeedTester(self._config.get('speed_limit_per_user', 0)) return self._speed_tester_d[uid]",if self.mu:
"def process_error(self, data): error = data.get('error') if error: <IF_STMT> raise AuthCanceled(self) else: raise AuthUnknownError(self, 'Jawbone error was {0}'.format(error)) return super().process_error(data)",if error == 'access_denied':
"def _do_test_fetch_result(self, results, remote): self.assertGreater(len(results), 0) self.assertIsInstance(results[0], FetchInfo) for info in results: self.assertIsInstance(info.note, string_types) <IF_STMT> self.assertTrue(info.flags) self.assertIsInstance(info.ref, (SymbolicReference, Reference)) if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD): self.assertIsInstance(info.old_commit, Commit) else: self.assertIsNone(info.old_commit)","if isinstance(info.ref, Reference):"
"def init_ftp_server(self): if self.get_config('ftpd', 'enabled', False, boolean=True): accountfile = from_utf8_or_none(self.get_config('ftpd', 'accounts.file', None)) <IF_STMT> accountfile = abspath_expanduser_unicode(accountfile, base=self.basedir) accounturl = self.get_config('ftpd', 'accounts.url', None) ftp_portstr = self.get_config('ftpd', 'port', '8021') from allmydata.frontends import ftpd s = ftpd.FTPServer(self, accountfile, accounturl, ftp_portstr) s.setServiceParent(self)",if accountfile:
"def configured_request_log_handlers(config, prefix='query_log', default_logger=None): """"""Returns configured query loggers as defined in the `config`."""""" handlers = [] for section in config.sections(): if section.startswith(prefix): options = dict(config.items(section)) type_ = options.pop('type') <IF_STMT> logger = default_logger or get_logger() handler = ext.request_log_handler('default', logger) else: handler = ext.request_log_handler(type_, **options) handlers.append(handler) return handlers",if type_ == 'default':
"def string(self): """"""Returns a PlayString in string format from the Patterns values"""""" string = '' for item in self.data: if isinstance(item, (PGroup, GeneratorPattern)): string += item.string() <IF_STMT> string += '(' + ''.join([s.string() if hasattr(s, 'string') else str(s) for s in item.data]) + ')' else: string += str(item) return string","elif isinstance(item, Pattern):"
"def locked_deps(package, poetry): reqs = [] packages = poetry.locker.locked_repository(False).packages for p in packages: dep = p.to_dependency() line = '{}=={}'.format(p.name, p.version) requirement = dep.to_pep_508() <IF_STMT> line += '; {}'.format(requirement.split(';')[1].strip()) reqs.append(line) return (reqs, defaultdict(list))",if ';' in requirement:
"def _paste_columns(self, topleft_corner, columns): starting_column = topleft_corner[1] number_of_columns = self.number_of_columns() for index, column in enumerate(columns): set_index = starting_column + index <IF_STMT> self.set_column_at(set_index, column, starting=topleft_corner[0]) else: real_column = [constants.DEFAULT_NA] * topleft_corner[0] real_column += column self.extend_columns([real_column]) self.__width, self.__array = uniform(self.__array)",if set_index < number_of_columns:
"def check_objects_exist(self, compare_id, raise_exc=True): for uid in convert_compare_id_to_list(compare_id): if not self.existence_quick_check(uid): <IF_STMT> raise FactCompareException('{} not found in database'.format(uid)) return True return False",if raise_exc:
"def __add__(self, other): if hasattr(other, 'unit_type'): <IF_STMT> raise UnitError('Adding different types of units is not allowed') if other.unit != self.unit: other = other.to(self.unit) return self.__class__(np.array(self) + np.array(other), unit_type=self.unit_type, unit=self.unit)",if other.unit_type != self.unit_type:
"def extract(self, tar): max_nb = maxNbFile(self) for index, field in enumerate(tar.array('file')): <IF_STMT> self.warning('TAR archive contains many files, but only first %s files are processed' % max_nb) break meta = Metadata(self) self.extractFile(field, meta) if meta.has('filename'): title = _('File ""%s""') % meta.getText('filename') else: title = _('File') self.addGroup(field.name, meta, title)",if max_nb is not None and max_nb <= index:
"def task_management_menu(activation, request): """"""Available tasks actions."""""" actions = [] if request.user.has_perm(activation.flow_class._meta.manage_permission_name): for transition in activation.get_available_transitions(): <IF_STMT> url = activation.flow_task.get_task_url(activation.task, transition.name, user=request.user, namespace=request.resolver_match.namespace) if url: actions.append((transition.name.replace('_', ' ').title(), url)) return {'actions': actions, 'request': request}",if transition.can_proceed(activation):
"def handle_default_mac_address(facts): for suffix in ('', '_eth0', '_igb0', '_bnx0', '_bge0', '_nfo0', '_nge0'): mac = facts.get('macaddress{}'.format(suffix)) if mac: try: result = MACAddressField.normalize(mac) except ValueError: continue <IF_STMT> continue return result",if result[:6] in MAC_PREFIX_BLACKLIST:
"def run(self): consumer = KafkaConsumer(bootstrap_servers='localhost:9092', auto_offset_reset='earliest') consumer.subscribe(['my-topic']) self.valid = 0 self.invalid = 0 for message in consumer: <IF_STMT> self.valid += 1 else: self.invalid += 1 if consumer_stop.is_set(): break consumer.close()",if len(message.value) == msg_size:
"def createFields(self, fields): self.destroyFields() for name, label, args in fields: kwargs = dict(validator=_TransferValidator(name)) <IF_STMT> kwargs.update(args) stxt = wx.StaticText(self, -1, label) txt = wx.TextCtrl(self, **kwargs) self._contentSizer.Add(stxt, 0, wx.ALIGN_CENTER_VERTICAL | wx.ALIGN_RIGHT) self._contentSizer.Add(txt, 0, wx.EXPAND) self.__dict__[name] = '' self._fields[name] = (stxt, txt)",if args:
def poll_kafka(self): while True: val = self.do_poll() if val: yield self._emit(val) else: yield gen.sleep(self.poll_interval) <IF_STMT> break self._close_consumer(),if self.stopped:
def _generate_toc(line): while 1: if line.startswith('2'): line = 5 while 1: <IF_STMT> line = 6 break elif not line: line = 7 break elif not line: break return 1,if line:
def find_script(scriptId_or_file_or_url): for item in file_to_scriptId: if item['scriptId'].lower() == scriptId_or_file_or_url.lower(): return item['file'] <IF_STMT> return item['scriptId'] if item['url'].lower() == scriptId_or_file_or_url.lower(): return item['scriptId'] return None,if item['file'].lower() == scriptId_or_file_or_url.lower():
def __get_impute_number(some_data): impute_num_list = None data_size = None for line in some_data: processed_data = line[1][0] index_list = line[1][1] <IF_STMT> data_size = len(processed_data) impute_num_list = [0 for _ in range(data_size + 1)] impute_num_list[data_size] += 1 for index in index_list: impute_num_list[index] += 1 return np.array(impute_num_list),if not data_size:
"def get_shipping_address(self): """"""Returns Address object from shipping address fields if present"""""" for fieldname in ('shipping_address_name', 'shipping_address'): shipping_field = self.meta.get_field(fieldname) if shipping_field and shipping_field.fieldtype == 'Link': <IF_STMT> return frappe.get_doc('Address', self.get(fieldname)) return {}",if self.get(fieldname):
"def _get_spawn_property(self, constraints, constraint_name, services): if services: <IF_STMT> return services[0].image elif constraint_name == CPUS_CONSTRAINT: return services[0].cpus for constraint in constraints: if constraint.name == constraint_name: return constraint.value return None",if constraint_name == IMAGE_CONSTRAINT:
"def latest_extra_data(self, extra_dirs=None): base_name = os.path.splitext(os.path.basename(self.file_name))[0] extra_dirs.append(self.board.GetPlotOptions().GetOutputDirectory()) file_dir_name = os.path.dirname(self.file_name) directories = [file_dir_name] for dir in extra_dirs: if not os.path.isabs(dir): dir = os.path.join(file_dir_name, dir) <IF_STMT> directories.append(dir) return find_latest_schematic_data(base_name, directories)",if os.path.exists(dir):
"def _checkForLeftRightModifiers(cls, mod_state): mod_value = 0 mod_strs = [] for k, v in cls._OS_MODIFIERS: <IF_STMT> mod_value += KeyboardConstants._modifierCodes.getID(v) mod_strs.append(modifier_name_mappings.get(v, 'MISSING_MOD_NAME')) return (mod_value, mod_strs)",if mod_state & k > 0:
"def _decode_pattern_list(data): rv = [] contains_dict = False for item in data: <IF_STMT> item = _decode_pattern_list(item) elif isinstance(item, dict): item = _decode_pattern_dict(item) contains_dict = True rv.append(item) if not contains_dict: rv = sorted(rv) return rv","if isinstance(item, list):"
"def get_blob(self, blobname, ctlr=None, specific_dir=None): self._acquire_lock() try: dbsubpath = self._dbsubpath_from_blobname(blobname, ctlr=ctlr, specific_dir=specific_dir) <IF_STMT> return self.lang_zone.load_blob(dbsubpath) else: return None finally: self._release_lock()",if dbsubpath is not None:
"def get_tasks(self): for task in asyncio.all_tasks(loop=self.middleware.loop): formatted = None frame = None frames = [] for frame in task.get_stack(): cur_frame = get_frame_details(frame, self.logger) if cur_frame: frames.append(cur_frame) <IF_STMT> formatted = traceback.format_stack(frame) yield {'stack': formatted, 'frames': frames}",if frame:
"def main(args): optim = Adam({'lr': args.lr}) elbo = JitTrace_ELBO() if args.jit else Trace_ELBO() svi = SVI(model, guide, optim, loss=elbo) pyro.clear_param_store() for j in range(args.num_epochs): loss = svi.step(data) <IF_STMT> logging.info('[epoch %04d] loss: %.4f' % (j + 1, loss)) for name, value in pyro.get_param_store().items(): logging.info(name) logging.info(value.detach().cpu().numpy())",if j % 100 == 0:
"def create_var_list(scope, var_lists, shape): vars = [] for idx, v in enumerate(var_lists): name = '{}_{}'.format(scope, idx) <IF_STMT> var = fluid.data(name, shape=v.shape) else: var = fluid.data(name, shape=shape + list(v[0].shape)) var.stop_gradient = False vars.append(var) return vars",if shape is None:
"def dr_relation(self, C, trans, nullable): dr_set = {} state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: if p.lr_index < p.len - 1: a = p.prod[p.lr_index + 1] <IF_STMT> if a not in terms: terms.append(a) if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append('$end') return terms",if a in self.grammar.Terminals:
"def get_field_values(self, fields): field_values = [] for field in fields: <IF_STMT> value = self.get_title_display() elif field == 'country': try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = '' elif field == 'salutation': value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values",if field == 'title':
"def run(self, event, lambda_context): self.setup_exec_environment(event) resource_sets = self.get_resource_sets(event) result_sets = {} for (account_id, region), rarns in resource_sets.items(): self.assume_member({'account': account_id, 'region': region}) resources = self.resolve_resources(event) rset = result_sets.setdefault((account_id, region), []) <IF_STMT> rset.extend(self.run_resource_set(event, resources)) return result_sets",if resources:
"def read(self, sock): data = self.sock.recv(64 * 1024) ready_to_read, ready_to_write, in_error = select.select([self.sock], [], [], self.timeout) while len(ready_to_read) == 1: more_data = self.sock.recv(64 * 1024) <IF_STMT> break data = data + more_data ready_to_read, ready_to_write, in_error = select.select([self.sock], [], [], self.timeout) return data",if len(more_data) == 0:
"def _check_ids(el, filename, parent_id): """"""Recursively walks through tree and check if every object has ID"""""" for child in el: <IF_STMT> msg = 'Widget has no ID in %s; class %s; Parent id: %s' % (filename, child.attrib['class'], parent_id) assert 'id' in child.attrib and child.attrib['id'], msg for subel in child: if subel.tag == 'child': _check_ids(subel, filename, child.attrib['id'])",if child.tag == 'object':
"def get(self, request, *args, **kwargs): url = self.get_redirect_url(**kwargs) if url: <IF_STMT> return http.HttpResponsePermanentRedirect(url) else: return http.HttpResponseRedirect(url) else: logger.warning('Gone: %s' % self.request.path, extra={'status_code': 410, 'request': self.request}) return http.HttpResponseGone()",if self.permanent:
"def test_representation(self): for name in self.model.ssm.shapes.keys(): if name == 'obs': pass <IF_STMT> actual = self.results2.filter_results.obs_cov desired = np.diag(self.true_measurement_error_variances)[:, :, np.newaxis] assert_equal(actual, desired) else: assert_equal(getattr(self.results2.filter_results, name), getattr(self.results.filter_results, name))",elif name == 'obs_cov':
"def process_formdata(self, valuelist): if valuelist: date_str = ' '.join(valuelist) <IF_STMT> self.data = None raise ValidationError(self.gettext('Please input a date/time value')) parse_kwargs = self.parse_kwargs.copy() if 'default' not in parse_kwargs: try: parse_kwargs['default'] = self.default() except TypeError: parse_kwargs['default'] = self.default try: self.data = parser.parse(date_str, **parse_kwargs) except ValueError: self.data = None raise ValidationError(self.gettext('Invalid date/time input'))",if not date_str:
"def get_bounding_box(self): for key in self.h5f['Data_Products'].keys(): <IF_STMT> lats = self.h5f['Data_Products'][key][key + '_Gran_0'].attrs['G-Ring_Latitude'] lons = self.h5f['Data_Products'][key][key + '_Gran_0'].attrs['G-Ring_Longitude'] break else: raise KeyError('Cannot find bounding coordinates!') return (lons.ravel(), lats.ravel())",if key.startswith('VIIRS') and key.endswith('GEO'):
"def _get_doc_contents(self, attr_name): value = getattr(self, attr_name) if isinstance(value, BasicCommand.FROM_FILE): <IF_STMT> trailing_path = value.filename else: trailing_path = os.path.join(self.name, attr_name + '.rst') root_module = value.root_module doc_path = os.path.join(os.path.abspath(os.path.dirname(root_module.__file__)), 'examples', trailing_path) with _open(doc_path) as f: return f.read() else: return value",if value.filename is not None:
"def __truediv__(self, val): if isinstance(val, Vector3): if val.x == 0 or val.y == 0 or val.z == 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr) else: <IF_STMT> raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val) return Vector3.build_from_gdobj(gd_obj)",if val is 0:
def _get_all_plugin_configs(self): with opentracing.global_tracer().start_active_span('_get_all_plugin_configs'): <IF_STMT> self._plugin_configs = {pc.identifier: pc for pc in PluginConfiguration.objects.all()} return self._plugin_configs,"if not hasattr(self, '_plugin_configs'):"
"def msg(self, module, level, msg, *args, **kwargs): if self.level < level or level > len(LEVELS): return msg = str(msg).format(*args, **kwargs) with self.lock: self.output.write(FORMAT.format(module=module, level=LEVELS[level], msg=msg)) <IF_STMT> self.output.flush()","if hasattr(self.output, 'flush'):"
"def opentemplatefile(self, options, fulltemplatepath): """"""Opens the template file (if required)."""""" if fulltemplatepath is not None: <IF_STMT> return open(fulltemplatepath, 'r') else: self.warning('missing template file %s' % fulltemplatepath) return None",if os.path.isfile(fulltemplatepath):
"def b58(args, parser): for arg in args.input: blob, is_hex_input = parse_arg(arg, args.b) <IF_STMT> print(b2h(blob)) print(b2a_base58(blob)) print(b2a_hashed_base58(blob)) else: print(b2h(blob)) print(b2a_base58(blob)) try: blob = a2b_hashed_base58(arg) print('valid hashed b58') print('contents: ', b2h(blob)) except Exception: print('not hashed b58')",if is_hex_input:
"def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('{} {}'.format(shlex_quote(editor), shlex_quote(filename)), env=environ, shell=True) exit_code = c.wait() <IF_STMT> raise ClickException('{}: Editing failed!'.format(editor)) except OSError as e: raise ClickException('{}: Editing failed: {}'.format(editor, e))",if exit_code != 0:
"def ascii85decode(data): n = b = 0 out = '' for c in data: <IF_STMT> n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack('>L', b) n = b = 0 elif c == 'z': assert n == 0 out += '\x00\x00\x00\x00' elif c == '~': if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack('>L', b)[:n - 1] break return out",if '!' <= c and c <= 'u':
"def channel_to_netid(channel_name_or_id): try: channel = int(channel_name_or_id) except ValueError: netid = 'NETID_{}'.format(channel_name_or_id.upper()) <IF_STMT> channel = getattr(ics, netid) else: raise ValueError('channel must be an integer or a valid ICS channel name') return channel","if hasattr(ics, netid):"
"def _find_this_and_next_frame(self, stack): for i in range(len(stack)): <IF_STMT> if i == len(stack) - 1: return (stack[i], None) else: return (stack[i], stack[i + 1]) raise AssertionError(""Frame doesn't exist anymore"")",if stack[i].id == self._frame_id:
"def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): if isinstance(value, dict): if key in org_dict: <IF_STMT> raise ValueError('Mismatch between org_dict and upd_dict at node {}'.format(key)) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value","if not isinstance(org_dict[key], dict):"
"def __myreduce(self, elements): first = elements[0] for i in range(1, len(elements), 2): <IF_STMT> first = first and elements[i + 1] elif elements[i] == 'or': first = first or elements[i + 1] self.stack = [] if isinstance(first, list): return [first] return first",if elements[i] == 'and':
"def test_to_json_na(self): self.df.loc[self.df['BoroName'] == 'Queens', 'Shape_Area'] = np.nan text = self.df.to_json() data = json.loads(text) self.assertTrue(len(data['features']) == 5) for f in data['features']: props = f['properties'] self.assertEqual(len(props), 4) <IF_STMT> self.assertTrue(props['Shape_Area'] is None)",if props['BoroName'] == 'Queens':
"def process(self, resources): resources = self.filter_resources(resources, 'TableStatus', self.valid_status) if not len(resources): return futures = [] client = local_session(self.manager.session_factory).client('dynamodb') with self.executor_factory(max_workers=2) as w: for table_set in chunks(resources, 20): futures.append(w.submit(self.delete_table, client, table_set)) for f in as_completed(futures): <IF_STMT> self.log.error('Exception deleting dynamodb table set \n %s' % f.exception())",if f.exception():
"def skip_loss_scaling(self, backend_config=None): if self.loss_scaling is not False: <IF_STMT> msg = 'loss_scaling is tested when dtype is float16.' return (True, msg) if backend_config is not None and (not backend_config.use_cuda): msg = 'loss_scaling is tested when use_cuda is True.' return (True, msg) return (False, None)",if self.dtype != numpy.float16:
"def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None): progress = Progress(len(meshes), None) fp.write('\n  <library_controllers>\n') for mIdx, mesh in enumerate(meshes): subprog = Progress()(0, 0.5) if skel: writeSkinController(fp, human, mesh, skel, config) subprog(0.5, 1) <IF_STMT> writeMorphController(fp, mesh, shapes[mIdx], config) progress.step() fp.write('  </library_controllers>\n')",if shapes is not None:
"def doit(): recipes_path = expanduser('recipes.pprint') recipe_dicts = eval(open(recipes_path).read()) for r in recipe_dicts: for key in r.keys(): <IF_STMT> del r[key] for c in r['comments']: for key in c.keys(): if key not in ('comment', 'title'): del c[key] f = open('stripped.pprint', 'w') f.write(pformat(recipe_dicts)) f.close()","if key not in ('desc', 'comments'):"
"def _dispatchBubblingEvent(self, tag, evtType, evtObject): for node in tag.parents: <IF_STMT> break if not node._listeners: continue if evtObject._stoppedPropagation: continue capture_listeners, bubbling_listeners = self._get_listeners(node, evtType) for c in bubbling_listeners: evtObject.currentTarget = node._node self.do_dispatch(c, evtObject)",if node is None:
"def connect(self): if self.session is None: self.session = requests.Session() <IF_STMT> self.session.mount('httpsds8k://', requests.adapters.HTTPAdapter()) else: self.session.mount('https://', requests.adapters.HTTPAdapter()) self.session.verify = self.verify","if isinstance(self.verify, six.string_types):"
"def get_latest_tasks(cls, tasks): tasks_group = {} for task in tasks: task_key = cls.task_key(task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id) if task_key not in tasks_group: tasks_group[task_key] = task <IF_STMT> tasks_group[task_key] = task return tasks_group",elif task.f_task_version > tasks_group[task_key].f_task_version:
"def wrapper(cached=True, reset=False): nonlocal cached_venv_dir if not cached or not cached_venv_dir or reset: venv_dir = os.environ.get('_VENV_DIR_') or load_settings(lazy=True).get('venv_dir') if venv_dir: <IF_STMT> venv_dir = VENV_DIR_ISOLATED elif venv_dir == 'shared': venv_dir = VENV_DIR_SHARED else: venv_dir = VENV_DIR_SHARED cached_venv_dir = venv_dir return cached_venv_dir",if venv_dir == 'isolated':
"def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug('__walk_dir_tree. START dir=%s', dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith('py'): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) <IF_STMT> dir_list.append((f, ret)) return dir_list",if ret:
"def read_ansible_config(project_path, variables_of_interest): fnames = ['/etc/ansible/ansible.cfg'] if project_path: fnames.append(os.path.join(project_path, 'ansible.cfg')) values = {} try: parser = ConfigParser() parser.read(fnames) <IF_STMT> for var in variables_of_interest: if var in parser['defaults']: values[var] = parser['defaults'][var] except Exception: logger.exception('Failed to read ansible configuration(s) {}'.format(fnames)) return values",if 'defaults' in parser:
"def inference(self, x_all, data_loader): for i in range(len(self.convs)): output = [] for src_id, edge_index, size in data_loader: x = x_all[src_id].to(self.device) edge_index = edge_index.to(self.device) x = self.convs[i](x, edge_index) x = x[:size[1]] <IF_STMT> x = F.relu(x) output.append(x.cpu()) x_all = torch.cat(output, dim=0) return F.log_softmax(x_all, dim=-1)",if i != self.num_layers - 1:
"def guard_transform(transform): """"""Return an Affine transformation instance."""""" if not isinstance(transform, Affine): <IF_STMT> raise TypeError('GDAL-style transforms have been deprecated.  This exception will be raised for a period of time to highlight potentially confusing errors, but will eventually be removed.') else: transform = Affine(*transform) return transform",if tastes_like_gdal(transform):
"def _tokenize(self, text): if tf.is_tensor(text): rank = len(text.shape) <IF_STMT> return self._tokenize_tensor(text) elif rank == 1: return self._tokenize_batch_tensor(text) else: raise ValueError('Unsupported tensor rank %d for tokenization' % rank) elif isinstance(text, list): return list(map(self.tokenize, text)) else: text = tf.compat.as_text(text) return self._tokenize_string(text)",if rank == 0:
def validate_export(namespace): destination = namespace.destination if destination == 'file': if namespace.path is None or namespace.format_ is None: raise CLIError('usage error: --path PATH --format FORMAT') elif destination == 'appconfig': <IF_STMT> raise CLIError('usage error: --config-name NAME | --connection-string STR') elif destination == 'appservice': if namespace.appservice_account is None: raise CLIError('usage error: --appservice-account NAME_OR_ID'),if namespace.dest_name is None and namespace.dest_connection_string is None:
"def dispatch(self, request, *args, **kwargs): settings = self.get_settings(self.form_class.settings) initial = self.get_initial_form_data(settings) form = self.form_class(request=request, initial=initial) if request.method == 'POST': form = self.form_class(request.POST, request.FILES, request=request, initial=initial) <IF_STMT> form.save(settings) messages.success(request, _('Settings have been saved.')) return redirect(request.path_info) return self.render(request, {'form': form, 'form_settings': settings})",if form.is_valid():
"def get_modules(path): modules = set() for dirpath, dirnames, filenames in os.walk(path): for filename in filenames: <IF_STMT> cutoff = len(path) + 1 fullpath = os.path.join(dirpath[cutoff:], filename) modules.add(fullpath) return modules",if filename.endswith('.py'):
"def _make_input_layers(self, rebuild=False): for name, layer in self.layer_map.items(): layer.left_in_edges = len(layer.in_edges) if len(layer.in_edges) == 0: if rebuild: <IF_STMT> self.input_layers.append(name) else: self.input_layers.append(name)",if not layer.get_attr('scope'):
"def _get_status(self): connection_errors_allowed = 10 while True: try: content = requests.get(self.__status_details_url).json() except (requests.ConnectionError, requests.HTTPError) as e: <IF_STMT> yield e content = {'processed': False, 'code': 'being_processed'} connection_errors_allowed -= 1 yield content",if not connection_errors_allowed:
def show(self): if len(self.figures.keys()) == 0: return if not SETTINGS.plot_split: if SETTINGS.plot_backend.lower() == 'qt4agg': self.tabbed_qt4_window() <IF_STMT> self.tabbed_qt5_window() elif SETTINGS.plot_backend.lower() == 'tkagg': self.tabbed_tk_window() else: plt.show() else: plt.show(),elif SETTINGS.plot_backend.lower() == 'qt5agg':
"def emit(self, record): msg = self.format(record) self.lock.acquire() try: msg = self.encode(msg) <IF_STMT> self.perform_rollover() self.write(msg) self.flush() finally: self.lock.release()","if self.should_rollover(record, len(msg)):"
"def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext if hasattr(names, '__contains__'): <IF_STMT> __builtin__.__dict__['gettext'] = __builtin__.__dict__['_'] if 'ngettext' in names: __builtin__.__dict__['ngettext'] = unicode and self.ungettext or self.ngettext if 'lgettext' in names: __builtin__.__dict__['lgettext'] = self.lgettext if 'lngettext' in names: __builtin__.__dict__['lngettext'] = self.lngettext",if 'gettext' in names:
"def test_simulate_moment_steps_set_state(dtype): q0, q1 = cirq.LineQubit.range(2) circuit = cirq.Circuit(cirq.H(q0), cirq.H(q1), cirq.H(q0), cirq.H(q1)) simulator = cirq.Simulator(dtype=dtype) for i, step in enumerate(simulator.simulate_moment_steps(circuit)): np.testing.assert_almost_equal(step.state_vector(), np.array([0.5] * 4)) <IF_STMT> step.set_state_vector(np.array([1, 0, 0, 0], dtype=dtype))",if i == 0:
"def get_config_settings(): config = {} for plugin in extension_loader.MANAGER.plugins: fn_name = plugin.name function = plugin.plugin <IF_STMT> fn_module = importlib.import_module(function.__module__) if hasattr(fn_module, 'gen_config'): config[fn_name] = fn_module.gen_config(function._takes_config) return yaml.safe_dump(config, default_flow_style=False)","if hasattr(function, '_takes_config'):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 18: self.set_queue_name(d.getPrefixedString()) continue if tt == 24: self.set_pause(d.getBoolean()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def enable(self): """"""enable the patch."""""" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: <IF_STMT> return if not self.PY2: raise IncompatiblePatch('Python 2 not supported!') if pyv == 3: if self.PY3 == SKIP: return if not self.PY3: raise IncompatiblePatch('Python 3 not supported!') self.pre_enable() self.do_enable() self.enabled = True",if self.PY2 == SKIP:
"def to_dict(self) -> JSONDict: data = dict() for key in iter(self.__dict__): if key == 'bot' or key.startswith('_'): continue value = self.__dict__[key] <IF_STMT> if hasattr(value, 'to_dict'): data[key] = value.to_dict() else: data[key] = value if data.get('from_user'): data['from'] = data.pop('from_user', None) return data",if value is not None:
"def _resolve_result(self, f=None): try: if f: results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) if self._single_result: r = results[0] <IF_STMT> raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)","if isinstance(r, Exception):"
"def print_monitor(args): from pylearn2.utils import serial import gc for model_path in args: if len(args) > 1: print(model_path) model = serial.load(model_path) monitor = model.monitor del model gc.collect() channels = monitor.channels <IF_STMT> print('old file, not all fields parsed correctly') else: print('epochs seen: ', monitor._epochs_seen) print('time trained: ', max((channels[key].time_record[-1] for key in channels))) for key in sorted(channels.keys()): print(key, ':', channels[key].val_record[-1])","if not hasattr(monitor, '_epochs_seen'):"
"def apply(self, **kwargs: Any) -> None: for node in self.document.traverse(addnodes.index): <IF_STMT> msg = __('4 column based index found. It might be a bug of extensions you use: %r') % node['entries'] logger.warning(msg, location=node) for i, entry in enumerate(node['entries']): if len(entry) == 4: node['entries'][i] = entry + (None,)",if 'entries' in node and any((len(entry) == 4 for entry in node['entries'])):
"def cleanup_empty_directories(path: str): """"""Remove all empty folders inside (and including) 'path'"""""" path = os.path.normpath(path) while 1: repeat = False for root, dirs, files in os.walk(path, topdown=False): <IF_STMT> try: remove_dir(root) repeat = True except: pass if not repeat: break if not os.listdir(path): try: remove_dir(path) except: pass",if not dirs and (not files) and (root != path):
"def expect_flow_sequence_item(self): if isinstance(self.event, SequenceEndEvent): self.indent = self.indents.pop() self.flow_level -= 1 <IF_STMT> self.write_indicator(u',', False) self.write_indent() self.write_indicator(u']', False) self.state = self.states.pop() else: self.write_indicator(u',', False) if self.canonical or self.column > self.best_width: self.write_indent() self.states.append(self.expect_flow_sequence_item) self.expect_node(sequence=True)",if self.canonical:
"def test_loss_diff(self): losses = [] for use_cuda in [True, False]: for use_py_func_op in [True, False]: L = test_main(use_cuda, use_py_func_op, self.use_parallel_executor) <IF_STMT> losses.append(L) for idx in six.moves.range(len(losses) - 1): max_diff = np.max(np.abs(losses[idx] - losses[0])) self.assertAlmostEqual(max_diff, 0, delta=0.001)",if L is not None:
"def check_file(f, path): if not (ignore_substring and ignore_substring in f): if substring in f: compl_path = os.path.join(path, f) <IF_STMT> return compl_path return False",if os.path.isfile(compl_path):
"def is_valid_block(self): """"""check wheter the block is valid in the current position"""""" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): <IF_STMT> return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True",if self.block.pos.x + i < 0:
"def is_fail_state(state): if type(state.addr) == SootAddressDescriptor and state.addr.method == SootMethodDescriptor.from_soot_method(onclick_method): sols = state.solver.eval_upto(state.memory_soot.stack.load('$z0'), 2) assert len(sols) == 1 <IF_STMT> return True return False",if sols[0] == 0:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.add_delete_status(d.getVarInt32()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _init_weight(self): for m in self.modules(): <IF_STMT> n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2.0 / n)) elif isinstance(m, SyncBatchNorm): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_()","if isinstance(m, nn.Conv2d):"
"def wrapper(*args, **kwargs): global _exception try: fn(*args, **kwargs) except Exception: _exception = sys.exc_info() et, ev, tb = _exception if getattr(ev, 'filename', None) is None: filename = traceback.extract_tb(tb)[-1][0] else: filename = ev.filename <IF_STMT> _error_files.append(filename) raise",if filename not in _error_files:
"def purge_messages(self): with self.app.connection_for_write() as connection: count = self.app.control.purge(connection=connection) <IF_STMT> print('purge: Erased {0} {1} from the queue.\n'.format(count, pluralize(count, 'message')))",if count:
"def read_series(rec): found = [] for tag in ('440', '490', '830'): fields = rec.get_fields(tag) if not fields: continue for f in fields: this = [] for k, v in f.get_subfields(['a', 'v']): if k == 'v' and v: this.append(v) continue v = v.rstrip('.,; ') if v: this.append(v) <IF_STMT> found += [' -- '.join(this)] return found",if this:
def calc_position_values(positions): values = [] for position in positions: <IF_STMT> values.append(0.0) else: values.append(position.last_sale_price * position.amount) return values,"if isinstance(position.asset, Future):"
"def _loc(obj): try: fn = getattr(obj, '__file__', None) if fn is not None: return ' @%s' % (fn,) obj = getattr(obj, 'im_func', obj) code = getattr(obj, '__code__', None) <IF_STMT> return ' @%s:%s' % (code.co_filename, code.co_firstlineno) except Exception: pass return ''",if code is not None:
"def _convert_user_into_remote(self, username, exclude=['all']): remotes = {remote.name: list(remote.urls) for remote in self.repository.remotes} for name in (self.name, 'upstream') + tuple(remotes.keys()): if name in remotes and name not in exclude: for url in remotes[name]: <IF_STMT> yield name",if self.fqdn in url and username == url.split('/')[-2].split(':')[-1]:
"def _extract_level(self): """"""Extract level and component if available (lazy)."""""" if self._level is None: split_tokens = self.split_tokens if not split_tokens: self._level = False self._component = False return x = self.log_levels.index(split_tokens[1]) if split_tokens[1] in self.log_levels else None <IF_STMT> self._level = split_tokens[1] self._component = split_tokens[2] else: self._level = False self._component = False",if x is not None:
"def addnode(self, parent, data): print('aaa', data) for i in data: print(i) <IF_STMT> continue if isinstance(i, tuple): item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0]) self.addnode(item, i[1]) else: item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0])",if i == '-':
"def getdsturl(tcpdata): import logging log = logging.getLogger('getdsturl') p = parseHeader(tcpdata, type='request') if p is None: log.warn('parseHeader returned None') return if p.has_key('uri') and p.has_key('headers'): <IF_STMT> r = 'http://%s%s' % (p['headers']['host'][0], p['uri']) return r else: log.warn('seems like no host header was set') else: log.warn('parseHeader did not give us a nice return %s' % p)",if p['headers'].has_key('host'):
"def assert_not_none(obj, msg=None, values=True): """"""Fail the test if given object is None."""""" _msg = 'is None' if obj is None: if msg is None: msg = _msg <IF_STMT> msg = '%s: %s' % (msg, _msg) _report_failure(msg)",elif values is True:
"def sort(self): sorted_models = [] concrete_models = set() models = list(self.data) while len(sorted_models) < len(models): found = False for model in models: if model in sorted_models: continue dependencies = self.dependencies.get(model._meta.concrete_model) <IF_STMT> sorted_models.append(model) concrete_models.add(model._meta.concrete_model) found = True if not found: return self.data = OrderedDict(((model, self.data[model]) for model in sorted_models))",if not (dependencies and dependencies.difference(concrete_models)):
"def load_vocab_dict(vocab_file_path): """"""Load vocabs, vocab: {""word"": 1, ...}"""""" logging.info('Loading vocab from {}'.format(vocab_file_path)) with open(vocab_file_path, encoding='utf-8') as in_f: vocabs = {} for line in in_f: parts = line.rstrip().split('\t') <IF_STMT> continue vocabs[parts[0]] = parts[1] logging.info('Loded {} vocabs from {}'.format(len(vocabs), vocab_file_path)) return vocabs",if len(parts) < 2:
"def get_layers_from_suite(self, suite, suiteClass): top_layer = suiteClass() layers_dict = OrderedDict() for test in self.flatten_suite(suite): layer = getattr(test, 'layer', None) <IF_STMT> if layer not in layers_dict: layers_dict[layer] = LayerSuite(self.session, layer=layer) layers_dict[layer].addTest(test) else: top_layer.addTest(test) self.get_parent_layers(layers_dict) return (top_layer, layers_dict)",if layer:
"def team_scores(self, team_scores, time): """"""Store output of team scores to a JSON file"""""" data = [] for score in team_scores['fixtures']: <IF_STMT> item = {'date': score['date'].split('T')[0], 'homeTeamName': score['homeTeamName'], 'goalsHomeTeam': score['result']['goalsHomeTeam'], 'goalsAwayTeam': score['result']['goalsAwayTeam'], 'awayTeamName': score['awayTeamName']} data.append(item) self.generate_output({'team_scores': data})",if score['status'] == 'FINISHED':
"def run(self, root): footnotesDiv = self.footnotes.makeFootnotesDiv(root) if footnotesDiv is not None: result = self.footnotes.findFootnotesPlaceholder(root) if result: child, parent, isText = result ind = list(parent).index(child) <IF_STMT> parent.remove(child) parent.insert(ind, footnotesDiv) else: parent.insert(ind + 1, footnotesDiv) child.tail = None else: root.append(footnotesDiv)",if isText:
"def delete_target_group(self, target_group_arn): if target_group_arn not in self.target_groups: raise TargetGroupNotFoundError() target_group = self.target_groups[target_group_arn] if target_group: <IF_STMT> raise ResourceInUseError(""The target group '{}' is currently in use by a listener or a rule"".format(target_group_arn)) del self.target_groups[target_group_arn] return target_group",if self._any_listener_using(target_group_arn):
"def run_pending(self, now=None): """"""Runs the command if scheduled"""""" now = now or datetime.now() if self.is_enabled(): <IF_STMT> self.last_run = now next_time = self.schedule(self.last_run).get_next() if next_time < now: self.last_run = now return self.run() return -1",if self.last_run is None:
"def _fix_exception_context(new_exc, old_exc): while 1: exc_context = new_exc.__context__ <IF_STMT> return if exc_context is None or exc_context is frame_exc: break new_exc = exc_context new_exc.__context__ = old_exc",if exc_context is old_exc:
"def delete_backend(self, backend_tag: BackendTag, force_kill: bool=False) -> Optional[GoalId]: async with self.write_lock: for endpoint, info in self.endpoint_state.get_endpoints().items(): <IF_STMT> raise ValueError(""Backend '{}' is used by endpoint '{}' and cannot be deleted. Please remove the backend from all endpoints and try again."".format(backend_tag, endpoint)) return self.backend_state.delete_backend(backend_tag, force_kill)",if backend_tag in info['traffic'] or backend_tag in info['shadows']:
"def lint(self, request): try: html_linter = UnwrapObject(self._koLintService.getLinterForLanguage('HTML')) return html_linter.lint(request, TPLInfo=self._tplPatterns) except: <IF_STMT> self._checkValidVersion_complained['lint'] = True log.exception('Problem in koPHPLinter.lint') return koLintResults()",if 'lint' not in self._checkValidVersion_complained:
"def get_commit(self, rev): """"""Get commit object identified by `rev` (SHA or branch or tag name)."""""" for prefix in ['refs/heads/', 'refs/tags/', '']: key = prefix + rev try: obj = self[encode_for_git(key)] <IF_STMT> obj = self[obj.object[1]] return obj except KeyError: pass raise KeyError(rev)","if isinstance(obj, dulwich.objects.Tag):"
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url, timeout=1).json().get('config', {}) <IF_STMT> meta['nomad_version'] = resp.get('Version') if 'Region' in resp: meta['nomad_region'] = resp.get('Region') if 'Datacenter' in resp: meta['nomad_datacenter'] = resp.get('Datacenter') except Exception as ex: self.log.debug('Error getting Nomad version: %s' % str(ex)) return meta",if 'Version' in resp:
"def _waitFakenetStopped(self, timeoutsec=None): retval = False while True: <IF_STMT> retval = True break time.sleep(1) if timeoutsec is not None: timeoutsec -= 1 if timeoutsec <= 0: break return retval",if self._confirmFakenetStopped():
"def send_message(self, message): smtp = smtplib.SMTP(self.smtp_host, self.smtp_port) try: smtp.ehlo() if self.smtp_tls: smtp.starttls() <IF_STMT> smtp.login(self.smtp_user, self.smtp_password) smtp.sendmail(self.from_user, self.recipients, message.as_string()) finally: smtp.close()",if self.smtp_user:
"def set_tracker_icon(tracker_icon, cell): if tracker_icon: pixbuf = tracker_icon.get_cached_icon() <IF_STMT> pixbuf = get_pixbuf_at_size(tracker_icon.get_filename(), 16) tracker_icon.set_cached_icon(pixbuf) else: pixbuf = create_blank_pixbuf() with warnings.catch_warnings(): warnings.simplefilter('ignore') cell.set_property('pixbuf', pixbuf)",if pixbuf is None:
"def __create_index(self, collection, index, unique): doc = collection.find_one(projection={'_id': 1}) if doc is None: try: indexes = list(collection.list_indexes()) except OperationFailure: indexes = [] <IF_STMT> collection.create_index(index, unique=unique)",if index not in indexes:
"def read_oclc(fields): if '035' not in fields: return {} found = [] for line in fields['035']: for v in get_subfield_values(line, ['a']): m = re_oclc.match(v) <IF_STMT> continue oclc = m.group(1) if oclc not in found: found.append(oclc) return {'oclc_number': found} if found else {}",if not m:
"def closest_enemy_ant(self, row1, col1, filter=None): min_dist = maxint closest_ant = None for ant in self.enemy_ants(): <IF_STMT> dist = self.distance(row1, col1, ant[0][0], ant[0][1]) if dist < min_dist: min_dist = dist closest_ant = ant[0] return closest_ant",if filter is None or ant not in filter:
"def fromVariant(variant): if hasattr(QtCore, 'QVariant') and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) <IF_STMT> return variant.toDouble()[0] elif t == QtCore.QVariant.Int: return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() elif t == QtCore.QVariant.Invalid: return None else: raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName()) else: return variant",elif t == QtCore.QVariant.Double:
"def _check_old_with_state(self): add_vec = False for op in self.ops: <IF_STMT> try: op.get_coeff(0.0, self.args) except TypeError as e: nfunc = _StateAsArgs(self.coeff) op = EvoElement((op.qobj, nfunc, nfunc, 'func')) add_vec = True if add_vec: self.dynamics_args += [('_state_vec', 'vec', None)]",if op.type == 'func':
"def _read_readable(self, readable): blocksize = 8192 if self.debuglevel > 0: print('sendIng a read()able') encode = self._is_textIO(readable) if encode and self.debuglevel > 0: print('encoding file using iso-8859-1') while True: datablock = readable.read(blocksize) <IF_STMT> break if encode: datablock = datablock.encode('iso-8859-1') yield datablock",if not datablock:
"def read_chat_forever(reader, pub_socket): line = reader.readline() who = 'someone' while line: print('Chat:', line.strip()) if line.startswith('name:'): who = line.split(':')[-1].strip() try: pub_socket.send_pyobj((who, line)) except socket.error as e: <IF_STMT> raise line = reader.readline() print('Participant left chat.')",if e[0] != 32:
"def _wrapped() -> None: should_run = app.is_leader() if on_leader else True if should_run: with self.trace(shortlabel(fun), trace_enabled=traced): <IF_STMT> task_takes_app = cast(Callable[[AppT], Awaitable], fun) return await task_takes_app(app) else: task = cast(Callable[[], Awaitable], fun) return await task()",if inspect.signature(fun).parameters:
"def Decode(self, filedesc): while True: chunk = filedesc.Read(4) if not chunk: return if chunk == b'QUUX': yield b'NORF' <IF_STMT> yield b'BLARGH'",if chunk == b'THUD':
"def _get_modules(fn): finder = modulefinder.ModuleFinder() finder.run_script(fn) all = [] for m in finder.modules.values(): if not isinstance(m, modulefinder.Module): continue <IF_STMT> continue if m.__file__.endswith('.so'): continue if m.__file__.startswith('/Library/Frameworks'): continue all.append(m) return all",if not m.__file__:
"def _read(self, size): """"""Return size bytes from the stream."""""" if self.comptype == 'tar': return self.__read(size) c = len(self.dbuf) while c < size: buf = self.__read(self.bufsize) <IF_STMT> break try: buf = self.cmp.decompress(buf) except IOError: raise ReadError('invalid compressed data') self.dbuf += buf c += len(buf) buf = self.dbuf[:size] self.dbuf = self.dbuf[size:] return buf",if not buf:
def cluster_list(tokeniser): clusterids = [] value = tokeniser() try: if value == '[': while True: value = tokeniser() <IF_STMT> break clusterids.append(ClusterID(value)) else: clusterids.append(ClusterID(value)) if not clusterids: raise ValueError('no cluster-id in the cluster list') return ClusterList(clusterids) except ValueError: raise ValueError('invalud cluster list'),if value == ']':
"def from_data(cls, value, currency, includes_tax=None): if includes_tax is None: <IF_STMT> msg = 'Missing includes_tax argument for %s.from_data' raise TypeError(msg % (cls.__name__,)) includes_tax = cls.includes_tax if includes_tax: return TaxfulPrice(value, currency) else: return TaxlessPrice(value, currency)",if cls.includes_tax is None:
"def THUMB(image, nx=120, ny=120, gae=False, name='thumb'): if image: <IF_STMT> request = current.request from PIL import Image import os img = Image.open(os.path.join(request.folder, 'uploads', image)) img.thumbnail((nx, ny), Image.ANTIALIAS) root, ext = os.path.splitext(image) thumb = '%s_%s%s' % (root, name, ext) img.save(request.folder + 'uploads/' + thumb) return thumb else: return image",if not gae:
"def _get_two_devices(self, require_same_type=False): tpus = extensions.tpu_devices() if FLAGS.requires_tpu: if len(tpus) == 2: res = tpus else: raise ValueError('This test requires 2 TPU cores but %s are found' % len(tpus)) elif len(tpus) == 2: res = tpus <IF_STMT> res = ('CPU:0', 'GPU:0') else: res = ('CPU:0', 'CPU:1') return res",elif self._hasGPU() and (not require_same_type):
"def _format_repos(self, value): result = {} if value: for path, config in iteritems(value): <IF_STMT> path = os.path.abspath(__import__(path).__file__) result[path] = config return result",if path[0] != '/':
"def skipIndent(self, s, i, width): ws = 0 n = len(s) while i < n and ws < width: <IF_STMT> ws += abs(self.tab_width) - ws % abs(self.tab_width) elif s[i] == ' ': ws += 1 else: break i += 1 return i",if s[i] == '\t':
"def get_assets_historical_range_close_price(self, start_dt, end_dt, asset_symbols, adjusted=False): """""" """""" prices_df = None for ds in self.data_sources: try: prices_df = ds.get_assets_historical_closes(start_dt, end_dt, asset_symbols, adjusted=adjusted) <IF_STMT> return prices_df except Exception: raise return prices_df",if prices_df is not None:
"def matchBrackets(string): rest = string[1:] inside = '(' while rest != '' and (not rest.startswith(')')): <IF_STMT> part, rest = matchBrackets(rest) inside = inside + part else: inside = inside + rest[0] rest = rest[1:] if rest.startswith(')'): return (inside + ')', rest[1:]) raise AssertionError(""Unmatched bracket in string '"" + string + ""'"")",if rest.startswith('('):
"def is_different(item, seen): is_diff = True if item not in seen: for value in other: <IF_STMT> is_diff = False break if is_diff: seen.append(item) return is_diff","if comparator(iteratee(item), iteratee(value)):"
"def write_conditional_formatting(worksheet): """"""Write conditional formatting to xml."""""" wb = worksheet.parent for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules): cf = Element('conditionalFormatting', {'sqref': range_string}) for rule in rules: if rule.dxf is not None: <IF_STMT> rule.dxfId = len(wb._differential_styles) wb._differential_styles.append(rule.dxf) cf.append(rule.to_tree()) yield cf",if rule.dxf != DifferentialStyle():
"def checkForFinishedThreads(self): """"""Mark terminated threads with endTime."""""" for t in self.unfinishedThreads: <IF_STMT> t.endTime = time.process_time() if getattr(t, 'status', None) is None: t.status = 'ended'",if not t.is_alive():
"def _process_dispatch_entries(self, dispatch_info_external): path_only_entries = [] hostname_entries = [] for entry in dispatch_info_external.dispatch: parsed_url = dispatchinfo.ParsedURL(entry.url) <IF_STMT> hostname_entries.append(entry) else: path_only_entries.append((parsed_url, entry.server)) if hostname_entries: logging.warning('Hostname routing is not supported by the development server. The following dispatch entries will not match any requests:\n%s', '\n\t'.join((str(entry) for entry in hostname_entries))) self._entries = path_only_entries",if parsed_url.host:
"def iter_ReassignParameters(self, inputNode, variables, nodeByID): for node in inputNode.getReassignParameterNodes(nodeByID): yield from iterNodeCommentLines(node) yield from iterInputConversionLines(node, variables) socket = node.inputs[0] <IF_STMT> expression = getCopyExpression(socket, variables) else: expression = variables[socket] if node.conditionSocket is None: conditionPrefix = '' else: conditionPrefix = 'if {}: '.format(variables[node.conditionSocket]) yield '{}{} = {}'.format(conditionPrefix, variables[node.linkedParameterSocket], expression)",if socket.isUnlinked and socket.isCopyable():
"def _feed_data(self, data_pair: types.Sequence, type_: str) -> types.Sequence: result = [] type_list = [ChartType.LINES, ChartType.CUSTOM] if type_ in type_list: result = data_pair else: for n, v in data_pair: try: lng, lat = self.get_coordinate(n) result.append({'name': n, 'value': [lng, lat, v]}) except TypeError as err: <IF_STMT> raise NonexistentCoordinatesException(err, (n, v)) return result",if self._is_ignore_nonexistent_coord is not True:
"def _parse_whois(self, txt): asn, desc = (None, b'') for l in txt.splitlines(): if not asn and l.startswith(b'origin:'): asn = l[7:].strip().decode('utf-8') if l.startswith(b'descr:'): if desc: desc += b'\\n' desc += l[6:].strip() <IF_STMT> desc = desc.strip().decode('utf-8') break return (asn, desc)",if asn is not None and desc.strip():
"def _resolve_result(self, f=None): try: <IF_STMT> results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) if self._single_result: r = results[0] if isinstance(r, Exception): raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)",if f:
"def new_org(type=ORG_DEFAULT, block=True, **kwargs): if type == ORG_DEFAULT: org = reserve_pooled(type=type, **kwargs) <IF_STMT> org = queue.reserve('queued_org', block=block, type=type, **kwargs) if org: new_pooled() return org org = Organization(type=type, **kwargs) org.initialize() org.commit() return org else: org = Organization(type=type, **kwargs) org.queue_initialize(block=block) return org",if not org:
"def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] <IF_STMT> args = rulesList[ruleIndex][-1] if maxLength == 4: shouldRunMethod, method, isCorrect = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) elif maxLength == 3: shouldRunMethod, method = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking",if len(rulesList[ruleIndex]) == maxLength:
"def setHighlightedItem(self, item): if item != None: for listItem in self.children.getItems(): <IF_STMT> self.children.setCurrentItem(listItem) return else: self.children.setCurrentItem(None)","if self.loadHandler.matchesItem(listItem, item):"
"def getForts(location): global forts lforts = [] for i in forts: f = (i['latitude'], i['longitude']) d = vincenty(location, f).meters <IF_STMT> lforts.append(i) return lforts",if d < 900:
"def page_file(self, page): if page.isroot: raise PathLookupError('Can not export: %s', page) elif self.namespace: <IF_STMT> name = page.relname(self.namespace) else: raise PathLookupError('%s not a child of %s' % (page, self.namespace)) else: name = page.name return self.dir.file(encode_filename(name) + '.' + self.ext)",if page.ischild(self.namespace):
"def to_json_dict(self): d = super().to_json_dict() if self.header is not None: if isinstance(self.header, RenderedContent): d['header'] = self.header.to_json_dict() else: d['header'] = self.header if self.subheader is not None: <IF_STMT> d['subheader'] = self.subheader.to_json_dict() else: d['subheader'] = self.subheader d['text'] = RenderedContent.rendered_content_list_to_json(self.text) return d","if isinstance(self.subheader, RenderedContent):"
def fixfunnychars(addr): i = 0 while i < len(addr): c = addr[i] <IF_STMT> c = '-' addr = addr[:i] + c + addr[i + 1:] i = i + len(c) return addr,if c not in goodchars:
"def refactor_stdin(self, doctests_only=False): input = sys.stdin.read() if doctests_only: self.log_debug('Refactoring doctests in stdin') output = self.refactor_docstring(input, '<stdin>') <IF_STMT> self.processed_file(output, '<stdin>', input) else: self.log_debug('No doctest changes in stdin') else: tree = self.refactor_string(input, '<stdin>') if self.write_unchanged_files or (tree and tree.was_changed): self.processed_file(str(tree), '<stdin>', input) else: self.log_debug('No changes in stdin')",if self.write_unchanged_files or output != input:
"def test_compute_gradient(self): for y, y_pred in zip(self.y_list, self.predict_list): lse_grad = self.lae_loss.compute_grad(y, y_pred) diff = y_pred - y <IF_STMT> grad = 1 elif diff < consts.FLOAT_ZERO: grad = -1 else: grad = 0 self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)",if diff > consts.FLOAT_ZERO:
"def restart(self): try: try: <IF_STMT> try: self.daemon.stop() except: pass except: self.log.critical(traceback.format_exc()) logging.shutdown() args = [sys.executable] + [os.path.join(base_path, os.path.basename(__file__))] + sys.argv[1:] subprocess.Popen(args) except: self.log.critical(traceback.format_exc())",if self.runAsDaemon():
"def classifyws(s, tabwidth): raw = effective = 0 for ch in s: <IF_STMT> raw = raw + 1 effective = effective + 1 elif ch == '\t': raw = raw + 1 effective = (effective // tabwidth + 1) * tabwidth else: break return (raw, effective)",if ch == ' ':
"def code_match(code, select, ignore): if ignore: assert not isinstance(ignore, unicode) for ignored_code in [c.strip() for c in ignore]: if mutual_startswith(code.lower(), ignored_code.lower()): return False if select: assert not isinstance(select, unicode) for selected_code in [c.strip() for c in select]: <IF_STMT> return True return False return True","if mutual_startswith(code.lower(), selected_code.lower()):"
"def get_tokens_unprocessed(self, text): from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): <IF_STMT> token = Name.Function elif token is Name and value in ASYVARNAME: token = Name.Variable yield (index, token, value)",if token is Name and value in ASYFUNCNAME:
"def makeDataURI(data=None, mimetype=None, filename=None): import base64 if not mimetype: <IF_STMT> import mimetypes mimetype = mimetypes.guess_type(filename)[0].split(';')[0] else: raise Exception('You need to provide a mimetype or a filename for makeDataURI') return 'data:' + mimetype + ';base64,' + ''.join(base64.encodestring(data).split())",if filename:
"def add_attributes(attributes, all_base64): lines = [] oc_attr = None for attr in attributes: <IF_STMT> for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) oc_attr = attr break for attr in attributes: if attr != oc_attr and attr in attributes: for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) return lines",if attr.lower() == 'objectclass':
"def read_optional_seed(fill, base='', ext='', timeout=5): try: md, ud, vd = read_seeded(base, ext, timeout) fill['user-data'] = ud fill['vendor-data'] = vd fill['meta-data'] = md return True except url_helper.UrlError as e: <IF_STMT> return False raise",if e.code == url_helper.NOT_FOUND:
"def _get_spawn_property(self, constraints, constraint_name, services): if services: if constraint_name == IMAGE_CONSTRAINT: return services[0].image <IF_STMT> return services[0].cpus for constraint in constraints: if constraint.name == constraint_name: return constraint.value return None",elif constraint_name == CPUS_CONSTRAINT:
def delete_api(self): retries = 0 while retries < 10: try: self.client.delete_rest_api(restApiId=self.api_id) break except exceptions.ClientError as e: <IF_STMT> retries += 1 time.sleep(5) else: raise,if e.response['Error']['Code'] == 'TooManyRequestsException':
"def GetSelected(self): if self.GetStyleL('style') & self.Style.LBS_MULTIPLESEL: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) <IF_STMT> return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) if result != LB_ERR: return result",if result:
"def compare_objects(left, right): left_fields = left.map_value.fields right_fields = right.map_value.fields for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)): keyCompare = Order._compare_to(left_key, right_key) if keyCompare != 0: return keyCompare value_compare = Order.compare(left_fields[left_key], right_fields[right_key]) <IF_STMT> return value_compare return Order._compare_to(len(left_fields), len(right_fields))",if value_compare != 0:
"def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) <IF_STMT> types.append('v') elif op_luf_start(op, 'GPRz'): types.append('z') elif op_luf_start(op, 'GPRy'): types.append('y') else: die('Unhandled op type {}'.format(op)) return types","elif op_luf_start(op, 'GPRv'):"
"def _iter_indented_subactions(self, action): try: get_subactions = action._get_subactions except AttributeError: pass else: self._indent() <IF_STMT> for subaction in sorted(get_subactions(), key=lambda x: x.dest): yield subaction else: for subaction in get_subactions(): yield subaction self._dedent()","if isinstance(action, argparse._SubParsersAction):"
"def has_safe_repr(value): """"""Does the node have a safe representation?"""""" if value is None or value is NotImplemented or value is Ellipsis: return True if type(value) in (bool, int, float, complex, range_type, Markup) + string_types: return True if type(value) in (tuple, list, set, frozenset): for item in value: <IF_STMT> return False return True elif type(value) is dict: for key, value in iteritems(value): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False",if not has_safe_repr(item):
"def _compute_missing_fields_error(context, field_defs, incoming_fields): missing_fields = [] for field_name, field_def in field_defs.items(): <IF_STMT> missing_fields.append(field_name) if missing_fields: if len(missing_fields) == 1: return create_missing_required_field_error(context, missing_fields[0]) else: return create_missing_required_fields_error(context, missing_fields)",if not field_def.is_optional and field_name not in incoming_fields:
"def _list(self): data_sources = self.mkt_contract.functions.getAllProviders().call() data = [] for index, data_source in enumerate(data_sources): <IF_STMT> if 'test' not in Web3.toText(data_source).lower(): data.append(dict(dataset=self.to_text(data_source))) return pd.DataFrame(data)",if index > 0:
"def close_file_in_all_editorstacks(self, editorstack_id_str, index): for editorstack in self.editorstacks: <IF_STMT> editorstack.blockSignals(True) editorstack.close_file(index, force=True) editorstack.blockSignals(False)",if str(id(editorstack)) != editorstack_id_str:
"def _remove_custom_marker_object_instances(self): for id, obj in list(self._objects.items()): <IF_STMT> logger.info(""Removing CustomObject instance: id %s = obj '%s'"", id, obj) del self._objects[id]","if isinstance(obj, objects.CustomObject):"
"def append(self, labels): if isinstance(labels, list): for label in labels: <IF_STMT> self.__menuLabels.append(label) self.__enabledLabels.append(label) elif not labels in self.__menuLabels: self.__menuLabels.append(labels) self.__enabledLabels.append(labels)",if not label in self.__menuLabels:
"def _close_tree(view: View, defx: Defx, context: Context) -> None: for target in context.targets: <IF_STMT> view.close_tree(target['action__path'], defx._index) else: view.close_tree(target['action__path'].parent, defx._index) view.search_file(target['action__path'].parent, defx._index)",if target['is_directory'] and target['is_opened_tree']:
"def FirstFetch(self): q = collections.deque(['buddy', 'group', 'discuss']) while q: tinfo = q.popleft() <IF_STMT> cl = self.List(tinfo) if cl: q.extend(cl) time.sleep(1.0)","if self.Update(tinfo) and tinfo in ('group', 'discuss'):"
"def _sort_values_jobconf(self): """"""Jobconf dictionary to enable sorting by value."""""" if not self._sort_values: return {} hadoop_version = self.get_hadoop_version() jobconf = {} for k, v in _SORT_VALUES_JOBCONF.items(): <IF_STMT> jobconf[translate_jobconf(k, hadoop_version)] = v else: for j in translate_jobconf_for_all_versions(k): jobconf[j] = v return jobconf",if hadoop_version:
"def list(self): for fname in os.listdir(self.path): fpath = os.path.join(self.path, fname) <IF_STMT> yield (fname, get_etag_from_file(fpath))",if os.path.isfile(fpath) and fname.endswith(self.fileext):
"def get_environment_variable_value(val): env_val = val if val is not None and isinstance(val, str): match = re.search('^\\${(?P<environment_key_name>\\w+)*}$', val) <IF_STMT> env_val = os.environ.get(match.group('environment_key_name')) return env_val",if match is not None:
"def L_op(self, inputs, outputs, grads): x, = inputs gz, = grads if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] cst = np.asarray(np.sqrt(np.pi) / 2.0, dtype=upcast(x.type.dtype, gz.type.dtype)) return (gz * cst * exp(erfinv(x) ** 2),)",if x.type in discrete_types:
"def is_test_finished(self): retcode = self.process.poll() if retcode is not None: logger.info('Phantom done its work with exit code: %s', retcode) self.phout_finished.set() return abs(retcode) else: info = self.get_info() <IF_STMT> eta = int(info.duration) - (int(time.time()) - int(self.start_time)) self.publish('eta', eta) return -1",if info:
"def icon(display_icon): """"""returns empty dict if show_icons is False, else the icon passed"""""" kws = {} if get_icon_switch(): <IF_STMT> kws = {'icon_value': custom_icon(display_icon)} elif display_icon != 'OUTLINER_OB_EMPTY': kws = {'icon': display_icon} return kws",if display_icon.startswith('SV_'):
"def raise_to_cubic(bzs): result = [] for sp in bzs: r = [] for bz in sp: <IF_STMT> r.append((bz[0], lerppt(2.0 / 3, bz[0], bz[1]), lerppt(2.0 / 3, bz[2], bz[1]), bz[2])) else: r.append(bz) result.append(r) return result",if len(bz) == 3:
def readline(self): while 1: line = self._readline() if line: self._filelineno += 1 return line <IF_STMT> return line self.nextfile(),if not self._file:
"def readlines(self): """"""Returns a list of all lines (optionally parsed) in the file."""""" if self.grammar: tot = [] while 1: line = self.file.readline() <IF_STMT> break tot.append(line) return tot return self.file.readlines()",if not line:
"def visit_return(self, node): scope = self._peek_scope() assert scope.ilk == 'function' if not scope.get('returns'): citdl = self._citdl_from_node(node.children[1]) <IF_STMT> scope.attrs['returns'] = citdl",if citdl and citdl is not 'None':
"def load_json_file(file_path): """"""load a file into a json object"""""" try: with open(file_path) as small_file: return json.load(small_file) except OSError as e: print(e) print('trying to read file in blocks') with open(file_path) as big_file: json_string = '' while True: block = big_file.read(64 * (1 << 20)) json_string = json_string + block <IF_STMT> break return json.loads(json_string)",if not block:
"def rotate(cls, axis, theta): """"""Prepare a quaternion that represents a rotation on a given axis."""""" if isinstance(axis, str): if axis in ('x', 'X'): axis = V.X <IF_STMT> axis = V.Y elif axis in ('z', 'Z'): axis = V.Z axis = axis.normalize() s = math.sin(theta / 2.0) c = math.cos(theta / 2.0) return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)","elif axis in ('y', 'Y'):"
"def is_valid_block(self): """"""check wheter the block is valid in the current position"""""" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False <IF_STMT> return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True",if self.block.pos.y + j < 0:
def dump_token_list(tokens): for token in tokens: if token.token_type == TOKEN_TEXT: writer.write(token.contents) <IF_STMT> writer.print_expr(token.contents) touch_var(token.contents),elif token.token_type == TOKEN_VAR:
"def encode(name, value): try: <IF_STMT> value, params = value return _encode_parametrized(name, value, params) return _encode_unstructured(name, value) except Exception: _log.exception('Failed to encode %s %s' % (name, value)) raise","if parametrized.is_parametrized(name, value):"
"def conversation_to_fb_format(conversation): assert len(conversation) > 1 lines = [] for i in range(0, len(conversation), 2): <IF_STMT> lines.append('%d %s\t%s' % (i / 2 + 1, conversation[i], conversation[i + 1])) else: lines.append('%d %s' % (i / 2 + 1, conversation[i])) return '\n'.join(lines)",if i + 1 < len(conversation):
"def _handle_js_events(self, change): if self.js_events: <IF_STMT> for event in self.js_events: event_name = event['name'] if event_name in self.event_handlers: self.event_handlers[event_name](event['detail']) self.js_events = []",if self.event_handlers:
"def escapeall(self, lines): """"""Escape all lines in an array according to the output options."""""" result = [] for line in lines: if Options.html: line = self.escape(line, EscapeConfig.html) if Options.iso885915: line = self.escape(line, EscapeConfig.iso885915) line = self.escapeentities(line) <IF_STMT> line = self.escape(line, EscapeConfig.nonunicode) result.append(line) return result",elif not Options.unicode:
"def filter_testsuite(suite, matcher, level=None): """"""Returns a flattened list of test cases that match the given matcher."""""" if not isinstance(suite, unittest.TestSuite): raise TypeError('not a TestSuite', suite) results = [] for test in suite._tests: if level is not None and getattr(test, 'level', 0) > level: continue if isinstance(test, unittest.TestCase): testname = test.id() <IF_STMT> results.append(test) else: filtered = filter_testsuite(test, matcher, level) results.extend(filtered) return results",if matcher(testname):
"def _close_brackets(self, fragment): stack = [] for char in fragment: if char in self._PARENS.keys(): stack.append(char) <IF_STMT> if stack and self._PARENS[stack[-1]] == char: stack.pop() else: return '' return ''.join((self._PARENS[paren] for paren in reversed(stack)))",elif char in self._PARENS.values():
"def restrict(points): result = [] for p in points: if point_inside_mesh(bvh, p): result.append(p) else: loc, normal, index, distance = bvh.find_nearest(p) <IF_STMT> result.append(tuple(loc)) return result",if loc is not None:
"def _check_ids(el, filename, parent_id): """"""Recursively walks through tree and check if every object has ID"""""" for child in el: if child.tag == 'object': msg = 'Widget has no ID in %s; class %s; Parent id: %s' % (filename, child.attrib['class'], parent_id) assert 'id' in child.attrib and child.attrib['id'], msg for subel in child: <IF_STMT> _check_ids(subel, filename, child.attrib['id'])",if subel.tag == 'child':
"def _checkIfSuccessfulCallback(self, result, error=False, **kwargs): if error: connection_error = kwargs.get('connection_error', False) <IF_STMT> log.debug('During direct file upload compute is not visible. Fallback to upload via controller.') self._fileUploadToController() elif 'message' in result: log.error('Error while direct file upload: {}'.format(result['message'])) return self._callback(result, error, **kwargs)",if connection_error:
"def getCellPropertyNames_aux(self, col_id): if col_id == 'name': <IF_STMT> return ['places_busy'] baseName = self.image_icon if self.isOpen: return [baseName + '_open'] else: return [baseName + '_closed'] return []",if self.image_icon == 'places_busy':
"def delete_volume(self, volume_id): if volume_id in self.volumes: volume = self.volumes[volume_id] <IF_STMT> raise VolumeInUseError(volume_id, volume.attachment.instance.id) return self.volumes.pop(volume_id) raise InvalidVolumeIdError(volume_id)",if volume.attachment:
"def dashboards(self): dashboards = OrderedDict() for slug, path in enumerate(app_settings.DASHBOARDS): <IF_STMT> slug, path = path pk = str(slug) klass = import_string(path) dashboards[pk] = klass(pk=pk) if not dashboards: raise ImproperlyConfigured('No dashboards found.') return dashboards","if isinstance(path, (list, tuple)):"
"def test_reader(config, device, logger): loader = build_dataloader(config, 'Train', device, logger) import time starttime = time.time() count = 0 try: for data in loader(): count += 1 <IF_STMT> batch_time = time.time() - starttime starttime = time.time() logger.info('reader: {}, {}, {}'.format(count, len(data[0]), batch_time)) except Exception as e: logger.info(e) logger.info('finish reader: {}, Success!'.format(count))",if count % 1 == 0:
"def on_adapter_selected(self, menuitem, adapter_path): if menuitem.props.active: <IF_STMT> logging.info('selected %s', adapter_path) self.blueman.Config['last-adapter'] = adapter_path_to_name(adapter_path) self.blueman.List.set_adapter(adapter_path)",if adapter_path != self.blueman.List.Adapter.get_object_path():
"def set_note_pinned(self, key, pinned): n = self.notes[key] old_pinned = utils.note_pinned(n) if pinned != old_pinned: <IF_STMT> n['systemtags'] = [] systemtags = n['systemtags'] if pinned: systemtags.append('pinned') else: systemtags.remove('pinned') n['modifydate'] = time.time() self.notify_observers('change:note-status', events.NoteStatusChangedEvent(what='modifydate', key=key))",if 'systemtags' not in n:
"def setMinCores(self, rpcObjects=None): tasks = self._getSelected(rpcObjects) if tasks: current = max([task.data.min_cores for task in tasks]) title = 'Set Minimum Cores' body = 'Please enter the new minimum cores value:' value, choice = QtWidgets.QInputDialog.getDouble(self._caller, title, body, current, 0, 50000, 0) <IF_STMT> for task in tasks: task.setMinCores(float(value)) self._update()",if choice:
"def _1_0_cloud_ips_cip_jsjc5(self, method, url, body, headers): if method == 'DELETE': return self.test_response(httplib.OK, '') elif method == 'PUT': body = json.loads(body) <IF_STMT> return self.test_response(httplib.OK, '') else: return self.test_response(httplib.BAD_REQUEST, '{""error_name"":""bad dns"", ""errors"": [""Bad dns""]}')","if body.get('reverse_dns', None) == 'fred.co.uk':"
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: if 'title' in child.tag: title = str(child.text) <IF_STMT> pub_date = str(child.text) if 'description' in child.tag: description = str(child.text) print_stdout(color_line(title, 14) + ' (' + bold_line(pub_date) + ')') print_stdout(format_paragraph(strip_tags(description))) print_stdout()",if 'pubDate' in child.tag:
"def oregon_battery(self, offset): nib = self.decoded_nibbles batt = 'OK' if nib[offset][3] != '': <IF_STMT> batt = 'Low' self.put(nib[offset][0], nib[offset][1], self.out_ann, [2, ['Batt ' + batt, batt]])","if int(nib[offset][3], 16) >> 2 & 1 == 1:"
"def body_stream() -> typing.AsyncGenerator[bytes, None]: while True: message = await queue.get() <IF_STMT> break assert message['type'] == 'http.response.body' yield message.get('body', b'') task.result()",if message is None:
"def _wait_for_reboot(): try: state = self._conn.reboot_domain(instance['name']) <IF_STMT> LOG.debug(_('instance %s: rebooted'), instance['name']) timer.stop() except Exception: LOG.exception(_('_wait_for_reboot failed')) timer.stop()",if state == power_state.RUNNING:
"def _get_sequence_vector(sequence, tokenizer, format_dtype, unit_to_id, lowercase=True, unknown_symbol=UNKNOWN_SYMBOL): unit_sequence = tokenizer(sequence.lower() if lowercase else sequence) unit_indices_vector = np.empty(len(unit_sequence), dtype=format_dtype) for i in range(len(unit_sequence)): curr_unit = unit_sequence[i] <IF_STMT> unit_indices_vector[i] = unit_to_id[curr_unit] else: unit_indices_vector[i] = unit_to_id[unknown_symbol] return unit_indices_vector",if curr_unit in unit_to_id:
"def forward(self, x: Tensor, edge_index: Adj) -> Tensor: """""""""""" if self.add_self_loops: if isinstance(edge_index, Tensor): edge_index, _ = remove_self_loops(edge_index) edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim)) <IF_STMT> edge_index = set_diag(edge_index) x_norm = F.normalize(x, p=2.0, dim=-1) return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)","elif isinstance(edge_index, SparseTensor):"
"def _init_req_settings(self, **kwargs): for req_attr in self._req_settings: req_attr_value = kwargs.get(req_attr) <IF_STMT> raise MissingRequiredConf(conf_name=req_attr_value) req_attr_value = get_validator(req_attr)(req_attr_value) self._settings[req_attr] = req_attr_value",if req_attr_value is None:
"def delete(identifier, filenames=None, **kwargs): item = get_item(identifier) if filenames: if not isinstance(filenames, (set, list)): filenames = [filenames] for f in item.iter_files(): <IF_STMT> continue f.delete(**kwargs)",if f.name not in filenames:
"def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: if isinstance(decorator, NameExpr): if self.process_name_expr_decorator(decorator, o): is_abstract = True elif isinstance(decorator, MemberExpr): <IF_STMT> is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)","if self.process_member_expr_decorator(decorator, o):"
"def split_trading_pair(trading_pair: str) -> Optional[Tuple[str, str]]: try: m = RE_4_LETTERS_QUOTE.match(trading_pair) <IF_STMT> m = RE_3_LETTERS_QUOTE.match(trading_pair) if m is None: m = RE_2_LETTERS_QUOTE.match(trading_pair) return (m.group(1), m.group(2)) except Exception: return None",if m is None:
"def traverse_states(root): todo = [root] model = self.model while len(todo): iter = todo.pop(0) yield model.get_state(iter, treeindex) path = model.get_path(iter) <IF_STMT> children = [] child = model.iter_children(iter) while child: children.append(child) child = model.iter_next(child) todo = children + todo yield None",if treeview.row_expanded(path):
"def as_list(self, compact=True, storage_to_dict=True, datetime_to_str=False, custom_types=None): if storage_to_dict: items = [] for row in self: item = row.as_dict(datetime_to_str, custom_types) for jdata in self._joins_: <IF_STMT> item[jdata[0]] = row[jdata[0]].as_list() items.append(item) else: items = [item for item in self] return items",if not jdata[2]:
"def zip(target, source, env): compression = env.get('ZIPCOMPRESSION', 0) zf = zipfile.ZipFile(str(target[0]), 'w', compression) for s in source: if s.isdir(): for dirpath, dirnames, filenames in os.walk(str(s)): for fname in filenames: path = os.path.join(dirpath, fname) <IF_STMT> zf.write(path) else: zf.write(str(s)) zf.close()",if os.path.isfile(path):
def remove_PBA_files(): if monkey_island.cc.services.config.ConfigService.get_config(): windows_filename = monkey_island.cc.services.config.ConfigService.get_config_value(PBA_WINDOWS_FILENAME_PATH) linux_filename = monkey_island.cc.services.config.ConfigService.get_config_value(PBA_LINUX_FILENAME_PATH) if linux_filename: remove_file(linux_filename) <IF_STMT> remove_file(windows_filename),if windows_filename:
"def test_takewhile(self): for s in (range(10), range(0), range(1000), (7, 11), range(2000, 2200, 5)): for g in (G, I, Ig, S, L, R): tgt = [] for elem in g(s): <IF_STMT> break tgt.append(elem) self.assertEqual(list(takewhile(isEven, g(s))), tgt) self.assertRaises(TypeError, takewhile, isEven, X(s)) self.assertRaises(TypeError, takewhile, isEven, N(s)) self.assertRaises(ZeroDivisionError, list, takewhile(isEven, E(s)))",if not isEven(elem):
"def download_file(url, file): try: xlog.info('download %s to %s', url, file) opener = get_opener() req = opener.open(url, cafile='') CHUNK = 16 * 1024 with open(file, 'wb') as fp: while True: chunk = req.read(CHUNK) <IF_STMT> break fp.write(chunk) return True except: xlog.info('download %s to %s fail', url, file) return False",if not chunk:
"def set_preferred_lane(self, preferred_lane: int=None) -> 'AbstractEnv': env_copy = copy.deepcopy(self) if preferred_lane: for v in env_copy.road.vehicles: <IF_STMT> v.route = [(lane[0], lane[1], preferred_lane) for lane in v.route] v.LANE_CHANGE_MAX_BRAKING_IMPOSED = 1000 return env_copy","if isinstance(v, IDMVehicle):"
"def resolve(self, value: Optional[T]) -> T: v: Optional[Any] = value if value is None: t = os.environ.get(self.envvar) if self.type is bool and t: v = t in ['true', 'True', '1', 'yes'] <IF_STMT> v = t elif t: v = ast.literal_eval(t) if t is not None else None if v is None: v = self.default return v",elif self.type is str and t:
"def test_read_lazy_A(self): want = ['x' * 100, EOF_sigil] self.dataq.put(want) telnet = telnetlib.Telnet(HOST, self.port) self.dataq.join() time.sleep(self.block_short) self.assertEqual('', telnet.read_lazy()) data = '' while True: try: read_data = telnet.read_lazy() data += read_data <IF_STMT> telnet.fill_rawq() except EOFError: break self.assertTrue(want[0].startswith(data)) self.assertEqual(data, want[0])",if not read_data:
"def request_put_json(url, headers): """"""Makes a PUT request and returns the JSON response"""""" try: response = requests.put(url, headers=headers) <IF_STMT> return response.json() else: raise RadarrRequestError('Invalid response received from Radarr: %s' % response.content) except RequestException as e: raise RadarrRequestError('Unable to connect to Radarr at %s. Error: %s' % (url, e))",if response.status_code == 200:
"def firebase_analysis(urls): firebase_db = [] logger.info('Detecting Firebase URL(s)') for url in urls: <IF_STMT> returl, is_open = open_firebase(url) fbdic = {'url': returl, 'open': is_open} if fbdic not in firebase_db: firebase_db.append(fbdic) return firebase_db",if 'firebaseio.com' in url:
"def logprob(self, sample): if self._log: return self._prob_dict.get(sample, _NINF) el<IF_STMT> return _NINF elif self._prob_dict[sample] == 0: return _NINF else: return math.log(self._prob_dict[sample], 2)",if sample not in self._prob_dict:
"def is_image(self, input): try: <IF_STMT> return True elif isinstance(input, str): if not os.path.isfile(input): raise ValueError('input must be a file') img = Image.open(input) _ = img.size return True else: return False except: return False","if isinstance(input, (np.ndarray, Image.Image)):"
"def extract(self): for battery in self.vars: for line in dopen('/proc/acpi/battery/' + battery + '/state').readlines(): l = line.split() if len(l) < 3: continue if l[0:2] == ['remaining', 'capacity:']: remaining = int(l[2]) continue <IF_STMT> rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1","elif l[0:2] == ['present', 'rate:']:"
"def get_app_module(module_name, raise_on_failure=True): try: __import__(module_name) except ImportError: if sys.exc_info()[-1].tb_next: raise RuntimeError(f""While importing '{module_name}', an ImportError was raised:\n\n{traceback.format_exc()}"") <IF_STMT> raise RuntimeError(f""Could not import '{module_name}'."") else: return return sys.modules[module_name]",elif raise_on_failure:
"def process_shutdown_hooks(self): for plugin_name in self.DISCOVERED.keys(): try: package = 'mailpile.plugins.%s' % plugin_name _, manifest = self.DISCOVERED[plugin_name] <IF_STMT> for method_name in self._mf_path(manifest, 'lifecycle', 'shutdown'): method = self._get_method(package, method_name) method(self.config) except: traceback.print_exc(file=sys.stderr)",if package in sys.modules:
"def _check_arch(self, arch): if arch is None: return try: from pycuda.driver import Context capability = Context.get_device().compute_capability() <IF_STMT> from warnings import warn warn('trying to compile for a compute capability higher than selected GPU') except Exception: pass","if tuple(map(int, tuple(arch.split('_')[1]))) > capability:"
"def phpinfo_ext(content): indexes = SubstrFind(content, 'AbracadabrA') found = len(indexes) > 0 got = '' if found: start = indexes[0] + 11 for x in range(start, len(content)): <IF_STMT> break got += content[x] return got",if content[x] == '<':
def update_leaderboard(wait_time): conn = get_connection() cursor = conn.cursor(MySQLdb.cursors.DictCursor) while True: try: if use_log: log.info('Updating leaderboard and adding some sigma') cursor.execute('call generate_leaderboard;') <IF_STMT> break for s in range(wait_time): time.sleep(1) except KeyboardInterrupt: break except: log.error(traceback.format_exc()) break cursor.close() conn.close(),if wait_time == 0:
"def writeBit(self, state, endian): if self._bit_pos == 7: self._bit_pos = 0 <IF_STMT> if endian is BIG_ENDIAN: self._byte |= 1 else: self._byte |= 128 self._output.write(chr(self._byte)) self._byte = 0 else: if state: if endian is BIG_ENDIAN: self._byte |= 1 << self._bit_pos else: self._byte |= 1 << 7 - self._bit_pos self._bit_pos += 1",if state:
"def getreportopt(config): reportopts = '' reportchars = config.option.reportchars if not config.option.disablepytestwarnings and 'w' not in reportchars: reportchars += 'w' elif config.option.disablepytestwarnings and 'w' in reportchars: reportchars = reportchars.replace('w', '') if reportchars: for char in reportchars: <IF_STMT> reportopts += char elif char == 'a': reportopts = 'fEsxXw' return reportopts",if char not in reportopts and char != 'a':
"def validate_module(self, pipeline): if self.mode == MODE_UNTANGLE: <IF_STMT> path = os.path.join(self.training_set_directory.get_absolute_path(), self.training_set_file_name.value) if not os.path.exists(path): raise ValidationError(""Can't find file %s"" % self.training_set_file_name.value, self.training_set_file_name)",if self.training_set_directory.dir_choice != URL_FOLDER_NAME:
"def reshape(w, h): try: <IF_STMT> h = 1 glViewport(0, 0, w, h) glMatrixMode(GL_PROJECTION) glLoadIdentity() glMatrixMode(GL_MODELVIEW) updatePickingBuffer() except Exception: log.error('gl.reshape', exc_info=True)",if h == 0:
"def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False <IF_STMT> value = PseudoNamespace(value) elif isinstance(value, tuple): value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and (not isinstance(item, PseudoNamespace)): value[i] = PseudoNamespace(item) if tuple_converted: value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)","if isinstance(value, dict):"
"def scan_search(state): delim = state.source[state.position - 1] while True: c = state.consume() if c == delim: state.start += 1 state.backup() content = state.emit() state.consume() token = TokenSearchForward if c == '/' else TokenSearchBackward return (scan_range, [token(content)]) <IF_STMT> raise ValueError('unclosed search pattern: {0}'.format(state.source))",elif c == EOF:
"def fromVariant(variant): if hasattr(QtCore, 'QVariant') and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) elif t == QtCore.QVariant.Double: return variant.toDouble()[0] elif t == QtCore.QVariant.Int: return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() <IF_STMT> return None else: raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName()) else: return variant",elif t == QtCore.QVariant.Invalid:
"def __iter__(self): i = 0 for category, filename in list(self.input_files.items()): for line in open(filename): line = self._clean_line(line) if self.accept_criteria(i): yield Opinion(line, category) i += 1 <IF_STMT> print('\tReaded {} examples'.format(i))",if i % 1000 == 0:
"def test_listing_all_frameworks_and_check_frameworks_by_order(self): """"""List all frameworks and check if frameworks appear by order"""""" result = subprocess.check_output(self.command_as_list([UMAKE, '--list'])) previous_framework = None for element in result.split(b'\n'): <IF_STMT> current_framework = element[:element.find(b':')] if previous_framework: self.assertTrue(previous_framework < current_framework) previous_framework = current_framework else: previous_framework = None",if element.startswith(b'\t'):
"def _locate_code(self, event): if self._current_code_view is None: return iid = self.tree.focus() if iid != '': values = self.tree.item(iid)['values'] <IF_STMT> start_line, start_col, end_line, end_col = values[1:5] self._current_code_view.select_range(TextRange(start_line, start_col, end_line, end_col))","if isinstance(values, list) and len(values) >= 5:"
"def __setattr__(self, attr, value): """"""Provides additional checks on recipient fields."""""" if attr in ['to', 'cc', 'bcc']: if isinstance(value, basestring): <IF_STMT> return check_email_valid(value, attr) else: for address in value: check_email_valid(address, attr) elif attr == 'headers': check_headers_valid(value) super(EmailMessage, self).__setattr__(attr, value)","if value == '' and getattr(self, 'ALLOW_BLANK_EMAIL', False):"
"def _scanDirectory(self, dirIter, f): while len(f) < 250: try: info = next(dirIter) except StopIteration: if not f: raise EOFError return f <IF_STMT> info.addCallback(self._cbScanDirectory, dirIter, f) return else: f.append(info) return f","if isinstance(info, defer.Deferred):"
def iterator(): try: while True: yield from pullparser.read_events() data = source.read(16 * 1024) if not data: break pullparser.feed(data) root = pullparser._close_and_return_root() yield from pullparser.read_events() it.root = root finally: <IF_STMT> source.close(),if close_source:
def test_until_timeout(self): timer = TestTimer(self.timeout) while not timer.is_timed_out(): <IF_STMT> self.log_success(timer) return sleep(DELAY_BETWEEN_ANALYSIS) LOGGER.debug('Waiting until all analyzers passed. Time passed: {}'.format(timer.get_time_taken())) self.log_failure(timer) assert False,if self.all_analyzers_pass():
"def start(self): """"""Start our callback in its own perpetual timer thread."""""" if self.frequency > 0: threadname = self.name or self.__class__.__name__ <IF_STMT> self.thread = PerpetualTimer(self.frequency, self.callback) self.thread.bus = self.bus self.thread.setName(threadname) self.thread.start() self.bus.log('Started monitor thread %r.' % threadname) else: self.bus.log('Monitor thread %r already started.' % threadname)",if self.thread is None:
"def set_flavour(flavour, request=None, permanent=False): if flavour not in settings.FLAVOURS: raise ValueError(u""'%r' is no valid flavour. Allowed flavours are: %s"" % (flavour, ', '.join(settings.FLAVOURS))) request = request or getattr(_local, 'request', None) if request: request.flavour = flavour <IF_STMT> flavour_storage.set(request, flavour) elif permanent: raise ValueError(u'Cannot set flavour permanently, no request available.') _local.flavour = flavour",if permanent:
"def get_images(image_path, support_ext='.jpg|.jpeg|.png'): if not os.path.exists(image_path): raise Exception(f'Image path {image_path} invalid') if os.path.isfile(image_path): return [image_path] imgs = [] for item in os.listdir(image_path): ext = os.path.splitext(item)[1][1:].strip().lower() <IF_STMT> item_path = os.path.join(image_path, item) imgs.append(item_path) return imgs",if len(ext) > 0 and ext in support_ext:
"def write_text(self, text): """"""Writes re-indented text into the buffer."""""" should_indent = False rows = [] for row in text.split('\n'): <IF_STMT> row = '{}'.format(row) if '\x08' in row: row = row.replace('\x08', '', 1) should_indent = True elif not len(row.strip()): should_indent = False rows.append(row) self.write('{}\n'.format('\n'.join(rows)))",if should_indent:
"def build_priorities(self, _iter, priorities): while _iter is not None: if self.files_treestore.iter_has_child(_iter): self.build_priorities(self.files_treestore.iter_children(_iter), priorities) <IF_STMT> priorities[self.files_treestore.get_value(_iter, 3)] = self.files_treestore.get_value(_iter, 0) _iter = self.files_treestore.iter_next(_iter) return priorities","elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep):"
"def _validate_sample(self, value): mask = self.support(value) if not_jax_tracer(mask): <IF_STMT> warnings.warn('Out-of-support values provided to log prob method. The value argument should be within the support.') return mask",if not np.all(mask):
"def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split('] ')[1] + '.' except IndexError: error_msg = str(err_msg.args[0]) + '.' if settings.INIT_TEST == True: <IF_STMT> print(settings.FAIL_STATUS) elif settings.VERBOSITY_LEVEL < 1: print('') print(settings.print_critical_msg(error_msg)) raise SystemExit()",if settings.VERBOSITY_LEVEL < 2:
"def add_party(self, party_type, party): party_doc = frappe.new_doc(party_type) if party_type == 'Customer': party_doc.customer_name = party else: supplier_group = frappe.db.get_single_value('Buying Settings', 'supplier_group') <IF_STMT> frappe.throw(_('Please Set Supplier Group in Buying Settings.')) party_doc.supplier_name = party party_doc.supplier_group = supplier_group party_doc.flags.ignore_mandatory = True party_doc.save(ignore_permissions=True)",if not supplier_group:
"def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic <IF_STMT> polymorphic_key = polymorphic if isinstance(polymorphic_key, bool): polymorphic_key = 'type' if data.get(polymorphic_key) == model.__name__: return model raise ImproperlyConfigured(u'No model found for data: {!r}'.format(data))",if polymorphic:
"def cleanup_expired_revoked_tokens(): """"""Remove tokens that have now expired from the revoked token table."""""" revoked_tokens = db.session.query(RevokedToken).all() for revoked_token in revoked_tokens: <IF_STMT> pass else: db.session.delete(revoked_token) db.session.commit()",if Journalist.validate_token_is_not_expired_or_invalid(revoked_token.token):
"def matches_filter(key, values): if key == 'location': if location_type in ('availability-zone', 'availability-zone-id'): return offering.get('Location') in values <IF_STMT> return any((v for v in values if offering.get('Location').startswith(v))) else: return False elif key == 'instance-type': return offering.get('InstanceType') in values else: return False",elif location_type == 'region':
"def autoname(self): naming_method = frappe.db.get_value('HR Settings', None, 'emp_created_by') if not naming_method: throw(_('Please setup Employee Naming System in Human Resource > HR Settings')) elif naming_method == 'Naming Series': set_name_by_naming_series(self) elif naming_method == 'Employee Number': self.name = self.employee_number <IF_STMT> self.set_employee_name() self.name = self.employee_name self.employee = self.name",elif naming_method == 'Full Name':
"def readHexStringFromStream(stream): stream.read(1) txt = '' x = b_('') while True: tok = readNonWhitespace(stream) if not tok: raise PdfStreamError('Stream has ended unexpectedly') if tok == b_('>'): break x += tok <IF_STMT> txt += chr(int(x, base=16)) x = b_('') if len(x) == 1: x += b_('0') if len(x) == 2: txt += chr(int(x, base=16)) return createStringObject(b_(txt))",if len(x) == 2:
"def test_technical_on(self): data = {'developer_comments': 'Test comment!', 'whiteboard-public': 'Whiteboard info.'} response = self.client.post(self.technical_edit_url, data) assert response.context['form'].errors == {} addon = self.get_addon() for k in data: if k == 'developer_comments': assert str(getattr(addon, k)) == str(data[k]) <IF_STMT> assert str(addon.whiteboard.public) == str(data[k]) else: assert getattr(addon, k) == (data[k] == 'on')",elif k == 'whiteboard-public':
"def create_season_posters(self, show_obj, force=False): if self.season_posters and show_obj: result = [] for ep_obj in show_obj.episodes: <IF_STMT> sickrage.app.log.debug('Metadata provider ' + self.name + ' creating season posters for ' + show_obj.name) result = result + [self.save_season_poster(show_obj, ep_obj.season)] return all(result) return False","if not self._has_season_poster(show_obj, ep_obj.season) or force:"
"def get_prefixes(self, guild: Optional[discord.Guild]=None) -> List[str]: ret: List[str] gid: Optional[int] = guild.id if guild else None if gid in self._cached: ret = self._cached[gid].copy() else: if gid is not None: ret = await self._config.guild_from_id(gid).prefix() <IF_STMT> ret = await self.get_prefixes(None) else: ret = self._global_prefix_overide or await self._config.prefix() self._cached[gid] = ret.copy() return ret",if not ret:
"def checkUnchangedIvars(obj, d, exceptions=None): if not exceptions: exceptions = [] ok = True for key in d: if key not in exceptions: <IF_STMT> g.trace('changed ivar: %s old: %s new: %s' % (key, repr(d.get(key)), repr(getattr(obj, key)))) ok = False return ok","if getattr(obj, key) != d.get(key):"
"def validate_ip(address): try: if socket.inet_aton(address): <IF_STMT> debug_msg('setcore', 'this is a valid IP address', 5) return True else: print_error('This is not a valid IP address...') raise socket.error else: raise socket_error except socket.error: return False",if len(address.split('.')) == 4:
"def kernel(x, y): diff = safe_norm(x - y, ord=2) if self._normed() and x.ndim >= 1 else x - y kernel_res = jnp.exp(-diff ** 2 / bandwidth) if self._mode == 'matrix': <IF_STMT> return kernel_res * jnp.identity(x.shape[0]) else: return jnp.diag(kernel_res) else: return kernel_res",if self.matrix_mode == 'norm_diag':
"def __init__(self, transforms): assert isinstance(transforms, collections.abc.Sequence) self.transforms = [] for transform in transforms: if isinstance(transform, dict): transform = build_from_cfg(transform, PIPELINES) self.transforms.append(transform) <IF_STMT> self.transforms.append(transform) else: raise TypeError('transform must be callable or a dict')",elif callable(transform):
"def translate(self, message: str, plural_message: Optional[str]=None, count: Optional[int]=None) -> str: if plural_message is not None: assert count is not None <IF_STMT> message = plural_message message_dict = self.translations.get('plural', {}) else: message_dict = self.translations.get('singular', {}) else: message_dict = self.translations.get('unknown', {}) return message_dict.get(message, message)",if count != 1:
"def install_requires(cls, reduced_dependencies): install_requires = OrderedSet() for dep in reduced_dependencies: <IF_STMT> for req in dep.payload.requirements: install_requires.add(str(req.requirement)) elif cls.has_provides(dep): install_requires.add(dep.provides.key) return install_requires",if cls.is_requirements(dep):
"def doit(): recipes_path = expanduser('recipes.pprint') recipe_dicts = eval(open(recipes_path).read()) for r in recipe_dicts: for key in r.keys(): if key not in ('desc', 'comments'): del r[key] for c in r['comments']: for key in c.keys(): <IF_STMT> del c[key] f = open('stripped.pprint', 'w') f.write(pformat(recipe_dicts)) f.close()","if key not in ('comment', 'title'):"
"def setup(self, name): value = self.default if self.environ: full_environ_name = self.full_environ_name(name) <IF_STMT> value = self.to_python(os.environ[full_environ_name]) elif self.environ_required: raise ValueError('Value {0!r} is required to be set as the environment variable {1!r}'.format(name, full_environ_name)) self.value = value return value",if full_environ_name in os.environ:
"def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): if line == '': continue <IF_STMT> next_is_highlight = True elif next_is_highlight: highlights.append(line) else: article_lines.append(line) article = ' '.join(article_lines) abstract = ' '.join(highlights) return (article, abstract)",elif line.startswith('@highlight'):
"def _ordered_tag_specs(entity_tag_specs: Optional[List[EntityTagSpec]]) -> List[EntityTagSpec]: """"""Ensure that order of entity tag specs matches CRF layer order."""""" if entity_tag_specs is None: return [] crf_order = [ENTITY_ATTRIBUTE_TYPE, ENTITY_ATTRIBUTE_ROLE, ENTITY_ATTRIBUTE_GROUP] ordered_tag_spec = [] for tag_name in crf_order: for tag_spec in entity_tag_specs: <IF_STMT> ordered_tag_spec.append(tag_spec) return ordered_tag_spec",if tag_name == tag_spec.tag_name:
"def checkDrag(self, root, target): """"""Return False if target is any descendant of root."""""" c = self message = 'Can not drag a node into its descendant tree.' for z in root.subtree(): <IF_STMT> if g.app.unitTesting: g.app.unitTestDict['checkMoveWithParentWithWarning'] = True else: c.alert(message) return False return True",if z == target:
"def get_adapter(self, pattern=None): adapters = self.get_adapters() if pattern is None: <IF_STMT> return adapters[0] else: raise DBusNoSuchAdapterError('No adapter(s) found') else: for adapter in adapters: path = adapter.get_object_path() if path.endswith(pattern) or adapter['Address'] == pattern: return adapter raise DBusNoSuchAdapterError('No adapters found with pattern: %s' % pattern)",if len(adapters):
"def __init__(self, children, quiet_exceptions=()): self.keys = None if isinstance(children, dict): self.keys = list(children.keys()) children = children.values() self.children = [] for i in children: if not isinstance(i, YieldPoint): i = convert_yielded(i) <IF_STMT> i = YieldFuture(i) self.children.append(i) assert all((isinstance(i, YieldPoint) for i in self.children)) self.unfinished_children = set(self.children) self.quiet_exceptions = quiet_exceptions",if is_future(i):
"def _make_callback(self): callback = self.callback for plugin in self.all_plugins(): try: if hasattr(plugin, 'apply'): callback = plugin.apply(callback, self) else: callback = plugin(callback) except RouteReset: return self._make_callback() <IF_STMT> update_wrapper(callback, self.callback) return callback",if not callback is self.callback:
"def _check_conflict(func, other_funcs): if steps[func]: for other_func in other_funcs: <IF_STMT> raise ValueError(""Can't specify both %s and %s"" % (func, other_func))",if steps[other_func] and other_func != func:
"def shutdown(self, cleanup=True): super(LocalDistributedRunner, self).shutdown() global _dummy_cpu_actor global _dummy_cuda_actor if cleanup: if _dummy_cpu_actor or _dummy_cuda_actor: assert not self.is_actor(), ""Actor shouldn't have a dummy actor."" <IF_STMT> ray.kill(_dummy_cpu_actor) if _dummy_cuda_actor: ray.kill(_dummy_cuda_actor) _dummy_cpu_actor = None _dummy_cuda_actor = None",if _dummy_cpu_actor:
"def _publish(self, data): retry = True while True: try: if not retry: self._redis_connect() return self.redis.publish(self.channel, pickle.dumps(data)) except redis.exceptions.ConnectionError: <IF_STMT> logger.error('Cannot publish to redis... retrying') retry = False else: logger.error('Cannot publish to redis... giving up') break",if retry:
"def simulate_policy(args): data = torch.load(args.file) policy = data['evaluation/policy'] env = data['evaluation/env'] print('Policy loaded') if args.gpu: set_gpu_mode(True) policy.cuda() while True: path = rollout(env, policy, max_path_length=args.H, render=True) <IF_STMT> env.log_diagnostics([path]) logger.dump_tabular()","if hasattr(env, 'log_diagnostics'):"
"def get_bucket_latest_versions(self, bucket_name): versions = self.get_bucket_versions(bucket_name) latest_modified_per_key = {} latest_versions = {} for version in versions: name = version.name last_modified = version.last_modified version_id = version.version_id latest_modified_per_key[name] = max(last_modified, latest_modified_per_key.get(name, datetime.datetime.min)) <IF_STMT> latest_versions[name] = version_id return latest_versions",if last_modified == latest_modified_per_key[name]:
"def _get_ntp_entity(self, peer_type): ntp_entities = {} command = 'show ntp peers' ntp_peers_table = self._get_command_table(command, 'TABLE_peers', 'ROW_peers') for ntp_peer in ntp_peers_table: <IF_STMT> continue peer_addr = napalm.base.helpers.ip(ntp_peer.get('PeerIPAddress').strip()) ntp_entities[peer_addr] = {} return ntp_entities","if ntp_peer.get('serv_peer', '').strip() != peer_type:"
"def kaiming_init(module, a=0, mode='fan_out', nonlinearity='relu', bias=0, distribution='normal'): assert distribution in ['uniform', 'normal'] if hasattr(module, 'weight') and module.weight is not None: <IF_STMT> nn.init.kaiming_uniform_(module.weight, a=a, mode=mode, nonlinearity=nonlinearity) else: nn.init.kaiming_normal_(module.weight, a=a, mode=mode, nonlinearity=nonlinearity) if hasattr(module, 'bias') and module.bias is not None: nn.init.constant_(module.bias, bias)",if distribution == 'uniform':
"def _get_arguments(self, name: str, source: Dict[str, List[bytes]], strip: bool=True) -> List[str]: values = [] for v in source.get(name, []): s = self.decode_argument(v, name=name) <IF_STMT> s = RequestHandler._remove_control_chars_regex.sub(' ', s) if strip: s = s.strip() values.append(s) return values","if isinstance(s, unicode_type):"
"def __str__(self): s = '{' sep = '' for k, v in self.iteritems(): s += sep if type(k) == str: s += ""'%s'"" % k else: s += str(k) s += ': ' <IF_STMT> s += ""'%s'"" % v else: s += str(v) sep = ', ' s += '}' return s",if type(v) == str:
"def contains(self, other_route): if isinstance(other_route, list): return self.to_list()[0:len(other_route)] == other_route assert len(other_route.outgoing) <= 1, 'contains(..) cannot be called after a merge' assert len(self.outgoing) <= 1, 'contains(..) cannot be called after a merge' if other_route.task_spec == self.task_spec: if other_route.outgoing and self.outgoing: return self.outgoing[0].contains(other_route.outgoing[0]) <IF_STMT> return True elif not other_route.outgoing: return True return False",elif self.outgoing:
"def iter_help(cls): for variable_name, value in sorted(cls.__dict__.items()): <IF_STMT> continue variable_type, variable_text = cls.process_pydoc(getattr(value, '__doc__')) yield (variable_name, variable_type, variable_text)",if not variable_name.startswith('PEX_'):
"def _clean_dict(json_dict): for key, value in json_dict.items(): if isinstance(value, list): json_dict[key] = list(OrderedSet(map(_clean_string, value))) <IF_STMT> json_dict[key] = _clean_dict(value) return OrderedDict(filter(lambda x: x[1], json_dict.items()))","elif isinstance(value, dict):"
"def _createdir(self): if not os.path.exists(self._dir): try: os.makedirs(self._dir, 448) except OSError as e: <IF_STMT> raise EnvironmentError(""Cache directory '%s' does not exist and could not be created'"" % self._dir)",if e.errno != errno.EEXIST:
"def JobWait(self, waiter): while True: result = waiter.WaitForOne(False) <IF_STMT> return wait_status.Cancelled(result) if result == -1: break if self.state != job_state_e.Running: break return wait_status.Proc(self.status)",if result > 0:
"def _deserialize_pickle5_data(self, data): try: in_band, buffers = unpack_pickle5_buffers(data) <IF_STMT> obj = pickle.loads(in_band, buffers=buffers) else: obj = pickle.loads(in_band) except pickle.pickle.PicklingError: raise DeserializationError() return obj",if len(buffers) > 0:
"def svgGetPaths(svgCode): doc = xmlparseString(svgCode) svg = doc.documentElement paths = findPathNodes(svg) isFigmaSVG = svgCode.find('Figma</desc>') != -1 if len(paths) == 0: return (paths, (0, 0)) paths2 = [] for path in paths: id = path.getAttribute('id') <IF_STMT> tr = nodeTranslation(path) d = path.getAttribute('d') paths2.append((d, tr)) return (paths2, isFigmaSVG)",if not isFigmaSVG or (id is None or id.find('stroke') == -1):
"def get_track_id_from_json(item): """"""Try to extract video Id from various response types"""""" fields = ['contentDetails/videoId', 'snippet/resourceId/videoId', 'id/videoId', 'id'] for field in fields: node = item for p in field.split('/'): <IF_STMT> node = node.get(p) if node: return node return ''","if node and isinstance(node, dict):"
"def save(self): self._idx_lock.acquire() try: if self._is_idx_dirty: <IF_STMT> self._mk_dbdir() self.db.save_pickle(join(self.base_dir, 'dirs_from_basename'), self._dirs_from_basename) self._is_idx_dirty = False finally: self._idx_lock.release()",if not exists(self.base_dir):
"def _init_from_response(self, response): self.id = response['id'] self.uri = response.get('mongodb_auth_uri', response['mongodb_uri']) for member in response['members']: if member['state'] == 1: self.primary = Server(member['server_id'], member['host']) <IF_STMT> self.secondary = Server(member['server_id'], member['host']) return self",elif member['state'] == 2:
"def verify_secret_key(request): """"""Verifies secret key for a request"""""" if request.user.username: return True else: key = request.GET['secret'] user_id, secret = key.split('.', 1) try: profile = User.objects.get(pk=user_id) except: return False <IF_STMT> request.user = profile.user return True return False","if key == get_secret_key(request, profile):"
"def compute(self, split): rd = random.Random(self.seed + split.index) if self.withReplacement: olddata = list(self.prev.iterator(split)) sampleSize = int(math.ceil(len(olddata) * self.frac)) for i in range(sampleSize): yield rd.choice(olddata) else: for i in self.prev.iterator(split): <IF_STMT> yield i",if rd.random() <= self.frac:
def splitIntoWords(name): wordlist = [] wordstart = 0 l = len(name) for i in range(l): c = name[i] n = None if c == ' ' or c == '-': n = name[wordstart:i] <IF_STMT> n = name[wordstart:i + 1] if n: wordstart = i if c == '-' and n != '': n += '-' if c == ' ' or c == '-': wordstart = i + 1 wordlist.append(n) return wordlist,elif i == l - 1:
"def check_file(f, path): if not (ignore_substring and ignore_substring in f): <IF_STMT> compl_path = os.path.join(path, f) if os.path.isfile(compl_path): return compl_path return False",if substring in f:
"def keyPressEvent(self, event): """"""Add up and down arrow key events to built in functionality."""""" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) <IF_STMT> self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)",elif keyPressed == Constants.DOWN_KEY:
"def _get_disk_size(cls, path, ignored=None): if ignored is None: ignored = [] if path in ignored: return 0 total = 0 for entry in scandir(path): <IF_STMT> total += cls._get_disk_size(entry.path, ignored=ignored) elif entry.is_file(): total += entry.stat().st_size return total",if entry.is_dir():
"def _handle_rate_limit(self, exception: RedditAPIException) -> Optional[Union[int, float]]: for item in exception.items: if item.error_type == 'RATELIMIT': amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) if 'minute' in amount_search.group(2): seconds *= 60 <IF_STMT> sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None",if seconds <= int(self.config.ratelimit_seconds):
"def validate(self): try: f = int(eval(self.setting.getValue(), {}, {})) <IF_STMT> return (ERROR, 'This setting should not be below ' + str(self.minValue)) if self.maxValue is not None and f > self.maxValue: return (ERROR, 'This setting should not be above ' + str(self.maxValue)) return (SUCCESS, '') except (ValueError, SyntaxError, TypeError, NameError): return (ERROR, '""' + str(self.setting.getValue()) + '"" is not a valid whole number or expression')",if self.minValue is not None and f < self.minValue:
"def rename(self, remote_name, new_remote_name): remotes = self.load_remotes() remotes.rename(remote_name, new_remote_name) with self._cache.editable_packages.disable_editables(): for ref in self._cache.all_refs(): with self._cache.package_layout(ref).update_metadata() as metadata: <IF_STMT> metadata.recipe.remote = new_remote_name for pkg_metadata in metadata.packages.values(): if pkg_metadata.remote == remote_name: pkg_metadata.remote = new_remote_name remotes.save(self._filename)",if metadata.recipe.remote == remote_name:
"def _convert_idx(self, idx): graph_idx = 0 node_idx = idx for i in range(len(self.graphs)): <IF_STMT> graph_idx = i break else: node_idx -= self.graphs[i].number_of_nodes() return (graph_idx, node_idx)",if node_idx < self.graphs[i].number_of_nodes():
"def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs): for app_config in apps.get_app_configs(): <IF_STMT> continue if verbosity >= 2: print('Running pre-migrate handlers for application %s' % app_config.label) models.signals.pre_migrate.send(sender=app_config, app_config=app_config, verbosity=verbosity, interactive=interactive, using=db, **kwargs)",if app_config.models_module is None:
"def slice(self, slice): gridscope = GridScope(globals=self.globals) for key in self.user_added: value = self[key] <IF_STMT> grid = value sliced = np.sum(grid[slice, ...], axis=0) logger.debug('sliced %s from %r to %r', key, grid.shape, sliced.shape) gridscope[key] = sliced else: gridscope[key] = value return gridscope","if isinstance(value, np.ndarray):"
"def get_last_tagged(self): if not self.last_tagged: last = datetime(1970, 1, 1) for tag in self.tags: <IF_STMT> last = tag.last_seen self.update(set__last_tagged=last) return last else: return self.last_tagged",if tag.last_seen > last:
"def recalculate_user_disk_usage(app, **kwargs): user_id = kwargs.get('user_id', None) sa_session = app.model.context if user_id: user = sa_session.query(app.model.User).get(app.security.decode_id(user_id)) <IF_STMT> user.calculate_and_set_disk_usage() else: log.error('Recalculate user disk usage task failed, user %s not found' % user_id) else: log.error('Recalculate user disk usage task received without user_id.')",if user:
"def log_items(self, interface, action, media, items): if not items: return for item in items: <IF_STMT> continue log.info('[%s:%s](%s) %r (%r)', interface, action, media, item.get('title'), item.get('year')) if media == 'shows': self.log_episodes(item)",if not item:
"def test_unbiased_coin_has_no_second_order(): counts = Counter() for i in range(256): buf = bytes([i]) data = ConjectureData.for_buffer(buf) result = cu.biased_coin(data, 0.5) <IF_STMT> counts[result] += 1 assert counts[False] == counts[True] > 0",if data.buffer == buf:
"def gettempfilename(suffix): """"""Returns a temporary filename"""""" if '_' in os.environ: <IF_STMT> tmpdir = '.' if 'TMP' in os.environ: tmpdir = os.environ['TMP'] import time import random random.seed(time.time()) random_part = 'file%d' % random.randint(0, 1000000000) return os.path.join(tmpdir, random_part + suffix) return tempfile.mktemp(suffix)",if os.environ['_'].find('wine') >= 0:
"def _get_functionapp_runtime_language(self, app_settings): functions_worker_runtime = [setting['value'] for setting in app_settings if setting['name'] == 'FUNCTIONS_WORKER_RUNTIME'] if functions_worker_runtime: functionapp_language = functions_worker_runtime[0] <IF_STMT> return SUPPORTED_LANGUAGES[functionapp_language] raise LanguageNotSupportException(functionapp_language) return None",if SUPPORTED_LANGUAGES.get(functionapp_language) is not None:
"def seek(self, offset, whence=io.SEEK_SET): if self.mode == WRITE: if whence != io.SEEK_SET: if whence == io.SEEK_CUR: offset = self.offset + offset else: raise ValueError('Seek from end not supported') <IF_STMT> raise OSError('Negative seek in write mode') count = offset - self.offset chunk = bytes(1024) for i in range(count // 1024): self.write(chunk) self.write(bytes(count % 1024)) elif self.mode == READ: self._check_not_closed() return self._buffer.seek(offset, whence) return self.offset",if offset < self.offset:
"def stop(self): """"""Stop the HTTP server."""""" if self.running: self.httpserver.stop() <IF_STMT> portend.free(*self.bound_addr, timeout=Timeouts.free) self.running = False self.bus.log('HTTP Server %s shut down' % self.httpserver) else: self.bus.log('HTTP Server %s already shut down' % self.httpserver)","if isinstance(self.bind_addr, tuple):"
"def dump_json(testcase, json_file): """"""dump HAR entries to json testcase"""""" logger.info('dump testcase to JSON format.') with open(json_file, 'w', encoding='utf-8') as outfile: my_json_str = json.dumps(testcase, ensure_ascii=False, indent=4) <IF_STMT> my_json_str = my_json_str.decode('utf-8') outfile.write(my_json_str) logger.info('Generate JSON testcase successfully: {}'.format(json_file))","if isinstance(my_json_str, bytes):"
"def find_comment(line): """"""Finds the index of a comment # and returns None if not found"""""" instring, instring_char = (False, '') for i, char in enumerate(line): if char in ('""', ""'""): if instring: <IF_STMT> instring = False instring_char = '' else: instring = True instring_char = char elif char == '#': if not instring: return i return None",if char == instring_char:
"def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] <IF_STMT> links = rule.process_links(links) for link in links: seen.add(link) request = self._build_request(n, link) yield rule._process_request(request, response)",if links and rule.process_links:
"def _process_iter(self, line_iter): samples = [] buf = [] for line in line_iter: if not buf and line.startswith('#') and self._has_comment: continue line = line.split() <IF_STMT> buf.append(line) elif buf: samples.append(tuple(map(list, zip(*buf)))) buf = [] if buf: samples.append(tuple(map(list, zip(*buf)))) return samples",if line:
"def _set_input_expanded(self, inp, expand, scroll=True): getobj = self._builder.get_object arrow = getobj('by%s_expander_arrow' % (inp.name,)) grid = getobj('by%s_curve_grid' % (inp.name,)) if expand: arrow.set_property('arrow-type', Gtk.ArrowType.DOWN) grid.show_all() <IF_STMT> GLib.idle_add(self._scroll_setting_editor, grid) else: arrow.set_property('arrow-type', Gtk.ArrowType.RIGHT) grid.hide()",if scroll:
"def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 <IF_STMT> groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return (groups, words)",elif group > 1:
"def add_field_to_csv_file(fieldName, fieldNameMap, fieldsList, fieldsTitles, titles): for ftList in fieldNameMap[fieldName]: <IF_STMT> fieldsList.append(ftList) fieldsTitles[ftList] = ftList add_titles_to_csv_file([ftList], titles)",if ftList not in fieldsTitles:
"def get_transform(self, img): check_dtype(img) assert img.ndim in [2, 3], img.ndim from .transform import LazyTransform, TransformList tfms = [] for idx, a in enumerate(self.augmentors): if idx == 0: t = a.get_transform(img) else: t = LazyTransform(a.get_transform) <IF_STMT> tfms.extend(t.tfms) else: tfms.append(t) return TransformList(tfms)","if isinstance(t, TransformList):"
"def __init__(self, template, context, body_stream=None): if body_stream is None: <IF_STMT> raise RuntimeError('Async mode requires a body stream to be passed to a template module.  Use the async methods of the API you are using.') body_stream = list(template.root_render_func(context)) self._body_stream = body_stream self.__dict__.update(context.get_exported()) self.__name__ = template.name",if context.environment.is_async:
"def url_locations(urls, faker=False): locations = [] for url in urls: <IF_STMT> response = request.urlopen(request.Request(url, headers=fake_headers), None) else: response = request.urlopen(request.Request(url)) locations.append(response.url) return locations",if faker:
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): <IF_STMT> all_satisfy = False readies[idx] = count_fun(selector) break if all_satisfy: break if timeout and timeout + start_time < time.time(): raise TimeoutError('Wait cluster start timeout') time.sleep(1)",if readies[idx] < min_counts[idx]:
"def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return (args, dict(kwargs)) except (TypeError, ValueError): args, kwargs = ((), {}) if a is not None: if isinstance(a, dict): args = tuple() kwargs = a elif isinstance(a, tuple): <IF_STMT> args, kwargs = (a[0:-1], a[-1]) else: args = a kwargs = {} return (args, kwargs)","if isinstance(a[-1], dict):"
"def _override_options(options, **overrides): """"""Override options."""""" for opt, val in overrides.items(): passed_value = getattr(options, opt, _Default()) if opt in ('ignore', 'select') and passed_value: value = process_value(opt, passed_value.value) value += process_value(opt, val) setattr(options, opt, value) <IF_STMT> setattr(options, opt, process_value(opt, val))","elif isinstance(passed_value, _Default):"
"def get_first_file_by_stem(dir_path, stem, exts=None): dir_path = Path(dir_path) stem = stem.lower() if dir_path.exists(): for x in sorted(list(scandir(str(dir_path))), key=lambda x: x.name): <IF_STMT> continue xp = Path(x.path) if xp.stem.lower() == stem and (exts is None or xp.suffix.lower() in exts): return xp return None",if not x.is_file():
"def testShortCircuit(self): """"""Test that creation short-circuits to reuse existing references"""""" sd = {} for s in self.ss: sd[s] = 1 for t in self.ts: <IF_STMT> self.assert_(sd.has_key(safeRef(t.x))) self.assert_(safeRef(t.x) in sd) else: self.assert_(sd.has_key(safeRef(t))) self.assert_(safeRef(t) in sd)","if hasattr(t, 'x'):"
"def _gen_Less(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered('<', lhs, rhs)) <IF_STMT> result.append(self.builder.icmp_signed('<', lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)",elif ret_type == int_type:
def _resolve_aliases(tasks_or_files): for task_or_file in tasks_or_files: <IF_STMT> for t_or_f in _resolve_aliases(task_or_file.deps): yield t_or_f else: yield task_or_file,"if isinstance(task_or_file, Alias):"
"def report(properties): for name, value in properties: <IF_STMT> if hasattr(value, 'uniobj'): value = value.uniobj line = base64.b64decode(value.encode()).decode() + '\n\n' terminalreporter.write_line(line)",if name.startswith('hypothesis-statistics-'):
"def throw_404(self, n): _dirname = os.path.dirname(sverchok.__file__) path1 = os.path.join(_dirname, 'docs', '404.html') path2 = os.path.join(_dirname, 'docs', '404_custom.html') with open(path1) as origin: with open(path2, 'w') as destination: for line in origin: <IF_STMT> destination.write(line.replace('{{variable}}', n.bl_label)) else: destination.write(line) webbrowser.open(path2)",if '{{variable}}' in line:
"def rm_empty_dirs(dirpath, interactive=False, dry_run=False): for name in os.listdir(dirpath): path = join(dirpath, name) <IF_STMT> rm_empty_dirs(path, interactive, dry_run) if not os.listdir(dirpath): if interactive: raise NotImplementedError(""'-i' not implemented"") if dry_run: log.info(""rmdir `%s' (dry-run)"", dirpath) else: log.info(""rmdir `%s'"", dirpath) os.rmdir(dirpath)",if isdir(path):
"def get_run_cmd(submission_dir): """"""Get the language of a submission"""""" with CD(submission_dir): <IF_STMT> with open('run.sh') as f: for line in f: if line[0] != '#': return line.rstrip('\r\n')",if os.path.exists('run.sh'):
"def _do_test_fetch_result(self, results, remote): self.assertGreater(len(results), 0) self.assertIsInstance(results[0], FetchInfo) for info in results: self.assertIsInstance(info.note, string_types) if isinstance(info.ref, Reference): self.assertTrue(info.flags) self.assertIsInstance(info.ref, (SymbolicReference, Reference)) <IF_STMT> self.assertIsInstance(info.old_commit, Commit) else: self.assertIsNone(info.old_commit)",if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD):
"def __set__(self, instance, value): super().__set__(instance, value) value = instance._data[self.name] if value is not None: <IF_STMT> instance._data[self.name] = self._convert_from_datetime(value) else: instance._data[self.name] = value","if isinstance(value, datetime.datetime):"
"def put(self, can_split=False): if isinstance(self.expr, NodeConst): <IF_STMT> self.expr.put() else: self.line_more('(') self.expr.put(can_split=True) self.line_more(')') else: self.put_expr(self.expr, can_split=can_split) self.line_more('.') self.line_more(NAME_SPACE.make_attr_name(self.expr, self.attrname)) return self",if self.expr.is_str():
"def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment <IF_STMT> continue if '-' in egg_fragment: key = '-'.join(egg_fragment.split('-')[:-1]).lower() else: key = egg_fragment if key == dist.key: return url.split('#', 1)[0] return None",if not egg_fragment:
"def _parse_lines(self, lines): for line in lines: self.size += len(line) words = line.strip().split('\t') if len(words) > 1: wset = set(words[1:]) <IF_STMT> self.WORDS[words[0]] |= wset else: self.WORDS[words[0]] = wset",if words[0] in self.WORDS:
"def __call__(self, target): if not self.check_run_always: for algo in self.algos: if not algo(target): return False return True else: res = True for algo in self.algos: <IF_STMT> res = algo(target) elif hasattr(algo, 'run_always'): if algo.run_always: algo(target) return res",if res:
"def _cmd_flags_as_data(cmd_flags): data = {} for flag_name, cmd_flag in cmd_flags.items(): cmd_flag_data = _cmd_flag_as_data(cmd_flag) <IF_STMT> data[flag_name] = cmd_flag_data return data",if cmd_flag_data:
"def _csv_iterator(data_path, ngrams, yield_cls=False): tokenizer = get_tokenizer('basic_english') with io.open(data_path, encoding='utf8') as f: reader = unicode_csv_reader(f) for row in reader: tokens = ' '.join(row[1:]) tokens = tokenizer(tokens) <IF_STMT> yield (int(row[0]) - 1, ngrams_iterator(tokens, ngrams)) else: yield ngrams_iterator(tokens, ngrams)",if yield_cls:
"def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) if start == -1: start = index elif char in BRACKETS: if not stack: return (-1, -1) <IF_STMT> return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)",if stack.pop() != BRACKETS[char]:
def get_and_set_be_comp(self): all_be_comp = [] for page in self.pages: if page.relations.be_comp_norm is not None: all_be_comp.extend(page.relations.be_comp_norm) <IF_STMT> all_be_comp.extend(page.relations.be_comp) return set(all_be_comp),if page.relations.be_comp is not None:
"def iterload(self): delim = self.options.delimiter rowdelim = self.options.row_delimiter with self.source.open_text() as fp: with Progress(total=filesize(self.source)) as prog: for line in splitter(fp, rowdelim): <IF_STMT> continue prog.addProgress(len(line)) row = list(line.split(delim)) if len(row) < self.nVisibleCols: row.extend([None] * (self.nVisibleCols - len(row))) yield row",if not line:
"def process_module(name, module, parent): if parent: modules[parent]['items'].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) if get_module_type(name) == 'py3status': module['.group'] = parent for k, v in list(module.items()): if k.startswith('on_click'): process_onclick(k, v, name) del module[k] <IF_STMT> module['items'] = [] return module","if isinstance(v, ModuleDefinition):"
"def test_identify_accepts_space_separated_hosts(self): ru, iu = self.mock_all_identify() file_ip = open(tests.VALID_FILE_IP) for i, line in enumerate(file_ip): <IF_STMT> expected_url, expected_host = ('http://192.168.1.1/', 'example.com') elif i == 2: expected_url, expected_host = ('http://192.168.1.2/drupal/', 'example.com') identify_line(line) args, kwargs = ru.call_args_list[-1] self.assertEquals(args[0], expected_url) self.assertEquals(args[1], expected_host)",if i < 2:
"def get_version(module): for key in version_keys: <IF_STMT> version = getattr(module, key) if isinstance(version, types.ModuleType): version = get_version(version) return version return 'Unknown'","if hasattr(module, key):"
"def whoami(self): """"""Return user relevant login information."""""" account_data = {} for k in ('email', 'account_id'): value = self.conf.get(k) <IF_STMT> account_info = self.get_account_information() value = account_info.get(k, 'unknown') self.conf.set(k, value) self.conf.save() account_data[k] = value return account_data",if not value:
def do(self): if self.in_class_scope(): selected_str = self.view.substr(self.selected_region) for symbol in self.view.symbols(): <IF_STMT> self.view.sel().clear() self.view.sel().add(symbol[0]) self.view.show(symbol[0]) return self.window.run_command('goto_definition'),if symbol[1] == selected_str:
"def __iter__(self): i = 0 for category, filename in list(self.input_files.items()): for line in open(filename): line = self._clean_line(line) <IF_STMT> yield Opinion(line, category) i += 1 if i % 1000 == 0: print('\tReaded {} examples'.format(i))",if self.accept_criteria(i):
"def recvmsg(self, *args): while True: try: return self._sock.recvmsg(*args) except error as ex: <IF_STMT> raise self._wait(self._read_event)",if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
"def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: <IF_STMT> if field.attname == 'id': continue elif field.attname.endswith('ptr_id'): continue if getattr(field, 'editable', True): fds.add(field.attname) return fds","if hasattr(field, 'attname'):"
"def prepare_fields(all_fields, submit_fields, submit): if len(list(submit_fields.items(multi=True))) > 1: if not submit: raise exceptions.InvalidSubmitError() <IF_STMT> raise exceptions.InvalidSubmitError() return _filter_fields(all_fields, lambda f: not isinstance(f, fields.Submit) or f == submit) return all_fields",if submit not in submit_fields.getlist(submit.name):
"def tag_configure(self, *args, **keys): if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get('foreground') underline = keys.get('underline') <IF_STMT> self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace('oops', args, keys)",if val:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = code == 501 and re.search('Reference #[0-9A-Fa-f.]+', page, re.I) is not None <IF_STMT> break return retval",if retval:
"def refine_pointer_names_input(lines): """"""Return  a list of width_info_t. Skip comments and blank lines"""""" global comment_pattern widths_list = [] for line in lines: pline = comment_pattern.sub('', line).strip() if pline == '': continue wrds = pline.split() ntokens = len(wrds) <IF_STMT> bbytes, name, suffix = wrds else: die('Bad number of tokens on line: ' + line) widths_list.append((bbytes, name, suffix)) return widths_list",if ntokens == 3:
"def notify(title, message, retcode=None): """"""Sends message over Telegram using telegram-send, title is ignored."""""" if not path.exists(config_file): <IF_STMT> makedirs(config_dir) print('Follow the instructions to configure the Telegram backend.\n') configure(config_file) send(messages=[message], conf=config_file)",if not path.exists(config_dir):
"def find_on_path(targets): """"""Search the PATH for a program and return full path"""""" if sabnzbd.WIN32: paths = os.getenv('PATH').split(';') else: paths = os.getenv('PATH').split(':') if isinstance(targets, str): targets = (targets,) for path in paths: for target in targets: target_path = os.path.abspath(os.path.join(path, target)) <IF_STMT> return target_path return None","if os.path.isfile(target_path) and os.access(target_path, os.X_OK):"
"def test_name_attribute(self): for cons in self.hash_constructors: h = cons() self.assertIsInstance(h.name, str) <IF_STMT> self.assertIn(h.name, self.supported_hash_names) else: self.assertNotIn(h.name, self.supported_hash_names) self.assertEqual(h.name, hashlib.new(h.name).name)",if h.name in self.supported_hash_names:
"def find_marriage(database, family): """"""find the marriage of a family"""""" for event_ref in family.get_event_ref_list(): event = database.get_event_from_handle(event_ref.ref) <IF_STMT> return event return None",if event and event.type.is_marriage() and event_ref.role.is_family():
"def test_find_ancestors(self): vhsblocks = self.config.parser_root.find_blocks('VirtualHost') macro_test = False nonmacro_test = False for vh in vhsblocks: <IF_STMT> ancs = vh.find_ancestors('Macro') self.assertEqual(len(ancs), 1) macro_test = True else: ancs = vh.find_ancestors('Macro') self.assertEqual(len(ancs), 0) nonmacro_test = True self.assertTrue(macro_test) self.assertTrue(nonmacro_test)",if '/macro/' in vh.metadata['augeaspath'].lower():
def readline(self): while 1: line = self._readline() <IF_STMT> self._filelineno += 1 return line if not self._file: return line self.nextfile(),if line:
"def read_oclc(fields): if '035' not in fields: return {} found = [] for line in fields['035']: for v in get_subfield_values(line, ['a']): m = re_oclc.match(v) if not m: continue oclc = m.group(1) <IF_STMT> found.append(oclc) return {'oclc_number': found} if found else {}",if oclc not in found:
"def get_new_unlinked_nodes(before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict): affected_nodes = [] for node_id, socket in zip(before_inputted_nodes, before_input_sockets): if not socket in input_sockets: <IF_STMT> if not node_id in affected_nodes: affected_nodes.append(node_id) return affected_nodes",if node_id in nodes_dict:
"def set_available_qty(self): for d in self.get('required_items'): <IF_STMT> d.available_qty_at_source_warehouse = get_latest_stock_qty(d.item_code, d.source_warehouse) if self.wip_warehouse: d.available_qty_at_wip_warehouse = get_latest_stock_qty(d.item_code, self.wip_warehouse)",if d.source_warehouse:
"def _unique_product_recursive(pools, result, i): if i >= len(pools): yield tuple(result) return for e in pools[i]: <IF_STMT> result[i] = e yield from _unique_product_recursive(pools, result, i + 1) result[i] = _SENTINEL",if e not in result:
def fileno(self): try: return self.sock.fileno() except socket.error: self.close() ex = sys.exc_info()[1] <IF_STMT> raise EOFError() else: raise,if get_exc_errno(ex) == errno.EBADF:
"def expand_block(self, feat): """"""Expand any blocks which are near the start or end of a contig."""""" chrom_end = self._ref_sizes.get(feat.chrom) if chrom_end: if feat.start < self._end_buffer: feat.start = 0 <IF_STMT> feat.stop = chrom_end return feat",if feat.stop >= chrom_end - self._end_buffer:
"def prepare_parser(self, parser): docs = [self.parse_doc(doc) for doc in (self.doc, __doc__) if doc] for doc in docs: for long_opt, help in items(doc): option = parser._option_string_actions[long_opt] <IF_STMT> option.help = ' '.join(help).format(default=option.default) return parser",if option is not None:
"def negate(monad): sql = monad.getsql()[0] translator = monad.translator if translator.dialect == 'Oracle': result_sql = ['IS_NULL', sql] else: result_sql = ['EQ', sql, ['VALUE', '']] if monad.nullable: <IF_STMT> result_sql = ['OR', result_sql, ['IS_NULL', sql]] else: result_sql = ['EQ', ['COALESCE', sql, ['VALUE', '']], ['VALUE', '']] result = BoolExprMonad(result_sql, nullable=False) result.aggregated = monad.aggregated return result","if isinstance(monad, AttrMonad):"
"def _ReadN(self, stdin_fd, n): chunks = [] bytes_left = n while bytes_left > 0: chunk = posix.read(stdin_fd, n) <IF_STMT> break chunks.append(chunk) bytes_left -= len(chunk) s = ''.join(chunks) return s",if len(chunk) == 0:
"def instance_reader(): for epoch_index in range(epoch): if shuffle: <IF_STMT> np.random.seed(shuffle_seed) np.random.shuffle(examples) if phase == 'train': self.current_train_epoch = epoch_index for index, example in enumerate(examples): if phase == 'train': self.current_train_example = index + 1 feature = self.convert_example(index, example, self.get_labels(), self.max_seq_len, self.tokenizer) instance = self.generate_instance(feature) yield instance",if shuffle_seed is not None:
"def close(self): fileobj = self.fileobj if fileobj is None: return self.fileobj = None try: if self.mode == WRITE: fileobj.write(self.compress.flush()) write32u(fileobj, self.crc) write32u(fileobj, self.size & 4294967295) finally: myfileobj = self.myfileobj <IF_STMT> self.myfileobj = None myfileobj.close()",if myfileobj:
"def rsa_public_key_parse(key_material): import sshpubkeys.exceptions from sshpubkeys.keys import SSHKey try: <IF_STMT> key_material = key_material.encode('ascii') decoded_key = base64.b64decode(key_material).decode('ascii') public_key = SSHKey(decoded_key) except (sshpubkeys.exceptions.InvalidKeyException, UnicodeDecodeError): raise ValueError('bad key') if not public_key.rsa: raise ValueError('bad key') return public_key.rsa","if not isinstance(key_material, six.binary_type):"
"def import_type(library, name): if library.name != idaapi.cvar.idati.name: last_ordinal = idaapi.get_ordinal_qty(idaapi.cvar.idati) type_id = idaapi.import_type(library, -1, name) <IF_STMT> return last_ordinal",if type_id != idaapi.BADORD:
"def OnDropFiles(self, x, y, files): filteredList = [] if self.filenameFilter is not None: for f in files: for ext in self.filenameFilter: <IF_STMT> filteredList.append(f) else: filteredList = files if len(filteredList) > 0: self.callback(filteredList)",if f.endswith(ext) or f.endswith(ext.upper()):
"def _get_most_recent_update(self, versions): recent = None for version in versions: updated = datetime.datetime.strptime(version['updated'], '%Y-%m-%dT%H:%M:%SZ') if not recent: recent = updated <IF_STMT> recent = updated return recent.strftime('%Y-%m-%dT%H:%M:%SZ')",elif updated > recent:
"def __setstate__(self, servers_ids: List[str]): self.try_list = [] for server_id in servers_ids: <IF_STMT> self.add_to_try_list(sabnzbd.Downloader.server_dict[server_id])",if server_id in sabnzbd.Downloader.server_dict:
"def remove_command(self, command_id): for command in self.config['commands']: <IF_STMT> self.config['commands'].remove(command) component.get('EventManager').emit(ExecuteCommandRemovedEvent(command_id)) break self.config.save()",if command[EXECUTE_ID] == command_id:
"def wrapper(*args, **kargs): offspring = func(*args, **kargs) for child in offspring: for i in xrange(len(child)): <IF_STMT> child[i] = max elif child[i] < min: child[i] = min return offspring",if child[i] > max:
"def dispatch(self, request, *args, **kwargs): self.product = get_object_or_404(self.product_model, pk=kwargs['product_pk']) if not self.product.is_review_permitted(request.user): <IF_STMT> message = _('You have already reviewed this product!') else: message = _(""You can't leave a review for this product."") messages.warning(self.request, message) return redirect(self.product.get_absolute_url()) return super().dispatch(request, *args, **kwargs)",if self.product.has_review_by(request.user):
def PlayPause(self): state = self.graphManager.GetState(10) if state == 2: self.Pause() elif state == 1: self.Play() elif state == 0: <IF_STMT> self.Stop() self.PlayingItem = self.SelectedItem self.LoadFile(self.SelectedItem.Path) self.Play() else: self.Play() else: pass self.NotifyPropertyChanged('IsPlaying') self.NotifyPropertyChanged('Duration'),if self.SelectedItem != None and self.filename != self.SelectedItem.Path:
"def decref(self, *keys): for tileable_key, tileable_id in keys: if tileable_key not in self._executed_tileables: continue _graph_key, ids = self._executed_tileables[tileable_key] <IF_STMT> ids.remove(tileable_id) if len(ids) != 0: continue self.delete_data(tileable_key)",if tileable_id in ids:
"def get_git_description(self): if self.is_a_git_repo(): exit_code, stdout, stderr = execute_command_and_capture_output('git', 'describe', '--always', '--tags', '--dirty') <IF_STMT> raise PyBuilderException('Cannot determine git description: git describe failed:\n{0}'.format(stderr)) else: return stdout.strip() else: raise PyBuilderException('Cannot determine git description: project is not a git repo.')",if exit_code != 0:
"def _code_for_module(self, module): text = '""%s"" [shape=ellips]' % module.name for item in list(module.items()): <IF_STMT> text += '\n""%s""' % item text += '\n""%s"" -> ""%s""' % (module.name, item) else: text += self._code_for_module(item) text += '\n""%s"" -> ""%s""' % (module.name, item.name) return text","if isinstance(item, str):"
"def test_images_p_is_stochastic_parameter(self): aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3])) seen = [0, 0] for _ in sm.xrange(1000): observed = aug.augment_image(self.image) <IF_STMT> seen[0] += 1 elif np.array_equal(observed, self.image_flipped): seen[1] += 1 else: assert False assert np.allclose(seen, [700, 300], rtol=0, atol=75)","if np.array_equal(observed, self.image):"
"def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str: if self.objtype == 'function': <IF_STMT> return _('%s() (built-in function)') % name_cls[0] return _('%s() (in module %s)') % (name_cls[0], modname) elif self.objtype == 'data': if not modname: return _('%s (built-in variable)') % name_cls[0] return _('%s (in module %s)') % (name_cls[0], modname) else: return ''",if not modname:
"def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None): del debug_context for attribute_name, attribute in six.iteritems(self._attributes): attribute_value = attribute.to_xml_string(prefix_root) if attribute_name == self._spec.identifier and attribute_value is None: xml_element.set(attribute_name, self.full_identifier) <IF_STMT> continue else: xml_element.set(attribute_name, attribute_value)",elif attribute_value is None:
def index_def(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.index_def_ is None: self.index_def_ = Index() finally: self.lazy_init_lock_.release() return self.index_def_,if self.index_def_ is None:
"def _ord_to_str(ordinal, weights): """"""Reverse function of _str_to_ord."""""" chars = [] for weight in weights: <IF_STMT> return ''.join(chars) ordinal -= 1 index, ordinal = divmod(ordinal, weight) chars.append(_ALPHABET[index]) return ''.join(chars)",if ordinal == 0:
"def tip_texts(self): """"""Return the tip texts of the Toolbar (without window text)"""""" texts = [] for i in range(0, self.button_count()): btn_tooltip_index = self.get_button_struct(i).iString <IF_STMT> btn_tooltip_index = i btn_text = self.get_tool_tips_control().get_tip_text(btn_tooltip_index + 1) texts.append(btn_text) return texts",if not -1 <= btn_tooltip_index < self.get_tool_tips_control().tool_count():
def _initCaseSets(self): self._cs = {} self._css = {} for cs in self._caseSets: if not self._cs.has_key(cs.CaseSetName): self._cs[cs.CaseSetName] = {} self._css[cs.CaseSetName] = cs else: raise Exception('duplicate case set name') for c in cs.Cases: idx = tuple(c.index) <IF_STMT> self._cs[cs.CaseSetName][idx] = c else: raise Exception('duplicate case index'),if not self._cs[cs.CaseSetName].has_key(idx):
"def is_image(self, input): try: if isinstance(input, (np.ndarray, Image.Image)): return True <IF_STMT> if not os.path.isfile(input): raise ValueError('input must be a file') img = Image.open(input) _ = img.size return True else: return False except: return False","elif isinstance(input, str):"
"def __init__(self, opt, shared=None): super().__init__(opt, shared) if not shared: self.episodes = [] self.num_exs = 0 <IF_STMT> self._setup_data(opt.get('parlaidialogteacher_datafile')) else: self.episodes = shared['episodes'] self.num_exs = sum((len(e) for e in self.episodes)) self.id = opt['task'] self.reset()",if opt.get('parlaidialogteacher_datafile') is not None:
"def draw(l, n, th=2): clear() l = l * f ** n shapesize(l / 100.0, l / 100.0, th) for k in tiledict: h, x, y = k setpos(x, y) setheading(h) <IF_STMT> shape('kite') color('black', (0, 0.75, 0)) else: shape('dart') color('black', (0.75, 0, 0)) stamp()",if tiledict[k]:
"def visit_Assign(self, node): if len(node.targets) == 1: if isinstance(node.targets[0], ast.Subscript): plugPath = self.__plugPath(self.__path(node.targets[0])) <IF_STMT> self.plugWrites.add(plugPath) self.visit(node.value)",if plugPath:
"def StripTypeInfo(rendered_data): """"""Strips type information from rendered data. Useful for debugging."""""" if isinstance(rendered_data, (list, tuple)): return [StripTypeInfo(d) for d in rendered_data] elif isinstance(rendered_data, dict): <IF_STMT> return StripTypeInfo(rendered_data['value']) else: result = {} for k, v in rendered_data.items(): result[k] = StripTypeInfo(v) return result else: return rendered_data",if 'value' in rendered_data and 'type' in rendered_data:
"def _match_greater_than_or_equal(search_base, attribute, value, candidates): matches = list() for entry in candidates: dn = entry.get('dn') <IF_STMT> continue value_from_directory = entry.get('attributes').get(attribute) if str(value_from_directory) >= str(value): entry['type'] = 'searchResEntry' matches.append(entry) return matches",if not dn.endswith(search_base):
"def _get_changes(diff): """"""Get a list of changed versions from git."""""" changes_dict = {} for line in diff: if not line.startswith('-') and (not line.startswith('+')): continue if line.startswith('+++ ') or line.startswith('--- '): continue name, version = parse_versioned_line(line[1:]) <IF_STMT> changes_dict[name] = Change(name) if line.startswith('-'): changes_dict[name].old = version elif line.startswith('+'): changes_dict[name].new = version return [change for _name, change in sorted(changes_dict.items())]",if name not in changes_dict:
"def append_row(tbody, cells): row = nodes.row() tbody += row for cell in cells: entry = nodes.entry() row += entry <IF_STMT> node = nodes.paragraph(text=cell) else: node = cell entry += node","if isinstance(cell, six.text_type):"
"def _testdata_to_is_unauthed_access_permitted(tests_config, node_type): res = [] for x in tests_config['endpoint_tests']: if node_type not in x['type']: continue <IF_STMT> continue h = x['tests']['is_unauthed_access_permitted'] for p in h['locations']: res.append((p, h.get('vhost', None))) return res",if 'is_unauthed_access_permitted' not in x['tests']:
"def process_ceph_status(output): res = patternchk.search(output) if not res: return {} ceph_stats = res.group() if not ceph_stats: return {} ret = {} rd = wr = iops = None rd = numberchk.search(ceph_stats) if rd is not None: ret['rd'] = rd.group() wr = numberchk.search(ceph_stats, rd.end()) if wr is not None: ret['wr'] = wr.group() iops = numberchk.search(ceph_stats, wr.end()) <IF_STMT> ret['iops'] = iops.group() return ret",if iops is not None:
"def construct_type_storage_plugin_registry(pipeline_def, system_storage_def): from dagster.core.definitions import PipelineDefinition, SystemStorageDefinition check.inst_param(pipeline_def, 'pipeline_def', PipelineDefinition) check.inst_param(system_storage_def, 'system_storage_def', SystemStorageDefinition) type_plugins = [] for type_obj in pipeline_def.all_runtime_types(): for auto_plugin in type_obj.auto_plugins: <IF_STMT> type_plugins.append((type_obj, auto_plugin)) return TypeStoragePluginRegistry(type_plugins)",if auto_plugin.compatible_with_storage_def(system_storage_def):
"def attr(**kw): kw = kw.items() kw.sort() parts = [] for name, value in kw: <IF_STMT> continue if name.endswith('_'): name = name[:-1] parts.append('%s=""%s""' % (html_quote(name), html_quote(value))) return html(' '.join(parts))",if value is None:
"def test_shape(): from lasagne.init import Initializer for klass in Initializer.__subclasses__(): <IF_STMT> for sub_klass in klass.__subclasses__(): assert sub_klass().sample((12, 23)).shape == (12, 23) else: assert klass().sample((12, 23)).shape == (12, 23)",if len(klass.__subclasses__()):
"def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: if key == KDTREE_KEY: continue item = data[key] if torch.is_tensor(item) and num_points == item.shape[0]: item = item[self._indices].clone() <IF_STMT> item = item.clone() setattr(new_data, key, item) return new_data",elif torch.is_tensor(item):
"def vars(self): ret = [] if op.disklist: varlist = op.disklist elif not op.full: varlist = ('total',) else: varlist = [] for name in self.discover: <IF_STMT> continue varlist.append(name) varlist.sort() for name in varlist: if name in self.discover + ['total'] or name in op.diskset: ret.append(name) return ret",if self.diskfilter.match(name):
"def _convertNbBytesinNbBits(self, nbBytes): nbMinBit = None nbMaxBit = None if nbBytes is not None: <IF_STMT> nbMinBit = nbBytes * 8 nbMaxBit = nbMinBit else: if nbBytes[0] is not None: nbMinBit = nbBytes[0] * 8 if nbBytes[1] is not None: nbMaxBit = nbBytes[1] * 8 return (nbMinBit, nbMaxBit)","if isinstance(nbBytes, int):"
"def after_test(self, results, tmp_dir): return_data = dict() if not results or not results.get('data'): return return_data for filename in results['data']: <IF_STMT> continue with open(filename, 'r') as f: log_content = f.read() log_analyser.make_log_analyses(log_content, return_data) return return_data","if not has_ext(filename, '.log'):"
"def ensure_vm_was_torn_down(): vm_labels = [] for vm_ref in xenapi_fake.get_all('VM'): vm_rec = xenapi_fake.get_record('VM', vm_ref) <IF_STMT> vm_labels.append(vm_rec['name_label']) self.assertEquals(vm_labels, ['1'])",if not vm_rec['is_control_domain']:
"def spool_print(*args, **kwargs): with _print_lock: if framework.Framework._spool: framework.Framework._spool.write(f'{args[0]}{os.linesep}') framework.Framework._spool.flush() <IF_STMT> return builtins._print(*args, **kwargs)",if framework.Framework._mode == Mode.JOB:
"def _parse_lines(self, linesource): """"""Parse lines of text for functions and classes"""""" functions = [] classes = [] for line in linesource: <IF_STMT> name = self._get_object_name(line) if not name.startswith('_'): functions.append(name) elif line.startswith('class '): name = self._get_object_name(line) if not name.startswith('_'): classes.append(name) else: pass functions.sort() classes.sort() return (functions, classes)",if line.startswith('def ') and line.count('('):
"def test_connect_using_sslcontext_verified(self): with support.transient_internet(self.testServer): can_verify = check_ssl_verifiy(self.testServer, self.remotePort) <IF_STMT> self.skipTest(""SSL certificate can't be verified"") support.get_attribute(smtplib, 'SMTP_SSL') context = ssl.create_default_context() with support.transient_internet(self.testServer): server = smtplib.SMTP_SSL(self.testServer, self.remotePort, context=context) server.ehlo() server.quit()",if not can_verify:
"def generate_segment_memory(chart_type, race_configs, environment): structures = [] for race_config in race_configs: <IF_STMT> title = chart_type.format_title(environment, race_config.track, es_license=race_config.es_license, suffix='%s-segment-memory' % race_config.label) chart = chart_type.segment_memory(title, environment, race_config) if chart: structures.append(chart) return structures",if 'segment_memory' in race_config.charts:
"def __iter__(self): line = b'' while True: data = self.read(-1) <IF_STMT> break generator = StringIO(data) assert b'\n' not in line, line line += next(generator) if line.endswith(b'\n'): yield line line = b'' ll = list(generator) if not ll: continue for line in ll[:-1]: yield line line = ll[-1] if line.endswith(b'\n'): yield line line = b'' if line: yield line",if not data:
"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] if x.type in float_types: return (gz * sgn(x),) return (gz * x / abs(x),)",if x.type in discrete_types:
"def is_ncname(name): first = name[0] if first == '_' or category(first) in NAME_START_CATEGORIES: for i in xrange(1, len(name)): c = name[i] <IF_STMT> if c in ALLOWED_NAME_CHARS: continue return 0 return 1 else: return 0",if not category(c) in NAME_CATEGORIES:
"def _read_rows_from(self, avro_reader, header): count = 0 maximum = self.limit if self.limit is not None else sys.maxsize for i, record in enumerate(avro_reader): if i < self.skip: continue <IF_STMT> break count += 1 row = self._map_row_from(header, record) yield row",if count >= maximum:
"def decorated(cls, *args, **kwargs): storage_res = STORAGE_RES_MAPPING[cls.__class__.__name__][func.__name__] with utils.patch_vnxsystem as patched_vnx: <IF_STMT> patched_vnx.return_value = storage_res[DEFAULT_STORAGE_RES] adapter = PROTOCOL_MAPPING[protocol](cls.configuration) return func(cls, adapter, storage_res, *args, **kwargs)",if DEFAULT_STORAGE_RES in storage_res:
"def _replace_file(src, dst): try: <IF_STMT> raise OSError('Could not replace ""%s"" -> ""%s""' % (src, dst)) except: time.sleep(0.5) if not _MoveFileEx(src, dst, 1): raise OSError('Could not replace ""%s"" -> ""%s""' % (src, dst))","if not _MoveFileEx(src, dst, 1):"
"def read_track_raw(self, redundancy=1): self._log('read track raw') data = [] await self.lower.write([CMD_READ_RAW, redundancy]) while True: packet = await self.lower.read() <IF_STMT> raise GlasgowAppletError('FIFO overflow while reading track') elif packet[-1] == 254: data.append(packet[:-1]) return b''.join(data) else: data.append(packet)",if packet[-1] == 255:
"def get_template_sources(self, template_name, template_dirs=None): template_name = self.prepare_template_name(template_name) for loader in self.template_source_loaders: <IF_STMT> try: for result in loader.get_template_sources(template_name, template_dirs): yield result except UnicodeDecodeError: raise except ValueError: pass","if hasattr(loader, 'get_template_sources'):"
"def __init__(self, reg, shtype, shimm, va): if shimm == 0: if shtype == S_ROR: shtype = S_RRX <IF_STMT> shimm = 32 self.reg = reg self.shtype = shtype self.shimm = shimm self.va = va",elif shtype == S_LSR or shtype == S_ASR:
"def pop_many(self, limit=None): if limit is None: limit = DEFAULT_SYNC_OFFLINE_ACTIVITY heartbeats = [] count = 0 while count < limit: heartbeat = self.pop() if not heartbeat: break heartbeats.append(heartbeat) count += 1 <IF_STMT> yield heartbeats heartbeats = [] if heartbeats: yield heartbeats",if count % HEARTBEATS_PER_REQUEST == 0:
"def _set_live(self, live, _): if live is not None and (not self.live): <IF_STMT> live = [live] if len(live) == 0: mode = 'Memory' elif len(live) == 1: mode = live[0] else: raise RuntimeError('--live parameter should specify only one mode.') live_plugin = self.session.plugins.live(mode=mode) live_plugin.live() self.session.register_flush_hook(self, live_plugin.close) return live","if isinstance(live, basestring):"
"def capture_output(redirect_stderr=True): oldout, olderr = (sys.stdout, sys.stderr) try: out = StringIO() sys.stdout = out <IF_STMT> sys.stderr = out else: sys.stderr = StringIO() yield out except: if redirect_stderr: traceback.print_exc() else: raise finally: sys.stdout, sys.stderr = (oldout, olderr)",if redirect_stderr:
"def run(self): self.mpd.connect() events = ['player'] while True: if 'player' in events: status = self.mpd.status() handler = getattr(self, 'on_' + status['state'], None) <IF_STMT> handler(status) else: self._log.debug(u'unhandled status ""{0}""', status) events = self.mpd.events()",if handler:
"def get_full_qualified_name(self, node: Element) -> str: if node.get('reftype') == 'option': progname = node.get('std:program') command = ws_re.split(node.get('reftarget')) <IF_STMT> command.insert(0, progname) option = command.pop() if command: return '.'.join(['-'.join(command), option]) else: return None else: return None",if progname:
"def _get_sources(self): servers = self.config['servers'] 'maps urls to extractors' server_links = {'mp4upload': 'mp4upload.com', 'gcloud': 'gcloud.live', 'gcloud': 'fembed.com'} soup = helpers.soupify(helpers.get(self.url)).select('iframe') for a in servers: for b in soup: for c in server_links: <IF_STMT> return [(c, b.get('src'))] logger.warn('Unsupported URL') return ''",if server_links[c] in b.get('src') and a == c:
"def _self_set(self, context): if self.keys is not None: return new_dict = context.get_pynames(['self', 'd'])[1] if new_dict and isinstance(new_dict.get_object().get_type(), Dict): args = arguments.ObjectArguments([new_dict]) items = new_dict.get_object()['popitem'].get_object().get_returned_object(args) context.save_per_name(items) else: holding = _infer_sequence_for_pyname(new_dict) <IF_STMT> context.save_per_name(holding)","if holding is not None and isinstance(holding.get_type(), Tuple):"
"def create(): """"""Create a new post for the current user."""""" if request.method == 'POST': title = request.form['title'] body = request.form['body'] error = None <IF_STMT> error = 'Title is required.' if error is not None: flash(error) else: db.session.add(Post(title=title, body=body, author=g.user)) db.session.commit() return redirect(url_for('blog.index')) return render_template('blog/create.html')",if not title:
"def _find_host_dir_ldconfig(self, arch='x86-64'): """"""Find host nvidia libraries via ldconfig"""""" dir_list = set() ld_data = Uprocess().get_output(['ldconfig', '-p']) if ld_data: regexp = '[ |\t]%s[^ ]* .*%s.*=> (/.*)' for line in ld_data.split('\n'): for lib in self._nvidia_main_libs: match = re.search(regexp % (lib, arch), line) <IF_STMT> dir_list.add(os.path.realpath(os.path.dirname(match.group(1))) + '/') return dir_list",if match:
"def migrate_replay_storage(apps, schema_editor): model = apps.get_model('terminal', 'ReplayStorage') init_storage_data(model) setting = get_setting(apps, schema_editor, 'TERMINAL_REPLAY_STORAGE') if not setting: return values = get_storage_data(setting) for name, meta in values.items(): tp = meta.pop('TYPE', None) <IF_STMT> continue model.objects.create(name=name, type=tp, meta=meta)","if not tp or name in ['default', 'null']:"
"def load_distribution(args: CommandLineArguments) -> CommandLineArguments: if args.distribution is not None: args.distribution = Distribution[args.distribution] if args.distribution is None or args.release is None: d, r = detect_distribution() if args.distribution is None: args.distribution = d <IF_STMT> args.release = r if args.distribution is None: die(""Couldn't detect distribution."") return args",if args.distribution == d and d != Distribution.clear and (args.release is None):
"def fieldset_string_to_field(fieldset_dict, model): if isinstance(fieldset_dict['fields'], tuple): fieldset_dict['fields'] = list(fieldset_dict['fields']) i = 0 for dict_field in fieldset_dict['fields']: if isinstance(dict_field, string_types): fieldset_dict['fields'][i] = model._meta.get_field_by_name(dict_field)[0] <IF_STMT> dict_field[1]['recursive'] = True fieldset_string_to_field(dict_field[1], model) i += 1","elif isinstance(dict_field, list) or isinstance(dict_field, tuple):"
"def icon(display_icon): """"""returns empty dict if show_icons is False, else the icon passed"""""" kws = {} if get_icon_switch(): if display_icon.startswith('SV_'): kws = {'icon_value': custom_icon(display_icon)} <IF_STMT> kws = {'icon': display_icon} return kws",elif display_icon != 'OUTLINER_OB_EMPTY':
"def cancel_helper(self, node, to_cancel): children = set(self.workflow.successors(node)) for child in children: <IF_STMT> to_cancel.append(child.id_) self.cancelled.append(node.id_) await self.cancel_helper(child, to_cancel) return to_cancel",if self.parent_map[child.id_] == 1:
"def getStatusString(self): if not self._isAvailable: return 'Doodle3D box not found' if self._printing: if self._blockIndex < len(self._fileBlocks): ret = 'Sending GCode: %.1f%%' % (float(self._blockIndex) * 100.0 / float(len(self._fileBlocks))) <IF_STMT> ret = 'Finished sending GCode to Doodle3D box.' else: ret = 'Different print still running...' return ret return 'Printer found, waiting for print command.'",elif len(self._fileBlocks) > 0:
"def test_archive_files_message(self): filelist = ['test.torrent', 'deluge.png'] arc_filepath = archive_files('test-arc', [get_test_data_file(f) for f in filelist], message='test') result_files = filelist + ['archive_message.txt'] with tarfile.open(arc_filepath, 'r') as tar: self.assertEqual(tar.getnames(), result_files) for tar_info in tar: self.assertTrue(tar_info.isfile()) <IF_STMT> result = tar.extractfile(tar_info).read().decode() self.assertEqual(result, 'test')",if tar_info.name == 'archive_message.txt':
"def _format_arg(self, opt, spec, val): if opt in ['in_files']: return scans_for_fnames(ensure_list(val)) if opt == 'fwhm': if not isinstance(val, list): return [val, val, val] <IF_STMT> if len(val) == 1: return [val[0], val[0], val[0]] else: return val return super(Smooth, self)._format_arg(opt, spec, val)","if isinstance(val, list):"
"def fuzzy_sum(self, currency, rounding=ROUND_UP): a = Money.ZEROS[currency].amount fuzzy = False for m in self: <IF_STMT> a += m.amount elif m.amount: a += m.convert(currency, rounding=None).amount fuzzy = True r = Money(a, currency, rounding=rounding) r.fuzzy = fuzzy return r",if m.currency == currency:
"def _read_potfiles(src_root, potfiles): """"""Returns a list of paths for a POTFILES.in file"""""" paths = [] with open(potfiles, 'r', encoding='utf-8') as h: for line in h: line = line.strip() <IF_STMT> continue paths.append(os.path.normpath(os.path.join(src_root, line))) return paths",if not line or line.startswith('#'):
"def applyMath(self, val, math, frmt): try: x = eval(val) <IF_STMT> x = eval(math) val = ('{0' + frmt + '}').format(x) except: dprint(__name__, 0, 'CCmds_applyMath: Error in math {0}, frmt {1}\n{2}', math, frmt, traceback.format_exc()) dprint(__name__, 2, 'CCmds_applyMath: {0}', val) return val",if math != '':
def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): <IF_STMT> self.save_model() if self.should_save_checkpoint(): self.save_checkpoint() if self.should_eval_model(): self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result(),if self.should_save_model():
"def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full <IF_STMT> node = '' if ifrom is None: ifrom = '' if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True",if node is None:
"def _collect(self, writer=None): for artifact_name in self.plugin_args.artifacts: for hit in self.collect_artifact(artifact_name): <IF_STMT> writer.write_result(hit['result']) yield hit",if 'result' in hit and writer:
"def proc(qtbot, caplog): """"""A fixture providing a GUIProcess and cleaning it up after the test."""""" p = guiprocess.GUIProcess('testprocess') yield p if p._proc.state() == QProcess.Running: with caplog.at_level(logging.ERROR): with qtbot.waitSignal(p.finished, timeout=10000, raising=False) as blocker: p._proc.terminate() <IF_STMT> p._proc.kill() p._proc.waitForFinished()",if not blocker.signal_triggered:
"def getsequences(self): """"""Return the set of sequences for the folder."""""" sequences = {} fullname = self.getsequencesfilename() try: f = open(fullname, 'r') except IOError: return sequences while 1: line = f.readline() <IF_STMT> break fields = line.split(':') if len(fields) != 2: self.error('bad sequence in %s: %s' % (fullname, line.strip())) key = fields[0].strip() value = IntSet(fields[1].strip(), ' ').tolist() sequences[key] = value return sequences",if not line:
def get_coeffs(e): coeffs = [] for du in all_delu_dict.keys(): <IF_STMT> coeffs.append(self.as_coeffs_dict[e]) elif du in self.as_coeffs_dict[e].keys(): coeffs.append(self.as_coeffs_dict[e][du]) else: coeffs.append(0) return np.array(coeffs),if type(self.as_coeffs_dict[e]).__name__ == 'float':
"def block_items(objekt, block, eldict): if objekt not in block: if isinstance(objekt.type, PyType): if objekt.type not in block: block.append(objekt.type) block.append(objekt) if isinstance(objekt, PyType): others = [p for p in eldict.values() if isinstance(p, PyElement) and p.type[1] == objekt.name] for item in others: <IF_STMT> block.append(item) return block",if item not in block:
"def FindPrefix(self, prefix): self.log.WriteText('Looking for prefix: %s\n' % prefix) if prefix: prefix = prefix.lower() length = len(prefix) for x in range(self.GetCount()): text = self.GetString(x) text = text.lower() <IF_STMT> self.log.WriteText('Prefix %s is found.\n' % prefix) return x self.log.WriteText('Prefix %s is not found.\n' % prefix) return -1",if text[:length] == prefix:
"def encode(self, input, errors='strict'): if self.encoder is None: result = codecs.utf_32_encode(input, errors) <IF_STMT> self.encoder = codecs.utf_32_le_encode else: self.encoder = codecs.utf_32_be_encode return result else: return self.encoder(input, errors)",if sys.byteorder == 'little':
"def __call__(self, message, keyname): if keyname in self.keyring: key = self.keyring[keyname] <IF_STMT> if message: GSSTSigAdapter.parse_tkey_and_step(key, message, keyname) return key else: return None","if isinstance(key, Key) and key.algorithm == GSS_TSIG:"
"def unicode_metrics(metrics): for i, metric in enumerate(metrics): for key, value in metric.items(): if isinstance(value, basestring): metric[key] = unicode(value, errors='replace') elif isinstance(value, tuple) or isinstance(value, list): value_list = list(value) for j, value_element in enumerate(value_list): <IF_STMT> value_list[j] = unicode(value_element, errors='replace') metric[key] = tuple(value_list) metrics[i] = metric return metrics","if isinstance(value_element, basestring):"
"def step(self, action): assert self.action_space.contains(action) if self._state == 4: if action and self._case: return (self._state, 10.0, True, {}) else: return (self._state, -10, True, {}) elif action: <IF_STMT> self._state = 2 else: self._state += 1 elif self._state == 2: self._state = self._case return (self._state, -1, False, {})",if self._state == 0:
"def get_superuser(self): try: query = dict() <IF_STMT> query[get_user_model().USERNAME_FIELD] = 'admin' else: query[get_user_model().USERNAME_FIELD] = 'admin@django-cms.org' admin = get_user_model().objects.get(**query) except get_user_model().DoesNotExist: admin = self._create_user('admin', is_staff=True, is_superuser=True) return admin",if get_user_model().USERNAME_FIELD != 'email':
"def newend(self): newenddatetime = self._newenddate if not self.checkallday.state: <IF_STMT> tzinfo = self.conf.default.default_timezone else: tzinfo = self.enddt.tzinfo try: newendtime = self._newendtime newenddatetime = datetime.combine(newenddatetime, newendtime) newenddatetime = tzinfo.localize(newenddatetime) except TypeError: return None return newenddatetime","if not hasattr(self.enddt, 'tzinfo') or self.enddt.tzinfo is None:"
"def run(self): to_delete = set() for k, v in iteritems(self.objs): if k.startswith('_'): continue <IF_STMT> to_delete.add(k) if v['_class'] == 'Task': v['submission_format'] = list((self.objs[k]['filename'] for k in v.get('submission_format', list()))) for k in to_delete: del self.objs[k] return self.objs",if v['_class'] == 'SubmissionFormatElement':
"def update_reserved_qty_for_subcontract(self): for d in self.supplied_items: <IF_STMT> stock_bin = get_bin(d.rm_item_code, d.reserve_warehouse) stock_bin.update_reserved_qty_for_sub_contracting()",if d.rm_item_code:
"def process(self): if 'Length' in self.outputs and self.outputs['Length'].is_linked: if 'Data' in self.inputs and self.inputs['Data'].is_linked: data = self.inputs['Data'].sv_get(deepcopy=False) <IF_STMT> out = [[len(data)]] elif self.level == 1: out = [self.count(data, self.level)] else: out = self.count(data, self.level) self.outputs['Length'].sv_set(out)",if not self.level:
"def _user_has_perm(user, perm, obj): anon = user.is_anonymous() for backend in auth.get_backends(): <IF_STMT> if hasattr(backend, 'has_perm'): if obj is not None: if backend.supports_object_permissions and backend.has_perm(user, perm, obj): return True elif backend.has_perm(user, perm): return True return False",if not anon or backend.supports_anonymous_user:
"def visit(self, node=None): """"""Walks over a node.  If no node is provided, the tree is used."""""" if node is None: node = self.tree if node is None: raise RuntimeError('no node or tree given!') for clsname in map(_lowername, type.mro(node.__class__)): meth = getattr(self, 'visit_' + clsname, None) <IF_STMT> rtn = meth(node) break else: msg = 'could not find valid visitor method for {0} on {1}' nodename = node.__class__.__name__ selfname = self.__class__.__name__ raise AttributeError(msg.format(nodename, selfname)) return rtn",if callable(meth):
"def add_fade_out(compositor, fade_out_length): clip = _get_compositor_clip(compositor) keyframe_property, property_klass, keyframes = _get_kfproperty_klass_and_keyframes(compositor, clip) if fade_out_length > 0: <IF_STMT> return _do_user_add_fade_out(keyframe_property, property_klass, keyframes, fade_out_length, clip) else: _show_length_error_dialog() return None",if fade_out_length + 1 <= clip.clip_length():
"def make_timesheet_records(): employees = get_timesheet_based_salary_slip_employee() for e in employees: ts = make_timesheet(e.employee, simulate=True, billable=1, activity_type=get_random('Activity Type'), company=frappe.flags.company) frappe.db.commit() rand = random.random() if rand >= 0.3: make_salary_slip_for_timesheet(ts.name) rand = random.random() <IF_STMT> make_sales_invoice_for_timesheet(ts.name)",if rand >= 0.2:
"def _target_from_batch(self, batch): targets = [] for name in self.labels: target = getattr(batch, name) <IF_STMT> label_vocab = self.metadata.target.vocab.stoi batch_label_list = getattr(batch, Target.TARGET_LABEL_FIELD) target = align_target_labels(target, batch_label_list, label_vocab) targets.append(target) if len(targets) == 1: return targets[0] return tuple(targets)","if name in [Target.TARGET_PROB_FIELD, Target.TARGET_LOGITS_FIELD]:"
"def detectForms(html): erreur = '' soup = BeautifulSoup(html, 'html.parser') detectedForms = soup.find_all('form') returnForms = [] if len(detectedForms) > 0: for f in detectedForms: fileInputs = f.findChildren('input', {'type': re.compile('file', re.I)}) <IF_STMT> returnForms.append((f, fileInputs)) return returnForms",if len(fileInputs) > 0:
"def _updateNewCardRatio(self): if self.col.conf['newSpread'] == NEW_CARDS_DISTRIBUTE: <IF_STMT> self.newCardModulus = (self.newCount + self.revCount) // self.newCount if self.revCount: self.newCardModulus = max(2, self.newCardModulus) return self.newCardModulus = 0",if self.newCount:
"def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: <IF_STMT> del comments[k] if single in lower: parts = lower[single].split('/', 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)",if k in comments:
"def check_physical(self, line): """"""Run all physical checks on a raw input line."""""" self.physical_line = line for name, check, argument_names in self._physical_checks: self.init_checker_state(name, argument_names) result = self.run_check(check, argument_names) if result is not None: offset, text = result self.report_error(self.line_number, offset, text, check) <IF_STMT> self.indent_char = line[0]",if text[:4] == 'E101':
"def dependencies(self): deps = [] midx = None if self.ref is not None: query = GroupQuery(self.ref) g = query.execute(self.schema) <IF_STMT> log.debug(self.schema) raise TypeNotFound(self.ref) deps.append(g) midx = 0 return (midx, deps)",if g is None:
"def __init__(self, metadata=None): <IF_STMT> db = get_session() metadata = lookup_feed(db, self.__feed_name__) if not metadata: raise Exception('Must have feed metadata in db already, should sync metadata before invoking instance operations') super(AnchoreServiceFeed, self).__init__(metadata=metadata)",if not metadata:
"def testGetPartRect(self): """"""Make sure the part rectangles are retrieved correctly"""""" for i in range(0, self.ctrl.part_count()): part_rect = self.ctrl.get_part_rect(i) self.assertEqual(part_rect.left, self.part_rects[i].left) <IF_STMT> self.assertEqual(part_rect.right, self.part_rects[i].right) self.assertEqual(part_rect.top, self.part_rects[i].top) self.assertFalse(abs(part_rect.bottom - self.part_rects[i].bottom) > 2) self.assertRaises(IndexError, self.ctrl.get_part_rect, 99)",if i != self.ctrl.part_count() - 1:
"def __call__(self, ctx): if ctx.range and ctx.value: if self.raw: ctx.range.raw_value = ctx.value return scalar = ctx.meta.get('scalar', False) <IF_STMT> ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0])) self._write_value(ctx.range, ctx.value, scalar)",if not scalar:
"def basic_get(self, queue, no_ack=False, **kwargs): """"""Get message by direct access (synchronous)."""""" try: message = self.Message(self._get(queue), channel=self) <IF_STMT> self.qos.append(message, message.delivery_tag) return message except Empty: pass",if not no_ack:
def http_client(cls) -> aiohttp.ClientSession: if cls._client is None: <IF_STMT> raise EnvironmentError('Event loop must be running to start HTTP client session.') cls._client = aiohttp.ClientSession(request_class=SSLClientRequest) return cls._client,if not asyncio.get_event_loop().is_running():
def createMimeType(self): audio = False for prop in self.array('header/content/stream_prop'): guid = prop['content/type'].value if guid == VideoHeader.guid: return u'video/x-ms-wmv' <IF_STMT> audio = True if audio: return u'audio/x-ms-wma' else: return u'video/x-ms-asf',if guid == AudioHeader.guid:
"def _removeCachedRFInfo(self, cache_key, path, removeChildPaths): log.debug('_removeCachedRFInfo: cache_key %r, path %r', cache_key, path) if self._cachedFiles.has_key(cache_key): cache = self._cachedFiles[cache_key] if cache.has_key(path): del cache[path] if removeChildPaths: from remotefilelib import addslash dirPath = addslash(path) for keypath in cache.keys(): <IF_STMT> del cache[keypath]",if keypath.startswith(dirPath):
"def format(self, obj, context, maxlevels, level): if isinstance(obj, unicode): return (obj, True, False) if isinstance(obj, bytes): convert = False try: codecs.decode(obj) except: convert = True <IF_STMT> return ('0x{}'.format(obj), True, False) return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)",if convert:
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: if module is None: module = self.name if section is None: section = 'all_sections' if s_name is None: s_name = f['s_name'] <IF_STMT> source = os.path.abspath(os.path.join(f['root'], f['fn'])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning('Tried to add data source for {}, but was missing fields data'.format(self.name))",if source is None:
"def open(self, *args, **kwargs): if kwargs.get('json') is not None: with self.session_transaction() as sess: api_key_headers = Headers({'CSRF-Token': sess.get('nonce')}) headers = kwargs.pop('headers', Headers()) <IF_STMT> headers = Headers(headers) headers.extend(api_key_headers) kwargs['headers'] = headers return super(CTFdTestClient, self).open(*args, **kwargs)","if isinstance(headers, dict):"
"def get_params(self): if not hasattr(self, 'input_space'): raise AttributeError('Input space has not been provided.') rval = [] for layer in self.layers: for param in layer.get_params(): <IF_STMT> logger.info(type(layer)) layer_params = layer.get_params() assert not isinstance(layer_params, set) for param in layer_params: if param not in rval: rval.append(param) rval = [elem for elem in rval if elem not in self.freeze_set] assert all([elem.name is not None for elem in rval]) return rval",if param.name is None:
"def _animate_strategy(self, speed=1): if self._animating == 0: return if self._apply_strategy() is not None: <IF_STMT> return if self._animate.get() == 1: self._root.after(3000, self._animate_strategy) elif self._animate.get() == 2: self._root.after(1000, self._animate_strategy) else: self._root.after(20, self._animate_strategy)",if self._animate.get() == 0 or self._step.get() == 1:
def charAt(pos): this.cok() pos = pos.to_int() s = this.to_string() if 0 <= pos < len(s.value): char = s.value[pos] <IF_STMT> s.Js(char) return s.CHAR_BANK[char] return s.CHAR_BANK[''],if char not in s.CHAR_BANK:
"def find_executable(names): for name in names: fpath, fname = os.path.split(name) if fpath: if is_executable(name): return name else: for path in os.environ['PATH'].split(os.pathsep): exe_file = os.path.join(path, name) <IF_STMT> return exe_file return None",if is_executable(exe_file):
"def match_file(self, file, tff_format): match = tff_format.search(file.filename.replace('\\', '/')) if match: result = {} for name, value in match.groupdict().items(): value = value.strip() <IF_STMT> value = value.lstrip('0') if self.ui.replace_underscores.isChecked(): value = value.replace('_', ' ') result[name] = value return result else: return {}",if name in self.numeric_tags:
"def __init__(self, filename: str='checkpoint', frequency: Union[int, List[int]]=1, on: Union[str, List[str]]='epoch_end'): if isinstance(frequency, list): <IF_STMT> raise ValueError('If you pass a list for checkpoint frequencies, the `on` parameter has to be a list with the same length.') self._frequency = frequency super(_TuneCheckpointCallback, self).__init__(on) self._filename = filename self._counter = Counter() self._cp_count = 0","if not isinstance(on, list) or len(frequency) != len(on):"
"def download(cls, architecture, path='./'): if cls.sanity_check(architecture): architecture_file = path + 'imagenet_{}.pth'.format(architecture) if not os.path.exists(architecture_file): kwargs = {} <IF_STMT> kwargs['transform_input'] = False model = models.__dict__[architecture](pretrained=True, **kwargs) torch.save(model, architecture_file) print('PyTorch pretrained model is saved as [{}].'.format(architecture_file)) else: print('File [{}] existed!'.format(architecture_file)) return architecture_file else: return None",if architecture == 'inception_v3':
"def __exit__(self, exc_type, exc_value, traceback): self.signal.disconnect(self._listener) if not self.signal_sent: self.test_case.fail('Signal was not sent.') return if self.required_kwargs is not None: missing_kwargs = [] for k in self.required_kwargs: <IF_STMT> missing_kwargs.append(k) if missing_kwargs: self.test_case.fail('Signal missing required arguments: %s' % ','.join(missing_kwargs))",if k not in self.received_kwargs:
"def Assign(left, right): names = [] if isinstance(left, ast.Name): return ast.Assign([ast.AssName(left.name, 'OP_ASSIGN')], right) elif isinstance(left, ast.Tuple): names = [] for child in left.getChildren(): <IF_STMT> raise SyntaxError('that assignment not supported') names.append(child.name) ass_list = [ast.AssName(name, 'OP_ASSIGN') for name in names] return ast.Assign([ast.AssTuple(ass_list)], right) else: raise SyntaxError(""Can't do that yet"")","if not isinstance(child, ast.Name):"
"def readVorbisComment(metadata, comment): metadata.producer = getValue(comment, 'vendor') for item in comment.array('metadata'): <IF_STMT> key, value = item.value.split('=', 1) key = key.upper() if key in VORBIS_KEY_TO_ATTR: key = VORBIS_KEY_TO_ATTR[key] setattr(metadata, key, value) elif value: metadata.warning('Skip Vorbis comment %s: %s' % (key, value))",if '=' in item.value:
"def _read_readable(self, readable): blocksize = 8192 if self.debuglevel > 0: print('sendIng a read()able') encode = self._is_textIO(readable) if encode and self.debuglevel > 0: print('encoding file using iso-8859-1') while True: datablock = readable.read(blocksize) if not datablock: break <IF_STMT> datablock = datablock.encode('iso-8859-1') yield datablock",if encode:
"def TryMerge(self, d): while 1: tt = d.getVarInt32() if tt == 12: break if tt == 18: self.set_value(d.getPrefixedString()) continue <IF_STMT> self.set_flags(d.get32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 29:
def needs_rebuild(self): for ratio in self.sprite.config['ratios']: cocos2d_path = self.output_path(ratio) <IF_STMT> try: data = plistlib.readPlist(cocos2d_path) assert data[self.meta_key]['hash'] == self.sprite.hash except Exception: continue return True return False,if os.path.exists(cocos2d_path):
"def on_epoch_end(self, batch, logs=None): weight_mask_ops = [] for layer in self.prunable_layers: if isinstance(layer, pruning_wrapper.PruneLowMagnitude): <IF_STMT> layer.pruning_obj.weight_mask_op() else: weight_mask_ops.append(layer.pruning_obj.weight_mask_op()) K.batch_get_value(weight_mask_ops)",if tf.executing_eagerly():
"def is_valid_origin(origin): if not settings.SENTRY_ALLOW_ORIGIN: return False if settings.SENTRY_ALLOW_ORIGIN == '*': return True if not origin: return False origin = origin.lower() for value in settings.SENTRY_ALLOW_ORIGIN: if isinstance(value, string_types): if value.lower() == origin: return True el<IF_STMT> return True return False",if value.match(origin):
"def get_menu_title(self): handle = self.obj.get_handle() if handle: who = get_participant_from_event(self.db, handle) desc = self.obj.get_description() event_name = self.obj.get_type() if desc: event_name = '%s - %s' % (event_name, desc) <IF_STMT> event_name = '%s - %s' % (event_name, who) dialog_title = _('Event: %s') % event_name else: dialog_title = _('New Event') return dialog_title",if who:
def memory(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.memory_ is None: self.memory_ = SystemStat() finally: self.lazy_init_lock_.release() return self.memory_,if self.memory_ is None:
"def __str__(self): fmt = '%#x' if isinstance(self.target, six.integer_types) else '%r' args = [] for arg in self.args: args.append(self._special_repr(arg)) name = self.name or fmt % self.target arg_str = [] for arg in args: <IF_STMT> arg_str.append(hex(arg)) else: arg_str.append(str(arg)) return '%s(%s)' % (name, ', '.join(arg_str))","if isinstance(arg, six.integer_types) and arg > 256:"
"def change_password(username='flexget', password='', session=None): check = zxcvbn.zxcvbn(password, user_inputs=[username]) if check['score'] < 3: warning = check['feedback']['warning'] suggestions = ' '.join(check['feedback']['suggestions']) message = ""Password '{}' is not strong enough. "".format(password) if warning: message += warning + ' ' <IF_STMT> message += 'Suggestions: {}'.format(suggestions) raise WeakPassword(message) user = get_user(username=username, session=session) user.password = str(generate_password_hash(password)) session.commit()",if suggestions:
"def _on_workflow_object_saved(sender, instance, created, *args, **kwargs): for instance_workflow in instance.river.all(instance.__class__): <IF_STMT> instance_workflow.initialize_approvals() if not instance_workflow.get_state(): init_state = getattr(instance.__class__.river, instance_workflow.field_name).initial_state instance_workflow.set_state(init_state) instance.save()",if created:
"def recvmsg_into(self, buffers, *args): while True: try: <IF_STMT> return self._sock.recvmsg_into(buffers, *args) return self._sock.recvmsg_into(buffers) except error as ex: if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0: raise self._wait(self._read_event)",if args:
def _generate_toc(line): while 1: <IF_STMT> line = 5 while 1: if line: line = 6 break elif not line: line = 7 break elif not line: break return 1,if line.startswith('2'):
"def tearDown(self): for filename in os.listdir(from_here('lib')): <IF_STMT> try: os.remove(from_here('lib', filename)) except OSError: pass",if filename not in self.files_to_keep:
def parse_literal_object(node): value = 0 unit = get_default_weight_unit() for field in node.fields: <IF_STMT> try: value = decimal.Decimal(field.value.value) except decimal.DecimalException: raise GraphQLError(f'Unsupported value: {field.value.value}') if field.name.value == 'unit': unit = field.value.value return Weight(**{unit: value}),if field.name.value == 'value':
"def run(self): to_delete = set() for k, v in iteritems(self.objs): if k.startswith('_'): continue if v['_class'] == 'SubmissionFormatElement': to_delete.add(k) <IF_STMT> v['submission_format'] = list((self.objs[k]['filename'] for k in v.get('submission_format', list()))) for k in to_delete: del self.objs[k] return self.objs",if v['_class'] == 'Task':
def _detect_too_many_digits(f): ret = [] for node in f.nodes: for ir in node.irs: for read in ir.read: <IF_STMT> value_as_str = read.original_value if '00000' in value_as_str: ret.append(node) return ret,"if isinstance(read, Constant):"
def split_path_info(path): path = path.strip('/') clean = [] for segment in path.split('/'): if not segment or segment == '.': continue <IF_STMT> if clean: del clean[-1] else: clean.append(segment) return tuple(clean),elif segment == '..':
"def callback(f): unfinished_children.remove(f) if not unfinished_children: try: result_list = [i.result() for i in children] except Exception: future.set_exc_info(sys.exc_info()) else: <IF_STMT> future.set_result(dict(zip(keys, result_list))) else: future.set_result(result_list)",if keys is not None:
"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if gz.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz * x * 2,)",if x.type in discrete_types:
"def perform_page_up(self, event): try: first_visible_idx = self.index('@0,0') row, _ = map(int, first_visible_idx.split('.')) <IF_STMT> self.mark_set('insert', '1.0') except Exception as e: logger.exception('Could not perform page up', exc_info=e)",if row == 1:
"def __str__(self): s = '' for k, v in self._members.items(): <IF_STMT> s += k + ' : ' + ';'.join(getattr(self, item)) + '\n' elif isinstance(v.get('type'), str): s += k + ' : ' + getattr(self, k) + '\n' return s","if isinstance(v.get('type'), list):"
"def _shared_pool(**opts): if 'host' in opts: key = '%s:%s/%s' % (opts['host'], opts['port'], opts['db']) else: key = '%s/%s' % (opts['path'], opts['db']) pool = _pool_cache.get(key) <IF_STMT> return pool with _pool_lock: pool = _pool_cache.get(key) if pool is not None: return pool pool = ConnectionPool(**opts) _pool_cache[key] = pool return pool",if pool is not None:
"def _override_settings(self, overriden_settings: dict): for setting_name, setting_value in overriden_settings.items(): value = setting_value <IF_STMT> value = getattr(self, setting_name, {}) value.update(ObjDict(setting_value)) setattr(self, setting_name, value)","if isinstance(setting_value, dict):"
"def match_tls_context(self, host: str, ir: 'IR'): for context in ir.get_tls_contexts(): hosts = context.get('hosts') or [] for context_host in hosts: <IF_STMT> ir.logger.debug('Matched host {} with TLSContext {}'.format(host, context.get('name'))) self.sni = True return context return None",if context_host == host:
"def get_form_datas(self): if self.request_method == 'get': initial = dict(self.request.GET.items()) for k in initial: try: f = self.opts.get_field(k) except models.FieldDoesNotExist: continue <IF_STMT> initial[k] = initial[k].split(',') return {'initial': initial} else: return {'data': self.request.POST, 'files': self.request.FILES}","if isinstance(f, models.ManyToManyField):"
"def run_until(loop, pred, timeout=30): deadline = time.time() + timeout while not pred(): <IF_STMT> timeout = deadline - time.time() if timeout <= 0: raise futures.TimeoutError() loop.run_until_complete(tasks.sleep(0.001, loop=loop))",if timeout is not None:
"def update_translations(): pot_path = os.path.join(root, 'messages.pot') template = read_po(open(pot_path, 'rb')) for locale in get_locales(): po_path = os.path.join(root, locale, 'messages.po') mo_path = os.path.join(root, locale, 'messages.mo') <IF_STMT> catalog = read_po(open(po_path, 'rb')) catalog.update(template) f = open(po_path, 'wb') write_po(f, catalog) f.close() print('updated', po_path) compile_translations()",if os.path.exists(po_path):
"def get_queryset_for_content_type(self, content_type_id): """"""Return the QuerySet from the QuerySetSequence for a ctype."""""" content_type = ContentType.objects.get_for_id(content_type_id) for queryset in self.queryset.get_querysets(): <IF_STMT> model = queryset.model.__bases__[0] else: model = queryset.model if model == content_type.model_class(): return queryset",if queryset.model.__name__ == 'QuerySequenceModel':
def __bypass_wizard(self): bypass = False if self.device.remote_op.dir_exist(self.project_folder): msg = 'A Tweak with the same PROJECT_NAME ({}) already exists. Do you want to delete it and start from scratch?'.format(self.options['project_name']) clean = choose_boolean(msg) <IF_STMT> self.device.remote_op.dir_delete(self.project_folder) else: bypass = True return bypass,if clean:
"def wrapper(cached=True, reset=False): nonlocal cached_venv_dir if not cached or not cached_venv_dir or reset: venv_dir = os.environ.get('_VENV_DIR_') or load_settings(lazy=True).get('venv_dir') if venv_dir: if venv_dir == 'isolated': venv_dir = VENV_DIR_ISOLATED <IF_STMT> venv_dir = VENV_DIR_SHARED else: venv_dir = VENV_DIR_SHARED cached_venv_dir = venv_dir return cached_venv_dir",elif venv_dir == 'shared':
"def run(self): while not self._stop: for i in range(0, self._interval): time.sleep(1) <IF_STMT> self.__logger.debug('%s - ping thread stopped' % self.name) return ping = PingIqProtocolEntity() self._layer.waitPong(ping.getId()) if not self._stop: self._layer.sendIq(ping)",if self._stop:
"def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext if hasattr(names, '__contains__'): if 'gettext' in names: __builtin__.__dict__['gettext'] = __builtin__.__dict__['_'] <IF_STMT> __builtin__.__dict__['ngettext'] = unicode and self.ungettext or self.ngettext if 'lgettext' in names: __builtin__.__dict__['lgettext'] = self.lgettext if 'lngettext' in names: __builtin__.__dict__['lngettext'] = self.lngettext",if 'ngettext' in names:
"def on_task_output(self, task, config): for entry in task.entries: if 'torrent' in entry: <IF_STMT> log.debug('Writing modified torrent file for %s' % entry['title']) with open(entry['file'], 'wb+') as f: f.write(entry['torrent'].encode())",if entry['torrent'].modified:
"def batchSites(self, sites): i = 0 res = list() siteList = list() for site in sites: <IF_STMT> data = self.threadSites(siteList) if data is None: return res for ret in list(data.keys()): if data[ret]: res.append(f'{ret}:{data[ret]}') i = 0 siteList = list() siteList.append(site) i += 1 return res",if i >= self.opts['_maxthreads']:
def width_pixels(self): w = self.style_width if self._absolute_size and w == 'auto': w = self._absolute_size.width if type(w) is NumberUnit: if self._relative_element == self: rew = self._parent_size.width if self._parent_size else 0 elif self._relative_element is None: rew = 0 else: rew = self._relative_element.width_pixels <IF_STMT> rew = 0 w = w.val(base=rew) return w,if rew == 'auto':
def get_lang3(lang): try: <IF_STMT> ret_value = get(part1=lang).part3 elif len(lang) == 3: ret_value = lang else: ret_value = '' except KeyError: ret_value = lang return ret_value,if len(lang) == 2:
"def update_timer(): global _timer if time.time() - os.stat(config.TRAILS_FILE).st_mtime >= config.UPDATE_PERIOD: _ = None while True: _ = load_trails(True) <IF_STMT> trails.clear() trails.update(_) break else: time.sleep(LOAD_TRAILS_RETRY_SLEEP_TIME) _timer = threading.Timer(config.UPDATE_PERIOD, update_timer) _timer.start()",if _:
"def __call__(self, model): if hasattr(model, 'module'): model = model.module conv1_lr_mult = self.paramwise_cfg.get('conv1_lr_mult', 1.0) params = [] for name, param in model.named_parameters(): param_group = {'params': [param]} <IF_STMT> param_group['lr'] = self.base_lr * conv1_lr_mult params.append(param_group) optimizer_cfg['params'] = params return build_from_cfg(optimizer_cfg, OPTIMIZERS)",if name.startswith('conv1') and param.requires_grad:
"def _get_conf(self): conf = {} for path in map(Path, self.template_paths): conf_path = path / 'conf.json' <IF_STMT> with conf_path.open() as f: conf = recursive_update(conf, json.load(f)) return conf",if conf_path.exists():
"def _base_keywords(self, fw_version=False, image=False): keywords = dict() if image: keywords['image_uri'] = ""'my:image'"" if fw_version: keywords['framework_version'] = 'fw_version' <IF_STMT> else ""'{}'"".format(self.framework_version) return keywords",if fw_version == 'named'
"def check_grads(grads_and_vars): has_nan_ops = [] amax_ops = [] for grad, _ in grads_and_vars: if grad is not None: <IF_STMT> x = grad.values else: x = grad has_nan_ops.append(tf.reduce_any(tf.is_nan(x))) amax_ops.append(tf.reduce_max(tf.abs(x))) has_nan = tf.reduce_any(has_nan_ops) amax = tf.reduce_max(amax_ops) return (has_nan, amax)","if isinstance(grad, tf.IndexedSlices):"
"def new_org(type=ORG_DEFAULT, block=True, **kwargs): if type == ORG_DEFAULT: org = reserve_pooled(type=type, **kwargs) if not org: org = queue.reserve('queued_org', block=block, type=type, **kwargs) <IF_STMT> new_pooled() return org org = Organization(type=type, **kwargs) org.initialize() org.commit() return org else: org = Organization(type=type, **kwargs) org.queue_initialize(block=block) return org",if org:
"def _consumer_healthy(self): abnormal_num = 0 for w in self._consumers: <IF_STMT> abnormal_num += 1 if self._use_process: errmsg = 'consumer[{}] exit abnormally with exitcode[{}]'.format(w.pid, w.exitcode) else: errmsg = 'consumer[{}] exit abnormally'.format(w.ident) logger.warn(errmsg) if abnormal_num > 0: logger.warn('{} consumers have exited abnormally!!!'.format(abnormal_num)) return abnormal_num == 0",if not w.is_alive() and w.id not in self._consumer_endsig:
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: <IF_STMT> module = self.name if section is None: section = 'all_sections' if s_name is None: s_name = f['s_name'] if source is None: source = os.path.abspath(os.path.join(f['root'], f['fn'])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning('Tried to add data source for {}, but was missing fields data'.format(self.name))",if module is None:
"def startTest(self, test): unittest.TestResult.startTest(self, test) current_case = test.test.__class__.__name__ if self.showAll: <IF_STMT> self.stream.writeln(current_case) self._last_case = current_case self.stream.write('%s' % str(test.test._testMethodName).ljust(60)) self.stream.flush()",if current_case != self._last_case:
"def _calc_freq(item): try: <IF_STMT> ao = sum([int(x) for x in item.split(':')[ao_index].split(',')]) ro = int(item.split(':')[ro_index]) freq = ao / float(ao + ro) elif af_index is not None: freq = float(item.split(':')[af_index]) else: freq = 0.0 except (IndexError, ValueError, ZeroDivisionError): freq = 0.0 return freq",if ao_index is not None and ro_index is not None:
"def contains_version(self, version): """"""Returns True if version is contained in this range."""""" if len(self.bounds) < 5: for bound in self.bounds: i = bound.version_containment(version) <IF_STMT> return True if i == -1: return False else: _, contains = self._contains_version(version) return contains return False",if i == 0:
"def _codegen_impl(self, state: CodegenState, default_semicolon: bool=False) -> None: with state.record_syntactic_position(self): state.add_token('global') self.whitespace_after_global._codegen(state) last_name = len(self.names) - 1 for i, name in enumerate(self.names): name._codegen(state, default_comma=i != last_name) semicolon = self.semicolon if isinstance(semicolon, MaybeSentinel): <IF_STMT> state.add_token('; ') elif isinstance(semicolon, Semicolon): semicolon._codegen(state)",if default_semicolon:
"def getLatestXci(self, version=None): highest = None for nsp in self.getFiles(): try: <IF_STMT> if version is not None and nsp.version == version: return nsp if not highest or int(nsp.version) > int(highest.version): highest = nsp except BaseException: pass return highest",if nsp.path.endswith('.xci'):
"def _process_iter(self, line_iter): samples = [] buf = [] for line in line_iter: if not buf and line.startswith('#') and self._has_comment: continue line = line.split() if line: buf.append(line) <IF_STMT> samples.append(tuple(map(list, zip(*buf)))) buf = [] if buf: samples.append(tuple(map(list, zip(*buf)))) return samples",elif buf:
def examine_tree(tree): for node in tree.post_order(): <IF_STMT> continue print(repr(str(node))) verdict = raw_input() if verdict.strip(): print(find_pattern(node)) return,"if isinstance(node, pytree.Leaf):"
def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] if ch == '{': return True elif ch == '(': openParen += 1 elif ch == ')': if openParen == 0: return False openParen -= 1 <IF_STMT> return False i += 1 return False,elif ch == ';' or ch == '}':
"def scan_resource_conf(self, conf): self.evaluated_keys = 'user_data' if 'user_data' in conf.keys(): user_data = conf['user_data'][0] <IF_STMT> if string_has_secrets(user_data): return CheckResult.FAILED return CheckResult.PASSED","if isinstance(user_data, str):"
def strip_suffixes(path: str) -> str: t = path while True: if t.endswith('.xz'): t = t[:-3] elif t.endswith('.raw'): t = t[:-4] elif t.endswith('.tar'): t = t[:-4] <IF_STMT> t = t[:-6] else: break return t,elif t.endswith('.qcow2'):
"def classify(self, url, text): for match in self.rules.match(data=text): if (url, match) in self.matches: continue self.matches.append((url, match)) <IF_STMT> continue self.handle_match_etags(match) rule = match.rule meta = match.meta tags = ','.join([' '.join(t.split('_')) for t in match.tags]) log.ThugLogging.log_classifier('text', url, rule, tags, meta) for c in self.custom_classifiers: self.custom_classifiers[c](url, text)","if self.discard_url_match(url, match):"
"def is_symmetric_iterative(root): if root is None: return True stack = [[root.left, root.right]] while stack: left, right = stack.pop() <IF_STMT> continue if left is None or right is None: return False if left.val == right.val: stack.append([left.left, right.right]) stack.append([left.right, right.left]) else: return False return True",if left is None and right is None:
"def __str__(self): if self.looptype.is_pretest: <IF_STMT> return '%d-While(!%s)[%s]' % (self.num, self.name, self.cond) return '%d-While(%s)[%s]' % (self.num, self.name, self.cond) elif self.looptype.is_posttest: return '%d-DoWhile(%s)[%s]' % (self.num, self.name, self.cond) elif self.looptype.is_endless: return '%d-WhileTrue(%s)[%s]' % (self.num, self.name, self.cond) return '%d-WhileNoType(%s)' % (self.num, self.name)",if self.false in self.loop_nodes:
"def listdir(path='.'): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] if is_bytes: good = fname != b'.' and fname == b'..' else: good = fname != '.' and fname != '..' if good: <IF_STMT> fname = fsdecode(fname) res.append(fname) return res",if not is_bytes:
"def exitval_from_opts(options, project): exit_value_from = options.get('--exit-code-from') if exit_value_from: <IF_STMT> log.warning('using --exit-code-from implies --abort-on-container-exit') options['--abort-on-container-exit'] = True if exit_value_from not in [s.name for s in project.get_services()]: log.error('No service named ""%s"" was found in your compose file.', exit_value_from) sys.exit(2) return exit_value_from",if not options.get('--abort-on-container-exit'):
def shrink(self): Node.shrink(self) if self.size < NUM_SIZE_LEVELS: <IF_STMT> self.glue_spec = self.glue_spec.copy() self.glue_spec.width *= SHRINK_FACTOR,if self.glue_spec.width != 0.0:
"def _clean_text(self, text): """"""Performs invalid character removal and whitespace cleanup on text."""""" output = [] for char in text: cp = ord(char) if cp == 0 or cp == 65533 or _is_control(char): continue <IF_STMT> output.append(' ') else: output.append(char) return ''.join(output)",if _is_whitespace(char):
"def config_update(self, *updates): filename = os.path.join(self.path, '.git', 'config') with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) if value is None: config.remove_option(section, key) continue <IF_STMT> continue except (NoSectionError, NoOptionError): pass if value is not None: config.set_value(section, key, value)",if old == value:
"def generate_securecc_object(args): obj, phony_obj = args if not os.path.exists(obj): shutil.copy(phony_obj, obj) else: digest = blade_util.md5sum_file(obj) phony_digest = blade_util.md5sum_file(phony_obj) <IF_STMT> shutil.copy(phony_obj, obj)",if digest != phony_digest:
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: try: body = str(request.body, 'utf-8') <IF_STMT> else str(request.body) except TypeError: body = str(request.body) if old in body: request.body = body.replace(old, new) return request","if isinstance(request.body, bytes)"
"def _apply_regex(self, regex, input): import re re_match = re.match(regex, input) if re_match and any(re_match.groups()): kwargs = {} has_val = False for k, v in re_match.groupdict(default='0').items(): val = int(v) if val > -1: has_val = True kwargs[k] = val <IF_STMT> return datetime.timedelta(**kwargs)",if has_val:
"def test_method_mismatch(): line = 'def {}(self' skip_files = ['__init__.py', 'i3pystatus.py'] errors = [] for _file in sorted(MODULE_PATH.iterdir()): if _file.suffix == '.py' and _file.name not in skip_files: with _file.open() as f: <IF_STMT> errors.append((_file.stem, _file)) if errors: line = 'Method mismatched error(s) detected!\n\n' for error in errors: line += 'Method `{}` is not in module `{}`\n'.format(*error) print(line[:-1]) assert False",if f'def {_file.stem}(self' not in f.read():
"def iter_flat(self): for f in self.layout: e = getattr(self, f[0]) <IF_STMT> if len(f) == 3: yield (e, f[2]) else: yield (e, DIR_NONE) elif isinstance(e, Record): yield from e.iter_flat() else: raise TypeError","if isinstance(e, Signal):"
"def _identify_csv_files(self, csv_dir): try: product_csvs = [csv_filename for csv_filename in os.listdir(csv_dir) if csv_filename.endswith('.csv')] except FileNotFoundError as not_found: product_csvs = [] <IF_STMT> raise not_found return product_csvs",if not_found.filename != csv_dir:
"def gen_new_segments(datadir, spk_list): if not os.path.isfile(os.path.join(datadir, 'segments')): raise ValueError('no segments file found in datadir') new_segments = open(os.path.join(datadir, 'new_segments'), 'w', encoding='utf-8') segments = open(os.path.join(datadir, 'segments'), 'r', encoding='utf-8') while True: line = segments.readline() <IF_STMT> break spk = line.split('_')[0] if spk in spk_list: new_segments.write(line) (new_segments.close(), segments.close())",if not line:
"def colorspace(self): """"""PDF name of the colorspace that best describes this image"""""" if self.image_mask: return None if self._colorspaces: <IF_STMT> return self._colorspaces[0] if self._colorspaces[0] in ('/DeviceCMYK', '/ICCBased'): return self._colorspaces[0] if self._colorspaces[0] == '/Indexed' and self._colorspaces[1] in self.SIMPLE_COLORSPACES: return self._colorspaces[1] raise NotImplementedError('not sure how to get colorspace: ' + repr(self._colorspaces))",if self._colorspaces[0] in self.SIMPLE_COLORSPACES:
"def handle_bytes(self, event): self.bytes += event.data if event.message_finished: self.queue.put_nowait({'type': 'websocket.receive', 'bytes': self.bytes}) self.bytes = b'' <IF_STMT> self.read_paused = True self.transport.pause_reading()",if not self.read_paused:
"def get_latest_tasks(cls, tasks): tasks_group = {} for task in tasks: task_key = cls.task_key(task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id) <IF_STMT> tasks_group[task_key] = task elif task.f_task_version > tasks_group[task_key].f_task_version: tasks_group[task_key] = task return tasks_group",if task_key not in tasks_group:
"def determine_load_order(): dependencies = TypeMapItem._get_dependencies() ordered = dict() while dependencies: found_next = False for type_name, unloaded in dependencies.items(): if not unloaded: ordered[type_name] = len(ordered) found_next = True break <IF_STMT> raise Exception('recursive loading dependency') dependencies.pop(type_name) for unloaded in dependencies.values(): unloaded.discard(type_name) return ordered",if found_next is False:
"def _find_gist_with_file(user, filename, env): import requests page = 1 url = 'https://api.github.com/users/%s/gists' % user while True: resp = requests.get(url, params={'page': page, 'per_page': 100}, headers=_github_auth_headers(env)) gists = resp.json() <IF_STMT> return None for gist in gists: for name in gist['files']: if name == filename: return gist page += 1",if not gists:
"def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis): out = output_tensor((ndim + num_newaxis,), 'int64') for i in const_range(out.shape[0]): <IF_STMT> out[i] = data_shape[i] elif i < axis + num_newaxis: out[i] = int64(1) else: out[i] = data_shape[i - num_newaxis] return out",if i < axis:
"def check_graph(self, graph, verify, interactive): if verify and (not os.path.exists(self._target_folder)): raise ConanException('Manifest folder does not exist: %s' % self._target_folder) for node in graph.ordered_iterate(): <IF_STMT> continue self._handle_recipe(node, verify, interactive) self._handle_package(node, verify, interactive)","if node.recipe in (RECIPE_CONSUMER, RECIPE_VIRTUAL):"
"def when(self, matches, context): to_remove = [] for filepart in matches.markers.named('path'): patterns = defaultdict(list) for match in reversed(matches.range(filepart.start, filepart.end, predicate=lambda m: 'weak-duplicate' in m.tags)): <IF_STMT> to_remove.append(match) else: patterns[match.name].append(match.pattern) return to_remove",if match.pattern in patterns[match.name]:
"def __call__(self, session_path): """"""Get raw session object from `session_path`."""""" new_session = copy.deepcopy(self._template) session_keys = new_session.keys() old_session = self._load_file(session_path) for attribute in dir(self): if attribute.startswith('set_'): target = attribute[4:].capitalize() <IF_STMT> raise ValueError('Invalid attribute: %r' % attribute) function = getattr(self, attribute) new_session[target] = function(old_session) return new_session",if target not in session_keys:
"def set_recent_terminal(cls, view): terminal = Terminal.from_id(view.id()) if not terminal: return logger.debug('set recent view: {}'.format(view.id())) panel_name = terminal.panel_name if panel_name and panel_name != EXEC_PANEL: window = panel_window(view) <IF_STMT> cls._recent_panel[window.id()] = panel_name cls._recent_view[window.id()] = view else: window = view.window() if window: cls._recent_view[window.id()] = view",if window:
"def _testValue(self, value, idx): if self.__singleTypeConstraint: self.__singleTypeConstraint(value) elif self.__multipleTypeConstraint: if idx not in self.__multipleTypeConstraint: raise error.ValueConstraintError(value) constraint, status = self.__multipleTypeConstraint[idx] <IF_STMT> raise error.ValueConstraintError(value) constraint(value)",if status == 'ABSENT':
"def SaveIfUnsure(self): if self.ed.Modify: msg = 'Save changes to ""' + self.fullPath + '""?' print(msg) decision = self.DisplayMessage(msg, True) <IF_STMT> self.CmdSave() return decision return True",if decision:
"def before_get(self, args, kwargs): refresh = request.args.get('refresh') if refresh == 'true': refresh_settings() kwargs['id'] = 1 if is_logged_in(): verify_jwt_in_request() <IF_STMT> self.schema = SettingSchemaAdmin else: self.schema = SettingSchemaNonAdmin else: self.schema = SettingSchemaPublic",if current_user.is_admin or current_user.is_super_admin:
"def send(message: dict) -> None: nonlocal status_code, response_headers, response_started if message['type'] == 'http.response.start': assert not response_started status_code = message['status'] response_headers = message.get('headers', []) response_started = True elif message['type'] == 'http.response.body': assert not response_complete.is_set() body = message.get('body', b'') more_body = message.get('more_body', False) if body and method != b'HEAD': body_parts.append(body) <IF_STMT> response_complete.set()",if not more_body:
"def update(self, pycomp): newstate = pycomp[self.halpin] if newstate != self.state: <IF_STMT> self.itemconfig(self.oh, fill=self.on_color) self.state = 1 else: self.itemconfig(self.oh, fill=self.off_color) self.state = 0",if newstate == 1:
"def cut_all_tracks(frame): tracks_cut_data = [] for i in range(1, len(current_sequence().tracks) - 1): <IF_STMT> tracks_cut_data.append(None) else: tracks_cut_data.append(get_cut_data(current_sequence().tracks[i], frame)) data = {'tracks_cut_data': tracks_cut_data} action = edit.cut_all_action(data) action.do_edit() updater.repaint_tline()",if current_sequence().tracks[i].edit_freedom == appconsts.LOCKED:
"def visit(ignored, dir, files): if os.path.basename(dir) not in test_names: for name in test_names: if name + '.py' in files: path = os.path.join(dir, name + '.py') if matcher(path[baselen:]): results.append(path) return if '__init__.py' not in files: stderr('%s is not a package' % dir) return for file in files: <IF_STMT> path = os.path.join(dir, file) if matcher(path[baselen:]): results.append(path)",if file.startswith('test') and file.endswith('.py'):
def status_string(self): if not self.live: if self.expired: return _('expired') <IF_STMT> return _('scheduled') elif self.workflow_in_progress: return _('in moderation') else: return _('draft') elif self.approved_schedule: return _('live + scheduled') elif self.workflow_in_progress: return _('live + in moderation') elif self.has_unpublished_changes: return _('live + draft') else: return _('live'),elif self.approved_schedule:
"def create(self): if request.method == 'POST': <IF_STMT> Note.create(user=auth.get_logged_in_user(), message=request.form['message']) next = request.form.get('next') or self.dashboard_url() return redirect(next)",if request.form.get('message'):
def get_current_migration(): ver = 0 while True: next_ver = ver + 1 migration_func = globals().get('migration_%d' % next_ver) <IF_STMT> return ver ver = next_ver,if not migration_func:
"def resource_hdfs(uri, **kwargs): if 'hdfs://' in uri: uri = uri[len('hdfs://'):] d = re.match(hdfs_pattern, uri).groupdict() d = dict(((k, v) for k, v in d.items() if v is not None)) path = d.pop('path') kwargs.update(d) try: subtype = types_by_extension[path.split('.')[-1]] <IF_STMT> subtype = Directory(subtype) path = path.rsplit('/', 1)[0] + '/' except KeyError: subtype = type(resource(path)) return HDFS(subtype)(path, **kwargs)",if '*' in path:
"def _s_wise_max(a_indices, a_indptr, vals, out_max): n = len(out_max) for i in range(n): if a_indptr[i] != a_indptr[i + 1]: m = a_indptr[i] for j in range(a_indptr[i] + 1, a_indptr[i + 1]): <IF_STMT> m = j out_max[i] = vals[m]",if vals[j] > vals[m]:
def stroke(s): keys = [] on_left = True for k in s: if k in 'EU*-': on_left = False if k == '-': continue <IF_STMT> keys.append(k) elif on_left: keys.append(k + '-') else: keys.append('-' + k) return Stroke(keys),elif k == '*':
def __check_finished(self): if self.global_finished: return if not self.finished: <IF_STMT> self.finished = True self.__send_finished() else: val = self.__compare_working_vec_and_prev_rank() if val <= len(self.working_vec) * self.epsilon * 2: self.finished = True self.__send_finished(),if self.step >= self.max_steps:
"def test_interval_is_more_than_1(self, mock_save_check): state = {} check = Interval('test_file', period=4) for i in range(13): check.on_checkpoint(state) if i == 3: self.assertTrue(mock_save_check.call_count == 1) <IF_STMT> self.assertFalse(mock_save_check.call_count == 2) elif i == 7: self.assertTrue(mock_save_check.call_count == 2) self.assertTrue(mock_save_check.call_count == 3)",elif i == 6:
"def start(self, para=None, callback=None): if not self.load(): return if para != None or self.show(): <IF_STMT> para = self.para win = WidgetsManager.getref('Macros Recorder') if win != None: win.write('{}>{}'.format(self.title, para)) if self.asyn and IPy.uimode() != 'no': threading.Thread(target=self.runasyn, args=(para, callback)).start() else: self.runasyn(para, callback)",if para == None:
"def find_test_functions(collections): if not isinstance(collections, list): collections = [collections] functions = [] for collection in collections: if not isinstance(collection, dict): collection = vars(collection) for key in sorted(collection): value = collection[key] <IF_STMT> functions.append(value) return functions","if isinstance(value, types.FunctionType) and hasattr(value, 'unittest'):"
"def test_too_old(self): job = MRNullSpark(['-r', 'emr', '--image-version', '3.7.0']) job.sandbox() with job.make_runner() as runner: self.launch(runner) message = runner._cluster_spark_support_warning() self.assertIsNotNone(message) self.assertIn('support Spark', message) self.assertNotIn('Python 3', message) <IF_STMT> self.assertIn('3.8.0', message) else: self.assertIn('4.0.0', message)",if PY2:
"def RenderValue(self, value): if self.limit_lists == 0: return '<lists are omitted>' elif self.limit_lists == -1: return [self._PassThrough(v) for v in value] else: result = [self._PassThrough(v) for v in list(value)[:self.limit_lists]] <IF_STMT> result.append(dict(type=FetchMoreLink.__name__, url='to/be/implemented')) return result",if len(value) > self.limit_lists:
"def add_stack_attribute(self, memop_index): for op in self.operands: <IF_STMT> self.add_attribute('STACKPUSH%d' % memop_index) return elif op.bits == 'XED_REG_STACKPOP': self.add_attribute('STACKPOP%d' % memop_index) return die('Did not find stack push/pop operand')",if op.bits == 'XED_REG_STACKPUSH':
"def apply_response(*args, **kwargs): if 'Authorization' in request.headers.keys(): creds = str(b64decode(request.headers['Authorization'].replace('Basic ', '')), 'utf-8') <IF_STMT> return ('Authorized', 200) resp = Response('Unauthorized') resp.headers['WWW-Authenticate'] = 'Basic ABC' return (resp, 401)","if creds in ['root:pass', 'root:admin']:"
"def find_privileged_containers(self): logger.debug('Trying to find privileged containers and their pods') privileged_containers = [] if self.pods_endpoint_data: for pod in self.pods_endpoint_data['items']: for container in pod['spec']['containers']: <IF_STMT> privileged_containers.append((pod['metadata']['name'], container['name'])) return privileged_containers if len(privileged_containers) > 0 else None","if container.get('securityContext', {}).get('privileged'):"
"def get_asset_gl_entry(self, gl_entries): for item in self.get('items'): <IF_STMT> if is_cwip_accounting_enabled(item.asset_category): self.add_asset_gl_entries(item, gl_entries) if flt(item.landed_cost_voucher_amount): self.add_lcv_gl_entries(item, gl_entries) self.update_assets(item, item.valuation_rate) return gl_entries",if item.is_fixed_asset:
"def test_pickling(self): for i in range(pickle.HIGHEST_PROTOCOL + 1): p = pickle.dumps(self.s, i) dup = pickle.loads(p) self.assertEqual(self.s, dup, '%s != %s' % (self.s, dup)) <IF_STMT> self.s.x = 10 p = pickle.dumps(self.s, i) dup = pickle.loads(p) self.assertEqual(self.s.x, dup.x)","if type(self.s) not in (set, frozenset):"
"def f(p, args): try: source, port = args except: print('argument error') return o = p.get_config(source) for p in o.resources.port: <IF_STMT> continue print(p.resource_id) conf = p.configuration for k in self._port_settings: try: v = getattr(conf, k) except AttributeError: continue print('%s %s' % (k, v))",if p.resource_id != port:
"def replace(self, sub, repl): """"""Replaces any occurrences of ""sub"" with ""repl"" """""" new = [] for item in self.data: <IF_STMT> new.append(item.replace(sub, repl)) elif item == sub: new.append(repl) else: new.append(item) return self.new(new)","if isinstance(item, metaPattern):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue <IF_STMT> self.add_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def receive(debug=debug): if should_shutdown and should_shutdown(): debug('worker got sentinel -- exiting') raise SystemExit(EX_OK) try: ready, req = _receive(1.0) if not ready: return None except (EOFError, IOError) as exc: <IF_STMT> return None debug('worker got %s -- exiting', type(exc).__name__) raise SystemExit(EX_FAILURE) if req is None: debug('worker got sentinel -- exiting') raise SystemExit(EX_FAILURE) return req",if get_errno(exc) == errno.EINTR:
"def _trim_files_in_dir(dir, patterns, log=None): if log: log(""trim '%s' files under '%s'"", ""', '"".join(patterns), dir) from fnmatch import fnmatch for dirpath, dirnames, filenames in os.walk(dir): for d in dirnames[:]: for pat in patterns: if fnmatch(d, pat): _rmtree(join(dirpath, d)) dirnames.remove(d) break for f in filenames[:]: for pat in patterns: <IF_STMT> os.remove(join(dirpath, f)) break","if fnmatch(f, pat):"
"def refactor_stdin(self, doctests_only=False): input = sys.stdin.read() if doctests_only: self.log_debug('Refactoring doctests in stdin') output = self.refactor_docstring(input, '<stdin>') if self.write_unchanged_files or output != input: self.processed_file(output, '<stdin>', input) else: self.log_debug('No doctest changes in stdin') else: tree = self.refactor_string(input, '<stdin>') <IF_STMT> self.processed_file(str(tree), '<stdin>', input) else: self.log_debug('No changes in stdin')",if self.write_unchanged_files or (tree and tree.was_changed):
"def test_get_e_above_hull(self): for entry in self.pd.stable_entries: self.assertLess(self.pd.get_e_above_hull(entry), 1e-11, 'Stable entries should have e above hull of zero!') for entry in self.pd.all_entries: <IF_STMT> e_ah = self.pd.get_e_above_hull(entry) self.assertTrue(isinstance(e_ah, Number)) self.assertGreaterEqual(e_ah, 0)",if entry not in self.pd.stable_entries:
"def setup(self, name): value = self.default if self.environ: full_environ_name = self.full_environ_name(name) if full_environ_name in os.environ: value = self.to_python(os.environ[full_environ_name]) <IF_STMT> raise ValueError('Value {0!r} is required to be set as the environment variable {1!r}'.format(name, full_environ_name)) self.value = value return value",elif self.environ_required:
"def process_transactions(l1_block: 'l1_block_model.L1BlockModel') -> Dict[str, bool]: txn_map: Dict[str, bool] = {} try: verify_keys = get_verifying_keys(l1_block.dc_id) <IF_STMT> verify_transactions(l1_block, verify_keys, txn_map) else: mark_invalid(l1_block, txn_map) except Exception: mark_invalid(l1_block, txn_map) return txn_map","if verify_block(l1_block, verify_keys):"
"def get_values(self): if self.cache: key = [self.data.get(i) for i in ('url', 'format', 'expr')] contents = self.cache.get(('value-from', key)) <IF_STMT> return contents contents = self._get_values() if self.cache: self.cache.save(('value-from', key), contents) return contents",if contents is not None:
def _run_scalar_data(run): data = {} step = None last_step = None for s in indexlib.iter_run_scalars(run): key = s['tag'] data[key] = s['last_val'] last_step = s['last_step'] <IF_STMT> step = last_step if data: if step is None: step = last_step data['step'] = step return data,if key == 'loss':
"def getRemovedFiles(oldContents, newContents, destinationFolder): toRemove = [] for filename in list(oldContents.keys()): <IF_STMT> destFile = os.path.join(destinationFolder, filename.lstrip('/')) if os.path.isfile(destFile): toRemove.append(filename) return toRemove",if filename not in newContents:
"def sort_classes(classes: List[Tuple[str, ClassIR]]) -> List[Tuple[str, ClassIR]]: mod_name = {ir: name for name, ir in classes} irs = [ir for _, ir in classes] deps = OrderedDict() for ir in irs: <IF_STMT> deps[ir] = set() if ir.base: deps[ir].add(ir.base) deps[ir].update(ir.traits) sorted_irs = toposort(deps) return [(mod_name[ir], ir) for ir in sorted_irs]",if ir not in deps:
"def get_sources(urls, trusted_hosts): trusted_hosts = [six.moves.urllib.parse.urlparse(url).netloc for url in trusted_hosts] sources = [] for url in urls: parsed_url = six.moves.urllib.parse.urlparse(url) netloc = parsed_url.netloc if '@' in netloc: _, _, netloc = netloc.rpartition('@') name, _, _ = netloc.partition('.') verify_ssl = True <IF_STMT> verify_ssl = False sources.append({'url': url, 'name': name, 'verify_ssl': verify_ssl}) return sources",if netloc in trusted_hosts:
"def _insert_to_nonfull_node(self, node: Node, key): i = len(node.keys) - 1 while i >= 0 and node.keys[i] >= key: i -= 1 if node.is_leaf: node.keys.insert(i + 1, key) else: if len(node.children[i + 1].keys) >= self.max_number_of_keys: self._split_child(node, i + 1) <IF_STMT> i += 1 self._insert_to_nonfull_node(node.children[i + 1], key)",if node.keys[i + 1] < key:
"def _variable_state(self, char, index): self._variable_chars.append(char) if char == '}' and (not self._is_escaped(self._string, index)): self._open_curly -= 1 if self._open_curly == 0: <IF_STMT> raise StopIteration self._state = self._waiting_item_state elif char in self._identifiers: self._state = self._internal_variable_start_state",if not self._can_have_item():
def __next__(self): if self.index > 0: <IF_STMT> raise StopIteration if len(self.saved) > self.index: obj = self.saved[self.index] self.index += 1 else: obj = self.saved[0] self.index = 1 else: try: obj = next(self.iterable) except StopIteration: if not self.saved: raise obj = self.saved[0] self.index = 1 else: self.saved.append(obj) return obj,if not self.saved:
"def get_host_info(self, host): """"""Return hostvars for a single host"""""" if host in self.inventory['_meta']['hostvars']: return self.inventory['_meta']['hostvars'][host] elif self.args.host and self.inventory['_meta']['hostvars']: match = None for k, v in self.inventory['_meta']['hostvars'].items(): <IF_STMT> match = k break if match: return self.inventory['_meta']['hostvars'][match] else: raise VMwareMissingHostException('%s not found' % host) else: raise VMwareMissingHostException('%s not found' % host)",if self.inventory['_meta']['hostvars'][k]['name'] == self.args.host:
def readline(self): if self.peek is not None: line = self.peek self.peek = None else: line = self.file.readline() if not line: return line if he.match(line): return line while 1: self.peek = self.file.readline() <IF_STMT> return line line = line + self.peek self.peek = None,if len(self.peek) == 0 or (self.peek[0] != ' ' and self.peek[0] != '\t'):
"def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), '127.0.0.255') elif i == 255: self.assertEqual(str(ip), '127.0.1.0') <IF_STMT> self.assertEqual(str(ip), '127.0.3.233') elif i == 65534: self.assertEqual(str(ip), '127.0.255.255') elif i == 65535: self.assertEqual(str(ip), '127.1.0.0')",elif i == 1000:
"def __new__(cls, a=1, b=0.5): if cls._instances: cls._instances[:] = [instance for instance in cls._instances if instance()] for instance in cls._instances: <IF_STMT> return instance() o = super(Prior, cls).__new__(cls, a, b) cls._instances.append(weakref.ref(o)) return cls._instances[-1]()",if instance().a == a and instance().b == b:
"def forward(self, x): if self.is_nan: <IF_STMT> return torch.isnan(x).float() else: return torch.isnan(torch.index_select(x, 1, self.column_indices)).float() elif self.features == 'all': return torch.eq(x, self.missing_values).float() else: return torch.eq(torch.index_select(x, 1, self.column_indices), self.missing_values).float()",if self.features == 'all':
"def __mro_entries__(self, bases): if self._name: return super().__mro_entries__(bases) if self.__origin__ is Generic: if Protocol in bases: return () i = bases.index(self) for b in bases[i + 1:]: <IF_STMT> return () return (self.__origin__,)","if isinstance(b, _BaseGenericAlias) and b is not self:"
"def _set_frequency(self, value): if not self._pwm and value is not None: self._connection.set_PWM_frequency(self._number, value) self._connection.set_PWM_range(self._number, 10000) self._connection.set_PWM_dutycycle(self._number, 0) self._pwm = True elif self._pwm and value is not None: <IF_STMT> self._connection.set_PWM_frequency(self._number, value) self._connection.set_PWM_range(self._number, 10000) elif self._pwm and value is None: self._connection.write(self._number, 0) self._pwm = False",if value != self._connection.get_PWM_frequency(self._number):
"def literal(self): if self.peek('""'): lit, lang, dtype = self.eat(r_literal).groups() <IF_STMT> lang = lang else: lang = None if dtype: dtype = dtype else: dtype = None if lang and dtype: raise ParseError(""Can't have both a language and a datatype"") lit = unquote(lit) return Literal(lit, lang, dtype) return False",if lang:
"def _staged_model_references(self, load_relationships=False): for name, field in self._fields.items(): if isinstance(field, BaseRelationship): try: if load_relationships: value = getattr(self, name) else: value = self.data_store.get(name, ('staged', 'committed')) except (AttributeError, KeyError, PathResolutionError): continue <IF_STMT> continue if not isinstance(value, ModelCollection): value = [value] for related in value: related_name = field.related_name yield (related, related_name)",if value is None:
"def __call__(self, target): if not self.check_run_always: for algo in self.algos: if not algo(target): return False return True else: res = True for algo in self.algos: if res: res = algo(target) elif hasattr(algo, 'run_always'): <IF_STMT> algo(target) return res",if algo.run_always:
"def addRow(self, row): r = [] for j in range(self.numColumn): w, s = calWidth(row[j], self.maxWidth) <IF_STMT> self.W[j] = w r.append((w, s)) self.M.append(r)",if w > self.W[j]:
"def parse(s): text, anns = ('', []) s = re.sub('(<category[^<>]*>)( +)', '\\2\\1', s) s = re.sub('( +)(<\\/category>)', '\\2\\1', s) rest = s.strip() while True: m = re.match('^(.*?)<category=""([^""]+)"">(.*?)</category>(.*)$', rest) <IF_STMT> break pre, type_, tagged, rest = m.groups() text += pre anns.append((len(text), len(text) + len(tagged), type_, tagged)) text += tagged text += rest return (text, anns)",if not m:
"def _generate_examples(self, filepath): with open(filepath) as f: line_num = -1 while True: line_num += 1 sentence = f.readline().strip() pronoun = f.readline().strip() candidates = [c.strip() for c in f.readline().strip().split(',')] correct = f.readline().strip() f.readline() <IF_STMT> break yield (line_num, {'sentence': sentence, 'pronoun': pronoun, 'candidates': candidates, 'label': candidates.index(correct)})",if not sentence:
"def format_unencoded(self, tokensource, outfile): <IF_STMT> self._write_lineno(outfile) for ttype, value in tokensource: color = self._get_color(ttype) for line in value.splitlines(True): if color: outfile.write('<%s>%s</>' % (color, line.rstrip('\n'))) else: outfile.write(line.rstrip('\n')) if line.endswith('\n'): if self.linenos: self._write_lineno(outfile) else: outfile.write('\n') if self.linenos: outfile.write('\n')",if self.linenos:
"def refresh_pool_in_list(pool_list, conn, uuid): for row in pool_list.get_model(): <IF_STMT> row[3] = get_pool_size_percent(conn, uuid) row[2] = conn.get_pool(uuid).is_active() return",if row[0] == uuid:
"def save_claims_for_resolve(self, claim_infos): to_save = {} for info in claim_infos: <IF_STMT> if info['value']: to_save[info['claim_id']] = info else: for key in ('certificate', 'claim'): if info.get(key, {}).get('value'): to_save[info[key]['claim_id']] = info[key] return self.save_claims(to_save.values())",if 'value' in info:
"def rx(self, text): r = [] for c in text: <IF_STMT> r.append(c) elif c < ' ': r.append(unichr(9216 + ord(c))) else: r.extend((unichr(8320 + ord(d) - 48) for d in '{:d}'.format(ord(c)))) r.append(' ') return ''.join(r)",if ' ' <= c < '\x7f' or c in '\r\n\x08\t':
"def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() if event is h11.NEED_DATA: break <IF_STMT> continue elif isinstance(event, h11.Response): context['h11_response'] = event raise LoopAbort else: raise RuntimeError('Unexpected h11 event {}'.format(event))","elif isinstance(event, h11.InformationalResponse):"
"def validate_text(dialect, attr): val = getattr(dialect, attr) if not isinstance(val, text_type): <IF_STMT> raise Error('""{0}"" must be string, not bytes'.format(attr)) raise Error('""{0}"" must be string, not {1}'.format(attr, type(val).__name__)) if len(val) != 1: raise Error('""{0}"" must be a 1-character string'.format(attr))",if type(val) == bytes:
def _refresh(self): self.uiProfileSelectComboBox.clear() self.uiProfileSelectComboBox.addItem('default') try: if os.path.exists(self.profiles_path): for profile in sorted(os.listdir(self.profiles_path)): <IF_STMT> self.uiProfileSelectComboBox.addItem(profile) except OSError: pass,if not profile.startswith('.'):
"def get_entry(self, ip): self.parse() options = [] for line_type, components in self._contents: if line_type == 'option': pieces, _tail = components <IF_STMT> options.append(pieces[1:]) return options",if len(pieces) and pieces[0] == ip:
"def __new__(mcls, cls_name, bases, d): offset = 0 for base in bases: for realbase in base.__mro__: offset += len(realbase.__dict__.get('_methods_', [])) for i, args in enumerate(d.get('_methods_', [])): name = args[0] restype = args[1] <IF_STMT> continue argtypes = args[2:] m = COMMethod(name, offset + i, restype, argtypes) d[name] = m return type(ctypes.c_void_p).__new__(mcls, cls_name, bases, dict(d))",if restype is None:
"def _compare_caffe_tvm(caffe_out, tvm_out, is_network=False): for i in range(len(caffe_out)): <IF_STMT> caffe_out[i] = caffe_out[i][:1] tvm.testing.assert_allclose(caffe_out[i], tvm_out[i], rtol=1e-05, atol=1e-05)",if is_network:
"def update_transcoder(self): self.save_button.set_visible(False) if self.cast and self.fn: self.transcoder = Transcoder(self.cast, self.fn, lambda did_transcode=None: GLib.idle_add(self.update_status, did_transcode), self.transcoder) if self.autoplay: self.autoplay = False self.play_clicked(None) el<IF_STMT> self.transcoder.destroy() self.transcoder = None GLib.idle_add(self.update_media_button_states)",if self.transcoder:
"def deserialize(x): t = type(x) if t is list: return list(imap(deserialize, x)) if t is dict: if '_id_' not in x: return {key: deserialize(val) for key, val in iteritems(x)} obj = objmap.get(x['_id_']) <IF_STMT> entity_name = x['class'] entity = database.entities[entity_name] pk = x['_pk_'] obj = entity[pk] return obj return x",if obj is None:
"def release(self, conn, error=False): if not conn.is_closed: <IF_STMT> self.connections.append(conn) else: self.close_callable(conn)",if not error and len(self.connections) < self.pool_size:
"def install_symlinks(self): """"""Create symlinks for some applications files."""""" if self.has_symlinks(): for app_path in self.app_path: for symlink in self.symlinks.values(): root = symlink['root'] dest = path.join(str(app_path), symlink['dest']) <IF_STMT> self.backup.create(dest) symlink_file(root, dest)",if path.exists(dest):
def _fill_array(): global _array for i in range(624): y = (_array[i] & _bitmask2) + (_array[(i + 1) % 624] & _bitmask3) _array[i] = _array[(i + 397) % 624] ^ y >> 1 <IF_STMT> _array[i] ^= 2567483615,if y % 2 != 0:
"def parseLeftHandSideExpressionAllowCall(): marker = None expr = None args = None property = None marker = createLocationMarker() expr = parseNewExpression() if matchKeyword('new') else parsePrimaryExpression() while (match('.') or match('[')) or match('('): if match('('): args = parseArguments() expr = delegate.createCallExpression(expr, args) elif match('['): property = parseComputedMember() expr = delegate.createMemberExpression('[', expr, property) else: property = parseNonComputedMember() expr = delegate.createMemberExpression('.', expr, property) <IF_STMT> marker.end() marker.apply(expr) return expr",if marker:
"def unregister_zombies(self): """"""Unregister zombie builds (those whose builddir is gone)."""""" from pprint import pprint pprint(self.configs) for build_num, config in self.configs.items(): obj_dir_path = join(config.buildDir, _srcTreeName_from_config(config), 'mozilla', config.mozObjDir) <IF_STMT> self.unregister(build_num, ""zombie (`%s' does not exist)"" % obj_dir_path)",if not exists(obj_dir_path):
"def isUpdateAvailable(self, localOnly=False): nsp = self.getLatestFile() if not nsp: if not nsp: <IF_STMT> return True else: return False try: latest = self.lastestVersion(localOnly=localOnly) if latest is None: return False if int(nsp.version) < int(latest): return True except BaseException as e: Print.error('isUpdateAvailable exception %s: %s' % (self.id, str(e))) pass return False",if not self.isUpdate or (self.version and int(self.version) > 0):
"def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) <IF_STMT> default = default.total_seconds() if isinstance(actual, Enum): actual = actual.value if actual != default: yield Error(reason='mismatch', setting=setting_name, default=default, actual=actual)","if isinstance(default, timedelta):"
"def config_update(self, *updates): filename = os.path.join(self.path, '.git', 'config') with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) <IF_STMT> config.remove_option(section, key) continue if old == value: continue except (NoSectionError, NoOptionError): pass if value is not None: config.set_value(section, key, value)",if value is None:
"def __init__(self, search_space): self.params = {} for key in search_space.keys(): <IF_STMT> self.params[key] = Factor(search_space[key]['_value']) else: raise RuntimeError(""G_BFS Tuner doesn't support this kind of parameter: "" + str(search_space[key]['_type']))",if search_space[key]['_type'] == 'factor':
"def largest_image_url(self): if not self.imgs and (not self.top_img): return None if self.top_img: return self.top_img max_area = 0 max_url = None for img_url in self.imgs: dimension = fetch_image_dimension(img_url, self.useragent, referer=self.url) area = self.calculate_area(img_url, dimension) <IF_STMT> max_area = area max_url = img_url log.debug('using max img {}'.format(max_url)) return max_url",if area > max_area:
"def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): if hasattr(field, 'document_type'): field_cls = field.document_type if field_cls in inspected: continue if hasattr(field_cls, '_geo_indices'): geo_indices += field_cls._geo_indices(inspected) <IF_STMT> geo_indices.append(field) return geo_indices",elif field._geo_index:
"def __call__(self, trainer): self._t += 1 optimizer = self._get_optimizer(trainer) value = self._init * self._rate ** self._t if self._target is not None: <IF_STMT> if value / self._target > 1: value = self._target elif value / self._target < 1: value = self._target self._update_value(optimizer, value)",if self._rate > 1:
"def _parse_chunked(self, data): body = [] trailers = {} n = 0 lines = data.split(b'\r\n') while True: size, chunk = lines[n:n + 2] size = int(size, 16) <IF_STMT> n += 1 break self.assertEqual(size, len(chunk)) body.append(chunk) n += 2 if n > len(lines): break return b''.join(body)",if size == 0:
"def _gen_opnds(ii): for op in ii.parsed_operands: if op.lookupfn_name in ['MASK1', 'MASKNOT0']: continue if op.visibility == 'SUPPRESSED': continue <IF_STMT> continue yield op",if op.name == 'BCAST':
"def allow_request(self, request, view): if settings.API_THROTTLING: request_allowed = super(GranularUserRateThrottle, self).allow_request(request, view) if not request_allowed: user = getattr(request, 'user', None) <IF_STMT> log.info('User %s throttled for scope %s', request.user, self.scope) ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user) return request_allowed else: return True",if user and request.user.is_authenticated:
"def _make_callback(self): callback = self.callback for plugin in self.all_plugins(): try: <IF_STMT> callback = plugin.apply(callback, self) else: callback = plugin(callback) except RouteReset: return self._make_callback() if not callback is self.callback: update_wrapper(callback, self.callback) return callback","if hasattr(plugin, 'apply'):"
"def OnDeleteLine(self, items): for n in items: if n >= 0: name1 = self.items[n][2] name2 = self.items[n][4] del self.items[n] <IF_STMT> self.bindiff.matched1.remove(name1) if name2 in self.bindiff.matched2: self.bindiff.matched2.remove(name2) return [Choose.ALL_CHANGED] + items",if name1 in self.bindiff.matched1:
"def on_treeview_buttonrelease(self, widget, event, data=None): if self.promptToSave(): return True else: x = int(event.x) y = int(event.y) time = event.time pthinfo = widget.get_path_at_pos(x, y) if pthinfo is not None: path, col, cellx, celly = pthinfo currentPath, currentCol = widget.get_cursor() if currentPath != path: widget.set_cursor(path, col, 0) <IF_STMT> self.__popupMenu(event) return False",if event.button == 3:
"def __lt__(self, other): try: if self._version != other._version: return self._version < other._version if self._ip != other._ip: return self._ip < other._ip <IF_STMT> return self.netmask < other.netmask return False except AttributeError: return NotImplemented",if self.netmask != other.netmask:
"def config_video_apply(self, dev_id_info): df, da, add_define, hf, ha, add_hotplug = self.make_apply_data() ignore = add_hotplug if self.editted(EDIT_VIDEO_MODEL): model = self.get_combo_label_value('video-model') <IF_STMT> add_define(self.vm.define_video_model, dev_id_info, model) return self._change_config_helper(df, da, hf, ha)",if model:
"def write(self, b): if self._write_watcher is None: raise UnsupportedOperation('write') while True: try: return _write(self._fileno, b) except (IOError, OSError) as ex: <IF_STMT> raise wait_on_watcher(self._write_watcher, None, None, self.hub)",if ex.args[0] not in ignored_errors:
"def scan_resource_conf(self, conf): if 'enabled' in conf and conf['enabled'][0]: retention_block = conf['retention_policy'][0] if retention_block['enabled'][0]: retention_in_days = force_int(retention_block['days'][0]) <IF_STMT> return CheckResult.PASSED return CheckResult.FAILED",if retention_in_days and retention_in_days >= 90:
"def _find_gist_with_file(user, filename, env): import requests page = 1 url = 'https://api.github.com/users/%s/gists' % user while True: resp = requests.get(url, params={'page': page, 'per_page': 100}, headers=_github_auth_headers(env)) gists = resp.json() if not gists: return None for gist in gists: for name in gist['files']: <IF_STMT> return gist page += 1",if name == filename:
"def parse_position_spec(self): line = self.lookahead() if line.startswith('jump=') or line.startswith('jcnd='): self.consume() return True mo = self._position_re.match(line) if not mo: return False position, id, name = mo.groups() if id: table = self._position_table_map[position] <IF_STMT> self.position_ids[table, id] = name else: name = self.position_ids.get((table, id), '') self.positions[self._position_map[position]] = name self.consume() return True",if name:
"def remove_header(self, header): new_msg = b'' old_msg = self.msg_bytes.split('\n') i = 0 while True: line = old_msg[i] i += 1 <IF_STMT> new_msg += line if line == '': break new_msg += old_msg[i:] self.msg_bytes = new_msg",if not line.startswith(b'%s: ' % header):
"def on_janitor_selection_changed(self, selection): model, iter = selection.get_selected() if iter: if self.janitor_model.iter_has_child(iter): iter = self.janitor_model.iter_children(iter) plugin = model[iter][self.JANITOR_PLUGIN] for row in self.result_model: <IF_STMT> self.result_view.get_selection().select_path(row.path) log.debug('scroll_to_cell: %s' % row.path) self.result_view.scroll_to_cell(row.path)",if row[self.RESULT_PLUGIN] == plugin:
"def record_line(self, frame, event, arg): """"""Records line execution time."""""" if event == 'line': <IF_STMT> runtime = time.time() - self.prev_timestamp self.lines.append([self.prev_path, self.prev_lineno, runtime]) self.prev_lineno = frame.f_lineno self.prev_path = frame.f_code.co_filename self.prev_timestamp = time.time() return self.record_line",if self.prev_timestamp:
"def get_outdated_docs(self) -> Iterator[str]: for docname in self.env.found_docs: if docname not in self.env.all_docs: yield docname continue targetname = path.join(self.outdir, docname + self.out_suffix) try: targetmtime = path.getmtime(targetname) except Exception: targetmtime = 0 try: srcmtime = path.getmtime(self.env.doc2path(docname)) <IF_STMT> yield docname except OSError: pass",if srcmtime > targetmtime:
"def _fetch_all_channels(self, force=False): """"""Fetch all channel feeds from cache or network."""""" channels = self._get_channel_configs(force=force) enabled = self._settings.get(['enabled_channels']) forced = self._settings.get(['forced_channels']) all_channels = {} for key, config in channels.items(): <IF_STMT> continue if 'url' not in config: continue data = self._get_channel_data(key, config, force=force) if data is not None: all_channels[key] = data return all_channels",if key not in enabled and key not in forced:
"def _get_cortex_binary(kmer, cortex_dir): cortex_bin = None for check_bin in sorted(glob.glob(os.path.join(cortex_dir, 'bin', 'cortex_var_*'))): kmer_check = int(os.path.basename(check_bin).split('_')[2]) <IF_STMT> cortex_bin = check_bin break assert cortex_bin is not None, 'Could not find cortex_var executable in %s for kmer %s' % (cortex_dir, kmer) return cortex_bin",if kmer_check >= kmer:
"def test_numeric_literals(self):  @udf(BigIntVal(FunctionContext, SmallIntVal)) def fn(context, a): <IF_STMT> return 1729 elif a < 0: return None elif a < 10: return a + 5 else: return a * 2",if a is None:
"def cs(self): """"""ConfigSpace representation of this search space."""""" cs = CS.ConfigurationSpace() for k, v in self.kwvars.items(): if isinstance(v, NestedSpace): _add_cs(cs, v.cs, k) <IF_STMT> hp = v.get_hp(name=k) _add_hp(cs, hp) else: _rm_hp(cs, k) return cs","elif isinstance(v, Space):"
"def lineReceived(self, line): if self.state == 'connected': self.messageFilename = line self.state = 'gotMessageFilename' if self.state == 'gotMessageFilename': if line: self.metaInfo.append(line) else: <IF_STMT> self.transport.loseConnection() return self.filterMessage()",if not self.metaInfo:
"def __init__(self, reg, shtype, shimm, va): if shimm == 0: <IF_STMT> shtype = S_RRX elif shtype == S_LSR or shtype == S_ASR: shimm = 32 self.reg = reg self.shtype = shtype self.shimm = shimm self.va = va",if shtype == S_ROR:
"def check_data(self, var_name: str, val: Dict[Any, Any]) -> None: if not isinstance(val, dict): raise AssertionError(f'{var_name} is not a dictionary') for key, value in val.items(): <IF_STMT> raise AssertionError(f'{var_name} has a non-string key') check_data(self.value_type, f'{var_name}[{key}]', value)","if not isinstance(key, str):"
"def write_conditional_formatting(worksheet): """"""Write conditional formatting to xml."""""" df = DifferentialStyle() wb = worksheet.parent for cf in worksheet.conditional_formatting: for rule in cf.rules: <IF_STMT> rule.dxfId = wb._differential_styles.add(rule.dxf) yield cf.to_tree()",if rule.dxf and rule.dxf != df:
"def _find_wordpress_compiler(self): """"""Find WordPress compiler plugin."""""" if self.wordpress_page_compiler is not None: return plugin_info = self.site.plugin_manager.getPluginByName('wordpress', 'PageCompiler') if plugin_info is not None: <IF_STMT> self.site.plugin_manager.activatePluginByName(plugin_info.name) plugin_info.plugin_object.set_site(self.site) self.wordpress_page_compiler = plugin_info.plugin_object",if not plugin_info.is_activated:
"def _confirm(config): cli.out('You are about to initialize a Guild environment:') for name, val in config.prompt_params: <IF_STMT> cli.out('  {}:'.format(name)) for x in val: cli.out('{}'.format(x)) else: cli.out('  {}: {}'.format(name, val)) return cli.confirm('Continue?', default=True)","if isinstance(val, tuple):"
"def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): if ok_node(nodes[i]): node = nodes[i] <IF_STMT> if ok_node(node.value): return node.value else: return None else: return nodes[i] return None","if isinstance(node, ast.Starred):"
"def _is_binary(fname, limit=80): try: with open(fname, 'rb') as f: for i in range(limit): char = f.read(1) if char == b'\x00': return True if char == b'\n': return False if char == b'': return except OSError as e: <IF_STMT> return True raise e return False",if xp.ON_WINDOWS and is_app_execution_alias(fname):
"def render(self): x = '<span>' for idx, arg in enumerate(self.args, start=1): if isinstance(arg, (tuple, list)): value, desc = arg else: value, desc = (arg, arg) attrs = self.attrs.copy() attrs['name'] = self.name attrs['type'] = 'radio' attrs['value'] = value attrs['id'] = self.name + str(idx) <IF_STMT> attrs['checked'] = 'checked' x += '<input %s/> %s' % (attrs, net.websafe(desc)) x += '</span>' return x",if self.value == value:
"def test01b_gml(self): """"""Testing GML output."""""" for g in self.geometries.wkt_out: geom = OGRGeometry(g.wkt) exp_gml = g.gml <IF_STMT> exp_gml = exp_gml.replace('GeometryCollection', 'MultiGeometry') self.assertEqual(exp_gml, geom.gml)","if GDAL_VERSION >= (1, 8):"
"def _update_recording(self, frame, config): """"""Adds a frame to the current video output."""""" should_record = config['is_recording'] if should_record: <IF_STMT> self.is_recording = True print('Starting recording using %s', self.video_writer.current_output().name()) self.video_writer.write_frame(frame) elif self.is_recording: self.is_recording = False self.video_writer.finish() print('Finished recording')",if not self.is_recording:
"def activate(self, ctx): for idx in ctx.chooser_selection: func_ea = idaapi.getn_func(idx - 1).startEA cfunc = helper.decompile_function(func_ea) obj = api.VariableObject(cfunc.get_lvars()[0], 0) <IF_STMT> NewDeepSearchVisitor(cfunc, 0, obj, cache.temporary_structure).process()",if cfunc:
"def finish(self, event, commit=0): target = self.target source = self.source widget = self.initial_widget root = self.root try: del root.__dnd self.initial_widget.unbind(self.release_pattern) self.initial_widget.unbind('<Motion>') widget['cursor'] = self.save_cursor self.target = self.source = self.initial_widget = self.root = None <IF_STMT> if commit: target.dnd_commit(source, event) else: target.dnd_leave(source, event) finally: source.dnd_end(target, event)",if target:
"def run_epoch(model: BaseModel, loader, device: str, num_batches: int): model.eval() with Ctq(loader) as tq_loader: for batch_idx, data in enumerate(tq_loader): <IF_STMT> process(model, data, device) else: break",if batch_idx < num_batches:
"def find(d, target): remainingDicts = [d] while len(remainingDicts) > 0: current = remainingDicts.pop() for k, v in current.iteritems(): <IF_STMT> return v if isinstance(v, dict): remainingDicts.insert(0, v) return None",if k == target:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_time(d.getVarInt64()) continue <IF_STMT> self.set_level(d.getVarInt32()) continue if tt == 26: self.set_log_message(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
"def _merge_dict(d1, d2): for k, v2 in d2.items(): v1 = d1.get(k) <IF_STMT> raise Exception('{} is not recognized by client_config'.format(k)) if isinstance(v1, Mapping) and isinstance(v2, Mapping): _merge_dict(v1, v2) else: d1[k] = v2 return d1",if v1 is None:
"def build_and_apply_filters(query, objects, filter_func): if objects is not None: if isinstance(objects, str): query = query.filter(filter_func(objects)) <IF_STMT> t = [] for obj in objects: t.append(filter_func(obj)) query = query.filter(or_(*t)) return query","elif isinstance(objects, list):"
"def _worker_task(self, num: int): while True: try_ = 0 f = self.q.get() while try_ <= self.retries: rr = f() <IF_STMT> break try_ += 1 with self.stat_lock: self.exit_stat |= rr.ret_val self.q.task_done()",if not rr.retry:
"def get_benchmark_id_title_map(input_tree): input_root = input_tree.getroot() ret = {} for namespace in [XCCDF11_NS, XCCDF12_NS]: candidates = [] scrape_benchmarks(input_root, namespace, candidates) for _, elem in candidates: _id = elem.get('id') <IF_STMT> continue title = '<unknown>' for element in elem.findall('{%s}title' % namespace): title = element.text break ret[_id] = title return ret",if _id is None:
"def _call_tensor_ufunc(self, x1, x2, out=None, where=None): <IF_STMT> or hasattr(x2, '__tensor_ufunc__'): ufunc = x1.__tensor_ufunc__ if hasattr(x1, '__tensor_ufunc__') else x2.__tensor_ufunc__ ret = ufunc(type(self), [x1, x2], out, where, **self.ufunc_extra_params) if ret is NotImplemented: return return ret","if hasattr(x1, '__tensor_ufunc__')"
def remove_namespaces(xml): for elem in xml.getiterator(): if elem.tag is etree.Comment: continue i = elem.tag.find('}') <IF_STMT> elem.tag = elem.tag[i + 1:] return xml,if i > 0:
"def attributive(adjective, gender=MALE): w = adjective.lower() if PLURAL in gender and (not is_vowel(w[-1:])): return w + 'es' if PLURAL in gender and w.endswith(('a', 'e')): return w + 's' if w.endswith('o'): if FEMININE in gender and PLURAL in gender: return w[:-1] + 'as' if FEMININE in gender: return w[:-1] + 'a' <IF_STMT> return w + 's' return w",if PLURAL in gender:
def atbash(s): translated = '' for i in range(len(s)): n = ord(s[i]) if s[i].isalpha(): if s[i].isupper(): x = n - ord('A') translated += chr(ord('Z') - x) <IF_STMT> x = n - ord('a') translated += chr(ord('z') - x) else: translated += s[i] return translated,if s[i].islower():
def _add_all(self): stream = BytesIO() for page in self.graph_manager.pages: stream.write(page.url.encode('utf8')) <IF_STMT> for link in page.links: stream.write(link.url.encode('utf8')) stream.write(linesep.encode('utf8')) stream.seek(0) self.frontier.add_seeds(stream),if not page.has_errors:
"def test_bigrand_ranges(self): for i in [40, 80, 160, 200, 211, 250, 375, 512, 550]: start = self.gen.randrange(2 ** i) stop = self.gen.randrange(2 ** (i - 2)) <IF_STMT> return self.assertTrue(start <= self.gen.randrange(start, stop) < stop)",if stop <= start:
"def on_connect(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app['websockets'].add(web_socket) try: async for msg in web_socket: <IF_STMT> await self.on_status(None) elif msg.type == WSMsgType.ERROR: print('web socket connection closed with exception %s' % web_socket.exception()) finally: self.app['websockets'].discard(web_socket) return web_socket",if msg.type == WSMsgType.TEXT:
"def __cut_all(self, sentence): dag = self.get_DAG(sentence) old_j = -1 for k, L in iteritems(dag): if len(L) == 1 and k > old_j: yield sentence[k:L[0] + 1] old_j = L[0] else: for j in L: <IF_STMT> yield sentence[k:j + 1] old_j = j",if j > k:
def filter_forms(forms): result = [] seen = set() for form in forms: if form in self._lemma_pos_offset_map: <IF_STMT> if form not in seen: result.append(form) seen.add(form) return result,if pos in self._lemma_pos_offset_map[form]:
"def __init__(self, el): self.elements = list(el) parameters = {} tokens = [] token_quote = '@' for key, value in el.attrib.items(): if key == 'token_quote': token_quote = value <IF_STMT> for token in value.split(','): tokens.append((token, REQUIRED_PARAMETER)) elif key.startswith('token_'): token = key[len('token_'):] tokens.append((token, value)) for name, default in tokens: parameters[name] = (token_quote, default) self.parameters = parameters",if key == 'tokens':
"def setPositionAfterSort(self, sortChildren): c = self p = c.p p_v = p.v parent = p.parent() parent_v = p._parentVnode() if sortChildren: p = parent or c.rootPosition() else: <IF_STMT> p = parent.firstChild() else: p = leoNodes.Position(parent_v.children[0]) while p and p.v != p_v: p.moveToNext() p = p or parent return p",if parent:
"def next(self): while not self.closed or not self._buffer.empty(): <IF_STMT> try: chunck = next(self._input_iterator) return chunck except StopIteration: self.closed = True raise StopIteration() except Exception as ex: log.error('Failed downloading: %s' % ex) else: try: return self._buffer.get(block=True, timeout=1.0) except Empty: pass raise StopIteration()",if self._input_iterator:
"def _gen_GreaterEqual(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered('>=', lhs, rhs)) <IF_STMT> result.append(self.builder.icmp_signed('>=', lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)",elif ret_type == int_type:
"def save_settings(self, settings): for setting in self.settings: setting_obj = settings[setting] new_value = self.cleaned_data.get(setting) if setting_obj.python_type == 'image': <IF_STMT> self.save_image(setting_obj, new_value) elif self.cleaned_data.get('%s_delete' % setting): self.delete_image(setting_obj) else: self.save_setting(setting_obj, new_value)",if new_value and new_value != self.initial.get(setting):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_events().TryMerge(tmp) continue <IF_STMT> self.set_timeout_seconds(d.getDouble()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 17:
"def _trim_steps(self, num_steps): """"""Trims a given number of steps from the end of the sequence."""""" steps_trimmed = 0 for i in reversed(range(len(self._events))): if self._events[i].event_type == PolyphonicEvent.STEP_END: <IF_STMT> del self._events[i + 1:] break steps_trimmed += 1 elif i == 0: self._events = [PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None)] break",if steps_trimmed == num_steps:
"def save(self): data = self.cleaned_data previous_data = google_integration_model.get_by_account_id(self.account_id) if previous_data: previous_file = previous_data.get('file_id') else: previous_file = None json_key_file = data.get('json_key') if json_key_file: data['file_id'] = files_model.add(json_key_file) del data['json_key'] <IF_STMT> files_model.delete(previous_file) google_integration_model.save(data, account_id=self.account_id)",if previous_file:
"def _register(self, class_): with self.lock: table, slots = self._schema(class_) cur = self.db.execute('PRAGMA table_info(%s)' % table) available = cur.fetchall() <IF_STMT> available = [row[1] for row in available] missing_slots = (s for s in slots if s not in available) for slot in missing_slots: self.db.execute('ALTER TABLE %s ADD COLUMN %s TEXT' % (table, slot)) else: self.db.execute('CREATE TABLE %s (%s)' % (table, ', '.join(('%s TEXT' % s for s in slots))))",if available:
"def describe_auto_scaling_instances(self, instance_ids): instance_states = [] for group in self.autoscaling_groups.values(): instance_states.extend([x for x in group.instance_states <IF_STMT>]) return instance_states",if not instance_ids or x.instance.id in instance_ids
"def add_nicknames(self, fields, data): """"""Read the NICKNAME property of a VCard."""""" for nick in self.split_unescaped(data, ','): nickname = nick.strip() <IF_STMT> name = Name() name.set_nick_name(self.unesc(nickname)) self.person.add_alternate_name(name)",if nickname:
"def while1_test(a, b, c): while 1: <IF_STMT> if b: a = 3 b = 0 elif c: c = 0 else: a += b + c break return (a, b, c)",if a != 2:
"def get_stream(conf, reload=False): if not conf: return conf if 'class' in conf: class_name = conf.pop('class') <IF_STMT> cls = globals()[class_name] inst = cls(**conf) else: inst = resolve_name(class_name, reload=reload)(**conf) elif 'stream' in conf: inst = conf['stream'] elif 'filename' in conf: inst = FileStream(**conf) else: raise ValueError('stream configuration invalid') return {'stream': inst}",if not '.' in class_name:
"def check_physical(self, line): """"""Run all physical checks on a raw input line."""""" self.physical_line = line for name, check, argument_names in self._physical_checks: self.init_checker_state(name, argument_names) result = self.run_check(check, argument_names) <IF_STMT> offset, text = result self.report_error(self.line_number, offset, text, check) if text[:4] == 'E101': self.indent_char = line[0]",if result is not None:
def delete_oidc_session_tokens(session): if session: if 'oidc_access_token' in session: del session['oidc_access_token'] if 'oidc_id_token' in session: del session['oidc_id_token'] if 'oidc_id_token_expiration' in session: del session['oidc_id_token_expiration'] <IF_STMT> del session['oidc_login_next'] if 'oidc_refresh_token' in session: del session['oidc_refresh_token'] if 'oidc_state' in session: del session['oidc_state'],if 'oidc_login_next' in session:
"def _fix_exception_context(new_exc, old_exc): while 1: exc_context = new_exc.__context__ if exc_context is old_exc: return <IF_STMT> break new_exc = exc_context new_exc.__context__ = old_exc",if exc_context is None or exc_context is frame_exc:
"def _write_all(self, out): while len(out) > 0: n = self.sock.send(out) if n <= 0: raise EOFError() <IF_STMT> return out = out[n:] return",if n == len(out):
def view(input_path): if not exists(input_path): raise IOError('{0} not found'.format(input_path)) ua = None bundle_info = None try: archive = archive_factory(input_path) <IF_STMT> raise NotMatched('No matching archive type found') ua = archive.unarchive_to_temp() bundle_info = ua.bundle.info finally: if ua is not None: ua.remove() return bundle_info,if archive is None:
"def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: if strip: line = line.strip() skip = False if skip_blanks: skip = line.isspace() or not line <IF_STMT> yield line",if not skip:
"def migrate_key(key, source, target): if source in config and key in config[source]: if config.get(target) is None: config[target] = {} <IF_STMT> config[target][key] = config[source][key] del config[source][key] return True return False",if key not in config[target]:
"def get_params(self): if not hasattr(self, 'input_space'): raise AttributeError('Input space has not been provided.') rval = [] for layer in self.layers: for param in layer.get_params(): if param.name is None: logger.info(type(layer)) layer_params = layer.get_params() assert not isinstance(layer_params, set) for param in layer_params: <IF_STMT> rval.append(param) rval = [elem for elem in rval if elem not in self.freeze_set] assert all([elem.name is not None for elem in rval]) return rval",if param not in rval:
"def _build_kwargs_string(cls, expectation): kwargs = [] for k, v in expectation['kwargs'].items(): if k == 'column': kwargs.insert(0, ""{}='{}'"".format(k, v)) <IF_STMT> kwargs.append(""{}='{}'"".format(k, v)) else: kwargs.append('{}={}'.format(k, v)) return ', '.join(kwargs)","elif isinstance(v, str):"
"def binary_search(_list, left, right, target): if right >= left: mid = (left + right) // 2 if _list[mid] == target: return mid <IF_STMT> return binary_search(_list, left, mid - 1, target) return binary_search(_list, mid + 1, right, target) return False",if _list[mid] > target:
"def _set_name(self, name): if name is not None: name = os.path.basename(name) <IF_STMT> name, ext = os.path.splitext(name) name = name[:255 - len(ext)] + ext self._name = name",if len(name) > 255:
"def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: <IF_STMT> cursor = '0' while cursor != 0: pieces = [cursor] if match is not None: pieces.extend(['MATCH', match]) if count is not None: pieces.extend(['COUNT', count]) response = await self.execute_command_on_nodes([node], 'SCAN', *pieces) cursor, data = list(response.values())[0] for item in data: yield item",if 'master' in node['flags']:
"def drf_url(context, viewname, *args, **kwargs): """"""Helper for DjangoRestFramework's ``reverse`` in templates."""""" request = context.get('request') if request: <IF_STMT> request.versioning_scheme = api_settings.DEFAULT_VERSIONING_CLASS() request.version = request.versioning_scheme.determine_version(request, *args, **kwargs) return drf_reverse(viewname, request=request, args=args, kwargs=kwargs)","if not hasattr(request, 'versioning_scheme'):"
"def __call__(self, ctx): if ctx.range and ctx.value: <IF_STMT> ctx.range.raw_value = ctx.value return scalar = ctx.meta.get('scalar', False) if not scalar: ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0])) self._write_value(ctx.range, ctx.value, scalar)",if self.raw:
"def removeNamedItemNS(self, namespaceURI, localName): n = self.getNamedItemNS(namespaceURI, localName) if n is not None: _clear_id_cache(self._ownerElement) del self._attrsNS[n.namespaceURI, n.localName] del self._attrs[n.nodeName] <IF_STMT> n.ownerElement = None return n else: raise xml.dom.NotFoundErr()","if hasattr(n, 'ownerElement'):"
"def __find_image(self, relpath): image_path = None for rp in self._resource_paths: for root, dirs, files in os.walk(rp): <IF_STMT> image_path = os.path.join(root, relpath) break if image_path is not None: break return image_path",if relpath in files:
"def get_config_value(self, path, raise_if_not_found=True): if not path.is_concrete(): raise ValueError(""Can't access config by masked path: %s"" % path) cfg = self._config for key in path: if key not in cfg: <IF_STMT> raise ValueError('Key not found: %r' % key) else: return None cfg = cfg[key] return cfg",if raise_if_not_found:
"def unbind(**kwargs): for event, callback in kwargs.items(): <IF_STMT> raise Exception('Unknown {!r} event'.format(event)) else: for listener in _callbacks[event][:]: if listener.callback == callback: _callbacks[event].remove(listener) if event == 'on_new_intent': _activity.unregisterNewIntentListener(listener) elif event == 'on_activity_result': _activity.unregisterActivityResultListener(listener)",if event not in _callbacks:
"def _escape_attrib(text): try: if '&' in text: text = text.replace('&', '&amp;') if '<' in text: text = text.replace('<', '&lt;') if '>' in text: text = text.replace('>', '&gt;') <IF_STMT> text = text.replace('""', '&quot;') if '\n' in text: text = text.replace('\n', '&#10;') return text except (TypeError, AttributeError): _raise_serialization_error(text)","if '""' in text:"
"def _get_options(self, kwargs): options = {} for option in self._options: <IF_STMT> self._validate_option(option, kwargs[option]) options[option] = kwargs[option] else: options[option] = getattr(self, '_' + option) return options",if option in kwargs:
"def _parse_version_parts(s): for part in component_re.split(s): part = replace(part, part) <IF_STMT> continue if part[:1] in '0123456789': yield part.zfill(8) else: yield ('*' + part) yield '*final'","if part in ['', '.']:"
def collect_deps(lib): queue = list(lib.deps_all) visited = set(queue) visited.add(lib) deps = [] while queue: next_queue = [] for lib in queue: for dep in lib.deps_all: <IF_STMT> next_queue.append(dep) visited.add(dep) deps.append(collect_path_sorted_lib_idxs(queue)) queue = next_queue return deps,if dep not in visited:
"def process_chunks(self, chunks): chunk_id = self._chunk_id self._chunk_id += len(chunks) chunk_data = [] for chunk in chunks: <IF_STMT> msg = 'Metric data exceeds maximum size of {} bytes. Dropping it.'.format(MAX_LINE_SIZE) wandb.termerror(msg, repeat=False) util.sentry_message(msg) else: chunk_data.append(chunk.data) return {'offset': chunk_id, 'content': chunk_data}",if len(chunk.data) > MAX_LINE_SIZE:
"def truncateLogFile(): global logfilename logger.warn('Truncating log file %s' % logfilename) with open(logfilename, 'w') as f: f.write('') for i in range(1, 25): rotatedFilename = '%s.%d' % (logfilename, i) <IF_STMT> logger.info('Deleting rotated file %s' % rotatedFilename) os.unlink(rotatedFilename)",if os.path.exists(rotatedFilename):
"def _page_contains(self, text): browser = self._current_browser() browser.switch_to_default_content() if self._is_text_present(text): return True subframes = self._element_find('xpath=//frame|//iframe', False, False) self._debug('Current frame has %d subframes' % len(subframes)) for frame in subframes: browser.switch_to_frame(frame) found_text = self._is_text_present(text) browser.switch_to_default_content() <IF_STMT> return True return False",if found_text:
"def get_project_name_git(): is_git = check_output(['git', 'rev-parse', '--git-dir'], stderr=subprocess.STDOUT) if is_git: project_address = check_output(['git', 'config', '--local', 'remote.origin.url']) <IF_STMT> project_address = project_address.decode() project_name = [i for i in re.split('[/:\\s\\\\]|\\.git', project_address) if i][-1] return project_name.strip()","if isinstance(project_address, bytes) and str != bytes:"
"def timer(ratio, step, additive): t = 0 slowmode = False while 1: <IF_STMT> slowmode |= bool((yield t)) else: slowmode = bool((yield t)) if slowmode: t += step * ratio else: t += step",if additive:
"def _call_connection_lost(self, exc): try: if self._protocol_connected: self._protocol.connection_lost(exc) finally: self._sock.close() self._sock = None self._protocol = None self._loop = None server = self._server <IF_STMT> server._detach() self._server = None",if server is not None:
def _think(self): try: <IF_STMT> random.choice(self.peers.values()).send_getaddrs(count=8) except: log.err() return random.expovariate(1 / 20),if len(self.addr_store) < self.preferred_storage and self.peers:
def merge_force_collapse(self): p = self.pending while len(p) > 1: <IF_STMT> self.merge_at(-3) else: self.merge_at(-2),if len(p) >= 3 and p[-3].len < p[-1].len:
"def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO <IF_STMT> old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)","if hasattr(signal, 'SIGTTOU'):"
"def change_palette_name(self, palette_name): if isinstance(palette_name, str): <IF_STMT> log.info('Palette name %s not found', palette_name) return log.debug('Settings palette name to %s', palette_name) self.settings.styleFont.set_string('palette', PALETTES[palette_name]) self.settings.styleFont.set_string('palette-name', palette_name) self.set_colors_from_settings()",if palette_name not in PALETTES:
"def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): if k in value: <IF_STMT> return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): if not nested_match(x, y): return False return True return False","if not nested_match(v, value[k]):"
"def _on_event(self, event): event_id = event['event_id'] if event_id == MpvEventID.END_FILE: reason = event['event']['reason'] logger.debug('Current song finished. reason: %d' % reason) <IF_STMT> self.media_finished.emit() elif event_id == MpvEventID.FILE_LOADED: self.media_loaded.emit()",if self.state != State.stopped and reason != MpvEventEndFile.ABORTED:
"def __exit__(self, exc_type, exc_value, traceback): self.close() with DB.connection_context(): rows = SessionRecord.delete().where(SessionRecord.f_session_id == self._session_id).execute() <IF_STMT> LOGGER.debug(f'delete session {self._session_id} record') else: LOGGER.warning(f'failed delete session {self._session_id} record')",if rows > 0:
"def decorator(*args, **kwargs): g._flask_user_allow_unconfirmed_email = True try: user_manager = current_app.user_manager allowed = _is_logged_in_with_confirmed_email(user_manager) <IF_STMT> return user_manager.unauthenticated_view() return view_function(*args, **kwargs) finally: g._flask_user_allow_unconfirmed_email = False",if not allowed:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <IF_STMT> self.set_limit(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
"def addOptions(parser): for optname in options.keys('default'): <IF_STMT> continue action = 'store_true' if options[optname] is False else 'store' try: parser.add_argument('--' + optname.replace('_', '-'), action=action, dest=optname, default=None, help=options._opts._get(optname).helpstr) except argparse.ArgumentError: pass",if optname.startswith('color_') or optname.startswith('disp_'):
"def make_relative_to(self, kwds, relative_to): if relative_to and os.path.dirname(relative_to): dirname = os.path.dirname(relative_to) kwds = kwds.copy() for key in ffiplatform.LIST_OF_FILE_NAMES: <IF_STMT> lst = kwds[key] if not isinstance(lst, (list, tuple)): raise TypeError(""keyword '%s' should be a list or tuple"" % (key,)) lst = [os.path.join(dirname, fn) for fn in lst] kwds[key] = lst return kwds",if key in kwds:
"def _options_fcheck(self, name, xflags, table): for entry in table: <IF_STMT> break if entry.flags & XTOPT_MAND and (not xflags & 1 << entry.id): raise XTablesError('%s: --%s must be specified' % (name, entry.name)) if not xflags & 1 << entry.id: continue",if entry.name is None:
"def _load_cmds(): prefix = 'AOE_CMD_' g = globals() for k, v in iteritems(g): <IF_STMT> name = 'aoe' + k[len(prefix):].lower() try: mod = __import__(name, g, level=1) AOE.set_cmd(v, getattr(mod, name.upper())) except (ImportError, AttributeError): continue",if k.startswith(prefix):
"def test_list_sizes(self): sizes = self.driver.list_sizes() self.assertEqual(len(sizes), 7, 'Wrong sizes count') for size in sizes: self.assertTrue(isinstance(size.price, float), 'Wrong size price type') <IF_STMT> self.assertEqual(size.price, 0, 'Size price should be zero by default')",if self.driver.api_name == 'openstack':
"def testToFileBinary(self): z = dns.zone.from_file(here('example'), 'example') try: f = open(here('example3-binary.out'), 'wb') z.to_file(f) f.close() ok = compare_files('testToFileBinary', here('example3-binary.out'), here('example3.good')) finally: <IF_STMT> os.unlink(here('example3-binary.out')) self.assertTrue(ok)",if not _keep_output:
def ip_list(_): ips = [] for ip in _.split(' '): <IF_STMT> continue elif isip(ip): ips.append(IP.create(ip)) else: raise TypeError('ip %s is invalid' % ip) return ips,if not ip:
"def _wait_for_state(self, server_id, state, retries=50): for i in (0, retries): server = self.ex_get_server(server_id) if server.extra['status']['state'] == state: return sleep(5) <IF_STMT> raise Exception('Retries count reached')",if i == retries:
"def _stretch_prev(data): clip, track, item_id, item_data = data try: prev_index = track.clips.index(clip) - 1 if prev_index < 0: return <IF_STMT> clip = track.clips[prev_index] data = (clip, track, item_id, item_data) _cover_blank_from_next(data, True) except: pass",if track.clips[prev_index].is_blanck_clip == True:
"def characters(self, ch): if self._inside_fuzzable: modified_value = self._fuzzed_parameters[self._fuzzable_index][1] <IF_STMT> modified_value = modified_value.get_value() if self._fuzzed_parameters[self._fuzzable_index][0] == 'base64': enc_val = base64.b64encode(modified_value) else: enc_val = cgi.escape(modified_value).encode('ascii', 'xmlcharrefreplace') self.fuzzed_xml_string += enc_val else: self.fuzzed_xml_string += ch","if isinstance(modified_value, DataToken):"
"def _make_sure_scheduler_ready(self, timeout=120): check_start_time = time.time() while True: workers_meta = self._scheduler_service._resource_ref.get_workers_meta() <IF_STMT> self._pool.sleep(0.5) if time.time() - check_start_time > timeout: raise TimeoutError('Check worker ready timed out.') else: break",if not workers_meta:
"def tiles_around(self, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width: for dy in range(-radius, radius + 1): ny = y + dy <IF_STMT> if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps",if ny >= 0 and ny < self.height and (dx != 0 or dy != 0):
"def tearDown(self): for i in ScriptVersion.objects.all(): name = i.script_path.name utils.get_storage().delete(name) <IF_STMT> try: utils.get_storage(local=False).delete(name) except WindowsError: print('unable to delete {}'.format(name)) name += 'c' try: utils.get_storage().delete(name) except WindowsError: print('unable to delete {}'.format(name)) super(ScriptTearDown, self).tearDown()",if wooey_settings.WOOEY_EPHEMERAL_FILES:
"def _fill_tc_results(self): tids = list(self.tc._results.keys()) fields = ['failures', 'errors', 'skipped', 'expectedFailures'] for tid in tids: result = self.tc._results[tid] for field in fields: <IF_STMT> self.tc._results[field] = [] self.tc._results[field].extend(result[field])",if not field in self.tc._results:
"def check_mixin_inheritance(bases): for b in bases: check_mixin_inheritance(b.__bases__) for k, v in vars(b).items(): <IF_STMT> _type_info[k] = _process_item(v)","if _is_interesting(k, v):"
"def _check_params(swa_freq): params = [swa_freq] params_none = [param is None for param in params] if not all(params_none) and any(params_none): warnings.warn('Some of swa_start, swa_freq is None, ignoring other') for i, param in enumerate(params): <IF_STMT> params[i] = int(param) warnings.warn('Casting swa_start, swa_freq to int') return (not any(params_none), params)","if param is not None and (not isinstance(param, int)):"
"def findBookmark(self, bookmark, root=None): if root == None: root = self.bookmarks for i, b in enumerate(root): if isinstance(b, list): res = self.findBookmark(bookmark, b) <IF_STMT> return [i] + res elif b == bookmark or b['/Title'] == bookmark: return [i] return None",if res:
"def best_match(self, matches, default=None): best_quality = -1 result = default for server_item in matches: for client_item, quality in self: <IF_STMT> break if self._value_matches(server_item, client_item) and quality > 0: best_quality = quality result = server_item return result",if quality <= best_quality:
def validate_external_users(self): if self.user and settings.ALLOW_OAUTH2_FOR_EXTERNAL_USERS is False: external_account = get_external_account(self.user) <IF_STMT> raise oauth2.AccessDeniedError(_('OAuth2 Tokens cannot be created by users associated with an external authentication provider ({})').format(external_account)),if external_account is not None:
def get_tzname(self): tzname = None if settings.USE_TZ: <IF_STMT> tzname = timezone.get_current_timezone_name() else: tzname = timezone._get_timezone_name(self.tzinfo) return tzname,if self.tzinfo is None:
"def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: if hasattr(field, 'attname'): if field.attname == 'id': continue elif field.attname.endswith('ptr_id'): continue <IF_STMT> fds.add(field.attname) return fds","if getattr(field, 'editable', True):"
"def p_advsimd_secondary(val, va, mnem, opcode, flags, opers): if opcode == INS_VORR: src1 = val >> 16 & 15 src2 = val & 15 <IF_STMT> opers = (ArmRegOper(rctx.getRegisterIndex(rbase % d)), ArmRegOper(rctx.getRegisterIndex(rbase % n))) return ('vmov', INS_VMOV, None, opers) return (None, None, None, None)",if src1 == src2:
"def list_urls(self): for idx, job in enumerate(self.urlwatcher.jobs): <IF_STMT> print('%d: %s' % (idx + 1, repr(job))) else: pretty_name = job.pretty_name() location = job.get_location() if pretty_name != location: print('%d: %s ( %s )' % (idx + 1, pretty_name, location)) else: print('%d: %s' % (idx + 1, pretty_name)) return 0",if self.urlwatch_config.verbose:
"def _split_auth_string(auth_string): """"""split a digest auth string into individual key=value strings"""""" prev = None for item in auth_string.split(','): try: if prev.count('""') == 1: prev = '%s,%s' % (prev, item) continue except AttributeError: <IF_STMT> prev = item continue else: raise StopIteration yield prev.strip() prev = item yield prev.strip() raise StopIteration",if prev == None:
"def _get_user_auth_session_cookie(self, url, username, password): get_response = requests.get(url) if 'auth' in get_response.url: credentials = {'login': username, 'password': password} session = requests.Session() session.post(get_response.url, data=credentials) cookie_auth_key = 'authservice_session' cookie_auth_value = session.cookies.get(cookie_auth_key) <IF_STMT> return cookie_auth_key + '=' + cookie_auth_value",if cookie_auth_value:
"def copychunked(src, dest): chunksize = 524288 fsrc = src.open('rb') try: fdest = dest.open('wb') try: while 1: buf = fsrc.read(chunksize) <IF_STMT> break fdest.write(buf) finally: fdest.close() finally: fsrc.close()",if not buf:
"def iterate_all_python_files(base_path): for dirname, subdirlist, filelist in os.walk(base_path): if '__pycache__' in dirname: continue for filename in filelist: <IF_STMT> yield os.path.join(base_path, dirname, filename)",if filename.endswith('.py'):
"def discover(self, *objlist): ret = [] for l in self.splitlines(): if l[0] != 'intr': continue for name, i in enumerate(l[2:]): <IF_STMT> ret.append(str(name)) return ret",if int(i) > 10:
"def call_url(self, expected_url, with_error=False): try: with self.best_url_selector.select_best_url() as url: self.assertEqual(urlparse(expected_url), url) <IF_STMT> raise RequestException('error connecting to {}'.format(url)) except RequestException: pass",if with_error:
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None): PolicyNetBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): <IF_STMT> action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32) self._action_space = action_space self._append_output_layer()",if action_space is None:
"def gettempfilename(suffix): """"""Returns a temporary filename"""""" if '_' in os.environ: if os.environ['_'].find('wine') >= 0: tmpdir = '.' <IF_STMT> tmpdir = os.environ['TMP'] import time import random random.seed(time.time()) random_part = 'file%d' % random.randint(0, 1000000000) return os.path.join(tmpdir, random_part + suffix) return tempfile.mktemp(suffix)",if 'TMP' in os.environ:
"def get_url(self): if self.url_patterns: v_url = match1(self.html, *self.url_patterns) <IF_STMT> v_url = compact_unquote(v_url) self.v_url = [v_url]",if v_url.startswith('http%3A'):
"def drain(self, fd): """"""Make `fd` unreadable."""""" while True: try: <IF_STMT> return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.EAGAIN: return raise","if not os.read(fd, 4096):"
"def tearDown(self): for pidfile in self.pidfiles: <IF_STMT> continue with open(pidfile) as f: pid = f.read() if not pid: return pid = int(pid) try: os.kill(pid, signal.SIGKILL) except OSError: pass for pidfile in self.pidfiles: if os.path.exists(pidfile): os.unlink(pidfile) self.tearDownBasedir()",if not os.path.exists(pidfile):
"def main(): input_fname, out_fname = sys.argv[1:] index = Indexes() offset = 0 reader_wrapper = GFFReaderWrapper(fileinput.FileInput(input_fname), fix_strand=True) for feature in list(reader_wrapper): <IF_STMT> convert_gff_coords_to_bed(feature) index.add(feature.chrom, feature.start, feature.end, offset) offset += feature.raw_size index.write(open(out_fname, 'wb'))","if isinstance(feature, GenomicInterval):"
"def _s_wise_max(a_indices, a_indptr, vals, out_max): n = len(out_max) for i in range(n): <IF_STMT> m = a_indptr[i] for j in range(a_indptr[i] + 1, a_indptr[i + 1]): if vals[j] > vals[m]: m = j out_max[i] = vals[m]",if a_indptr[i] != a_indptr[i + 1]:
"def update_encryption_keys(self, options): if not options['pools'] and (not options['datasets']): raise CallError('Please specify pools/datasets to update') async with ENCRYPTION_CACHE_LOCK: keys = await self.encryption_keys() for pool in options['pools']: keys['geli'][pool['name']] = pool['passphrase'] for dataset in options['datasets']: keys['zfs'][dataset['name']] = dataset['passphrase'] await self.middleware.call('cache.put', 'failover_encryption_keys', keys) <IF_STMT> await self.sync_keys_to_remote_node(lock=False)",if options['sync_keys']:
"def set_lineno(self, lineno, override=False): """"""Set the line numbers of the node and children."""""" todo = deque([self]) while todo: node = todo.popleft() if 'lineno' in node.attributes: <IF_STMT> node.lineno = lineno todo.extend(node.iter_child_nodes()) return self",if node.lineno is None or override:
"def is_ArAX_implicit(ii): a, implicit_fixed = (0, 0) for op in _gen_opnds(ii): <IF_STMT> a += 1 elif op_reg(op) and op_implicit_specific_reg(op): implicit_fixed += 1 else: return False return a == 1 and implicit_fixed <= 1","if op_luf_start(op, 'ArAX'):"
"def __iter__(self): if hasattr(self, 'error_dict'): for field, errors in self.error_dict.items(): yield (field, list(ValidationError(errors))) else: for error in self.error_list: message = error.message <IF_STMT> message %= error.params yield force_text(message)",if error.params:
"def _mul_matrix(self, other): if isinstance(other, ConstantDiagLazyTensor): <IF_STMT> raise ValueError(f'Dimension Mismatch: Must have same diag_shape, but got {self.diag_shape} and {other.diag_shape}') return self.__class__(self.diag_values * other.diag_values, diag_shape=self.diag_shape) return super()._mul_matrix(other)",if not self.diag_shape == other.diag_shape:
"def test_no_metadata_when_py_is_pep8(py_file): """"""This test assumes that all Python files in the jupytext folder follow PEP8 rules"""""" nb = read(py_file) for i, cell in enumerate(nb.cells): <IF_STMT> cell.metadata.pop('title') if i == 0 and (not cell.source): assert cell.metadata == {'lines_to_next_cell': 0}, py_file else: assert not cell.metadata, (py_file, cell.source)",if 'title' in cell.metadata:
"def forward(self, x: Tensor, edge_index: Adj) -> Tensor: """""""""""" if self.add_self_loops: <IF_STMT> edge_index, _ = remove_self_loops(edge_index) edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim)) elif isinstance(edge_index, SparseTensor): edge_index = set_diag(edge_index) x_norm = F.normalize(x, p=2.0, dim=-1) return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)","if isinstance(edge_index, Tensor):"
"def should_wait(self, offer_hash: str): with self._lock: if self._offer_hash is not None: <IF_STMT> logger.debug('already processing another offer (%s vs %s)', self._offer_hash, offer_hash) return True if self._started == self._wtct_num_subtasks: logger.info('all subtasks for `%s` have been started', self._offer_hash) return True return False",if self._offer_hash != offer_hash:
"def _wrap_linespans(self, inner): s = self.linespans i = self.linenostart - 1 for t, line in inner: <IF_STMT> i += 1 yield (1, '<span id=""%s-%d"">%s</span>' % (s, i, line)) else: yield (0, line)",if t:
"def onRemoteResponse(self, response, request_info): if isinstance(response, (dict,)): <IF_STMT> msg = 'Celery echo: %s\nElapsed Time: %d' self.setText(msg % (response['echo'], self.wait_cnt)) else: msg = 'Waiting for Celery (id, checkno): %s, %d' Label.setText(self, msg % (self.task_id, self.wait_cnt)) else: self.setText('Could not get remote response as a dictionary')",if 'echo' in response:
"def Visit_expr_stmt(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.ASSIGN_OPERATOR)","if isinstance(child, pytree.Leaf) and child.value == '=':"
"def _list_outputs(self): outputs = self.output_spec().get() isHeader = True for key in self._outfields: outputs[key] = [] with open(self.inputs.in_file, 'r') as fid: for line in fid.readlines(): <IF_STMT> isHeader = False continue entry = self._parse_line(line) outputs = self._append_entry(outputs, entry) return outputs",if self.inputs.header and isHeader:
"def _get_tables(self, schema): cursor = self._get_cursor() schemas = self.configuration.get('schemas', self.configuration.get('database', '')).split(',') for schema_name in schemas: cursor.columns(schema=schema_name) for column in cursor: table_name = '{}.{}'.format(column[1], column[2]) <IF_STMT> schema[table_name] = {'name': table_name, 'columns': []} schema[table_name]['columns'].append(column[3]) return list(schema.values())",if table_name not in schema:
"def __setitem__(self, index, value): if self._physics.is_dirty and (not self._triggers_dirty): self._physics.forward() super(_SynchronizingArrayWrapper, self).__setitem__(index, value) if isinstance(self._backing_index, collections.Iterable): <IF_STMT> resolved_index = (self._backing_index[index[0]],) + index[1:] else: resolved_index = self._backing_index[index] self._backing_array[resolved_index] = value if self._triggers_dirty: self._physics.mark_as_dirty()","if isinstance(index, tuple):"
"def fit_test_data(self, data, fit_values, imputer_value): for j in range(len(data)): for i in range(len(data[j])): <IF_STMT> data[j][i] = str(fit_values[i]) return data",if data[j][i] in imputer_value:
"def Compare_in(t, x): if not isinstance(x.ops[0], (ast.NotIn, ast.In)): return if t.enable_snippets: from ..snippets import _in, in_es6 if t.enable_es6: t.add_snippet(in_es6) sname = 'in_es6' else: t.add_snippet(_in) sname = '_in' result = JSCall(JSAttribute('_pj', sname), [x.left, x.comparators[0]]) <IF_STMT> result = JSUnaryOp(JSOpNot(), result) return result","if isinstance(x.ops[0], ast.NotIn):"
"def __init__(self, f): self._refs = {} self._peeled = {} for line in f.readlines(): sha, name = line.rstrip(b'\n').split(b'\t') if name.endswith(ANNOTATED_TAG_SUFFIX): name = name[:-3] <IF_STMT> raise ValueError('invalid ref name %r' % name) self._peeled[name] = sha else: if not check_ref_format(name): raise ValueError('invalid ref name %r' % name) self._refs[name] = sha",if not check_ref_format(name):
def info(args): p = Python37Parser() if len(args) > 0: arg = args[0] <IF_STMT> from uncompyle6.parser.parse37 import Python37Parser p = Python37Parser() elif arg == '3.8': from uncompyle6.parser.parse38 import Python38Parser p = Python38Parser() else: raise RuntimeError('Only 3.7 and 3.8 supported') p.check_grammar() if len(sys.argv) > 1 and sys.argv[1] == 'dump': print('-' * 50) p.dump_grammar(),if arg == '3.7':
"def test_ESPnetDataset_text_float(text_float): dataset = IterableESPnetDataset(path_name_type_list=[(text_float, 'data8', 'text_float')], preprocess=preprocess) for key, data in dataset: if key == 'a': assert all(data['data8'] == np.array([1.4, 3.4], dtype=np.float32)) <IF_STMT> assert all(data['data8'] == np.array([0.9, 9.3], dtype=np.float32))",if key == 'b':
"def getting(self, key, lock=False): if not lock: yield self.get(key) else: locked = False try: data = self._get_or_lock(key) locked = data is None yield data finally: <IF_STMT> self._release_lock(key)",if locked:
"def mkdir(self, path, parents=True, raise_if_exists=False): if self.exists(path): <IF_STMT> raise luigi.target.NotADirectory() elif raise_if_exists: raise luigi.target.FileAlreadyExists() else: return self.conn.files_create_folder_v2(path)",if not self.isdir(path):
"def _get_initiated_elections(cls, height, txns): elections = [] for tx in txns: <IF_STMT> continue elections.append({'election_id': tx.id, 'height': height, 'is_concluded': False}) return elections","if not isinstance(tx, Election):"
"def recalc_active(self, ts): if not self.active_seconds: self.active_seconds.append(ts) self.data[ts] = {} if ts not in self.active_seconds: <IF_STMT> for i in range(max(self.active_seconds) + 1, ts + 1): self.active_seconds.append(i) self.active_seconds.sort() self.data[i] = {} while len(self.active_seconds) > self.window: self.active_seconds.pop(0) for sec in self.data.keys(): if sec not in self.active_seconds: self.data.pop(sec)",if ts > max(self.active_seconds):
"def get_scalar_base(schema, scalar) -> Tuple[str, ...]: base = base_type_name_map.get(scalar.id) if base is not None: return base for ancestor in scalar.get_ancestors(schema).objects(schema): <IF_STMT> try: base = base_type_name_map[ancestor.id] except KeyError: base = common.get_backend_name(schema, ancestor, catenate=False) return base raise ValueError(f'cannot determine backend type for scalar type {scalar.get_name(schema)}')",if not ancestor.get_is_abstract(schema):
def __next__(self): try: value = next(self._iterable) <IF_STMT> self.start() else: self.update(self.value + 1) return value except StopIteration: self.finish() raise except GeneratorExit: self.finish(dirty=True) raise,if self.start_time is None:
"def change_password(username='flexget', password='', session=None): check = zxcvbn.zxcvbn(password, user_inputs=[username]) if check['score'] < 3: warning = check['feedback']['warning'] suggestions = ' '.join(check['feedback']['suggestions']) message = ""Password '{}' is not strong enough. "".format(password) <IF_STMT> message += warning + ' ' if suggestions: message += 'Suggestions: {}'.format(suggestions) raise WeakPassword(message) user = get_user(username=username, session=session) user.password = str(generate_password_hash(password)) session.commit()",if warning:
"def _options_fcheck(self, name, xflags, table): for entry in table: if entry.name is None: break if entry.flags & XTOPT_MAND and (not xflags & 1 << entry.id): raise XTablesError('%s: --%s must be specified' % (name, entry.name)) <IF_STMT> continue",if not xflags & 1 << entry.id:
"def parse_ports(container_name, connection_configuration): while True: ports_command = docker_util.build_docker_simple_command('port', container_name=container_name, **connection_configuration) with tempfile.TemporaryFile(prefix='docker_port_') as stdout_file: exit_code = subprocess.call(ports_command, shell=True, stdout=stdout_file, preexec_fn=os.setpgrp) <IF_STMT> stdout_file.seek(0) ports_raw = stdout_file.read().decode('utf-8') return ports_raw",if exit_code == 0:
"def _init_ti_table(): global _ti_table _ti_table = [] for fname, name in zip(kc.STRFNAMES, kc.STRNAMES): seq = termcap.get(name) if not seq: continue k = _name_to_key(fname) <IF_STMT> _ti_table.append((list(bytearray(seq)), k))",if k:
"def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return (args, dict(kwargs)) except (TypeError, ValueError): args, kwargs = ((), {}) if a is not None: <IF_STMT> args = tuple() kwargs = a elif isinstance(a, tuple): if isinstance(a[-1], dict): args, kwargs = (a[0:-1], a[-1]) else: args = a kwargs = {} return (args, kwargs)","if isinstance(a, dict):"
def fork_with_import_lock(level): release = 0 in_child = False try: try: for i in range(level): imp.acquire_lock() release += 1 pid = os.fork() in_child = not pid finally: for i in range(release): imp.release_lock() except RuntimeError: if in_child: <IF_STMT> print('RuntimeError in child') os._exit(1) raise if in_child: os._exit(0) self.wait_impl(pid),if verbose > 1:
"def _capture_hub(self, create): if self.hub is None: current_hub = get_hub() if create else get_hub_if_exists() <IF_STMT> return if self.hub is None: self.hub = current_hub",if current_hub is None:
"def get_user_makepkg_path(cls) -> Optional[str]: if cls._user_makepkg_path == 'unset': possible_paths = [os.path.expanduser('~/.makepkg.conf'), os.path.join(CONFIG_ROOT, 'pacman/makepkg.conf')] config_path: Optional[str] = None for path in possible_paths: <IF_STMT> config_path = path cls._user_makepkg_path = config_path return cls._user_makepkg_path",if os.path.exists(path):
"def createValue(self): mode = [] for name in self._text_keys: <IF_STMT> if 4 <= len(mode): mode.append('...') break else: mode.append(name) if mode: return ', '.join(mode) else: return '(none)'",if self[name].value:
"def keyPressEvent(self, event): if event.key() in (Qt.Key_Right, Qt.Key_Left): direction = 1 if event.key() == Qt.Key_Left: direction = -1 <IF_STMT> print('shift') direction *= 10 self.timeline.setValue(self.timeline.value() + direction) else: super(VideoPlayerWidget, self).keyPressEvent(event)",if event.modifiers() == Qt.ShiftModifier:
"def validate_wrapper(*args, **kwargs): result = self.validate_func(*args, **kwargs) if request.is_xhr: <IF_STMT> result = {} result.setdefault('success', True) values = result.get('values', {}) for key, value in tmpl_context.form_values.iteritems(): values.setdefault(key, value) return result","if not isinstance(result, dict):"
"def copy_metadata_to(self, target_dir): prefix = os.path.join(self.egg_info, '') for path in self.ei_cmd.filelist.files: <IF_STMT> target = os.path.join(target_dir, path[len(prefix):]) ensure_directory(target) self.copy_file(path, target)",if path.startswith(prefix):
"def _get_switch_info(self, cmd_list): stdout, stderr, sw_data = (None, None, None) try: stdout, stderr = self._run_ssh(cmd_list, True) LOG.debug('CLI output from ssh - output: %s', stdout) <IF_STMT> sw_data = stdout.splitlines() return sw_data except processutils.ProcessExecutionError as e: msg = _('Error while getting data via ssh: (command=%(cmd)s error=%(err)s).') % {'cmd': cmd_list, 'err': six.text_type(e)} LOG.error(msg) raise exception.CiscoZoningCliException(reason=msg)",if stdout:
"def analyze(vw): for va, dest in vw.findPointers(): loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) <IF_STMT> continue if bytes[offset - 2:offset] == b'\xff\x15': if vw.getLocation(va): vw.delLocation(va) vw.makeCode(va - 2)",if offset < 2:
"def _freeze_stages(self): """"""Freeze parameters."""""" if self.frozen_stages >= 0: <IF_STMT> self.stem.eval() for param in self.stem.parameters(): param.requires_grad = False else: self.norm1.eval() for m in [self.conv1, self.norm1]: for param in m.parameters(): param.requires_grad = False for i in range(1, self.frozen_stages + 1): m = getattr(self, f'layer{i}') m.eval() for param in m.parameters(): param.requires_grad = False",if self.deep_stem:
"def seek(self, timestamp, log=True): """"""Seek to a particular timestamp in the movie."""""" if self.status in [PLAYING, PAUSED]: player = self._player if player and player.is_seekable(): player.set_time(int(timestamp * 1000.0)) self._vlc_clock.reset(timestamp) <IF_STMT> self._pause_time = timestamp if log: logAttrib(self, log, 'seek', timestamp)",if self.status == PAUSED:
def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] <IF_STMT> return True elif ch == '(': openParen += 1 elif ch == ')': if openParen == 0: return False openParen -= 1 elif ch == ';' or ch == '}': return False i += 1 return False,if ch == '{':
"def update(events): if failsToWriteToIDClasses(): print('Skip event: cannot write to ID classes') return if didNameChange() or events.intersection({'File', 'Addon', 'Tree'}): updateEverything() if problems.canAutoExecute(): nodeTrees = list(iterAutoExecutionNodeTrees(events)) <IF_STMT> setupExecutionUnits() executeNodeTrees(nodeTrees) afterExecution() finishExecutionUnits()",if len(nodeTrees) > 0:
def check_all_verified(self): if not self.all_verified: new_all_verified = not self.lines.filter(verified=False).exists() <IF_STMT> self.all_verified = True if self.require_verification: self.add_log_entry(_('All rows requiring verification have been verified.')) self.require_verification = False self.save() return self.all_verified,if new_all_verified:
"def parse_for(cls, tagname, parser, bits, options): if bits: <IF_STMT> bits.pop(0) if len(bits): options['for'] = Variable(bits.pop(0)) else: raise TemplateSyntaxError('%s: expected an argument after ""for"".' % tagname) elif not cls.optional_for_parameter: raise TemplateSyntaxError('Unknown argument for %s tag: %r.' % (tagname, bits[0]))",if bits[0] == 'for':
"def _get_cuda_device(*args): for arg in args: <IF_STMT> check_cuda_available() return Device(arg) if isinstance(arg, ndarray): if arg.device is None: continue return arg.device if available and isinstance(arg, Device): return arg return DummyDevice","if type(arg) is not bool and isinstance(arg, _integer_types):"
"def while1_test(a, b, c): while 1: if a != 2: <IF_STMT> a = 3 b = 0 elif c: c = 0 else: a += b + c break return (a, b, c)",if b:
"def write_notes(self, family, father, mother): self.write_note_of_person(father) self.write_note_of_person(mother) child_ref_list = family.get_child_ref_list() if child_ref_list: for child_ref in child_ref_list: child = self.db.get_person_from_handle(child_ref.ref) <IF_STMT> self.write_note_of_person(child)",if child:
"def GetFile(cls, session, sig, mode='r'): sig = sig[:cls.HASH_LEN] while len(sig) > 0: fn = cls.SaveFile(session, sig) try: <IF_STMT> return (open(fn, mode), sig) except (IOError, OSError): pass if len(sig) > 1: sig = sig[:-1] elif 'r' in mode: return (None, sig) else: return (open(fn, mode), sig) return (None, None)",if os.path.exists(fn):
"def _generate_expression(self): e = [] for part in PARSE_RE.split(self._format): if not part: continue elif part == '{{': e.append('\\{') elif part == '}}': e.append('\\}') <IF_STMT> e.append(self._handle_field(part)) else: e.append(REGEX_SAFETY.sub(self._regex_replace, part)) return ''.join(e)",elif part[0] == '{' and part[-1] == '}':
"def get_cfg_dict(self, with_meta=True): options_dict = self.merged_options if with_meta: <IF_STMT> options_dict.update({'package': 'yandextank.plugins.{}'.format(self.plugin)}) if self.enabled is not None: options_dict.update({'enabled': self.enabled}) return options_dict",if self.plugin:
"def __str__(self): _outicalfile = self._icalfile for unit in self.units: for location in unit.getlocations(): match = re.match('\\[(?P<uid>.+)\\](?P<property>.+)', location) for component in self._icalfile.components(): <IF_STMT> continue if component.uid.value != match.groupdict()['uid']: continue for property in component.getChildren(): if property.name == match.groupdict()['property']: property.value = unit.target if _outicalfile: return str(_outicalfile.serialize()) else: return ''",if component.name != 'VEVENT':
"def process_events(self, events): for event in events: key = (event.ident, event.filter) if event.ident == self._force_wakeup_fd: self._force_wakeup.drain() continue receiver = self._registered[key] <IF_STMT> del self._registered[key] if type(receiver) is _core.Task: _core.reschedule(receiver, outcome.Value(event)) else: receiver.put_nowait(event)",if event.flags & select.KQ_EV_ONESHOT:
"def forward(self, start=True, search=False, target=None, include_current=False): """"""Move one step forward in the history."""""" if target is None: target = self.saved_line if self.index > 1: <IF_STMT> self.index -= self.find_partial_match_forward(target, include_current) elif start: self.index -= self.find_match_forward(target, include_current) else: self.index -= 1 return self.entry else: self.index = 0 return self.saved_line",if search:
"def _charlabels(self, options): """"""Get labels for characters (PRIVATE)."""""" self.charlabels = {} opts = CharBuffer(options) while True: w = opts.next_word() if w is None: break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state c = opts.next_nonwhitespace() if c is None: break <IF_STMT> raise NexusError(""Missing ',' in line %s."" % options)","elif c != ',':"
"def _get_cloudstorage_bucket_iam_member_bindings(self, raw_bucket): bucket_iam_policy = raw_bucket.iam_policy member_bindings = {} if bucket_iam_policy: for binding in bucket_iam_policy._bindings: <IF_STMT> for member in binding['members']: if member not in member_bindings: member_bindings[member] = [binding['role']] else: member_bindings[member].append(binding['role']) return member_bindings",if 'legacy' not in binding['role']:
"def _gen(): for i in dataset(): if isinstance(i, tuple) or isinstance(i, list): if fn(*i) is True: yield i el<IF_STMT> yield i",if fn(i) is True:
"def set_img_to_eval_imgs(self, scores, img_ids, method): for img_id, score in zip(img_ids, scores): <IF_STMT> self.img_to_eval[img_id] = dict() self.img_to_eval[img_id]['image_id'] = img_id self.img_to_eval[img_id][method] = score",if img_id not in self.img_to_eval:
"def _compute_totals(self): totals = {} for entry in self.entries: for k, v in entry.nutrition_information.items(): <IF_STMT> totals[k] = v else: totals[k] += v self._totals = totals",if k not in totals:
"def analyzeFunction(vw, funcva): for fromva, tova, rtype, rflags in vw.getXrefsFrom(funcva, v_const.REF_CODE): loc = vw.getLocation(tova) if loc is None: continue va, size, ltype, linfo = loc <IF_STMT> continue vw.makeFunctionThunk(funcva, linfo)",if ltype != v_const.LOC_IMPORT:
"def clear_output_directory(self): files = os.listdir(os.path.join('functional', 'output')) for f in files: <IF_STMT> continue path = os.path.join('functional', 'output', f) if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path)","if f in ('README.txt', '.svn', 'CVS'):"
"def test_output_files_as_none_string(self): for name in ('Output', 'Report', 'Log', 'XUnit', 'DebugFile'): attr = (name[:-4] if name.endswith('File') else name).lower() settings = RobotSettings({name.lower(): 'NoNe'}) assert_equals(settings[name], None) <IF_STMT> assert_equals(getattr(settings, attr), None)","if hasattr(settings, attr):"
def is_rotated(box_list): if type(box_list) == np.ndarray: return box_list.shape[1] == 5 elif type(box_list) == list: <IF_STMT> return False return np.all(np.array([len(obj) == 5 and (type(obj) == list or type(obj) == np.ndarray) for obj in box_list])) return False,if box_list == []:
"def visit_loop(self): v = self.vS.top_front() i = self.iS.top_front() num_edges = len(self.graph[v].edges) while i <= num_edges: <IF_STMT> self.finish_edge(v, i - 1) if i < num_edges and self.begin_edge(v, i): return i += 1 self.finish_visiting(v)",if i > 0:
"def GetConvertersByClass(value_cls): """"""Returns all converters that take given value as an input value."""""" try: return ExportConverter.converters_cache[value_cls] except KeyError: results = [cls for cls in ExportConverter.classes.values() if cls.input_rdf_type == value_cls] <IF_STMT> results = [DataAgnosticExportConverter] ExportConverter.converters_cache[value_cls] = results return results",if not results:
"def migrate_Context(self): for old_obj in self.session_old.query(self.model_from['Context']): new_obj = self.model_to['Context']() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns._data.keys(): continue value = getattr(old_obj, key) <IF_STMT> value = 0 setattr(new_obj, key, value) self.session_new.add(new_obj)",if key == 'tip_timetolive' and value < 0:
"def _bind_to(self, url, bind): """"""Bind to a Connectable in the caller's thread."""""" if isinstance(bind, util.string_types + (url.URL,)): try: self.context._engine = self.__engines[bind] except KeyError: e = sqlalchemy.create_engine(bind) self.__engines[bind] = e self.context._engine = e else: <IF_STMT> self.__engines[bind] = bind self.context._engine = bind",if bind not in self.__engines:
"def _gen_Less(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): <IF_STMT> result.append(self.builder.fcmp_ordered('<', lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed('<', lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)",if ret_type == real_type:
"def _store_pickle_output(self, pickle_output): if pickle_output: <IF_STMT> self.error(""Can't use without --output"", 'pickle-output') elif not load_pytd.is_pickle(self.output_options.output): self.error('Must specify %s file for --output' % load_pytd.PICKLE_EXT, 'pickle-output') self.output_options.pickle_output = pickle_output",if self.output_options.output is None:
"def resolve_identifier(self, identifier): if ':' in identifier: conn, pn = identifier.split(':') <IF_STMT> pn = int(pn) return self.resolve_identifier(self.connector_table[conn][pn]) else: return identifier",if pn.isdigit():
"def add_braces_and_labels(self): for attr in ('horizontal_parts', 'vertical_parts'): if not hasattr(self, attr): continue parts = getattr(self, attr) for subattr in ('braces', 'labels'): <IF_STMT> self.add(getattr(parts, subattr))","if hasattr(parts, subattr):"
"def on_janitor_selection_changed(self, selection): model, iter = selection.get_selected() if iter: <IF_STMT> iter = self.janitor_model.iter_children(iter) plugin = model[iter][self.JANITOR_PLUGIN] for row in self.result_model: if row[self.RESULT_PLUGIN] == plugin: self.result_view.get_selection().select_path(row.path) log.debug('scroll_to_cell: %s' % row.path) self.result_view.scroll_to_cell(row.path)",if self.janitor_model.iter_has_child(iter):
"def canonical_standard_headers(self, headers): interesting_headers = ['content-md5', 'content-type', 'date'] hoi = [] if 'Date' in headers: del headers['Date'] headers['Date'] = self._get_date() for ih in interesting_headers: found = False for key in headers: lk = key.lower() if headers[key] is not None and lk == ih: hoi.append(headers[key].strip()) found = True <IF_STMT> hoi.append('') return '\n'.join(hoi)",if not found:
"def boolean(value): if isinstance(value, str): v = value.lower() if v in ('1', 'yes', 'true', 'on'): return True <IF_STMT> return False raise ValueError(value) return bool(value)","if v in ('0', 'no', 'false', 'off'):"
"def get_extension_for_class(self, extclass): if extclass is UnrecognizedExtension: raise TypeError(""UnrecognizedExtension can't be used with get_extension_for_class because more than one instance of the class may be present."") for ext in self: <IF_STMT> return ext raise ExtensionNotFound('No {} extension was found'.format(extclass), extclass.oid)","if isinstance(ext.value, extclass):"
"def sysargs_to_mainargs(): """"""builds main args from sys.argv"""""" relative_out_dir = None if len(sys.argv) > 1 and sys.argv[1].startswith('--'): a = sys.argv.pop(1) <IF_STMT> print(__doc__) sys.exit(1) elif a.startswith('--reldir='): relative_out_dir = a[len('--reldir='):] else: print('*** Error, Unknown option:', a) print(__doc__) sys.exit(1) other_session = sys.argv[1] return (relative_out_dir, other_session)",if a.startswith('--help'):
"def _scanDirectory(self, dirIter, f): while len(f) < 250: try: info = next(dirIter) except StopIteration: <IF_STMT> raise EOFError return f if isinstance(info, defer.Deferred): info.addCallback(self._cbScanDirectory, dirIter, f) return else: f.append(info) return f",if not f:
"def register_options(config_block): for name in common_block: safe_declare_common_option(config_block, name) <IF_STMT> config_block.get(name).declare_as_argument()",if config_block.get(name)._argparse is None:
"def _loc(obj): try: fn = getattr(obj, '__file__', None) <IF_STMT> return ' @%s' % (fn,) obj = getattr(obj, 'im_func', obj) code = getattr(obj, '__code__', None) if code is not None: return ' @%s:%s' % (code.co_filename, code.co_firstlineno) except Exception: pass return ''",if fn is not None:
"def _remove_temporary_files(self, temporary_files): """"""Internal function for cleaning temporary files"""""" for file_object in temporary_files: file_name = file_object.name file_object.close() <IF_STMT> os.remove(file_name) arff_file_name = file_name + '.arff' if os.path.exists(arff_file_name): os.remove(arff_file_name)",if os.path.exists(file_name):
"def show(self): """"""Overrides Qt Method"""""" QWidget.show(self) self.emit(SIGNAL('visibility_changed(bool)'), True) if self.editor is not None: text = self.editor.get_selected_text() <IF_STMT> self.search_text.setEditText(text) self.search_text.lineEdit().selectAll() self.refresh() else: self.search_text.lineEdit().selectAll() self.search_text.setFocus()",if len(text) > 0:
def flush_input() -> None: if not self.is_done: keys = self.input.flush_keys() self.key_processor.feed_multiple(keys) self.key_processor.process_keys() <IF_STMT> f.set_exception(EOFError),if self.input.closed:
"def get_default_taxes_and_charges(master_doctype, tax_template=None, company=None): if not company: return {} if tax_template and company: tax_template_company = frappe.db.get_value(master_doctype, tax_template, 'company') <IF_STMT> return default_tax = frappe.db.get_value(master_doctype, {'is_default': 1, 'company': company}) return {'taxes_and_charges': default_tax, 'taxes': get_taxes_and_charges(master_doctype, default_tax)}",if tax_template_company == company:
"def dump_prefs(self): ret = '' for pref in self.prefs: if type(self.prefs[pref].value) == int: value = str(self.prefs[pref].value) <IF_STMT> value = 'true' if self.prefs[pref].value == True else 'false' else: value = '""%s""' % self.prefs[pref].value ret += pref + ': ' + value + ' (' + self.prefs[pref].anon_source + ')\n' return ret",elif type(self.prefs[pref].value) == bool:
"def dumps(o, **kwargs): """"""Dumps JSON object."""""" try: return _engine[1](o) except: ExceptionClass, why = sys.exc_info()[:2] <IF_STMT> raise JSONError(why) else: raise why","if any([issubclass(ExceptionClass, e) for e in _engine[2]]):"
"def main(): import sys, getopt try: opts, args = getopt.getopt(sys.argv[1:], 'ho:', ['help', 'output=']) except getopt.GetoptError as err: usage() sys.exit(1) output = None for o, a in opts: <IF_STMT> usage() sys.exit() elif o in ('-o', '--output'): output = a else: usage() sys.exit(1) if not args: usage() sys.exit(1) concat_flv(args, output)","if o in ('-h', '--help'):"
"def close_group(self): """"""Closes a grouping for previous filters"""""" if self._filters: if len(self._open_group_flag) < len(self._close_group_flag) + 1: raise RuntimeError('Not enough open groups to close.') <IF_STMT> flt_sentence = self._filters[-2] else: flt_sentence = self._filters[-1] flt_sentence[1] = flt_sentence[1] + ')' self._close_group_flag.append(False) else: raise RuntimeError(""No filters present. Can't close a group"") return self","if isinstance(self._filters[-1], ChainOperator):"
"def _GetPlugins(self, base_class): items = [] for name in sorted(base_class.classes.keys()): cls = base_class.classes[name] if cls == output_plugin.UnknownOutputPlugin: continue <IF_STMT> items.append(ApiOutputPluginDescriptor().InitFromOutputPluginClass(cls)) return items",if cls.description:
"def _set_helper(settings, path, value, data_type=None): path = _to_settings_path(path) method = settings.set if data_type is not None: name = None if data_type == bool: name = 'setBoolean' elif data_type == float: name = 'setFloat' elif data_type == int: name = 'setInt' <IF_STMT> method = getattr(settings, name) method(path, value) settings.save()",if name is not None:
"def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): if not isinstance(values, list): values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: if value is None: continue <IF_STMT> key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield (url_quote_plus(key) + '=' + url_quote_plus(value))","if not isinstance(key, bytes):"
"def validate_data(self, data, schema): verrors = ValidationErrors() provider = data['provider'] if provider == 'custom': for k in ('custom_ddns_server', 'custom_ddns_path'): <IF_STMT> verrors.add(f'{schema}.{k}', 'Required when using a custom provider.') elif provider not in await self.provider_choices(): verrors.add(f'{schema}.provider', 'Please select a valid provider.') verrors.check()",if not data[k]:
"def render(self): x = '<span>' for idx, arg in enumerate(self.args, start=1): <IF_STMT> value, desc = arg else: value, desc = (arg, arg) attrs = self.attrs.copy() attrs['name'] = self.name attrs['type'] = 'radio' attrs['value'] = value attrs['id'] = self.name + str(idx) if self.value == value: attrs['checked'] = 'checked' x += '<input %s/> %s' % (attrs, net.websafe(desc)) x += '</span>' return x","if isinstance(arg, (tuple, list)):"
"def search_rotate(array, val): low, high = (0, len(array) - 1) while low <= high: mid = (low + high) // 2 <IF_STMT> return mid if array[low] <= array[mid]: if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 elif array[mid] <= val <= array[high]: low = mid + 1 else: high = mid - 1 return -1",if val == array[mid]:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('wangzhan\\.360\\.cn', headers.get('X-Powered-By-360wzb', ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def _recalculate(self): parent_path = tuple(self._get_parent_path()) if parent_path != self._last_parent_path: spec = self._path_finder(self._name, parent_path) <IF_STMT> if spec.submodule_search_locations: self._path = spec.submodule_search_locations self._last_parent_path = parent_path return self._path",if spec is not None and spec.loader is None:
"def _get_directory_item_content(filename, return_binary, encoding): content = None if os.path.exists(filename): <IF_STMT> mode = 'rb' encoding = None else: mode = 'r' with codecs.open(filename, mode, encoding=encoding) as file_obj: content = file_obj.read() return content",if return_binary:
"def randint(self, beg, end): if beg == 1 and end == 10: self.icnt1_10 += 1 <IF_STMT> self.icnt1_10 = 1 return self.RINT1_10[self.icnt1_10 - 1] if beg == 65 and end == 90: self.icnt65_90 += 1 if self.icnt65_90 > len(self.RINT65_90): self.icnt65_90 = 1 return self.RINT65_90[self.icnt65_90 - 1] raise Exception('Not implemented')",if self.icnt1_10 > len(self.RINT1_10):
"def _get_two_devices(self, require_same_type=False): tpus = extensions.tpu_devices() if FLAGS.requires_tpu: <IF_STMT> res = tpus else: raise ValueError('This test requires 2 TPU cores but %s are found' % len(tpus)) elif len(tpus) == 2: res = tpus elif self._hasGPU() and (not require_same_type): res = ('CPU:0', 'GPU:0') else: res = ('CPU:0', 'CPU:1') return res",if len(tpus) == 2:
"def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return '[%s, %s]' % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return '?' <IF_STMT> return 'True' elif nfrom.src2 == nto: return 'False' return ''",elif nfrom.src1 == nto:
"def send_frame_imm(self, frame): if frame.name == 's_frame': frame.RecvSeq = self.rsn <IF_STMT> gevent.kill(self.t2_caller) self.telegram_count = 0 response_string = ' '.join((hex(n) for n in frame.build())) logger.info('%s <--- s_frame %s  (%s)', self.address, response_string, self.session_id) return self.sock.send(frame.build())",if self.t2_caller:
"def lin2lin(cp, size, size2): _check_params(len(cp), size) _check_size(size2) if size == size2: return cp new_len = len(cp) / size * size2 result = create_string_buffer(new_len) for i in range(_sample_count(cp, size)): sample = _get_sample(cp, size, i) if size < size2: sample = sample << 4 * size2 / size <IF_STMT> sample = sample >> 4 * size / size2 sample = _overflow(sample, size2) _put_sample(result, size2, i, sample) return result.raw",elif size > size2:
"def tangent(self, t): result = np.array([0, 0, 0]) o = self.omega for i, coeff in enumerate(self.coeffs): j = i // 2 <IF_STMT> result += -(j + 1) * o * coeff * sin((j + 1) * o * t) else: result += (j + 1) * o * coeff * cos((j + 1) * o * t) return result",if i % 2 == 0:
"def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() <IF_STMT> if GPIO.input(self._channel) == self._expected: if not pressed: pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) elif pressed: pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)",if now - when_pressed > self._debounce_time:
"def check_dimensions(nrow, ncol): if nrow is not None: if nrow < 1: warn(""'nrow' must be greater than 0. Your value has been ignored."", PlotnineWarning) nrow = None else: nrow = int(nrow) if ncol is not None: <IF_STMT> warn(""'ncol' must be greater than 0. Your value has been ignored."", PlotnineWarning) ncol = None else: ncol = int(ncol) return (nrow, ncol)",if ncol < 1:
"def visit_FunctionDef(self, node: ast.FunctionDef) -> None: """"""Handles FunctionDef node and set context."""""" if self.current_function is None: self.add_entry(node.name) <IF_STMT> self.add_final_entry(node.name) if self.is_overload(node.decorator_list): self.add_overload_entry(node) self.context.append(node.name) self.current_function = node for child in node.body: self.visit(child) self.context.pop() self.current_function = None",if self.is_final(node.decorator_list):
"def ret(stmt, params=()): match = limit_re.match(stmt) if match: <IF_STMT> n = params[-1] params = params[:-1] else: n = int(match.group(2)) store.sql(match.group(1), params) return [store.cursor.fetchone() for i in xrange(n)] return selectall(stmt, params)",if match.group(2) == '?':
"def OnBodyClick(self, event=None): try: c = self.c p = c.currentPosition() <IF_STMT> self.OnActivateBody(event=event) c.k.showStateAndMode(w=c.frame.body.bodyCtrl) g.doHook('bodyclick2', c=c, p=p, v=p, event=event) except: g.es_event_exception('bodyclick')","if not g.doHook('bodyclick1', c=c, p=p, v=p, event=event):"
"def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) if isinstance(default, timedelta): default = default.total_seconds() <IF_STMT> actual = actual.value if actual != default: yield Error(reason='mismatch', setting=setting_name, default=default, actual=actual)","if isinstance(actual, Enum):"
"def fromVariant(variant): if hasattr(QtCore, 'QVariant') and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) elif t == QtCore.QVariant.Double: return variant.toDouble()[0] <IF_STMT> return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() elif t == QtCore.QVariant.Invalid: return None else: raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName()) else: return variant",elif t == QtCore.QVariant.Int:
"def decode_list(self, prop, value): if not isinstance(value, list): value = [value] if hasattr(prop, 'item_type'): item_type = getattr(prop, 'item_type') dec_val = {} for val in value: <IF_STMT> k, v = self.decode_map_element(item_type, val) try: k = int(k) except: k = v dec_val[k] = v value = dec_val.values() return value",if val is not None:
"def has_valid_checksum(self, number): given_number, given_checksum = (number[:-1], number[-1]) calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) if fragment.isalnum(): calculated_checksum += int(fragment[-1]) if parameter == 1: parameter = 7 <IF_STMT> parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum",elif parameter == 3:
"def encoder(s, *args, **kwargs): r = [] _in = [] for c in s: if ord(c) in PRINTABLE: doB64(_in, r) r.append(c.encode()) <IF_STMT> doB64(_in, r) r.append(b'&-') else: _in.append(c) doB64(_in, r) return (b''.join(r), len(s))",elif c == '&':
"def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] <IF_STMT> value = conv(value) setattr(instance, attr, value) return collected_models",if conv is not None:
"def try_to_find_osquery(self): extention = '' <IF_STMT> extention = '.exe' try: return resources.get_resource('osqueryi' + extention) except IOError as e: if platform.system() == 'Windows': result = 'c:\\ProgramData\\osquery\\osqueryi.exe' if os.access(result, os.R_OK): return result else: return spawn.find_executable('osqueryi') raise e",if platform.system() == 'Windows':
"def get_cached_stats(self, split=tfds.Split.TRAIN): """"""Returns basic statistics for cached dataset."""""" self.assert_cached() if split not in self._stats: stats_path = get_stats_path(self.cache_dir, split) <IF_STMT> raise ValueError(""Stats do not exist for '%s' split: %s"" % (self.name, split)) with tf.io.gfile.GFile(stats_path) as f: self._stats[split] = json.load(f) return self._stats[split]",if not tf.io.gfile.exists(stats_path):
"def _network_connections_in_results(data): for plugin_name, plugin_result in data.iteritems(): <IF_STMT> continue if 'device' not in plugin_result: continue if 'connections' in plugin_result['device']: for conn in plugin_result['device']['connections']: if conn['connection_type'] == ConnectionType.network.name: return True return False",if plugin_result['status'] == 'error':
"def register_asyncio_task(self, task, module_path=None): if self._current['metadata'] is None: <IF_STMT> raise RuntimeError('module_path must be supplied for late-binded tasks') else: self.list[module_path]['asyncio.task'].append(task) else: self._current['asyncio.task'].append(task)",if module_path is None:
"def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split('/', 1) if parts[0]: comments[single] = [parts[0]] <IF_STMT> comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)",if len(parts) > 1:
"def api(request, app): marker = request.keywords.get('api') bpkwargs = {} kwargs = {} if marker: <IF_STMT> bpkwargs['url_prefix'] = marker.kwargs.pop('prefix') if 'subdomain' in marker.kwargs: bpkwargs['subdomain'] = marker.kwargs.pop('subdomain') kwargs = marker.kwargs blueprint = Blueprint('api', __name__, **bpkwargs) api = restplus.Api(blueprint, **kwargs) app.register_blueprint(blueprint) yield api",if 'prefix' in marker.kwargs:
"def _get_pip_index_urls(sources): index_urls = [] trusted_hosts = [] for source in sources: url = source.get('url') <IF_STMT> continue index_urls.append(url) if source.get('verify_ssl', True): continue host = six.moves.urllib.parse.urlparse(source['url']).hostname trusted_hosts.append(host) return (index_urls, trusted_hosts)",if not url:
"def add_aggregation_data(self, payload): for timestamp, payload_data in payload.items(): if 'interval_aggs' in payload_data: self.unwrap_interval_buckets(timestamp, None, payload_data['interval_aggs']['buckets']) <IF_STMT> self.unwrap_term_buckets(timestamp, payload_data['bucket_aggs']['buckets']) else: self.check_matches(timestamp, None, payload_data)",elif 'bucket_aggs' in payload_data:
"def _handle_unverified_signed_presence(self, pres): verified = self.verify(pres['status'], pres['signed']) if verified.key_id: if not self.get_keyid(pres['from']): known_keyids = [e['keyid'] for e in self.gpg.list_keys()] <IF_STMT> self.gpg.recv_keys(self.key_server, verified.key_id) self.set_keyid(jid=pres['from'], keyid=verified.key_id) self.xmpp.event('signed_presence', pres)",if verified.key_id not in known_keyids:
"def __init__(self, *args, **kwargs): """"""Initialize the texture."""""" super().__init__(*args, **kwargs) assert_empty_kwargs(**kwargs) if len(args) == 1: if isinstance(args[0], vtk.vtkTexture): self._from_texture(args[0]) elif isinstance(args[0], np.ndarray): self._from_array(args[0]) elif isinstance(args[0], vtk.vtkImageData): self._from_image_data(args[0]) <IF_STMT> self._from_file(filename=args[0]) else: raise TypeError(f'Table unable to be made from ({type(args[0])})')","elif isinstance(args[0], str):"
"def get_manifest_data(manifestpath): """"""Reads a manifest file, returns a dictionary-like object."""""" plist = {} try: plist = FoundationPlist.readPlist(manifestpath) except FoundationPlist.NSPropertyListSerializationException: display.display_error(u'Could not read plist: %s', manifestpath) <IF_STMT> try: os.unlink(manifestpath) except OSError as err: display.display_error(u'Failed to delete plist: %s', err) else: display.display_error('plist does not exist.') return plist",if os.path.exists(manifestpath):
"def _get_proxy(self): url_dissected = url_dissector.findall(self.session['proxy']) if url_dissected and len(url_dissected[0]) == 3: protocol, host, port = url_dissected[0] <IF_STMT> return (socks.PROXY_TYPE_SOCKS5, host, int(port)) if protocol == 'socks4': return (socks.PROXY_TYPE_SOCKS4, host, int(port)) if protocol.startswith('http'): return (socks.PROXY_TYPE_HTTP, host, int(port)) return (None, None, None)",if protocol == 'socks5':
"def nud(self): self.first = [] comma = False if self.token.id != ')': while 1: if self.token.id == ')': break self.first.append(self.expression()) <IF_STMT> comma = True self.advance(',') else: break self.advance(')') if not self.first or comma: return self else: return self.first[0]","if self.token.id == ',':"
"def _debug_log(self, text, level): if text and 'log' in self.config.sys.debug: if not text.startswith(self.log_prefix): text = '%slog(%s): %s' % (self.log_prefix, level, text) <IF_STMT> return self.log_parent.log(level, text) else: self.term.write(self._fmt_log(text, level=level))",if self.log_parent is not None:
"def remove_checker(self, namespace, checker): for c in pyomo.core.check.ModelCheckRunner._checkers(all=True): if c._checkerName() == checker: <IF_STMT> for i in range(namespace.checkers[c._checkerPackage()].count(c._checkerName())): namespace.checkers[c._checkerPackage()].remove(c._checkerName())","if namespace.checkers.get(c._checkerPackage(), None) is not None:"
"def check_if_role_exists(self, role_name, parsed_globals): parameters = {'RoleName': role_name} try: self._call_iam_operation('GetRole', parameters, parsed_globals) except botocore.exceptions.ClientError as e: role_not_found_code = 'NoSuchEntity' error_code = e.response.get('Error', {}).get('Code', '') <IF_STMT> return False else: raise e return True",if role_not_found_code == error_code:
def GetClipboardText(): text = '' if OpenClipboard(0): hClipMem = GetClipboardData(CF_TEXT) <IF_STMT> GlobalLock.restype = c_char_p text = GlobalLock(hClipMem) GlobalUnlock(hClipMem) CloseClipboard() return ensure_unicode(text),if hClipMem:
"def test_log_action_class(): v = Mock() for k, v in amo.LOG_BY_ID.items(): <IF_STMT> cls = 'action-' + v.action_class else: cls = '' assert render('{{ log_action_class(id) }}', {'id': v.id}) == cls",if v.action_class is not None:
"def _get_distinct_albumartists(config, session, web_client, query): logger.debug(f'Getting distinct albumartists: {query}') if query: search_result = _get_search(config, session, web_client, query, album=True) return {artist.name for album in search_result.albums for artist in album.artists if album.artists} else: return {track.album.artist.name for track in _get_playlist_tracks(config, session) <IF_STMT>}",if track.album and track.album.artist
"def _get_commands(): proc = Popen(['react-native', '--help'], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if not line: continue <IF_STMT> should_yield = True continue if should_yield: yield line.split(' ')[0]",if 'Commands:' in line:
"def __call__(self, job): import tensorboard_logger as tl budget = job.kwargs['budget'] timestamps = job.timestamps result = job.result exception = job.exception time_step = int(timestamps['finished'] - self.start_time) if result is not None: tl.log_value('BOHB/all_results', result['loss'] * -1, time_step) <IF_STMT> self.incumbent = result['loss'] tl.log_value('BOHB/incumbent_results', self.incumbent * -1, time_step)",if result['loss'] < self.incumbent:
"def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith('#'): continue if line.startswith('['): <IF_STMT> repos.append(current_repo) current_repo = {} current_repo['name'] = line[1:-1] if current_repo and '=' in line: key, value = line.split('=', 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos",if current_repo:
"def selector(): while True: rlist, _, _ = select([proc.stdout, proc.stderr], [], [], line_timeout) <IF_STMT> raise ProcessLineTimedOut('popen line timeout expired', getattr(proc, 'argv', None), getattr(proc, 'machine', None)) for stream in rlist: yield (stream is proc.stderr, decode(stream.readline(linesize)))",if not rlist and line_timeout:
"def getBranchFromFile(): global _gitdir branch = None if _gitdir: headFile = os.path.join(_gitdir, 'HEAD') if os.path.isfile(headFile): with open(headFile, 'r', encoding='utf-8') as f: line = f.readline() if line: <IF_STMT> branch = line.split('/')[-1].strip() else: branch = 'HEAD' return branch",if line.startswith('ref'):
"def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) if id is None: continue <IF_STMT> if id not in dict or not dict[id].is_alive(): d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()","elif isinstance(id, collections.Hashable):"
"def _print_items(items, _filter=None): if _filter: print('Displaying items matching filter: %s' % _filter) print() for item in items: filtered_out = False for f in _filter.split(): <IF_STMT> filtered_out = True if not filtered_out: print(item) print()",if f.lower() not in item.lower():
"def _cbAllRecords(self, results): ans, auth, add = ([], [], []) for res in results: <IF_STMT> ans.extend(res[1][0]) auth.extend(res[1][1]) add.extend(res[1][2]) return (ans, auth, add)",if res[0]:
"def __status_update(self): was_active = False while True: if self.analytics_instance.active: was_active = True msg = 'Active (%s)' % self.analytics_instance.progress self.broadcast(msg, 'analytics', 'analyticsUpdate') <IF_STMT> self.broadcast('Inactive', 'analytics', 'analyticsUpdate') was_active = False time.sleep(0.2)",if was_active and (not self.analytics_instance.active):
"def plugin_song(self, song): for tag in ['album']: values = filter(None, map(album_to_sort, song.list(tag))) <IF_STMT> song[tag + 'sort'] = '\n'.join(values) for tag in ['artist', 'albumartist', 'performer']: values = filter(None, map(artist_to_sort, song.list(tag))) if values and tag + 'sort' not in song: song[tag + 'sort'] = '\n'.join(values)",if values and tag + 'sort' not in song:
"def update(h, s): with lock: try: i, c = find_cell(h) except KeyError: return <IF_STMT> c.content = parse(s) render_from(i, clear_after=True)",if not c.frozen and c.content != s:
"def get_parameters(self, names, with_decryption): result = [] if len(names) > 10: raise ValidationException(""1 validation error detected: Value '[{}]' at 'names' failed to satisfy constraint: Member must have length less than or equal to 10."".format(', '.join(names))) for name in names: <IF_STMT> result.append(self.get_parameter(name, with_decryption)) return result",if name in self._parameters:
"def entered_file_action(self, path): attempt_copy = True path = self.try_append_extension(path) directory = os.path.dirname(path) if not os.path.exists(directory): try: self.create_folder(directory) except OSError as e: attempt_copy = False sublime.error_message(""Cannot create '"" + path + ""'."" + ' See console for details') print(""Exception: %s '%s'"" % (e.strerror, e.filename)) if attempt_copy: copy_success, new_file = self._copy_file(path) <IF_STMT> self.open_file(new_file)",if copy_success:
"def acquire(self): """"""Acquire semaphore by decrementing value using spin-lock algorithm."""""" while True: with self._cache.transact(retry=True): value = self._cache.get(self._key, default=self._value) <IF_STMT> self._cache.set(self._key, value - 1, expire=self._expire, tag=self._tag) return time.sleep(0.001)",if value > 0:
"def commit(self): doc = {} for field, default in self.fields.iteritems(): <IF_STMT> value = getattr(self, field) if field in self.commit_fields or value != default: doc[field] = getattr(self, field) with open(self.path, 'w') as settings_file: settings_file.write(json.dumps(doc, indent=4))","if hasattr(self, field):"
"def parse_entrypoints(self, content: str, root=None) -> RootDependency: if root is None: root = RootDependency() entrypoints = [] group = 'console_scripts' for line in content.split('\n'): line = line.strip() <IF_STMT> continue if line[0] == '[' and line[-1] == ']': group = line[1:-1] else: entrypoints.append(EntryPoint.parse(text=line, group=group)) root.entrypoints = tuple(entrypoints) return root",if not line or line[0] in '#;':
"def request_with_retries(endpoint, timeout=30): start = time.time() while True: try: return requests.get('http://127.0.0.1:8000' + endpoint, timeout=timeout) except requests.RequestException: <IF_STMT> raise TimeoutError time.sleep(0.1)",if time.time() - start > timeout:
"def get_expression(self): """"""Return the expression as a printable string."""""" l = [] for c in self.content: <IF_STMT> l.append(c.op) if c.child is not None: l.append('(' + c.child.get_expression() + ')') else: l.append('%d' % c.get_value()) return ''.join(l)",if c.op is not None:
"def nrgen_asc(self): for generation in range(self.generations_asc - 1, 0, -1): for p in range(len(self.data[generation])): person, parents, child, userdata = self.data[generation][p] <IF_STMT> return generation return 1",if person:
def check_all_verified(self): if not self.all_verified: new_all_verified = not self.lines.filter(verified=False).exists() if new_all_verified: self.all_verified = True <IF_STMT> self.add_log_entry(_('All rows requiring verification have been verified.')) self.require_verification = False self.save() return self.all_verified,if self.require_verification:
"def sort(self, cmp=None, key=None, reverse=False): """"""Standard list sort method"""""" if key: temp = [(key(v), v) for v in self] temp.sort(key=lambda x: x[0], reverse=reverse) self[:] = [v[1] for v in temp] else: temp = list(self) <IF_STMT> temp.sort(cmp=cmp, reverse=reverse) else: temp.sort(reverse=reverse) self[:] = temp",if cmp is not None:
"def process_formdata(self, valuelist): if valuelist: date_str = ' '.join(valuelist) if not date_str: self.data = None raise ValidationError(self.gettext('Please input a date/time value')) parse_kwargs = self.parse_kwargs.copy() <IF_STMT> try: parse_kwargs['default'] = self.default() except TypeError: parse_kwargs['default'] = self.default try: self.data = parser.parse(date_str, **parse_kwargs) except ValueError: self.data = None raise ValidationError(self.gettext('Invalid date/time input'))",if 'default' not in parse_kwargs:
"def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis): out = output_tensor((ndim + num_newaxis,), 'int64') for i in const_range(out.shape[0]): if i < axis: out[i] = data_shape[i] <IF_STMT> out[i] = int64(1) else: out[i] = data_shape[i - num_newaxis] return out",elif i < axis + num_newaxis:
"def _Return(self, t): self._fill('return ') if t.value: if isinstance(t.value, Tuple): text = ', '.join([name.name for name in t.value.asList()]) self._write(text) else: self._dispatch(t.value) <IF_STMT> self._write('; ')",if not self._do_indent:
"def blas_header_version(): version = (9,) if detect_macos_sdot_bug(): <IF_STMT> version += (1,) else: version += (2,) return version",if detect_macos_sdot_bug.fix_works:
"def get_queues(self, region: str, attribute_names: []): sqs_client = AWSFacadeUtils.get_client('sqs', self.session, region) try: raw_queues = await run_concurrently(sqs_client.list_queues) except Exception as e: print_exception(f'Failed to list SQS queues: {e}') return [] else: <IF_STMT> return [] queue_urls = raw_queues['QueueUrls'] return await map_concurrently(self._get_queue_attributes, queue_urls, region=region, attribute_names=attribute_names)",if 'QueueUrls' not in raw_queues:
"def popupFrameXdiff(job, frame1, frame2, frame3=None): """"""Opens a frame xdiff."""""" for command in ['/usr/bin/xxdiff', '/usr/local/bin/xdiff']: if os.path.isfile(command): for frame in [frame1, frame2, frame3]: <IF_STMT> command += ' --title1 %s %s' % (frame.data.name, getFrameLogFile(job, frame)) shellOut(command)",if frame:
"def wrap(*args, **kwargs): callargs = getcallargs(fun, *args, **kwargs) if callargs['sock'] is None: COUNT['count'] += 1 with IPSet() as sock: callargs['sock'] = sock <IF_STMT> callargs.update(callargs.pop('kwargs')) return fun(**callargs) return fun(*args, **kwargs)",if 'kwargs' in callargs:
"def set_multi(self, value): del self[atype] for addr in value: if not isinstance(addr, Address): <IF_STMT> addr['type'] = atype elif 'atype' in addr and 'type' not in addr: addr['type'] = addr['atype'] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)",if atype != 'all':
"def test_connection(self, data=None, raise_alert=False): try: result = self._test_connection(self.connection_config(data)) except CallError as e: result = {'error': True, 'exception': str(e)} if result['error']: <IF_STMT> config = self.middleware.call_sync('kmip.config') self.middleware.call_sync('alert.oneshot_create', 'KMIPConnectionFailed', {'server': config['server'], 'error': result['exception']}) return False else: return True",if raise_alert:
"def test05_geometries(self): """"""Testing Geometries from Data Source Features."""""" for source in ds_list: ds = DataSource(source.ds) for layer in ds: for feat in layer: g = feat.geom self.assertEqual(source.geom, g.geom_name) self.assertEqual(source.gtype, g.geom_type) <IF_STMT> self.assertEqual(source.srs_wkt, g.srs.wkt)","if hasattr(source, 'srs_wkt'):"
"def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug('__walk_dir_tree. START dir=%s', dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith('py'): <IF_STMT> self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list",if self.module_registrant:
"def setData(self, data=None): for nRow in range(self.nRows): for nCol in range(self.nCols): <IF_STMT> self.SetCellValue(nRow, nCol, '%f' % data[nRow, nCol]) else: self.SetCellValue(nRow, nCol, '0.000') self.AutoSize()",if data is not None and nRow < data.shape[0] and (nCol < data.shape[1]):
"def __init__(self, *args, **kwargs): """"""Initialize the texture."""""" super().__init__(*args, **kwargs) assert_empty_kwargs(**kwargs) if len(args) == 1: if isinstance(args[0], vtk.vtkTexture): self._from_texture(args[0]) elif isinstance(args[0], np.ndarray): self._from_array(args[0]) <IF_STMT> self._from_image_data(args[0]) elif isinstance(args[0], str): self._from_file(filename=args[0]) else: raise TypeError(f'Table unable to be made from ({type(args[0])})')","elif isinstance(args[0], vtk.vtkImageData):"
"def delete_old_post_save(sender, instance, raw, created, update_fields, using, **kwargs): """"""Post_save on all models with file fields, deletes old files"""""" if raw or created: return for field_name, new_file in cache.fields_for_model_instance(instance): <IF_STMT> old_file = cache.get_field_attr(instance, field_name) if old_file != new_file: delete_file(instance, field_name, old_file, using) cache.make_cleanup_cache(instance)",if update_fields is None or field_name in update_fields:
"def do_refresh(self): try: <IF_STMT> service_status = agent_status() self.properties.service_status_label.setText(HUMAN_SERVICE_STATUS[service_status]) finally: QTimer.singleShot(REFRESH_PERIOD, self.do_refresh)",if self.isVisible():
"def json_dumps(data): """"""Return data in nicely formatted json."""""" try: return json.dumps(data, indent=1, sort_keys=True, separators=(',', ': '), default=json_serialize_default) except UnicodeDecodeError: <IF_STMT> data = json_preserialize_binary(data) return json.dumps(data) raise","if sys.version_info[:2] == (2, 7):"
"def __init__(self, aList): for element in aList: if len(element) > 0: if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) elif element.text: text = element.text.strip() <IF_STMT> self.append(text)",if text:
"def __init__(self, token): self._convert_to_ascii = False self._find = None if token.search is None: return flags = 0 self._match_this_many = 1 if token.options: if 'g' in token.options: self._match_this_many = 0 if 'i' in token.options: flags |= re.IGNORECASE <IF_STMT> self._convert_to_ascii = True self._find = re.compile(token.search, flags | re.DOTALL) self._replace = _CleverReplace(token.replace)",if 'a' in token.options:
def get_next(self): if self.current > self.maximum: raise StopIteration else: <IF_STMT> payl = '%0' + str(self.width) + 'd' payl = payl % self.current else: payl = str(self.current) self.current += 1 return payl,if self.width:
"def any(self, provider_name): result = authomatic.login(Webapp2Adapter(self), provider_name) if result: apis = [] <IF_STMT> result.user.update() if result.user.credentials: apis = config.config.get(provider_name, {}).get('_apis', {}) nice_provider_name = config.config.get(provider_name, {}).get('_name') or provider_name.capitalize() render(self, result, result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)))",if result.user:
"def _get_lun_id(self, volume, target_name): """"""Get lun id of the voluem in a target."""""" pool = volume_utils.extract_host(volume.host, level='pool') volume_name = self._trans_name_down(volume.name) lun_id = None luns = self._get_lun_list(target_name) for lun in luns: mappinglvm = lun.get('mappingLvm') lun_name = mappinglvm.replace('%s/' % pool, '') <IF_STMT> lun_id = lun.get('id') return lun_id",if lun_name == volume_name:
"def save_settings(self, settings): for setting in self.settings: setting_obj = settings[setting] new_value = self.cleaned_data.get(setting) <IF_STMT> if new_value and new_value != self.initial.get(setting): self.save_image(setting_obj, new_value) elif self.cleaned_data.get('%s_delete' % setting): self.delete_image(setting_obj) else: self.save_setting(setting_obj, new_value)",if setting_obj.python_type == 'image':
"def setup_with_driver(self): if not self.__class__.shared_state_initialized: try: self.setup_shared_state() self.logout_if_needed() except Exception: self.__class__.shared_state_in_error = True raise finally: self.__class__.shared_state_initialized = True el<IF_STMT> raise unittest.SkipTest('Skipping test, failed to initialize state previously.')",if self.__class__.shared_state_in_error:
"def _get_replication_type_param(k, v): words = v.split() if len(words) == 2 and words[0] == '<in>': REPLICA_SYNC_TYPES = {'sync': constants.REPLICA_SYNC_MODEL, 'async': constants.REPLICA_ASYNC_MODEL} sync_type = words[1].lower() <IF_STMT> return REPLICA_SYNC_TYPES[sync_type] msg = _(""replication_type spec must be specified as replication_type='<in> sync' or '<in> async'."") LOG.error(msg) raise exception.InvalidInput(reason=msg)",if sync_type in REPLICA_SYNC_TYPES:
"def request(self, host, handler, request_body, verbose=False): for i in (0, 1): try: return self.single_request(host, handler, request_body, verbose) except socket.error as e: if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE): raise except http_client.BadStatusLine: <IF_STMT> raise",if i:
"def make_sales_return_records(): <IF_STMT> for data in frappe.get_all('Delivery Note', fields=['name'], filters={'docstatus': 1}): if random.random() < 0.1: try: dn = make_sales_return(data.name) dn.insert() dn.submit() frappe.db.commit() except Exception: frappe.db.rollback()",if random.random() < 0.1:
"def getStatusString(self): if not self._isAvailable: return 'Doodle3D box not found' if self._printing: <IF_STMT> ret = 'Sending GCode: %.1f%%' % (float(self._blockIndex) * 100.0 / float(len(self._fileBlocks))) elif len(self._fileBlocks) > 0: ret = 'Finished sending GCode to Doodle3D box.' else: ret = 'Different print still running...' return ret return 'Printer found, waiting for print command.'",if self._blockIndex < len(self._fileBlocks):
"def coro(*args, **kw): res = func(*args, **kw) if isinstance(res, futures.Future) or inspect.isgenerator(res): res = (yield from res) elif _AwaitableABC is not None: try: await_meth = res.__await__ except AttributeError: pass else: <IF_STMT> res = (yield from await_meth()) return res","if isinstance(res, _AwaitableABC):"
def _skip_to_next_iteration_group(self): while True: if self._currkey is self._marker: pass elif self._tgtkey is self._marker: break el<IF_STMT> break newvalue = next(self._iterator) if self._keyfunc is None: newkey = newvalue else: newkey = self._keyfunc(newvalue) self._currkey = newkey self._currvalue = newvalue,if not self._tgtkey == self._currkey:
def in_quadview(context): for area in context.window.screen.areas: <IF_STMT> continue for space in area.spaces: if space.type != 'VIEW_3D': continue if len(space.region_quadviews) > 0: return True return False,if area.type != 'VIEW_3D':
"def find_from_pythonpath(name): for dirpath in sys.path: <IF_STMT> continue path = os.path.join(dirpath, name) if os.path.isfile(path): return path return None",if not os.path.isdir(dirpath):
"def detailed_exceptions_wrapper(self, *args, **kwargs): try: return meth(self, *args, **kwargs) except ScriptError as e: info = e.args[0] <IF_STMT> raise info.setdefault('type', ScriptError.SPLASH_LUA_ERROR) info.setdefault('splash_method', _name) raise e","if not isinstance(info, dict):"
"def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema['properties'].items(): response = 'Test response' items = value['properties']['value'].get('items') enum = value['properties']['value'].get('enum') if items: response = [items['enum'][0]] elif enum: response = enum[0] <IF_STMT> response = {'question': {'value': 'Test Response'}} test_metadata[key] = {'value': response} return test_metadata",elif value['properties']['value'].get('properties'):
"def separate_keys(self, keys, torrent_ids): """"""Separates the input keys into torrent class keys and plugins keys"""""" if self.torrents: for torrent_id in torrent_ids: <IF_STMT> status_keys = list(self.torrents[torrent_id].status_funcs) leftover_keys = list(set(keys) - set(status_keys)) torrent_keys = list(set(keys) - set(leftover_keys)) return (torrent_keys, leftover_keys) return ([], [])",if torrent_id in self.torrents:
"def upgrade(): bind = op.get_bind() op.add_column('slices', sa.Column('datasource_id', sa.Integer())) session = db.Session(bind=bind) for slc in session.query(Slice).all(): if slc.druid_datasource_id: slc.datasource_id = slc.druid_datasource_id <IF_STMT> slc.datasource_id = slc.table_id session.merge(slc) session.commit() session.close()",if slc.table_id:
"def __call__(self, controller, environ, context): context.session = session = SessionObject(environ, **self.options) environ['beaker.session'] = session environ['beaker.get_session'] = self._get_session if 'paste.testing_variables' in environ: environ['paste.testing_variables']['session'] = session response = self.next_handler(controller, environ, context) if session.accessed(): session.persist() session_headers = session.__dict__['_headers'] if session_headers['set_cookie']: cookie = session_headers['cookie_out'] <IF_STMT> response.headers.extend((('Set-cookie', cookie),)) return response",if cookie:
"def propagate(self, user, change_action=None, author=None): """"""Propagate current translation to all others."""""" result = False for unit in self.same_source_units: <IF_STMT> continue if unit.target == self.target and unit.state == self.state: continue unit.target = self.target unit.state = self.state unit.save_backend(user, False, change_action=change_action, author=None, run_checks=False) result = True return result","if not user.has_perm('unit.edit', unit):"
"def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict['model'].items(): for model in value: <IF_STMT> model_meta = value[model] if model.endswith('Param'): model_param = value[model] LOGGER.info('load model') self.set_model_meta(model_meta) self.set_model_param(model_param) self.phi = np.array([model_param.phi_a])",if model.endswith('Meta'):
"def name(self): """"""Get the enumeration name of this storage class."""""" if self._name_map is None: self._name_map = {} for key, value in StorageClass.__dict__.items(): <IF_STMT> self._name_map[value] = key return self._name_map[self]","if isinstance(value, StorageClass):"
def relro(self): try: gnu_relro = lief.ELF.SEGMENT_TYPES.GNU_RELRO flags = lief.ELF.DYNAMIC_TAGS.FLAGS bind_now = lief.ELF.DYNAMIC_FLAGS.BIND_NOW if self.elf.get(gnu_relro): <IF_STMT> return 'Full RELRO' else: return 'Partial RELRO' return 'No RELRO' except lief.not_found: return 'No RELRO',if bind_now in self.elf.get(flags):
"def test_counter_instantiation(self): self.assertIs(type(typing_extensions.Counter()), collections.Counter) self.assertIs(type(typing_extensions.Counter[T]()), collections.Counter) self.assertIs(type(typing_extensions.Counter[int]()), collections.Counter)  class C(typing_extensions.Counter[T]): ... if TYPING_3_5_3: self.assertIs(type(C[int]()), C) <IF_STMT> self.assertEqual(C.__bases__, (typing_extensions.Counter,)) else: self.assertEqual(C.__bases__, (collections.Counter, typing.Generic))",if not PEP_560:
"def handle_exception(self, e, result): for k in sorted(result.thrift_spec): <IF_STMT> continue _, exc_name, exc_cls, _ = result.thrift_spec[k] if isinstance(e, exc_cls): setattr(result, exc_name, e) return True return False",if result.thrift_spec[k][1] == 'success':
"def find_from_pythonpath(name): for dirpath in sys.path: if not os.path.isdir(dirpath): continue path = os.path.join(dirpath, name) <IF_STMT> return path return None",if os.path.isfile(path):
"def parse_location(srclocation): loc = symbols.Location(get_value(srclocation, 'file'), get_value(srclocation, 'project')) <IF_STMT> loc = symbols.InstalledLocation(symbols.parse_package(get_value(srclocation, 'package')), parse_package_db(get_value(srclocation, 'db'))) if loc.is_null(): loc = symbols.OtherLocation(get_value(srclocation, 'source')) return loc if not loc.is_null() else None",if loc.is_null():
def execute(self): logger.debug(f'host {self.host} try ports: {default_ports}') for single_port in default_ports: <IF_STMT> logger.debug(f'Reachable port found: {single_port}') self.publish_event(OpenPortEvent(port=single_port)),"if self.test_connection(self.host, single_port):"
"def get_dynamic_incoming_outgoing_rate(self, sle): if sle.recalculate_rate: rate = self.get_incoming_outgoing_rate_from_transaction(sle) <IF_STMT> sle.incoming_rate = rate else: sle.outgoing_rate = rate",if flt(sle.actual_qty) >= 0:
"def _naf(mult): """"""Calculate non-adjacent form of number."""""" ret = [] while mult: <IF_STMT> nd = mult % 4 if nd >= 2: nd = nd - 4 ret += [nd] mult -= nd else: ret += [0] mult //= 2 return ret",if mult % 2:
"def indent_xml(elem, level=0): """"""Do our pretty printing and make Matt very happy."""""" i = '\n' + level * '  ' if elem: <IF_STMT> elem.text = i + '  ' if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i elif level and (not elem.tail or not elem.tail.strip()): elem.tail = i",if not elem.text or not elem.text.strip():
def clockface(radius): reset() pensize(7) for i in range(60): jump(radius) <IF_STMT> fd(25) jump(-radius - 25) else: dot(3) jump(-radius) rt(6),if i % 5 == 0:
"def OnTextEntered(self, evt): text = self.GetValue() if self.doSearch(text): self.searches.append(text) <IF_STMT> del self.searches[0] self.SetMenu(self.MakeMenu()) self.SetValue('')",if len(self.searches) > self.maxSearches:
"def wrapped_send(bot, location, content=None, preprocessor=None, **kwargs): try: <IF_STMT> content = await preprocessor(bot, location, content) await location.send(content, **kwargs) except Exception as _exc: main_log.error('I could not send an owner notification to %s (%s)', location, location.id, exc_info=_exc)",if preprocessor is not None:
"def explode(self, obj): """"""Determine if the object should be exploded."""""" if obj in self._done: return False result = False for item in self._explode: <IF_STMT> if obj._moId == item._moId: result = True elif obj.__class__.__name__ == item.__name__: result = True if result: self._done.add(obj) return result","if hasattr(item, '_moId'):"
"def _verify_treestore(itr, tree_values): i = 0 while itr: values = tree_values[i] if treestore[itr][0] != values[0]: return False <IF_STMT> if not _verify_treestore(treestore.iter_children(itr), values[1]): return False itr = treestore.iter_next(itr) i += 1 return True",if treestore.iter_children(itr):
"def types(model_cls): attr_name = '{0}_types'.format(model_cls.__name__.lower()) types = {} for plugin in find_plugins(): plugin_types = getattr(plugin, attr_name, {}) for field in plugin_types: <IF_STMT> raise PluginConflictException(u'Plugin {0} defines flexible field {1} which has already been defined with another type.'.format(plugin.name, field)) types.update(plugin_types) return types",if field in types and plugin_types[field] != types[field]:
"def set_origin(self, origin): if self.origin is None: <IF_STMT> origin = origin.origin if not isinstance(origin, patsy.origin.Origin): origin = None self.origin = origin","if hasattr(origin, 'origin'):"
def items(self): if self._items is not None: return self._items items = self.get_option('recent-connections') if not items: self._items = [] return self._items for i in reversed(items): <IF_STMT> items.remove(i) try: i['device'] = self.get_device_path(i) except AdapterNotFound: i['device'] = None except DeviceNotFound: items.remove(i) i['time'] = float(i['time']) self._items = items return self._items,if 'name' not in i or 'uuid' not in i:
"def test_doc_attributes(self): print_test_name('TEST DOC ATTRIBUTES') correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) if original_schema.doc is not None: correct += 1 if original_schema.type == 'record': for f in original_schema.fields: <IF_STMT> self.fail(""Failed to preserve 'doc' in fields: "" + example.schema_string) self.assertEqual(correct, len(DOC_EXAMPLES))",if f.doc is None:
"def StopBackgroundWorkload(self): """"""Stop the background workoad."""""" for workload in background_workload.BACKGROUND_WORKLOADS: <IF_STMT> if self.OS_TYPE in workload.EXCLUDED_OS_TYPES: raise NotImplementedError() workload.Stop(self)",if workload.IsEnabled(self):
"def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): resolved = super(SearchQuery, self).resolve_expression(query, allow_joins, reuse, summarize, for_save) if self.config: <IF_STMT> resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save) else: resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save) return resolved","if not hasattr(self.config, 'resolve_expression'):"
"def resolve_ip(filename, foffset, ip, need_line): sym, soffset, line = (None, 0, None) if filename and filename.startswith('/'): sym, soffset = resolve_sym(filename, foffset) if not sym: sym, soffset = resolve_sym(filename, ip) <IF_STMT> line = resolve_line(filename, ip) else: sym, soffset = kernel.resolve_kernel(ip) return (sym, soffset, line)",if need_line:
"def create_model(self, dataset, weight_name=Checkpoint._LATEST): if not self.is_empty: run_config = copy.deepcopy(self._checkpoint.run_config) model = instantiate_model(run_config, dataset) <IF_STMT> for k, v in self._checkpoint.model_props.items(): setattr(model, k, v) delattr(self._checkpoint, 'model_props') self._initialize_model(model, weight_name) return model else: raise ValueError('Checkpoint is empty')","if hasattr(self._checkpoint, 'model_props'):"
"def get_py2exe_datafiles(): datapath = get_data_path() head, tail = os.path.split(datapath) d = {} for root, dirs, files in os.walk(datapath): <IF_STMT> files.remove('Matplotlib.nib') files = [os.path.join(root, filename) for filename in files] root = root.replace(tail, 'mpl-data') root = root[root.index('mpl-data'):] d[root] = files return d.items()",if 'Matplotlib.nib' in files:
"def mouseClickEvent(self, ev): if ev.button() == QtCore.Qt.LeftButton and self.allowAdd: pos = ev.pos() if pos.x() < 0 or pos.x() > self.length: return <IF_STMT> return pos.setX(min(max(pos.x(), 0), self.length)) self.addTick(pos.x() / self.length) elif ev.button() == QtCore.Qt.RightButton: self.showMenu(ev)",if pos.y() < 0 or pos.y() > self.tickSize:
"def image_preprocess(self, image): with tf.name_scope('image_preprocess'): <IF_STMT> image = tf.cast(image, tf.float32) mean = [0.485, 0.456, 0.406] std = [0.229, 0.224, 0.225] if self.image_bgr: mean = mean[::-1] std = std[::-1] image_mean = tf.constant(mean, dtype=tf.float32) * 255.0 image_std = tf.constant(std, dtype=tf.float32) * 255.0 image = (image - image_mean) / image_std return image",if image.dtype.base_dtype != tf.float32:
"def _addConsoleMessage(self, type: str, args: List[JSHandle]) -> None: if not self.listeners(Page.Events.Console): for arg in args: self._client._loop.create_task(arg.dispose()) return textTokens = [] for arg in args: remoteObject = arg._remoteObject <IF_STMT> textTokens.append(arg.toString()) else: textTokens.append(str(helper.valueFromRemoteObject(remoteObject))) message = ConsoleMessage(type, ' '.join(textTokens), args) self.emit(Page.Events.Console, message)",if remoteObject.get('objectId'):
"def _handle_guild_scalar(self, add_scalar, _tag, _value, step=None): """"""Handler for guild.summary.SummaryWriter.add_scalar."""""" vals = self._summary_values(step) if vals: self.log.debug('summary values via add_scalar: %s', vals) for tag, val in vals.items(): <IF_STMT> add_scalar(tag, val, step)",if val is not None:
"def _get_token_from_cookie(self): for cookie in self.session.cookies: if cookie.name == 'X-APPLE-WEBAUTH-VALIDATE': match = search('\\bt=([^:]+)', cookie.value) <IF_STMT> raise Exception(""Can't extract token from %r"" % cookie.value) return {'token': match.group(1)} raise Exception('Token cookie not found')",if match is None:
"def unpack_RK(rk_str): flags = BYTES_ORD(rk_str[0]) if flags & 2: i, = unpack('<i', rk_str) i >>= 2 <IF_STMT> return i / 100.0 return float(i) else: d, = unpack('<d', b'\x00\x00\x00\x00' + BYTES_LITERAL(chr(flags & 252)) + rk_str[1:4]) if flags & 1: return d / 100.0 return d",if flags & 1:
"def _parse_photo(self): cat = 'lib' for photosection in self.plex.library.sections(): <IF_STMT> self._load_attrs(photosection, cat) for photoalbum in photosection.all(): self._load_attrs(photoalbum, cat) for photo in photoalbum.photos(): self._load_attrs(photo, cat)",if photosection.TYPE == library.PhotoSection.TYPE:
def count(num): cnt = 0 for i in range(num): try: <IF_STMT> raise ValueError if i % 3: raise ArithmeticError('1') except Exception as e: cnt += 1 return cnt,if i % 2:
"def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: <IF_STMT> jid = self.xmpp.boundjid.full if node is None: node = '' if ifrom is None: ifrom = '' if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True",if jid is None:
"def __call__(self, environ, start_response): script_name = environ.get('HTTP_X_SCRIPT_NAME') if script_name is not None: <IF_STMT> warnings.warn(""'X-Script-Name' header should not end in '/' (found: %r). Please fix your proxy's configuration."" % script_name) script_name = script_name.rstrip('/') environ['SCRIPT_NAME'] = script_name return super(ProxyFix, self).__call__(environ, start_response)",if script_name.endswith('/'):
"def backwardKillParagraph(self, event): """"""Kill the previous paragraph."""""" c = self.c w = self.editWidget(event) if not w: return self.beginCommand(w, undoType='backward-kill-paragraph') try: self.backwardParagraphHelper(event, extend=True) i, j = w.getSelectionRange() <IF_STMT> i = min(i + 1, j) c.killBufferCommands.kill(event, i, j, force=True, undoType=None) w.setSelectionRange(i, i, insert=i) finally: self.endCommand(changed=True, setLabel=True)",if i > 0:
"def bracket_replace(code): new = '' for e in bracket_split(code, ['()', '[]'], False): if e[0] == '[': name = '#PYJSREPL' + str(len(REPL)) + '{' new += name REPL[name] = e <IF_STMT> name = '@PYJSREPL' + str(len(REPL)) + '}' new += name REPL[name] = e else: new += e return new",elif e[0] == '(':
"def regenerate(self, request, **kwargs): obj = self.get_object() if 'all' in request.data: for user in User.objects.all(): <IF_STMT> token = Token.objects.get(user=user) token.delete() Token.objects.create(user=user) return Response('') if 'username' in request.data: obj = get_object_or_404(User, username=request.data['username']) self.check_object_permissions(self.request, obj) token = Token.objects.get(user=obj) token.delete() token = Token.objects.create(user=obj) return Response({'token': token.key})",if not user.is_anonymous():
"def signal_notebook_switch_page(self, notebook, current_page, index): if not hasattr(self.parent, 'rpc'): return self.last_page_id = index for tab in self.tabs.values(): <IF_STMT> continue if hasattr(tab, 'load_campaign_information'): tab.load_campaign_information(force=False)",if current_page != tab.box:
"def get_word_parens_range(self, offset, opening='(', closing=')'): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): <IF_STMT> open_count += 1 if self.code[index] == closing: open_count -= 1 if open_count == 0: return (start_parens, index + 1) index += 1 return (start_parens, index)",if self.code[index] == opening:
"def append(self, child): if child not in (None, self): tag = child_tag(self._tag) if tag: if isinstance(child, Html): if child.tag != tag: child = Html(tag, child) <IF_STMT> child = Html(tag, child) super().append(child)",elif not child.startswith('<%s' % tag):
"def cvPreprocess(): import cv2 imgarr_orig = [] image_ext_list = ['.jpg', '.png', '.JPEG', '.jpeg', '.PNG', '.JPG'] for file in onlyfiles: fimg = imgroot + file <IF_STMT> print(fimg + ' is not an image file') continue img1 = cv2.imread(fimg) if img1 is None: print('ERROR opening ', fimg) continue img1 = cv2.resize(img1, (896, 896)) imgarr_orig.append(img1) return imgarr_orig",if any([x in image_ext_list for x in fimg]):
"def replace_nodes_in_symbol_table(symbols: SymbolTable, replacements: Dict[SymbolNode, SymbolNode]) -> None: for name, node in symbols.items(): <IF_STMT> if node.node in replacements: new = replacements[node.node] old = node.node replace_object_state(new, old) node.node = new if isinstance(node.node, (Var, TypeAlias)): node.node.accept(NodeReplaceVisitor(replacements))",if node.node:
"def __find_audio_offset(self, fileobj): byte = 0 while not byte & 128: byte = ord(fileobj.read(1)) size = to_int_be(fileobj.read(3)) try: block_type = self.METADATA_BLOCKS[byte & 127] except IndexError: block_type = None <IF_STMT> block_type(fileobj) else: fileobj.read(size) return fileobj.tell()",if block_type and block_type._distrust_size:
"def startJail(self, name): with self.__lock: jail = self.__jails[name] <IF_STMT> jail.start() elif name in self.__reload_state: logSys.info('Jail %r reloaded', name) del self.__reload_state[name] if jail.idle: jail.idle = False",if not jail.isAlive():
def get_resolved_dependencies(self): dependencies = [] for dependency in self.envconfig.deps: <IF_STMT> package = resolve_package(package_spec=dependency.name) if package != dependency.name: dependency = dependency.__class__(package) dependencies.append(dependency) return dependencies,if dependency.indexserver is None:
"def _compile(self): if not self._compiled: <IF_STMT> return try: self._tokens = boolExpression.parseString(self._query, parseAll=self.strict) except ParseException: raise self._compiled = True",if self._is_match_all():
"def _compute_features(self, images): output_blobs = self._forward(images) features = [] for blob in output_blobs: blob = blob.reshape((blob.shape[0], blob.shape[1])) <IF_STMT> blob = blob.max(0) else: blob = self.merge(blob) features.append(blob) return np.vstack(features)",if self.merge == 'max':
def _list_shape_iter(shape): last_shape = _void for item in shape: <IF_STMT> if last_shape is _void: raise ValueError('invalid shape spec: Ellipsis cannot be thefirst element') while True: yield last_shape last_shape = item yield item,if item is Ellipsis:
"def tokenize_url(self, field): field = field.strip() tokens = field.split(':') offset = 0 if tokens[0] == 'http': offset = 1 dstport = 80 <IF_STMT> inttokens = tokens[2].split('/') dstport = int(inttokens[0]) elif tokens[0] == 'https': dstport = 443 elif tokens[-1] is not None: dstport = int(tokens[-1]) tld = tldextract.extract(tokens[offset]) fqdn = '.'.join((part for part in tld if part)) return (fqdn, dstport)",if len(tokens) > 2:
"def assert_summary_equals(self, records, tag, step, value): for record in records[1:]: <IF_STMT> continue if record.step != step: continue self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor)) return self.fail('Could not find record for tag {} and step {}'.format(tag, step))",if record.summary.value[0].tag != tag:
"def getAttrDefault(key, fallback=None): try: default = defaultValuesCache[key] except KeyError: attrInfo = getAttributeInfo(key) <IF_STMT> default = defaultValuesCache[key] = None else: default = defaultValuesCache[key] = attrInfo.defaultValue if default is None: default = fallback return default",if attrInfo is None:
"def __getattr__(self, key): if key in self._raw: val = self._raw[key] if key in ('date',): return pd.Timestamp(val) elif key in ('open', 'close'): return pd.Timestamp(val).time() <IF_STMT> return pd.Timestamp(val[:2] + ':' + val[-2:]).time() else: return val return super().__getattr__(key)","elif key in ('session_open', 'session_close'):"
"def _combine_to_jointcaller(processed): """"""Add joint calling information to variants, while collapsing independent regions."""""" by_vrn_file = collections.OrderedDict() for data in (x[0] for x in processed): key = (tz.get_in(('config', 'algorithm', 'jointcaller'), data), data['vrn_file']) <IF_STMT> by_vrn_file[key] = [] by_vrn_file[key].append(data) out = [] for grouped_data in by_vrn_file.values(): cur = grouped_data[0] out.append([cur]) return out",if key not in by_vrn_file:
"def assign_type(self, wb_type): if isinstance(wb_type, ListType): assigned_type = self.params['element_type'].assign_type(wb_type.params['element_type']) <IF_STMT> return ListType(assigned_type) return InvalidType()","if not isinstance(assigned_type, InvalidType):"
"def set_billing_hours_and_amount(self): if not self.project: for timesheet in self.timesheets: ts_doc = frappe.get_doc('Timesheet', timesheet.time_sheet) if not timesheet.billing_hours and ts_doc.total_billable_hours: timesheet.billing_hours = ts_doc.total_billable_hours <IF_STMT> timesheet.billing_amount = ts_doc.total_billable_amount",if not timesheet.billing_amount and ts_doc.total_billable_amount:
"def add_changeset(repo_path, path_to_filename_in_archive): try: subprocess.check_output(['hg', 'add', path_to_filename_in_archive], stderr=subprocess.STDOUT, cwd=repo_path) except Exception as e: error_message = ""Error adding '{}' to repository: {}"".format(path_to_filename_in_archive, unicodify(e)) <IF_STMT> error_message += '\nOutput was:\n%s' % unicodify(e.output) raise Exception(error_message)","if isinstance(e, subprocess.CalledProcessError):"
"def full_path(self, *args, **query): """"""Return a full path"""""" path = None if args: <IF_STMT> raise TypeError('full_url() takes exactly 1 argument (%s given)' % len(args)) path = args[0] if not path: path = self.path elif not path.startswith('/'): path = remove_double_slash('%s/%s' % (self.path, path)) return iri_to_uri(path, query)",if len(args) > 1:
"def retry_http_basic_auth(self, host, req, realm): user, pw = self.passwd.find_user_password(realm, host) if pw is not None: raw = '%s:%s' % (user, pw) auth = 'Basic %s' % base64.b64encode(raw).strip() <IF_STMT> return None req.add_unredirected_header(self.auth_header, auth) return self.parent.open(req, timeout=req.timeout) else: return None","if req.get_header(self.auth_header, None) == auth:"
"def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: <IF_STMT> continue item = data[key] if torch.is_tensor(item) and num_points == item.shape[0]: item = item[self._indices].clone() elif torch.is_tensor(item): item = item.clone() setattr(new_data, key, item) return new_data",if key == KDTREE_KEY:
def flat(tree): stack = [tree] result = [] stack_pop = stack.pop stack_extend = stack.extend result_append = result.append while stack: x = stack_pop() <IF_STMT> result_append(x) else: try: stack_extend(x) except TypeError: result_append(x) return result[::-1],"if isinstance(x, basestring):"
"def do_remove(self): if self.netconf.locked('dhcp'): <IF_STMT> pid = read_pid_file('/var/run/dnsmasq.pan1.pid') else: pid = self.pid if not kill(pid, 'dnsmasq'): logging.info('Stale dhcp lockfile found') self.netconf.unlock('dhcp')",if not self.pid:
"def set_xticklabels(self, labels=None, step=None, **kwargs): """"""Set x axis tick labels on the bottom row of the grid."""""" for ax in self.axes[-1, :]: <IF_STMT> labels = [l.get_text() for l in ax.get_xticklabels()] if step is not None: xticks = ax.get_xticks()[::step] labels = labels[::step] ax.set_xticks(xticks) ax.set_xticklabels(labels, **kwargs) return self",if labels is None:
"def _resolved_values(self): values = [] for k, v in self.values.items() if hasattr(self.values, 'items') else self.values: if self.mapper: <IF_STMT> desc = _entity_descriptor(self.mapper, k) values.extend(desc._bulk_update_tuples(v)) elif isinstance(k, attributes.QueryableAttribute): values.extend(k._bulk_update_tuples(v)) else: values.append((k, v)) else: values.append((k, v)) return values","if isinstance(k, util.string_types):"
"def _print_handles(self, text, handle_list): for handle in handle_list: source, citation = self.get_source_or_citation(handle, False) _LOG.debug('\n\n\n') <IF_STMT> _LOG.debug('---- %s -- source %s' % (text, source.get_title())) elif citation: _LOG.debug('---- %s -- citation %s' % (text, citation.get_page())) else: _LOG.debug('---- %s -- handle %s' % (text, handle))",if source:
"def test_items(self): expectException = len(self.sparse_data) < len(self.data) and (not self.instance.A._default_val is None) try: test = self.instance.A.items() <IF_STMT> self.validateDict(self.sparse_data.items(), test) else: self.validateDict(self.data.items(), test) except ValueError: if not expectException: raise",if self.instance.A._default_val is None:
"def __new__(cls, name, bases, d): rv = type.__new__(cls, name, bases, d) if 'methods' not in d: methods = set(rv.methods or []) for key, value in d.iteritems(): if key in http_method_funcs: methods.add(key.upper()) <IF_STMT> rv.methods = sorted(methods) return rv",if methods:
def getResultSummary(self): if self.descriptionDone is not None or self.description is not None: stepsumm = util.join_list(self.descriptionDone or self.description) <IF_STMT> stepsumm += u' ' + util.join_list(self.descriptionSuffix) else: stepsumm = u'finished' if self.results != SUCCESS: stepsumm += u' (%s)' % Results[self.results] return {u'step': stepsumm},if self.descriptionSuffix:
"def analyze_items(items, category_id, agg_data): for item in items: if not agg_data['cat_asp'].get(category_id, None): agg_data['cat_asp'][category_id] = [] agg_data['cat_asp'][category_id].append(float(item.sellingStatus.currentPrice.value)) if getattr(item.listingInfo, 'watchCount', None): agg_data['watch_count'] += int(item.listingInfo.watchCount) <IF_STMT> agg_data['postal_code'] = item.postalCode","if getattr(item, 'postalCode', None):"
"def _Determine_Do(self): from os.path import join self.applicable = 1 siloedPythonInstallDir = black.configure.items['siloedPythonInstallDir'].Get() if sys.platform == 'darwin': siloedPyVer = black.configure.items['siloedPyVer'].Get() self.value = join(siloedPythonInstallDir, 'Python.framework', 'Versions', siloedPyVer, 'bin') else: self.value = siloedPythonInstallDir <IF_STMT> self.value = join(self.value, 'bin') self.determined = 1",if sys.platform != 'win32':
"def work(self): idle_times = 0 while True: <IF_STMT> log.info('Stop sync worker') break try: job = self.commit_queue.get(timeout=self.timeout, block=True) if job['type'] == 'commit': self.commits.append(job) log.debug('Got a commit job') idle_times = 0 idle.clear() except Empty: log.debug('Nothing to do right now, going idle') if idle_times > self.min_idle_times: idle.set() idle_times += 1 self.on_idle()",if shutting_down.is_set():
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def expand_group(client: Any, group_key: str): """"""Determines if an email is really a dl."""""" try: response = list_members(client, group_key, propagate_errors=True) if response.get('members'): return [x['email'] for x in response.get('members', [])] except HttpError as e: <IF_STMT> pass return []",if e.resp.status == 404:
"def validate_against_domain(cls, ensemble: Optional['PolicyEnsemble'], domain: Optional[Domain]) -> None: if ensemble is None: return for p in ensemble.policies: if not isinstance(p, TwoStageFallbackPolicy): continue <IF_STMT> raise InvalidDomain(""The intent '{0}' must be present in the domain file to use TwoStageFallbackPolicy. Either include the intent '{0}' in your domain or exclude the TwoStageFallbackPolicy from your policy configuration"".format(p.deny_suggestion_intent_name))",if domain is None or p.deny_suggestion_intent_name not in domain.intents:
"def _ndvi(nir_data, red_data): out = np.zeros_like(nir_data) rows, cols = nir_data.shape for y in range(0, rows): for x in range(0, cols): nir = nir_data[y, x] red = red_data[y, x] <IF_STMT> continue soma = nir + red out[y, x] = (nir - red) / soma return out",if nir == red:
"def sysroot(): cmd = 'set sysroot remote:/' if is_android(): <IF_STMT> gdb.execute(cmd) else: print(message.notice('sysroot is already set, skipping %r' % cmd))",if gdb.parameter('sysroot') == 'target:':
"def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() if now - when_pressed > self._debounce_time: if GPIO.input(self._channel) == self._expected: <IF_STMT> pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) elif pressed: pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)",if not pressed:
"def find_comment(line): """"""Finds the index of a comment # and returns None if not found"""""" instring, instring_char = (False, '') for i, char in enumerate(line): if char in ('""', ""'""): if instring: if char == instring_char: instring = False instring_char = '' else: instring = True instring_char = char <IF_STMT> if not instring: return i return None",elif char == '#':
"def _deduplicate_data(self): dup_lines = [] hash_set = set() for i, fields in enumerate(self.data): fields_hash = hash(self.separator.join(fields)) <IF_STMT> dup_lines.append(i) log.debug('Found duplicate entry in tool data table ""%s"", but duplicates are not allowed, removing additional entry for: ""%s""', self.name, fields) else: hash_set.add(fields_hash) for i in reversed(dup_lines): self.data.pop(i)",if fields_hash in hash_set:
"def sample_independent(self, study: Study, trial: FrozenTrial, param_name: str, param_distribution: distributions.BaseDistribution) -> Any: self._raise_error_if_multi_objective(study) if self._warn_independent_sampling: complete_trials = self._get_trials(study) <IF_STMT> self._log_independent_sampling(trial, param_name) return self._independent_sampler.sample_independent(study, trial, param_name, param_distribution)",if len(complete_trials) >= self._n_startup_trials:
"def publish(self): """"""Publish new events to the subscribers."""""" while True: event = await self.event_source.get() str_buffer = [] <IF_STMT> return if isinstance(event, str): str_buffer.append(event) elif event.type == EventTypes.BLOCK_VALID: str_buffer = map(json.dumps, eventify_block(event.data)) for str_item in str_buffer: for _, websocket in self.subscribers.items(): await websocket.send_str(str_item)",if event == POISON_PILL:
"def push(self): advice = self.check() if not self._context['silent']: if not self.hasPendingSync(advice): print('No changes to push.') return choice = input('Continue? y/N:') <IF_STMT> print('Aborted on user command') return print('push local changes to remote...') self._publish.syncRemote(self._context['srcroot'], advice)",if choice != 'y':
"def readline(self, limit=-1): i = self._rbuf.find('\n') while i < 0 and (not 0 < limit <= len(self._rbuf)): new = self._raw_read(self._rbufsize) if not new: break i = new.find('\n') <IF_STMT> i += len(self._rbuf) self._rbuf = self._rbuf + new if i < 0: i = len(self._rbuf) else: i += 1 if 0 <= limit < len(self._rbuf): i = limit data, self._rbuf = (self._rbuf[:i], self._rbuf[i:]) return data",if i >= 0:
"def main(): init_app(set_backends=True, routes=False) dry_run = '--dry' in sys.argv if not dry_run: script_utils.add_file_logger(logger, __file__) with transaction.atomic(): normalize_source_tags() add_claimed_tags() add_osf_provider_tags() add_prereg_campaign_tags() <IF_STMT> raise RuntimeError('Dry run, transaction rolled back')",if dry_run:
"def iter_segments(self): while not self.closed: for chunk in filter(self.valid_chunk, self.chunks): self.logger.debug('Adding chunk {0} to queue', chunk.num) yield chunk <IF_STMT> return self.chunk_id = chunk.num + 1 if self.wait(self.module_info_reload_time): try: self.process_module_info() except StreamError as err: self.logger.warning('Failed to process module info: {0}', err)",if self.closed:
"def SetItems(self, choices): self.choices = choices self.choice_names = self.get_choice_names() self.list_dlg.SetItems(self.get_choice_labels()) labels = self.get_choice_labels() for i in range(len(self.choices)): if self.choices[i][1] is None: self.list_dlg.SetItemBackgroundColour(i, 'pink') <IF_STMT> self.list_dlg.SetItemForegroundColour(i, 'grey') self.SetChecked(self.checked) self.Refresh()",elif labels[i].endswith('Name!)'):
"def combine_logs(audit_logs, statement_text_logs): for audit_transaction in audit_logs: for audit_query in audit_logs[audit_transaction]: matching_statement_text_logs = statement_text_logs.get(hash(audit_query)) if matching_statement_text_logs: statement_text_log = matching_statement_text_logs.pop() if statement_text_log: <IF_STMT> audit_query.start_time = statement_text_log.start_time if statement_text_log.end_time: audit_query.end_time = statement_text_log.end_time",if statement_text_log.start_time:
"def handle_data(self, data): if self.in_span or self.in_div: if data == 'No such user (please note that login is case sensitive)': self.no_user = True <IF_STMT> self.bad_pw = True elif data == 'User with that email already exists': self.already_exists = True",elif data == 'Invalid password':
"def K(exp): """"""Helper function to specify keymap"""""" import re modifier_strs = [] while True: m = re.match('\\A(C|Ctrl|M|Alt|Shift|Super|Win)-', exp) <IF_STMT> break modifier = m.group(1) modifier_strs.append(modifier) exp = re.sub('\\A{}-'.format(modifier), '', exp) key_str = exp.upper() key = getattr(Key, key_str) return Combo(create_modifiers_from_strings(modifier_strs), key)",if m is None:
"def local_min(self, hmap): rows = len(hmap) cols = len(hmap[0]) min_list = [] for row in range(rows): for col in range(cols): for d_row, d_col in ((1, 0), (0, 1), (-1, 0), (0, -1)): h_row = (row + d_row) % rows h_col = (col + d_col) % cols <IF_STMT> break else: min_list.append((row, col)) return min_list",if hmap[h_row][h_col] < hmap[row][col]:
"def _check_processing(self): now = time.time() self.mutex.acquire() while self.processing.qsize() and self.processing.top and (self.processing.top.exetime < now): task = self.processing.get_nowait() <IF_STMT> continue task.exetime = 0 self.priority_queue.put(task) logger.info('processing: retry %s', task.taskid) self.mutex.release()",if task.taskid is None:
"def autoname(self): naming_method = frappe.db.get_value('HR Settings', None, 'emp_created_by') if not naming_method: throw(_('Please setup Employee Naming System in Human Resource > HR Settings')) elif naming_method == 'Naming Series': set_name_by_naming_series(self) <IF_STMT> self.name = self.employee_number elif naming_method == 'Full Name': self.set_employee_name() self.name = self.employee_name self.employee = self.name",elif naming_method == 'Employee Number':
"def __fixdict(self, dict): for key in dict.keys(): if key[:6] == 'start_': tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = (getattr(self, key), end) elif key[:4] == 'end_': tag = key[4:] start, end = self.elements.get(tag, (None, None)) <IF_STMT> self.elements[tag] = (start, getattr(self, key))",if end is None:
"def parseAGL(filename): m = {} for line in readLines(filename): line = line.strip() <IF_STMT> name, uc = tuple([c.strip() for c in line.split(';')]) if uc.find(' ') == -1: m[int(uc, 16)] = name return m",if len(line) > 0 and line[0] != '#':
"def password(self, password): self._password = password if password: <IF_STMT> self.eeprint('Install sshpass to using password: https://duckduckgo.com/?q=install+sshpass\n' + 'Note! There are a lot of security reasons to stop using password auth.') verbose = '-v' if '-v' in self.sshpass else [] self.sshpass = ['sshpass', '-p', password] + verbose else: self.sshpass = []",if not which('sshpass'):
"def test_region_redirects_multiple_requests(self): try: response = self.client.list_objects(Bucket=self.bucket_name) self.assertEqual(response['ResponseMetadata']['HTTPStatusCode'], 200) second_response = self.client.list_objects(Bucket=self.bucket_name) self.assertEqual(second_response['ResponseMetadata']['HTTPStatusCode'], 200) except ClientError as e: error = e.response['Error'].get('Code', None) <IF_STMT> self.fail('S3 client failed to redirect to the proper region.')",if error == 'PermanentRedirect':
"def get_action_type(action_space): """"""Method to get the action type to choose prob. dist. to sample actions from NN logits output"""""" if isinstance(action_space, spaces.Box): shape = action_space.shape assert len(shape) == 1 <IF_STMT> return 'continuous' else: return 'multi_continuous' elif isinstance(action_space, spaces.Discrete): return 'discrete' elif isinstance(action_space, spaces.MultiDiscrete): return 'multi_discrete' elif isinstance(action_space, spaces.MultiBinary): return 'multi_binary' else: raise NotImplementedError",if shape[0] == 1:
def remove_stale_sockets(self): with self.lock: <IF_STMT> for sock_info in self.sockets.copy(): age = _time() - sock_info.last_checkout if age > self.opts.max_idle_time_ms: self.sockets.remove(sock_info) sock_info.close() while len(self.sockets) + self.active_sockets < self.opts.min_pool_size: sock_info = self.connect() with self.lock: self.sockets.add(sock_info),if self.opts.max_idle_time_ms is not None:
"def _setReadyState(self, state: str) -> None: if state != self.__readyState: self.__log_debug('- %s -> %s', self.__readyState, state) self.__readyState = state if state == 'open': self.emit('open') <IF_STMT> self.emit('close') self.remove_all_listeners()",elif state == 'closed':
"def currentLevel(self): currentStr = '' for stackType, stackValue in self.stackVals: <IF_STMT> if isinstance(stackValue, str): currentStr += ""['"" + stackValue + ""']"" else: currentStr += '[' + str(stackValue) + ']' elif stackType == 'listLike': currentStr += '[' + str(stackValue) + ']' elif stackType == 'getattr': currentStr += "".__getattribute__('"" + stackValue + ""')"" else: raise Exception(f'Cannot get attribute of type {stackType}') return currentStr",if stackType == 'dict':
def filter_latest_pkgs(pkgs): pkgname2latest = {} for x in pkgs: pkgname = core.normalize_pkgname(x.pkgname) <IF_STMT> pkgname2latest[pkgname] = x elif x.parsed_version > pkgname2latest[pkgname].parsed_version: pkgname2latest[pkgname] = x return pkgname2latest.values(),if pkgname not in pkgname2latest:
"def test_url_invalid_set(): for line in URL_INVALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) <IF_STMT> continue mbox = address.parse(line, strict=True) assert_equal(mbox, None)",if match:
"def check_block(cls, block): if len(block) == 4 and block[0][0] and (block[0][0][0] == '@') and block[2][0] and (block[2][0][0] == '+') and block[1][0]: match = cls.bases_regexp.match(block[1][0]) <IF_STMT> start, end = match.span() if end - start == len(block[1][0]): return True return False",if match:
"def load_from_file(self, filename): self._filename = filename if os.path.exists(filename): <IF_STMT> raise IOError('%s exists and is not a file' % filename) with open(filename, 'r') as f: self._properties = json.load(f) else: mkpath(os.path.dirname(filename)) self.save_to_file()",if not os.path.isfile(filename):
def add_system_info_creds_to_config(creds): for user in creds: ConfigService.creds_add_username(creds[user]['username']) if 'password' in creds[user] and creds[user]['password']: ConfigService.creds_add_password(creds[user]['password']) <IF_STMT> ConfigService.creds_add_lm_hash(creds[user]['lm_hash']) if 'ntlm_hash' in creds[user] and creds[user]['ntlm_hash']: ConfigService.creds_add_ntlm_hash(creds[user]['ntlm_hash']),if 'lm_hash' in creds[user] and creds[user]['lm_hash']:
"def line_number(self): if self._line_range: line_range = self._line_range <IF_STMT> return '%03d-%03d' % (line_range.start, line_range.stop - 1) else: return '%03d' % line_range.start",if line_range.stop - line_range.start > 1:
"def smooth(self, y, x=None, weights=None): if self.method == 'target_df': <IF_STMT> self.fit(y, x=x, weights=weights, pen=self.pen) else: self.fit_target_df(y, x=x, weights=weights, df=self.target_df) elif self.method == 'optimize_gcv': self.fit_optimize_gcv(y, x=x, weights=weights)","if hasattr(self, 'pen'):"
"def dict_from_cursor(data=None, keys=None): filtered_dict = {} data = bson_dumps(data) python_dict = json.loads(data) for key in keys: value = python_dict.get(key) if type(value) is dict: mongo_id = value.get('$oid') <IF_STMT> value = mongo_id if key == '_id': key = 'id' filtered_dict[key] = value return filtered_dict",if mongo_id:
"def pytest_plugin_registered(self, plugin): nodeid = None try: p = py.path.local(plugin.__file__) except AttributeError: pass else: if p.basename.startswith('conftest.py'): nodeid = p.dirpath().relto(self.config.rootdir) <IF_STMT> nodeid = nodeid.replace(p.sep, '/') self.parsefactories(plugin, nodeid)",if p.sep != '/':
"def _escape_unsafe_values(self, *values): """"""Escape unsafe values (name, section name) for API version 2.10 and below"""""" for value in values: <IF_STMT> yield value else: self.task.log.info(""Converting unsafe hyper parameter name/section '{}' to '{}'"".format(value, '_' + value)) yield ('_' + value)",if value not in UNSAFE_NAMES_2_10:
"def _identifier_split(self, identifier): """"""Return (name, start, end) string tuple from an identifier (PRIVATE)."""""" if '/' in identifier: name, start_end = identifier.rsplit('/', 1) <IF_STMT> try: start, end = start_end.split('-') return (name, int(start), int(end)) except ValueError: pass return (identifier, None, None)",if start_end.count('-') == 1:
"def _complete_initial_layout(self): """"""Finish initial layout; called after toplevel win is positioned"""""" for paned in self._get_paneds(): <IF_STMT> pos = paned._initial_divider_position GLib.idle_add(paned.set_position, pos)",if paned._initial_divider_position:
"def _init_mapping(self, result): for wamp_uri, full_name in result.items(): for prefix in self.PREFIXES: <IF_STMT> continue short_name = full_name[len(prefix):] self._mapping[short_name] = wamp_uri",if not full_name.startswith(prefix):
"def get_bounce_message(reason, ses_data, details): if reason != 'bounce': return if ses_data: bouncedRecipients = ses_data.get('bounce', {}).get('bouncedRecipients') <IF_STMT> recipient = bouncedRecipients[0] return recipient.get('diagnosticCode') or recipient.get('status') elif details: return details",if bouncedRecipients:
"def do_If(self, node, elif_flag=False): self.div('statement') self.keyword('elif' if elif_flag else 'if') self.visit(node.test) self.colon() self.div_body(node.body) if node.orelse: node1 = node.orelse[0] <IF_STMT> self.do_If(node1, elif_flag=True) else: self.keyword('else') self.colon() self.div_body(node.orelse) self.end_div('statement')","if isinstance(node1, ast.If) and len(node.orelse) == 1:"
"def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) if not match and parent_path != '': if len(pattern.dirs) <= len(parent_path_dirs): match = pattern.match(os.path.sep.join(parent_path_dirs[:len(pattern.dirs)])) <IF_STMT> matched = not negative return matched",if match:
def test_11_wait_for_first_reboot_with_bhyve(): if update_version is None: pytest.skip('No update found') elif download_failed is True: pytest.skip(f'Downloading {selected_trains} failed') elif reboot is False: pytest.skip('Reboot is False skip') el<IF_STMT> pytest.skip('skip no vm_name') else: while vm_state(vm_name) != 'stopped': sleep(5) assert vm_start(vm_name) is True sleep(1),if vm_name is None:
def _check_network_private(test_network): test_net = ipaddress.IPNetwork(test_network) test_start = test_net.network test_end = test_net.broadcast for network in settings.vpn.safe_priv_subnets: network = ipaddress.IPNetwork(network) net_start = network.network net_end = network.broadcast <IF_STMT> return True return False,if test_start >= net_start and test_end <= net_end:
def remove_stale_sockets(self): with self.lock: if self.opts.max_idle_time_ms is not None: for sock_info in self.sockets.copy(): age = _time() - sock_info.last_checkout <IF_STMT> self.sockets.remove(sock_info) sock_info.close() while len(self.sockets) + self.active_sockets < self.opts.min_pool_size: sock_info = self.connect() with self.lock: self.sockets.add(sock_info),if age > self.opts.max_idle_time_ms:
"def hint(self, button): """"""As hilight, but marks GTK Button as well"""""" active = None for b in self.button_widgets.values(): if b.widget.get_sensitive(): b.widget.set_state(Gtk.StateType.NORMAL) <IF_STMT> active = b.widget if active is not None: active.set_state(Gtk.StateType.ACTIVE) self.hilight(button)",if b.name == button:
"def post_process(self, retcode): if not self.ok_codes: return retcode for code in self.ok_codes: self.log.debug('Comparing %s with %s codes', code, retcode) <IF_STMT> self.log.info('Exit code %s was changed to 0 by RCAssert plugin', code) return 0 self.log.info('Changing exit code to %s because RCAssert pass list was unsatisfied', self.fail_code) return self.fail_code",if code == int(retcode):
"def get_form_kwargs(self): result = super().get_form_kwargs() if self.request.method != 'POST': if self.initial: result.pop('data', None) result.pop('files', None) <IF_STMT> result['data'] = self.request.GET return result",if self.has_all_fields() and (not self.empty_form):
"def transform_first_chunk(self, headers, chunk, finishing): if self._chunking: <IF_STMT> self._chunking = False else: headers['Transfer-Encoding'] = 'chunked' chunk = self.transform_chunk(chunk, finishing) return (headers, chunk)",if 'Content-Length' in headers or 'Transfer-Encoding' in headers:
"def copy_stream(self, in_fd, out_fd, length=2 ** 64): total = 0 while 1: available_to_read = min(length - total, self.BUFFERSIZE) data = in_fd.read(available_to_read) <IF_STMT> break out_fd.write(data) total += len(data) self.session.report_progress('Reading %s @ %#x', in_fd.urn, total)",if not data:
"def _trim_steps(self, num_steps): """"""Trims a given number of steps from the end of the sequence."""""" steps_trimmed = 0 for i in reversed(range(len(self._events))): <IF_STMT> if steps_trimmed == num_steps: del self._events[i + 1:] break steps_trimmed += 1 elif i == 0: self._events = [PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None)] break",if self._events[i].event_type == PolyphonicEvent.STEP_END:
"def get_img_file(dir_name: str) -> list: """"""Get all image file paths in several directories which have the same parent directory."""""" images = [] for parent, _, filenames in os.walk(dir_name): for filename in filenames: <IF_STMT> continue img_path = os.path.join(parent, filename) images.append(img_path) return images",if not is_image_file(filename):
def get_agg_title(clause): attr = str(clause.attribute) if clause.aggregation is None: <IF_STMT> return attr[:15] + '...' + attr[-10:] return f'{attr}' elif attr == 'Record': return f'Number of Records' else: if len(attr) > 15: return f'{clause._aggregation_name.capitalize()} of {attr[:15]}...' return f'{clause._aggregation_name.capitalize()} of {attr}',if len(attr) > 25:
"def _check_realign(data): """"""Check for realignment, which is not supported in GATK4"""""" if 'gatk4' not in data['algorithm'].get('tools_off', []) and (not 'gatk4' == data['algorithm'].get('tools_off')): <IF_STMT> raise ValueError('In sample %s, realign specified but it is not supported for GATK4. Realignment is generally not necessary for most variant callers.' % dd.get_sample_name(data))",if data['algorithm'].get('realign'):
"def __call__(self, target): if 'weights' not in target.temp: return True targets = target.temp['weights'] for cname in target.children: if cname in targets: c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) <IF_STMT> return True if 'cash' in target.temp: cash_deviation = abs((target.capital - targets.value) / targets.value - target.temp['cash']) if cash_deviation > self.tolerance: return True return False",if deviation > self.tolerance:
def status_string(self): if not self.live: <IF_STMT> return _('expired') elif self.approved_schedule: return _('scheduled') elif self.workflow_in_progress: return _('in moderation') else: return _('draft') elif self.approved_schedule: return _('live + scheduled') elif self.workflow_in_progress: return _('live + in moderation') elif self.has_unpublished_changes: return _('live + draft') else: return _('live'),if self.expired:
"def __getitem__(self, item): if item == 'EntityId': <IF_STMT> if self.use_uuid: super(PlayerDict, self).__setitem__('EntityId', self.get_name_from_uuid()) else: super(PlayerDict, self).__setitem__('EntityId', self._name) return super(PlayerDict, self).__getitem__(item)",if 'EntityId' not in self:
"def _to_num_bytes(java_mem_str): if isinstance(java_mem_str, string_types): for i, magnitude in enumerate(('k', 'm', 'g', 't'), start=1): <IF_STMT> return int(java_mem_str[:-1]) * 1024 ** i return int(java_mem_str)",if java_mem_str.lower().endswith(magnitude):
"def test_layout_instantiate_subplots(self): layout = Curve(range(10)) + Curve(range(10)) + Image(np.random.rand(10, 10)) + Curve(range(10)) + Curve(range(10)) plot = mpl_renderer.get_plot(layout) positions = [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0)] self.assertEqual(sorted(plot.subplots.keys()), positions) for i, pos in enumerate(positions): adjoint = plot.subplots[pos] <IF_STMT> self.assertEqual(adjoint.subplots['main'].layout_num, i + 1)",if 'main' in adjoint.subplots:
"def __str__(self): width = int(os.environ.get('COLUMNS', '80')) s = self.getSynopsis() + '\n' + ""(use 'tahoe --help' to view global options)\n"" + '\n' + self.getUsage() if self.description: s += '\n' + wrap_paragraphs(self.description, width) + '\n' if self.description_unwrapped: du = textwrap.dedent(self.description_unwrapped) <IF_STMT> du = du[1:] s += '\n' + du + '\n' return s",if du.startswith('\n'):
"def open(self, path, mode='rb', cryptoType=-1, cryptoKey=-1, cryptoCounter=-1): if path is not None: <IF_STMT> self.close() if isinstance(path, str): self.f = open(path, mode) self._path = path self.f.seek(0, 2) self.size = self.f.tell() self.f.seek(0, 0) elif isinstance(path, BaseFile): self.f = path self.size = path.size else: raise IOError('Invalid file parameter') self.setupCrypto(cryptoType, cryptoKey, cryptoCounter)",if self.isOpen():
def get_search_columns_list(self) -> List[str]: ret_lst = list() for col_name in self.get_columns_list(): <IF_STMT> tmp_prop = self.get_property_first_col(col_name).name if not self.is_pk(tmp_prop) and (not self.is_fk(tmp_prop)) and (not self.is_image(col_name)) and (not self.is_file(col_name)): ret_lst.append(col_name) else: ret_lst.append(col_name) return ret_lst,if not self.is_relation(col_name):
"def get_artist(self, name): artist = self.artists.get(name) if not artist: <IF_STMT> try: artist = q(m.Artist).filter_by(name=name).one() except NoResultFound: pass if artist and self.ram_cache: self.add_artist(artist) return artist",if self.use_db:
"def _find_glob_metadata(cur_files, metadata): md_key = None for check_key in metadata.keys(): matches = 0 <IF_STMT> for fname in cur_files: if fnmatch.fnmatch(fname, '*/%s' % check_key): matches += 1 if matches == len(cur_files): md_key = check_key break if md_key: return metadata[md_key]",if '*' in check_key:
"def extract_copy(data: bytearray, mem: bytearray, memstart: int, datastart: int, size: int): for i in range(size): <IF_STMT> mem[memstart + i] = data[datastart + i] else: mem[memstart + i] = 0",if datastart + i < len(data):
"def rpc_get_image(self, sender, image_hash): self.router.addContact(sender) try: <IF_STMT> self.log.warning('Image hash is not 20 characters %s' % image_hash) raise Exception('Invalid image hash') self.log.info('serving image %s to %s' % (image_hash.encode('hex'), sender)) with open(self.db.filemap.get_file(image_hash.encode('hex')), 'rb') as filename: image = filename.read() return [image] except Exception: self.log.warning('could not find image %s' % image_hash[:20].encode('hex')) return None",if len(image_hash) != 20:
"def preprocess_mnist(raw, withlabel, ndim, scale, image_dtype, label_dtype, rgb_format): images = raw['x'] if ndim == 2: images = images.reshape(-1, 28, 28) elif ndim == 3: images = images.reshape(-1, 1, 28, 28) <IF_STMT> images = np.broadcast_to(images, (len(images), 3) + images.shape[2:]) elif ndim != 1: raise ValueError('invalid ndim for MNIST dataset') images = images.astype(image_dtype) images *= scale / 255.0 if withlabel: labels = raw['y'].astype(label_dtype) return (images, labels) return images",if rgb_format:
"def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type <IF_STMT> token = Keyword.Type yield (index, token, value)",elif self.platformhighlighting and value in self.linux_types:
"def _match(self, pattern, input_string, context=None): for index in find_all(input_string, pattern, **self._kwargs): match = Match(index, index + len(pattern), pattern=self, input_string=input_string, **self._match_kwargs) <IF_STMT> yield match",if match:
"def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split('] ')[1] + '.' except IndexError: error_msg = str(err_msg.args[0]) + '.' if settings.INIT_TEST == True: if settings.VERBOSITY_LEVEL < 2: print(settings.FAIL_STATUS) el<IF_STMT> print('') print(settings.print_critical_msg(error_msg)) raise SystemExit()",if settings.VERBOSITY_LEVEL < 1:
"def recursive_select(tag): <IF_STMT> print('Calling select(""%s"") recursively on %s %s' % (next_token, tag.name, tag.attrs)) print('-' * 40) for i in tag.select(next_token, recursive_candidate_generator): if self._select_debug: print('(Recursive select picked up candidate %s %s)' % (i.name, i.attrs)) yield i if self._select_debug: print('-' * 40)",if self._select_debug:
"def detect(self, agent, result): word = self.checkWords(agent) if word: result[self.info_type] = dict(name=self.name) result['bot'] = self.bot version = self.getVersion(agent, word) if version: result[self.info_type]['version'] = version <IF_STMT> result['platform'] = {'name': self.platform, 'version': version} return True",if self.platform:
def is_display_marc(data): if data.startswith('(Length implementation at offset 22 should hold a digit. Assuming 0)'): return True try: lines = data.split('\n') leader = lines[0] assert re_leader.match(leader) for line in lines[1:]: <IF_STMT> assert re_control.match(line) else: assert re_data.match(line) return True except AssertionError: return False,if line.startswith('00'):
"def nodejslib(self): if not hasattr(self, '_nodejslib'): for lib in self.libs: <IF_STMT> self._nodejslib = lib break else: self._nodejslib = None return self._nodejslib",if lib.name == 'node.js stdlib':
"def get(self, key, default=None, type=None): for d in self.dicts: <IF_STMT> if type is not None: try: return type(d[key]) except ValueError: continue return d[key] return default",if key in d:
"def add_callers(target, source): """"""Combine two caller lists in a single list."""""" new_callers = {} for func, caller in target.items(): new_callers[func] = caller for func, caller in source.items(): if func in new_callers: <IF_STMT> new_callers[func] = tuple([i[0] + i[1] for i in zip(caller, new_callers[func])]) else: new_callers[func] += caller else: new_callers[func] = caller return new_callers","if isinstance(caller, tuple):"
"def work(src, vsi_dest): gdal.Mkdir(vsi_dest, 511) for item in src.iterdir(): item_vsi_dest = os.path.join(vsi_dest, item.name) <IF_STMT> work(item, item_vsi_dest) else: VsiFileSystem.copy_to(str(item), item_vsi_dest)",if item.is_dir():
"def __getitem__(self, key): if isinstance(key, raw_types.Qid): return self._operation_touching(key) elif isinstance(key, Iterable): qubits_to_keep = frozenset(key) ops_to_keep = tuple((op for op in self.operations <IF_STMT>)) return Moment(ops_to_keep)",if not qubits_to_keep.isdisjoint(frozenset(op.qubits))
def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split('.') test_ver = test_version.split('.') if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: <IF_STMT> return True elif runtime_ver[1] == test_ver[1]: if runtime_ver[2] > test_ver[2]: return True return False,if runtime_ver[1] > test_ver[1]:
"def populate(self, item): path = self.getItemPath(item) for name in sorted(os.listdir(path)): <IF_STMT> continue pathname = os.path.join(path, name) if os.path.isdir(pathname): item.addChild(name, True) elif name.lower().endswith('.target') and os.path.isfile(pathname): item.addChild(name, False)",if name[0] == '.':
"def runTests(self): """"""Run tests"""""" runner = self._makeRunner() try: self.result = runner.run(self.test) except Exception as e: log.exception('Internal Error') sys.stderr.write('Internal Error: runTests aborted: %s\n' % e) <IF_STMT> sys.exit(1) if self.exit: sys.exit(not self.result.wasSuccessful())",if self.exit:
"def __setitem__(self, key, value): """"""Like :meth:`set` but also supports index/slice based setting."""""" if isinstance(key, (slice, int)): <IF_STMT> value = [value] value = [(_unicodify_header_value(k), _unicodify_header_value(v)) for k, v in value] for _, v in value: self._validate_value(v) if isinstance(key, int): self._list[key] = value[0] else: self._list[key] = value else: self.set(key, value)","if isinstance(key, int):"
def toggle_fullscreen_hide_tabbar(self): if self.is_fullscreen(): <IF_STMT> if self.guake and self.guake.notebook_manager: self.guake.notebook_manager.set_notebooks_tabbar_visible(False) elif self.guake and self.guake.notebook_manager: v = self.settings.general.get_boolean('window-tabbar') self.guake.notebook_manager.set_notebooks_tabbar_visible(v),if self.settings.general.get_boolean('fullscreen-hide-tabbar'):
"def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) <IF_STMT> sChild.declaration = None sChild.docname = None sChild.line = None if sChild.siblingAbove is not None: sChild.siblingAbove.siblingBelow = sChild.siblingBelow if sChild.siblingBelow is not None: sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None",if sChild.declaration and sChild.docname == docname:
"def visit_hierarchichttprequest(self, request): files = [] body_file = request.config.get('body-file') if body_file: files.append(body_file) uploads = request.config.get('upload-files', []) files.extend([x['path'] for x in uploads if not has_variable_pattern(x['path'])]) if 'jsr223' in request.config: jsrs = request.config.get('jsr223') <IF_STMT> jsrs = [jsrs] for jsr in jsrs: if 'script-file' in jsr: files.append(jsr.get('script-file')) return files","if isinstance(jsrs, dict):"
"def find_commands(management_dir): command_dir = os.path.join(management_dir, 'commands') commands = [] try: for f in os.listdir(command_dir): if f.startswith('_'): continue elif f.endswith('.py') and f[:-3] not in commands: commands.append(f[:-3]) <IF_STMT> commands.append(f[:-4]) except OSError: pass return commands",elif f.endswith('.pyc') and f[:-4] not in commands:
"def show_panel(panel_id): for position in _positions_names: pos_panel_ids = _get_position_panels(position) if len(pos_panel_ids) == 0: continue <IF_STMT> continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) if notebook_page == panel_widget: notebook.set_current_page(i)",if len(pos_panel_ids) == 1:
"def is_cwl_record(d): """"""Check if an input is a CWL record, from any level of nesting."""""" if isinstance(d, dict): <IF_STMT> return d else: recs = list(filter(lambda x: x is not None, [is_cwl_record(v) for v in d.values()])) return recs[0] if recs else None else: return None",if d.get('type') == 'record':
"def _flags_data_(self, main_mod, model_paths, flags_dest): try: sys_path, mod_path = python_util.find_module(main_mod, model_paths) except ImportError as e: <IF_STMT> self.log.warning('cannot import flags from %s: %s', main_mod, e) return {} else: package = self._main_spec_package(main_mod) return self._flags_data_for_path(mod_path, package, sys_path, flags_dest)",if os.getenv('NO_WARN_FLAGS_IMPORT') != '1':
"def __str__(self): messages = [self.__class__.__name__, '('] annotation = self.annotation messages.append(self.annotation.surrounds_attribute or '') if annotation.tag_attributes: <IF_STMT> messages.append(';') for f, ta, ea in self.tag_data: messages += [ea, ': attribute ""', ta, '""'] start, end = (annotation.start_index, annotation.end_index) messages.append(', template[%s:%s])' % (start, end)) return ''.join(map(str, messages))",if annotation.surrounds_attribute:
"def _on_view_count_change(self, *args): with self.output: logger.debug('views: %d', self.image.view_count) <IF_STMT> try: logger.debug('was dirty, and needs an update') self.update() finally: self._dirty = False",if self._dirty and self.image.view_count > 0:
"def network_state(self, device): cmd = ['tc', 'qdisc', 'show', 'dev', device] try: output = self.host_exec.run(cmd) <IF_STMT> return NetworkState.SLOW if ' loss ' in output: return NetworkState.FLAKY if ' duplicate ' in output: return NetworkState.DUPLICATE return NetworkState.NORMAL except Exception: return NetworkState.UNKNOWN",if ' delay ' in output:
"def _remove(self, item): """"""Internal removal of an item"""""" for sibling in self.lines[self.lines.index(item) + 1:]: <IF_STMT> env = sibling.env sibling.env = item.env sibling.env.update(env) sibling.env.job = sibling break elif sibling == '': self.lines.remove(sibling) else: break self.crons.remove(item) self.lines.remove(item) return 1","if isinstance(sibling, CronItem):"
"def _get_transformations(self, current_text, indices_to_modify): transformed_texts = [] words = current_text.words for idx in indices_to_modify: word = words[idx] swap_idxs = list(set(range(len(words))) - {idx}) <IF_STMT> swap_idx = random.choice(swap_idxs) swapped_text = current_text.replace_word_at_index(idx, words[swap_idx]).replace_word_at_index(swap_idx, word) transformed_texts.append(swapped_text) return transformed_texts",if swap_idxs:
"def _unlock_restarted_vms(self, pool_name): result = [] for vm in await self.middleware.call('vm.query', [('autostart', '=', True)]): for device in vm['devices']: if device['dtype'] not in ('DISK', 'RAW'): continue path = device['attributes'].get('path') <IF_STMT> continue if path.startswith(f'/dev/zvol/{pool_name}/') or path.startswith(f'/mnt/{pool_name}/'): result.append(vm) break return result",if not path:
def parse_literal_object(node): value = 0 unit = get_default_weight_unit() for field in node.fields: if field.name.value == 'value': try: value = decimal.Decimal(field.value.value) except decimal.DecimalException: raise GraphQLError(f'Unsupported value: {field.value.value}') <IF_STMT> unit = field.value.value return Weight(**{unit: value}),if field.name.value == 'unit':
"def _extract_level(self): """"""Extract level and component if available (lazy)."""""" if self._level is None: split_tokens = self.split_tokens if not split_tokens: self._level = False self._component = False return x = self.log_levels.index(split_tokens[1]) <IF_STMT> else None if x is not None: self._level = split_tokens[1] self._component = split_tokens[2] else: self._level = False self._component = False",if split_tokens[1] in self.log_levels
"def _average_import_time(n: int, module: Text) -> float: total = 0 for _ in range(n): lines = subprocess.getoutput(f'{sys.executable} -X importtime -c ""import {module}""').splitlines() parts = lines[-1].split('|') <IF_STMT> raise Exception(f'Import time not found for {module}.') total += int(parts[1].strip()) / 1000000 return total / n",if parts[-1].strip() != module:
"def send_preamble(self): """"""Transmit version/status/date/server, via self._write()"""""" if self.origin_server: if self.client_is_modern(): self._write('HTTP/%s %s\r\n' % (self.http_version, self.status)) if not self.headers.has_key('Date'): self._write('Date: %s\r\n' % time.asctime(time.gmtime(time.time()))) <IF_STMT> self._write('Server: %s\r\n' % self.server_software) else: self._write('Status: %s\r\n' % self.status)",if self.server_software and (not self.headers.has_key('Server')):
"def test_source_address(self): for addr, is_ipv6 in VALID_SOURCE_ADDRESSES: <IF_STMT> warnings.warn('No IPv6 support: skipping.', NoIPv6Warning) continue with HTTPConnectionPool(self.host, self.port, source_address=addr, retries=False) as pool: r = pool.request('GET', '/source_address') assert r.data == b(addr[0])",if is_ipv6 and (not HAS_IPV6_AND_DNS):
"def _run_commands(self, tool, commands, dry_run=False): if dry_run: self._dry_run_commands(tool, commands) return for command in commands: try: with original_ld_library_path(): self.subprocess_utils.run(command, capture_output=True, check=True) except OSError as ex: <IF_STMT> raise ValueError(self._TOOL_NOT_FOUND_MESSAGE % tool) raise ex self._write_success_message(tool)",if ex.errno == errno.ENOENT:
"def test_float_overflow(self): import sys big_int = int(sys.float_info.max) * 2 for t in float_types + [c_longdouble]: self.assertRaises(OverflowError, t, big_int) <IF_STMT> self.assertRaises(OverflowError, t.__ctype_be__, big_int) if hasattr(t, '__ctype_le__'): self.assertRaises(OverflowError, t.__ctype_le__, big_int)","if hasattr(t, '__ctype_be__'):"
"def init_weights(self): for n, p in self.named_parameters(): <IF_STMT> torch.nn.init.zeros_(p) elif 'fc' in n: torch.nn.init.xavier_uniform_(p)",if 'bias' in n:
"def _compute_dependencies(self): """"""Gather the lists of dependencies and adds to all_parts."""""" for part in self.all_parts: dep_names = self.after_requests.get(part.name, []) for dep_name in dep_names: dep = self.get_part(dep_name) <IF_STMT> raise errors.SnapcraftAfterPartMissingError(part.name, dep_name) part.deps.append(dep)",if not dep:
"def _delete_object(step): try: api = kubernetes.client.CustomObjectsApi() api.delete_namespaced_custom_object(group='zalando.org', version='v1', plural='kopfexamples', namespace='default', name=f'kopf-example-{step}', body={}) except kubernetes.client.rest.ApiException as e: <IF_STMT> pass else: raise",if e.status in [404]:
"def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: if not d.enabled: continue <IF_STMT> continue value = d.get(key) if value: for f in filters: if f(key, value): return None return value",if key_len > d.longest_key:
"def fork_with_monitor(receiver: Receiver, func, *args, **kwargs): current_actor = self() send(ForkWithMonitor(current_actor, func, args, kwargs), receiver) while True: message = recv(current_actor) <IF_STMT> return message.new_actor else: send(message, current_actor) return","if isinstance(message, ForkResponse):"
"def read(self, size=-1): if self._offset or size > -1: if self._offset >= self._key.size: return '' <IF_STMT> sizeStr = str(self._offset + size - 1) else: sizeStr = '' hdrs = {'Range': 'bytes=%d-%s' % (self._offset, sizeStr)} else: hdrs = {} buf = self._key.get_contents_as_string(headers=hdrs) self._offset += len(buf) return buf",if size > -1:
def operations(self): registered_operations = {} for fn in hooks.get_hooks('register_image_operations'): registered_operations.update(dict(fn())) operations = [] for op_spec in self.spec.split('|'): op_spec_parts = op_spec.split('-') <IF_STMT> raise InvalidFilterSpecError('Unrecognised operation: %s' % op_spec_parts[0]) op_class = registered_operations[op_spec_parts[0]] operations.append(op_class(*op_spec_parts)) return operations,if op_spec_parts[0] not in registered_operations:
"def find_widget(self, pos): for widget in self.subwidgets[::-1]: <IF_STMT> r = widget.rect if r.collidepoint(pos): return widget.find_widget(subtract(pos, r.topleft)) return self",if widget.visible:
"def _get_body(self): if self._bodytree is None: bodytxt = self._message.accumulate_body() <IF_STMT> att = settings.get_theming_attribute('thread', 'body') att_focus = settings.get_theming_attribute('thread', 'body_focus') self._bodytree = TextlinesList(bodytxt, att, att_focus) return self._bodytree",if bodytxt:
"def config_mode(self, config_command='conf t', pattern=''): output = '' <IF_STMT> output = self.send_command_timing(config_command, strip_command=False, strip_prompt=False) if 'to enter configuration mode anyway' in output: output += self.send_command_timing('YES', strip_command=False, strip_prompt=False) if not self.check_config_mode(): raise ValueError('Failed to enter configuration mode') return output",if not self.check_config_mode():
"def is_enabled(self): try: cmd = subprocess.Popen('netsh advfirewall show currentprofile', stdout=subprocess.PIPE) out = cmd.stdout.readlines() for l in out: <IF_STMT> state = l.split()[-1].strip() return state == 'ON' except: return None",if l.startswith('State'):
"def __rpc_devices(self, *args): data_to_send = {} for device in self.__connected_devices: <IF_STMT> data_to_send[device] = self.__connected_devices[device]['connector'].get_name() return {'code': 200, 'resp': data_to_send}",if self.__connected_devices[device]['connector'] is not None:
"def _mock_manager_nfx(self, *args, **kwargs): if args: <IF_STMT> raise RpcError() elif args[0].tag == 'get-software-information' and args[0].find('./*') is None: return True else: return self._read_file('sw_info_nfx_' + args[0].tag + '.xml')",if args[0].tag == 'command':
"def empty_logs(self, logs=None): if self.quick_log: self.quick_log = [] elif is_main_thread(): self.logs = [] el<IF_STMT> del self.thread_logs[current_thread_id()]",if logs and self.thread_logs.get(current_thread_id()):
"def read_cb(dir_path): df_dict = dict() for fold in ['train', 'val', 'test']: columns = ['premise', 'hypothesis'] <IF_STMT> columns.append('label') jsonl_path = os.path.join(dir_path, '{}.jsonl'.format(fold)) df = read_jsonl_superglue(jsonl_path) df = df[columns] df_dict[fold] = df return (df_dict, None)",if fold != 'test':
def _forward_main_responses(self): while self._should_keep_going(): line = self._proc.stdout.readline() <IF_STMT> continue if not line: break with self._response_lock: sys.stdout.write(line) sys.stdout.flush() self._main_backend_is_fresh = False,if self._main_backend_is_fresh and self._looks_like_echo(line):
"def _check_type(T, allowed): if T not in allowed: <IF_STMT> allowed.add(T) else: types = ', '.join([t.__name__ for t in allowed] + [T.__name__]) raise TypeError('unsupported mixed types: %s' % types)",if len(allowed) == 1:
"def split_named_range(range_string): """"""Separate a named range into its component parts"""""" for range_string in SPLIT_NAMED_RANGE_RE.split(range_string)[1::2]: match = NAMED_RANGE_RE.match(range_string) <IF_STMT> raise NamedRangeException('Invalid named range string: ""%s""' % range_string) else: match = match.groupdict() sheet_name = match['quoted'] or match['notquoted'] xlrange = match['range'] sheet_name = sheet_name.replace(""''"", ""'"") yield (sheet_name, xlrange)",if match is None:
"def clean(self): to_del = [] for i, file_ in enumerate(self.files): try: os.remove(file_) to_del.append(i) except Exception: <IF_STMT> to_del.append(i) for i in to_del[::-1]: del self.files[i]",if not os.path.isfile(file_):
"def lazy_init(self): f = open(self.filename) self.base = {} while 1: l = f.readline() <IF_STMT> break l = l.strip().split(',') if len(l) != 3: continue c, lat, long = l self.base[c] = (float(long), float(lat)) f.close()",if not l:
"def onto_evo_target(self): if self._onto_evo_target is None: self._get_onto_evo_target() if self._onto_evo_target_qobj is None: <IF_STMT> self._onto_evo_target_qobj = self._onto_evo_target else: rev_dims = [self.sys_dims[1], self.sys_dims[0]] self._onto_evo_target_qobj = Qobj(self._onto_evo_target, dims=rev_dims) return self._onto_evo_target_qobj","if isinstance(self._onto_evo_target, Qobj):"
"def _dnsname_to_pat(dn): pats = [] for frag in dn.split('.'): <IF_STMT> pats.append('[^.]+') else: frag = re.escape(frag) pats.append(frag.replace('\\*', '[^.]*')) return re.compile('\\A' + '\\.'.join(pats) + '\\Z', re.IGNORECASE)",if frag == '*':
"def update(id): """"""Update a post if the current user is the author."""""" post = get_post(id) if request.method == 'POST': title = request.form['title'] body = request.form['body'] error = None <IF_STMT> error = 'Title is required.' if error is not None: flash(error) else: post.title = title post.body = body db.session.commit() return redirect(url_for('blog.index')) return render_template('blog/update.html', post=post)",if not title:
"def __iter__(self): for token in base.Filter.__iter__(self): <IF_STMT> attrs = OrderedDict() for name, value in sorted(token['data'].items(), key=_attr_key): attrs[name] = value token['data'] = attrs yield token","if token['type'] in ('StartTag', 'EmptyTag'):"
"def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic if polymorphic: polymorphic_key = polymorphic if isinstance(polymorphic_key, bool): polymorphic_key = 'type' <IF_STMT> return model raise ImproperlyConfigured(u'No model found for data: {!r}'.format(data))",if data.get(polymorphic_key) == model.__name__:
"def _setup_tag(self, tag): tag.py_obj = self self.riot_tag = tag handlers = {} for ev in lifecycle_ev: f = getattr(self, ev.replace('-', '_')) <IF_STMT> tag.on(ev, f)",if f:
"def selection_only(self): selection_only = False sel = self.sel() if (self.context == 'selection' or self.context == 'both') and len(sel): <IF_STMT> selection_only = True elif self.threshold and (not sel[0].empty()): text = self.view.substr(sel[0]) match = re.search(self.threshold, text) if match: selection_only = True else: selection_only = False return selection_only",if len(sel) > 1:
"def find_torrents_to_fetch(torrent_ids): to_fetch = [] t = time() for torrent_id in torrent_ids: torrent = self.torrents[torrent_id] <IF_STMT> to_fetch.append(torrent_id) else: for key in keys: if t - self.cache_times[torrent_id].get(key, 0.0) > self.cache_time: to_fetch.append(torrent_id) break return to_fetch",if t - torrent[0] > self.cache_time:
"def filter(callbackfn): array = this.to_object() arr_len = array.get('length').to_uint32() if not callbackfn.is_callable(): raise this.MakeError('TypeError', 'callbackfn must be a function') T = arguments[1] res = [] k = 0 while k < arr_len: if array.has_property(str(k)): kValue = array.get(str(k)) <IF_STMT> res.append(kValue) k += 1 return res","if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value:"
"def generate_py_upgrades(data): """"""Generate the list of upgrades in upgrades.py."""""" print(' upgrades.py '.center(60, '-')) print('class Upgrades(enum.IntEnum):') print('  """"""The list of upgrades, as returned from RequestData.""""""') for upgrade in sorted(data.upgrades, key=lambda a: a.name): <IF_STMT> print('  %s = %s' % (upgrade.name, upgrade.upgrade_id)) print('\n')",if upgrade.name and upgrade.upgrade_id in static_data.UPGRADES:
"def get_first_n(l, n, reverse=False): cur_n = 0 res = [] for si in reversed(l) if reverse else l: if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date): res.append(si) cur_n += 1 <IF_STMT> break return res[::-1] if reverse else res",if cur_n >= n:
"def _fill_cache(self): for task in linux_pslist.linux_pslist(self._config).calculate(): for filp, fd in task.lsof(): filepath = linux_common.get_path(task, filp) <IF_STMT> to_add = filp.dentry.d_inode.i_ino.v() self.fd_cache[to_add] = [task, filp, fd, filepath]",if type(filepath) == str and filepath.find('socket:[') != -1:
"def is_ArAX_implicit(ii): a, implicit_fixed = (0, 0) for op in _gen_opnds(ii): if op_luf_start(op, 'ArAX'): a += 1 <IF_STMT> implicit_fixed += 1 else: return False return a == 1 and implicit_fixed <= 1",elif op_reg(op) and op_implicit_specific_reg(op):
"def auto_resize(self, name: str) -> None: """"""recompute widget width based on max length of all of the values"""""" widget = self.find_widget(name) for column in range(len(widget._columns) - 1): sizes = [len(x[0][column]) + 1 for x in widget.options] <IF_STMT> sizes.append(len(widget._titles[column]) + 1) widget._columns[column] = max(sizes)",if widget._titles:
"def dns_set_secondary_nameserver(): from dns_update import set_secondary_dns try: return set_secondary_dns([ns.strip() for ns in re.split('[, ]+', request.form.get('hostnames') or '') <IF_STMT>], env) except ValueError as e: return (str(e), 400)",if ns.strip() != ''
"def assert_inputs(inputs, can_be_used=True): with self._different_user_and_history() as other_history_id: response = self._run('cat1', other_history_id, inputs) <IF_STMT> assert response.status_code == 200 else: self._assert_dataset_permission_denied_response(response)",if can_be_used:
"def _handle_start(self, tag, attrib): if 'translatable' in attrib: <IF_STMT> self._translate = True if 'comments' in attrib: self._comments.append(attrib[attrib.index('comments') + 1])",if attrib[attrib.index('translatable') + 1] == 'yes':
"def render(self): """"""What to show when printed."""""" viz = '' for y in range(self.grid.height): for x in range(self.grid.width): c = self.grid[y][x] <IF_STMT> viz += ' ' else: viz += self.converter(c) viz += '\n' return viz",if c is None:
"def _sorted_layers(self, structure, top_layer_id): """"""Return the image layers sorted"""""" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) <IF_STMT> break if 'parent' not in structure['repolayers'][next_layer]['json']: break next_layer = structure['repolayers'][next_layer]['json']['parent'] if not next_layer: break return sorted_layers",if 'json' not in structure['repolayers'][next_layer]:
"def check_sync(self): login_failures = get_login_failures(datetime.now(), catmsgs()) if login_failures: return Alert(SSHLoginFailuresAlertClass, {'count': len(login_failures), 'failures': ''.join(login_failures <IF_STMT> else login_failures[:2] + [f'... {len(login_failures) - 4} more ...\n'] + login_failures[-2:])})",if len(login_failures) <= 5
"def on_user_auth_login_success(sender, user, request, **kwargs): if settings.USER_LOGIN_SINGLE_MACHINE_ENABLED: user_id = 'single_machine_login_' + str(user.id) session_key = cache.get(user_id) <IF_STMT> session = import_module(settings.SESSION_ENGINE).SessionStore(session_key) session.delete() cache.set(user_id, request.session.session_key, None)",if session_key and session_key != request.session.session_key:
"def slots_for_entities(self, entities): if self.store_entities_as_slots: slot_events = [] for s in self.slots: <IF_STMT> matching_entities = [e['value'] for e in entities if e['entity'] == s.name] if matching_entities: if s.type_name == 'list': slot_events.append(SlotSet(s.name, matching_entities)) else: slot_events.append(SlotSet(s.name, matching_entities[-1])) return slot_events else: return []",if s.auto_fill:
"def get(self, id): obj = self.klass.objects.get(id=id) if hasattr(obj, 'sharing'): <IF_STMT> return render_template('{}/single.html'.format(self.klass.__name__.lower()), obj=obj) abort(403) else: return render_template('{}/single.html'.format(self.klass.__name__.lower()), obj=obj) return request.referrer",if group_user_permission(obj):
"def __call__(self, module, *x): """"""Grab the instantiated layer and evaluate it."""""" operation = getattr(module, self.name) try: <IF_STMT> return self.func(operation, *x) return operation(*x) except: logger.error('Failed to apply layer: %s', self.name) for i, X in enumerate(x): logger.error('  Input shape #%d: %s', i + 1, list(X.size())) raise",if self.func:
"def req(s, poll, msg, expect): do_req = True xid = None while True: <IF_STMT> xid = s.put(msg)['xid'] events = poll.poll(2) for fd, event in events: response = s.get() if response['xid'] != xid: do_req = False continue if response['options']['message_type'] != expect: raise Exception('DHCP protocol error') return response do_req = True",if do_req:
"def _state_old_c_params(self, token): self._saved_tokens.append(token) if token == ';': self._saved_tokens = [] self._state = self._state_dec_to_imp elif token == '{': <IF_STMT> self._saved_tokens = [] self._state_dec_to_imp(token) return self._state = self._state_global for tkn in self._saved_tokens: self._state(tkn) elif token == '(': self._state = self._state_global for tkn in self._saved_tokens: self._state(tkn)",if len(self._saved_tokens) == 2:
"def assert_tensors_equal(sess, t1, t2, n): """"""Compute tensors `n` times and ensure that they are equal."""""" for _ in range(n): v1, v2 = sess.run([t1, t2]) if v1.shape != v2.shape: return False <IF_STMT> return False return True",if not np.all(v1 == v2):
"def http_error_302(self, url, fp, errcode, errmsg, headers, data=None): """"""Error 302 -- relocated (temporarily)."""""" self.tries += 1 if self.maxtries and self.tries >= self.maxtries: <IF_STMT> meth = self.http_error_500 else: meth = self.http_error_default self.tries = 0 return meth(url, fp, 500, 'Internal Server Error: Redirect Recursion', headers) result = self.redirect_internal(url, fp, errcode, errmsg, headers, data) self.tries = 0 return result","if hasattr(self, 'http_error_500'):"
"def get_satellite_list(self, daemon_type=''): res = {} for t in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver', 'broker']: if daemon_type and daemon_type != t: continue satellite_list = [] res[t] = satellite_list daemon_name_attr = t + '_name' daemons = self.app.get_daemons(t) for dae in daemons: <IF_STMT> satellite_list.append(getattr(dae, daemon_name_attr)) return res","if hasattr(dae, daemon_name_attr):"
"def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): <IF_STMT> return False f = open(fname, 'rb') s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: if decrypter is not None: if read_only: return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, 'wb') f.write(s) f.close() return True",if decrypter is None:
"def logic(): while 1: yield (clock.posedge, reset.negedge) <IF_STMT> count.next = 0 elif enable: count.next = f1(n)",if reset == ACTIVE_LOW:
"def get_project_translation(request, project=None, component=None, lang=None): """"""Return project, component, translation tuple for given parameters."""""" if lang and component: translation = get_translation(request, project, component, lang) component = translation.component project = component.project else: translation = None if component: component = get_component(request, project, component) project = component.project <IF_STMT> project = get_project(request, project) return (project or None, component or None, translation or None)",elif project:
"def run(self, sql, encoding=None): stream = lexer.tokenize(sql, encoding) for filter_ in self.preprocess: stream = filter_.process(stream) stream = StatementSplitter().process(stream) for stmt in stream: <IF_STMT> stmt = grouping.group(stmt) for filter_ in self.stmtprocess: filter_.process(stmt) for filter_ in self.postprocess: stmt = filter_.process(stmt) yield stmt",if self._grouping:
"def get_word_parens_range(self, offset, opening='(', closing=')'): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): if self.code[index] == opening: open_count += 1 <IF_STMT> open_count -= 1 if open_count == 0: return (start_parens, index + 1) index += 1 return (start_parens, index)",if self.code[index] == closing:
def _get_inherited_env_vars(self): env_vars = os.environ.copy() for var_name in ENV_VARS_BLACKLIST: if var_name.lower() in env_vars: del env_vars[var_name.lower()] <IF_STMT> del env_vars[var_name.upper()] return env_vars,if var_name.upper() in env_vars:
"def adapt_datetimefield_value(self, value): if value is None: return None if hasattr(value, 'resolve_expression'): return value if timezone.is_aware(value): <IF_STMT> value = timezone.make_naive(value, self.connection.timezone) else: raise ValueError('SQLite backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value)",if settings.USE_TZ:
"def dragMoveEvent(self, event): data = event.mimeData() urls = data.urls() if urls and urls[0].scheme() == 'file': event.acceptProposedAction() indexRow = self.indexAt(event.pos()).row() window = self.parent().parent().parent().parent().parent().parent() <IF_STMT> indexRow = window.playlist.count() window.setPlaylistInsertPosition(indexRow) else: super(MainWindow.PlaylistWidget, self).dragMoveEvent(event)",if indexRow == -1 or not window.clearedPlaylistNote:
"def explode(self, obj): """"""Determine if the object should be exploded."""""" if obj in self._done: return False result = False for item in self._explode: if hasattr(item, '_moId'): if obj._moId == item._moId: result = True el<IF_STMT> result = True if result: self._done.add(obj) return result",if obj.__class__.__name__ == item.__name__:
"def _maybe_clean(self): """"""Clean the cache if it's time to do so."""""" now = time.time() if self.next_cleaning <= now: keys_to_delete = [] for k, v in self.data.items(): <IF_STMT> keys_to_delete.append(k) for k in keys_to_delete: del self.data[k] now = time.time() self.next_cleaning = now + self.cleaning_interval",if v.expiration <= now:
"def test_doc_attributes(self): print_test_name('TEST DOC ATTRIBUTES') correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) if original_schema.doc is not None: correct += 1 <IF_STMT> for f in original_schema.fields: if f.doc is None: self.fail(""Failed to preserve 'doc' in fields: "" + example.schema_string) self.assertEqual(correct, len(DOC_EXAMPLES))",if original_schema.type == 'record':
"def save_as(self): """"""Save *as* the currently edited file"""""" editorstack = self.get_current_editorstack() if editorstack.save_as(): fname = editorstack.get_current_filename() <IF_STMT> self.emit(SIGNAL('open_dir(QString)'), osp.dirname(fname)) self.__add_recent_file(fname)","if CONF.get('workingdir', 'editor/save/auto_set_to_basedir'):"
"def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) if isinstance(default, timedelta): default = default.total_seconds() if isinstance(actual, Enum): actual = actual.value <IF_STMT> yield Error(reason='mismatch', setting=setting_name, default=default, actual=actual)",if actual != default:
"def JobWait(self, waiter): while True: result = waiter.WaitForOne(False) if result > 0: return wait_status.Cancelled(result) if result == -1: break <IF_STMT> break return wait_status.Proc(self.status)",if self.state != job_state_e.Running:
"def object_hook(obj): obj_len = len(obj) if obj_len == 1: if '$date' in obj: return datetime.fromtimestamp(obj['$date'] / 1000, tz=timezone.utc) + timedelta(milliseconds=obj['$date'] % 1000) <IF_STMT> return time(*[int(i) for i in obj['$time'].split(':')]) if obj_len == 2 and '$type' in obj and ('$value' in obj): if obj['$type'] == 'date': return date(*[int(i) for i in obj['$value'].split('-')]) return obj",if '$time' in obj:
"def before_FunctionDef(self, node): s = self.format(node, print_body=False) if self.test_kind is 'test': print(s) self.indent += 1 self.context_stack.append(node) if self.pass_n == 1: self.stats.defs += 1 <IF_STMT> if self.class_name in self.classes: the_class = self.classes.get(self.class_name) methods = the_class.get('methods') methods[node.name] = self.format(node.args)",if self.class_name not in self.special_class_names:
"def setAttributeNS(self, namespaceURI, qualifiedName, value): prefix, localname = _nssplit(qualifiedName) attr = self.getAttributeNodeNS(namespaceURI, localname) if attr is None: attr = Attr(qualifiedName, namespaceURI, localname, prefix) attr.value = value attr.ownerDocument = self.ownerDocument self.setAttributeNode(attr) else: if value != attr.value: attr.value = value <IF_STMT> _clear_id_cache(self) if attr.prefix != prefix: attr.prefix = prefix attr.nodeName = qualifiedName",if attr.isId:
"def main(): try: from wsgiref.simple_server import make_server from wsgiref.validate import validator <IF_STMT> port[0] = get_open_port() wsgi_application = WsgiApplication(msgpackrpc_application) server = make_server(host, port[0], validator(wsgi_application)) logger.info('Starting interop server at %s:%s.' % (host, port[0])) logger.info('WSDL is at: /?wsdl') server.serve_forever() except ImportError: print('Error: example server code requires Python >= 2.5')",if port[0] == 0:
"def yield_modules(path): """"""Yield all Python modules underneath *path*"""""" for dpath, dnames, fnames in os.walk(path): module = tuple(dpath.split('/')[1:]) for fname in fnames: if not fname.endswith('.py'): continue fpath = os.path.join(dpath, fname) <IF_STMT> yield (fpath, module) else: yield (fpath, module + (fname[:-3],)) dnames[:] = [x for x in dnames if os.path.exists(os.path.join(dpath, x, '__init__.py'))]",if fname == '__init__.py':
"def dump_section(name, section): lines.append('[%s]\n' % name) for key, value in section.all_items(): if not key.startswith('_'): try: <IF_STMT> lines.append('%s=%s\n' % (key, section.definitions[key].tostring(value))) else: lines.append('%s=%s\n' % (key, value)) except: logger.exception('Error serializing ""%s"" in section ""[%s]""', key, name) lines.append('\n')",if key in section.definitions:
"def testCreateTimeout(self): cluster = None try: env_path = 'conda://' + os.environ['CONDA_PREFIX'] log_config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'yarn-logging.conf') with self.assertRaises(TimeoutError): cluster = new_cluster(env_path, log_config=log_config_file, worker_cache_mem='64m', log_when_fail=True, timeout=1) finally: <IF_STMT> cluster.stop()",if cluster is not None:
"def read_phrases(data_dir, movies=None): res = {} for parts in iterate_entries(data_dir, 'movie_lines.txt'): l_id, m_id, l_str = (parts[0], parts[2], parts[4]) if movies and m_id not in movies: continue tokens = utils.tokenize(l_str) <IF_STMT> res[l_id] = tokens return res",if tokens:
"def get_Subclass_of(rt): for y in [getattr(Ast, x) for x in dir(Ast)]: yt = clr.GetClrType(y) if rt == yt: continue if yt.IsAbstract: continue <IF_STMT> yield yt.Name",if yt.IsSubclassOf(rt):
"def retrieve(self, aclass): """"""Look for a specifc class/name in the packet"""""" resu = [] for x in self.payload: try: if isinstance(aclass, str): if x.name == aclass: resu.append(x) el<IF_STMT> resu.append(x) resu += x.retrieve(aclass) except: pass return resu","if isinstance(x, aclass):"
"def _max_physical(self): """"""How big is the physical screen?"""""" try: mxy, mxx = struct.unpack('hh', fcntl.ioctl(sys.stderr.fileno(), termios.TIOCGWINSZ, 'xxxx')) <IF_STMT> raise ValueError except (ValueError, NameError): mxy, mxx = curses.newwin(0, 0).getmaxyx() return (mxy - 1, mxx - 1)","if (mxy, mxx) == (0, 0):"
"def deserialize(self, cassette_string): cassette_dict = self.base_serializer.deserialize(cassette_string) for interaction in cassette_dict['interactions']: response = interaction['response'] headers = response['headers'] <IF_STMT> rg, size, filename = self._parse_headers(headers) with open(join(self.directory, filename), 'rb') as f: f.seek(rg[0]) content = f.read(rg[1] - rg[0] + 1) response['body']['string'] = content return cassette_dict",if 'Content-Range' in headers and 'Content-Disposition' in headers:
"def parse_head(fileobj, parser): """"""Return a list of key, value pairs."""""" while 1: data = fileobj.read(CHUNK) try: parser.feed(data) except EndOfHeadError: break <IF_STMT> break return parser.http_equiv",if len(data) != CHUNK:
"def _check_no_empty_dimension_lists(config): """"""Verify that at least one dimension is not an empty list"""""" logging.info('Checking provided dimensions are valid') for feature in config.get('test-suites').values(): for test_name, test in feature.items(): for dimensions_config in test.values(): for dimensions_group in dimensions_config: <IF_STMT> logging.error('Values assigned to dimensions in test %s cannot be empty', test_name) raise AssertionError",if [] in dimensions_group.values():
"def aggregate_sorted(self, items): create = self.createCombiner merge = self.mergeValue i = None for i, (k, v) in enumerate(items): if i == 0: curr_key = k curr_value = create(v) <IF_STMT> yield (curr_key, curr_value) curr_key = k curr_value = create(v) else: curr_value = merge(curr_value, v) if i is not None: yield (curr_key, curr_value)",elif k != curr_key:
"def _run_iptables(self, version, cmd, *args): ipt_cmd = '{} {}'.format(self._iptables_command(version), cmd) if self._has_w_argument is None: result = self.run_expect([0, 2], ipt_cmd, *args) <IF_STMT> self._has_w_argument = False return self._run_iptables(version, cmd, *args) else: self._has_w_argument = True return result.stdout.rstrip('\r\n') else: return self.check_output(ipt_cmd, *args)",if result.rc == 2:
"def handle_data(self, data): if self.in_span or self.in_div: if data == 'No such user (please note that login is case sensitive)': self.no_user = True elif data == 'Invalid password': self.bad_pw = True <IF_STMT> self.already_exists = True",elif data == 'User with that email already exists':
"def configure(self, **kw): """"""Configure the image."""""" res = () for k, v in _cnfmerge(kw).items(): if v is not None: if k[-1] == '_': k = k[:-1] if hasattr(v, '__call__'): v = self._register(v) <IF_STMT> v = self.tk._createbytearray(v) res = res + ('-' + k, v) self.tk.call((self.name, 'config') + res)","elif k in ('data', 'maskdata'):"
"def run(self): if self.distribution.install_requires: self.distribution.fetch_build_eggs(self.distribution.install_requires) if self.distribution.tests_require: self.distribution.fetch_build_eggs(self.distribution.tests_require) if self.test_suite: cmd = ' '.join(self.test_args) <IF_STMT> self.announce('skipping ""unittest %s"" (dry run)' % cmd) else: self.announce('running ""unittest %s""' % cmd) self.with_project_on_sys_path(self.run_tests)",if self.dry_run:
"def wrapped(request, *args, **kwargs): if not gargoyle.is_active(key, request): if not redirect_to: raise Http404(""Switch '%s' is not active"" % key) <IF_STMT> return HttpResponseRedirect(redirect_to) else: return HttpResponseRedirect(reverse(redirect_to)) return func(request, *args, **kwargs)",elif redirect_to.startswith('/'):
def strip_suffixes(path: str) -> str: t = path while True: <IF_STMT> t = t[:-3] elif t.endswith('.raw'): t = t[:-4] elif t.endswith('.tar'): t = t[:-4] elif t.endswith('.qcow2'): t = t[:-6] else: break return t,if t.endswith('.xz'):
"def tags(self): label = '' for dt in constants.DOMAIN_TYPES: <IF_STMT> label = dt[1] result = [{'name': self.type, 'label': label, 'type': 'dom'}] if self.transport: result.append({'name': self.transport.service, 'label': self.transport.service, 'type': 'srv', 'color': 'info'}) return result",if self.type == dt[0]:
"def find_first_of_filetype(content, filterfiltype, attr='name'): """"""Find the first of the file type."""""" filename = '' for _filename in content: if isinstance(_filename, str): if _filename.endswith(f'.{filterfiltype}'): filename = _filename break el<IF_STMT> filename = getattr(_filename, attr) break return filename","if getattr(_filename, attr).endswith(f'.{filterfiltype}'):"
"def check_data_array_types(self, *arrays): result = [] for array in arrays: <IF_STMT> result.append(array) continue result.append(np.asanyarray(array)) if not result[-1].shape: raise RuntimeError('Given data-array is of unexpected type %s. Please pass numpy arrays instead.' % type(array)) return result",if array is None or scipy.sparse.issparse(array):
"def description(self): global role_descriptions description = role_descriptions[self.role_field] content_type = self.content_type model_name = None if content_type: model = content_type.model_class() model_name = re.sub('([a-z])([A-Z])', '\\1 \\2', model.__name__).lower() value = description if type(description) == dict: value = description.get(model_name) <IF_STMT> value = description.get('default') if '%s' in value and content_type: value = value % model_name return value",if value is None:
"def popupFrameXdiff(job, frame1, frame2, frame3=None): """"""Opens a frame xdiff."""""" for command in ['/usr/bin/xxdiff', '/usr/local/bin/xdiff']: <IF_STMT> for frame in [frame1, frame2, frame3]: if frame: command += ' --title1 %s %s' % (frame.data.name, getFrameLogFile(job, frame)) shellOut(command)",if os.path.isfile(command):
"def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs['groups'] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs['groups'] if 'group_name' in group: mykwargs['source_security_group_name'] = group['group_name'] if 'user_id' in group: mykwargs['source_security_group_owner_id'] = group['user_id'] <IF_STMT> mykwargs['source_security_group_id'] = group['group_id'] groups_args_split.append(mykwargs) return groups_args_split",if 'group_id' in group:
"def _mangle_phone(phone, config): regexp = config.get('REGEXP') if regexp: try: m = re.match('^/(.*)/(.*)/$', regexp) <IF_STMT> phone = re.sub(m.group(1), m.group(2), phone) except re.error: log.warning(u'Can not mangle phone number. Please check your REGEXP: {0!s}'.format(regexp)) return phone",if m:
"def getScramRange(src): scramRange = None for mod in src.item.activeModulesIter(): <IF_STMT> scramRange = max(scramRange or 0, mod.maxRange or 0) return scramRange",if _isRegularScram(mod) or _isHicScram(mod):
"def snapshot(self): try: self.freeze() <IF_STMT> snapshot = self.get_ec2_connection().create_snapshot(self.volume_id) else: snapshot = self.server.ec2.create_snapshot(self.volume_id) boto.log.info('Snapshot of Volume %s created: %s' % (self.name, snapshot)) except Exception: boto.log.info('Snapshot error') boto.log.info(traceback.format_exc()) finally: status = self.unfreeze() return status",if self.server == None:
"def closeststack(self, card): closest = None cdist = 999999999 for stack in self.openstacks: dist = (stack.x - card.x) ** 2 + (stack.y - card.y) ** 2 <IF_STMT> closest = stack cdist = dist return closest",if dist < cdist:
"def _sock_send(self, msg): try: if isinstance(msg, str): msg = msg.encode('ascii') if self.dogstatsd_tags: msg = msg + b'|#' + self.dogstatsd_tags.encode('ascii') <IF_STMT> self.sock.send(msg) except Exception: Logger.warning(self, 'Error sending message to statsd', exc_info=True)",if self.sock:
"def styleRow(self, row, selected): if row > 0 and row < self.getRowCount(): <IF_STMT> self.getRowFormatter().addStyleName(row, 'user-SelectedRow') else: self.getRowFormatter().removeStyleName(row, 'user-SelectedRow')",if selected:
"def __gather_epoch_end_eval_results(self, outputs): eval_results = [] for epoch_output in outputs: result = epoch_output[0].__class__.gather(epoch_output) if 'checkpoint_on' in result: result.checkpoint_on = result.checkpoint_on.mean() <IF_STMT> result.early_stop_on = result.early_stop_on.mean() eval_results.append(result) if len(eval_results) == 1: eval_results = eval_results[0] return eval_results",if 'early_stop_on' in result:
"def network_state(self, device): cmd = ['tc', 'qdisc', 'show', 'dev', device] try: output = self.host_exec.run(cmd) if ' delay ' in output: return NetworkState.SLOW if ' loss ' in output: return NetworkState.FLAKY <IF_STMT> return NetworkState.DUPLICATE return NetworkState.NORMAL except Exception: return NetworkState.UNKNOWN",if ' duplicate ' in output:
"def canberra_grad(x, y): result = 0.0 grad = np.zeros(x.shape) for i in range(x.shape[0]): denominator = np.abs(x[i]) + np.abs(y[i]) <IF_STMT> result += np.abs(x[i] - y[i]) / denominator grad[i] = np.sign(x[i] - y[i]) / denominator - np.abs(x[i] - y[i]) * np.sign(x[i]) / denominator ** 2 return (result, grad)",if denominator > 0:
"def readwrite(obj, flags): try: if flags & select.POLLIN: obj.handle_read_event() <IF_STMT> obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: if e.args[0] not in _DISCONNECTED: obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()",if flags & select.POLLOUT:
"def get_func_name(obj): if inspect.ismethod(obj): match = RE_BOUND_METHOD.match(repr(obj)) <IF_STMT> cls = match.group('class') if not cls: return match.group('name') return '%s.%s' % (match.group('class'), match.group('name')) return None",if match:
"def __init__(self, connection): self.username = connection.username self.password = connection.password self.domain = connection.domain self.hash = connection.hash self.lmhash = '' self.nthash = '' self.aesKey = connection.aesKey self.kdcHost = connection.kdcHost self.kerberos = connection.kerberos if self.hash is not None: <IF_STMT> self.lmhash, self.nthash = self.hash.split(':') else: self.nthash = self.hash if self.password is None: self.password = ''",if self.hash.find(':') != -1:
"def indent_xml(elem, level=0): """"""Do our pretty printing and make Matt very happy."""""" i = '\n' + level * '  ' if elem: if not elem.text or not elem.text.strip(): elem.text = i + '  ' if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i el<IF_STMT> elem.tail = i",if level and (not elem.tail or not elem.tail.strip()):
"def add_braces_and_labels(self): for attr in ('horizontal_parts', 'vertical_parts'): <IF_STMT> continue parts = getattr(self, attr) for subattr in ('braces', 'labels'): if hasattr(parts, subattr): self.add(getattr(parts, subattr))","if not hasattr(self, attr):"
"def error_messages(file_list, files_removed): if files_removed is None: return for remove_this, reason in files_removed: if file_list is not None: file_list.remove(remove_this) <IF_STMT> print(' REMOVED : (' + str(remove_this) + ')   is not PNG file format') elif reason == 1: print(' REMOVED : (' + str(remove_this) + ')   already exists') elif reason == 2: print(' REMOVED : (' + str(remove_this) + ')   file unreadable')",if reason == 0:
"def keep_vocab_item(word, count, min_count, trim_rule=None): default_res = count >= min_count if trim_rule is None: return default_res else: rule_res = trim_rule(word, count, min_count) if rule_res == RULE_KEEP: return True <IF_STMT> return False else: return default_res",elif rule_res == RULE_DISCARD:
"def func(x0): bind = 0 backups = [] vinputs = [] for i, i0 in zip(inputs, inputs0): if i is None: continue vinputs += [i] <IF_STMT> i.d[...] = x0[bind:bind + i.size].reshape(i.shape) bind += i.size backups.append(i.d.copy()) f.forward(vinputs, outputs) for ind, i in enumerate(inputs): if i is None: continue i.d[...] = backups[ind] return sum([np.sum(o.g * o.d) for o in outputs])",if i0 is not None:
"def _handle_js_events(self, change): if self.js_events: if self.event_handlers: for event in self.js_events: event_name = event['name'] <IF_STMT> self.event_handlers[event_name](event['detail']) self.js_events = []",if event_name in self.event_handlers:
"def validate(leaves): for leaf in leaves: <IF_STMT> pass elif leaf.has_form('List', None) or leaf.has_form('Association', None): if validate(leaf.leaves) is not True: return False else: return False return True","if leaf.has_form(('Rule', 'RuleDelayed'), 2):"
"def ascii85decode(data): n = b = 0 out = '' for c in data: if '!' <= c and c <= 'u': n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack('>L', b) n = b = 0 elif c == 'z': assert n == 0 out += '\x00\x00\x00\x00' elif c == '~': <IF_STMT> for _ in range(5 - n): b = b * 85 + 84 out += struct.pack('>L', b)[:n - 1] break return out",if n:
"def to_text(self, origin=None, relativize=True, **kw): next = self.next.choose_relativity(origin, relativize) text = '' for window, bitmap in self.windows: bits = [] for i in xrange(0, len(bitmap)): byte = bitmap[i] for j in xrange(0, 8): <IF_STMT> bits.append(dns.rdatatype.to_text(window * 256 + i * 8 + j)) text += ' ' + ' '.join(bits) return '%s%s' % (next, text)",if byte & 128 >> j:
"def _on_response(self, widget, response): value = None if response == Gtk.ResponseType.OK: <IF_STMT> value = self.spinbutton.get_value_as_int() else: value = self.spinbutton.get_value() self.deferred.callback(value) self.destroy()",if self.value_type is int:
"def send_preamble(self): """"""Transmit version/status/date/server, via self._write()"""""" if self.origin_server: <IF_STMT> self._write('HTTP/%s %s\r\n' % (self.http_version, self.status)) if not self.headers.has_key('Date'): self._write('Date: %s\r\n' % time.asctime(time.gmtime(time.time()))) if self.server_software and (not self.headers.has_key('Server')): self._write('Server: %s\r\n' % self.server_software) else: self._write('Status: %s\r\n' % self.status)",if self.client_is_modern():
"def _save_postinsts_common(self, dst_postinst_dir, src_postinst_dir): num = 0 for p in self._get_delayed_postinsts(): bb.utils.mkdirhier(dst_postinst_dir) <IF_STMT> shutil.copy(os.path.join(src_postinst_dir, p + '.postinst'), os.path.join(dst_postinst_dir, '%03d-%s' % (num, p))) num += 1","if os.path.exists(os.path.join(src_postinst_dir, p + '.postinst')):"
"def edge_data_from_bmesh_edges(bm, edge_data): initial_index = bm.edges.layers.int.get('initial_index') if initial_index is None: raise Exception('bmesh has no initial_index layer') edge_data_out = [] n_edge_data = len(edge_data) for edge in bm.edges: idx = edge[initial_index] <IF_STMT> debug('Unexisting edge_data[%s] [0 - %s]', idx, n_edge_data) edge_data_out.append(None) else: edge_data_out.append(edge_data[idx]) return edge_data_out",if idx < 0 or idx >= n_edge_data:
"def write(self, data): try: c_written = DWORD() buffer = create_string_buffer(data) <IF_STMT> raise WinError() except: self.close()","if not WriteFile(self.pStdin, buffer, len(buffer), byref(c_written), None):"
"def get_icon(svg_path, size): pixbuf = GdkPixbuf.Pixbuf.new_from_file_at_scale(svg_path, size, size, True) data = bytearray(pixbuf.get_pixels()) channels = pixbuf.get_n_channels() assert channels == 4 new_data = bytearray() for c in range(3): x = 0 for i in range(0, len(data), 4): <IF_STMT> new_data.append(127) new_data.append(data[i + c]) x += 1 return new_data",if x == 0 or x % 128 == 0:
"def _get_instance_attribute(self, attr, default=None, defaults=None, incl_metadata=False): if self.instance is None or not hasattr(self.instance, attr): if incl_metadata and attr in self.parsed_metadata: return self.parsed_metadata[attr] elif defaults is not None: for value in defaults: if callable(value): value = value() <IF_STMT> return value return default return getattr(self.instance, attr)",if value is not None:
"def forward(self, x): if self.ffn_type in (1, 2): x0 = self.wx0(x) <IF_STMT> x1 = x elif self.ffn_type == 2: x1 = self.wx1(x) out = self.output(x0 * x1) out = self.dropout(out) out = self.LayerNorm(out + x) return out",if self.ffn_type == 1:
def load(cls): if not cls._loaded: cls.log.debug('Loading tile_sets...') <IF_STMT> cls._find_tile_sets(PATHS.TILE_SETS_DIRECTORY) else: cls.tile_sets = JsonDecoder.load(PATHS.TILE_SETS_JSON_FILE) cls.log.debug('Done!') cls._loaded = True,if not horizons.globals.fife.use_atlases:
"def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: if section == NAME: return to_qvariant('Name') <IF_STMT> return to_qvariant('Version') elif section == ACTION: return to_qvariant('Action') elif section == DESCRIPTION: return to_qvariant('Description') return to_qvariant()",elif section == VERSION:
"def find_enabled_item(self, e): x, y = e.local if 0 <= x < (self.width - self.margin - self.scroll_button_size if self.scrolling else self.width): h = self.font.get_linesize() i = (y - h // 2) // h + self.scroll items = self._items if 0 <= i < len(items): item = items[i] <IF_STMT> return item",if item.enabled:
"def set_parallel_limit(environment): parallel_limit = environment.get('COMPOSE_PARALLEL_LIMIT') if parallel_limit: try: parallel_limit = int(parallel_limit) except ValueError: raise errors.UserError('COMPOSE_PARALLEL_LIMIT must be an integer (found: ""{}"")'.format(environment.get('COMPOSE_PARALLEL_LIMIT'))) <IF_STMT> raise errors.UserError('COMPOSE_PARALLEL_LIMIT can not be less than 2') parallel.GlobalLimit.set_global_limit(parallel_limit)",if parallel_limit <= 1:
"def migrate_identifier(self, raw_identifier: int): if self.unique_cog_identifier in self.data: return poss_identifiers = [str(raw_identifier), str(hash(raw_identifier))] for ident in poss_identifiers: <IF_STMT> self.data[self.unique_cog_identifier] = self.data[ident] del self.data[ident] _save_json(self.data_path, self.data) break",if ident in self.data:
"def _memoize(*args, **kwargs): str_args = [] for arg in args: <IF_STMT> str_args.append(six.text_type(arg)) else: str_args.append(arg) args_ = ','.join(list(str_args) + ['{0}={1}'.format(k, kwargs[k]) for k in sorted(kwargs)]) if args_ not in cache: cache[args_] = func(*args, **kwargs) return cache[args_]","if not isinstance(arg, six.string_types):"
"def extract(self): for battery in self.vars: for line in dopen('/proc/acpi/battery/' + battery + '/state').readlines(): l = line.split() <IF_STMT> continue if l[0:2] == ['remaining', 'capacity:']: remaining = int(l[2]) continue elif l[0:2] == ['present', 'rate:']: rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1",if len(l) < 3:
"def version_iter(q, limit=500, offset=0): q['limit'] = limit q['offset'] = offset while True: url = base_url() + '/version' v = jsonload(url) <IF_STMT> return for i in query(q): yield i q['offset'] += limit",if not v:
"def _letf_btn_press(self, event): try: elem = self.identify(event.x, event.y) index = self.index('@%d,%d' % (event.x, event.y)) <IF_STMT> self.state(['pressed']) self.pressed_index = index except Exception: return",if 'closebutton' in elem:
"def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment if not egg_fragment: continue <IF_STMT> key = '-'.join(egg_fragment.split('-')[:-1]).lower() else: key = egg_fragment if key == dist.key: return url.split('#', 1)[0] return None",if '-' in egg_fragment:
"def viewTreeItemClicked(self, event): if DEBUG: print('viewTreeitemClicked:', event.__dict__, file=sys.stderr) self.unmarkTargets() vuid = self.viewTree.viewTree.identify_row(event.y) if vuid: view = self.vc.viewsById[vuid] <IF_STMT> coords = view.getCoords() if view.isTarget(): self.markTarget(coords[0][0], coords[0][1], coords[1][0], coords[1][1]) self.viewDetails.set(view)",if view:
"def getVar(self, name): value = self.tinfoil.run_command('dataStoreConnectorFindVar', self.dsindex, name) overrides = None if isinstance(value, dict): if '_connector_origtype' in value: value['_content'] = self.tinfoil._reconvert_type(value['_content'], value['_connector_origtype']) del value['_connector_origtype'] <IF_STMT> overrides = value['_connector_overrides'] del value['_connector_overrides'] return (value, overrides)",if '_connector_overrides' in value:
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = [] kwspaces = self.kwspaces striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for idx, obj in enumerate(self.data): if isinstance(obj, NestedSpace): sub_config = _strip_config_space(config, prefix=str(idx)) ret.append(obj.sample(**sub_config)) <IF_STMT> ret.append(config[str(idx)]) else: ret.append(obj) return ret","elif isinstance(obj, SimpleSpace):"
"def main(): for filename in sys.argv[1:]: <IF_STMT> print(filename, 'Directory!') continue with open(filename, 'rb') as f: data = f.read() if b'\x00' in data: print(filename, 'Binary!') continue newdata = data.replace(b'\r\n', b'\n') if newdata != data: print(filename) with open(filename, 'wb') as f: f.write(newdata)",if os.path.isdir(filename):
"def normalize_crlf(tree): for elem in tree.getiterator(): if elem.text: elem.text = elem.text.replace('\r\n', '\n') <IF_STMT> elem.tail = elem.tail.replace('\r\n', '\n')",if elem.tail:
"def RegisterValue(self, value): """"""Puts a given value into an appropriate bin."""""" if self.bins: for b in self.bins: <IF_STMT> b.num += 1 return self.bins[-1].num += 1",if b.range_max_value > value:
"def all_commands(): all_cmds = [] for bp in BINPATHS: cmds = [fn[:-3] for fn in os.listdir(bp) <IF_STMT> and (not fn.startswith('.')) and os.path.isfile(os.path.join(bp, fn))] all_cmds += cmds all_cmds.sort() return all_cmds",if fn.endswith('.py')
"def base64_encode_image_mapper(self, tag, url): if tag == 'img': if url in self.kp_images: image_data = base64.b64encode(self.kp_images[url]) image_mimetype = mimetypes.guess_type(url)[0] <IF_STMT> return 'data:{};base64, '.format(image_mimetype) + image_data.decode('utf-8') return None",if image_mimetype is not None:
"def validate_input(self): if self.validation_fn: success, err = self.validation_fn(self.str) <IF_STMT> spaces = ' ' * self.textwin_width self.textwin.addstr(self.y + 2, 0, spaces) self.textwin.addstr(self.y + 2, 0, err, curses.color_pair(4)) return success else: return True",if not success:
"def start_prompt(self): """"""Start the interpreter."""""" logger.show('Coconut Interpreter:') logger.show(""(type 'exit()' or press Ctrl-D to end)"") self.start_running() while self.running: try: code = self.get_input() if code: compiled = self.handle_input(code) <IF_STMT> self.execute(compiled, use_eval=None) except KeyboardInterrupt: printerr('\nKeyboardInterrupt')",if compiled:
"def __exit__(self, exc_type, exc_val, exc_tb): if self.channel and self.channel.connection: conn_errors = self.channel.connection.client.connection_errors <IF_STMT> try: self.cancel() except Exception: pass","if not isinstance(exc_val, conn_errors):"
"def pack(data, size, endian): buf = [] for i in data: num = int(i) <IF_STMT> num += 1 << size * 8 d = [b'\x00'] * size i = size - 1 while i >= 0: b = num & 255 d[i] = bytes((b,)) if PY3 else chr(b) num >>= 8 i -= 1 if endian == '<': d = b''.join((d[i:i + 1][0] for i in reversed(xrange(len(d))))) else: d = b''.join(d) buf.append(d) return b''.join(buf)",if num < 0:
"def _sample_new_noise_and_add(self, *, tf_sess=None, override=False): if self.framework == 'tf': <IF_STMT> tf_sess.run(self.tf_remove_noise_op) tf_sess.run(self.tf_sample_new_noise_and_add_op) else: if override and self.weights_are_currently_noisy: self._remove_noise() self._sample_new_noise() self._add_stored_noise() self.weights_are_currently_noisy = True",if override and self.weights_are_currently_noisy:
def hdfs_link_js(url): link = 'javascript:void(0)' if url: path = Hdfs.urlsplit(url)[2] if path: link = ('/filebrowser/view=%s' <IF_STMT> else '/filebrowser/home_relative_view=/%s') % path return link,if path.startswith(posixpath.sep)
"def set_xticklabels(self, labels=None, step=None, **kwargs): """"""Set x axis tick labels on the bottom row of the grid."""""" for ax in self.axes[-1, :]: if labels is None: labels = [l.get_text() for l in ax.get_xticklabels()] <IF_STMT> xticks = ax.get_xticks()[::step] labels = labels[::step] ax.set_xticks(xticks) ax.set_xticklabels(labels, **kwargs) return self",if step is not None:
"def _get_statement_from_file(user, fs, snippet): script_path = snippet['statementPath'] if script_path: script_path = script_path.replace('hdfs://', '') <IF_STMT> return fs.do_as_user(user, fs.read, script_path, 0, 16 * 1024 ** 2)","if fs.do_as_user(user, fs.isfile, script_path):"
def doWorkUnit(self): if len(self.workers): try: w = self.workers.popleft() w.next() self.workers.append(w) except StopIteration: <IF_STMT> self.invalidate() else: time.sleep(0.001),"if hasattr(w, 'needsRedraw') and w.needsRedraw:"
"def _find_l1_phash_mul(cdict): candidate_lengths = _find_candidate_lengths_mul(cdict.tuple2int) for p in candidate_lengths: hash_f = hashmul.hashmul_t(p) <IF_STMT> return l1_phash_t(cdict, hash_f) del hash_f return None",if hash_f.is_perfect(iter(cdict.tuple2int.values())):
"def _find_next_tab_stop(self, direction): old_focus = self._focus self._focus += direction while self._focus != old_focus: if self._focus < 0: self._focus = len(self._layouts) - 1 if self._focus >= len(self._layouts): self._focus = 0 try: <IF_STMT> self._layouts[self._focus].focus(force_first=True) else: self._layouts[self._focus].focus(force_last=True) break except IndexError: self._focus += direction",if direction > 0:
"def _get_py_flags(self): res = dict(self.flags) cflags = res.pop('cflags', '') for fl in cflags.split('|'): fl = fl.strip() if fl == 'GA_USE_DOUBLE': res['have_double'] = True <IF_STMT> res['have_small'] = True if fl == 'GA_USE_COMPLEX': res['have_complex'] = True if fl == 'GA_USE_HALF': res['have_half'] = True return res",if fl == 'GA_USE_SMALL':
"def _install_provision_configs(self): config = self._config.plugins[self.full_name] files = config.get('provision_config_files', []) if files: <IF_STMT> log.critical('Error installing provisioning configs') return False else: log.debug('Provision config files successfully installed') return True else: log.debug('No provision config files configured') return True","if not install_provision_configs(files, self._mountpoint):"
"def postfile(self): for clientip, serverips in self.client_conns.items(): target_count = len(serverips) S = min((len(self.server_conns[serverip]) for serverip in serverips)) <IF_STMT> continue self.write('Scanning IP: {} / S score: {:.1f} / Number of records: {}'.format(clientip, S, target_count))",if S > 2 or target_count < 5:
"def update_defaults(self, *values, **kwargs): for value in values: <IF_STMT> self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning('Configuration file {} does not exist.'.format(value)) elif isinstance(value, type(None)): pass else: raise ValueError('Cannot interpret {}'.format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)",if type(value) == dict:
"def __init__(self, aList): for element in aList: <IF_STMT> if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) elif element.text: text = element.text.strip() if text: self.append(text)",if len(element) > 0:
"def _get_py_flags(self): res = dict(self.flags) cflags = res.pop('cflags', '') for fl in cflags.split('|'): fl = fl.strip() <IF_STMT> res['have_double'] = True if fl == 'GA_USE_SMALL': res['have_small'] = True if fl == 'GA_USE_COMPLEX': res['have_complex'] = True if fl == 'GA_USE_HALF': res['have_half'] = True return res",if fl == 'GA_USE_DOUBLE':
"def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() <IF_STMT> break elif isinstance(event, h11.InformationalResponse): continue elif isinstance(event, h11.Response): context['h11_response'] = event raise LoopAbort else: raise RuntimeError('Unexpected h11 event {}'.format(event))",if event is h11.NEED_DATA:
def status_string(self): if not self.live: if self.expired: return _('expired') elif self.approved_schedule: return _('scheduled') elif self.workflow_in_progress: return _('in moderation') else: return _('draft') elif self.approved_schedule: return _('live + scheduled') elif self.workflow_in_progress: return _('live + in moderation') <IF_STMT> return _('live + draft') else: return _('live'),elif self.has_unpublished_changes:
"def _update_input_entries(entries): for entry in entries: comma = entry.get('comma_separated', False) <IF_STMT> entry['regex'] = '([^{}\\[\\]]*)\\{' + entry['regex'] else: entry['regex'] = '([^,{}\\[\\]]*)\\{' + entry['regex'] entry['type'] = 'input'",if comma:
"def get_release(): regexp = re.compile(""^__version__\\W*=\\W*'([\\d.abrc]+)'"") here = os.path.dirname(__file__) root = os.path.dirname(here) init_py = os.path.join(root, 'aiomysql', '__init__.py') with open(init_py) as f: for line in f: match = regexp.match(line) <IF_STMT> return match.group(1) else: raise RuntimeError('Cannot find version in aiomysql/__init__.py')",if match is not None:
"def add_to_auto_transitions(cls, base): result = {} for name, method in base.__dict__.items(): if callable(method) and hasattr(method, '_django_fsm'): for name, transition in method._django_fsm.transitions.items(): <IF_STMT> result.update({name: method}) return result",if transition.custom.get('auto'):
"def _paginate(self, get_page, page_size): for page in itertools.count(start=1): params = {'page': page, 'per_page': page_size} response, items = get_page(params) for item in items: yield item if self._is_last_page(response): break <IF_STMT> break",if len(items) < page_size:
"def forward(self, x): bs = x.size(0) cur = self.stem(x) layers = [cur] for layer_id in range(self.num_layers): cur = self.layers[layer_id](layers) layers.append(cur) <IF_STMT> for i, layer in enumerate(layers): layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)](layer) cur = layers[-1] cur = self.gap(cur).view(bs, -1) cur = self.dropout(cur) logits = self.dense(cur) return logits",if layer_id in self.pool_layers_idx:
"def evaluate(self, x, y, z): vertex = Vector((x, y, z)) nearest, normal, idx, distance = self.bvh.find_nearest(vertex) if self.use_normal: <IF_STMT> sign = (v - nearest).dot(normal) sign = copysign(1, sign) else: sign = 1 return sign * np.array(normal) else: dv = np.array(nearest - vertex) if self.falloff is not None: norm = np.linalg.norm(dv) len = self.falloff(norm) dv = len * dv return dv else: return dv",if self.signed_normal:
"def to_terminal(self): """"""Yield lines to be printed to a terminal."""""" for name, mi in self._sort(self.filtered_results): <IF_STMT> yield (name, (mi['error'],), {'error': True}) continue rank = mi['rank'] color = MI_RANKS[rank] to_show = '' if self.config.show: to_show = ' ({0:.2f})'.format(mi['mi']) yield ('{0} - {1}{2}{3}{4}', (name, color, rank, to_show, RESET), {})",if 'error' in mi:
"def _get_widget_by_name(self, container, name): """"""Recursively search to return the named child widget."""""" LOGGER.log() children = container.get_children() for child in children: if child.name == name: return child if isinstance(child, gtk.Container): found_child = self._get_widget_by_name(child, name) <IF_STMT> return found_child",if found_child:
"def PyJsHoisted_hasComputed_(mutatorMap, this, arguments, var=var): var = Scope({u'this': this, u'arguments': arguments, u'mutatorMap': mutatorMap}, var) var.registers([u'mutatorMap', u'key']) for PyJsTemp in var.get(u'mutatorMap'): var.put(u'key', PyJsTemp) <IF_STMT> return var.get(u'true') return Js(False)",if var.get(u'mutatorMap').get(var.get(u'key')).get(u'_computed'):
"def get_result_json_path(self): if self._result_json_path is None: <IF_STMT> self._result_json_path = get_unique_file(self.path, PARALLEL_RESULT_JSON_PREFIX, PARALLEL_RESULT_JSON_SUFFIX) return self._result_json_path",if self.envconfig.config.option.resultjson:
"def timer(ratio, step, additive): t = 0 slowmode = False while 1: if additive: slowmode |= bool((yield t)) else: slowmode = bool((yield t)) <IF_STMT> t += step * ratio else: t += step",if slowmode:
"def _split_long_text(text, idx, size): splited_text = text.split() if len(splited_text) > 25: if idx == 0: first = '' else: first = ' '.join(splited_text[:10]) <IF_STMT> last = '' else: last = ' '.join(splited_text[-10:]) return '{}(...){}'.format(first, last) return text",if idx != 0 and idx == size - 1:
"def test_tag_priority(self): for tag in _low_priority_D_TAG: val = ENUM_D_TAG[tag] <IF_STMT> for tag2 in ENUM_D_TAG: if tag2 == tag: continue self.assertNotEqual(ENUM_D_TAG[tag2], val)",if _DESCR_D_TAG[val] == tag:
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: <IF_STMT> return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) if isinstance(t1, ptr_class): return t1 elif isinstance(t2, ptr_class): return t2 else: return ptr_class(BottomType()) return n_cls()","if isinstance(t1, ptr_class) and isinstance(t2, ptr_class):"
"def parse(self, html: HTML) -> [ProxyIP]: ip_list: [ProxyIP] = [] for ip_row in html.find('table.proxytbl tr'): ip_element = ip_row.find('td:nth-child(1)', first=True) port_element = ip_row.find('td:nth-child(2)', first=True) try: <IF_STMT> port_str = re.search('//]]> (\\d+)', port_element.text).group(1) p = ProxyIP(ip=ip_element.text, port=port_str) ip_list.append(p) except AttributeError: pass return ip_list",if ip_element and port_element:
"def _reformat(self): document = self.suggestions.document() cursor = self.suggestions.textCursor() block = document.begin() style_format = {self.STYLE_TRANSLATION: self._translation_char_format, self.STYLE_STROKES: self._strokes_char_format} while block != document.end(): style = block.userState() fmt = style_format.get(style) <IF_STMT> cursor.setPosition(block.position()) cursor.select(QTextCursor.BlockUnderCursor) cursor.setCharFormat(fmt) block = block.next()",if fmt is not None:
"def check_uncore_event(e): if uncore_exists(e.unit): <IF_STMT> warn_once('Uncore unit ' + e.unit + ' missing cmask for ' + e.name) return None if e.umask and (not uncore_exists(e.unit, '/format/umask')): warn_once('Uncore unit ' + e.unit + ' missing umask for ' + e.name) return None return e if e.unit not in missing_boxes: warn_once('Uncore unit ' + e.unit + ' missing') missing_boxes.add(e.unit) return None","if e.cmask and (not uncore_exists(e.unit, '/format/cmask')):"
"def check(ip, port, timeout): try: socket.setdefaulttimeout(timeout) s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((ip, int(port))) flag = 'envi' s.send(flag) data = s.recv(1024) s.close() <IF_STMT> return u'Zookeeper Unauthorized access' except: pass",if 'Environment' in data:
"def getid(self): uid = u'' try: filename = self.xmlelement.iterancestors(self.namespaced('file')).next().get('original') <IF_STMT> uid = filename + ID_SEPARATOR except StopIteration: pass uid += unicode(self.xmlelement.get('id') or u'').replace(ID_SEPARATOR_SAFE, ID_SEPARATOR) return uid",if filename:
"def identify(self, vivisect_workspace, function_vas): candidate_functions = {} for fva in function_vas: fname = vivisect_workspace.getName(fva) default_name = 'sub_%.8x' % fva <IF_STMT> self.d('Identified %s at VA 0x%08X ' % (fname, fva)) candidate_functions[fva] = True return candidate_functions",if fname != default_name:
"def nud(self): self.first = [] comma = False if self.token.id != ')': while 1: <IF_STMT> break self.first.append(self.expression()) if self.token.id == ',': comma = True self.advance(',') else: break self.advance(')') if not self.first or comma: return self else: return self.first[0]",if self.token.id == ')':
"def allow_syncdb(self, db, model): for router in self.routers: try: method = router.allow_syncdb except AttributeError: pass else: allow = method(db, model) <IF_STMT> return allow return True",if allow is not None:
def status_string(self): if not self.live: if self.expired: return _('expired') elif self.approved_schedule: return _('scheduled') <IF_STMT> return _('in moderation') else: return _('draft') elif self.approved_schedule: return _('live + scheduled') elif self.workflow_in_progress: return _('live + in moderation') elif self.has_unpublished_changes: return _('live + draft') else: return _('live'),elif self.workflow_in_progress:
"def _on_config_changed(changed_name: str) -> None: """"""Call config_changed hooks if the config changed."""""" for mod_info in _module_infos: if mod_info.skip_hooks: continue for option, hook in mod_info.config_changed_hooks: <IF_STMT> hook() else: cfilter = config.change_filter(option) cfilter.validate() if cfilter.check_match(changed_name): hook()",if option is None:
"def test_slowest_interrupted(self): code = TEST_INTERRUPTED test = self.create_test('sigint', code=code) for multiprocessing in (False, True): with self.subTest(multiprocessing=multiprocessing): <IF_STMT> args = ('--slowest', '-j2', test) else: args = ('--slowest', test) output = self.run_tests(*args, exitcode=130) self.check_executed_tests(output, test, omitted=test, interrupted=True) regex = '10 slowest tests:\n' self.check_line(output, regex)",if multiprocessing:
"def insert_files(self, urls, pos): """"""Not only images"""""" image_extensions = ['.png', '.jpg', '.bmp', '.gif'] for url in urls: <IF_STMT> path = url.path() ext = os.path.splitext(path)[1] if os.path.exists(path) and ext in image_extensions: self._insert_image_from_path(path) else: self.parent.resource_edit.add_attach(path)",if url.scheme() == 'file':
"def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) elif isinstance(arg, Query): accum.append(arg) <IF_STMT> accum.extend(arg.get_proxy_fields()) elif isclass(arg) and issubclass(arg, Model): accum.extend(arg._meta.declared_fields) return accum","elif isinstance(arg, ModelAlias):"
"def get_identifiers(self): ids = [] ifaces = [i['name'] for i in self.middleware.call_sync('interface.query')] for entry in glob.glob(f'{self._base_path}/interface-*'): ident = entry.rsplit('-', 1)[-1] if ident not in ifaces: continue <IF_STMT> ids.append(ident) ids.sort(key=RRDBase._sort_disks) return ids","if os.path.exists(os.path.join(entry, 'if_octets.rrd')):"
"def _validate_required_settings(self, application_id, application_config, required_settings, should_throw=True): """"""All required keys must be present"""""" for setting_key in required_settings: <IF_STMT> if should_throw: raise ImproperlyConfigured(MISSING_SETTING.format(application_id=application_id, setting=setting_key)) else: return False return True",if setting_key not in application_config.keys():
"def digests(): if not OpenVPN.DIGESTS: proc = subprocess.Popen(['openvpn', '--show-digests'], stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate() <IF_STMT> OpenVPN.DIGESTS = {v.split(' ')[0].strip(): v.split(' ', 1)[1].strip() for v in filter(lambda v: v and v.endswith('bit digest size'), stdout.decode('utf8').split('\n'))} return OpenVPN.DIGESTS",if not proc.returncode:
"def iterate_demo_dirs(dir_name, env_name): for env_file_name in glob.glob(os.path.join(dir_name, '**', 'env_id.txt'), recursive=True): with open(env_file_name, 'r', encoding='utf-8') as fd: dir_env_name = fd.readline() <IF_STMT> continue yield os.path.dirname(env_file_name)",if dir_env_name != env_name:
"def validate_rights(namespace): if 'Manage' in namespace.rights: <IF_STMT> raise CLIError(""Error : Assigning 'Manage' to --rights requires 'Listen' and 'Send' to be included with. e.g. --rights Manage Send Listen"")",if 'Listen' not in namespace.rights or 'Send' not in namespace.rights:
"def apply_patches(ctx, patched=False, pre=False): if patched: vendor_dir = _get_patched_dir(ctx) else: vendor_dir = _get_vendor_dir(ctx) log('Applying pre-patches...') patch_dir = Path(__file__).parent / 'patches' / vendor_dir.name if pre: <IF_STMT> pass for patch in patch_dir.glob('*.patch'): if not patch.name.startswith('_post'): apply_patch(ctx, patch) else: patches = patch_dir.glob('*.patch' if not patched else '_post*.patch') for patch in patches: apply_patch(ctx, patch)",if not patched:
"def log_sock(s, event_type=None): if sock_silent: pass el<IF_STMT> logsocket.sendto(ensure_str(s), (host, port)) elif event_type in show_event: logsocket.sendto(ensure_str(s), (host, port)) else: pass",if event_type is None:
"def replace_params(path: str, param_convertors: typing.Dict[str, Convertor], path_params: typing.Dict[str, str]) -> typing.Tuple[str, dict]: for key, value in list(path_params.items()): <IF_STMT> convertor = param_convertors[key] value = convertor.to_string(value) path = path.replace('{' + key + '}', value) path_params.pop(key) return (path, path_params)",if '{' + key + '}' in path:
"def data(self, index: QModelIndex, role=Qt.DisplayRole): if not index.isValid(): return None if role == Qt.DisplayRole or role == Qt.EditRole: i = index.row() j = index.column() fieldtype = self.field_types[i] if j == 0: return fieldtype.caption elif j == 1: return fieldtype.function.name <IF_STMT> return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]",elif j == 2:
"def delta_page(self, x: float=0.0, y: float=0.0) -> None: if y.is_integer(): y = int(y) if y == 0: pass elif y < 0: self.page_up(count=-y) <IF_STMT> self.page_down(count=y) y = 0 if x == 0 and y == 0: return size = self._widget.page().mainFrame().geometry() self.delta(int(x * size.width()), int(y * size.height()))",elif y > 0:
"def _process_symbols(self, tokens): opening_paren = False for index, token, value in tokens: <IF_STMT> token = self.MAPPINGS.get(value, Name.Function) elif token == Literal and value in self.BUILTINS_ANYWHERE: token = Name.Builtin opening_paren = value == '(' and token == Punctuation yield (index, token, value)","if opening_paren and token in (Literal, Name.Variable):"
"def ext_service(self, entity_id, typ, service, binding=None): known_entity = False for key, _md in self.metadata.items(): srvs = _md.ext_service(entity_id, typ, service, binding) <IF_STMT> return srvs elif srvs is None: pass else: known_entity = True if known_entity: raise UnsupportedBinding(binding) else: raise UnknownSystemEntity(entity_id)",if srvs:
"def find_library_nt(name): results = [] for directory in os.environ['PATH'].split(os.pathsep): fname = os.path.join(directory, name) <IF_STMT> results.append(fname) if fname.lower().endswith('.dll'): continue fname = fname + '.dll' if os.path.isfile(fname): results.append(fname) return results",if os.path.isfile(fname):
"def getRemovedFiles(oldContents, newContents, destinationFolder): toRemove = [] for filename in list(oldContents.keys()): if filename not in newContents: destFile = os.path.join(destinationFolder, filename.lstrip('/')) <IF_STMT> toRemove.append(filename) return toRemove",if os.path.isfile(destFile):
"def escapeall(self, lines): """"""Escape all lines in an array according to the output options."""""" result = [] for line in lines: <IF_STMT> line = self.escape(line, EscapeConfig.html) if Options.iso885915: line = self.escape(line, EscapeConfig.iso885915) line = self.escapeentities(line) elif not Options.unicode: line = self.escape(line, EscapeConfig.nonunicode) result.append(line) return result",if Options.html:
"def body(self): order = ['ok_header', 'affected_rows', 'last_insert_id', 'server_status', 'warning_count', 'state_track', 'info'] string = b'' for key in order: item = getattr(self, key) section_pack = b'' if item is None: continue <IF_STMT> section_pack = item else: section_pack = getattr(self, key).toStringPacket() string += section_pack self.setBody(string) return self._body","elif isinstance(item, bytes):"
"def _get_instantiation(self): if self._data is None: f, l, c, o = (c_object_p(), c_uint(), c_uint(), c_uint()) conf.lib.clang_getInstantiationLocation(self, byref(f), byref(l), byref(c), byref(o)) <IF_STMT> f = File(f) else: f = None self._data = (f, int(l.value), int(c.value), int(o.value)) return self._data",if f:
"def analyze_items(items, category_id, agg_data): for item in items: <IF_STMT> agg_data['cat_asp'][category_id] = [] agg_data['cat_asp'][category_id].append(float(item.sellingStatus.currentPrice.value)) if getattr(item.listingInfo, 'watchCount', None): agg_data['watch_count'] += int(item.listingInfo.watchCount) if getattr(item, 'postalCode', None): agg_data['postal_code'] = item.postalCode","if not agg_data['cat_asp'].get(category_id, None):"
"def mock_default_data_dir(tmp_path: pathlib.Path): """"""Changes the default `--data_dir` to tmp_path."""""" tmp_path = tmp_path / 'datasets' default_data_dir = os.environ.get('TFDS_DATA_DIR') try: os.environ['TFDS_DATA_DIR'] = os.fspath(tmp_path) yield tmp_path finally: <IF_STMT> os.environ['TFDS_DATA_DIR'] = default_data_dir else: del os.environ['TFDS_DATA_DIR']",if default_data_dir:
"def has_valid_checksum(self, number): given_number, given_checksum = (number[:-1], number[-1]) calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) <IF_STMT> calculated_checksum += int(fragment[-1]) if parameter == 1: parameter = 7 elif parameter == 3: parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum",if fragment.isalnum():
"def _cleanup_volumes(self, context, instance_id): bdms = self.db.block_device_mapping_get_all_by_instance(context, instance_id) for bdm in bdms: LOG.debug(_('terminating bdm %s') % bdm) <IF_STMT> volume = self.volume_api.get(context, bdm['volume_id']) self.volume_api.delete(context, volume)",if bdm['volume_id'] and bdm['delete_on_termination']:
"def _split_zipped_payload(self, packet_bunch): """"""Split compressed payload"""""" while packet_bunch: <IF_STMT> payload_length = struct.unpack_from('<I', packet_bunch[0:3] + b'\x00')[0] else: payload_length = struct.unpack('<I', packet_bunch[0:3] + b'\x00')[0] self._packet_queue.append(packet_bunch[0:payload_length + 4]) packet_bunch = packet_bunch[payload_length + 4:]",if PY2:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_application_key(d.getPrefixedString()) continue if tt == 18: self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def update_transitive(self, conanfile): transitive = getattr(conanfile, 'python_requires', None) if not transitive: return for name, transitive_py_require in transitive.all_items(): existing = self._pyrequires.get(name) <IF_STMT> raise ConanException('Conflict in py_requires %s - %s' % (existing.ref, transitive_py_require.ref)) self._transitive[name] = transitive_py_require",if existing and existing.ref != transitive_py_require.ref:
"def call(cls, func, *args): try: f = cls._func_cache[func] except KeyError: <IF_STMT> f = cls._func_cache[func] = getattr(vim.funcs, func) else: f = cls._func_cache[func] = vim.Function(func) return f(*args)",if IS_NVIM:
"def __call__(self, *args, **kwargs): if self is S: if args: raise TypeError('S() takes no positional arguments, got: %r' % (args,)) <IF_STMT> raise TypeError('S() expected at least one kwarg, got none') return _t_child(self, '(', (args, kwargs))",if not kwargs:
"def tiles_around_factor(self, factor, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width * factor: for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0): <IF_STMT> ps.append((nx, ny)) return ps","if predicate is None or predicate((nx, ny)):"
"def _plugin_get_requirements(self, requirements_iter): plugin_requirements = {'platform': [], 'python': [], 'network': [], 'native': []} for requirement in requirements_iter: key = requirement[0] values = requirement[1] <IF_STMT> values = [values] if key in plugin_requirements: plugin_requirements[key].extend(values) else: warning('{}={}: No supported requirement'.format(key, values)) return plugin_requirements","if isinstance(values, str) or isinstance(values, bool):"
"def test_engine_api_sdl(sdl, expected, pass_to, clean_registry): from tartiflette import Engine if pass_to == 'engine': e = Engine(sdl) else: e = Engine() if isinstance(expected, Exception): with pytest.raises(Exception): <IF_STMT> await e.cook(sdl) else: await e.cook() else: if pass_to == 'cook': await e.cook(sdl) else: await e.cook() assert e._schema is not None",if pass_to == 'cook':
"def update(self, other_dict, option_parser): if isinstance(other_dict, Values): other_dict = other_dict.__dict__ other_dict = other_dict.copy() for setting in option_parser.lists.keys(): if hasattr(self, setting) and setting in other_dict: value = getattr(self, setting) <IF_STMT> value += other_dict[setting] del other_dict[setting] self._update_loose(other_dict)",if value:
"def _cast_Time(iso, curs): if iso: <IF_STMT> return iso else: return DateTime(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())[:3] + time.strptime(iso[:8], '%H:%M:%S')[3:]))","if iso in ['-infinity', 'infinity']:"
"def _get_default_urlpatterns(self): package_string = '.'.join(self.__module__.split('.')[:-1]) if getattr(self, 'urls', None): try: mod = import_module('.%s' % self.urls, package_string) except ImportError: mod = import_module(self.urls) urlpatterns = mod.urlpatterns el<IF_STMT> urls_mod = import_module('.urls', package_string) urlpatterns = urls_mod.urlpatterns else: urlpatterns = patterns('') return urlpatterns","if module_has_submodule(import_module(package_string), 'urls'):"
"def escape2null(text): """"""Return a string with escape-backslashes converted to nulls."""""" parts = [] start = 0 while 1: found = text.find('\\', start) <IF_STMT> parts.append(text[start:]) return ''.join(parts) parts.append(text[start:found]) parts.append('\x00' + text[found + 1:found + 2]) start = found + 2",if found == -1:
"def check(self, obj): if '*' in self.states: return {'state': self.dispatcher.current_state()} try: state = self.ctx_state.get() except LookupError: chat, user = self.get_target(obj) if chat or user: state = await self.dispatcher.storage.get_state(chat=chat, user=user) self.ctx_state.set(state) <IF_STMT> return {'state': self.dispatcher.current_state(), 'raw_state': state} else: if state in self.states: return {'state': self.dispatcher.current_state(), 'raw_state': state} return False",if state in self.states:
"def get_tokens_unprocessed(self, text): from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name and value in ASYFUNCNAME: token = Name.Function <IF_STMT> token = Name.Variable yield (index, token, value)",elif token is Name and value in ASYVARNAME:
"def write_family_handle(self, family, index=1): sp = '  ' * index self.write_primary_tag('family', family, index) if family: rel = escxml(family.get_relationship().xml_str()) <IF_STMT> self.g.write('  %s<rel type=""%s""/>\n' % (sp, rel))",if rel != '':
"def pop1_bytes(self) -> bytes: if not self.values: raise InsufficientStack('Wanted 1 stack item as bytes, had none') else: item_type, popped = self._pop_typed() if item_type is int: return int_to_big_endian(popped) <IF_STMT> return popped else: raise _busted_type(item_type, popped)",elif item_type is bytes:
"def setDefaultComponents(self): if self._componentTypeLen == self._componentValuesSet: return idx = self._componentTypeLen while idx: idx = idx - 1 <IF_STMT> if self.getComponentByPosition(idx) is None: self.setComponentByPosition(idx) elif not self._componentType[idx].isOptional: if self.getComponentByPosition(idx) is None: raise error.PyAsn1Error('Uninitialized component #%s at %r' % (idx, self))",if self._componentType[idx].isDefaulted:
"def _cloneComponentValues(self, myClone, cloneValueFlag): idx = 0 l = len(self._componentValues) while idx < l: c = self._componentValues[idx] <IF_STMT> if isinstance(c, base.AbstractConstructedAsn1Item): myClone.setComponentByPosition(idx, c.clone(cloneValueFlag=cloneValueFlag)) else: myClone.setComponentByPosition(idx, c.clone()) idx = idx + 1",if c is not None:
"def endElement(self, tag): """"""Handle the end of an element."""""" if tag == 'author': developer = self.text.strip() <IF_STMT> self.author_list.append(developer) elif self.title == 'contributor' and developer not in self.contributor_list: self.contributor_list.append(developer)",if self.title == 'author' and developer not in self.author_list:
"def has_safe_repr(value): """"""Does the node have a safe representation?"""""" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: <IF_STMT> return False return True elif isinstance(value, dict): for key, value in value.iteritems(): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False",if not has_safe_repr(item):
"def test_all_wizards(self): mod = 'w3af.core.controllers.wizard.wizards.%s' w3af_core = w3afCore() for filename in os.listdir('w3af/core/controllers/wizard/wizards/'): wizard_id, ext = os.path.splitext(filename) <IF_STMT> continue klass = mod % wizard_id wizard_inst = factory(klass, w3af_core) yield (self._test_wizard_correct, wizard_inst) wizard_inst = factory(klass, w3af_core) yield (self._test_wizard_fail, wizard_inst)","if wizard_id in ('__init__', '.git') or ext == '.pyc':"
def test_bool_performance(self):  class Person(Document): name = StringField() Person.drop_collection() for i in range(100): Person(name='No: %s' % i).save() with query_counter() as q: <IF_STMT> pass assert q == 1 op = q.db.system.profile.find({'ns': {'$ne': '%s.system.indexes' % q.db.name}})[0] assert op['nreturned'] == 1,if Person.objects:
"def validate(self) -> None: if self.query: <IF_STMT> for arg_name in ('aur', 'repo'): if getattr(self, arg_name): raise MissingArgument('sysupgrade', arg_name)",if not self.sysupgrade:
"def __new__(cls, name, parents, dct): command_handlers = {} for attr_name, attr in dct.items(): <IF_STMT> handles_what = attr_name[len('handle_'):] if handles_what: command_handlers[handles_what] = attr dct['command_handlers'] = command_handlers return super(CommandHandlerMeta, cls).__new__(cls, name, parents, dct)",if callable(attr) and attr_name.startswith('handle_'):
"def pop_error_text(self, error_text): if error_text in self.__errors: self.__errors.remove(error_text) <IF_STMT> self.set_message_text(WELCOME_MESSAGE) else: self.set_message_text(next(self.__errors.__iter__()))",if len(self.__errors) == 0:
"def run(self, edit): self.clear_phantoms() regions = self.view.sel() for region in regions: region, _ = self.get_selection_from_region(region=region, regions_length=len(region), view=self.view) <IF_STMT> continue try: self.json_loads(self.view.substr(region), self.duplicate_key_hook) except Exception as ex: self.show_exception(region=region, msg=ex) return sublime.status_message('JSON Valid')",if region is None:
def update_leaderboard(wait_time): conn = get_connection() cursor = conn.cursor(MySQLdb.cursors.DictCursor) while True: try: <IF_STMT> log.info('Updating leaderboard and adding some sigma') cursor.execute('call generate_leaderboard;') if wait_time == 0: break for s in range(wait_time): time.sleep(1) except KeyboardInterrupt: break except: log.error(traceback.format_exc()) break cursor.close() conn.close(),if use_log:
"def _external_tables(self): tables = [] for name, df in self.extra_options.get('external_tables', {}).items(): <IF_STMT> raise TypeError('External table is not an instance of pandas dataframe') schema = sch.infer(df) chtypes = map(ClickhouseDataType.from_ibis, schema.types) structure = list(zip(schema.names, map(str, chtypes))) tables.append(dict(name=name, data=df.to_dict('records'), structure=structure)) return tables","if not isinstance(df, pd.DataFrame):"
"def getmod(self, nm): mod = None for thing in self.path: <IF_STMT> owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) if owner: mod = owner.getmod(nm) else: mod = thing.getmod(nm) if mod: break return mod","if isinstance(thing, basestring):"
"def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None): for assigned_attribute in variant.attributes.all(): header = f'{assigned_attribute.attribute.slug} (variant attribute)' if str(assigned_attribute.attribute.pk) in attribute_ids: value = get_attribute_value(assigned_attribute) <IF_STMT> data[pk][header] = value else: data[header] = value return data",if pk:
"def get_files(start_dir, includes, excludes): match_files = [] for root, dirs, files in os.walk(start_dir): if not re.search(excludes, root): files = [f for f in files <IF_STMT>] files = [os.path.join(root, f) for f in files] match_files += files else: print(""Excluding '%s'"" % root) return match_files","if re.search(includes, f) and (not re.search(excludes, f))"
"def findinDoc(self, tagpath, pos, end): result = None if end == -1: end = self.docSize else: end = min(self.docSize, end) foundat = -1 for j in range(pos, end): item = self.docList[j] <IF_STMT> name, argres = item.split(b'=', 1) else: name = item argres = '' if isinstance(tagpath, str): tagpath = tagpath.encode('utf-8') if name.endswith(tagpath): result = argres foundat = j break return (foundat, result)",if item.find(b'=') >= 0:
"def load_classes(module, base, blacklist): classes = [] for attr in dir(module): attr = getattr(module, attr) if inspect.isclass(attr): <IF_STMT> if attr is not base and attr not in blacklist: classes.append(attr) return classes","if issubclass(attr, base):"
"def run(): try: result = func() except Exception: future_cell[0] = TracebackFuture() future_cell[0].set_exc_info(sys.exc_info()) else: <IF_STMT> future_cell[0] = result else: future_cell[0] = TracebackFuture() future_cell[0].set_result(result) self.add_future(future_cell[0], lambda future: self.stop())",if is_future(result):
def lastCard(self): if self._answeredIds: <IF_STMT> try: return self.mw.col.getCard(self._answeredIds[-1]) except TypeError: return,if not self.card or self._answeredIds[-1] != self.card.id:
def run(self): global _cameras while 1: for cam in _cameras: <IF_STMT> cam.pygame_buffer = cam.capture.get_image(cam.pygame_buffer) else: cv.GrabFrame(cam.capture) cam._threadcapturetime = time.time() time.sleep(0.04),if cam.pygame_camera:
"def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == 'success': continue _, exc_name, exc_cls, _ = result.thrift_spec[k] <IF_STMT> setattr(result, exc_name, e) return True return False","if isinstance(e, exc_cls):"
"def for_module(cls, modname: str) -> 'ModuleAnalyzer': if ('module', modname) in cls.cache: entry = cls.cache['module', modname] if isinstance(entry, PycodeError): raise entry return entry try: filename, source = cls.get_module_source(modname) <IF_STMT> obj = cls.for_string(source, modname, filename or '<string>') elif filename is not None: obj = cls.for_file(filename, modname) except PycodeError as err: cls.cache['module', modname] = err raise cls.cache['module', modname] = obj return obj",if source is not None:
"def visit_productionlist(self, node): self.new_state() names = [] for production in node: names.append(production['tokenname']) maxlen = max((len(name) for name in names)) for production in node: <IF_STMT> self.add_text(production['tokenname'].ljust(maxlen) + ' ::=') lastname = production['tokenname'] else: self.add_text('%s' % (' ' * len(lastname))) self.add_text(production.astext() + '\n') self.end_state(wrap=False) raise nodes.SkipNode",if production['tokenname']:
"def transport_vmware_guestinfo(): rpctool = 'vmware-rpctool' not_found = None if not subp.which(rpctool): return not_found cmd = [rpctool, 'info-get guestinfo.ovfEnv'] try: out, _err = subp.subp(cmd) <IF_STMT> return out LOG.debug('cmd %s exited 0 with empty stdout: %s', cmd, out) except subp.ProcessExecutionError as e: if e.exit_code != 1: LOG.warning('%s exited with code %d', rpctool, e.exit_code) LOG.debug(e) return not_found",if out:
"def MakeWidthArray(fm): s = '{\n\t' cw = fm['Widths'] for i in xrange(0, 256): if chr(i) == ""'"": s += ""'\\''"" elif chr(i) == '\\': s += ""'\\\\'"" elif i >= 32 and i <= 126: s += ""'"" + chr(i) + ""'"" else: s += 'chr(%d)' % i s += ':' + fm['Widths'][i] if i < 255: s += ',' <IF_STMT> s += '\n\t' s += '}' return s",if (i + 1) % 22 == 0:
"def lookup_config_file(filename: str) -> Optional[str]: """"""Return config file PATH."""""" for path in [find_vcs_root(default='~'), '~']: f = os.path.expanduser('%s/%s' % (path, filename)) <IF_STMT> LOG.info('Found config file %s', f) return f return None",if os.path.isfile(f):
"def do_visual_mode(self): """"""Handle strokes in visual mode."""""" try: self.n1 = self.n = 1 self.do_state(self.vis_dispatch_d, mode_name='visual-line' if self.visual_line_flag else 'visual') <IF_STMT> self.visual_line_helper() except Exception: g.es_exception() self.quit()",if self.visual_line_flag:
"def cleanup(self): log.info('') log.info('Cleaning up.. ') status = self._capture_output('status', '--porcelain') status = status.split('\n') for line in status: filepath = line.split() <IF_STMT> continue filepath = filepath[-1] if filepath[-3:] != 'rej' and filepath[-5:] != 'porig': continue try: log.info('Removing temp file %s ' % filepath) os.remove(os.path.join(self.base_dir, filepath)) except: log.warn('File removal failed, you should manually remove %s' % filepath) pass",if len(filepath) == 0:
"def OnBodyRClick(self, event=None): try: c = self.c p = c.currentPosition() <IF_STMT> c.k.showStateAndMode(w=c.frame.body.bodyCtrl) g.doHook('bodyrclick2', c=c, p=p, v=p, event=event) except: g.es_event_exception('iconrclick')","if not g.doHook('bodyrclick1', c=c, p=p, v=p, event=event):"
"def receiver(): """"""receive messages with polling"""""" pull = ctx.socket(zmq.PULL) pull.connect(url) poller = Poller() poller.register(pull, zmq.POLLIN) while True: events = await poller.poll() <IF_STMT> print('recving', events) msg = await pull.recv_multipart() print('recvd', msg)",if pull in dict(events):
"def sched(self): for k, q in self.q.items(): <IF_STMT> ent = q.popleft() self.cur[k] = ent self.run_one(ent, k)",if q and k not in self.cur:
"def eval_dummy_genomes_iznn(genomes, config): for genome_id, genome in genomes: net = neat.iznn.IZNN.create(genome, config) <IF_STMT> net.reset() genome.fitness = 0.0 elif genome_id <= 150: genome.fitness = 0.5 else: genome.fitness = 1.0",if genome_id < 10:
"def handle_noargs(self, **options): from django.conf import settings, global_settings settings._setup() user_settings = module_to_dict(settings._wrapped) default_settings = module_to_dict(global_settings) output = [] for key in sorted(user_settings.keys()): if key not in default_settings: output.append('%s = %s  ###' % (key, user_settings[key])) <IF_STMT> output.append('%s = %s' % (key, user_settings[key])) return '\n'.join(output)",elif user_settings[key] != default_settings[key]:
def test_get_chat_thread(self): async with self.chat_client: await self._create_thread() get_thread_result = await self.chat_client.get_chat_thread(self.thread_id) assert get_thread_result.id == self.thread_id <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():
def consume(self): if not self.inputState.guessing: c = self.LA(1) <IF_STMT> self.append(c) else: c = self.inputState.input.LA(1) self.append(c) if c and c in '\t': self.tab() else: self.inputState.column += 1 self.inputState.input.consume(),if self.caseSensitive:
"def commandComplete(self, cmd): if self.property: <IF_STMT> return result = self.observer.getStdout() if self.strip: result = result.strip() propname = self.property self.setProperty(propname, result, 'SetPropertyFromCommand Step') self.property_changes[propname] = result else: new_props = self.extract_fn(cmd.rc, self.observer.getStdout(), self.observer.getStderr()) for k, v in iteritems(new_props): self.setProperty(k, v, 'SetPropertyFromCommand Step') self.property_changes = new_props",if cmd.didFail():
"def any(self, provider_name): result = authomatic.login(Webapp2Adapter(self), provider_name) if result: apis = [] if result.user: result.user.update() <IF_STMT> apis = config.config.get(provider_name, {}).get('_apis', {}) nice_provider_name = config.config.get(provider_name, {}).get('_name') or provider_name.capitalize() render(self, result, result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)))",if result.user.credentials:
"def set_lock(self, lock_closed=True, device=0, timeout=0): if self.handle: action = 2 if lock_closed else 1 reply = self.write_register(_R.receiver_pairing, action, device, timeout) <IF_STMT> return True _log.warn('%s: failed to %s the receiver lock', self, 'close' if lock_closed else 'open')",if reply:
"def connect_thread(self, sleep_time=0): time.sleep(sleep_time) try: while self.running and self._need_more_ip(): <IF_STMT> break self.connect_process() finally: self.thread_num_lock.acquire() self.thread_num -= 1 self.thread_num_lock.release()",if self.new_conn_pool.qsize() > self.config.https_connection_pool_max:
"def train_job(guest_party_id, host_party_id, arbiter_party_id, train_conf_path, train_dsl_path): train = TrainSBTModel() train.set_config(guest_party_id, host_party_id, arbiter_party_id, train_conf_path) train.set_dsl(train_dsl_path) status = train.submit() if status: is_success = train.wait_success(timeout=600) <IF_STMT> train.get_component_metrics() train.get_component_output_model() train.get_component_output_data() return train return False",if is_success:
"def get_version(): INIT = os.path.abspath(os.path.join(HERE, '..', 'pyftpdlib', '__init__.py')) with open(INIT, 'r') as f: for line in f: <IF_STMT> ret = eval(line.strip().split(' = ')[1]) assert ret.count('.') == 2, ret for num in ret.split('.'): assert num.isdigit(), ret return ret else: raise ValueError(""couldn't find version string"")",if line.startswith('__ver__'):
"def get_terminus_panel(self, window, visible_only=False): if visible_only: active_panel = window.active_panel() panels = [active_panel] if active_panel else [] else: panels = window.panels() for panel in panels: panel_name = panel.replace('output.', '') if panel_name == EXEC_PANEL: continue panel_view = window.find_output_panel(panel_name) if panel_view: terminal = Terminal.from_id(panel_view.id()) <IF_STMT> return panel_view return None",if terminal:
"def to_internal_value(self, data): site = get_current_site() pages_root = reverse('pages-root') ret = [] for path in data: <IF_STMT> path = path[len(pages_root):] if path.endswith('/'): path = path[:-1] page = get_page_from_path(site, path) if page: ret.append(page) return ret",if path.startswith(pages_root):
"def forward(self, inputs): input_dtype = inputs[0].dtype if self.comm.rank == self.root: <IF_STMT> inputs = tuple([item.astype(numpy.float32) for item in inputs]) y = self.comm.scatter(inputs, self.root) else: y = self.comm.scatter(None, self.root) if numpy.float16 == input_dtype: y = y.astype(input_dtype) return (y,)",if numpy.float16 == input_dtype:
"def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, 'admin'): continue admin_module = import_module('%s.admin' % app.name) <IF_STMT> extension = getattr(admin_module, 'MisagoAdminExtension')() if hasattr(extension, 'register_navigation_nodes'): extension.register_navigation_nodes(site) if hasattr(extension, 'register_urlpatterns'): extension.register_urlpatterns(urlpatterns)","if hasattr(admin_module, 'MisagoAdminExtension'):"
"def overwrite_timeout(initial_node: dict, path: str, hash_: str, size_: int, rsf: bool) -> int: minutes = 10 while minutes > 0: time.sleep(60) minutes -= 1 n = acd_client.get_metadata(initial_node['id']) <IF_STMT> return upload_complete(n, path, hash_, size_, rsf) logger.warning('Timeout while overwriting ""%s"".' % path) return UL_TIMEOUT",if n['version'] > initial_node['version']:
"def write(self, s, spos): if not s: return if not isinstance(s, basestring): s = str(s) slen = self.len if spos == slen: self.len = self.pos = spos + len(s) return if spos > slen: slen = spos newpos = spos + len(s) if spos < slen: if self.buflist: self.buf += ''.join(self.buflist) self.buflist = [self.buf[:spos], s, self.buf[newpos:]] <IF_STMT> slen = newpos else: self.buflist.append(s)",if newpos > slen:
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: if 'title' in child.tag: title = str(child.text) if 'pubDate' in child.tag: pub_date = str(child.text) <IF_STMT> description = str(child.text) print_stdout(color_line(title, 14) + ' (' + bold_line(pub_date) + ')') print_stdout(format_paragraph(strip_tags(description))) print_stdout()",if 'description' in child.tag:
"def get_sequence_type_str(x: Sequence[Any]) -> str: container_type = type(x).__name__ if not x: if container_type == 'list': return '[]' else: return container_type + '([])' elem_type = get_type_str(x[0]) if container_type == 'list': <IF_STMT> return '[' + elem_type + ']' else: return '[' + elem_type + ', ...]' elif len(x) == 1: return f'{container_type}([{elem_type}])' else: return f'{container_type}([{elem_type}, ...])'",if len(x) == 1:
"def signal_notebook_switch_page(self, notebook, current_page, index): if not hasattr(self.parent, 'rpc'): return self.last_page_id = index for tab in self.tabs.values(): if current_page != tab.box: continue <IF_STMT> tab.load_campaign_information(force=False)","if hasattr(tab, 'load_campaign_information'):"
"def format_string(self, templ, args): templ = self.to_native(templ) if isinstance(args, nodes.Arguments): args = args.arguments for i, arg in enumerate(args): arg = self.to_native(self.reduce_single(arg)) <IF_STMT> arg = str(arg).lower() templ = templ.replace('@{}@'.format(i), str(arg)) return templ","if isinstance(arg, bool):"
"def execute_Single(self, object, smooth): if getattr(object, 'type', '') == 'MESH': mesh = object.data <IF_STMT> smoothList = [smooth] * len(mesh.polygons) mesh.polygons.foreach_set('use_smooth', smoothList) mesh.polygons[0].use_smooth = smooth return object",if len(mesh.polygons) > 0:
"def _enumerate_visible_deps(self, dep, predicate): dependencies = sorted((x for x in getattr(dep, 'dependencies', []))) if not self.is_internal_only: dependencies.extend(sorted((x for x in getattr(dep, 'jar_dependencies', [])), key=lambda x: (x.org, x.name, x.rev, x.classifier))) for inner_dep in dependencies: dep_id, internal = self._dep_id(inner_dep) <IF_STMT> yield inner_dep",if predicate(internal):
def stop_test(self): if self.master: self.log.info('Ending cloud test...') if not self._last_status: self.get_master_status() <IF_STMT> self.master.stop() else: self.master.terminate(),if self._last_status['progress'] >= 100:
"def run(self, workspace): """"""Run the module"""""" if self.show_window: m = workspace.get_measurements() x = m.get_current_measurement(self.get_object(), self.x_axis.value) <IF_STMT> x = x[x > self.xbounds.min] x = x[x < self.xbounds.max] workspace.display_data.x = x workspace.display_data.title = '{} (cycle {})'.format(self.title.value, workspace.measurements.image_set_number)",if self.wants_xbounds:
"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz / (np.cast[x.type](1) - sqr(x)),)",if x.type in discrete_types:
"def _which(cls, progname): progname = progname.lower() for p in cls.env.path: for ext in cls._EXTENSIONS: fn = p / (progname + ext) <IF_STMT> return fn return None",if fn.access('x') and (not fn.is_dir()):
"def iterate(self, prod_, rule_): newProduction = '' for i in range(len(prod_)): step = self.production[i] if step == 'W': newProduction = newProduction + self.ruleW elif step == 'X': newProduction = newProduction + self.ruleX <IF_STMT> newProduction = newProduction + self.ruleY elif step == 'Z': newProduction = newProduction + self.ruleZ elif step != 'F': newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction",elif step == 'Y':
"def update(self, mapping, update_only=False): for name in mapping: if update_only and name in self: if hasattr(self[name], 'update'): self[name].update(mapping[name], update_only) continue self.field(name, mapping[name]) if update_only: for name in mapping._meta: <IF_STMT> self._meta[name] = mapping._meta[name] else: self._meta.update(mapping._meta)",if name not in self._meta:
"def Flatten(self, metadata, value_to_flatten): if metadata: self.metadata = metadata for desc in value_to_flatten.type_infos: if desc.name == 'metadata': continue <IF_STMT> setattr(self, desc.name, getattr(value_to_flatten, desc.name))","if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name):"
"def addnode(self, parent, data): print('aaa', data) for i in data: print(i) if i == '-': continue <IF_STMT> item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0]) self.addnode(item, i[1]) else: item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0])","if isinstance(i, tuple):"
"def load_timer(string): if '.' not in string: raise argparse.ArgumentTypeError(""Value for --benchmark-timer must be in dotted form. Eg: 'module.attr'."") mod, attr = string.rsplit('.', 1) if mod == 'pep418': <IF_STMT> import time return NameWrapper(getattr(time, attr)) else: from . import pep418 return NameWrapper(getattr(pep418, attr)) else: __import__(mod) mod = sys.modules[mod] return NameWrapper(getattr(mod, attr))",if PY3:
"def _is_an_attribute(self, pyname): if pyname is not None and isinstance(pyname, pynames.AssignedName): pymodule, lineno = self.pyname.get_definition_location() scope = pymodule.get_scope().get_inner_scope_for_line(lineno) if scope.get_kind() == 'Class': return pyname in list(scope.get_names().values()) parent = scope.parent <IF_STMT> return pyname in list(parent.get_names().values()) return False",if parent is not None and parent.get_kind() == 'Class':
"def _format_arg(self, name, spec, value): if name == 'title': if isinstance(value, bool) and value: return '--title' <IF_STMT> return '--title --title_text %s' % (value,) else: raise ValueError('Unknown value for ""title"" argument: ' + str(value)) return super(Pik, self)._format_arg(name, spec, value)","elif isinstance(value, str):"
"def total_form_count(self): """"""Returns the total number of forms in this FormSet."""""" if self.is_bound: return self.management_form.cleaned_data[TOTAL_FORM_COUNT] else: initial_forms = self.initial_form_count() total_forms = initial_forms + self.extra <IF_STMT> total_forms = initial_forms elif total_forms > self.max_num >= 0: total_forms = self.max_num return total_forms",if initial_forms > self.max_num >= 0:
"def GetTestNamesFromSuites(test_suite): """"""Takes a list of test suites and returns a list of contained test names."""""" suites = [test_suite] test_names = [] while suites: suite = suites.pop() for test in suite: <IF_STMT> suites.append(test) else: test_names.append(test.id()[len('gslib.tests.test_'):]) return test_names","if isinstance(test, unittest.TestSuite):"
"def readArgs(self, node): res = {} for c in self.getChildrenOf(node): val = c.getAttribute('val') <IF_STMT> res[str(c.nodeName)] = self.modules[val] elif val in self.mothers: res[str(c.nodeName)] = self.mothers[val] elif val != '': res[str(c.nodeName)] = eval(val) return res",if val in self.modules:
"def pop(self, k, default=Sentinel): with self._database.transaction(): node, is_single = self.convert_node(k) try: res = self[k] except KeyError: <IF_STMT> raise return default del self[node] return res",if default is Sentinel:
"def wrapped_strategy(self): if self.__wrapped_strategy is None: <IF_STMT> raise InvalidArgument(f'Expected definition to be a function but got {self.__definition!r} of type {type(self.__definition).__name__} instead.') result = self.__definition() if result is self: raise InvalidArgument('Cannot define a deferred strategy to be itself') check_strategy(result, 'definition()') self.__wrapped_strategy = result self.__definition = None return self.__wrapped_strategy",if not inspect.isfunction(self.__definition):
"def _on_fullscreen_requested(self, on): if not config.val.content.fullscreen.window: <IF_STMT> self.state_before_fullscreen = self.windowState() self.setWindowState(Qt.WindowFullScreen | self.state_before_fullscreen) elif self.isFullScreen(): self.setWindowState(self.state_before_fullscreen) log.misc.debug('on: {}, state before fullscreen: {}'.format(on, debug.qflags_key(Qt, self.state_before_fullscreen)))",if on:
"def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): <IF_STMT> self.__defaults_from_file(value) else: logger.warning('Configuration file {} does not exist.'.format(value)) elif isinstance(value, type(None)): pass else: raise ValueError('Cannot interpret {}'.format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)",if os.path.exists(value):
"def clear_output_directory(self): files = os.listdir(os.path.join('functional', 'output')) for f in files: if f in ('README.txt', '.svn', 'CVS'): continue path = os.path.join('functional', 'output', f) <IF_STMT> shutil.rmtree(path) else: os.remove(path)",if os.path.isdir(path):
"def do_remove(self): if self.netconf.locked('dhcp'): <IF_STMT> pid = read_pid_file('/var/run/udhcpd.pan1.pid') else: pid = self.pid if not kill(pid, 'udhcpd'): logging.info('Stale dhcp lockfile found') self.netconf.unlock('dhcp')",if not self.pid:
"def __getattr__(self, attr): if attr.endswith('[]'): searchName = attr[:-2] else: searchName = attr with _lazyLock: nestedClasses = _dependencyMap.get(self.__name__, []) <IF_STMT> return GetVmodlType(self.__name__ + '.' + attr) else: return super(LazyType, self).__getattribute__(attr)",if searchName in nestedClasses:
"def allow_request(self, request, view): request.server = None allow = True view_name = view.get_view_name() allowed_views = [u'System Data', u'Collectd Data', u'Legacy System Data'] if view_name in allowed_views: server_key = view.kwargs.get('server_key') server = server_model.get_server_by_key(server_key) <IF_STMT> request.server = server server_status = throttle_status(server=server) if server_status.allow == False: allow = False return allow",if server:
"def serve_until_stopped(self): import select abort = 0 while not abort: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) <IF_STMT> self.handle_request() logging._acquireLock() abort = self.abort logging._releaseLock()",if rd:
"def A(*args): if len(args) > 0 and hasattr(args[0], '__iter__'): <IF_STMT> return np.array(list(args), dtype=np.float32) else: l = list(args[0]) for e in args[1:]: if hasattr(e, '__iter__'): l.extend(e) else: l.append(e) return np.array(l, dtype=np.float32) return np.array(list(args), dtype=np.float32)",if len(args) == 1:
"def _fix(self): op = [] for k in range(self.size): o = random.choice(self._opts) <IF_STMT> op.append((o, self.rndstr * 1)) else: op.append((o.name, o.randval()._fix())) return op",if type(o) is str:
"def lint_dynamic(self, rule): for file in chain(rule.output, rule.input): <IF_STMT> yield Lint(title='The dynamic flag is deprecated', body='Use checkpoints instead, which are more powerful and less error-prone.', links=[links.checkpoints])","if is_flagged(file, 'dynamic'):"
"def visit(ignored, dir, files): if os.path.basename(dir) not in test_names: for name in test_names: if name + '.py' in files: path = os.path.join(dir, name + '.py') <IF_STMT> results.append(path) return if '__init__.py' not in files: stderr('%s is not a package' % dir) return for file in files: if file.startswith('test') and file.endswith('.py'): path = os.path.join(dir, file) if matcher(path[baselen:]): results.append(path)",if matcher(path[baselen:]):
"def wrapped(*args, **kwargs): try: func(*args, **kwargs) except AssertionError as e: <IF_STMT> time.sleep(t_interval) retry_assertion(interval=t_interval, retries=t_retries - 1)(func)(*args, **kwargs) else: raise e",if retries:
"def num2binary(l, bits=32): all = [] bin = '' for i in range(bits): if l & 1: bin = '1' + bin else: bin = '0' + bin l = l >> 1 <IF_STMT> all.append(bin) bin = '' if bin: all.append(bin) all.reverse() assert l in (0, -1), ""number doesn't fit in number of bits"" return string.join(all, ' ')",if not (i + 1) % 8:
"def closest_enemy_ant(self, row1, col1, filter=None): min_dist = maxint closest_ant = None for ant in self.enemy_ants(): if filter is None or ant not in filter: dist = self.distance(row1, col1, ant[0][0], ant[0][1]) <IF_STMT> min_dist = dist closest_ant = ant[0] return closest_ant",if dist < min_dist:
"def _wrap(cls, parent, value): if isinstance(value, dict): return value <IF_STMT> and all((isinstance(v, six.string_types) for v in value.values())) else cls(value) elif isinstance(value, list): return [cls._wrap(None, v) for v in value] else: return value","if parent in {'annotations', 'labels'}"
"def do_definition(tag): w.end_para() macro('.TP') w.started = True split = 0 pre = [] post = [] for typ, text in _bitlist(tag): <IF_STMT> post.append((typ, text)) elif text.lstrip().startswith(': '): split = 1 post.append((typ, text.lstrip()[2:].lstrip())) else: pre.append((typ, text)) _boldline(pre) w.write(_text(post)) w.started = False",if split:
"def updateTree(self, v, x, y, h, level): yfirst = y if level == 0: yfirst += 10 while v: h, indent = self.updateNode(v, x, y) y += h <IF_STMT> y = self.updateTree(v.firstChild(), x + indent, y, h, level + 1) v = v.next() return y",if v.isExpanded() and v.firstChild():
def loop(self): while True: job = self.check_queue() <IF_STMT> time.sleep(20) continue self.run_job(job) time.sleep(5),if not job:
"def _name_to_variable(self, name): """"""Find the corresponding variable given the specified name."""""" pointer = self for m_name in name.split('.'): <IF_STMT> num = int(m_name) pointer = pointer[num] else: pointer = getattr(pointer, m_name) return pointer",if m_name.isdigit():
"def fetch_cleanup(self): for cell in self.cover_cells: <IF_STMT> log.debug('Removing cover art fetch task for %s', cell.release['musicbrainz_albumid']) self.tagger.webservice.remove_task(cell.fetch_task)",if cell.fetch_task is not None:
"def _get_lcmap_info(self, vol_name): ret_vals = {'fc_id': '', 'fc_name': '', 'lc_map_count': '0'} for lcmap in self._lcmappings_list.values(): <IF_STMT> ret_vals['fc_id'] = lcmap['id'] ret_vals['fc_name'] = lcmap['name'] ret_vals['lc_map_count'] = '1' return ret_vals",if lcmap['source'] == vol_name or lcmap['target'] == vol_name:
"def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) if path is not None: row = self.get(path[0], 'device') if row: <IF_STMT> if self.menu is None: self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)",if self.Blueman is not None:
"def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: if predicate(item): return item <IF_STMT> result = self._find_node_with_predicate(item, predicate) if result: return result item, cookie = self._tree.GetNextChild(node, cookie) return None",if self._tree.ItemHasChildren(item):
"def expect_flow_sequence_item(self): if isinstance(self.event, SequenceEndEvent): self.indent = self.indents.pop() self.flow_level -= 1 if self.canonical: self.write_indicator(u',', False) self.write_indent() self.write_indicator(u']', False) self.state = self.states.pop() else: self.write_indicator(u',', False) <IF_STMT> self.write_indent() self.states.append(self.expect_flow_sequence_item) self.expect_node(sequence=True)",if self.canonical or self.column > self.best_width:
"def iteration(pts): n = len(pts) all_pts = pts + invert(pts) diagram = Voronoi(all_pts) vertices = restrict(diagram.vertices) centers = [] for site_idx in range(n): region_idx = diagram.point_region[site_idx] region = diagram.regions[region_idx] <IF_STMT> site = pts[site_idx] centers.append(site) continue region_verts = np.array([vertices[i] for i in region]) center = weighted_center(region_verts, weight_field) centers.append(tuple(center)) return centers",if -1 in region:
"def retry_call(self, key, f, time_expire, with_lock): self.RETRIES += 1 if self.RETRIES <= self.MAX_RETRIES: <IF_STMT> self.RETRIES = 0 return f() logger.error('sleeping %s seconds before reconnecting' % (2 * self.RETRIES)) time.sleep(2 * self.RETRIES) return self.__call__(key, f, time_expire, with_lock) else: self.RETRIES = 0 if self.fail_gracefully: return f raise RConnectionError('Redis instance is unavailable')",if self.fail_gracefully:
"def load_model(self, model_name: str, path: str=None, model_type=None) -> AbstractModel: if isinstance(model_name, AbstractModel): return model_name if model_name in self.models.keys(): return self.models[model_name] else: <IF_STMT> path = self.get_model_attribute(model=model_name, attribute='path') if model_type is None: model_type = self.get_model_attribute(model=model_name, attribute='type') return model_type.load(path=path, reset_paths=self.reset_paths)",if path is None:
"def _GetPathType(args: rdf_artifacts.ArtifactCollectorFlowArgs, client_os: str) -> rdf_paths.PathSpec.PathType: if args.use_tsk or args.use_raw_filesystem_access: <IF_STMT> return config.CONFIG['Server.raw_filesystem_access_pathtype'] else: return rdf_paths.PathSpec.PathType.TSK else: return rdf_paths.PathSpec.PathType.OS",if client_os == 'Windows':
"def iter_links(self): """"""Yields all links in the page"""""" document = html5lib.parse(self.content, transport_encoding=_get_encoding_from_headers(self.headers), namespaceHTMLElements=False) base_url = _determine_base_url(document, self.url) for anchor in document.findall('.//a'): link = _create_link_from_element(anchor, page_url=self.url, base_url=base_url) <IF_STMT> continue yield link",if link is None:
"def on_leave(self, original_node: CSTNodeT, updated_node: CSTNodeT) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]: if isinstance(updated_node, cst.Import): for alias in updated_node.names: name = alias.name <IF_STMT> return cst.RemoveFromParent() elif isinstance(updated_node, cst.ImportFrom): module = updated_node.module if isinstance(module, cst.Name) and module.value == 'e': return cst.RemoveFromParent() return updated_node","if isinstance(name, cst.Name) and name.value == 'b':"
"def http_request(self, request): ntlm_auth_header = request.get_header(self.auth_header, None) if ntlm_auth_header is None: user, pw = self.passwd.find_user_password(None, request.get_full_url()) <IF_STMT> auth = 'NTLM %s' % ntlm.create_NTLM_NEGOTIATE_MESSAGE(user) request.add_unredirected_header(self.auth_header, auth) return request",if pw is not None:
"def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() <IF_STMT> continue if line.startswith('['): if current_repo: repos.append(current_repo) current_repo = {} current_repo['name'] = line[1:-1] if current_repo and '=' in line: key, value = line.split('=', 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos",if not line or line.startswith('#'):
"def load_as_uint8(filename): image = gdal.Open(filename) image_array = np.array(image.ReadAsArray()) image_uint8 = np.zeros(image_array.shape, dtype=np.uint8) for k, band in enumerate(image_array): band_max = np.max(band) <IF_STMT> band = band.astype(np.float) / band_max * 255.0 image_uint8[k, :, :] = band return image_uint8",if band_max != 0:
"def _get_resource_group_name_of_staticsite(client, static_site_name): static_sites = client.list() for static_site in static_sites: if static_site.name.lower() == static_site_name.lower(): resource_group = _parse_resource_group_from_arm_id(static_site.id) <IF_STMT> return resource_group raise CLIError(""Static site was '{}' not found in subscription."".format(static_site_name))",if resource_group:
"def _translate_trace_addr(self, trace_addr, obj=None): if obj is None: for obj in self._aslr_slides: <IF_STMT> break else: raise Exception(""Can't figure out which object this address belongs to"") if obj not in self._aslr_slides: raise Exception('Internal error: object is untranslated') return trace_addr - self._aslr_slides[obj]",if obj.contains_addr(trace_addr - self._aslr_slides[obj]):
"def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: if len(spec) == 2: event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec <IF_STMT> self._events.register_first(event_name, handler) elif register_type is handlers.REGISTER_LAST: self._events.register_last(event_name, handler)",if register_type is handlers.REGISTER_FIRST:
"def __fixdict(self, dict): for key in dict.keys(): if key[:6] == 'start_': tag = key[6:] start, end = self.elements.get(tag, (None, None)) <IF_STMT> self.elements[tag] = (getattr(self, key), end) elif key[:4] == 'end_': tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = (start, getattr(self, key))",if start is None:
"def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema['properties'].items(): response = 'Test response' items = value['properties']['value'].get('items') enum = value['properties']['value'].get('enum') if items: response = [items['enum'][0]] <IF_STMT> response = enum[0] elif value['properties']['value'].get('properties'): response = {'question': {'value': 'Test Response'}} test_metadata[key] = {'value': response} return test_metadata",elif enum:
"def par_iter_next_batch(self, batch_ms: int): """"""Batches par_iter_next."""""" batch = [] if batch_ms == 0: batch.append(self.par_iter_next()) return batch t_end = time.time() + 0.001 * batch_ms while time.time() < t_end: try: batch.append(self.par_iter_next()) except StopIteration: <IF_STMT> raise StopIteration else: pass return batch",if len(batch) == 0:
"def get_node_map(self, nodes: List[Node], left_node_only=True): node_map = {} idx = 0 for node in nodes: <IF_STMT> continue node_map[node.id] = idx idx += 1 return node_map",if node.id != 0 and (not node.is_left_node and left_node_only):
"def compare_objects(left, right): left_fields = left.map_value.fields right_fields = right.map_value.fields for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)): keyCompare = Order._compare_to(left_key, right_key) <IF_STMT> return keyCompare value_compare = Order.compare(left_fields[left_key], right_fields[right_key]) if value_compare != 0: return value_compare return Order._compare_to(len(left_fields), len(right_fields))",if keyCompare != 0:
"def _resolve_policy_id(cmd, policy, policy_set_definition, client): policy_id = policy or policy_set_definition if not is_valid_resource_id(policy_id): <IF_STMT> policy_def = _get_custom_or_builtin_policy(cmd, client, policy) policy_id = policy_def.id else: policy_set_def = _get_custom_or_builtin_policy(cmd, client, policy_set_definition, None, None, True) policy_id = policy_set_def.id return policy_id",if policy:
"def _passes_cortex_depth(line, min_depth): """"""Do any genotypes in the cortex_var VCF line passes the minimum depth requirement?"""""" parts = line.split('\t') cov_index = parts[8].split(':').index('COV') passes_depth = False for gt in parts[9:]: cur_cov = gt.split(':')[cov_index] cur_depth = sum((int(x) for x in cur_cov.split(','))) <IF_STMT> passes_depth = True return passes_depth",if cur_depth >= min_depth:
"def __init__(self, itemtype, cnf={}, *, master=None, **kw): <IF_STMT> if 'refwindow' in kw: master = kw['refwindow'] elif 'refwindow' in cnf: master = cnf['refwindow'] else: master = tkinter._default_root if not master: raise RuntimeError('Too early to create display style: no root window') self.tk = master.tk self.stylename = self.tk.call('tixDisplayStyle', itemtype, *self._options(cnf, kw))",if not master:
"def serialize_groups_for_summary(node): groups = node.osf_groups n_groups = len(groups) group_string = '' for index, group in enumerate(groups): <IF_STMT> separator = '' elif index == n_groups - 2: separator = ' & ' else: separator = ', ' group_string = group_string + group.name + separator return group_string",if index == n_groups - 1:
"def do(txn): txn.execute('SELECT valid_to, mode, caseset, spec FROM testspec WHERE id = ?', [specId]) row = txn.fetchone() if row is None: raise Exception(""no test specification with ID '%s'"" % specId) else: validTo, mode, caseset, spec = row if validTo is not None: raise Exception('test spec no longer active') <IF_STMT> raise Exception('case set %s not loaded in database' % caseset) spec = json.loads(spec) res = self._css[caseset].generateCasesByTestee(spec) return res",if not self._css.has_key(caseset):
def get_and_set_titles(self): all_titles = [] for page in self.pages: if page.orig_phrase != '': all_titles.append(page.orig_phrase) all_titles.append(page.orig_phrase_norm) <IF_STMT> all_titles.append(page.wiki_title) all_titles.append(page.wiki_title_norm) return set(all_titles),if page.wiki_title != '':
"def spool_print(*args, **kwargs): with _print_lock: <IF_STMT> framework.Framework._spool.write(f'{args[0]}{os.linesep}') framework.Framework._spool.flush() if framework.Framework._mode == Mode.JOB: return builtins._print(*args, **kwargs)",if framework.Framework._spool:
"def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) <IF_STMT> if len(pattern.dirs) <= len(parent_path_dirs): match = pattern.match(os.path.sep.join(parent_path_dirs[:len(pattern.dirs)])) if match: matched = not negative return matched",if not match and parent_path != '':
"def __str__(self, prefix='', printElemNumber=0): res = '' cnt = 0 for e in self.task_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'Task%s {\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '}\n' cnt += 1 return res",if printElemNumber:
"def when(self, matches, context): ret = [] for to_check in matches.range(predicate=lambda match: 'has-neighbor-before' in match.tags): next_match = matches.next(to_check, index=0) next_group = matches.markers.next(to_check, lambda marker: marker.name == 'group', 0) <IF_STMT> next_match = next_group if next_match and (not matches.input_string[to_check.end:next_match.start].strip(seps)): break ret.append(to_check) return ret",if next_group and (not next_match or next_group.start < next_match.start):
def get_coeffs(e): coeffs = [] for du in all_delu_dict.keys(): if type(self.as_coeffs_dict[e]).__name__ == 'float': coeffs.append(self.as_coeffs_dict[e]) <IF_STMT> coeffs.append(self.as_coeffs_dict[e][du]) else: coeffs.append(0) return np.array(coeffs),elif du in self.as_coeffs_dict[e].keys():
"def clean(self): username = self.cleaned_data.get('username') password = self.cleaned_data.get('password') message = ERROR_MESSAGE if username and password: self.user_cache = authenticate(username=username, password=password) if self.user_cache is None: raise ValidationError(message % {'username': self.username_field.verbose_name}) <IF_STMT> raise ValidationError(message % {'username': self.username_field.verbose_name}) return self.cleaned_data",elif not self.user_cache.is_active or not self.user_cache.is_staff:
"def moveFailedFolder(filepath, failed_folder): if config.Config().failed_move(): root_path = str(pathlib.Path(filepath).parent) file_name = pathlib.Path(filepath).name destination_path = root_path + '/' + failed_folder + '/' <IF_STMT> print('[-]Create symlink to Failed output folder') os.symlink(filepath, destination_path + '/' + file_name) else: print('[-]Move to Failed output folder') shutil.move(filepath, destination_path) return",if config.Config().soft_link():
"def test_save_mp3(self, test_mode, bit_rate): if test_mode in ['fileobj', 'bytesio']: <IF_STMT> raise unittest.SkipTest('mp3 format with variable bit rate is known to not yield the exact same result as sox command.') self.assert_save_consistency('mp3', compression=bit_rate, test_mode=test_mode)",if bit_rate is not None and bit_rate < 1:
"def _upstream_nodes_executed(self, node: pipeline_pb2.PipelineNode) -> bool: """"""Returns `True` if all the upstream nodes have been successfully executed."""""" upstream_nodes = [node for node_id, node in self._node_map.items() if node_id in set(node.upstream_nodes)] if not upstream_nodes: return True for node in upstream_nodes: upstream_node_executions = task_gen_utils.get_executions(self._mlmd_handle, node) <IF_STMT> return False return True",if not task_gen_utils.is_latest_execution_successful(upstream_node_executions):
"def reinit(): for name, var in _ns_registry._registry[u'pixie.stdlib']._registry.iteritems(): name = munge(name) <IF_STMT> continue if var.is_defined() and isinstance(var.deref(), BaseCode): globals()[name] = unwrap(var) else: globals()[name] = var",if name in globals():
"def i2repr(self, pkt, x): if type(x) is list or type(x) is tuple: return repr(x) <IF_STMT> r = [] else: r = '' i = 0 while x: if x & 1: if self.multi: r += [self.names[i]] else: r += self.names[i] i += 1 x >>= 1 if self.multi: r = '+'.join(r) return r",if self.multi:
"def prompts_dict(self, *args, **kwargs): r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs) if self.workflow_job: if self.workflow_job.inventory_id: r['inventory'] = self.workflow_job.inventory <IF_STMT> r.update(self.workflow_job.char_prompts) return r",if self.workflow_job.char_prompts:
"def did_evm_write_storage_callback(self, state, address, offset, value): for location, reads in self._get_location_and_reads(state): for address_i, offset_i in reads: if address_i == address: <IF_STMT> self.add_finding(state, *location)",if state.can_be_true(offset == offset_i):
"def update_quality_inspection(self): if self.inspection_required: reference_type = reference_name = '' <IF_STMT> reference_name = self.name reference_type = 'Stock Entry' for d in self.items: if d.quality_inspection: frappe.db.set_value('Quality Inspection', d.quality_inspection, {'reference_type': reference_type, 'reference_name': reference_name})",if self.docstatus == 1:
"def _target(self): <IF_STMT> self.setup.push_thread() try: while self.running: record = self.subscriber.recv() if record: try: self.queue.put(record, timeout=0.05) except Full: pass finally: if self.setup is not None: self.setup.pop_thread()",if self.setup is not None:
def check(self): global MySQLdb import MySQLdb try: args = {} <IF_STMT> args['user'] = mysql_user if mysql_pwd: args['passwd'] = mysql_pwd if mysql_host: args['host'] = mysql_host if mysql_port: args['port'] = mysql_port if mysql_socket: args['unix_socket'] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception('Cannot interface with MySQL server: %s' % e),if mysql_user:
"def writeBool(self, bool): if self.state == BOOL_WRITE: <IF_STMT> ctype = CompactType.TRUE else: ctype = CompactType.FALSE self.__writeFieldHeader(ctype, self.__bool_fid) elif self.state == CONTAINER_WRITE: if bool: self.__writeByte(CompactType.TRUE) else: self.__writeByte(CompactType.FALSE) else: raise AssertionError('Invalid state in compact protocol')",if bool:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def init_panel(self): if not hasattr(self, 'output_view'): <IF_STMT> self.output_view = self.window.create_output_panel('markdown') else: self.output_view = self.window.get_output_panel('markdown')",if is_ST3():
"def sql(self, engine): adapter = get_adapter(engine) tokens = [self.name, adapter.type_to_sql(self.type, self.limit)] for k, v in self.options.items(): result = adapter.column_option_to_sql(self.name, k, v) <IF_STMT> continue elif isinstance(result, dict): self.constraints.append(result['constraint']) else: tokens.append(result) return ' '.join(tokens)",if result is None:
def get_igst_invoices(self): self.igst_invoices = [] for d in self.tax_details: is_igst = True if d[1] in self.gst_accounts.igst_account else False <IF_STMT> self.igst_invoices.append(d[0]),if is_igst and d[0] not in self.igst_invoices:
"def updateParticle(part, best, phi1, phi2): u1 = numpy.random.uniform(0, phi1, len(part)) u2 = numpy.random.uniform(0, phi2, len(part)) v_u1 = u1 * (part.best - part) v_u2 = u2 * (best - part) part.speed += v_u1 + v_u2 for i, speed in enumerate(part.speed): if abs(speed) < part.smin: part.speed[i] = math.copysign(part.smin, speed) <IF_STMT> part.speed[i] = math.copysign(part.smax, speed) part += part.speed",elif abs(speed) > part.smax:
"def summaries_with_matching_keyword(keyword, summary_dir): """"""Yields summary protos matching given keyword from event file."""""" event_paths = tf.io.gfile.glob(os.path.join(summary_dir, 'events*')) for event in tf.compat.v1.train.summary_iterator(event_paths[-1]): <IF_STMT> for value in event.summary.value: if keyword in value.tag: logging.error(event) yield event.summary",if event.summary is not None:
"def _RemoveToken(self, doc_id, token): """"""Removes a token occurrence for a document."""""" if token in self._inverted_index: postings = self._inverted_index[token] postings.Remove(doc_id, token.position) <IF_STMT> del self._inverted_index[token]",if not postings.postings:
"def check_recursive_filters(self, space, name): for the_filter in self.filterdb.get_filters(space): for rule in the_filter.get_rules(): values = list(rule.values()) <IF_STMT> return True return False","if issubclass(rule.__class__, MatchesFilterBase) and name in values:"
"def main(): for filename in sys.argv[1:]: if os.path.isdir(filename): print(filename, 'Directory!') continue with open(filename, 'rb') as f: data = f.read() <IF_STMT> print(filename, 'Binary!') continue newdata = data.replace(b'\r\n', b'\n') if newdata != data: print(filename) with open(filename, 'wb') as f: f.write(newdata)",if b'\x00' in data:
"def fit(self, dataset, intent): self.language = dataset['language'] self.slots_keywords = dict() utterances = dataset['intents'][intent]['utterances'] for utterance in utterances: for chunk in utterance['data']: <IF_STMT> text = chunk['text'] if self.config.get('lowercase', False): text = text.lower() self.slots_keywords[text] = [chunk['entity'], chunk['slot_name']] return self",if 'slot_name' in chunk:
"def linkGradient(self, slaveGradient, connect=True): if connect: fn = lambda g, slave=slaveGradient: slave.restoreState(g.saveState()) self.linkedGradients[id(slaveGradient)] = fn self.sigGradientChanged.connect(fn) self.sigGradientChanged.emit(self) else: fn = self.linkedGradients.get(id(slaveGradient), None) <IF_STMT> self.sigGradientChanged.disconnect(fn)",if fn:
"def _get_field_values(serial_str, field_name): ret_list = [] stream = StringIO(serial_str) for obj_dict in yaml.safe_load(stream): <IF_STMT> field_value = obj_dict['fields'][field_name] if isinstance(field_value, six.string_types): ret_list.append(field_value) else: ret_list.append(str(field_value)) return ret_list",if 'fields' in obj_dict and field_name in obj_dict['fields']:
"def scrapeHeadlines(text): headlines = '' lines = text.splitlines() for line in lines: if string.find(line, '<a href') == 0: pos1 = string.find(line, '<b>') if pos1 > 0: pos2 = string.find(line, '</b>') <IF_STMT> headlines += line[pos1 + len('<b>'):pos2] + '.\n' return headlines",if pos2 > 0:
"def getCVEActions(self, cve, **args): actions = [] for plugin in self.getWebPlugins(): try: actions_ = plugin.getCVEActions(cve, **args) <IF_STMT> for action in actions_: action['auth'] = plugin.requiresAuth action['plugin'] = plugin.getUID() actions.append(action) except Exception as e: print('[!] Plugin %s failed on fetching CVE actions!' % plugin.getName()) print('[!]  -> %s' % e) return actions",if actions_:
"def _sensors_to_fields(oldrec, sensor_map): if oldrec: newrec = dict() for k in sensor_map: <IF_STMT> newrec[k] = oldrec[sensor_map[k]] if newrec: newrec['dateTime'] = oldrec['dateTime'] newrec['usUnits'] = oldrec['usUnits'] return newrec return None",if sensor_map[k] in oldrec:
"def rdd_generator(): while not tf_feed.should_stop(): batch = tf_feed.next_batch(1) <IF_STMT> features = batch['x'][0] label = batch['y_'][0] yield (features, label) else: return",if len(batch['x']) > 0:
"def _get_modules(fn): finder = modulefinder.ModuleFinder() finder.run_script(fn) all = [] for m in finder.modules.values(): if not isinstance(m, modulefinder.Module): continue if not m.__file__: continue if m.__file__.endswith('.so'): continue <IF_STMT> continue all.append(m) return all",if m.__file__.startswith('/Library/Frameworks'):
"def clean(self): d = super().clean() if d['issue_giftcard']: if d['tax_rule'] and d['tax_rule'].rate > 0: self.add_error('tax_rule', _('Gift card products should not be associated with non-zero tax rates since sales tax will be applied when the gift card is redeemed.')) <IF_STMT> self.add_error('admission', _('Gift card products should not be admission products at the same time.')) return d",if d['admission']:
"def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: return True elif name in cls.__dict__: return False elif name in self.get_attr(cls, '__annotations__', {}): return False <IF_STMT> return False return False","elif isinstance(obj, ObjectMember) and obj.class_ is cls:"
"def dictToKW(d): out = [] items = list(d.items()) items.sort() for k, v in items: if not isinstance(k, str): raise NonFormattableDict(""%r ain't a string"" % k) <IF_STMT> raise NonFormattableDict(""%r ain't an identifier"" % k) out.append('\n\x00{}={},'.format(k, prettify(v))) return ''.join(out)",if not r.match(k):
"def report_add_status(torrentlist, succ_cnt, fail_cnt, fail_msgs): if fail_cnt == 0: torrentlist.report_message('Torrents Added', '{!success!}Successfully added %d torrent(s)' % succ_cnt) else: msg = '{!error!}Failed to add the following %d torrent(s):\n {!input!}' % fail_cnt + '\n '.join(fail_msgs) <IF_STMT> msg += '\n \n{!success!}Successfully added %d torrent(s)' % succ_cnt torrentlist.report_message('Torrent Add Report', msg)",if succ_cnt != 0:
"def merge(self, other): d = self._name2ft for name, (f, t) in other._name2ft.items(): <IF_STMT> f2, t2 = d[name] f = f + f2 t = t + t2 d[name] = (f, t)",if name in d:
"def handle_command(self, parameters): response = '' for ip_token in parameters: <IF_STMT> ip = netaddr.IPNetwork(ip_token)[0] if not (ip.is_loopback() or ip.is_private() or ip.is_reserved()): response += '{0} location: {1}\n'.format(ip_token, ip_location(ip_token)) else: response += '{0}: hrm...loopback? private ip?\n'.format(ip_token) else: response = '{0} is not an IP address'.format(ip_token) return response",if is_ip(ip_token):
"def letterrange(first, last, charset): for k in range(len(last)): for x in product(*[chain(charset)] * (k + 1)): result = ''.join(x) <IF_STMT> if first != result: continue else: first = None yield result if result == last: return",if first:
"def artifacts_base_dir(self): if not self._artifacts_base_dir: try: artifacts_base_dir = os.path.abspath(self.get_option(self.SECTION, 'artifacts_base_dir')) except ValidationError: artifacts_base_dir = os.path.abspath('logs') <IF_STMT> os.makedirs(artifacts_base_dir) os.chmod(self.artifacts_base_dir, 493) self._artifacts_base_dir = artifacts_base_dir return self._artifacts_base_dir",if not os.path.exists(artifacts_base_dir):
"def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): if value == ChangeType.REMOVED: <IF_STMT> deletes.append(name) elif name in doc_map: if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)",if name in doc_map:
"def __setattr__(self, name, val): BitmapSprite.__setattr__(self, name, val) if name in ('name', 'size'): <IF_STMT> self.image_data = self.theme.load_icon(self.name, self.size, 0) else: self.image_data = None",if self.__dict__.get('name') and self.__dict__.get('size'):
def extract_deps(file): deps = set() for line in open(file).readlines(): line = line.strip() <IF_STMT> words = line.split() if words[0] == 'import' or (words[0] == 'from' and words[2] == 'import'): deps.add(words[1]) return deps,if line.startswith('import') or line.startswith('from'):
"def run_query(self, query, user): connection = self._get_connection() statement = None error = None try: statement = connection.execute(query) columns = [{'name': n, 'friendly_name': n, 'type': _type_mapper(t)} for n, t in statement.columns().items()] cnames = statement.column_names() rows = [dict(zip(cnames, row)) for row in statement] data = {'columns': columns, 'rows': rows} json_data = json_dumps(data) finally: <IF_STMT> statement.close() connection.close() return (json_data, error)",if statement is not None:
"def find_setup_py_above(a_file): """"""Return the directory containing setup.py somewhere above *a_file*"""""" root = os.path.dirname(os.path.abspath(a_file)) while not os.path.exists(os.path.join(root, 'setup.py')): prev, root = (root, os.path.dirname(root)) <IF_STMT> raise NoSetupPyFound('could not find my setup.py above %r' % (a_file,)) return root",if root == prev:
"def check_index(self, is_sorted=True, unique=True, index=None): """"""Sanity checks"""""" if not index: index = self.index if is_sorted: test = pd.DataFrame(lrange(len(index)), index=index) test_sorted = test.sort() if not test.index.equals(test_sorted.index): raise Exception('Data is not be sorted') if unique: <IF_STMT> raise Exception('Duplicate index entries')",if len(index) != len(index.unique()):
"def _compare_address_strings(self, a, b): a_segments = a.count(':') b_segments = b.count(':') if a_segments and b_segments: <IF_STMT> return True if a.rstrip(':').startswith(b.rstrip(':')) or b.rstrip(':').startswith(a.rstrip(':')): return True if a_segments >= 2 and b_segments >= 2 and (a.split(':')[:2] == b.split(':')[:2]): return True return a.split('.', 1)[-1] == b.split('.', 1)[-1]","if a_segments == b_segments and a_segments in (4, 5, 6, 7):"
"def collect(self): for vacb in self.GetVACBs(): filename = vacb.SharedCacheMap.FileObject.file_name_with_drive() <IF_STMT> yield (vacb, bool(self.kernel_address_space.vtop(vacb.BaseAddress.v())), vacb.BaseAddress.v(), vacb.Overlay.FileOffset.QuadPart, filename)",if filename:
"def _visit_table(self, expr): node = expr.op() if isinstance(expr, ir.TableExpr): base_table = _find_blocking_table(expr) if base_table is not None: base_node = base_table.op() if self._is_root(base_node): pass else: self.foreign_table = expr el<IF_STMT> for arg in node.flat_args(): if isinstance(arg, ir.Expr): self._visit(arg)",if not node.blocks():
def channel_details(self) -> SnapChannelDetails: if self._channel_details is None: channel = self._payload.get('channel') <IF_STMT> raise RuntimeError(f'no channel found for {self._payload!r}') self._channel_details = SnapChannelDetails(channel) return self._channel_details,if channel is None:
"def __setattr__(self, attr, val): if hasattr(self, attr): old = getattr(self, attr) if isinstance(old, Setting): <IF_STMT> raise ValueError('Attempting to reassign setting %s with %s' % (old, val)) log.warn('Setting attr %s via __setattr__ instead of set()!', attr) return old.set(val) log.debug('Setting {%s => %s}' % (attr, val)) return object.__setattr__(self, attr, val)","if isinstance(val, Setting):"
"def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) if start == -1: start = index elif char in BRACKETS: <IF_STMT> return (-1, -1) if stack.pop() != BRACKETS[char]: return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)",if not stack:
"def copy_layer(layer, keep_bias=True, name_template=None, weights=None, reuse_symbolic_tensors=True, **kwargs): config = layer.get_config() if name_template is None: config['name'] = None else: config['name'] = name_template % config['name'] if keep_bias is False and config.get('use_bias', False): config['use_bias'] = False if weights is None: <IF_STMT> weights = layer.weights[:-1] else: weights = layer.get_weights()[:-1] return get_layer_from_config(layer, config, weights=weights, **kwargs)",if reuse_symbolic_tensors:
"def find_go_srcs(path): srcs, tests = ([], []) for name in os.listdir(path): if name.startswith('.') or not name.endswith('.go'): continue if os.path.isfile(os.path.join(path, name)): <IF_STMT> tests.append(name) else: srcs.append(name) return (srcs, tests)",if name.endswith('_test.go'):
"def first_text(self, node): """"""find first paragraph to use as a summary"""""" if node.tagname == 'paragraph': return deepcopy(node) else: for child in node: <IF_STMT> ans = self.first_text(child) if ans: return ans return None","if hasattr(child, 'tagname'):"
def ServerInference(self): candidates = [] score = [] for symbol in self.symbols: for m in symbol.getMessages(): dst = m.getPattern()[0] <IF_STMT> score[candidates.index(dst)] += 1 else: candidates.append(dst) score.append(1) print(candidates) if score.count(max(score)) == 1 and len(candidates) > 2: self.server = candidates[score.index(max(score))],if dst in candidates:
"def generateMapItemTypedNode(self, key, value): if type(value) == SigmaRegularExpressionModifier: regex = str(value) if not (regex.startswith('^') or regex.startswith('.*')): regex = '.*' + regex <IF_STMT> regex = regex + '.*' return '%s MATCHES %s' % (self.cleanKey(key), self.generateValueNode(regex)) else: raise NotImplementedError(""Type modifier '{}' is not supported by backend"".format(value.identifier))",if not (regex.endswith('$') or regex.endswith('.*')):
"def get_max_vertical_scroll() -> int: prev_lineno = ui_content.cursor_position.y used_height = 0 for lineno in range(ui_content.cursor_position.y - 1, -1, -1): used_height += get_line_height(lineno) <IF_STMT> return prev_lineno else: prev_lineno = lineno return prev_lineno",if used_height > scroll_offsets_top:
"def _options_values(self): """"""Simulate option values for partially configured objects."""""" try: return self.__options_values except AttributeError: self.__options_values = {**self.keywords} position = 0 for name, option in self.func.__options__: if not option.positional: break <IF_STMT> continue self.__options_values[name] = self.args[position] if len(self.args) >= position + 1 else None position += 1 return self.__options_values",if name in self.keywords:
"def key(self): addr = self.m('key').obj_offset addr = self.read_ptr(addr) ret = '' if addr: ret = self.obj_vm.read(addr, 256) <IF_STMT> idx = ret.find('\x00') if idx != -1: ret = ret[:idx] else: ret = '' return ret",if ret:
"def get_file_path(self, filepath, token): try: encoded_path, _, user = self.updown_auth_manager.get_resource_info(token) <IF_STMT> logger.info('Invalid path file!! %s: %s' % (user, filepath)) raise NotFoundException('File not found') logger.debug('Get file: user=%s path=%s' % (user, filepath)) file_path = os.path.normpath(os.path.join(self.base_store_folder, encoded_path)) return file_path except (jwt.ExpiredSignature, jwt.DecodeError, AttributeError): raise NotFoundException('File not found')","if not self._valid_path(filepath, encoded_path):"
def validate_and_handle(self): valid = self.validate(set_cursor=True) if valid: <IF_STMT> keep_text = self.accept_handler(self) else: keep_text = False if not keep_text: self.reset(),if self.accept_handler:
"def document_type(self): if isinstance(self.document_type_obj, basestring): <IF_STMT> self.document_type_obj = self.owner_document else: self.document_type_obj = get_document(self.document_type_obj) return self.document_type_obj",if self.document_type_obj == RECURSIVE_REFERENCE_CONSTANT:
"def _get_closest_end(end_after, begin_after): """"""returns the closest \\end, that is open"""""" end_iter = iter(end_after) begin_iter = iter(begin_after) while True: try: e = next(end_iter) except: raise NoEnvError('No closing environment detected') try: b = next(begin_iter) except: break <IF_STMT> break return e",if not e.begin() > b.begin():
"def group_curves(self, curves): result = [[curves[0]]] tolerance = self.concat_tolerance for curve1, curve2 in zip(curves, curves[1:]): _, t_max_1 = curve1.get_u_bounds() t_min_2, _ = curve2.get_u_bounds() end1 = curve1.evaluate(t_max_1) begin2 = curve2.evaluate(t_min_2) distance = np.linalg.norm(begin2 - end1) <IF_STMT> result.append([curve2]) else: result[-1].append(curve2) return result",if distance > tolerance:
"def iteraddcolumn(table, field, col, index, missing): it = iter(table) hdr = next(it) if index is None: index = len(hdr) outhdr = list(hdr) outhdr.insert(index, field) yield tuple(outhdr) for row, val in izip_longest(it, col, fillvalue=missing): <IF_STMT> row = [missing] * len(hdr) outrow = list(row) outrow.insert(index, val) yield tuple(outrow)",if row == missing:
"def validate_is_admin(self, attrs, source): project = attrs.get('project', None if self.object is None else self.object.project) if project is None: return attrs if self.object and self.object.user: <IF_STMT> raise ValidationError(_('The project owner must be admin.')) if not services.project_has_valid_admins(project, exclude_user=self.object.user): raise ValidationError(_('At least one user must be an active admin for this project.')) return attrs",if self.object.user.id == project.owner_id and (not attrs[source]):
"def handle_periodic(self): if self._closed: <IF_STMT> self._eventloop.remove(self._server_socket) self._server_socket.close() self._server_socket = None logging.info('closed TCP port %d', self._listen_port) for handler in list(self._fd_to_handlers.values()): handler.destroy() self._sweep_timeout()",if self._server_socket:
"def get_item(type_, preference): items = {} for item in playlist.findall('./info/%s/item' % type_): lang, label = (xpath_text(item, 'lg', default=None), xpath_text(item, 'label', default=None)) if lang and label: items[lang] = label.strip() for p in preference: <IF_STMT> return items[p]",if items.get(p):
"def save_all_changed_configs(self): """"""Save configuration changes to the user config file."""""" has_changes = False for ext_name in self.extensions: options = self.extensions[ext_name] for opt in options: <IF_STMT> has_changes = True if has_changes: self.userCfg.Save()","if self.set_user_value(ext_name, opt):"
"def extract_validators(namespace: Dict[str, Any]) -> Dict[str, List[Validator]]: validators: Dict[str, List[Validator]] = {} for var_name, value in namespace.items(): validator_config = getattr(value, VALIDATOR_CONFIG_KEY, None) <IF_STMT> fields, v = validator_config for field in fields: if field in validators: validators[field].append(v) else: validators[field] = [v] return validators",if validator_config:
"def _bindTable(self, tableName, create=False): for attempt in retry_azure(): with attempt: try: exists = self.tableService.exists(table_name=tableName) except AzureMissingResourceHttpError as e: if e.status_code != 404: raise else: if exists: return AzureTable(self.tableService, tableName) <IF_STMT> self.tableService.create_table(tableName) return AzureTable(self.tableService, tableName) else: return None",if create:
"def extract(self): for battery in self.vars: for line in dopen('/proc/acpi/battery/' + battery + '/state').readlines(): l = line.split() if len(l) < 3: continue <IF_STMT> remaining = int(l[2]) continue elif l[0:2] == ['present', 'rate:']: rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1","if l[0:2] == ['remaining', 'capacity:']:"
"def merge_syntactic_units(original_units, filtered_units, tags=None): units = [] for i in range(len(original_units)): <IF_STMT> continue text = original_units[i] token = filtered_units[i] tag = tags[i][1] if tags else None sentence = SyntacticUnit(text, token, tag) sentence.index = i units.append(sentence) return units",if filtered_units[i] == '':
"def copy_grads_to_fp32(self, fp16_net, fp32_weights): """"""Copy gradients from fp16 model to fp32 weight copy."""""" for fp32_param, fp16_param in zip(fp32_weights, fp16_net.parameters()): <IF_STMT> if fp32_param.grad is None: fp32_param.grad = fp32_param.data.new(fp32_param.size()) fp32_param.grad.copy_(fp16_param.grad)",if fp16_param.grad is not None:
"def gen_new_segments(datadir, spk_list): if not os.path.isfile(os.path.join(datadir, 'segments')): raise ValueError('no segments file found in datadir') new_segments = open(os.path.join(datadir, 'new_segments'), 'w', encoding='utf-8') segments = open(os.path.join(datadir, 'segments'), 'r', encoding='utf-8') while True: line = segments.readline() if not line: break spk = line.split('_')[0] <IF_STMT> new_segments.write(line) (new_segments.close(), segments.close())",if spk in spk_list:
"def _get_sources(include_per_machine=True, include_per_user=True): if _is_64bit_os(): <IF_STMT> yield (open_source(REGISTRY_SOURCE_CU), None) if include_per_machine: yield (open_source(REGISTRY_SOURCE_LM), '64bit') yield (open_source(REGISTRY_SOURCE_LM_WOW6432), '32bit') else: if include_per_user: yield (open_source(REGISTRY_SOURCE_CU), '32bit') if include_per_machine: yield (open_source(REGISTRY_SOURCE_LM), '32bit')",if include_per_user:
"def AddWindowMenu(self, pMenuBar): if pMenuBar and self._pWindowMenu: pos = pMenuBar.FindMenu(wx.GetStockLabel(wx.ID_HELP, wx.STOCK_NOFLAGS)) <IF_STMT> pMenuBar.Append(self._pWindowMenu, _('&Window')) else: pMenuBar.Insert(pos, self._pWindowMenu, _('&Window'))",if pos == wx.NOT_FOUND:
"def remove(self, res): """"""Remove resource"""""" msg_box = QMessageBox(QMessageBox.Critical, self.app.translate('ResourceEdit', 'Delete Resource'), self.app.translate('ResourceEdit', 'Are you sure want to delete this resource?'), QMessageBox.Yes | QMessageBox.No) ret = msg_box.exec_() if ret == QMessageBox.Yes: self._resources.remove(res) self._resource_labels[res].hide() del self._resource_labels[res] self.on_change() <IF_STMT> self.widget.hide() self.update_label()",if not self._resources:
"def reader(self, myself): ok = True line = '' while True: line = sys.stdin.readline().strip() if ok: <IF_STMT> ok = False continue elif not line: break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)",if not line:
"def _compute_ratios(counts, n_total, multilabel=False): computed_ratios = {} max_count = max(counts.values()) for class_name, count in counts.items(): <IF_STMT> ratio = (n_total - count) / count else: ratio = ratio = max_count / count computed_ratios[class_name] = ratio return computed_ratios",if multilabel:
"def test_tags(context_obj, sagemaker_session): tags = [{'Key': 'foo1', 'Value': 'bar1'}] context_obj.set_tags(tags) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags(ResourceArn=context_obj.context_arn)['Tags'] <IF_STMT> break time.sleep(5) assert len(actual_tags) > 0 assert [actual_tags[-1]] == tags",if actual_tags:
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs <IF_STMT> self._obs_buffer[1] = obs total_reward += reward if done: break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if i == self._skip - 1:
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') if sty.italic: fragment = '<i>%s</i>' % fragment <IF_STMT> fragment = '<u>%s</u>' % fragment if sty.strikeout: fragment = '<s>%s</s>' % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.underline:
"def GetConvertersByClass(value_cls): """"""Returns all converters that take given value as an input value."""""" try: return ExportConverter.converters_cache[value_cls] except KeyError: results = [cls for cls in ExportConverter.classes.values() <IF_STMT>] if not results: results = [DataAgnosticExportConverter] ExportConverter.converters_cache[value_cls] = results return results",if cls.input_rdf_type == value_cls
"def enable(self): """"""enable the patch."""""" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] <IF_STMT> if self.PY2 == SKIP: return if not self.PY2: raise IncompatiblePatch('Python 2 not supported!') if pyv == 3: if self.PY3 == SKIP: return if not self.PY3: raise IncompatiblePatch('Python 3 not supported!') self.pre_enable() self.do_enable() self.enabled = True",if pyv == 2:
def _maybe_uncompress(self): if not self._decompressed: compression_type = self.compression_type if compression_type != self.CODEC_NONE: data = memoryview(self._buffer)[self._pos:] if compression_type == self.CODEC_GZIP: uncompressed = gzip_decode(data) if compression_type == self.CODEC_SNAPPY: uncompressed = snappy_decode(data.tobytes()) <IF_STMT> uncompressed = lz4_decode(data.tobytes()) self._buffer = bytearray(uncompressed) self._pos = 0 self._decompressed = True,if compression_type == self.CODEC_LZ4:
"def transform(node, filename): root = ast.Module(None, node, lineno=1) nodes = [root] while nodes: node = nodes.pop() node.filename = filename <IF_STMT> node.dest = ast.Name('__context') elif node.__class__ is ast.Const and isinstance(node.value, str): try: node.value.decode('ascii') except UnicodeError: node.value = node.value.decode('utf-8') nodes.extend(node.getChildNodes()) return root","if node.__class__ in (ast.Printnl, ast.Print):"
"def __init__(self, json=None): if not json: self._mods = dict() return mods = collections.defaultdict(set) installed_path_patt = re.compile('.*[\\\\/]target[\\\\/]product[\\\\/][^\\\\/]+([\\\\/].*)$') for module in json.values(): for path in module['installed']: match = installed_path_patt.match(path) <IF_STMT> for path in module['path']: mods[match.group(1)].add(path) self._mods = {installed_path: sorted(src_dirs) for installed_path, src_dirs in mods.items()}",if match:
"def _findSubpath(self, path, A, B, inside): print('finding', A, B) sub = None for i in xrange(0, len(path) * 2): j = i % len(path) seg = path[j] if inside.isInside(seg.midPoint()): if eq(seg.A, A): sub = Path('subp') print('seg', sub is None, seg) if sub is not None: sub.append(seg) <IF_STMT> break print('found', sub) return sub","if eq(seg.B, B):"
"def on_click(self, event): button = event['button'] if button in [self.button_next, self.button_previous]: if self.station_data: self.scrolling = True <IF_STMT> self.active_index += 1 elif button == self.button_previous: self.active_index -= 1 self.active_index %= self.count_stations else: self.py3.prevent_refresh() elif button == self.button_refresh: self.idle_time = 0 else: self.py3.prevent_refresh()",if button == self.button_next:
"def __init_subclass__(cls, *, abstract=False): if abstract: return fields = {} for name in cls.__dict__: attr = cls.__dict__[name] <IF_STMT> continue if not isinstance(attr, CType): raise TypeError(f'field {cls.__name__}.{name!r} must be a Type') else: fields[name] = attr cls._fields = fields",if name.startswith('__') or callable(attr):
"def add(self, geom): """"""Add the geometry to this Geometry Collection."""""" if isinstance(geom, OGRGeometry): <IF_STMT> for g in geom: capi.add_geom(self.ptr, g.ptr) else: capi.add_geom(self.ptr, geom.ptr) elif isinstance(geom, six.string_types): tmp = OGRGeometry(geom) capi.add_geom(self.ptr, tmp.ptr) else: raise OGRException('Must add an OGRGeometry.')","if isinstance(geom, self.__class__):"
"def __str__(self): result = [] for x in self._fields_: key = x[0] value = getattr(self, key) fmt = '%s' if key in self._fmt_: fmt = self._fmt_[key] <IF_STMT> fmt = self._fmt_['<default>'] result.append(('%s: ' + fmt) % (key, value)) return self.__class__.__name__ + '(' + string.join(result, ', ') + ')'",elif '<default>' in self._fmt_:
"def add(self, *objs): for obj in objs: <IF_STMT> raise TypeError(""'%s' instance expected, got %r"" % (self.model._meta.object_name, obj)) setattr(obj, rel_field.name, self.instance) obj.save()","if not isinstance(obj, self.model):"
"def _eliminate_deprecated_list_indexing(idx): if not isinstance(idx, tuple): <IF_STMT> if _any((_should_unpack_list_index(i) for i in idx)): idx = tuple(idx) else: idx = (idx,) else: idx = (idx,) return idx","if isinstance(idx, Sequence) and (not isinstance(idx, ndarray)):"
"def __init__(self, parent=None, **kwargs): super(DefaultWidget, self).__init__(parent) self.parent = parent self.FSettings = SuperSettings.getInstance() self.defaultui = [] self.allui = [] self.__tabbyname = {} __defaultui = [ui(parent, self.FSettings) for ui in TabsWidget.__subclasses__()] for ui in __defaultui: <IF_STMT> self.defaultui.append(ui) self.allui.append(ui) self.__tabbyname[ui.Name] = ui setattr(self.__class__, ui.ID, ui)",if not ui.isSubitem:
"def onMouseMove(self, event): x, y = (event.xdata, event.ydata) if x is not None: extra_text = self.getExtraText(x, y) <IF_STMT> self.message('x,y=%5.4e,%5.4e %s' % (x, y, extra_text), index=0) else: self.message('x,y=%5.4e,%5.4e' % (x, y), index=0) else: self.message(None)",if extra_text:
"def tag_configure(self, *args, **keys): if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get('foreground') underline = keys.get('underline') if val: self.configDict[key] = val <IF_STMT> self.configUnderlineDict[key] = True else: g.trace('oops', args, keys)",if underline:
"def _flatten_shape(s, index): if s.is_array(): yield (index, s) else: assert s.is_tuple() for i, sub in enumerate(s.tuple_shapes()): subindex = index + (i,) <IF_STMT> yield from _flatten_shape(sub, subindex) else: yield (subindex, sub)",if sub.is_tuple():
"def delete_if_forked(ghrequest): FORKED = False query = '/user/repos' r = utils.query_request(query) for repo in r.json(): if repo['description']: <IF_STMT> FORKED = True url = f""/repos/{repo['full_name']}"" utils.query_request(url, method='DELETE') return FORKED",if ghrequest.target_repo_fullname in repo['description']:
def update_json(self): n_id = node_id(self) if self.autoreload: self.reload_json() if n_id not in self.json_data and self.current_text: self.reload_json() if n_id not in self.json_data: self.use_custom_color = True self.color = FAIL_COLOR return self.use_custom_color = True self.color = READY_COLOR json_data = self.json_data[n_id] for item in json_data: <IF_STMT> out = json_data[item][1] self.outputs[item].sv_set(out),if item in self.outputs and self.outputs[item].is_linked:
"def _check_num_states(self, num_states): """"""Track the number of states."""""" self._num_states += num_states if self._max_num_states is not None: <IF_STMT> raise RuntimeError('Too many states detected while running dynamic programming: got %d states but upper limit is %d.' % (self._num_states, self._max_num_states))",if self._num_states > self._max_num_states:
def __del__(self): try: <IF_STMT> if self._initialized: _gmp.mpz_clear(self._mpz_p) self._mpz_p = None except AttributeError: pass,if self._mpz_p is not None:
"def cmp(f1, f2): bufsize = 1024 * 8 with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) <IF_STMT> return False if not b1: return True",if b1 != b2:
"def _get_changes(diff): """"""Get a list of changed versions from git."""""" changes_dict = {} for line in diff: <IF_STMT> continue if line.startswith('+++ ') or line.startswith('--- '): continue name, version = parse_versioned_line(line[1:]) if name not in changes_dict: changes_dict[name] = Change(name) if line.startswith('-'): changes_dict[name].old = version elif line.startswith('+'): changes_dict[name].new = version return [change for _name, change in sorted(changes_dict.items())]",if not line.startswith('-') and (not line.startswith('+')):
"def analyze(vw): for va, dest in vw.findPointers(): loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) if offset < 2: continue if bytes[offset - 2:offset] == b'\xff\x15': <IF_STMT> vw.delLocation(va) vw.makeCode(va - 2)",if vw.getLocation(va):
"def match_blanks(self, s, i): if 1: return 0 else: if not self.showInvisibles: return 0 j = i n = len(s) while j < n and s[j] == ' ': j += 1 <IF_STMT> self.colorRangeWithTag(s, i, j, 'blank') return j - i else: return 0",if j > i:
"def compress(self, data_list): if data_list: if data_list[0] in self.empty_values: return None <IF_STMT> raise ValidationError(self.error_messages['invalid_date'], code='invalid_date') result = datetime.datetime.combine(*data_list) return from_current_timezone(result) return None",if data_list[1] in self.empty_values:
"def test_iter_keys(self): for name in ('interfaces', 'addresses', 'neighbours', 'routes', 'rules'): view = getattr(self.ndb, name) for key in view: assert isinstance(key, Record) obj = view.get(key) <IF_STMT> assert isinstance(obj, RTNL_Object)",if obj is not None:
"def has_selenium(): try: from selenium import selenium globals().update(selenium=selenium) sel = selenium(*sel_args) try: sel.do_command('shutdown', '') except Exception as e: <IF_STMT> raise result = True except ImportError: result = SeleniumFailed('selenium RC not installed') except Exception: msg = 'Error occurred initializing selenium: %s' % e result = SeleniumFailed(msg) globals().update(has_selenium=lambda: result) return result",if not 'Server Exception' in str(e):
"def analyze(vw): for va, dest in vw.findPointers(): loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) if offset < 2: continue <IF_STMT> if vw.getLocation(va): vw.delLocation(va) vw.makeCode(va - 2)",if bytes[offset - 2:offset] == b'\xff\x15':
"def get(_kwargs): exception_raised_every_time = True exception = None no_match = True for meter in self.meters: try: match = getattr(meter, func)(_kwargs) except KeyError as e: exception = e else: exception_raised_every_time = False <IF_STMT> selected_meters.append(meter) no_match = False if no_match: raise KeyError(""'No match for {}'"".format(_kwargs)) if exception_raised_every_time and exception is not None: raise exception",if match:
"def derive(self, key_material): if self._used: raise AlreadyFinalized self._used = True if not isinstance(key_material, bytes): raise TypeError('key_material must be bytes.') output = [b''] outlen = 0 counter = 1 while self._length > outlen: h = hashes.Hash(self._algorithm, self._backend) h.update(key_material) h.update(_int_to_u32be(counter)) <IF_STMT> h.update(self._sharedinfo) output.append(h.finalize()) outlen += len(output[-1]) counter += 1 return b''.join(output)[:self._length]",if self._sharedinfo is not None:
"def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = (end, end + size) <IF_STMT> part = gaussian[..., beg:end] elif cat_dim == -2: part = gaussian[..., beg:end, :] elif cat_dim == 1: part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)",if cat_dim == -1:
"def ghci_package_db(self, cabal): if cabal is not None and cabal != 'cabal': package_conf = [pkg for pkg in os.listdir(cabal) if re.match('packages-(.*)\\.conf', pkg)] <IF_STMT> return os.path.join(cabal, package_conf) return None",if package_conf:
"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz / x,)",if x.type in discrete_types:
"def __mro_entries__(self, bases): if self._name: return super().__mro_entries__(bases) if self.__origin__ is Generic: <IF_STMT> return () i = bases.index(self) for b in bases[i + 1:]: if isinstance(b, _BaseGenericAlias) and b is not self: return () return (self.__origin__,)",if Protocol in bases:
"def getvars(request, excludes): getvars = request.GET.copy() excludes = excludes.split(',') for p in excludes: if p in getvars: del getvars[p] <IF_STMT> return '&%s' % getvars.urlencode() else: return ''",if len(getvars.keys()) > 0:
"def check(self): now = time.time() for fn in os.listdir(self.basedir): <IF_STMT> continue absfn = os.path.join(self.basedir, fn) mtime = os.stat(absfn)[stat.ST_MTIME] if now - mtime > self.old: os.remove(absfn)",if fn in self.files:
"def run(self): while 1: gatekeeper.wait() results = [] results.append(self.__queue.get()) while len(results) < self.MAX_SONGS_PER_SUBMISSION: timeout = 0.5 / len(results) try: results.append(self.__queue.get(timeout=timeout)) except queue.Empty: break <IF_STMT> return for lookup_result in self.__process(results): self.__idle(self.__progress_cb, lookup_result) self.__queue.task_done()",if self.__stopped:
"def __getitem__(self, item): if isinstance(item, int): selected_polygons = [self.polygons[item]] elif isinstance(item, slice): selected_polygons = self.polygons[item] else: selected_polygons = [] <IF_STMT> item = item.nonzero() item = item.squeeze(1) if item.numel() > 0 else item item = item.tolist() for i in item: selected_polygons.append(self.polygons[i]) return PolygonList(selected_polygons, size=self.size)","if isinstance(item, torch.Tensor) and item.dtype == torch.uint8:"
def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name if file.is_include_file == True: filename = {} filename[file.name] = {'is_include_file': True} if file.file_type != common_type: <IF_STMT> filename = {} filename[file.name] = {'file_type': file.file_type} files.append(filename) return files,if type(filename) == str:
"def _(node): for __ in dir(node): if not __.startswith('_'): candidate = getattr(node, __) <IF_STMT> if '\\' in candidate: try: re.compile(candidate) except: errMsg = ""smoke test failed at compiling '%s'"" % candidate logger.error(errMsg) raise else: _(candidate)","if isinstance(candidate, str):"
"def _handle_children(self, removed, added): for obj in removed: obj.stop() for obj in added: obj.set(scene=self.scene, parent=self) if isinstance(obj, ModuleManager): obj.source = self elif is_filter(obj): obj.inputs.append(self) <IF_STMT> try: obj.start() except: exception()",if self.running:
"def mean(self): """"""Compute the mean of the value_field in the window."""""" if len(self.data) > 0: datasum = 0 datalen = 0 for dat in self.data: if 'placeholder' not in dat[0]: datasum += dat[1] datalen += 1 <IF_STMT> return datasum / float(datalen) return None else: return None",if datalen > 0:
"def get_master_info(accounts_config, master): master_info = None for a in accounts_config['accounts']: if a['name'] == master: master_info = a break <IF_STMT> master_info = a break if master_info is None: raise ValueError('Master account: %s not found in accounts config' % master) return master_info",if a['account_id'] == master:
def dataset_collector(dataset_collection_description): if dataset_collection_description is DEFAULT_DATASET_COLLECTOR_DESCRIPTION: return DEFAULT_DATASET_COLLECTOR el<IF_STMT> return DatasetCollector(dataset_collection_description) else: return ToolMetadataDatasetCollector(dataset_collection_description),if dataset_collection_description.discover_via == 'pattern':
"def _eliminate_deprecated_list_indexing(idx): if not isinstance(idx, tuple): if isinstance(idx, Sequence) and (not isinstance(idx, ndarray)): <IF_STMT> idx = tuple(idx) else: idx = (idx,) else: idx = (idx,) return idx",if _any((_should_unpack_list_index(i) for i in idx)):
def finalizer(): try: stdout.flush() stderr.flush() finally: time.sleep(0.001) stdout_pipe.stop_writing() stderr_pipe.stop_writing() writer.join(timeout=60) <IF_STMT> raise NailgunStreamWriterError('pantsd timed out while waiting for the stdout/err to finish writing to the socket.'),if writer.isAlive():
"def __init__(self, env, config, scope_infos, option_tracker): scope_infos = sorted(set(list(scope_infos)), key=lambda si: si.scope) self._parser_by_scope = {} for scope_info in scope_infos: scope = scope_info.scope parent_parser = None <IF_STMT> else self._parser_by_scope[enclosing_scope(scope)] self._parser_by_scope[scope] = Parser(env, config, scope_info, parent_parser, option_tracker=option_tracker)",if scope == GLOBAL_SCOPE
"def _load_start_paths(self) -> None: """"""Start the Read-Eval-Print Loop."""""" if self._startup_paths: for path in self._startup_paths: <IF_STMT> with open(path, 'rb') as f: code = compile(f.read(), path, 'exec') exec(code, self.get_globals(), self.get_locals()) else: output = self.app.output output.write('WARNING | File not found: {}\n\n'.format(path))",if os.path.exists(path):
"def validate(leaves): for leaf in leaves: if leaf.has_form(('Rule', 'RuleDelayed'), 2): pass <IF_STMT> if validate(leaf.leaves) is not True: return False else: return False return True","elif leaf.has_form('List', None) or leaf.has_form('Association', None):"
"def add(self, name, value, package=None): if name not in self._data[package]: self._data[package][name] = value el<IF_STMT> if isinstance(value, list): self._data[package][name].extend(value) else: self._data[package][name].append(value)","if isinstance(self._data[package][name], list):"
"def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return '[%s, %s]' % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return '?' elif nfrom.src1 == nto: return 'True' <IF_STMT> return 'False' return ''",elif nfrom.src2 == nto:
def _get_config(key): config = db.session.execute(Configs.__table__.select().where(Configs.key == key)).fetchone() if config and config.value: value = config.value if value and value.isdigit(): return int(value) <IF_STMT> if value.lower() == 'true': return True elif value.lower() == 'false': return False else: return value return KeyError,"elif value and isinstance(value, string_types):"
"def from_rows(cls, rows): subtitles = [] for row in rows: <IF_STMT> subtitles.append(cls.from_row(row)) return subtitles","if row.td.a is not None and row.td.get('class', ['lazy'])[0] != 'empty':"
"def _wx_node(self, parent_node, index, label, with_checkbox): ct_type = 1 if with_checkbox else 0 if index is not None: <IF_STMT> return self.InsertItemByIndex(parent_node, index, label, ct_type=ct_type) else: return self.InsertItem(parent_node, index, label, ct_type=ct_type) return self.AppendItem(parent_node, label, ct_type=ct_type)","if isinstance(index, int):"
"def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split('\n'): line = line.strip() if not line or line.startswith('#') or '.' not in line: continue <IF_STMT> reason = line.split(' # ')[1].split()[0].lower() if reason == 'scanning': continue retval[line.split(' # ')[0]] = (__info__, __reference__) return retval",if ' # ' in line:
"def _remove_event(self, event): i = bisect.bisect(self._eventq, event) while i > 0: i -= 1 e = self._eventq[i] if e.timestamp != event.timestamp: raise exception.EventNotFound(event) <IF_STMT> self._eventq.pop(i) return raise exception.EventNotFound(event)",elif id(e) == id(event):
"def _safe_get_content(self, session, resolve_from): try: resp = session.get(resolve_from, timeout=self._timeout) <IF_STMT> return resp.content raise self.ResolverError('Error status_code={0}'.format(resp.status_code)) except requests.RequestException: raise self.ResolverError('Request error from {0}'.format(resolve_from))",if resp.status_code == requests.codes.ok:
"def splitlines(self, sep=None, replace=None): """"""Return split lines from any file descriptor"""""" for fd in self.fd: fd.seek(0) for line in fd.readlines(): if replace and sep: yield line.replace(replace, sep).split(sep) <IF_STMT> yield line.replace(replace, ' ').split() else: yield line.split(sep)",elif replace:
"def disable_verity(): """"""Disables dm-verity on the device."""""" with log.waitfor('Disabling dm-verity on %s' % context.device): root() with AdbClient() as c: reply = c.disable_verity() if 'Verity already disabled' in reply: return <IF_STMT> reboot(wait=True) elif '0006closed' in reply: return else: log.error('Could not disable verity:\n%s' % reply)",elif 'Now reboot your device' in reply:
"def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if 'value' in msg: if not self.values: pass elif msg['value'] is None: msg['value'] = self.values[0] else: <IF_STMT> idx = indexOf(msg['value'], self.unicode_values) else: idx = indexOf(msg['value'], self.labels) msg['value'] = self._items[self.labels[idx]] msg.pop('options', None) return msg","if isIn(msg['value'], self.unicode_values):"
"def merge(module_name, tree1, tree2): for child in tree2.node: if isinstance(child, ast.Function): replaceFunction(tree1, child.name, child) elif isinstance(child, ast.Assign): replaceAssign(tree1, child.nodes[0].name, child) <IF_STMT> replaceClassMethods(tree1, child.name, child) else: raise TranslationError('Do not know how to merge %s' % child, child, module_name) return tree1","elif isinstance(child, ast.Class):"
"def handle(d: dict): for key, value in d.items(): <IF_STMT> if 'url' not in value: handle(value) else: global count count += 1",if type(value) == dict:
def __stop_loggers(self): if self._console_proc: utils.nuke_subprocess(self._console_proc) utils.nuke_subprocess(self._followfiles_proc) self._console_proc = self._followfile_proc = None <IF_STMT> self.job.warning_loggers.discard(self._logfile_warning_stream) self._logfile_warning_stream.close(),if self.job:
"def unicode_metrics(metrics): for i, metric in enumerate(metrics): for key, value in metric.items(): if isinstance(value, basestring): metric[key] = unicode(value, errors='replace') <IF_STMT> value_list = list(value) for j, value_element in enumerate(value_list): if isinstance(value_element, basestring): value_list[j] = unicode(value_element, errors='replace') metric[key] = tuple(value_list) metrics[i] = metric return metrics","elif isinstance(value, tuple) or isinstance(value, list):"
"def __getitem__(self, idx): if isinstance(idx, slice): start, stop, step = idx.indices(len(self)) return [self._revoked_cert(i) for i in range(start, stop, step)] else: idx = operator.index(idx) <IF_STMT> idx += len(self) if not 0 <= idx < len(self): raise IndexError return self._revoked_cert(idx)",if idx < 0:
"def _get_columns_and_column_names(row): column_names = [] columns = [] duplicate_counter = 1 for i, column_name in enumerate(row): <IF_STMT> column_name = 'column_{}'.format(xl_col_to_name(i)) if column_name in column_names: column_name = '{}{}'.format(column_name, duplicate_counter) duplicate_counter += 1 column_names.append(column_name) columns.append({'name': column_name, 'friendly_name': column_name, 'type': TYPE_STRING}) return (columns, column_names)",if not column_name:
"def format(self, format, dumper, attrib, data): if data: logger.warn('Unexpected data in %s object: %r', attrib['type'], data) try: return ImageGeneratorObjectType.format(self, format, dumper, attrib, data) except ValueError: <IF_STMT> attrib = attrib.copy() attrib['type'] = attrib['type'][6:] return dumper.dump_img(IMAGE, attrib, None)",if attrib['type'].startswith('image+'):
"def handle_facts_wwn(facts): disk_shares = [] for key, wwn in facts.iteritems(): <IF_STMT> continue path = key.replace('wwn_', '') disk_shares.append({'serial_number': normalize_wwn(wwn), 'volume': '/dev/mapper/%s' % path}) return disk_shares",if not key.startswith('wwn_mpath'):
"def _finalize_load(*exc_info): try: success_keys = [k for k in data_keys if k not in failed_keys] <IF_STMT> self._holder_ref.put_objects_by_keys(session_id, success_keys, pin_token=pin_token) if exc_info: raise exc_info[1].with_traceback(exc_info[2]) from None if failed_keys: raise StorageFull(request_size=storage_full_sizes[0], capacity=storage_full_sizes[1], affected_keys=list(failed_keys)) finally: shared_bufs[:] = []",if success_keys:
def _get_base64md5(self): if 'md5' in self.local_hashes and self.local_hashes['md5']: md5 = self.local_hashes['md5'] <IF_STMT> md5 = md5.encode('utf-8') return binascii.b2a_base64(md5).decode('utf-8').rstrip('\n'),"if not isinstance(md5, bytes):"
"def tag_configure(self, *args, **keys): trace = False and (not g.unitTesting) if trace: g.trace(args, keys) if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get('foreground') underline = keys.get('underline') <IF_STMT> self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace('oops', args, keys)",if val:
"def _findSubpath(self, path, A, B, inside): print('finding', A, B) sub = None for i in xrange(0, len(path) * 2): j = i % len(path) seg = path[j] if inside.isInside(seg.midPoint()): <IF_STMT> sub = Path('subp') print('seg', sub is None, seg) if sub is not None: sub.append(seg) if eq(seg.B, B): break print('found', sub) return sub","if eq(seg.A, A):"
"def indent_block(self, cursor): """"""Indent block after enter pressed"""""" at_start_of_line = cursor.positionInBlock() == 0 with self._neditor: cursor.insertBlock() <IF_STMT> indent = self._compute_indent(cursor) if indent is not None: cursor.insertText(indent) return True return False self._neditor.ensureCursorVisible()",if not at_start_of_line:
def checkpoint(): if checkpoint_asserts: self.assert_integrity_idxs_take() if node in self.idxs_memo: toposort(self.idxs_memo[node]) <IF_STMT> for take in self.take_memo[node]: toposort(take),if node in self.take_memo:
"def handle(self, *args, **options): with advisory_lock('send-notifications-command', wait=False) as acquired: <IF_STMT> qs = HistoryChangeNotification.objects.all().order_by('-id') for change_notification in iter_queryset(qs, itersize=100): try: send_sync_notifications(change_notification.pk) except HistoryChangeNotification.DoesNotExist: pass else: print('Other process already running')",if acquired:
"def _parse_version_parts(s): for part in component_re.split(s): part = replace(part, part) if part in ['', '.']: continue <IF_STMT> yield part.zfill(8) else: yield ('*' + part) yield '*final'",if part[:1] in '0123456789':
"def set_password(user_id): try: user = Journalist.query.get(user_id) except NoResultFound: abort(404) password = request.form.get('password') if set_diceware_password(user, password) is not False: <IF_STMT> revoke_token(user, user.last_token) user.session_nonce += 1 db.session.commit() return redirect(url_for('admin.edit_user', user_id=user_id))",if user.last_token is not None:
"def _get_normal_median_depth(normal_counts): depths = [] with open(normal_counts) as in_handle: header = None for line in in_handle: <IF_STMT> header = line.strip().split() elif header: n_vals = dict(zip(header, line.strip().split())) depths.append(int(n_vals['REF_COUNT']) + int(n_vals['ALT_COUNT'])) return np.median(depths)",if header is None and (not line.startswith('@')):
"def _gen_langs_in_db(self): for d in os.listdir(join(self.base_dir, 'db')): if d in self._non_lang_db_dirs: continue lang_path = join(self.base_dir, 'db', d, 'lang') <IF_STMT> log.warn(""unexpected lang-zone db dir without 'lang' file: `%s' (skipping)"" % dirname(lang_path)) continue fin = open(lang_path, 'r') try: lang = fin.read().strip() finally: fin.close() yield lang",if not exists(lang_path):
"def negate(monad): sql = monad.getsql()[0] translator = monad.translator if translator.dialect == 'Oracle': result_sql = ['IS_NULL', sql] else: result_sql = ['EQ', sql, ['VALUE', '']] <IF_STMT> if isinstance(monad, AttrMonad): result_sql = ['OR', result_sql, ['IS_NULL', sql]] else: result_sql = ['EQ', ['COALESCE', sql, ['VALUE', '']], ['VALUE', '']] result = BoolExprMonad(result_sql, nullable=False) result.aggregated = monad.aggregated return result",if monad.nullable:
"def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) elif isinstance(arg, Query): accum.append(arg) elif isinstance(arg, ModelAlias): accum.extend(arg.get_proxy_fields()) <IF_STMT> accum.extend(arg._meta.declared_fields) return accum","elif isclass(arg) and issubclass(arg, Model):"
"def get_hashes_from_fingerprint_with_reason(event, fingerprint): default_values = set(['{{ default }}', '{{default}}']) if any((d in fingerprint for d in default_values)): default_hashes = get_hashes_for_event_with_reason(event) hash_count = len(default_hashes[1]) else: hash_count = 1 hashes = OrderedDict(((bit, []) for bit in fingerprint)) for idx in xrange(hash_count): for bit in fingerprint: <IF_STMT> hashes[bit].append(default_hashes) else: hashes[bit] = bit return hashes.items()",if bit in default_values:
"def default(self, obj): if hasattr(obj, '__json__'): return obj.__json__() elif isinstance(obj, collections.Iterable): return list(obj) elif isinstance(obj, dt.datetime): return obj.isoformat() elif hasattr(obj, '__getitem__') and hasattr(obj, 'keys'): return dict(obj) elif hasattr(obj, '__dict__'): return {member: getattr(obj, member) for member in dir(obj) <IF_STMT> and (not hasattr(getattr(obj, member), '__call__'))} return json.JSONEncoder.default(self, obj)",if not member.startswith('_')
"def get_http_auth(self, name): auth = self._config.get('http-basic.{}'.format(name)) if not auth: username = self._config.get('http-basic.{}.username'.format(name)) password = self._config.get('http-basic.{}.password'.format(name)) if not username and (not password): return None else: username, password = (auth['username'], auth.get('password')) <IF_STMT> password = self.keyring.get_password(name, username) return {'username': username, 'password': password}",if password is None:
"def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) <IF_STMT> continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail('FATAL: bad directory %s in environment variable %s' % (dir, envvar))",if not dir:
"def PARSE_TWO_PARAMS(x, y): """"""used to convert different possible x/y params to a tuple"""""" if y is not None: return (x, y) elif isinstance(x, (list, tuple)): return (x[0], x[1]) else: if isinstance(x, UNIVERSAL_STRING): x = x.strip() <IF_STMT> return [int(w.strip()) for w in x.split(',')] return (x, x)","if ',' in x:"
"def _load_from_sym_dir(self, root): root = os.path.abspath(root) prefix_len = len(root) + 1 for base, _, filenames in os.walk(root): for filename in filenames: <IF_STMT> continue path = os.path.join(base, filename) lib_path = '/' + path[prefix_len:-4] self.add(lib_path, ELF.load_dump(path))",if not filename.endswith('.sym'):
"def is_vertical(self): if not self.isFloating(): par = self.parent() <IF_STMT> return par.dockWidgetArea(self) in (Qt.LeftDockWidgetArea, Qt.RightDockWidgetArea) return self.size().height() > self.size().width()","if par and hasattr(par, 'dockWidgetArea'):"
"def writeBit(self, state, endian): if self._bit_pos == 7: self._bit_pos = 0 if state: <IF_STMT> self._byte |= 1 else: self._byte |= 128 self._output.write(chr(self._byte)) self._byte = 0 else: if state: if endian is BIG_ENDIAN: self._byte |= 1 << self._bit_pos else: self._byte |= 1 << 7 - self._bit_pos self._bit_pos += 1",if endian is BIG_ENDIAN:
"def init(self): self.sock.setblocking(True) if self.parser is None: <IF_STMT> self.sock = ssl.wrap_socket(self.sock, server_side=True, **self.cfg.ssl_options) self.parser = http.RequestParser(self.cfg, self.sock, self.client)",if self.cfg.is_ssl:
"def construct_scalar(self, node): if isinstance(node, MappingNode): for key_node, value_node in node.value: <IF_STMT> return self.construct_scalar(value_node) return super().construct_scalar(node)","if key_node.tag == 'tag:yaml.org,2002:value':"
"def typeNewLine(self, line): if line >= 0: iter = self.buffer.get_iter_at_line(line) <IF_STMT> iter.forward_to_line_end() self.buffer.place_cursor(iter) elif line < 0: iter = self.buffer.get_end_iter() for i in range(line, -1): iter.backward_line() iter.forward_to_line_end() self.buffer.place_cursor(iter) press(self.view, '\n')",if not iter.ends_line():
"def stop(self): """"""Stops the slapd server, and waits for it to terminate"""""" if self._proc is not None: self._log.debug('stopping slapd') <IF_STMT> self._proc.terminate() else: import posix, signal posix.kill(self._proc.pid, signal.SIGHUP) self.wait()","if hasattr(self._proc, 'terminate'):"
"def _listen(self, consumer_id: str) -> AsyncIterable[Any]: try: while True: <IF_STMT> async for msg in self._listen_to_queue(consumer_id): if msg is not None: yield msg await asyncio.sleep(0.5) else: async for msg in self._listen_to_ws(): yield msg except asyncio.CancelledError: pass except Exception as e: raise e",if self._listening:
"def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, 'admin'): continue admin_module = import_module('%s.admin' % app.name) if hasattr(admin_module, 'MisagoAdminExtension'): extension = getattr(admin_module, 'MisagoAdminExtension')() <IF_STMT> extension.register_navigation_nodes(site) if hasattr(extension, 'register_urlpatterns'): extension.register_urlpatterns(urlpatterns)","if hasattr(extension, 'register_navigation_nodes'):"
"def update_job(self, job): if not self.redis.hexists(self.jobs_key, job.id): raise JobLookupError(job.id) with self.redis.pipeline() as pipe: pipe.hset(self.jobs_key, job.id, pickle.dumps(job.__getstate__(), self.pickle_protocol)) <IF_STMT> pipe.zadd(self.run_times_key, {job.id: datetime_to_utc_timestamp(job.next_run_time)}) else: pipe.zrem(self.run_times_key, job.id) pipe.execute()",if job.next_run_time:
"def _get_first_available_entry_node(self) -> Optional[str]: for entry_node in self.entry_nodes: if entry_node not in self.locked_entry_nodes: _, wait_until = self._parse_entry_node(entry_node) now = time.time() <IF_STMT> return entry_node return None",if wait_until <= now:
"def answers(self, other): if not isinstance(other, TCP): return 0 if conf.checkIPsrc: if not (self.sport == other.sport and self.dport == other.dport): return 0 if conf.check_TCPerror_seqack: if self.seq is not None: if self.seq != other.seq: return 0 if self.ack is not None: <IF_STMT> return 0 return 1",if self.ack != other.ack:
"def run(self): if self.check(): path = '/BWT/utils/logs/read_log.jsp?filter=&log=../../../../../../../../..{}'.format(self.filename) response = self.http_request(method='GET', path=path) <IF_STMT> print_success('Exploit success') print_status('Reading file: {}'.format(self.filename)) print_info(response.text) else: print_error('Exploit failed - could not read file') else: print_error('Exploit failed - device seems to be not vulnerable')",if response and response.status_code == 200 and len(response.text):
"def write(self, s): if self.closed: raise ValueError('write to closed file') if type(s) not in (unicode, str, bytearray): if isinstance(s, unicode): s = unicode.__getitem__(s, slice(None)) <IF_STMT> s = str.__str__(s) elif isinstance(s, bytearray): s = bytearray.__str__(s) else: raise TypeError('must be string, not ' + type(s).__name__) return self.shell.write(s, self.tags)","elif isinstance(s, str):"
"def test_checkblock_valid(self): for comment, fHeader, fCheckPoW, cur_time, blk in load_test_vectors('checkblock_valid.json'): try: <IF_STMT> CheckBlockHeader(blk, fCheckPoW=fCheckPoW, cur_time=cur_time) else: CheckBlock(blk, fCheckPoW=fCheckPoW, cur_time=cur_time) except ValidationError as err: self.fail('Failed ""%s"" with error %r' % (comment, err))",if fHeader:
"def _lookup_fqdn(ip): try: return [socket.getfqdn(socket.gethostbyaddr(ip)[0])] except socket.herror as err: <IF_STMT> log.debug('Unable to resolve address %s: %s', ip, err) else: log.error(err_message, err) except (socket.error, socket.gaierror, socket.timeout) as err: log.error(err_message, err)","if err.errno in (0, HOST_NOT_FOUND, NO_DATA):"
"def send_telnet(self, *args: str): try: shell = TelnetShell(self.host) for command in args: <IF_STMT> shell.check_or_download_busybox() shell.run_ftp() else: shell.exec(command) shell.close() except Exception as e: _LOGGER.exception(f'Telnet command error: {e}')",if command == 'ftp':
"def write(path, data, kind='OTHER', dohex=0): asserttype1(data) kind = string.upper(kind) try: os.remove(path) except os.error: pass err = 1 try: <IF_STMT> writelwfn(path, data) elif kind == 'PFB': writepfb(path, data) else: writeother(path, data, dohex) err = 0 finally: if err and (not DEBUG): try: os.remove(path) except os.error: pass",if kind == 'LWFN':
"def ApplyInScriptedSection(self, codeBlock, fn, args): self.BeginScriptedSection() try: try: return self._ApplyInScriptedSection(fn, args) finally: <IF_STMT> self.debugManager.OnLeaveScript() self.EndScriptedSection() except: self.HandleException(codeBlock)",if self.debugManager:
"def _escape_attrib(text): try: if '&' in text: text = text.replace('&', '&amp;') <IF_STMT> text = text.replace('<', '&lt;') if '>' in text: text = text.replace('>', '&gt;') if '""' in text: text = text.replace('""', '&quot;') if '\n' in text: text = text.replace('\n', '&#10;') return text except (TypeError, AttributeError): _raise_serialization_error(text)",if '<' in text:
"def compile_relation(self, method, expr, range_list, negated=False): ranges = [] for item in range_list[1]: <IF_STMT> ranges.append(self.compile(item[0])) else: ranges.append('%s..%s' % tuple(map(self.compile, item))) return '%s%s %s %s' % (self.compile(expr), negated and ' not' or '', method, ','.join(ranges))",if item[0] == item[1]:
def emptyTree(self): for child in self: childObj = child.getObject() del childObj[NameObject('/Parent')] if NameObject('/Next') in childObj: del childObj[NameObject('/Next')] <IF_STMT> del childObj[NameObject('/Prev')] if NameObject('/Count') in self: del self[NameObject('/Count')] if NameObject('/First') in self: del self[NameObject('/First')] if NameObject('/Last') in self: del self[NameObject('/Last')],if NameObject('/Prev') in childObj:
"def connect_to_uri(self, uri, autoconnect=None, do_start=True): try: conn = self._check_conn(uri) if not conn: conn = self.add_conn(uri) if autoconnect is not None: conn.set_autoconnect(bool(autoconnect)) self.show_manager() <IF_STMT> conn.open() return conn except Exception: logging.exception('Error connecting to %s', uri) return None",if do_start:
"def get_expression(self): """"""Return the expression as a printable string."""""" l = [] for c in self.content: if c.op is not None: l.append(c.op) <IF_STMT> l.append('(' + c.child.get_expression() + ')') else: l.append('%d' % c.get_value()) return ''.join(l)",if c.child is not None:
"def to_word_end(view, s): if mode == modes.NORMAL: pt = word_end_reverse(view, s.b, count) return sublime.Region(pt) elif mode in (modes.VISUAL, modes.VISUAL_BLOCK): <IF_STMT> pt = word_end_reverse(view, s.b - 1, count) if pt > s.a: return sublime.Region(s.a, pt + 1) return sublime.Region(s.a + 1, pt) pt = word_end_reverse(view, s.b, count) return sublime.Region(s.a, pt) return s",if s.a < s.b:
"def whichmodule(obj, name): """"""Find the module an object belong to."""""" module_name = getattr(obj, '__module__', None) if module_name is not None: return module_name for module_name, module in sys.modules.copy().items(): <IF_STMT> continue try: if _getattribute(module, name)[0] is obj: return module_name except AttributeError: pass return '__main__'",if module_name == '__main__' or module is None:
"def summarize_scalar_dict(name_data, step, name_scope='Losses/'): if name_data: with tf.name_scope(name_scope): for name, data in name_data.items(): <IF_STMT> tf.compat.v2.summary.scalar(name=name, data=data, step=step)",if data is not None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_content(d.getPrefixedString()) continue if tt == 18: self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue if tt == 32: self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name if file.is_include_file == True: filename = {} filename[file.name] = {'is_include_file': True} <IF_STMT> if type(filename) == str: filename = {} filename[file.name] = {'file_type': file.file_type} files.append(filename) return files,if file.file_type != common_type:
"def data(self, index: QModelIndex, role=Qt.DisplayRole): if not index.isValid(): return None if role == Qt.DisplayRole or role == Qt.EditRole: i = index.row() j = index.column() fieldtype = self.field_types[i] <IF_STMT> return fieldtype.caption elif j == 1: return fieldtype.function.name elif j == 2: return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]",if j == 0:
"def format_coord(x, y): x = int(x + 0.5) y = int(y + 0.5) try: <IF_STMT> return '%s @ %s [%4i, %4i]' % (cur_ax_dat[1][y, x], current, x, y) else: return '%s @ [%4i, %4i]' % (data[y, x], x, y) except IndexError: return ''",if dims:
"def getAllUIExtensions(self): extensions = [] if getExecutionCodeType() == 'MEASURE': text = getMeasurementResultString(self) extensions.append(TextUIExtension(text)) errorType = self.getErrorHandlingType() if errorType in ('MESSAGE', 'EXCEPTION'): data = infoByNode[self.identifier] message = data.errorMessage <IF_STMT> extensions.append(ErrorUIExtension(message)) extraExtensions = self.getUIExtensions() if extraExtensions is not None: extensions.extend(extraExtensions) return extensions",if message is not None and data.showErrorMessage:
"def on_notify(self, notification): subject = notification['subject'] if subject.startswith('remote_recording.'): if 'should_start' in subject and self.online: session_name = notification['session_name'] self.sensor.set_control_value('capture_session_name', session_name) self.sensor.set_control_value('local_capture', True) <IF_STMT> self.sensor.set_control_value('local_capture', False)",elif 'should_stop' in subject:
"def _log_conn_errors(self): if 'connection' in self.event.data: cinfo = self.event.data['connection'] <IF_STMT> err_msg = cinfo.get('error', [None, None])[1] if err_msg: self._log_status(err_msg)",if not cinfo.get('live'):
"def setChanged(self, c, changed): dw = c.frame.top i = self.indexOf(dw) if i < 0: return s = self.tabText(i) s = g.u(s) if len(s) > 2: if changed: <IF_STMT> title = '* ' + s self.setTabText(i, title) elif s.startswith('* '): title = s[2:] self.setTabText(i, title)",if not s.startswith('* '):
"def load_file_in_same_dir(ref_file, filename): """"""Load a given file. Works even when the file is contained inside a zip."""""" from couchpotato.core.helpers.encoding import toUnicode path = split_path(toUnicode(ref_file))[:-1] + [filename] for i, p in enumerate(path): <IF_STMT> zfilename = os.path.join(*path[:i + 1]) zfile = zipfile.ZipFile(zfilename) return zfile.read('/'.join(path[i + 1:])) return u(io.open(os.path.join(*path), encoding='utf-8').read())",if p.endswith('.zip'):
def __mpcReadyInSlaveMode(self): while True: time.sleep(10) if not win32gui.IsWindow(self.__listener.mpcHandle): <IF_STMT> self.callbacks.onMpcClosed(None) break,if self.callbacks.onMpcClosed:
"def _invalidate(self, resource_group_name: str, scale_set_name: str) -> None: with self._lock: <IF_STMT> del self._instance_cache[resource_group_name, scale_set_name] if resource_group_name in self._scale_set_cache: del self._scale_set_cache[resource_group_name] if resource_group_name in self._remaining_instances_cache: del self._remaining_instances_cache[resource_group_name]","if (resource_group_name, scale_set_name) in self._instance_cache:"
"def close(self): if self._serial is not None: try: self._serial.cancel_read() <IF_STMT> self._reading_thread.join() finally: try: self._serial.close() self._serial = None except Exception: logging.exception(""Couldn't close serial"")",if self._reading_thread:
"def channel_sizes(self): """"""List of channel sizes: [(width, height)]."""""" sizes = [] for channel in self.channel_info: <IF_STMT> sizes.append((self.mask_data.width, self.mask_data.height)) elif channel.id == ChannelID.REAL_USER_LAYER_MASK: sizes.append((self.mask_data.real_width, self.mask_data.real_height)) else: sizes.append((self.width, self.height)) return sizes",if channel.id == ChannelID.USER_LAYER_MASK:
def get_module_settings(): included_setting = [] module = DataGetter.get_module() if module is not None: if module.ticket_include: included_setting.append('ticketing') <IF_STMT> included_setting.append('payments') if module.donation_include: included_setting.append('donations') return included_setting,if module.payment_include:
"def _format_block(self, prefix: str, lines: List[str], padding: str=None) -> List[str]: if lines: <IF_STMT> padding = ' ' * len(prefix) result_lines = [] for i, line in enumerate(lines): if i == 0: result_lines.append((prefix + line).rstrip()) elif line: result_lines.append(padding + line) else: result_lines.append('') return result_lines else: return [prefix]",if padding is None:
"def get_task_by_id(events, task_id): if hasattr(Task, '_fields'): return events.state.tasks.get(task_id) else: _fields = Task._defaults.keys() task = events.state.tasks.get(task_id) <IF_STMT> task._fields = _fields return task",if task is not None:
"def check(self, value): try: <IF_STMT> v = value else: v = decimal.Decimal(str(value).replace(self.dot, '.')) return (v, None) except (ValueError, TypeError, decimal.InvalidOperation): return (value, translate(self.message))","if isinstance(value, decimal.Decimal):"
"def check_sales_order_on_hold_or_close(self, ref_fieldname): for d in self.get('items'): <IF_STMT> status = frappe.db.get_value('Sales Order', d.get(ref_fieldname), 'status') if status in ('Closed', 'On Hold'): frappe.throw(_('Sales Order {0} is {1}').format(d.get(ref_fieldname), status))",if d.get(ref_fieldname):
"def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): if k in value: if not nested_match(v, value[k]): return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): <IF_STMT> return False return True return False","if not nested_match(x, y):"
"def test_setup_app_sets_loader(self, app): prev = os.environ.get('CELERY_LOADER') try: cmd = MockCommand(app=app) cmd.setup_app_from_commandline(['--loader=X.Y:Z']) assert os.environ['CELERY_LOADER'] == 'X.Y:Z' finally: <IF_STMT> os.environ['CELERY_LOADER'] = prev else: del os.environ['CELERY_LOADER']",if prev is not None:
"def set_labels_for_constraints(self, constraints): for label in self._constraints_to_label_args(constraints): <IF_STMT> log.info(""setting node '%s' label '%s' to '%s'"", self.name, label.name, label.value) self.label_add(label.name, label.value)",if label not in self.labels:
"def _match(self, byte_chunk): quote_character = None data = byte_chunk.nhtml open_angle_bracket = data.rfind('<') if open_angle_bracket <= data.rfind('>'): return False for s in data[open_angle_bracket + 1:]: if s in ATTR_DELIMITERS: <IF_STMT> quote_character = None continue elif not quote_character: quote_character = s continue if quote_character == self.quote_character: return True return False",if quote_character and s == quote_character:
"def _display_history(config, script, base, head, currents=()): for sc in script.walk_revisions(base=base or 'base', head=head or 'heads'): <IF_STMT> sc._db_current_indicator = sc.revision in currents config.print_stdout(sc.cmd_format(verbose=verbose, include_branches=True, include_doc=True, include_parents=True))",if indicate_current:
"def set(self, key=None, value=None): if key is not None: k = str(key) if value is not None: self.store[k] = value el<IF_STMT> del self.store[k] else: self.store.clear()",if self.store.has_key(k):
"def _finalize_load(*exc_info): try: success_keys = [k for k in data_keys if k not in failed_keys] if success_keys: self._holder_ref.put_objects_by_keys(session_id, success_keys, pin_token=pin_token) <IF_STMT> raise exc_info[1].with_traceback(exc_info[2]) from None if failed_keys: raise StorageFull(request_size=storage_full_sizes[0], capacity=storage_full_sizes[1], affected_keys=list(failed_keys)) finally: shared_bufs[:] = []",if exc_info:
def ignore_module(module): result = False for check in ignore_these: if '/*' in check: if check[:-1] in module: result = True el<IF_STMT> result = True if result: print_warning('Ignoring module: ' + module) return result,if os.getcwd() + '/' + check + '.py' == module:
"def available(self, exception_flag=True): """"""True if the solver is available"""""" if exception_flag is False: return cplex_import_available el<IF_STMT> raise ApplicationError('No CPLEX <-> Python bindings available - CPLEX direct solver functionality is not available') else: return True",if cplex_import_available is False:
"def close(self, checkcount=False): self.mutex.acquire() try: <IF_STMT> self.openers -= 1 if self.openers == 0: self.do_close() else: if self.openers > 0: self.do_close() self.openers = 0 finally: self.mutex.release()",if checkcount:
"def __get__(self, obj, type=None): if obj is None: return self with self.lock: value = obj.__dict__.get(self.__name__, self._default_value) <IF_STMT> value = self.func(obj) obj.__dict__[self.__name__] = value return value",if value is self._default_value:
"def _test_pooling_iteration(input_shape, **kwargs): """"""One iteration of pool operation with given shapes and attributes"""""" x = -np.arange(np.prod(input_shape), dtype=np.float32).reshape(input_shape) - 1 with tf.Graph().as_default(): in_data = array_ops.placeholder(shape=input_shape, dtype='float32') nn_ops.pool(in_data, **kwargs) <IF_STMT> out_name = 'max_pool:0' else: out_name = 'avg_pool:0' compare_tf_with_tvm(x, 'Placeholder:0', out_name)",if kwargs['pooling_type'] == 'MAX':
def updateValue(self): if self._index: val = toInt(self._model.data(self._index)) <IF_STMT> self._updating = True self.setValue(val) self._updating = False,if self.sld.value() != val:
"def _count(self, element, count=True): if not isinstance(element, six.string_types): <IF_STMT> return 1 i = 0 for child in self.children: if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i",if self == element:
"def test_doctests(self): """"""Run tutorial doctests."""""" runner = doctest.DocTestRunner() failures = [] for test in doctest.DocTestFinder().find(TutorialDocTestHolder): failed, success = runner.run(test) <IF_STMT> name = test.name assert name.startswith('TutorialDocTestHolder.doctest_') failures.append(name[30:]) if failures: raise ValueError('%i Tutorial doctests failed: %s' % (len(failures), ', '.join(failures)))",if failed:
"def send_preamble(self): """"""Transmit version/status/date/server, via self._write()"""""" if self.origin_server: if self.client_is_modern(): self._write('HTTP/%s %s\r\n' % (self.http_version, self.status)) <IF_STMT> self._write('Date: %s\r\n' % time.asctime(time.gmtime(time.time()))) if self.server_software and (not self.headers.has_key('Server')): self._write('Server: %s\r\n' % self.server_software) else: self._write('Status: %s\r\n' % self.status)",if not self.headers.has_key('Date'):
"def _verify_unique_measurement_keys(operations: Iterable[ops.Operation]): seen: Set[str] = set() for op in operations: if isinstance(op.gate, ops.MeasurementGate): meas = op.gate key = protocols.measurement_key(meas) <IF_STMT> raise ValueError('Measurement key {} repeated'.format(key)) seen.add(key)",if key in seen:
def test_dtype_basics(df): df['new_virtual_column'] = df.x + 1 for name in df.column_names: <IF_STMT> assert df[name].values.dtype.kind in 'OSU' else: assert df[name].values.dtype == df.dtype(df[name]),if df.dtype(name) == str_type:
"def string_to_points(self, command, coord_string): numbers = string_to_numbers(coord_string) if command.upper() in ['H', 'V']: i = {'H': 0, 'V': 1}[command.upper()] xy = np.zeros((len(numbers), 2)) xy[:, i] = numbers <IF_STMT> xy[:, 1 - i] = self.relative_point[1 - i] elif command.upper() == 'A': raise Exception('Not implemented') else: xy = np.array(numbers).reshape((len(numbers) // 2, 2)) result = np.zeros((xy.shape[0], self.dim)) result[:, :2] = xy return result",if command.isupper():
"def get_count(self, peek=False): if self.argument_supplied: count = self.argument_value if self.argument_negative: if count == 0: count = -1 else: count = -count <IF_STMT> self.argument_negative = False if not peek: self.argument_supplied = False else: count = 1 return count",if not peek:
"def toggleSchedule(self, **kwargs): schedules = cfg.schedules() line = kwargs.get('line') if line: for i, schedule in enumerate(schedules): <IF_STMT> schedule_split = schedule.split() schedule_split[0] = '%d' % (schedule_split[0] == '0') schedules[i] = ' '.join(schedule_split) break cfg.schedules.set(schedules) config.save_config() sabnzbd.Scheduler.restart() raise Raiser(self.__root)",if schedule == line:
"def test_sanity_no_long_entities(CorpusType: Type[ColumnCorpus]): corpus = CorpusType() longest_entity = [] for sentence in corpus.get_all_sentences(): entities = sentence.get_spans('ner') for entity in entities: <IF_STMT> longest_entity = [t.text for t in entity.tokens] assert len(longest_entity) < 10, ' '.join(longest_entity)",if len(entity.tokens) > len(longest_entity):
"def _set_helper(settings, path, value, data_type=None): path = _to_settings_path(path) method = settings.set if data_type is not None: name = None if data_type == bool: name = 'setBoolean' elif data_type == float: name = 'setFloat' <IF_STMT> name = 'setInt' if name is not None: method = getattr(settings, name) method(path, value) settings.save()",elif data_type == int:
"def scan_page(self, address_space, page_offset, fullpage=False): """"""Runs through patchers for a single page"""""" if fullpage: pagedata = address_space.read(page_offset, PAGESIZE) for patcher in self.patchers: for offset, data in patcher.get_constraints(): if fullpage: testdata = pagedata[offset:offset + len(data)] else: testdata = address_space.read(page_offset + offset, len(data)) <IF_STMT> break else: yield patcher",if data != testdata:
"def accessSlice(self, node): self.visit(node.value) node.obj = self.getObj(node.value) self.access = _access.INPUT lower, upper = (node.slice.lower, node.slice.upper) if lower: self.visit(lower) if upper: self.visit(upper) if isinstance(node.obj, intbv): <IF_STMT> self.require(lower, 'Expected leftmost index') leftind = self.getVal(lower) if upper: rightind = self.getVal(upper) else: rightind = 0 node.obj = node.obj[leftind:rightind]",if self.kind == _kind.DECLARATION:
"def childConnectionLost(self, childFD): if self.state == 1: self.fail('got connectionLost(%d) during state 1' % childFD) return if self.state == 2: <IF_STMT> self.fail('got connectionLost(%d) (not 4) during state 2' % childFD) return self.state = 3 self.transport.closeChildFD(5) return",if childFD != 4:
"def _find_matches(self, file, lookup, **kwargs): matches = [] for format in lookup.values(): <IF_STMT> is_format, skwargs = format.sniffer_function(file, **kwargs) file.seek(0) if is_format: matches.append((format.name, skwargs)) return matches",if format.sniffer_function is not None:
"def ParseCodeLines(tokens, case): """"""Parse uncommented code in a test case."""""" _, kind, item = tokens.peek() <IF_STMT> raise ParseError('Expected a line of code (got %r, %r)' % (kind, item)) code_lines = [] while True: _, kind, item = tokens.peek() if kind != PLAIN_LINE: case['code'] = '\n'.join(code_lines) + '\n' return code_lines.append(item) tokens.next()",if kind != PLAIN_LINE:
"def _recursive_process(self): super(RecursiveObjectDownwardsVisitor, self)._recursive_process() while self._new_for_visit: func_ea, arg_idx = self._new_for_visit.pop() <IF_STMT> continue cfunc = helper.decompile_function(func_ea) if cfunc: assert arg_idx < len(cfunc.get_lvars()), 'Wrong argument at func {}'.format(to_hex(func_ea)) obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx) self.prepare_new_scan(cfunc, arg_idx, obj) self._recursive_process()",if helper.is_imported_ea(func_ea):
"def GetBoundingBoxMin(self): """"""Get the minimum bounding box."""""" x1, y1 = (10000, 10000) x2, y2 = (-10000, -10000) for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] if point[1] < y1: y1 = point[1] <IF_STMT> x2 = point[0] if point[1] > y2: y2 = point[1] return (x2 - x1, y2 - y1)",if point[0] > x2:
"def __init__(self, detail=None, headers=None, comment=None, body_template=None, location=None, add_slash=False): super(_HTTPMove, self).__init__(detail=detail, headers=headers, comment=comment, body_template=body_template) if location is not None: self.location = location <IF_STMT> raise TypeError('You can only provide one of the arguments location and add_slash') self.add_slash = add_slash",if add_slash:
"def __str__(self, prefix='', printElemNumber=0): res = '' cnt = 0 for e in self.presence_response_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'presence_response%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 return res",if printElemNumber:
"def _find_first_match(self, request): match_failed_reasons = [] for i, match in enumerate(self._matches): match_result, reason = match.matches(request) <IF_STMT> return (match, match_failed_reasons) else: match_failed_reasons.append(reason) return (None, match_failed_reasons)",if match_result:
"def index(self, req, volume_id): req_version = req.api_version_request metadata = super(Controller, self).index(req, volume_id) if req_version.matches(mv.ETAGS): data = jsonutils.dumps(metadata) <IF_STMT> data = data.encode('utf-8') resp = webob.Response() resp.headers['Etag'] = hashlib.md5(data).hexdigest() resp.body = data return resp return metadata",if six.PY3:
"def init(self): """"""Called after document is loaded."""""" self.asset_node = window.document.createElement('div') self.asset_node.id = 'Flexx asset container' window.document.body.appendChild(self.asset_node) if self.is_exported: if self.is_notebook: print('Flexx: I am in an exported notebook!') else: print('Flexx: I am in an exported app!') self.run_exported_app() else: print('Flexx: Initializing') <IF_STMT> self._remove_querystring() self.init_logging()",if not self.is_notebook:
"def get_default_person(self): """"""Return the default Person of the database."""""" person_handle = self.get_default_handle() if person_handle: person = self.get_person_from_handle(person_handle) if person: return person <IF_STMT> with BSDDBTxn(self.env, self.metadata) as txn: txn.put(b'default', None) return None else: return None",elif self.metadata and (not self.readonly):
def reader(): async with read: await wait_all_tasks_blocked() total_received = 0 while True: received = len(await read.receive_some(5000)) <IF_STMT> break total_received += received assert total_received == count * replicas,if not received:
"def array_module(a): if isinstance(a, np.ndarray): return np else: from pyopencl.array import Array <IF_STMT> return _CLFakeArrayModule(a.queue) else: raise TypeError('array type not understood: %s' % type(a))","if isinstance(a, Array):"
"def __str__(self): path = super(XPathExpr, self).__str__() if self.textnode: <IF_STMT> path = 'text()' elif path.endswith('::*/*'): path = path[:-3] + 'text()' else: path += '/text()' if self.attribute is not None: if path.endswith('::*/*'): path = path[:-2] path += '/@%s' % self.attribute return path",if path == '*':
"def update(self): if self.saved(): rgns = self.view.get_regions(self.region_key) <IF_STMT> rgn = Region.from_region(self.view, rgns[0], self.region_key) self.start = rgn.start self.end = rgn.end",if rgns:
"def PrintServerName(data, entries): if entries > 0: entrieslen = 26 * entries chunks, chunk_size = (len(data[:entrieslen]), entrieslen / entries) ServerName = [data[i:i + chunk_size] for i in range(0, chunks, chunk_size)] l = [] for x in ServerName: FP = WorkstationFingerPrint(x[16:18]) Name = x[:16].replace('\x00', '') <IF_STMT> l.append(Name + ' (%s)' % FP) else: l.append(Name) return l return None",if FP:
"def add_lookup(self, name_type, pyname, jsname, depth=-1): jsname = self.jsname(name_type, jsname) if self.local_prefix is not None: <IF_STMT> jsname = self.jsname(name_type, '%s.%s' % (self.local_prefix, jsname)) if self.lookup_stack[depth].has_key(pyname): name_type = self.lookup_stack[depth][pyname][0] if self.module_name != 'pyjslib' or pyname != 'int': self.lookup_stack[depth][pyname] = (name_type, pyname, jsname) return jsname",if jsname.find(self.local_prefix) != 0:
"def ensure_echo_on(): if termios: fd = sys.stdin <IF_STMT> attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, 'SIGTTOU'): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)",if fd.isatty():
"def get_query_results(user, query_id, bring_from_cache): query = _load_query(user, query_id) if bring_from_cache: <IF_STMT> results = query.latest_query_data.data else: raise Exception('No cached result available for query {}.'.format(query.id)) else: results, error = query.data_source.query_runner.run_query(query.query_text, user) if error: raise Exception('Failed loading results for query id {}.'.format(query.id)) else: results = json_loads(results) return results",if query.latest_query_data_id is not None:
"def on_tag_added_to_page(self, o, row, pagerow): self.flush_cache() if row['name'] in self.tags and self._matches_all(pagerow['id']): for treepath in self._find_all_pages(pagerow['name']): if len(treepath) == 1: treeiter = self.get_iter(treepath) self.emit('row-inserted', treepath, treeiter) <IF_STMT> self._emit_children_inserted(pagerow['id'], treepath)",if pagerow['n_children'] > 0:
"def _is_subnet_of(a, b): try: <IF_STMT> raise TypeError(f'{a} and {b} are not of the same version') return b.network_address <= a.network_address and b.broadcast_address >= a.broadcast_address except AttributeError: raise TypeError(f'Unable to test subnet containment between {a} and {b}')",if a._version != b._version:
"def consume(d={}): """"""Add attribute list to the dictionary 'd' and reset the list."""""" if AttributeList.attrs: d.update(AttributeList.attrs) AttributeList.attrs = {} <IF_STMT> options = parse_options(d['options'], (), 'illegal option name') for option in options: d[option + '-option'] = ''",if 'options' in d:
"def tearDown(self): for pidfile in self.pidfiles: if not os.path.exists(pidfile): continue with open(pidfile) as f: pid = f.read() if not pid: return pid = int(pid) try: os.kill(pid, signal.SIGKILL) except OSError: pass for pidfile in self.pidfiles: <IF_STMT> os.unlink(pidfile) self.tearDownBasedir()",if os.path.exists(pidfile):
"def sort(self, items): slow_sorts = [] switch_slow = False for sort in reversed(self.sorts): if switch_slow: slow_sorts.append(sort) <IF_STMT> switch_slow = True slow_sorts.append(sort) else: pass for sort in slow_sorts: items = sort.sort(items) return items",elif sort.order_clause() is None:
"def shortcut(input, ch_out, stride): ch_in = input.shape[1] if ch_in != ch_out: <IF_STMT> filter_size = 1 else: filter_size = 3 return conv_bn_layer(input, ch_out, filter_size, stride) else: return input",if stride == 1:
"def detab(self, text): """"""Remove a tab from the front of each line of the given text."""""" newtext = [] lines = text.split('\n') for line in lines: if line.startswith(' ' * self.tab_length): newtext.append(line[self.tab_length:]) <IF_STMT> newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",elif not line.strip():
"def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] <IF_STMT> attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models",if attr is None:
"def stop_loggers(self): super(NetconsoleHost, self).stop_loggers() if self.__logger: utils.nuke_subprocess(self.__logger) self.__logger = None <IF_STMT> self.job.warning_loggers.discard(self.__warning_stream) self.__warning_stream.close()",if self.job:
"def get_template_context(node, context, context_lines=3): line, source_lines, name = get_template_source_from_exception_info(node, context) debug_context = [] start = max(1, line - context_lines) end = line + 1 + context_lines for line_num, content in source_lines: <IF_STMT> debug_context.append({'num': line_num, 'content': content, 'highlight': line_num == line}) return {'name': name, 'context': debug_context}",if start <= line_num <= end:
"def arg_names(self, lineage, command_name, positional_arg=False): parent = '.'.join(lineage) arg_names = self.index['arg_names'].get(parent, {}).get(command_name, []) filtered_arg_names = [] for arg_name in arg_names: arg_data = self.get_argument_data(lineage, command_name, arg_name) <IF_STMT> filtered_arg_names.append(arg_name) return filtered_arg_names",if arg_data.positional_arg == positional_arg:
"def attributive(adjective, gender=MALE): w = adjective.lower() if PLURAL in gender and (not is_vowel(w[-1:])): return w + 'es' if PLURAL in gender and w.endswith(('a', 'e')): return w + 's' if w.endswith('o'): <IF_STMT> return w[:-1] + 'as' if FEMININE in gender: return w[:-1] + 'a' if PLURAL in gender: return w + 's' return w",if FEMININE in gender and PLURAL in gender:
"def _get_disk_size(cls, path, ignored=None): if ignored is None: ignored = [] if path in ignored: return 0 total = 0 for entry in scandir(path): if entry.is_dir(): total += cls._get_disk_size(entry.path, ignored=ignored) <IF_STMT> total += entry.stat().st_size return total",elif entry.is_file():
def validateHeaders(self): if 'Cookie' in self.headers: for session in self.factory.authenticated_sessions: <IF_STMT> return WebSocketProtocol.validateHeaders(self) return False,if 'TWISTED_SESSION=' + session.uid in self.headers['Cookie']:
"def _format_privilege_data(self, data): for key in ['spcacl']: if key in data and data[key] is not None: <IF_STMT> data[key]['added'] = parse_priv_to_db(data[key]['added'], self.acl) if 'changed' in data[key]: data[key]['changed'] = parse_priv_to_db(data[key]['changed'], self.acl) if 'deleted' in data[key]: data[key]['deleted'] = parse_priv_to_db(data[key]['deleted'], self.acl)",if 'added' in data[key]:
"def show_text(text): print(_stash.text_color('=' * 20, 'yellow')) lines = text.split('\n') while True: <IF_STMT> print('\n'.join(lines)) return else: print('\n'.join(lines[:100])) lines = lines[100:] prompt = _stash.text_color('(Press Return to continue)', 'yellow') raw_input(prompt) print('\n')",if len(lines) < 100:
"def run(self): TimeInspector.set_time_mark() for tuner_index, tuner_config in enumerate(self.pipeline_config): tuner = self.init_tuner(tuner_index, tuner_config) tuner.tune() <IF_STMT> self.global_best_res = tuner.best_res self.global_best_params = tuner.best_params self.best_tuner_index = tuner_index TimeInspector.log_cost_time('Finished tuner pipeline.') self.save_tuner_exp_info()",if self.global_best_res is None or self.global_best_res > tuner.best_res:
"def OnEvent(self, propGrid, aProperty, ctrl, event): if event.GetEventType() == wx.wxEVT_BUTTON: buttons = propGrid.GetEditorControlSecondary() if event.GetId() == buttons.GetButtonId(0): ... <IF_STMT> ... if event.GetId() == buttons.GetButtonId(2): ... return wx.propgrid.PGTextCtrlEditor.OnEvent(propGrid, aProperty, ctrl, event)",if event.GetId() == buttons.GetButtonId(1):
"def run(self, edit): view = self.view for sel in view.sel(): if not self.is_valid_scope(sel): continue region = view.extract_scope(sel.end()) content = self.extract_content(region) resolver, content = self.resolve(content) <IF_STMT> sublime.error_message('Could not resolve link:\n%s' % content) continue resolver.execute(content)",if content is None:
"def __init__(self, aList): for element in aList: if len(element) > 0: if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) <IF_STMT> text = element.text.strip() if text: self.append(text)",elif element.text:
"def put(self, can_split=False): for node in self.nodes[:1]: <IF_STMT> node.put(can_split=can_split) for node in self.nodes[1:]: self.line_more(SLICE_COLON, can_split_after=True) if self.has_value(node): node.put(can_split=can_split) return self",if self.has_value(node):
"def process_return_exits(self, exits): """"""Add arcs due to jumps from `exits` being returns."""""" for block in self.nearest_blocks(): if isinstance(block, TryBlock) and block.final_start is not None: block.return_from.update(exits) break <IF_STMT> for xit in exits: self.add_arc(xit.lineno, -block.start, xit.cause, ""didn't return from function {!r}"".format(block.name)) break","elif isinstance(block, FunctionBlock):"
"def find_commands(management_dir): command_dir = os.path.join(management_dir, 'commands') commands = [] try: for f in os.listdir(command_dir): if f.startswith('_'): continue <IF_STMT> commands.append(f[:-3]) elif f.endswith('.pyc') and f[:-4] not in commands: commands.append(f[:-4]) except OSError: pass return commands",elif f.endswith('.py') and f[:-3] not in commands:
def split_path_info(path): path = path.strip('/') clean = [] for segment in path.split('/'): <IF_STMT> continue elif segment == '..': if clean: del clean[-1] else: clean.append(segment) return tuple(clean),if not segment or segment == '.':
"def __init__(self, source_definition, **kw): super(RekallEFilterArtifacts, self).__init__(source_definition, **kw) for column in self.fields: <IF_STMT> raise errors.FormatError(u'Field definition should have both name and type.') mapped_type = column['type'] if mapped_type not in self.allowed_types: raise errors.FormatError(u'Unsupported type %s.' % mapped_type)",if 'name' not in column or 'type' not in column:
"def _name(self, sender, short=True, full_email=False): words = re.sub('[""<>]', '', sender).split() nomail = [w for w in words if not '@' in w] if nomail: if short: <IF_STMT> return nomail[1] return nomail[0] return ' '.join(nomail) elif words: if not full_email: return words[0].split('@', 1)[0] return words[0] return '(nobody)'",if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES:
"def _get_consuming_layers(self, check_layer): """"""Returns all the layers which are out nodes from the layer."""""" consuming_layers = [] for layer in self._config['layers']: for inbound_node in layer['inbound_nodes']: for connection_info in inbound_node: <IF_STMT> consuming_layers.append(layer) return consuming_layers",if connection_info[0] == check_layer['config']['name']:
"def _check_feasible_fuse(self, model): if not self.modules_to_fuse: return False for group in self.modules_to_fuse: <IF_STMT> raise MisconfigurationException(f'You have requested to fuse {group} but one or more of them is not your model attributes') return True","if not all((_recursive_hasattr(model, m) for m in group)):"
"def cancel_loan_repayment_entry(self): for loan in self.loans: <IF_STMT> repayment_entry = frappe.get_doc('Loan Repayment', loan.loan_repayment_entry) repayment_entry.cancel()",if loan.loan_repayment_entry:
"def update_channel_entries(self, request): try: request_parsed = await request.json() except (ContentTypeError, ValueError): return RESTResponse({'error': 'Bad JSON'}, status=HTTP_BAD_REQUEST) results_list = [] for entry in request_parsed: public_key = database_blob(unhexlify(entry.pop('public_key'))) id_ = entry.pop('id') error, result = self.update_entry(public_key, id_, entry) <IF_STMT> return RESTResponse(result, status=error) results_list.append(result) return RESTResponse(results_list)",if error:
"def delete(self, userId: str, bucket: str, key: str) -> bool: if not self.initialized: raise Exception('archive not initialized') try: with db.session_scope() as dbsession: rc = db_archivedocument.delete(userId, bucket, key, session=dbsession) <IF_STMT> raise Exception('failed to delete DB record') else: return True except Exception as err: raise err",if not rc:
"def handle_phase(task, config): """"""Function that runs all of the configured plugins which act on the current phase."""""" results = [] for item in config: for plugin_name, plugin_config in item.items(): if phase in plugin.get_phases_by_plugin(plugin_name): method = plugin.get_plugin_by_name(plugin_name).phase_handlers[phase] log.debug('Running plugin %s' % plugin_name) result = method(task, plugin_config) <IF_STMT> results.append(result) return itertools.chain(*results)",if phase == 'input' and result:
def guess_gitlab_remote(self): upstream = self.get_upstream_for_active_branch() integrated_remote = self.get_integrated_remote_name() remotes = self.get_remotes() if len(self.remotes) == 1: return list(remotes.keys())[0] elif upstream: tracked_remote = upstream.split('/')[0] if upstream else None <IF_STMT> return tracked_remote else: return None else: return integrated_remote,if tracked_remote and tracked_remote == integrated_remote:
"def do_test(self, path): reader = paddle.reader.creator.recordio(path) idx = 0 for e in reader(): if idx == 0: self.assertEqual(e, (1, 2, 3)) <IF_STMT> self.assertEqual(e, (4, 5, 6)) idx += 1 self.assertEqual(idx, 2)",elif idx == 1:
def gen_cpu_name(cpu): if cpu == 'simple': return event_download.get_cpustr() for j in known_cpus: if cpu == j[0]: <IF_STMT> return 'GenuineIntel-6-%02X-%d' % j[1][0] else: return 'GenuineIntel-6-%02X' % j[1][0] assert False,"if isinstance(j[1][0], tuple):"
"def read_kernel_cmdline_config(cmdline=None): if cmdline is None: cmdline = util.get_cmdline() if 'network-config=' in cmdline: data64 = None for tok in cmdline.split(): if tok.startswith('network-config='): data64 = tok.split('=', 1)[1] <IF_STMT> if data64 == KERNEL_CMDLINE_NETWORK_CONFIG_DISABLED: return {'config': 'disabled'} return util.load_yaml(_b64dgz(data64)) return None",if data64:
"def _verify_bot(self, ctx: 'Context') -> None: if ctx.guild is None: bot_user = ctx.bot.user else: bot_user = ctx.guild.me cog = ctx.cog <IF_STMT> raise discord.ext.commands.DisabledCommand() bot_perms = ctx.channel.permissions_for(bot_user) if not (bot_perms.administrator or bot_perms >= self.bot_perms): raise BotMissingPermissions(missing=self._missing_perms(self.bot_perms, bot_perms))","if cog and await ctx.bot.cog_disabled_in_guild(cog, ctx.guild):"
"def _split_values(self, value): if not self.allowed_values: return ('',) try: r = re.compile(self.allowed_values) except: print(self.allowed_values, file=sys.stderr) raise s = str(value) i = 0 vals = [] while True: m = r.search(s[i:]) <IF_STMT> break vals.append(m.group()) delimiter = s[i:i + m.start()] if self.delimiter is None and delimiter != '': self.delimiter = delimiter i += m.end() return tuple(vals)",if m is None:
"def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) <IF_STMT> return i return i",if not count and i:
"def set_page(self, page): """"""If a page is present as a bookmark than select it."""""" pagename = page.name with self.on_bookmark_clicked.blocked(): for button in self.scrolledbox.get_scrolled_children(): <IF_STMT> button.set_active(True) else: button.set_active(False)",if button.zim_path == pagename:
"def get_Subclass_of(rt): for y in [getattr(Ast, x) for x in dir(Ast)]: yt = clr.GetClrType(y) <IF_STMT> continue if yt.IsAbstract: continue if yt.IsSubclassOf(rt): yield yt.Name",if rt == yt:
"def update_parent_columns(self): """"""Update the parent columns of the current focus column."""""" f = self.columns.get_focus_column() col = self.col_list[f] while 1: parent, pcol = self.get_parent(col) if pcol is None: return changed = pcol.update_results(start_from=parent) <IF_STMT> return col = pcol",if not changed:
"def get_template_engine(themes): """"""Get template engine used by a given theme."""""" for theme_name in themes: engine_path = os.path.join(theme_name, 'engine') <IF_STMT> with open(engine_path) as fd: return fd.readlines()[0].strip() return 'mako'",if os.path.isfile(engine_path):
def reConnect(self): while self.retrymax is None or self.retries < self.retrymax: logger.info('Cobra reconnection attempt') try: self.conn = self.httpfact() <IF_STMT> self.authUser(self.authinfo) self.retries = 0 return except Exception as e: time.sleep(2 ** self.retries) self.retries += 1 self.trashed = True raise CobraHttpException('Retry Exceeded!'),if self._cobra_sessid:
"def __eq__(self, other): if isinstance(other, OrderedDict): if len(self) != len(other): return False for p, q in zip(list(self.items()), list(other.items())): <IF_STMT> return False return True return dict.__eq__(self, other)",if p != q:
"def __getExpectedSampleOffsets(self, tileOrigin, area1, area2): ts = GafferImage.ImagePlug.tileSize() data = [] for y in range(tileOrigin.y, tileOrigin.y + ts): for x in range(tileOrigin.x, tileOrigin.x + ts): pixel = imath.V2i(x, y) data.append(data[-1] if data else 0) <IF_STMT> data[-1] += 1 if GafferImage.BufferAlgo.contains(area2, pixel): data[-1] += 1 return IECore.IntVectorData(data)","if GafferImage.BufferAlgo.contains(area1, pixel):"
"def _get_changes(self): """"""Get changes from CHANGES.txt."""""" log_lines = [] found_version = False found_items = False with open('CHANGES.txt', 'r') as fp: for line in fp.readlines(): line = line.rstrip() if line.endswith(VERSION_TEXT_SHORT): found_version = True if not line.strip() and found_items: break <IF_STMT> log_lines.append(' ' * 2 + '* ' + line[2:]) found_items = True return log_lines",elif found_version and line.startswith('- '):
"def _next_hid(self, n=1): if len(self.datasets) == 0: return n else: last_hid = 0 for dataset in self.datasets: <IF_STMT> last_hid = dataset.hid return last_hid + n",if dataset.hid > last_hid:
"def setInt(self, path, value, **kwargs): if value is None: self.set(path, None, **kwargs) return minimum = kwargs.pop('min', None) maximum = kwargs.pop('max', None) try: intValue = int(value) if minimum is not None and intValue < minimum: intValue = minimum <IF_STMT> intValue = maximum except ValueError: self._logger.warning('Could not convert %r to a valid integer when setting option %r' % (value, path)) return self.set(path, intValue, **kwargs)",if maximum is not None and intValue > maximum:
"def _load_idle_extensions(self, sub_section, fp, lineno): extension_map = self.get_data('idle extensions') if extension_map is None: extension_map = {} extensions = [] while 1: line, lineno, bBreak = self._readline(fp, lineno) <IF_STMT> break line = line.strip() if line: extensions.append(line) extension_map[sub_section] = extensions self._save_data('idle extensions', extension_map) return (line, lineno)",if bBreak:
"def _get_config(key): config = db.session.execute(Configs.__table__.select().where(Configs.key == key)).fetchone() if config and config.value: value = config.value <IF_STMT> return int(value) elif value and isinstance(value, string_types): if value.lower() == 'true': return True elif value.lower() == 'false': return False else: return value return KeyError",if value and value.isdigit():
"def check_labels(self): print('Checking labels if they are outside the image') for i in self.Dataframe.index: image_name = os.path.join(self.project_path, i) im = PIL.Image.open(image_name) self.width, self.height = im.size for ind in self.individual_names: <IF_STMT> self.Dataframe = MainFrame.force_outside_labels_Nans(self, i, ind, self.uniquebodyparts) else: self.Dataframe = MainFrame.force_outside_labels_Nans(self, i, ind, self.multianimalbodyparts) return self.Dataframe",if ind == 'single':
"def remove_excluded(self): """"""Remove all sources marked as excluded."""""" sources = list(self.sources.values()) for src in sources: <IF_STMT> del self.sources[src.name] src.imports = [m for m in src.imports if not self._exclude(m)] src.imported_by = [m for m in src.imported_by if not self._exclude(m)]",if src.excluded:
"def parse_scientific_formats(data, tree): scientific_formats = data.setdefault('scientific_formats', {}) for elem in tree.findall('.//scientificFormats/scientificFormatLength'): type = elem.attrib.get('type') <IF_STMT> continue pattern = text_type(elem.findtext('scientificFormat/pattern')) scientific_formats[type] = numbers.parse_pattern(pattern)","if _should_skip_elem(elem, type, scientific_formats):"
"def _modifierCodes2Labels(cls, mods): <IF_STMT> return [] modconstants = cls._modifierCodes modNameList = [] for k in modconstants._keys: mc = modconstants._names[k] if mods & k == k: modNameList.append(mc) mods = mods - k if mods == 0: return modNameList return modNameList",if mods == 0:
def to_pig_latin(text: str): if text is None: return '' words = text.lower().strip().split(' ') text = [] for word in words: if word[0] in 'aeiou': text.append(f'{word}yay') else: for letter in word: <IF_STMT> text.append(f'{word[word.index(letter):]}{word[:word.index(letter)]}ay') break return ' '.join(text),if letter in 'aeiou':
"def __connect__(self) -> H2Protocol: <IF_STMT> async with self._connect_lock: self._state = _ChannelState.CONNECTING if not self._connected: try: self._protocol = await self._create_connection() except Exception: self._state = _ChannelState.TRANSIENT_FAILURE raise else: self._state = _ChannelState.READY return cast(H2Protocol, self._protocol)",if not self._connected:
"def run_commands(cmds): set_kubeconfig_environment_var() for cmd in cmds: process = subprocess.run(cmd, shell=True, check=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ) if process.stdout: logger.info(process.stdout) <IF_STMT> logger.info(process.stderr) return process.stdout",if process.stderr:
"def deserialize(x): t = type(x) if t is list: return list(imap(deserialize, x)) if t is dict: <IF_STMT> return {key: deserialize(val) for key, val in iteritems(x)} obj = objmap.get(x['_id_']) if obj is None: entity_name = x['class'] entity = database.entities[entity_name] pk = x['_pk_'] obj = entity[pk] return obj return x",if '_id_' not in x:
"def _parse_arguments(self, handler_method): spec = DynamicArgumentParser().parse(self._argspec, self.longname) if not self._supports_kwargs: <IF_STMT> raise DataError(""Too few '%s' method parameters for **kwargs support."" % self._run_keyword_method_name) if spec.kwonlyargs: raise DataError(""Too few '%s' method parameters for keyword-only arguments support."" % self._run_keyword_method_name) spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name) return spec",if spec.kwargs:
"def test_update_password_command(mocker, username, password, expected, changed): with mocker.patch.object(UpdatePassword, 'update_password', return_value=changed): result, stdout, stderr = run_command('update_password', username=username, password=password) <IF_STMT> assert stdout == expected else: assert str(result) == expected",if result is None:
"def characters(self, ch): if self.Text_tag: if self.Summary_tag: self.Summary_ch += ch <IF_STMT> self.Attack_Prerequisite_ch += ch elif self.Solution_or_Mitigation_tag: self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch",elif self.Attack_Prerequisite_tag:
"def _pybin_add_zip(pybin, libname, filter, exclusions, dirs, dirs_with_init_py): with zipfile.ZipFile(libname, 'r') as lib: name_list = lib.namelist() for name in name_list: <IF_STMT> if dirs is not None and dirs_with_init_py is not None: _update_init_py_dirs(name, dirs, dirs_with_init_py) pybin.writestr(name, lib.read(name))","if filter(name) and (not _is_python_excluded_path(name, exclusions)):"
"def parseAGL(filename): m = {} for line in readLines(filename): line = line.strip() if len(line) > 0 and line[0] != '#': name, uc = tuple([c.strip() for c in line.split(';')]) <IF_STMT> m[int(uc, 16)] = name return m",if uc.find(' ') == -1:
"def assertS_IS(self, name, mode): fmt = getattr(stat, 'S_IF' + name.lstrip('F')) self.assertEqual(stat.S_IFMT(mode), fmt) testname = 'S_IS' + name for funcname in self.format_funcs: func = getattr(stat, funcname, None) <IF_STMT> if funcname == testname: raise ValueError(funcname) continue if funcname == testname: self.assertTrue(func(mode)) else: self.assertFalse(func(mode))",if func is None:
"def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema['properties'].items(): response = 'Test response' items = value['properties']['value'].get('items') enum = value['properties']['value'].get('enum') <IF_STMT> response = [items['enum'][0]] elif enum: response = enum[0] elif value['properties']['value'].get('properties'): response = {'question': {'value': 'Test Response'}} test_metadata[key] = {'value': response} return test_metadata",if items:
"def decode_binary(binarystring): """"""Decodes a binary string into it's integer value."""""" n = 0 for c in binarystring: <IF_STMT> d = 0 elif c == '1': d = 1 else: raise ValueError('Not an binary number', binarystring) n = n * 2 + d return n",if c == '0':
def getZoneOffset(d): zoffs = 0 try: <IF_STMT> zoffs = 60 * int(d['tzhour']) + int(d['tzminute']) if d['tzsign'] != '-': zoffs = -zoffs except TypeError: pass return zoffs,if d['zulu'] == None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.add_module(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def _flow_open(self): rv = [] for pipe in self.pipes: if pipe._pipeline_all_methods_.issuperset({'open', self._method_open}): raise RuntimeError(f'{pipe.__class__.__name__} pipe has double open methods. Use `open` or `{self._method_open}`, not both.') if 'open' in pipe._pipeline_all_methods_: rv.append(pipe.open) <IF_STMT> rv.append(getattr(pipe, self._method_open)) return rv",if self._method_open in pipe._pipeline_all_methods_:
"def list_and_filter_commands(filter_str): sorted_commands = list(_pwndbg.commands.commands) sorted_commands.sort(key=lambda x: x.__name__) if filter_str: filter_str = filter_str.lower() results = [] for c in sorted_commands: name = c.__name__ docs = c.__doc__ <IF_STMT> docs = docs.strip() if docs: docs = docs.splitlines()[0] if not filter_str or filter_str in name.lower() or (docs and filter_str in docs.lower()): results.append((name, docs)) return results",if docs:
"def _scale_action(action: np.ndarray, spec: specs.Array): """"""Converts a single canonical action back to the given action spec."""""" if isinstance(spec, specs.BoundedArray): scale = spec.maximum - spec.minimum offset = spec.minimum <IF_STMT> action = np.clip(action, -1.0, 1.0) action = 0.5 * (action + 1.0) action *= scale action += offset return action",if clip:
"def genData(self, samples, inc, sps): self.prepModData(samples, inc, sps) data = Array.CreateInstance(float, samples) cycleLen = float(sps) / gcdlist(self.findAllFreq()) p = 1.0 c = 0 for i in range(int(cycleLen)): data[i] = p * self.ampl c = c + 2 * inc * self.freq * self.addModData(i) <IF_STMT> p = 1.0 else: p = -1.0 self.fillData(cycleLen, samples, data) return data",if int(c) % 2 == 0:
"def data_type(data, grouped=False, columns=None, key_on='idx', iter_idx=None): """"""Data type check for automatic import"""""" if iter_idx: return Data.from_mult_iters(idx=iter_idx, **data) if pd: <IF_STMT> return Data.from_pandas(data, grouped=grouped, columns=columns, key_on=key_on) if isinstance(data, (list, tuple, dict)): return Data.from_iter(data) else: raise ValueError('This data type is not supported by Vincent.')","if isinstance(data, (pd.Series, pd.DataFrame)):"
"def addNames(self, import_names, node_names): for names in node_names: if isinstance(names, basestring): name = names <IF_STMT> name = names[0] else: name = names[1] import_names[name] = True",elif names[1] is None:
"def validate_address(address_name): fields = ['pincode', 'city', 'country_code'] data = frappe.get_cached_value('Address', address_name, fields, as_dict=1) or {} for field in fields: <IF_STMT> frappe.throw(_('Please set {0} for address {1}').format(field.replace('-', ''), address_name), title=_('E-Invoicing Information Missing'))",if not data.get(field):
"def content(computer, name, values): """"""Compute the ``content`` property."""""" if len(values) == 1: value, = values <IF_STMT> return 'inhibit' if computer['pseudo_type'] else 'contents' elif value == 'none': return 'inhibit' return _content_list(computer, values)",if value == 'normal':
"def _replace_list(self, items): results = [] for item in items: listvar = self._replace_variables_inside_possible_list_var(item) <IF_STMT> results.extend(self[listvar]) else: results.append(self.replace_scalar(item)) return results",if listvar:
"def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs['groups'] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs['groups'] <IF_STMT> mykwargs['source_security_group_name'] = group['group_name'] if 'user_id' in group: mykwargs['source_security_group_owner_id'] = group['user_id'] if 'group_id' in group: mykwargs['source_security_group_id'] = group['group_id'] groups_args_split.append(mykwargs) return groups_args_split",if 'group_name' in group:
"def WriteFlowOutputPluginLogEntries(self, entries): """"""Writes flow output plugin log entries."""""" flow_ids = [(e.client_id, e.flow_id) for e in entries] for f in flow_ids: <IF_STMT> raise db.AtLeastOneUnknownFlowError(flow_ids) for e in entries: dest = self.flow_output_plugin_log_entries.setdefault((e.client_id, e.flow_id), []) to_write = e.Copy() to_write.timestamp = rdfvalue.RDFDatetime.Now() dest.append(to_write)",if f not in self.flows:
def connect(**auth): key = tuple(sorted(auth.items())) if key in connection_pool: ssh = connection_pool[key] <IF_STMT> ssh.connect(**auth) else: ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(**auth) connection_pool[key] = ssh return ssh,if not ssh.get_transport() or not ssh.get_transport().is_active():
"def __call__(self, *args, **kwargs): if self is S: <IF_STMT> raise TypeError('S() takes no positional arguments, got: %r' % (args,)) if not kwargs: raise TypeError('S() expected at least one kwarg, got none') return _t_child(self, '(', (args, kwargs))",if args:
"def read_images(self, paths=[]): images = [] for img_path in paths: assert os.path.isfile(img_path), ""The {} isn't a valid file."".format(img_path) img = cv2.imread(img_path) <IF_STMT> logger.info('error in loading image:{}'.format(img_path)) continue img = img[:, :, ::-1] images.append(img) return images",if img is None:
def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic if polymorphic: polymorphic_key = polymorphic <IF_STMT> polymorphic_key = 'type' if data.get(polymorphic_key) == model.__name__: return model raise ImproperlyConfigured(u'No model found for data: {!r}'.format(data)),"if isinstance(polymorphic_key, bool):"
"def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: token, = tokens if token.type == 'ident': <IF_STMT> if token.lower_value not in counter_style: return token.value elif token.lower_value != 'none': return token.value","if token.lower_value in ('decimal', 'disc'):"
"def setUp(self): yield helpers.TestHandlerWithPopulatedDB.setUp(self) for r in (yield tw(user.db_get_users, 1, 'receiver', 'en')): <IF_STMT> self.rcvr_id = r['id']",if r['pgp_key_fingerprint'] == 'BFB3C82D1B5F6A94BDAC55C6E70460ABF9A4C8C1':
"def check_that_oval_and_rule_id_match(xccdftree): for xccdfid, rule in rules_with_ids_generator(xccdftree): checks = rule.find('./{%s}check' % XCCDF11_NS) <IF_STMT> print(""Rule {0} doesn't have checks."".format(xccdfid), file=sys.stderr) continue assert_that_check_ids_match_rule_id(checks, xccdfid)",if checks is None:
"def MakeWidthArray(fm): s = '{\n\t' cw = fm['Widths'] for i in xrange(0, 256): <IF_STMT> s += ""'\\''"" elif chr(i) == '\\': s += ""'\\\\'"" elif i >= 32 and i <= 126: s += ""'"" + chr(i) + ""'"" else: s += 'chr(%d)' % i s += ':' + fm['Widths'][i] if i < 255: s += ',' if (i + 1) % 22 == 0: s += '\n\t' s += '}' return s","if chr(i) == ""'"":"
"def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), '127.0.0.255') elif i == 255: self.assertEqual(str(ip), '127.0.1.0') elif i == 1000: self.assertEqual(str(ip), '127.0.3.233') <IF_STMT> self.assertEqual(str(ip), '127.0.255.255') elif i == 65535: self.assertEqual(str(ip), '127.1.0.0')",elif i == 65534:
"def _fetch(obj, url, body, *args, **kwargs): if _is_running_from_main_thread(): body = urlencode(body).encode('utf-8') response = self.fetch(url, body=body, method='POST') <IF_STMT> raise luigi.rpc.RPCError('Errror when connecting to remote scheduler') return response.body.decode('utf-8')",if response.code >= 400:
"def isOrHasChild(parent, child): while child: if compare(parent, child): return True child = child.parentNode <IF_STMT> return False if child.nodeType != 1: child = None return False",if not child:
"def HandleCharFormatChange(self, id, code): if code == win32con.BN_CLICKED: editId = buttonControlMap.get(id) assert editId is not None, 'Format button has no associated edit control' editControl = self.GetDlgItem(editId) existingFormat = editControl.GetDefaultCharFormat() flags = win32con.CF_SCREENFONTS d = win32ui.CreateFontDialog(existingFormat, flags, None, self) <IF_STMT> cf = d.GetCharFormat() editControl.SetDefaultCharFormat(cf) self.SetModified(1) return 0",if d.DoModal() == win32con.IDOK:
"def test___iter___two_points(self): cba = LineString([(1, 2), (3, 4)]) for i, xy in enumerate(cba): assert i in [0, 1] <IF_STMT> assert np.allclose(xy, (1, 2)) elif i == 1: assert np.allclose(xy, (3, 4)) assert i == 1",if i == 0:
def main(self): self.model.clear() active_handle = self.get_active('Family') if active_handle: active = self.dbstate.db.get_family_from_handle(active_handle) <IF_STMT> self.display_attributes(active) else: self.set_has_data(False) else: self.set_has_data(False),if active:
"def findStyleName(element, style): oldStyle = DOM.getAttribute(element, 'className') if oldStyle is None: return -1 idx = oldStyle.find(style) lastPos = len(oldStyle) while idx != -1: if idx == 0 or oldStyle[idx - 1] == ' ': last = idx + len(style) <IF_STMT> break idx = oldStyle.find(style, idx + 1) return idx",if last == lastPos or (last < lastPos and oldStyle[last] == ' '):
"def result(self): """"""Gets the formatted string result."""""" if self.__group.isChecked(): <IF_STMT> return 'gt%d' % self.__min.value() if self.__lessThan.isChecked(): return 'lt%d' % self.__max.value() if self.__range.isChecked(): return '%d-%d' % (self.__min.value(), self.__max.value()) return ''",if self.__moreThan.isChecked():
"def get_generic_exception_from_err_details(err_details): err = None if err_details.errcls is not None: err = err_details.errcls(err_details.message) <IF_STMT> err.set_linecol(err_details.detail_json.get('line', -1), err_details.detail_json.get('column', -1)) return err",if err_details.errcls is not errors.InternalServerError:
"def convert_value(self, value, expression, connection, context): if value is None: return None geo_field = self.geo_field if geo_field.geodetic(connection): dist_att = 'm' else: units = geo_field.units_name(connection) <IF_STMT> dist_att = DistanceMeasure.unit_attname(units) else: dist_att = None if dist_att: return DistanceMeasure(**{dist_att: value}) return value",if units:
"def __init__(self, **kwargs): self.layout_cell = kwargs.pop('layout_cell') self.theme = kwargs.pop('theme') assert isinstance(self.layout_cell, LayoutCell) super(LayoutCellFormGroup, self).__init__(**kwargs) self.add_form_def('general', LayoutCellGeneralInfoForm, kwargs={'layout_cell': self.layout_cell, 'theme': self.theme}) plugin = self.layout_cell.instantiate_plugin() if plugin: form_class = plugin.get_editor_form_class() <IF_STMT> self.add_form_def('plugin', form_class, kwargs={'plugin': plugin})",if form_class:
"def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict['model'].items(): for model in value: <IF_STMT> model_meta = value[model] if model.endswith('Param'): model_param = value[model] LOGGER.info('load model') self.set_model_meta(model_meta) self.set_model_param(model_param) self.loss = self.get_loss_function()",if model.endswith('Meta'):
"def add_plugin_single(name, plugin_to_add, parent): plugin_existing = parent.get_plugins(name) if plugin_existing is None: parent.add_plugin(name, plugin_to_add) el<IF_STMT> parent.update_plugin(name, plugin_to_add) else: error('Duplicated plugin {}!'.format(name))",if not plugin_existing.is_callable_plugin():
"def get_details(guid): searchResultId = guid searchResult = SearchResult.get(SearchResult.id == searchResultId) details_link = searchResult.details if details_link: logger.info('Redirecting to details link %s ' % details_link) <IF_STMT> details_link = config.settings.main.dereferer.replace('$s', urllib.quote(details_link)) return redirect(details_link) logger.error('Unable to find details link for search result ID %d' % searchResultId) return ('Unable to find details', 500)",if config.settings.main.dereferer:
"def SurroundedByParens(token): """"""Check if it's an expression surrounded by parentheses."""""" while token: if token.value == ',': return False <IF_STMT> return not token.next_token if token.OpensScope(): token = token.matching_bracket.next_token else: token = token.next_token return False",if token.value == ')':
"def __str__(self, prefix='', printElemNumber=0): res = '' cnt = 0 for e in self.stat_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'stat%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 if self.has_more_files_found_: res += prefix + 'more_files_found: %s\n' % self.DebugFormatBool(self.more_files_found_) return res",if printElemNumber:
"def _get_constraints(self, params): constraints = {} for filter_name in self._get_filter_names(): raw_value = params.get(filter_name, None) <IF_STMT> constraints[filter_name] = self._get_value(raw_value) return constraints",if raw_value is not None:
"def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break if parser._subparsers._actions is None: break choices = parser._subparsers._actions[-1].choices value = getattr(args, 'level_%d' % level) <IF_STMT> parser.print_help() return if not choices: break if isinstance(choices, dict): parser = choices[value] else: return level += 1",if value is None:
"def prompts_dict(self, *args, **kwargs): r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs) if self.workflow_job: <IF_STMT> r['inventory'] = self.workflow_job.inventory if self.workflow_job.char_prompts: r.update(self.workflow_job.char_prompts) return r",if self.workflow_job.inventory_id:
"def _check_etc_hosts(): debug2(' > hosts\n') for line in open('/etc/hosts'): line = re.sub('#.*', '', line) words = line.strip().split() if not words: continue ip = words[0] names = words[1:] <IF_STMT> debug3('<%s %r\n' % (ip, names)) for n in names: check_host(n) found_host(n, ip)",if _is_ip(ip):
"def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None): for assigned_attribute in variant.attributes.all(): header = f'{assigned_attribute.attribute.slug} (variant attribute)' <IF_STMT> value = get_attribute_value(assigned_attribute) if pk: data[pk][header] = value else: data[header] = value return data",if str(assigned_attribute.attribute.pk) in attribute_ids:
"def scrub_time(self, time): debug('scrub_time: {0}'.format(time)) if time == 0: self.loop_backward() elif time == self.timer_duration: self.loop_forward() el<IF_STMT> self.timer_status = TIMER_STATUS_PAUSED elif self.timer_status == TIMER_STATUS_EXPIRED: self.timer_status = TIMER_STATUS_PAUSED self.timer_time = time",if self.timer_status == TIMER_STATUS_STOPPED:
"def leave_AssignTarget(self, original_node: cst.AssignTarget, updated_node: cst.AssignTarget) -> cst.AssignTarget: target = updated_node.target if isinstance(target, cst.Name): var_name = unmangled_name(target.value) <IF_STMT> return self.assignment_replacements[var_name].deep_clone() return updated_node",if var_name in self.assignment_replacements:
"def step(self, action): assert self.action_space.contains(action) if self._state == 4: <IF_STMT> return (self._state, 10.0, True, {}) else: return (self._state, -10, True, {}) elif action: if self._state == 0: self._state = 2 else: self._state += 1 elif self._state == 2: self._state = self._case return (self._state, -1, False, {})",if action and self._case:
"def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): <IF_STMT> node = nodes[i] if isinstance(node, ast.Starred): if ok_node(node.value): return node.value else: return None else: return nodes[i] return None",if ok_node(nodes[i]):
"def __contains__(self, table_name): """"""Check if the given table name exists in the database."""""" try: table_name = normalize_table_name(table_name) <IF_STMT> return True if table_name in self.views: return True return False except ValueError: return False",if table_name in self.tables:
"def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) if c['tries'] == 0 or c['tries'] is None: <IF_STMT> history[c['data']] = c['timestamp'] if len(history) >= count: break return history",if c['data'] not in history:
"def _state_dec_to_imp(self, token): if token in ('+', '-'): self._state = self._state_global else: super(ObjCStates, self)._state_dec_to_imp(token) <IF_STMT> self._state = self._state_objc_dec_begin self.context.restart_new_function(token)",if self._state != self._state_imp:
"def _additional_handlers(self): handlers = [] if self.session.get('proxy'): protocol, host, port = self._get_proxy() <IF_STMT> handlers.append(sockshandler.SocksiPyHandler(protocol, host, port)) else: raise ChannelException(messages.channels.error_proxy_format) ctx = ssl.create_default_context() ctx.check_hostname = False ctx.verify_mode = ssl.CERT_NONE handlers.append(urllib.request.HTTPSHandler(context=ctx)) return handlers",if protocol and host and port:
"def loadGCodeData(self, dataStream): if self._printing: return False self._lineCount = 0 for line in dataStream: if ';' in line: line = line[:line.index(';')] line = line.strip() <IF_STMT> continue self._lineCount += 1 self._doCallback() return True",if len(line) < 1:
"def get_headers_footers_xml(self, uri): for relKey, val in self.docx._part._rels.items(): <IF_STMT> yield (relKey, self.xml_to_string(parse_xml(val.target_part.blob)))",if val.reltype == uri and val.target_part.blob:
"def eventlist_name(name=None, key='core'): if not name: name = get_cpustr() cache = getdir() fn = name if os.path.exists(fn): return fn if '.json' not in name: fn = '%s-%s.json' % (name, key) if '/' in fn: return fn fn = '%s/%s' % (cache, fn) if not os.path.exists(fn): name = cpu_without_step(name) <IF_STMT> fn = '%s/%s' % (cache, name) else: fn = '%s/%s-%s.json' % (cache, name, key) return fn",if '*' in fn:
"def test09_authority(self): """"""Testing the authority name & code routines."""""" for s in srlist: <IF_STMT> srs = SpatialReference(s.wkt) for target, tup in s.auth.items(): self.assertEqual(tup[0], srs.auth_name(target)) self.assertEqual(tup[1], srs.auth_code(target))","if hasattr(s, 'auth'):"
"def astAssign(self, import_names, node): for node in node.nodes: <IF_STMT> import_names[node.name] = True else: self.warning('Ignoring Assign %s' % node.flags, node.lineno)",if node.flags == 'OP_ASSIGN':
"def _autojoin(self, __): if not self.auto_join: return try: result = self.get_bookmarks(method=self.storage_method) except XMPPError: return if self.storage_method == 'xep_0223': bookmarks = result['pubsub']['items']['item']['bookmarks'] else: bookmarks = result['private']['bookmarks'] for conf in bookmarks['conferences']: <IF_STMT> log.debug('Auto joining %s as %s', conf['jid'], conf['nick']) self.xmpp['xep_0045'].joinMUC(conf['jid'], conf['nick'], password=conf['password'])",if conf['autojoin']:
"def config_mode(self, config_command='conf t', pattern=''): output = '' if not self.check_config_mode(): output = self.send_command_timing(config_command, strip_command=False, strip_prompt=False) <IF_STMT> output += self.send_command_timing('YES', strip_command=False, strip_prompt=False) if not self.check_config_mode(): raise ValueError('Failed to enter configuration mode') return output",if 'to enter configuration mode anyway' in output:
"def work(self): idle_times = 0 while True: if shutting_down.is_set(): log.info('Stop sync worker') break try: job = self.commit_queue.get(timeout=self.timeout, block=True) if job['type'] == 'commit': self.commits.append(job) log.debug('Got a commit job') idle_times = 0 idle.clear() except Empty: log.debug('Nothing to do right now, going idle') <IF_STMT> idle.set() idle_times += 1 self.on_idle()",if idle_times > self.min_idle_times:
"def movies_iterator(): for row in self._tuple_iterator(query): id, guid, movie = self._parse(fields, row, offset=2) <IF_STMT> if id not in guids: guids[id] = Guid.parse(guid) guid = guids[id] yield (id, guid, movie)",if parse_guid:
"def timesince(value): diff = timezone.now() - value plural = '' if diff.days == 0: hours = int(diff.seconds / 3600.0) if hours != 1: plural = 's' return '%d hour%s ago' % (int(diff.seconds / 3600.0), plural) else: <IF_STMT> plural = 's' return '%d day%s ago' % (diff.days, plural)",if diff.days != 1:
"def connect(self, *args): if len(args) == 0: self.basepath = '/' return True else: self.basepath = args[0] <IF_STMT> return 'No such directory: {p}'.format(p=self.basepath) return True",if not os.path.isdir(self.basepath):
"def get_callable(self): if not self.func: prototype = self.get_prototype() self.func = cast(self.imp, prototype) <IF_STMT> self.func.restype = c_void_p else: self.func.restype = self.restype self.func.argtypes = self.argtypes return self.func",if self.restype == ObjCInstance or self.restype == ObjCClass:
"def on_task_output(self, task, config): for entry in task.entries: <IF_STMT> if entry['torrent'].modified: log.debug('Writing modified torrent file for %s' % entry['title']) with open(entry['file'], 'wb+') as f: f.write(entry['torrent'].encode())",if 'torrent' in entry:
"def update(self, data): results = [] while True: remain = BLOCK_SIZE - self._pos cur_data = data[:remain] cur_data_len = len(cur_data) cur_stream = self._stream[self._pos:self._pos + cur_data_len] self._pos = self._pos + cur_data_len data = data[remain:] results.append(numpy_xor(cur_data, cur_stream)) if self._pos >= BLOCK_SIZE: self._next_stream() self._pos = 0 <IF_STMT> break return b''.join(results)",if not data:
"def listed(output, pool): for line in output.splitlines(): name, mountpoint, refquota = line.split(b'\t') name = name[len(pool) + 1:] if name: refquota = int(refquota.decode('ascii')) <IF_STMT> refquota = None yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)",if refquota == 0:
"def set_multi(self, value): del self[atype] for addr in value: <IF_STMT> if atype != 'all': addr['type'] = atype elif 'atype' in addr and 'type' not in addr: addr['type'] = addr['atype'] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)","if not isinstance(addr, Address):"
"def get_migration_rate(volume): metadata = get_metadata(volume) rate = metadata.get('migrate_rate', None) if rate: <IF_STMT> return storops.VNXMigrationRate.parse(rate.lower()) else: LOG.warning('Unknown migration rate specified, using [high] as migration rate.') return storops.VNXMigrationRate.HIGH",if rate.lower() in storops.VNXMigrationRate.values():
def _check_params(self) -> None: if self.augmentation and self.ratio <= 0: raise ValueError('The augmentation ratio must be positive.') if self.clip_values is not None: if len(self.clip_values) != 2: raise ValueError('`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.') <IF_STMT> raise ValueError('Invalid `clip_values`: min >= max.'),if np.array(self.clip_values[0] >= self.clip_values[1]).any():
"def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break if pos > 0 and dn[pos - 1] != '\\': break elif pos > 1 and dn[pos - 1] == '\\': escaped = True for c in dn[pos - 2:0:-1]: if c == '\\': escaped = not escaped else: break <IF_STMT> break pos += 1 return pos",if not escaped:
"def get_objects(self): retval = [] for item in self._obj_list: if item is None: continue target = pickle.loads(item)[0] _class = map2class(target) if _class: obj = _class(self._dbstate, item) <IF_STMT> retval.append(obj) return retval",if obj:
"def get_databases(request): dbs = {} for key, value in global_env.items(): try: cond = isinstance(value, GQLDB) except: cond = isinstance(value, SQLDB) <IF_STMT> dbs[key] = value return dbs",if cond:
"def real_quick_ratio(buf1, buf2): try: <IF_STMT> return 0 s = SequenceMatcher(None, buf1.split('\n'), buf2.split('\n')) return s.real_quick_ratio() except: print('real_quick_ratio:', str(sys.exc_info()[1])) return 0",if buf1 is None or buf2 is None or buf1 == '' or (buf1 == ''):
"def SentSegRestoreSent(batch_words: List[List[str]], batch_tags: List[List[str]]) -> List[str]: ret = [] for words, tags in zip(batch_words, batch_tags): if len(tags) == 0: ret.append('') continue sent = words[0] punct = '' if tags[0] == 'O' else tags[0][-1] for word, tag in zip(words[1:], tags[1:]): <IF_STMT> sent += punct punct = tag[-1] sent += ' ' + word sent += punct ret.append(sent) return ret",if tag != 'O':
"def build(opt): dpath = os.path.join(opt['datapath'], 'MultiNLI') version = '1.0' if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def __iter__(self): iteration = self.start_iter while iteration <= self.num_iterations: if hasattr(self.batch_sampler.sampler, 'set_epoch'): self.batch_sampler.sampler.set_epoch(iteration) for batch in self.batch_sampler: iteration += 1 <IF_STMT> break yield batch",if iteration > self.num_iterations:
"def visit_title(self, node: Element) -> None: if isinstance(node.parent, addnodes.seealso): self.body.append('.IP ""') return elif isinstance(node.parent, nodes.section): if self.section_level == 0: raise nodes.SkipNode <IF_STMT> self.body.append('.SH %s\n' % self.deunicode(node.astext().upper())) raise nodes.SkipNode return super().visit_title(node)",elif self.section_level == 1:
def validate_feature_query_fields(namespace): if namespace.fields: fields = [] for field in namespace.fields: for feature_query_field in FeatureQueryFields: <IF_STMT> fields.append(feature_query_field) namespace.fields = fields,if field.lower() == feature_query_field.name.lower():
"def __init__(self, clock_pin, mosi_pin, miso_pin): self.lock = None self.clock = None self.mosi = None self.miso = None super(SPISoftwareBus, self).__init__() self.lock = RLock() self.clock_phase = False self.lsb_first = False self.bits_per_word = 8 try: self.clock = OutputDevice(clock_pin, active_high=True) if mosi_pin is not None: self.mosi = OutputDevice(mosi_pin) <IF_STMT> self.miso = InputDevice(miso_pin) except: self.close() raise",if miso_pin is not None:
"def sample_neg_items_for_u(u, num): neg_items = [] while True: if len(neg_items) == num: break neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0] <IF_STMT> neg_items.append(neg_id) return neg_items",if neg_id not in self.train_items[u] and neg_id not in neg_items:
"def _write_dump(self, command, output): if isinstance(self, HostDumper): prefix = 'host' elif isinstance(self, TargetDumper): prefix = 'target' else: prefix = 'unknown' for i in itertools.count(): filename = '%s_%02d_%s' % (prefix, i, command) fullname = os.path.join(self.dump_dir, filename) <IF_STMT> break with open(fullname, 'w') as dump_file: dump_file.write(output)",if not os.path.exists(fullname):
"def match_style(self, vmobject, recurse=True): self.set_style(**vmobject.get_style(), recurse=False) if recurse: submobs1, submobs2 = (self.submobjects, vmobject.submobjects) <IF_STMT> return self elif len(submobs2) == 0: submobs2 = [vmobject] for sm1, sm2 in zip(*make_even(submobs1, submobs2)): sm1.match_style(sm2) return self",if len(submobs1) == 0:
"def close_cb(self, worker): try: self.workers.remove(worker) <IF_STMT> self.h2_num -= 1 else: self.h1_num -= 1 except: pass",if worker.version == '2':
"def wait_for_syn(jid): i = 0 while 1: <IF_STMT> error('!!!WAIT FOR ACK TIMEOUT: job:%r fd:%r!!!', jid, self.synq._reader.fileno(), exc_info=1) req = _wait_for_syn() if req: type_, args = req if type_ == NACK: return False assert type_ == ACK return True i += 1",if i > 60:
"def send_log(self, session: aiohttp.ClientSession, request_dict: Dict[str, Any]): async with session.request(request_dict['method'], request_dict['url'], **request_dict['request_obj']) as resp: resp_text = await resp.text() self.logger().debug(f'Sent logs: {resp.status} {resp.url} {resp_text} ', extra={'do_not_send': True}) <IF_STMT> raise EnvironmentError('Failed sending logs to log server.')","if resp.status != 200 and resp.status not in {404, 405, 400}:"
"def _close_files(self, except_index=None): for tab_index in reversed(range(len(self.winfo_children()))): if except_index is not None and tab_index == except_index: continue else: editor = self.get_child_by_index(tab_index) <IF_STMT> self.forget(editor) editor.destroy()",if self.check_allow_closing(editor):
"def get_sorted_entry(field, bookid): if field == 'title' or field == 'authors': book = calibre_db.get_filtered_book(bookid) <IF_STMT> if field == 'title': return json.dumps({'sort': book.sort}) elif field == 'authors': return json.dumps({'author_sort': book.author_sort}) return ''",if book:
"def listdir(path='.'): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] if is_bytes: good = fname != b'.' and fname == b'..' else: good = fname != '.' and fname != '..' <IF_STMT> if not is_bytes: fname = fsdecode(fname) res.append(fname) return res",if good:
"def image_preprocess(self, image): with tf.name_scope('image_preprocess'): if image.dtype.base_dtype != tf.float32: image = tf.cast(image, tf.float32) mean = [0.485, 0.456, 0.406] std = [0.229, 0.224, 0.225] <IF_STMT> mean = mean[::-1] std = std[::-1] image_mean = tf.constant(mean, dtype=tf.float32) * 255.0 image_std = tf.constant(std, dtype=tf.float32) * 255.0 image = (image - image_mean) / image_std return image",if self.image_bgr:
"def eval_when(when): if hasattr(when, 'isatty') or when in ('always', 'never', 'auto', sys.stderr, sys.stdout): <IF_STMT> return True elif when == 'never': return False elif when == 'auto': return sys.stdout.isatty() else: return when.isatty() else: raise ValueError('text.when: must be a file-object or ""always"", ""never"" or ""auto""')",if when == 'always':
"def _get_plugin(self, name, lang=None, check=False): if lang is None: lang = self.get_lang() if name not in self.plugin_attrib_map: return None plugin_class = self.plugin_attrib_map[name] if plugin_class.is_extension: if (name, None) in self.plugins: return self.plugins[name, None] else: return None if check else self.init_plugin(name, lang) el<IF_STMT> return self.plugins[name, lang] else: return None if check else self.init_plugin(name, lang)","if (name, lang) in self.plugins:"
"def _remove_pending_resource(self, resource, res_id): with self._lock: pending_resources = self.pending_resources.get(res_id, []) for i, pending_resource in enumerate(pending_resources): <IF_STMT> pending_resources.pop(i) break if not pending_resources: self.pending_resources.pop(res_id, None) return res_id",if pending_resource.resource == resource:
"def assign_attributes_to_products(product_attributes): for value in product_attributes: pk = value['pk'] defaults = value['fields'] defaults['product_id'] = defaults.pop('product') defaults['assignment_id'] = defaults.pop('assignment') assigned_values = defaults.pop('values') assoc, created = AssignedProductAttribute.objects.update_or_create(pk=pk, defaults=defaults) <IF_STMT> assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))",if created:
"def recv_full(self, n): r = b'' while len(r) < n: rr = self.conn.recv(n - len(r)) <IF_STMT> raise IOError('need %d bytes, got %d', n, len(r)) r += rr return r",if not rr:
"def get_logsource(self, category, product, service): """"""Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain."""""" matching = list() for config in self: for logsource in config.logsources: <IF_STMT> matching.append(logsource) if logsource.rewrite is not None: category, product, service = logsource.rewrite return SigmaLogsourceConfiguration(matching, self.defaultindex)","if logsource.matches(category, product, service):"
"def test_circuit_structure(): ops = cirq.decompose_cphase_into_two_fsim(cirq.CZ, fsim_gate=cirq.google.SYC) num_interaction_moments = 0 for op in ops: assert len(op.qubits) in (0, 1, 2) <IF_STMT> num_interaction_moments += 1 assert isinstance(op.gate, cirq.google.SycamoreGate) assert num_interaction_moments == 2",if len(op.qubits) == 2:
"def verify_installed_repositories(self, installed_repositories=[], uninstalled_repositories=[]): for repository_name, repository_owner in installed_repositories: galaxy_repository = test_db_util.get_installed_repository_by_name_owner(repository_name, repository_owner) <IF_STMT> assert galaxy_repository.status == 'Installed', 'Repository {} should be installed, but is {}'.format(repository_name, galaxy_repository.status)",if galaxy_repository:
"def set_size_for_text(self, width, nlines=1): if width is not None: font = self.font d = 2 * self.margin <IF_STMT> width, height = font.size(width) width += d + 2 else: height = font.size('X')[1] self.size = (width, height * nlines + d)","if isinstance(width, basestring):"
def splitIntoWords(name): wordlist = [] wordstart = 0 l = len(name) for i in range(l): c = name[i] n = None <IF_STMT> n = name[wordstart:i] elif i == l - 1: n = name[wordstart:i + 1] if n: wordstart = i if c == '-' and n != '': n += '-' if c == ' ' or c == '-': wordstart = i + 1 wordlist.append(n) return wordlist,if c == ' ' or c == '-':
"def _parse(self): import yaml try: f = open(self.path, 'r') except IOError as e: <IF_STMT> log.warning('cannot read user config in %s: %s', self.path, e) else: try: return yaml.safe_load(f) or {} except Exception as e: log.warning('error loading user config in %s: %s', self.path, e) return {}",if e.errno != 2:
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: <IF_STMT> title = str(child.text) if 'pubDate' in child.tag: pub_date = str(child.text) if 'description' in child.tag: description = str(child.text) print_stdout(color_line(title, 14) + ' (' + bold_line(pub_date) + ')') print_stdout(format_paragraph(strip_tags(description))) print_stdout()",if 'title' in child.tag:
"def kth_smallest(root, k): stack = [] while root or stack: while root: stack.append(root) root = root.left root = stack.pop() k -= 1 <IF_STMT> break root = root.right return root.val",if k == 0:
"def _strip_headers(output, *args): if not args: args_lc = ('installed packages', 'available packages', 'available upgrades', 'updated packages', 'upgraded packages') else: args_lc = [x.lower() for x in args] ret = '' for line in salt.utils.itertools.split(output, '\n'): <IF_STMT> ret += line + '\n' return ret",if line.lower() not in args_lc:
def __str__(self): if self.name is not None: return self.name else: name = str(self.data) <IF_STMT> name = name[:10] + '...' + name[-10:] return 'Constant{%s}' % name,if len(name) > 20:
"def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) if path is not None: row = self.get(path[0], 'device') if row: if self.Blueman is not None: <IF_STMT> self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)",if self.menu is None:
"def h2i(self, pkt, x): if x is not None: <IF_STMT> warning('Fixed3_7: Input value too negative: %.8f' % x) x = -180.0 elif x >= 180.00000005: warning('Fixed3_7: Input value too positive: %.8f' % x) x = 180.0 x = int(round((x + 180.0) * 10000000.0)) return x",if x <= -180.00000005:
def mFRIDAY(self): try: _type = FRIDAY _channel = DEFAULT_CHANNEL pass self.match('fri') alt10 = 2 LA10_0 = self.input.LA(1) <IF_STMT> alt10 = 1 if alt10 == 1: pass self.match('day') self._state.type = _type self._state.channel = _channel finally: pass,if LA10_0 == 100:
"def xopen(file): if isinstance(file, str): if file == '-': return sys.stdin <IF_STMT> import gzip return gzip.open(file) else: return open(file) else: return file",elif file.endswith('.gz'):
"def write_bytes(out_data, encoding='ascii'): """"""Legacy for Python2 and Python3 compatible byte stream."""""" if sys.version_info[0] >= 3: if isinstance(out_data, type('')): <IF_STMT> return out_data.encode('utf-8') else: return out_data.encode('ascii', 'ignore') elif isinstance(out_data, type(b'')): return out_data msg = 'Invalid value for out_data neither unicode nor byte string: {}'.format(out_data) raise ValueError(msg)",if encoding == 'utf-8':
"def do_revision_view(request, *args, **kwargs): if request_creates_revision(request): try: with create_revision_base(manage_manually=manage_manually, using=using, atomic=atomic): response = func(request, *args, **kwargs) <IF_STMT> raise _RollBackRevisionView(response) _set_user_from_request(request) return response except _RollBackRevisionView as ex: return ex.response return func(request, *args, **kwargs)",if response.status_code >= 400:
"def testMasked(self): mask = (True, False) trainable_state = recurrent.TrainableState((tf.zeros([16]), tf.zeros([3])), mask) for var in trainable_state.trainable_variables: var.assign_add(tf.ones_like(var)) initial_state = trainable_state(batch_size=42) for s, trainable in zip(tree.flatten(initial_state), tree.flatten(mask)): <IF_STMT> self.assertNotAllClose(s, tf.zeros_like(s)) else: self.assertAllClose(s, tf.zeros_like(s))",if trainable:
"def _get_instance_attribute(self, attr, default=None, defaults=None, incl_metadata=False): if self.instance is None or not hasattr(self.instance, attr): if incl_metadata and attr in self.parsed_metadata: return self.parsed_metadata[attr] <IF_STMT> for value in defaults: if callable(value): value = value() if value is not None: return value return default return getattr(self.instance, attr)",elif defaults is not None:
"def process_config(self): super(SquidCollector, self).process_config() self.squid_hosts = {} for host in self.config['hosts']: matches = self.host_pattern.match(host) if matches.group(5): port = matches.group(5) else: port = 3128 <IF_STMT> nick = matches.group(2) else: nick = port self.squid_hosts[nick] = {'host': matches.group(3), 'port': int(port)}",if matches.group(2):
"def get_iterator(self, training=True): if training: if self._should_reset_train_loader: self.epochs += 1 self.train_iterator = iter(self.train_loader) self._should_reset_train_loader = False return self.train_iterator else: <IF_STMT> self.val_iterator = iter(self.validation_loader) self._should_reset_val_loader = False return self.val_iterator",if self._should_reset_val_loader:
"def _find_this_and_next_frame(self, stack): for i in range(len(stack)): if stack[i].id == self._frame_id: <IF_STMT> return (stack[i], None) else: return (stack[i], stack[i + 1]) raise AssertionError(""Frame doesn't exist anymore"")",if i == len(stack) - 1:
"def send_mail(success): backend = 'django.core.mail.backends.locmem.EmailBackend' if success else 'tests.FailingMailerEmailBackend' with self.settings(MAILER_EMAIL_BACKEND=backend): mailer.send_mail('Subject', 'Body', 'sender@example.com', ['recipient@example.com']) engine.send_all() <IF_STMT> Message.objects.retry_deferred() engine.send_all()",if not success:
"def check_dependencies(): """"""Ensure required tools for installation are present"""""" print('Checking required dependencies') for dep, msg in [(['git', '--version'], 'Git (http://git-scm.com/)'), (['wget', '--version'], 'wget'), (['bzip2', '-h'], 'bzip2')]: try: p = subprocess.Popen(dep, stderr=subprocess.STDOUT, stdout=subprocess.PIPE) out, code = p.communicate() except OSError: out = 'Executable not found' code = 127 <IF_STMT> raise OSError('bcbio-nextgen installer requires %s\n%s' % (msg, out))",if code == 127:
"def apply(self, chart, grammar, edge): if edge.is_incomplete(): return for prod in grammar.productions(): if edge.lhs() == prod.rhs()[0]: new_edge = ProbabilisticTreeEdge.from_production(prod, edge.start(), prod.prob()) <IF_STMT> yield new_edge","if chart.insert(new_edge, ()):"
"def run(self): if self.check(): path = '/../../../../../../../../../../../..{}'.format(self.filename) response = self.http_request(method='GET', path=path) <IF_STMT> return if response.status_code == 200 and response.text: print_success('Success! File: %s' % self.filename) print_info(response.text) else: print_error('Exploit failed') else: print_error('Device seems to be not vulnerable')",if response is None:
"def check_options(plugin, options): CONFLICT_OPTS = {'Phantom': [{'rps_schedule', 'instances_schedule', 'stpd_file'}]} for conflict_options in CONFLICT_OPTS.get(plugin, []): intersect = {option[0] for option in options} & conflict_options <IF_STMT> raise OptionsConflict('Conflicting options: {}: {}'.format(plugin, list(intersect))) return (plugin, options)",if len(intersect) > 1:
"def validate(self, document: Document) -> None: if not self.func(document.text): <IF_STMT> index = len(document.text) else: index = 0 raise ValidationError(cursor_position=index, message=self.error_message)",if self.move_cursor_to_end:
"def download_link(request, path_obj): if path_obj.file != '': <IF_STMT> text = _('Export') tooltip = _('Export translations') else: text = _('Download') tooltip = _('Download file') return {'href': '%s/download/' % path_obj.pootle_path, 'text': text, 'title': tooltip}",if path_obj.translation_project.project.is_monolingual():
"def _setup_factories(self, extrascopes, **kw): for factory, (scope, Default) in {'response_factory': (boto.mws.response, self.ResponseFactory), 'response_error_factory': (boto.mws.exception, self.ResponseErrorFactory)}.items(): <IF_STMT> setattr(self, '_' + factory, kw.pop(factory)) else: scopes = extrascopes + [scope] setattr(self, '_' + factory, Default(scopes=scopes)) return kw",if factory in kw:
def status_string(self): if not self.live: if self.expired: return _('expired') el<IF_STMT> return _('scheduled') elif self.workflow_in_progress: return _('in moderation') else: return _('draft') elif self.approved_schedule: return _('live + scheduled') elif self.workflow_in_progress: return _('live + in moderation') elif self.has_unpublished_changes: return _('live + draft') else: return _('live'),if self.approved_schedule:
"def _sleep_till_stopword(caplog, delay: float, patterns: Sequence[str]=(), *, interval: Optional[float]=None) -> bool: patterns = list(patterns or []) delay = delay or (10.0 if patterns else 1.0) interval = interval or min(1.0, max(0.1, delay / 10.0)) started = time.perf_counter() found = False while not found and time.perf_counter() - started < delay: for message in list(caplog.messages): <IF_STMT> found = True break else: time.sleep(interval) return found","if any((re.search(pattern, message) for pattern in patterns)):"
"def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith('#'): continue if line.startswith('['): if current_repo: repos.append(current_repo) current_repo = {} current_repo['name'] = line[1:-1] <IF_STMT> key, value = line.split('=', 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos",if current_repo and '=' in line:
def __enter__(self): with self._entry_lock: cutoff_time = datetime.datetime.now() - self._time_window while self._past_entries and self._past_entries[0] < cutoff_time: self._past_entries.popleft() <IF_STMT> self._past_entries.append(datetime.datetime.now()) return 0.0 to_wait = (self._past_entries[0] - cutoff_time).total_seconds() time.sleep(to_wait) self._past_entries.append(datetime.datetime.now()) return to_wait,if len(self._past_entries) < self._access_limit:
"def wrappper(*args, **kargs): offspring = func(*args, **kargs) for child in offspring: for i in range(len(child)): <IF_STMT> child[i] = max elif child[i] < min: child[i] = min return offspring",if child[i] > max:
"def migrate_Context(self): for old_obj in self.session_old.query(self.model_from['Context']): new_obj = self.model_to['Context']() for key in new_obj.__table__.columns._data.keys(): <IF_STMT> continue value = getattr(old_obj, key) if key == 'tip_timetolive' and value < 0: value = 0 setattr(new_obj, key, value) self.session_new.add(new_obj)",if key not in old_obj.__table__.columns._data.keys():
def fresh_workspace(self): i3 = IpcTest.i3_conn assert i3 workspaces = await i3.get_workspaces() while True: new_name = str(math.floor(random() * 100000)) <IF_STMT> await i3.command('workspace %s' % new_name) return new_name,if not any((w for w in workspaces if w.name == new_name)):
"def _sum_operation(values): values_list = list() if decimal_support: for v in values: <IF_STMT> values_list.append(v) elif isinstance(v, decimal128.Decimal128): values_list.append(v.to_decimal()) else: values_list = list((v for v in values if isinstance(v, numbers.Number))) sum_value = sum(values_list) return decimal128.Decimal128(sum_value) if isinstance(sum_value, decimal.Decimal) else sum_value","if isinstance(v, numbers.Number):"
"def detect(content, **kwargs): status = kwargs.get('status', 0) if status is not None and status == 405: detection_schema = (re.compile('error(s)?.aliyun(dun)?.(com|net)', re.I), re.compile('http(s)?://(www.)?aliyun.(com|net)', re.I)) for detection in detection_schema: <IF_STMT> return True",if detection.search(content) is not None:
"def __gather_epoch_end_eval_results(self, outputs): eval_results = [] for epoch_output in outputs: result = epoch_output[0].__class__.gather(epoch_output) <IF_STMT> result.checkpoint_on = result.checkpoint_on.mean() if 'early_stop_on' in result: result.early_stop_on = result.early_stop_on.mean() eval_results.append(result) if len(eval_results) == 1: eval_results = eval_results[0] return eval_results",if 'checkpoint_on' in result:
"def proto_library_config(append=None, **kwargs): """"""protoc config."""""" path = kwargs.get('protobuf_include_path') if path: _blade_config.warning('proto_library_config: protobuf_include_path has been renamed to protobuf_incs, and become a list') del kwargs['protobuf_include_path'] <IF_STMT> kwargs['protobuf_incs'] = path.split() else: kwargs['protobuf_incs'] = [path] _blade_config.update_config('proto_library_config', append, kwargs)","if isinstance(path, str) and ' ' in path:"
"def downgrade(): bind = op.get_bind() session = db.Session(bind=bind) for slc in session.query(Slice).filter(Slice.viz_type == 'pie').all(): try: params = json.loads(slc.params) if 'metric' in params: <IF_STMT> params['metrics'] = [params['metric']] del params['metric'] slc.params = json.dumps(params, sort_keys=True) except Exception: pass session.commit() session.close()",if params['metric']:
"def _resolve_params(self, api_params, optional_params, plan_vars): resolver = VariableResolver() api_params_resolved = resolver.resolve_variables(plan_vars, api_params) if optional_params is not None: optional_params_resolved = resolver.resolve_variables(plan_vars, optional_params) for key, value in optional_params_resolved.items(): <IF_STMT> api_params_resolved[key] = value return api_params_resolved",if key not in api_params_resolved and value is not None:
"def publish(self, name, stat): try: topic = 'stat.%s' % str(name) if 'subtopic' in stat: topic += '.%d' % stat['subtopic'] stat = json.dumps(stat) logger.debug('Sending %s' % stat) self.socket.send_multipart([b(topic), stat]) except zmq.ZMQError: <IF_STMT> pass else: raise",if self.socket.closed:
"def verify_packages(packages: Optional[Union[str, List[str]]]) -> None: if not packages: return if isinstance(packages, str): packages = packages.splitlines() for package in packages: <IF_STMT> continue match = RE_PATTERN.match(package) if match: name = match.group('name') operation = match.group('operation1') version = match.group('version1') _verify_package(name, operation, version) else: raise ValueError('Unable to read requirement: %s' % package)",if not package:
"def explode(self, obj): """"""Determine if the object should be exploded."""""" if obj in self._done: return False result = False for item in self._explode: if hasattr(item, '_moId'): <IF_STMT> result = True elif obj.__class__.__name__ == item.__name__: result = True if result: self._done.add(obj) return result",if obj._moId == item._moId:
"def iterRelativeExportCFiles(basepath): for root, dirs, files in os.walk(basepath, topdown=True): for directory in dirs: if isAddonDirectoryIgnored(directory): dirs.remove(directory) for filename in files: <IF_STMT> fullpath = os.path.join(root, filename) yield os.path.relpath(fullpath, basepath)",if not isExportCFileIgnored(filename):
"def get_asset_gl_entry(self, gl_entries): for item in self.get('items'): if item.is_fixed_asset: <IF_STMT> self.add_asset_gl_entries(item, gl_entries) if flt(item.landed_cost_voucher_amount): self.add_lcv_gl_entries(item, gl_entries) self.update_assets(item, item.valuation_rate) return gl_entries",if is_cwip_accounting_enabled(item.asset_category):
"def _check_no_tensors(parameters: Params): flat_params = tf.nest.flatten(parameters.params) for p in flat_params: if isinstance(p, Params): _check_no_tensors(p) <IF_STMT> raise TypeError('Saw a `Tensor` value in parameters:\n  {}'.format(parameters))",if tf.is_tensor(p):
"def _check_positional(results): positional = None for name, char in results: <IF_STMT> positional = name is None elif (name is None) != positional: raise TranslationError('format string mixes positional and named placeholders') return bool(positional)",if positional is None:
def active_cursor(self): if self.phase == _Phase.ADJUST: if self.zone == _EditZone.CONTROL_NODE: return self._crosshair_cursor <IF_STMT> return self._arrow_cursor return None,elif self.zone != _EditZone.EMPTY_CANVAS:
"def _addPending(self, path, reason, isDir=False): if path not in self.__pending: self.__pending[path] = [Utils.DEFAULT_SLEEP_INTERVAL, isDir] self.__pendingMinTime = 0 <IF_STMT> reason = [reason.maskname, reason.pathname] logSys.log(logging.MSG, 'Log absence detected (possibly rotation) for %s, reason: %s of %s', path, *reason)","if isinstance(reason, pyinotify.Event):"
"def has_safe_repr(value): """"""Does the node have a safe representation?"""""" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: if not has_safe_repr(item): return False return True elif isinstance(value, dict): for key, value in value.iteritems(): <IF_STMT> return False if not has_safe_repr(value): return False return True return False",if not has_safe_repr(key):
"def refund_balances(self): from liberapay.billing.transactions import refund_payin payins = self.get_refundable_payins() for exchange in payins: balance = self.get_balance_in(exchange.amount.currency) <IF_STMT> continue amount = min(balance, exchange.refundable_amount) status, e_refund = refund_payin(self.db, exchange, amount, self) if status != 'succeeded': raise TransferError(e_refund.note)",if balance == 0:
"def balanced_tokens_across_dcs(self, dcs): tokens = [] current_dc = dcs[0] count = 0 dc_count = 0 for dc in dcs: <IF_STMT> count += 1 else: new_tokens = [tk + dc_count * 100 for tk in self.balanced_tokens(count)] tokens.extend(new_tokens) current_dc = dc count = 1 dc_count += 1 new_tokens = [tk + dc_count * 100 for tk in self.balanced_tokens(count)] tokens.extend(new_tokens) return tokens",if dc == current_dc:
"def get_logsource(self, category, product, service): """"""Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain."""""" matching = list() for config in self: for logsource in config.logsources: if logsource.matches(category, product, service): matching.append(logsource) <IF_STMT> category, product, service = logsource.rewrite return SigmaLogsourceConfiguration(matching, self.defaultindex)",if logsource.rewrite is not None:
"def fill_squares(self, loc, type): value = type for n in range(self.no_players): self.map_data[loc[0]][loc[1]] = value <IF_STMT> value = chr(ord(value) + 1) loc = self.get_translate_loc(loc)",if type == '0':
"def _init_ti_table(): global _ti_table _ti_table = [] for fname, name in zip(kc.STRFNAMES, kc.STRNAMES): seq = termcap.get(name) <IF_STMT> continue k = _name_to_key(fname) if k: _ti_table.append((list(bytearray(seq)), k))",if not seq:
"def OnDelete(self, event): with wx.MessageDialog(self, 'Do you really want to delete the {} {}?'.format(self.getActiveEntity().name, self.entityName), 'Confirm Delete', wx.YES | wx.NO | wx.ICON_QUESTION) as dlg: dlg.CenterOnParent() <IF_STMT> self.DoDelete(self.getActiveEntity()) self.refreshEntityList() wx.PostEvent(self.entityChoices, wx.CommandEvent(wx.wxEVT_COMMAND_CHOICE_SELECTED))",if dlg.ShowModal() == wx.ID_YES:
"def _add(self, queue): if not queue.routing_key: <IF_STMT> queue.exchange = self.default_exchange queue.routing_key = self.default_routing_key if self.ha_policy: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_ha_policy(queue.queue_arguments) if self.max_priority is not None: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_max_priority(queue.queue_arguments) self[queue.name] = queue return queue",if queue.exchange is None or queue.exchange.name == '':
"def ParsePlacemark(self, node): ret = Placemark() for child in node.childNodes: <IF_STMT> ret.name = self.ExtractText(child) if child.nodeName == 'Point' or child.nodeName == 'LineString': ret.coordinates = self.ExtractCoordinates(child) return ret",if child.nodeName == 'name':
"def find_library_nt(name): results = [] for directory in os.environ['PATH'].split(os.pathsep): fname = os.path.join(directory, name) if os.path.isfile(fname): results.append(fname) <IF_STMT> continue fname = fname + '.dll' if os.path.isfile(fname): results.append(fname) return results",if fname.lower().endswith('.dll'):
"def _calc_freq(item): try: if ao_index is not None and ro_index is not None: ao = sum([int(x) for x in item.split(':')[ao_index].split(',')]) ro = int(item.split(':')[ro_index]) freq = ao / float(ao + ro) <IF_STMT> freq = float(item.split(':')[af_index]) else: freq = 0.0 except (IndexError, ValueError, ZeroDivisionError): freq = 0.0 return freq",elif af_index is not None:
def poll_kafka(self): while True: val = self.do_poll() <IF_STMT> yield self._emit(val) else: yield gen.sleep(self.poll_interval) if self.stopped: break self._close_consumer(),if val:
"def resolve_list_field(parent, args, ctx, info): if 'param' in args: return 'SUCCESS-[{}]'.format(str(args['param']) <IF_STMT> else '-'.join([str(item) for item in args['param']])) return 'SUCCESS'","if not isinstance(args['param'], list)"
"def login_hash(self, host, username, ntlmhash, domain): lmhash, nthash = ntlmhash.split(':') try: self.smbconn[host] = SMBConnection(host, host, sess_port=445, timeout=2) self.smbconn[host].login(username, '', domain, lmhash=lmhash, nthash=nthash) <IF_STMT> color('[+] Guest session established on %s...' % host) else: color('[+] User session establishd on %s...' % host) return True except Exception as e: color('[!] Authentication error occured') color('[!]', e) return False",if self.smbconn[host].isGuestSession() > 0:
"def _add(self, queue): if not queue.routing_key: if queue.exchange is None or queue.exchange.name == '': queue.exchange = self.default_exchange queue.routing_key = self.default_routing_key if self.ha_policy: <IF_STMT> queue.queue_arguments = {} self._set_ha_policy(queue.queue_arguments) if self.max_priority is not None: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_max_priority(queue.queue_arguments) self[queue.name] = queue return queue",if queue.queue_arguments is None:
"def safe_delete_pod(self, jobid, ignore_not_found=True): import kubernetes.client body = kubernetes.client.V1DeleteOptions() try: self.kubeapi.delete_namespaced_pod(jobid, self.namespace, body=body) except kubernetes.client.rest.ApiException as e: <IF_STMT> logger.warning('[WARNING] 404 not found when trying to delete the pod: {jobid}\n[WARNING] Ignore this error\n'.format(jobid=jobid)) else: raise e",if e.status == 404 and ignore_not_found:
"def __init__(self, element, spec): Extension.__init__(self, element, spec) self.spec = spec self.number = tuple(map(int, element.attrib['number'].split('.'))) self.api = element.attrib['api'] for removed in chain.from_iterable(element.findall('remove')): <IF_STMT> continue data = {'enum': spec.enums, 'command': spec.commands}[removed.tag] try: spec.add_remove(self.api, self.number, data[removed.attrib['name']]) except KeyError: pass",if removed.tag == 'type':
"def _convert_raw_source(self, source, languages): for row in source: example = self._read_example(row) <IF_STMT> continue for col, lang in zip(self.language_columns, languages): example[col] = lang yield example",if example is None:
def check_engine(engine): if engine == 'auto': if pa is not None: return 'pyarrow' <IF_STMT> return 'fastparquet' else: raise RuntimeError('Please install either pyarrow or fastparquet.') elif engine == 'pyarrow': if pa is None: raise RuntimeError('Please install pyarrow fisrt.') return engine elif engine == 'fastparquet': if fastparquet is None: raise RuntimeError('Please install fastparquet first.') return engine else: raise RuntimeError('Unsupported engine {} to read parquet.'.format(engine)),elif fastparquet is not None:
"def TryMerge(self, d): while 1: tt = d.getVarInt32() <IF_STMT> break if tt == 18: self.set_value(d.getPrefixedString()) continue if tt == 29: self.set_flags(d.get32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 12:
"def handle(self, request): try: if request.message.question[0].rdtype == dns.rdatatype.IXFR: <IF_STMT> text = ixfr else: text = retry_tcp_ixfr self.did_truncation = True else: text = axfr r = dns.message.from_text(text, one_rr_per_rrset=True, origin=self.origin) r.id = request.message.id return r except Exception: pass",if self.did_truncation:
"def read_kvfile_todict(file): if not os.path.isfile(file): return {} ret = {} with open(file, 'r') as FH: for l in FH.readlines(): l = l.strip() <IF_STMT> k, v = re.match('(\\S*)\\s*(.*)', l).group(1, 2) k = re.sub('____', ' ', k) ret[k] = v return ret",if l:
"def wrapper(*args, **kwargs): with capture_logs() as logs: try: function(*args, **kwargs) except Exception: <IF_STMT> print('%i errors logged:' % len(logs), file=sys.stderr) for message in logs: print(message, file=sys.stderr) raise else: if logs: for message in logs: print(message, file=sys.stderr) raise AssertionError('%i errors logged' % len(logs))",if logs:
"def batchSites(self, sites): i = 0 res = list() siteList = list() for site in sites: if i >= self.opts['_maxthreads']: data = self.threadSites(siteList) <IF_STMT> return res for ret in list(data.keys()): if data[ret]: res.append(f'{ret}:{data[ret]}') i = 0 siteList = list() siteList.append(site) i += 1 return res",if data is None:
"def datagram_received(self, data, addr): """"""Handle data from ``addr``."""""" if self.buffer and addr in self.buffer: data = self.buffer.pop(addr) + data while data: idx = data.find(self.separator) if idx >= 0: idx += len(self.separator) chunk, data = (data[:idx], data[idx:]) self.response(chunk, addr) else: <IF_STMT> self.buffer = {} self.buffer[addr] = data data = None",if self.buffer is None:
def tearDown(self): if self.node: <IF_STMT> with patch('golem.task.taskserver.TaskServer.quit'): self.node.client.quit() if self.node._db: self.node._db.close() super().tearDown(),if self.node.client:
"def _to_sentences(self, lines): text = '' sentence_objects = [] for line in lines: <IF_STMT> if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) sentence_objects.append(line) text = '' else: text += ' ' + line text = text.strip() if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) return sentence_objects","if isinstance(line, Sentence):"
"def _cloneComponentValues(self, myClone, cloneValueFlag): idx = 0 l = len(self._componentValues) while idx < l: c = self._componentValues[idx] if c is not None: <IF_STMT> myClone.setComponentByPosition(idx, c.clone(cloneValueFlag=cloneValueFlag)) else: myClone.setComponentByPosition(idx, c.clone()) idx = idx + 1","if isinstance(c, base.AbstractConstructedAsn1Item):"
"def split_quality(quality): anyQualities = [] bestQualities = [] for curQual in Quality.qualityStrings.keys(): if curQual & quality: anyQualities.append(curQual) <IF_STMT> bestQualities.append(curQual) return (sorted(anyQualities), sorted(bestQualities))",if curQual << 16 & quality:
def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: <IF_STMT> part = make_pattern(part) elif wtree[0] != '': for c in part: if c in special_chars: raise GlobError() subpattern.append(part) return ''.join(subpattern),"if isinstance(part, list):"
"def insert_not(self, aList): '''Change ""!"" to ""not"" except before ""=""''' i = 0 while i < len(aList): if self.is_string_or_comment(aList, i): i = self.skip_string_or_comment(aList, i) <IF_STMT> aList[i:i + 1] = list('not ') i += 4 else: i += 1","elif aList[i] == '!' and (not self.match(aList, i + 1, '=')):"
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: if isinstance(t1, ptr_class) and isinstance(t2, ptr_class): return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) if isinstance(t1, ptr_class): return t1 <IF_STMT> return t2 else: return ptr_class(BottomType()) return n_cls()","elif isinstance(t2, ptr_class):"
"def pre_validate(self, form): if self.data: values = list((c[0] for c in self.choices)) for d in self.data: <IF_STMT> raise ValueError(self.gettext(""'%(value)s' is not a valid choice for this field"") % dict(value=d))",if d not in values:
"def frontend_visible_config(config_dict): visible_dict = {} for name in CLIENT_WHITELIST: if name.lower().find('secret') >= 0: raise Exception('Cannot whitelist secrets: %s' % name) <IF_STMT> visible_dict[name] = config_dict.get(name, None) if 'ENTERPRISE_LOGO_URL' in config_dict: visible_dict['BRANDING'] = visible_dict.get('BRANDING', {}) visible_dict['BRANDING']['logo'] = config_dict['ENTERPRISE_LOGO_URL'] return visible_dict",if name in config_dict:
"def listdir(self, path=None): from azure.storage.blob import Blob dir_path = normalize_storage_path(self._append_path_to_prefix(path)) if dir_path: dir_path += '/' items = list() for blob in self.client.list_blobs(self.container, prefix=dir_path, delimiter='/'): <IF_STMT> items.append(self._strip_prefix_from_path(blob.name, dir_path)) else: items.append(self._strip_prefix_from_path(blob.name[:blob.name.find('/', len(dir_path))], dir_path)) return items",if type(blob) == Blob:
"def diff(self, resources): model = self.manager.resource_type for r in resources: hlabels = self.resolve_labels(r['projectId']) if not hlabels: continue delta = False rlabels = r.get('labels', {}) for k, v in hlabels.items(): if k not in rlabels or rlabels[k] != v: delta = True if not delta: continue rlabels = dict(rlabels) rlabels.update(hlabels) <IF_STMT> yield ('update', model.get_label_params(r, rlabels))",if delta:
def favorite(id): note = Note.query.get_or_404(id) if current_user != note.author: abort(403) else: <IF_STMT> note.is_favorite = True note.updated_date = datetime.utcnow() db.session.commit() flash('Note marked as favorite') else: note.is_favorite = False note.updated_date = datetime.utcnow() db.session.commit() flash('Note removed as favorite') return redirect(request.referrer),if not note.is_favorite:
"def enter_standby_instances(self, group_name, instance_ids, should_decrement): group = self.autoscaling_groups[group_name] original_size = group.desired_capacity standby_instances = [] for instance_state in group.instance_states: <IF_STMT> instance_state.lifecycle_state = 'Standby' standby_instances.append(instance_state) if should_decrement: group.desired_capacity = group.desired_capacity - len(instance_ids) group.set_desired_capacity(group.desired_capacity) return (standby_instances, original_size, group.desired_capacity)",if instance_state.instance.id in instance_ids:
"def _child_complete_hook(self, child_task): if child_task.task_spec == self.main_child_task_spec or self._should_cancel(child_task.task_spec): for sibling in child_task.parent.children: <IF_STMT> if sibling.task_spec == self.main_child_task_spec or (isinstance(sibling.task_spec, BoundaryEvent) and (not sibling._is_finished())): sibling.cancel() for t in child_task.workflow._get_waiting_tasks(): t.task_spec._update(t)",if sibling != child_task:
"def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue <IF_STMT> if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return (groups, words)",if word not in ignored and len(word) >= 2:
"def runTest(self): """"""This function will call api providing list of op_class"""""" if self.is_positive_test: response = indexes_utils.api_create_index_get_op_class(self) el<IF_STMT> with patch(self.mock_data['function_name'], side_effect=eval(self.mock_data['return_value'])): response = indexes_utils.api_create_index_get_op_class(self) indexes_utils.assert_status_code(self, response)",if self.mocking_required:
"def fn(value=None): for i in [-1, 0, 1, 2, 3, 4]: <IF_STMT> continue elif i == 0: yield 0 elif i == 1: yield 1 i = 0 yield value yield 2 else: try: v = i / value except: v = i yield v",if i < 0:
"def _update(self, flag): self._modified = False self._index = {} try: f = _io.open(self._dirfile, 'r', encoding='Latin-1') except OSError: <IF_STMT> raise self._modified = True else: with f: for line in f: line = line.rstrip() key, pos_and_siz_pair = _ast.literal_eval(line) key = key.encode('Latin-1') self._index[key] = pos_and_siz_pair","if flag not in ('c', 'n'):"
"def _network_connections_in_results(data): for plugin_name, plugin_result in data.iteritems(): if plugin_result['status'] == 'error': continue <IF_STMT> continue if 'connections' in plugin_result['device']: for conn in plugin_result['device']['connections']: if conn['connection_type'] == ConnectionType.network.name: return True return False",if 'device' not in plugin_result:
"def close(self) -> None: """"""Stop accepting writes and write file, if needed."""""" if not self._io: raise Exception('FileAvoidWrite does not support empty files.') buf = self.getvalue() self._io.close() try: with open(self._path, encoding='utf-8') as old_f: old_content = old_f.read() <IF_STMT> return except OSError: pass with open(self._path, 'w', encoding='utf-8') as f: f.write(buf)",if old_content == buf:
"def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): <IF_STMT> if name in doc_map: deletes.append(name) elif name in doc_map: if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)",if value == ChangeType.REMOVED:
"def preprocess(self, X: DataFrame, is_train=False, vect_max_features=1000, model_specific_preprocessing=False): X = super().preprocess(X=X) if model_specific_preprocessing: <IF_STMT> feature_types = self._get_types_of_features(X) X = self.preprocess_train(X, feature_types, vect_max_features) else: X = self.pipeline.transform(X) return X",if is_train:
"def setup_child(self, child): child.parent = self if self.document: child.document = self.document <IF_STMT> child.source = self.document.current_source if child.line is None: child.line = self.document.current_line",if child.source is None:
"def _compute_early_outs(self, quotas): for q in quotas: <IF_STMT> self.results[q] = (Quota.AVAILABILITY_ORDERED, 0) elif q.size is None: self.results[q] = (Quota.AVAILABILITY_OK, None) elif q.size == 0: self.results[q] = (Quota.AVAILABILITY_GONE, 0)",if q.closed and (not self._ignore_closed):
"def parse_function(self, l): bracket = l.find('(') fname = l[8:bracket] if self.properties: <IF_STMT> self.props[fname] = 1 self.propget[fname] = 1 elif self.properties[0] == 'propput': self.props[fname] = 1 self.propput[fname] = 1 else: self.functions[fname] = 1 self.properties = None",if self.properties[0] == 'propget':
def SetHelpListButtonStates(self): if self.listHelp.size() < 1: self.buttonHelpListEdit.config(state=DISABLED) self.buttonHelpListRemove.config(state=DISABLED) el<IF_STMT> self.buttonHelpListEdit.config(state=NORMAL) self.buttonHelpListRemove.config(state=NORMAL) else: self.buttonHelpListEdit.config(state=DISABLED) self.buttonHelpListRemove.config(state=DISABLED),if self.listHelp.curselection():
"def param_names() -> FrozenSet[Tuple[str, str]]: """"""Returns all module and parameter names as a set of pairs."""""" out = [] params = current_frame().params for mod_name, bundle in params.items(): <IF_STMT> warnings.warn(f'Invalid entry {mod_name!r} in params {params}') continue for name in bundle: out.append((mod_name, name)) return frozenset(out)","if not isinstance(bundle, Mapping):"
"def _classify_volume(self, ctxt, volumes): normal_volumes = [] replica_volumes = [] for v in volumes: volume_type = self._get_volume_replicated_type(ctxt, v) <IF_STMT> replica_volumes.append(v) else: normal_volumes.append(v) return (normal_volumes, replica_volumes)",if volume_type and v.status == 'available':
"def undump_descriptions_of_all_objects(inf): d = {} for l in inf: dash = l.find('-') if dash == -1: raise l mo = NRE.search(l) <IF_STMT> typstr = l[dash + 1:mo.start(0)] num = int(mo.group(0)) if str(num) != mo.group(0): raise mo.group(0) else: typstr = l[dash + 1:] num = None d[l[:dash]] = (typstr, num) return d",if mo:
"def _real_len(self, s): s_len = 0 in_esc = False prev = ' ' for c in replace_all({'\x00+': '', '\x00-': '', '\x00^': '', '\x01': '', '\t': ' '}, s): <IF_STMT> if c == 'm': in_esc = False elif c == '[' and prev == '\x1b': in_esc = True s_len -= 1 else: s_len += self._display_len(c) prev = c return s_len",if in_esc:
"def update_all(self, include_description=False): if self.background_update is None: episodes = [row[self.C_EPISODE] for row in self] else: episodes = [row[self.C_EPISODE] for index, row in enumerate(self) <IF_STMT>] episodes.extend(self.background_update.episodes) self._update_from_episodes(episodes, include_description)",if index < self.background_update.index
"def _debug_log(self, text, level): if text and 'log' in self.config.sys.debug: <IF_STMT> text = '%slog(%s): %s' % (self.log_prefix, level, text) if self.log_parent is not None: return self.log_parent.log(level, text) else: self.term.write(self._fmt_log(text, level=level))",if not text.startswith(self.log_prefix):
"def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: if not form.has_changed(): continue if self.can_delete and self._should_delete_form(form): continue self.new_objects.append(self.save_new(form, commit=commit)) <IF_STMT> self.saved_forms.append(form) return self.new_objects",if not commit:
"def get_master_info(accounts_config, master): master_info = None for a in accounts_config['accounts']: <IF_STMT> master_info = a break if a['account_id'] == master: master_info = a break if master_info is None: raise ValueError('Master account: %s not found in accounts config' % master) return master_info",if a['name'] == master:
"def update(attr, value=None): if value is not None: setattr(draft, attr, value) <IF_STMT> draft.size = len(value) draft.snippet = draft.calculate_html_snippet(value)",if attr == 'body':
"def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if 'value' in msg: if not self.values: pass <IF_STMT> msg['value'] = self.values[0] else: if isIn(msg['value'], self.unicode_values): idx = indexOf(msg['value'], self.unicode_values) else: idx = indexOf(msg['value'], self.labels) msg['value'] = self._items[self.labels[idx]] msg.pop('options', None) return msg",elif msg['value'] is None:
"def removeEmptyDir(path, removeRoot=True): if not os.path.isdir(path): return _files = os.listdir(path) if len(_files) > 0: for f in _files: if not f.startswith('.') and (not f.startswith('_')): fullpath = os.path.join(path, f) <IF_STMT> removeEmptyDir(fullpath) _files = os.listdir(path) if len(_files) == 0 and removeRoot: Print.info('Removing empty folder:' + path) os.rmdir(path)",if os.path.isdir(fullpath):
"def make_relative_to(self, kwds, relative_to): if relative_to and os.path.dirname(relative_to): dirname = os.path.dirname(relative_to) kwds = kwds.copy() for key in ffiplatform.LIST_OF_FILE_NAMES: if key in kwds: lst = kwds[key] <IF_STMT> raise TypeError(""keyword '%s' should be a list or tuple"" % (key,)) lst = [os.path.join(dirname, fn) for fn in lst] kwds[key] = lst return kwds","if not isinstance(lst, (list, tuple)):"
"def ending(self, state): print_title(' STABLE PINS ') path_lists = trace_graph(state.graph) for k in sorted(state.mapping): print(state.mapping[k].as_line(include_hashes=False)) paths = path_lists[k] for path in paths: <IF_STMT> print('User requirement') continue print('   ', end='') for v in reversed(path[1:]): line = state.mapping[v].as_line(include_hashes=False) print(' <=', line, end='') print() print()",if path == [None]:
"def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split('\n'): line = line.strip() <IF_STMT> continue if ' # ' in line: reason = line.split(' # ')[1].split()[0].lower() if reason == 'scanning': continue retval[line.split(' # ')[0]] = (__info__, __reference__) return retval",if not line or line.startswith('#') or '.' not in line:
"def __str__(self): """"""Returns human readable string representation, useful for debugging."""""" buf = StringIO() for idx, (class_batch_id, class_val) in enumerate(iteritems(self.data)): <IF_STMT> buf.write(u'  ...\n') break buf.write(u'  ClassBatch ""{0}""\n'.format(class_batch_id)) buf.write(u'{0}\n'.format(str(class_val))) return buf.getvalue()",if idx >= TO_STR_MAX_BATCHES:
"def find_caller(stack): """"""Finds info about first non-sqlalchemy call in stack"""""" for frame in stack: module = inspect.getmodule(frame[0]) if not hasattr(module, '__name__'): continue <IF_STMT> continue return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),) log.warning('Transaction from unknown origin') return (None, None, None, None)",if module.__name__.startswith('sqlalchemy'):
"def format_unencoded(self, tokensource, outfile): if self.linenos: self._write_lineno(outfile) for ttype, value in tokensource: color = self._get_color(ttype) for line in value.splitlines(True): <IF_STMT> outfile.write('<%s>%s</>' % (color, line.rstrip('\n'))) else: outfile.write(line.rstrip('\n')) if line.endswith('\n'): if self.linenos: self._write_lineno(outfile) else: outfile.write('\n') if self.linenos: outfile.write('\n')",if color:
"def __new__(cls, name, bases, attrs): klass = type.__new__(cls, name, bases, attrs) if 'cmds' in attrs: cmds = attrs['cmds'] <IF_STMT> cmd_handler_mapping[cmds] = klass else: for cmd in cmds: cmd_handler_mapping[cmd] = klass return klass","if isinstance(cmds, str):"
"def __getattr__(self, key): if key == key.upper(): <IF_STMT> return getattr(self._django_settings, key) elif hasattr(self._default_settings, key): return getattr(self._default_settings, key) raise AttributeError('%r object has no attribute %r' % (self.__class__.__name__, key))","if hasattr(self._django_settings, key):"
"def download_file(url): local_filename = url.split('/')[-1] outfile = os.path.join(AVATAR_DIR, local_filename) if not os.path.isfile(outfile): r = requests.get(url, stream=True) with open(outfile, 'wb') as f: for chunk in r.iter_content(chunk_size=1024): <IF_STMT> f.write(chunk) f.flush() return local_filename",if chunk:
"def check_default(self): if self.check(): self.credentials = [] data = LockedIterator(itertools.product(self.usernames, self.passwords)) self.run_threads(self.threads, self.target_function, data) <IF_STMT> return self.credentials return None",if self.credentials:
"def _process_frame(self, frame_num, frame_im, callback=None): """"""Adds any cuts detected with the current frame to the cutting list."""""" for detector in self._detector_list: cuts = detector.process_frame(frame_num, frame_im) if cuts and callback: callback(frame_im, frame_num) self._cutting_list += cuts for detector in self._sparse_detector_list: events = detector.process_frame(frame_num, frame_im) <IF_STMT> callback(frame_im, frame_num) self._event_list += events",if events and callback:
"def parse(cls, api, json): user = cls(api) setattr(user, '_json', json) for k, v in json.items(): if k == 'created_at': setattr(user, k, parse_datetime(v)) elif k == 'status': setattr(user, k, Status.parse(api, v)) <IF_STMT> if v is True: setattr(user, k, True) else: setattr(user, k, False) else: setattr(user, k, v) return user",elif k == 'following':
def dump_token_list(tokens): for token in tokens: <IF_STMT> writer.write(token.contents) elif token.token_type == TOKEN_VAR: writer.print_expr(token.contents) touch_var(token.contents),if token.token_type == TOKEN_TEXT:
"def parent_path(path): parent_dir = S3FileSystem._append_separator(path) if not s3.is_root(parent_dir): bucket_name, key_name, basename = s3.parse_uri(path) <IF_STMT> parent_dir = S3A_ROOT else: bucket_path = '%s%s' % (S3A_ROOT, bucket_name) key_path = '/'.join(key_name.split('/')[:-1]) parent_dir = s3.abspath(bucket_path, key_path) return parent_dir",if not basename:
"def write_framed_message(self, message): message_length = len(message) total_bytes_sent = 0 while message_length - total_bytes_sent > 0: <IF_STMT> buffer_length = BUFFER_SIZE else: buffer_length = message_length - total_bytes_sent self.write_buffer(message[total_bytes_sent:total_bytes_sent + buffer_length]) total_bytes_sent += buffer_length self.write_buffer_length(0)",if message_length - total_bytes_sent > BUFFER_SIZE:
"def reader(): with tarfile.open(filename, mode='r') as f: names = (each_item.name for each_item in f if sub_name in each_item.name) while True: for name in names: if six.PY2: batch = pickle.load(f.extractfile(name)) else: batch = pickle.load(f.extractfile(name), encoding='bytes') for item in read_batch(batch): yield item <IF_STMT> break",if not cycle:
"def splitOn(sequence, predicate, transformers): result = [] mode = predicate(sequence[0]) tmp = [sequence[0]] for e in sequence[1:]: p = predicate(e) <IF_STMT> result.extend(transformers[mode](tmp)) tmp = [e] mode = p else: tmp.append(e) result.extend(transformers[mode](tmp)) return result",if p != mode:
def stroke(s): keys = [] on_left = True for k in s: if k in 'EU*-': on_left = False if k == '-': continue elif k == '*': keys.append(k) <IF_STMT> keys.append(k + '-') else: keys.append('-' + k) return Stroke(keys),elif on_left:
"def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): if decrypter is None: return False f = open(fname, 'rb') s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: if decrypter is not None: <IF_STMT> return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, 'wb') f.write(s) f.close() return True",if read_only:
def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) if self._shuffle: if self._n_gpus > 1: random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret <IF_STMT> break,if self._mode == 'test':
"def on_epoch_end(self, batch, logs=None): weight_mask_ops = [] for layer in self.prunable_layers: <IF_STMT> if tf.executing_eagerly(): layer.pruning_obj.weight_mask_op() else: weight_mask_ops.append(layer.pruning_obj.weight_mask_op()) K.batch_get_value(weight_mask_ops)","if isinstance(layer, pruning_wrapper.PruneLowMagnitude):"
def stroke(s): keys = [] on_left = True for k in s: <IF_STMT> on_left = False if k == '-': continue elif k == '*': keys.append(k) elif on_left: keys.append(k + '-') else: keys.append('-' + k) return Stroke(keys),if k in 'EU*-':
"def _plot_figure(self, idx): with self.renderer.state(): self.plot.update(idx) <IF_STMT> figure_format = self.renderer.params('fig').objects[0] else: figure_format = self.renderer.fig return self.renderer._figure_data(self.plot, figure_format, as_script=True)[0]",if self.renderer.fig == 'auto':
"def custom_format(slither, result): elements = result['elements'] for element in elements: target = element['additional_fields']['target'] convention = element['additional_fields']['convention'] <IF_STMT> logger.info(f""The following naming convention cannot be patched: \n{result['description']}"") continue _patch(slither, result, element, target)",if convention == 'l_O_I_should_not_be_used':
"def refresh(self): if self._obj: person = self._db.get_person_from_handle(self._obj.get_reference_handle()) <IF_STMT> frel = str(self._obj.get_father_relation()) mrel = str(self._obj.get_mother_relation()) self._title = _('%(frel)s %(mrel)s') % {'frel': frel, 'mrel': mrel} self._value = person.get_primary_name().get_name()",if person:
"def append(self, child): if child not in (None, self): tag = child_tag(self._tag) <IF_STMT> if isinstance(child, Html): if child.tag != tag: child = Html(tag, child) elif not child.startswith('<%s' % tag): child = Html(tag, child) super().append(child)",if tag:
def _forward_main_responses(self): while self._should_keep_going(): line = self._proc.stdout.readline() if self._main_backend_is_fresh and self._looks_like_echo(line): continue <IF_STMT> break with self._response_lock: sys.stdout.write(line) sys.stdout.flush() self._main_backend_is_fresh = False,if not line:
"def forward(self, inputs): x = inputs['image'] out = self.conv0(x) out = self.downsample0(out) blocks = [] for i, conv_block_i in enumerate(self.darknet_conv_block_list): out = conv_block_i(out) <IF_STMT> out.stop_gradient = True if i in self.return_idx: blocks.append(out) if i < self.num_stages - 1: out = self.downsample_list[i](out) return blocks",if i == self.freeze_at:
"def check_backslashes(payload): if payload.count('\\') >= 15: if not settings.TAMPER_SCRIPTS['backslashes']: <IF_STMT> menu.options.tamper = menu.options.tamper + ',backslashes' else: menu.options.tamper = 'backslashes' from src.core.tamper import backslashes payload = backslashes.tamper(payload)",if menu.options.tamper:
"def __init__(self, config_lists): self.lens = len(config_lists) self.spaces = [] for config_list in config_lists: <IF_STMT> key, config = config_list elif isinstance(config_list, str): key = config_list config = None else: raise NotImplementedError('the type of config is Error!!! Please check the config information. Receive the type of config is {}'.format(type(config_list))) self.spaces.append(self._get_single_search_space(key, config)) self.init_tokens()","if isinstance(config_list, tuple):"
"def _source_tuple(af, address, port): if address or port: <IF_STMT> if af == socket.AF_INET: address = '0.0.0.0' elif af == socket.AF_INET6: address = '::' else: raise NotImplementedError(f'unknown address family {af}') return (address, port) else: return None",if address is None:
"def test_compatibility(self) -> None: for expected, user_agent in self.data: result = self.client_get('/compatibility', HTTP_USER_AGENT=user_agent) <IF_STMT> self.assert_json_success(result) elif expected == 'old': self.assert_json_error(result, 'Client is too old') else: assert False",if expected == 'ok':
"def __init__(self, parent_element): if parent_element.items(): self.update(dict(parent_element.items())) for element in parent_element: if len(element) > 0: <IF_STMT> aDict = ListParser(element) else: aDict = DictParser(element) if element.items(): aDict.update(dict(element.items())) self.update({element.tag: aDict}) elif element.items(): self.update({element.tag: dict(element.items())}) else: self.update({element.tag: element.text})",if element.tag == element[0].tag:
"def delta_page(self, x: float=0.0, y: float=0.0) -> None: if y.is_integer(): y = int(y) <IF_STMT> pass elif y < 0: self.page_up(count=-y) elif y > 0: self.page_down(count=y) y = 0 if x == 0 and y == 0: return size = self._widget.page().mainFrame().geometry() self.delta(int(x * size.width()), int(y * size.height()))",if y == 0:
"def reader(self, myself): ok = True line = '' while True: line = sys.stdin.readline().strip() <IF_STMT> if not line: ok = False continue elif not line: break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)",if ok:
"def leave(self, reason=None): try: if self.id.startswith('C'): log.info('Leaving channel %s (%s)', self, self.id) self._bot.webclient.channels_leave(channel=self.id) else: log.info('Leaving group %s (%s)', self, self.id) self._bot.webclient.groups_leave(channel=self.id) except SlackAPIResponseError as e: <IF_STMT> raise RoomError(f'Unable to leave channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e) self._id = None",if e.error == 'user_is_bot':
"def transport_vmware_guestinfo(): rpctool = 'vmware-rpctool' not_found = None if not subp.which(rpctool): return not_found cmd = [rpctool, 'info-get guestinfo.ovfEnv'] try: out, _err = subp.subp(cmd) if out: return out LOG.debug('cmd %s exited 0 with empty stdout: %s', cmd, out) except subp.ProcessExecutionError as e: <IF_STMT> LOG.warning('%s exited with code %d', rpctool, e.exit_code) LOG.debug(e) return not_found",if e.exit_code != 1:
"def handle_noargs(self, **options): from django.conf import settings, global_settings settings._setup() user_settings = module_to_dict(settings._wrapped) default_settings = module_to_dict(global_settings) output = [] for key in sorted(user_settings.keys()): <IF_STMT> output.append('%s = %s  ###' % (key, user_settings[key])) elif user_settings[key] != default_settings[key]: output.append('%s = %s' % (key, user_settings[key])) return '\n'.join(output)",if key not in default_settings:
"def channel_sizes(self): """"""List of channel sizes: [(width, height)]."""""" sizes = [] for channel in self.channel_info: if channel.id == ChannelID.USER_LAYER_MASK: sizes.append((self.mask_data.width, self.mask_data.height)) <IF_STMT> sizes.append((self.mask_data.real_width, self.mask_data.real_height)) else: sizes.append((self.width, self.height)) return sizes",elif channel.id == ChannelID.REAL_USER_LAYER_MASK:
"def get(self, key, default=None, version=None): fname = self._key_to_file(key, version) try: with io.open(fname, 'rb') as f: if not self._is_expired(f): return pickle.loads(zlib.decompress(f.read())) except IOError as e: <IF_STMT> raise return default",if e.errno != errno.ENOENT:
"def check_grads(grads_and_vars): has_nan_ops = [] amax_ops = [] for grad, _ in grads_and_vars: <IF_STMT> if isinstance(grad, tf.IndexedSlices): x = grad.values else: x = grad has_nan_ops.append(tf.reduce_any(tf.is_nan(x))) amax_ops.append(tf.reduce_max(tf.abs(x))) has_nan = tf.reduce_any(has_nan_ops) amax = tf.reduce_max(amax_ops) return (has_nan, amax)",if grad is not None:
"def daily(self, component): with component.repository.lock: path = self.get_linguas_path(component) <IF_STMT> self.commit_and_push(component, [path])","if self.sync_linguas(component, path):"
"def _set_posonly_args_def(self, argmts, vals): for v in vals: argmts.posonlyargs.append(v['arg']) d = v['default'] if d is not None: argmts.defaults.append(d) <IF_STMT> self._set_error('non-default argument follows default argument')",elif argmts.defaults:
"def isOrHasChild(parent, child): while child: if compare(parent, child): return True child = child.parentNode if not child: return False <IF_STMT> child = None return False",if child.nodeType != 1:
def Proc2(IntParIO): IntLoc = IntParIO + 10 while 1: <IF_STMT> IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 if EnumLoc == Ident1: break return IntParIO,if Char1Glob == 'A':
"def _GetParserChains(self, events): """"""Return a dict with a plugin count given a list of events."""""" parser_chains = {} for event in events: parser_chain = getattr(event, 'parser', None) if not parser_chain: continue <IF_STMT> parser_chains[parser_chain] += 1 else: parser_chains[parser_chain] = 1 return parser_chains",if parser_chain in parser_chains:
"def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): if not isinstance(values, list): values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: <IF_STMT> continue if not isinstance(key, bytes): key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield (url_quote_plus(key) + '=' + url_quote_plus(value))",if value is None:
def getZoneOffset(d): zoffs = 0 try: if d['zulu'] == None: zoffs = 60 * int(d['tzhour']) + int(d['tzminute']) <IF_STMT> zoffs = -zoffs except TypeError: pass return zoffs,if d['tzsign'] != '-':
"def run(self): predictor = DefaultPredictor(self.cfg) while True: task = self.task_queue.get() <IF_STMT> break idx, data = task result = predictor(data) self.result_queue.put((idx, result))","if isinstance(task, AsyncPredictor._StopToken):"
"def _VarRefOrWord(node, dynamic_arith): with tagswitch(node) as case: if case(arith_expr_e.VarRef): return True elif case(arith_expr_e.Word): <IF_STMT> return True return False",if dynamic_arith:
"def command(self, reset=True, wait=True, wait_all=False, quiet=False): try: <IF_STMT> return self._success(_('Loaded metadata index')) else: return self._error(_('Failed to load metadata index')) except IOError: return self._error(_('Failed to decrypt configuration, please log in!'))","if self._idx(reset=reset, wait=wait, wait_all=wait_all, quiet=quiet):"
"def init_weights(self): for module in self.decoder.modules(): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=0.02) elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) <IF_STMT> module.bias.data.zero_() for p in self.generator.parameters(): if p.dim() > 1: xavier_uniform_(p) else: p.data.zero_()","if isinstance(module, nn.Linear) and module.bias is not None:"
"def write_conditional_formatting(worksheet): """"""Write conditional formatting to xml."""""" wb = worksheet.parent for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules): cf = Element('conditionalFormatting', {'sqref': range_string}) for rule in rules: <IF_STMT> if rule.dxf != DifferentialStyle(): rule.dxfId = len(wb._differential_styles) wb._differential_styles.append(rule.dxf) cf.append(rule.to_tree()) yield cf",if rule.dxf is not None:
"def _format_changelog(self, changelog): """"""Format the changelog correctly and convert it to a list of strings"""""" if not changelog: return changelog new_changelog = [] for line in changelog.strip().split('\n'): line = line.strip() <IF_STMT> new_changelog.extend(['', line]) elif line[0] == '-': new_changelog.append(line) else: new_changelog.append('  ' + line) if not new_changelog[0]: del new_changelog[0] return new_changelog",if line[0] == '*':
"def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split('/', 1) <IF_STMT> comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)",if parts[0]:
"def __str__(self): result = [] for mask, quality in self._parsed: <IF_STMT> mask = '%s;q=%0.*f' % (mask, min(len(str(quality).split('.')[1]), 3), quality) result.append(mask) return ', '.join(result)",if quality != 1:
"def allprocs(self): common.set_plugin_members(self) tasksaddr = self.addr_space.profile.get_symbol('_tasks') queue_entry = obj.Object('queue_entry', offset=tasksaddr, vm=self.addr_space) seen = [tasksaddr] for task in queue_entry.walk_list(list_head=tasksaddr): <IF_STMT> proc = task.bsd_info.dereference_as('proc') yield proc seen.append(task.obj_offset)",if task.bsd_info and task.obj_offset not in seen:
"def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug('__walk_dir_tree. START dir=%s', dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith('py'): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) <IF_STMT> ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list",elif os.path.isdir(current):
"def get_code(self, address: Address) -> bytes: validate_canonical_address(address, title='Storage Address') code_hash = self.get_code_hash(address) if code_hash == EMPTY_SHA3: return b'' else: try: return self._journaldb[code_hash] except KeyError: raise MissingBytecode(code_hash) from KeyError finally: <IF_STMT> self._accessed_bytecodes.add(address)",if code_hash in self._get_accessed_node_hashes():
"def _strftime(value): if datetime: if isinstance(value, datetime.datetime): return '%04d%02d%02dT%02d:%02d:%02d' % (value.year, value.month, value.day, value.hour, value.minute, value.second) if not isinstance(value, (TupleType, time.struct_time)): <IF_STMT> value = time.time() value = time.localtime(value) return '%04d%02d%02dT%02d:%02d:%02d' % value[:6]",if value == 0:
def _read_mol2_records(filename): lines = [] start = True with open(filename) as handle: for line in handle: if line.startswith('@<TRIPOS>MOLECULE'): <IF_STMT> start = False else: yield lines lines = [] lines.append(line),if start:
"def set_column_strategy(self, attrs, strategy, opts=None, opts_only=False): strategy = self._coerce_strat(strategy) self.is_class_strategy = False for attr in attrs: cloned = self._generate() cloned.strategy = strategy cloned._generate_path(self.path, attr, 'column') cloned.propagate_to_loaders = True if opts: cloned.local_opts.update(opts) <IF_STMT> cloned.is_opts_only = True cloned._set_path_strategy() self.is_class_strategy = False",if opts_only:
"def decryptBlock(self, encryptedBlock): """"""Decrypt a single block"""""" if self.decryptBlockCount == 0: <IF_STMT> self.prior_CT_block = encryptedBlock return '' else: assert len(self.iv) == self.blockSize, 'Bad IV size on CBC decryption' self.prior_CT_block = self.iv dct = self.baseCipher.decryptBlock(encryptedBlock) ' XOR the prior decrypted CT with the prior CT ' dct_XOR_priorCT = xor(self.prior_CT_block, dct) self.prior_CT_block = encryptedBlock return dct_XOR_priorCT",if self.iv == None:
"def frontend_visible_config(config_dict): visible_dict = {} for name in CLIENT_WHITELIST: if name.lower().find('secret') >= 0: raise Exception('Cannot whitelist secrets: %s' % name) if name in config_dict: visible_dict[name] = config_dict.get(name, None) <IF_STMT> visible_dict['BRANDING'] = visible_dict.get('BRANDING', {}) visible_dict['BRANDING']['logo'] = config_dict['ENTERPRISE_LOGO_URL'] return visible_dict",if 'ENTERPRISE_LOGO_URL' in config_dict:
"def write(self, s): if self.closed: raise ValueError('write to closed file') if type(s) not in (unicode, str, bytearray): if isinstance(s, unicode): s = unicode.__getitem__(s, slice(None)) elif isinstance(s, str): s = str.__str__(s) <IF_STMT> s = bytearray.__str__(s) else: raise TypeError('must be string, not ' + type(s).__name__) return self.shell.write(s, self.tags)","elif isinstance(s, bytearray):"
"def __get_kb_shortcuts(directory, filename, default_shortcuts, min_shortcuts): shortcutstr, source = __read_first_in_directory_tree(directory, filename) if shortcutstr is None: shortcutstr = __read_or_default(filename, default_shortcuts) <IF_STMT> source = '[default kb_shortcuts]' else: source = filename kb_shortcuts = __parse_kb_shortcuts(shortcutstr, min_shortcuts, source) return kb_shortcuts",if shortcutstr == default_shortcuts:
"def demo(): d = StatusProgressDialog('A Demo', 'Doing something...') import win32api for i in range(100): <IF_STMT> d.SetText('Getting there...') if i == 90: d.SetText('Nearly done...') win32api.Sleep(20) d.Tick() d.Close()",if i == 50:
"def __getattribute__(self, item): try: val = self[item] if isinstance(val, str): val = import_string(val) <IF_STMT> val = [import_string(v) if isinstance(v, str) else v for v in val] self[item] = val except KeyError: val = super(ObjDict, self).__getattribute__(item) return val","elif isinstance(val, (list, tuple)):"
"def clear(self, key: Optional[str]=None): with self.lock: <IF_STMT> try: rv = self.data[key] self._heap_acc.remove((rv.acc, key)) self._heap_exp.remove((rv.exp, key)) del self.data[key] return except Exception: return self.data.clear() self._heap_acc = [] self._heap_exp = []",if key is not None:
"def resolve(self, path): match = self.regex.search(path) if match: kwargs = match.groupdict() <IF_STMT> args = () else: args = match.groups() kwargs.update(self.default_args) return ResolverMatch(self.callback, args, kwargs, self.name)",if kwargs:
"def check_selected(menu, path): selected = False if 'url' in menu: chop_index = menu['url'].find('?') if chop_index == -1: selected = path.startswith(menu['url']) else: selected = path.startswith(menu['url'][:chop_index]) if 'menus' in menu: for m in menu['menus']: _s = check_selected(m, path) <IF_STMT> selected = True if selected: menu['selected'] = True return selected",if _s:
"def check_match(word, word_list): matches = set() not_matches = set() for word2 in word_list: match = truncate_qgram(word, word2) <IF_STMT> matches.add((word, word2)) else: not_matches.add((word, word2)) return (matches, not_matches)",if match > 0.6:
"def _fatal_error(self, exc, message='Fatal error on pipe transport'): if isinstance(exc, (BrokenPipeError, ConnectionResetError)): <IF_STMT> logger.debug('%r: %s', self, message, exc_info=True) else: self._loop.call_exception_handler({'message': message, 'exception': exc, 'transport': self, 'protocol': self._protocol}) self._close(exc)",if self._loop.get_debug():
"def remove_existing_header(contents): """"""remove existing legal header, if any"""""" retval = [] skipping = False start_pattern = re.compile('^(/[*]BEGIN_LEGAL)|(#BEGIN_LEGAL)') stop_pattern = re.compile('^[ ]*(END_LEGAL[ ]?[*]/)|(#[ ]*END_LEGAL)') for line in contents: if start_pattern.match(line): skipping = True if skipping == False: retval.append(line) <IF_STMT> skipping = False return retval",if stop_pattern.match(line):
"def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict['model'].items(): for model in value: if model.endswith('Meta'): model_meta = value[model] <IF_STMT> model_param = value[model] LOGGER.info('load model') self.set_model_meta(model_meta) self.set_model_param(model_param) self.loss = self.get_loss_function()",if model.endswith('Param'):
"def __call__(self, exc_type, exc_value, exc_tb): if not isinstance(exc_value, SystemExit): enriched_tb = add_missing_qt_frames(exc_tb) if exc_tb else exc_tb for handler in self._handlers: <IF_STMT> break","if handler.handle(exc_type, exc_value, enriched_tb):"
"def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == ';': return i elif c == ""'"" or c == '""': i = g.skip_string(s, i) elif g.match(s, i, '//'): i = g.skip_to_end_of_line(s, i) <IF_STMT> i = g.skip_block_comment(s, i) else: i += 1 return i","elif g.match(s, i, '/*'):"
"def validate(self, signature, timestamp, nonce): if not self.token: raise WeixinMsgError('weixin token is missing') if self.expires_in: try: timestamp = int(timestamp) except ValueError: return False delta = time.time() - timestamp <IF_STMT> return False values = [self.token, str(timestamp), str(nonce)] s = ''.join(sorted(values)) hsh = hashlib.sha1(s.encode('utf-8')).hexdigest() return signature == hsh",if delta < 0 or delta > self.expires_in:
"def terminate(self): """"""Terminates process (sends SIGTERM)"""""" if not self._proc is None: <IF_STMT> self._proc.terminate() elif HAS_SUBPROCESS: self._proc.send_signal(15) else: self._proc.terminate() self._proc = None if IS_WINDOWS: self._stdout.close() self._cancel.cancel()",if IS_WINDOWS:
"def clear_bijector(bijector, _, state): if not isinstance(bijector, tfp.bijectors.Bijector): return _clear_bijector_cache(bijector) if isinstance(bijector, tfp.bijectors.Chain): for m in bijector.submodules: <IF_STMT> _clear_bijector_cache(m) return state","if isinstance(m, tfp.bijectors.Bijector):"
"def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return (args, dict(kwargs)) except (TypeError, ValueError): args, kwargs = ((), {}) if a is not None: if isinstance(a, dict): args = tuple() kwargs = a <IF_STMT> if isinstance(a[-1], dict): args, kwargs = (a[0:-1], a[-1]) else: args = a kwargs = {} return (args, kwargs)","elif isinstance(a, tuple):"
"def do_DELE(self, path): """"""Delete the specified file."""""" try: path = self.ftp_path(path) <IF_STMT> self.respond(b'550 Failed to delete file.') else: with self.config.vfs.check_access(path=path, user=self._uid, perms='w'): self.config.vfs.remove(path) self.respond(b'250 File removed.') except FSOperationNotPermitted: self.respond(b'500 Operation not permitted.') except (fs.errors.FSError, FilesystemError, FTPPrivilegeException): self.respond(b'550 Failed to delete file.')",if not self.config.vfs.isfile(path):
"def _get_conn(self): """"""Get ServerProxy instance"""""" if self.username and self.password: <IF_STMT> raise NotImplementedError() secure = self.scheme == 'https' return self.sp(self.uri, transport=BasicAuthTransport(secure, self.username, self.password), **self.sp_kwargs) return self.sp(self.uri, **self.sp_kwargs)",if self.scheme == 'scgi':
"def output(self): """"""Transform self into a list of (name, value) tuples."""""" header_list = [] for k, v in self.items(): <IF_STMT> k = self.encode(k) if not isinstance(v, basestring): v = str(v) if isinstance(v, unicodestr): v = self.encode(v) k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list","if isinstance(k, unicodestr):"
"def gprv_implicit_orax(ii): for i, op in enumerate(_gen_opnds(ii)): <IF_STMT> if op.name == 'REG0' and op_luf(op, 'GPRv_SB'): continue else: return False elif i == 1: if op.name == 'REG1' and op_luf(op, 'OrAX'): continue else: return False else: return False return True",if i == 0:
"def one_xmm_reg_imm8(ii): i, j, n = (0, 0, 0) for op in _gen_opnds(ii): <IF_STMT> n += 1 elif op_imm8(op): i += 1 elif op_imm8_2(op): j += 1 else: return False return n == 1 and i == 1 and (j <= 1)",if op_reg(op) and op_xmm(op):
"def pa(s, l, tokens): for attrName, attrValue in attrs: <IF_STMT> raise ParseException(s, l, 'no matching attribute ' + attrName) if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue: raise ParseException(s, l, ""attribute '%s' has value '%s', must be '%s'"" % (attrName, tokens[attrName], attrValue))",if attrName not in tokens:
"def __code_color(self, code): if code in self.last_dist.keys(): if int(code) == 0: return self.screen.markup.GREEN <IF_STMT> return self.screen.markup.MAGENTA else: return self.screen.markup.RED else: return ''",elif int(code) == 314:
def loop_check(self): in_loop = [] for node in self.nodes: node.dfs_loop_status = 'DFS_UNCHECKED' for node in self.nodes: if node.dfs_loop_status == 'DFS_UNCHECKED': self.dfs_loop_search(node) <IF_STMT> in_loop.append(node) for node in self.nodes: del node.dfs_loop_status return in_loop,if node.dfs_loop_status == 'DFS_LOOP_INSIDE':
"def _append_modifier(code, modifier): if modifier == 'euro': <IF_STMT> return code + '.ISO8859-15' _, _, encoding = code.partition('.') if encoding in ('ISO8859-15', 'UTF-8'): return code if encoding == 'ISO8859-1': return _replace_encoding(code, 'ISO8859-15') return code + '@' + modifier",if '.' not in code:
"def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args): triggered = False for i in self._touchable_widgets: if i.collide_point(touch.x, touch.y): triggered = True <IF_STMT> i.on_touch_down(touch) elif touch_event == 'move': i.on_touch_move(touch, *args) elif touch_event == 'up': i.on_touch_up(touch) return triggered",if touch_event == 'down':
"def body(self): order = ['ok_header', 'affected_rows', 'last_insert_id', 'server_status', 'warning_count', 'state_track', 'info'] string = b'' for key in order: item = getattr(self, key) section_pack = b'' <IF_STMT> continue elif isinstance(item, bytes): section_pack = item else: section_pack = getattr(self, key).toStringPacket() string += section_pack self.setBody(string) return self._body",if item is None:
"def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) elif op_luf_start(op, 'GPRv'): types.append('v') <IF_STMT> types.append('z') elif op_luf_start(op, 'GPRy'): types.append('y') else: die('Unhandled op type {}'.format(op)) return types","elif op_luf_start(op, 'GPRz'):"
"def load_name(self, name): if name in self.args: index = self.args[name] <IF_STMT> self.add_opcodes(JavaOpcodes.ALOAD_2(), java.Map.get(name)) else: self.add_opcodes(JavaOpcodes.ALOAD_1(), java.Array.get(index)) else: self.add_opcodes(ALOAD_name('#module'), python.Object.get_attribute(name))",if index is None:
"def get_field_type(self, name): fkey = (name, self.dummy) target = None op, name = name.split('_', 1) if op in {'delete', 'insert', 'update'}: target = super().get_field_type(name) if target is None: module, edb_name = self.get_module_and_name(name) target = self.edb_schema.get((module, edb_name), None) <IF_STMT> target = self.convert_edb_to_gql_type(target) self._fields[fkey] = target return target",if target is not None:
"def _parse_lines(self, lines): for line in lines: self.size += len(line) words = line.strip().split('\t') <IF_STMT> wset = set(words[1:]) if words[0] in self.WORDS: self.WORDS[words[0]] |= wset else: self.WORDS[words[0]] = wset",if len(words) > 1:
def get_new_id(self) -> str: with db.session.no_autoflush: identifier = self.issued_at.strftime('%Y%mU-') + '%06d' % (EventInvoice.query.count() + 1) count = EventInvoice.query.filter_by(identifier=identifier).count() <IF_STMT> return identifier return self.get_new_id(),if count == 0:
"def complete_use(self, text, *args, **kwargs): if text: all_possible_matches = filter(lambda x: x.startswith(text), self.main_modules_dirs) matches = set() for match in all_possible_matches: head, sep, tail = match[len(text):].partition('.') <IF_STMT> sep = '' matches.add(''.join((text, head, sep))) return list(matches) else: return self.main_modules_dirs",if not tail:
"def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: if isinstance(arg_type, ScalarArg): result.append(arg_type.dtype) <IF_STMT> result.append(None) if arg_type.with_offset: result.append(np.int64) else: raise RuntimeError('arg type not understood: %s' % type(arg_type)) return result","elif isinstance(arg_type, VectorArg):"
"def psea(pname): """"""Parse PSEA output file."""""" fname = run_psea(pname) start = 0 ss = '' with open(fname) as fp: for l in fp: <IF_STMT> start = 1 continue if not start: continue if l[0] == '\n': break ss = ss + l[0:-1] return ss",if l[0:6] == '>p-sea':
"def pad_with_zeros(logits, labels): """"""Pad labels on the length dimension to match logits length."""""" with tf.name_scope('pad_with_zeros', values=[logits, labels]): logits, labels = pad_to_same_length(logits, labels) <IF_STMT> logits, labels = pad_to_same_length(logits, labels, axis=2) return (logits, labels)",if len(labels.shape) == 3:
"def set_rating(self, value, songs, librarian): count = len(songs) if count > 1 and config.getboolean('browsers', 'rating_confirm_multiple'): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _('Change _Rating'), count, value) <IF_STMT> return for song in songs: song['~#rating'] = value librarian.changed(songs)",if dialog.run() != Gtk.ResponseType.YES:
"def test_schema_plugin_name_mismatch(self): for k, v in manager.resources.items(): for fname, f in v.filter_registry.items(): <IF_STMT> continue self.assertIn(fname, f.schema['properties']['type']['enum']) for aname, a in v.action_registry.items(): self.assertIn(aname, a.schema['properties']['type']['enum'])","if fname in ('or', 'and', 'not'):"
"def run(self, elem): """"""Inline check for attrs at start of tail."""""" if elem.tail: m = self.INLINE_RE.match(elem.tail) <IF_STMT> self.assign_attrs(elem, m.group(1)) elem.tail = elem.tail[m.end():]",if m:
"def _traverse(op): if topi.tag.is_broadcast(op.tag): if not op.same_as(output.op): <IF_STMT> const_ops.append(op) else: ewise_ops.append(op) for tensor in op.input_tensors: if isinstance(tensor.op, tvm.te.PlaceholderOp): ewise_inputs.append((op, tensor)) else: _traverse(tensor.op) else: assert op.tag == 'dense_pack' dense_res.append(op)",if not op.axis:
"def toPostArgs(self): """"""Return all arguments with openid. in front of namespaced arguments."""""" args = {} for ns_uri, alias in self.namespaces.iteritems(): <IF_STMT> continue if alias == NULL_NAMESPACE: ns_key = 'openid.ns' else: ns_key = 'openid.ns.' + alias args[ns_key] = ns_uri for (ns_uri, ns_key), value in self.args.iteritems(): key = self.getKey(ns_uri, ns_key) args[key] = value.encode('UTF-8') return args",if self.namespaces.isImplicit(ns_uri):
"def test_issue_530_async(self): try: rtm_client = RTMClient(token='I am not a token', run_async=True) await rtm_client.start() self.fail('Raising an error here was expected') except Exception as e: self.assertEqual(""The request to the Slack API failed.\nThe server responded with: {'ok': False, 'error': 'invalid_auth'}"", str(e)) finally: <IF_STMT> rtm_client.stop()",if not rtm_client._stopped:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.add_path(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False for pattern in S3Sync.exclude_files: <IF_STMT> exclude = True break if not exclude: full_path = root + '/' + file if include_checksums: checksum = self._hash_file(full_path) else: checksum = '' file_list[relpath + file] = [full_path, checksum] return file_list","if fnmatch.fnmatch(file, pattern):"
"def globs_relative_to_buildroot(self): buildroot = get_buildroot() globs = [] for bundle in self.bundles: fileset = bundle.fileset <IF_STMT> continue elif hasattr(fileset, 'filespec'): globs += bundle.fileset.filespec['globs'] else: globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()] super_globs = super().globs_relative_to_buildroot() if super_globs: globs += super_globs['globs'] return {'globs': globs}",if fileset is None:
"def __getstate__(self): state = super(_ExpressionBase, self).__getstate__() for i in _ExpressionBase.__pickle_slots__: state[i] = getattr(self, i) if safe_mode: state['_parent_expr'] = None if self._parent_expr is not None: _parent_expr = self._parent_expr() <IF_STMT> state['_parent_expr'] = _parent_expr return state",if _parent_expr is not None:
"def content_state_equal(v1, v2): """"""Test whether two contentState structures are equal, ignoring 'key' properties"""""" if type(v1) != type(v2): return False if isinstance(v1, dict): if set(v1.keys()) != set(v2.keys()): return False return all((k == 'key' or content_state_equal(v, v2[k]) for k, v in v1.items())) elif isinstance(v1, list): <IF_STMT> return False return all((content_state_equal(a, b) for a, b in zip(v1, v2))) else: return v1 == v2",if len(v1) != len(v2):
"def process_qemu_job(file_path: str, arch_suffix: str, root_path: Path, results_dict: dict, uid: str): result = check_qemu_executability(file_path, arch_suffix, root_path) if result: <IF_STMT> tmp_dict = dict(results_dict[uid]['results']) tmp_dict.update({arch_suffix: result}) else: tmp_dict = {arch_suffix: result} results_dict[uid] = {'path': file_path, 'results': tmp_dict}",if uid in results_dict:
"def _eq_meet(a, b): a_dtype, b_dtype = (_dtype(a), _dtype(b)) if a_dtype != b_dtype: higher_dtype = dtypes.promote_types(a_dtype, b_dtype) <IF_STMT> a = convert_element_type(a, b_dtype) else: b = convert_element_type(b, a_dtype) return eq(a, b)",if higher_dtype == a_dtype:
"def _assign(self, trans, code): try: <IF_STMT> trans.order = self.order_qs().get(code=code.rsplit('-', 1)[1], event__slug__iexact=code.rsplit('-', 1)[0]) else: trans.order = self.order_qs().get(code=code.rsplit('-', 1)[-1]) except Order.DoesNotExist: return JsonResponse({'status': 'error', 'message': _('Unknown order code')}) else: return self._retry(trans)",if '-' in code:
"def _recalculate(self): parent_path = tuple(self._get_parent_path()) if parent_path != self._last_parent_path: spec = self._path_finder(self._name, parent_path) if spec is not None and spec.loader is None: <IF_STMT> self._path = spec.submodule_search_locations self._last_parent_path = parent_path return self._path",if spec.submodule_search_locations:
"def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) <IF_STMT> attr_list[3] |= termios.ECHO if hasattr(signal, 'SIGTTOU'): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)",if not attr_list[3] & termios.ECHO:
def clean(self): with self._lock: min_index = min(self.indexes) <IF_STMT> self.repository = self.repository[min_index:] for pos in xrange(len(self.indexes)): self.indexes[pos] -= min_index,if min_index >= self.CLEANUP_NUM:
"def generate_changes(self, old): from weblate.trans.models.change import Change tracked = (('slug', Change.ACTION_RENAME_PROJECT),) for attribute, action in tracked: old_value = getattr(old, attribute) current_value = getattr(self, attribute) <IF_STMT> Change.objects.create(action=action, old=old_value, target=current_value, project=self, user=self.acting_user)",if old_value != current_value:
"def get_voices(cls): cmd = ['flite', '-lv'] voices = [] with tempfile.SpooledTemporaryFile() as out_f: subprocess.call(cmd, stdout=out_f) out_f.seek(0) for line in out_f: <IF_STMT> voices.extend([x.strip() for x in line[18:].split() if x.strip()]) return voices",if line.startswith('Voices available: '):
"def __init__(self, *args, **kwargs): dict.__init__(self, *args, **kwargs) for key, value in self.items(): if not isinstance(key, string_types): raise TypeError('key must be a str, not {}'.format(type(key))) if not isinstance(value, NUMERIC_TYPES): raise TypeError('value must be a NUMERIC_TYPES, not {}'.format(type(value))) <IF_STMT> self[key] = float(value)","if not isinstance(value, float):"
"def read_track_raw(self, redundancy=1): self._log('read track raw') data = [] await self.lower.write([CMD_READ_RAW, redundancy]) while True: packet = await self.lower.read() if packet[-1] == 255: raise GlasgowAppletError('FIFO overflow while reading track') <IF_STMT> data.append(packet[:-1]) return b''.join(data) else: data.append(packet)",elif packet[-1] == 254:
"def init(self): """"""Initialize from the database"""""" self.__effect = None if self.effectID: self.__effect = next((x for x in self.fighter.item.effects.values() if x.ID == self.effectID), None) <IF_STMT> pyfalog.error('Effect (id: {0}) does not exist', self.effectID) return self.build()",if self.__effect is None:
"def remove(self): key = self._key if key not in _key_to_collection: raise exc.InvalidRequestError('No listeners found for event %s / %r / %s ' % (self.target, self.identifier, self.fn)) dispatch_reg = _key_to_collection.pop(key) for collection_ref, listener_ref in dispatch_reg.items(): collection = collection_ref() listener_fn = listener_ref() <IF_STMT> collection.remove(self.with_wrapper(listener_fn))",if collection is not None and listener_fn is not None:
def atbash(s): translated = '' for i in range(len(s)): n = ord(s[i]) if s[i].isalpha(): <IF_STMT> x = n - ord('A') translated += chr(ord('Z') - x) if s[i].islower(): x = n - ord('a') translated += chr(ord('z') - x) else: translated += s[i] return translated,if s[i].isupper():
"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_cost_: res += prefix + 'cost <\n' res += self.cost_.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt = 0 for e in self.version_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'Version%s {\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '}\n' cnt += 1 return res",if printElemNumber:
"def readwrite(obj, flags): try: if flags & select.POLLIN: obj.handle_read_event() if flags & select.POLLOUT: obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: <IF_STMT> obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()",if e.args[0] not in _DISCONNECTED:
"def mro(cls): if self.ready: <IF_STMT> B2.__bases__ = (B1,) if cls.__name__ == 'B2': B1.__bases__ = (B2,) return type.mro(cls)",if cls.__name__ == 'B1':
"def create_hyperswap_volume(self, vol_name, size, units, pool, opts): vol_name = '""%s""' % vol_name params = [] if opts['rsize'] != -1: is_dr_pool = self.is_volume_type_dr_pools(pool, opts) <IF_STMT> self.check_data_reduction_pool_params(opts) params = self._get_hyperswap_volume_create_params(opts, is_dr_pool) hyperpool = '%s:%s' % (pool, opts['peer_pool']) self.ssh.mkvolume(vol_name, six.text_type(size), units, hyperpool, params)",if is_dr_pool:
"def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: <IF_STMT> continue if self.can_delete and self._should_delete_form(form): continue self.new_objects.append(self.save_new(form, commit=commit)) if not commit: self.saved_forms.append(form) return self.new_objects",if not form.has_changed():
"def create_monitored_items(event, dispatcher): print('Monitored Item') for idx in range(len(event.response_params)): <IF_STMT> nodeId = event.request_params.ItemsToCreate[idx].ItemToMonitor.NodeId print('Node {0} was created'.format(nodeId))",if event.response_params[idx].StatusCode.is_good():
"def close(self, linger=None): if not self.closed and self._fd is not None: for event in list(chain(self._recv_futures or [], self._send_futures or [])): <IF_STMT> try: event.future.cancel() except RuntimeError: pass self._clear_io_state() super(_AsyncSocket, self).close(linger=linger)",if not event.future.done():
"def stop_actors(self, monitor): """"""Maintain the number of workers by spawning or killing as required"""""" if monitor.cfg.workers: num_to_kill = len(self.managed_actors) - monitor.cfg.workers for i in range(num_to_kill, 0, -1): w, kage = (0, sys.maxsize) for worker in self.managed_actors.values(): age = worker.impl.age <IF_STMT> w, kage = (worker, age) self.manage_actor(monitor, w, True)",if age < kage:
"def get_version(module): for key in version_keys: if hasattr(module, key): version = getattr(module, key) <IF_STMT> version = get_version(version) return version return 'Unknown'","if isinstance(version, types.ModuleType):"
"def getBigramProb(self, w1, w2): """"""prob of seeing words w1 w2 next to each other."""""" w1 = w1.lower() w2 = w2.lower() val1 = self.bigrams.get(w1) if val1 != None: val2 = val1.get(w2) <IF_STMT> return val2 return self.addK / (self.getUnigramProb(w1) * self.numUniqueWords + self.numUniqueWords) return 0",if val2 != None:
def _getPartAbbreviation(self): if self._partAbbreviation is not None: return self._partAbbreviation elif '_partAbbreviation' in self._cache: return self._cache['_partAbbreviation'] else: pn = None for e in self.recurse().getElementsByClass('Instrument'): pn = e.partAbbreviation if pn is None: pn = e.instrumentAbbreviation <IF_STMT> break self._cache['_partAbbreviation'] = pn return pn,if pn is not None:
"def set_value(self, value, storedtime=None): self.namespace.acquire_write_lock() try: <IF_STMT> storedtime = time.time() debug('set_value stored time %r expire time %r', storedtime, self.expire_argument) self.namespace.set_value(self.key, (storedtime, self.expire_argument, value), expiretime=self.expire_argument) finally: self.namespace.release_write_lock()",if storedtime is None:
"def setRadioSquare(self, title, square=True): if self.platform == self.MAC: gui.warn('Square radiobuttons not available on Mac, for radiobutton %s', title) elif not self.ttkFlag: for k, v in self.widgetManager.group(WIDGET_NAMES.RadioButton).items(): if k.startswith(title + '-'): <IF_STMT> v.config(indicatoron=1) else: v.config(indicatoron=0) else: gui.warn('Square radiobuttons not available in ttk mode, for radiobutton %s', title)",if square:
"def render_func(self, node): if node.id in DEFAULT_FUNCTIONS: f = DEFAULT_FUNCTIONS[node.id] <IF_STMT> return f.sympy_func if node.id == 'int': return sympy.Function('int_') else: return sympy.Function(node.id)","if f.sympy_func is not None and isinstance(f.sympy_func, sympy.FunctionClass):"
"def __init__(self, source_definition, **kw): super(RekallEFilterArtifacts, self).__init__(source_definition, **kw) for column in self.fields: if 'name' not in column or 'type' not in column: raise errors.FormatError(u'Field definition should have both name and type.') mapped_type = column['type'] <IF_STMT> raise errors.FormatError(u'Unsupported type %s.' % mapped_type)",if mapped_type not in self.allowed_types:
"def run(self, lines): """"""Match and store Fenced Code Blocks in the HtmlStash."""""" text = '\n'.join(lines) while 1: m = FENCED_BLOCK_RE.search(text) <IF_STMT> lang = '' if m.group('lang'): lang = LANG_TAG % m.group('lang') code = CODE_WRAP % (lang, self._escape(m.group('code'))) placeholder = self.markdown.htmlStash.store(code, safe=True) text = '%s\n%s\n%s' % (text[:m.start()], placeholder, text[m.end():]) else: break return text.split('\n')",if m:
"def GetDisplayNameOf(self, pidl, flags): item = pidl_to_item(pidl) if flags & shellcon.SHGDN_FORPARSING: <IF_STMT> return item['name'] else: if flags & shellcon.SHGDN_FORADDRESSBAR: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING else: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING parent = shell.SHGetNameFromIDList(self.pidl, sigdn) return parent + '\\' + item['name'] else: return item['name']",if flags & shellcon.SHGDN_INFOLDER:
def test_buffer_play_stop(filled_buffer): assert filled_buffer.current_position[0] == 0 filled_buffer.play() for _ in range(100): assert filled_buffer.is_playing <IF_STMT> break else: time.sleep(0.001) else: pytest.fail('Did not advance position in buffer while playing.') filled_buffer.stop() assert not filled_buffer.is_playing pos = filled_buffer.current_position for _ in range(10): assert filled_buffer.current_position == pos time.sleep(0.001),if filled_buffer.current_position[0] > 0:
"def delete_service(service): try: win32serviceutil.RemoveService(service) logger.info(""Services: Succesfully removed service '{service}'"".format(service=service)) except pywintypes.error as e: errors = (winerror.ERROR_SERVICE_DOES_NOT_EXIST, winerror.ERROR_SERVICE_NOT_ACTIVE, winerror.ERROR_SERVICE_MARKED_FOR_DELETE) <IF_STMT> logger.exception(""Services: Failed to remove service '{service}'"".format(service=service))",if not any((error == e.winerror for error in errors)):
"def connect_to_server(self, server_cls): server = client = None try: sock, port = bind_unused_port() server = server_cls(ssl_options=_server_ssl_options()) server.add_socket(sock) client = SSLIOStream(socket.socket(), ssl_options=dict(cert_reqs=ssl.CERT_NONE)) yield client.connect(('127.0.0.1', port)) self.assertIsNotNone(client.socket.cipher()) finally: if server is not None: server.stop() <IF_STMT> client.close()",if client is not None:
"def allow_request(self, request, view): request.server = None allow = True view_name = view.get_view_name() allowed_views = [u'System Data', u'Collectd Data', u'Legacy System Data'] if view_name in allowed_views: server_key = view.kwargs.get('server_key') server = server_model.get_server_by_key(server_key) if server: request.server = server server_status = throttle_status(server=server) <IF_STMT> allow = False return allow",if server_status.allow == False:
"def log_start(self, prefix, msg): with self._log_lock: <IF_STMT> if self._last_log_prefix is not None: self._log_file.write('\n') self._log_file.write(prefix) self._log_file.write(msg) self._last_log_prefix = prefix",if self._last_log_prefix != prefix:
"def override(self, user_conf: dict): for k, v in user_conf.items(): <IF_STMT> for subkey, subval in v.items(): self.SEARCH_CONF[subkey] = subval else: setattr(self, k, v)",if k == 'SEARCH_CONF':
"def emit_classattribs(self, typebld): if hasattr(self, '_clrclassattribs'): for attrib_info in self._clrclassattribs: if isinstance(attrib_info, type): ci = clr.GetClrType(attrib_info).GetConstructor(()) cab = CustomAttributeBuilder(ci, ()) <IF_STMT> cab = attrib_info.GetBuilder() else: make_decorator = attrib_info() cab = make_decorator.GetBuilder() typebld.SetCustomAttribute(cab)","elif isinstance(attrib_info, CustomAttributeDecorator):"
"def load_classes(module, base, blacklist): classes = [] for attr in dir(module): attr = getattr(module, attr) if inspect.isclass(attr): if issubclass(attr, base): <IF_STMT> classes.append(attr) return classes",if attr is not base and attr not in blacklist:
"def search_scopes(self, key): for scope in self.scopes: <IF_STMT> return getattr(scope, key) if hasattr(scope, '__getitem__'): if key in scope: return scope[key]","if hasattr(scope, key):"
"def get_cfg_dict(self, with_meta=True): options_dict = self.merged_options if with_meta: if self.plugin: options_dict.update({'package': 'yandextank.plugins.{}'.format(self.plugin)}) <IF_STMT> options_dict.update({'enabled': self.enabled}) return options_dict",if self.enabled is not None:
"def render(self, context): for condition, nodelist in self.conditions_nodelists: <IF_STMT> try: match = condition.eval(context) except VariableDoesNotExist: match = None else: match = True if match: return nodelist.render(context) return ''",if condition is not None:
"def main(): base = sys.argv[1] filenames = sys.argv[2:] out = OutputByLength(base) n = 0 for filename in filenames: print('opening') for record in screed.open(filename): out.save(record.name, record.sequence) n += 1 <IF_STMT> print('...', n)",if n % 10000 == 0:
"def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data['given'] for case in test_data['cases']: if 'result' in case: test_type = 'result' elif 'error' in case: test_type = 'error' <IF_STMT> test_type = 'bench' else: raise RuntimeError('Unknown test type: %s' % json.dumps(case)) yield (given, test_type, case)",elif 'bench' in case:
def readline(self): if self.peek is not None: return '' line = self.file.readline() if not line: return line if self.boundary: if line == self.boundary + '\n': self.peek = line return '' <IF_STMT> self.peek = line return '' return line,if line == self.boundary + '--\n':
"def _get_cache_value(self, key, empty, type): """"""Used internally by the accessor properties."""""" if type is bool: return key in self if key in self: value = self[key] if value is None: return empty <IF_STMT> try: value = type(value) except ValueError: pass return value return None",elif type is not None:
"def _load_from_data(self, data): super(CliCommandHelpFile, self)._load_from_data(data) if isinstance(data, str) or not self.parameters or (not data.get('parameters')): return loaded_params = [] loaded_param = {} for param in self.parameters: loaded_param = next((n for n in data['parameters'] if n['name'] == param.name), None) <IF_STMT> param.update_from_data(loaded_param) loaded_params.append(param) self.parameters = loaded_params",if loaded_param:
def __str__(self): s = super().__str__() if self.print_suggestions: possible_keys = set(self.captured_args) - self.SPECIAL_ARGS <IF_STMT> s += '\nPossible config keys are: {}'.format(possible_keys) return s,if possible_keys:
"def family_add(self, handle_list): if self.active: person = self.get_active() <IF_STMT> while not self.change_person(person): pass else: self.change_person(None) else: self.dirty = True",if person:
"def recv_into(self, buffer, nbytes=None, flags=0): if buffer and nbytes is None: nbytes = len(buffer) elif nbytes is None: nbytes = 1024 if self._sslobj: <IF_STMT> raise ValueError('non-zero flags not allowed in calls to recv_into() on %s' % self.__class__) tmp_buffer = self.read(nbytes) v = len(tmp_buffer) buffer[:v] = tmp_buffer return v else: return socket.recv_into(self, buffer, nbytes, flags)",if flags != 0:
def removeInsideIslands(self): self.CleanPath = [] cleanpath = Path('Path') for path in self.NewPaths: for seg in path: inside = False for island in self.IntersectedIslands: issegin = island.isSegInside(seg) == 1 if issegin: if not seg in island: inside = True break <IF_STMT> cleanpath.append(seg) cleanpath = cleanpath.split2contours() self.CleanPath.extend(cleanpath),if not inside:
"def ETA(self): if self.done: prefix = 'Done' t = self.elapsed else: prefix = 'ETA ' if self.max is None: t = -1 <IF_STMT> t = 0 else: t = float(self.max - self.min) t /= self.cur - self.min t = (t - 1) * self.elapsed return '%s: %s' % (prefix, self.format_duration(t))",elif self.elapsed == 0 or self.cur == self.min:
"def columnToDataIndex(self, columnIndex): c = 0 for dataIndex, accessor in enumerate(self.vectorDataAccessors()): nc = accessor.numColumns() if c + nc > columnIndex: <IF_STMT> return (dataIndex, -1) else: return (dataIndex, columnIndex - c) c += nc raise IndexError(columnIndex)",if nc == 1:
"def as_nodes(self, files): """"""Returns a list of waflib.Nodes from a list of string of file paths"""""" nodes = [] for x in files: if not isinstance(x, str): d = x else: d = self.srcnode.find_node(x) <IF_STMT> raise Errors.WafError(""File '%s' was not found"" % x) nodes.append(d) return nodes",if not d:
def register_extension(ext): nonlocal commands try: parser = subparsers.add_parser(ext.name) <IF_STMT> cmd = ext.plugin() cmd.add_arguments(parser) cmd.__name__ = ext.name commands[ext.name] = cmd.handle else: commands[ext.name] = ext.plugin(parser) except Exception: logger.exception('Error while loading command {}.'.format(ext.name)),"if isinstance(ext.plugin, type) and issubclass(ext.plugin, BaseCommand):"
"def formatweekday(self, day, width): with TimeEncoding(self.locale) as encoding: if width >= 9: names = day_name else: names = day_abbr name = names[day] <IF_STMT> name = name.decode(encoding) return name[:width].center(width)",if encoding is not None:
"def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug('__walk_dir_tree. START dir=%s', dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) <IF_STMT> if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list",if os.path.isfile(current) and f.endswith('py'):
"def _EvalInScriptedSection(self, codeBlock, globals, locals=None): if self.debugManager: self.debugManager.OnEnterScript() <IF_STMT> return self.debugManager.adb.runeval(codeBlock, globals, locals) else: return eval(codeBlock, globals, locals) else: return eval(codeBlock, globals, locals)",if self.debugManager.adb.appDebugger:
"def load_multiple(fh, position=None, end=None): loaded = list() while position < end: new_box = load(fh, position, end) <IF_STMT> print('Error, failed to load box.') return None loaded.append(new_box) position = new_box.position + new_box.size() return loaded",if new_box is None:
"def test_loadTestsFromName__module_not_loaded(self): module_name = 'unittest2.test.dummy' sys.modules.pop(module_name, None) loader = unittest2.TestLoader() try: suite = loader.loadTestsFromName(module_name) self.assertIsInstance(suite, loader.suiteClass) self.assertEqual(list(suite), []) self.assertIn(module_name, sys.modules) finally: <IF_STMT> del sys.modules[module_name]",if module_name in sys.modules:
"def copy_file(s, d, xform=None): with open(s, 'rb') as f: text = f.read() if xform: d, text = xform(d, text) if os.path.exists(d): <IF_STMT> (print >> sys.stderr, 'Overwriting %s.' % d) else: (print >> sys.stderr, 'Not overwriting %s.' % d) return else: (print >> sys.stderr, 'Writing %s.' % d) with open(d, 'wb') as f: f.write(text)",if opts.force:
"def __setitem__(self, index, image): if isinstance(index, slice): tmp_idx = self.current_index slice_ = self.validate_slice(index) del self[slice_] self.extend(image, offset=slice_.start) self.current_index = tmp_idx else: <IF_STMT> raise TypeError('image must be an instance of wand.image.BaseImage, not ' + repr(image)) with self.index_context(index) as index: library.MagickRemoveImage(self.image.wand) library.MagickAddImage(self.image.wand, image.wand)","if not isinstance(image, BaseImage):"
"def _configure_legacy_instrument_class(self): if self.inherits: self.dispatch._update(self.inherits.dispatch) super_extensions = set(chain(*[m._deprecated_extensions for m in self.inherits.iterate_to_root()])) else: super_extensions = set() for ext in self._deprecated_extensions: <IF_STMT> ext._adapt_instrument_class(self, ext)",if ext not in super_extensions:
"def tearDown(self): exc, _, _ = sys.exc_info() if exc: try: <IF_STMT> diags = self.obj.get_error_diagnostics() if diags: for line in diags: ROOT_LOGGER.info(line) except BaseException: pass if self.captured_logger: self.captured_logger.removeHandler(self.log_recorder) self.log_recorder.close() sys.stdout = self.stdout_backup super(BZTestCase, self).tearDown()","if hasattr(self, 'obj') and isinstance(self.obj, SelfDiagnosable):"
"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): <IF_STMT> name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in list(self.unops.items()): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)",if name not in skip:
"def _parse_cachecontrol(self, r): if r not in self._cc_parsed: cch = r.headers.get(b'Cache-Control', b'') parsed = parse_cachecontrol(cch) <IF_STMT> for key in self.ignore_response_cache_controls: parsed.pop(key, None) self._cc_parsed[r] = parsed return self._cc_parsed[r]","if isinstance(r, Response):"
"def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: if isinstance(part, list): part = make_pattern(part) <IF_STMT> for c in part: if c in special_chars: raise GlobError() subpattern.append(part) return ''.join(subpattern)",elif wtree[0] != '':
"def iterjlines(f, header, missing): it = iter(f) if header is None: header = list() peek, it = iterpeek(it, 1) json_obj = json.loads(peek) <IF_STMT> header += [k for k in json_obj.keys() if k not in header] yield tuple(header) for o in it: json_obj = json.loads(o) yield tuple((json_obj[f] if f in json_obj else missing for f in header))","if hasattr(json_obj, 'keys'):"
"def logprob(self, sample): if self._log: return self._prob_dict.get(sample, _NINF) elif sample not in self._prob_dict: return _NINF <IF_STMT> return _NINF else: return math.log(self._prob_dict[sample], 2)",elif self._prob_dict[sample] == 0:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_public_certificate_list().TryMerge(tmp) continue if tt == 16: self.set_max_client_cache_time_in_second(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(""can't specify timeout for non-blocking acquire"") rc = False endtime = None self._cond.acquire() while self._value == 0: <IF_STMT> break if timeout is not None: if endtime is None: endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc",if not blocking:
def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): if self.should_save_model(): self.save_model() if self.should_save_checkpoint(): self.save_checkpoint() <IF_STMT> self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result(),if self.should_eval_model():
"def scrape_me(url_path, **options): host_name = get_host_name(url_path) if not options.get('test', False) else 'test_wild_mode' try: scraper = SCRAPERS[host_name] except KeyError: <IF_STMT> wild_scraper = SchemaScraperFactory.generate(url_path, **options) if not wild_scraper.schema.data: raise NoSchemaFoundInWildMode(url_path) return wild_scraper else: raise WebsiteNotImplementedError(host_name) return scraper(url_path, **options)","if options.get('wild_mode', False):"
"def iter_expressions(self): if not self._isrecord: tri_attr_context = [('target', SPECIAL_INOUT)] else: tri_attr_context = [('_target_o', SPECIAL_OUTPUT), ('_target_oe', SPECIAL_OUTPUT), ('_target_i', SPECIAL_INPUT)] tri_attr_context += [('o', SPECIAL_INPUT), ('oe', SPECIAL_INPUT), ('i', SPECIAL_OUTPUT)] for attr, target_context in tri_attr_context: <IF_STMT> yield (self, attr, target_context)","if getattr(self, attr) is not None:"
"def get_field_values(self, fields): field_values = [] for field in fields: if field == 'title': value = self.get_title_display() elif field == 'country': try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = '' <IF_STMT> value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values",elif field == 'salutation':
"def show_panel(panel_id): for position in _positions_names: pos_panel_ids = _get_position_panels(position) <IF_STMT> continue if len(pos_panel_ids) == 1: continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) if notebook_page == panel_widget: notebook.set_current_page(i)",if len(pos_panel_ids) == 0:
"def draw(self): program = self._program collection = self._collection mode = collection._mode if collection._need_update: collection._update() <IF_STMT> program['uniforms'] = collection._uniforms_texture program['uniforms_shape'] = collection._ushape if collection._indices_list is not None: program.draw(mode, collection._indices_buffer) else: program.draw(mode)",if collection._uniforms_list is not None:
"def release(provider, connection, cache=None): if cache is not None: db_session = cache.db_session if db_session is not None and db_session.ddl and cache.saved_fk_state: try: cursor = connection.cursor() sql = 'SET foreign_key_checks = 1' <IF_STMT> log_orm(sql) cursor.execute(sql) except: provider.pool.drop(connection) raise DBAPIProvider.release(provider, connection, cache)",if core.local.debug:
"def expanded_output(self): """"""Iterate over output files while dynamic output is expanded."""""" for f, f_ in zip(self.output, self.rule.output): <IF_STMT> expansion = self.expand_dynamic(f_) if not expansion: yield f_ for f, _ in expansion: file_to_yield = IOFile(f, self.rule) file_to_yield.clone_flags(f_) yield file_to_yield else: yield f",if f in self.dynamic_output:
"def __new__(cls, xs: Tuple[Optional[AbstractValue], core.Value]): pv, const = xs if not core.skip_checks: assert isinstance(pv, (AbstractValue, type(None))), xs assert isinstance(const, core.Tracer) or type(const) is Zero or core.valid_jaxtype(const), xs <IF_STMT> assert get_aval(const) == core.abstract_unit, xs return tuple.__new__(cls, xs)","if isinstance(pv, AbstractValue):"
"def MenuItemSearch(menu, item): for menuItem in list(menu.GetMenuItems()): label = menuItem.GetItemLabel() if not label: continue shortcutItem = Shortcut(menuItem=menuItem) shortcutItem.FromMenuItem() item.AppendItem(shortcutItem) subMenu = menuItem.GetSubMenu() <IF_STMT> MenuItemSearch(subMenu, shortcutItem)",if subMenu:
"def fill_potential_satellites_by_type(self, sat_type): setattr(self, 'potential_%s' % sat_type, []) for satellite in getattr(self, sat_type): getattr(self, 'potential_%s' % sat_type).append(satellite) for realm in self.higher_realms: for satellite in getattr(realm, sat_type): <IF_STMT> getattr(self, 'potential_%s' % sat_type).append(satellite)",if satellite.manage_sub_realms:
"def _gen(): while True: try: loop_val = it.next() except StopIteration: break self.mem.SetValue(lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly) if comp.cond: b = self.EvalExpr(comp.cond) else: b = True <IF_STMT> item = self.EvalExpr(node.elt) yield item",if b:
"def _iter_backtick_string(gen, line, back_start): for _, tokval, start, _, _ in gen: <IF_STMT> return BACKTICK_TAG + binascii.b2a_hex(line[back_start[1] + 1:start[1]].encode()).decode() else: raise SyntaxError(f'backtick quote at {back_start} does not match')",if tokval == '`':
"def to_internal_value(self, data): site = get_current_site() pages_root = reverse('pages-root') ret = [] for path in data: if path.startswith(pages_root): path = path[len(pages_root):] if path.endswith('/'): path = path[:-1] page = get_page_from_path(site, path) <IF_STMT> ret.append(page) return ret",if page:
def refresh(self): if self._exp_key is None: self._trials = [tt for tt in self._dynamic_trials if tt['state'] in JOB_VALID_STATES] else: self._trials = [tt for tt in self._dynamic_trials <IF_STMT>] self._ids.update([tt['tid'] for tt in self._trials]),if tt['state'] in JOB_VALID_STATES and tt['exp_key'] == self._exp_key
"def create_model(self, model): for field in model._meta.local_fields: if field.get_internal_type() == 'PositiveAutoField': autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column) <IF_STMT> self.deferred_sql.extend(autoinc_sql) super().create_model(model)",if autoinc_sql:
"def row_match(base_row, row): for op, val in list(row.items()): <IF_STMT> if base_row[op] != val: return False else: ildutil.ild_err(""BASE ROW %s doesn't have OD %s from row %s"" % (base_row, op, row)) return None return True",if op in base_row:
"def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None if isinstance(o, dict): name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, '__dict__', None) is o: o = r break elif isinstance(o, dict): name = web.dictfind(o, self.obj) <IF_STMT> name = None d.append(Object(o, name)) return d","if not isinstance(name, six.string_types):"
"def _run(env, remote): if device == 'vta': target = env.target <IF_STMT> assert tvm.runtime.enabled('rpc') program_fpga(remote, bitstream=None) reconfig_runtime(remote) elif device == 'arm_cpu': target = env.target_vta_cpu with autotvm.tophub.context(target): for _, wl in resnet_wkls: print(wl) run_conv2d(env, remote, wl, target)","if env.TARGET not in ['sim', 'tsim']:"
"def retrieve(self, aclass): """"""Look for a specifc class/name in the packet"""""" resu = [] for x in self.payload: try: if isinstance(aclass, str): <IF_STMT> resu.append(x) elif isinstance(x, aclass): resu.append(x) resu += x.retrieve(aclass) except: pass return resu",if x.name == aclass:
"def summary_passes(self): if self.config.option.tbstyle != 'no': if self.hasopt('P'): reports = self.getreports('passed') <IF_STMT> return self.write_sep('=', 'PASSES') for rep in reports: msg = self._getfailureheadline(rep) self.write_sep('_', msg) self._outrep_summary(rep)",if not reports:
"def fn(): random_states = {name: cls.random_state_function(state_spec=state_spec)() for name, state_spec in states_spec.items()} for name, action_spec in actions_spec.items(): <IF_STMT> mask = cls.random_mask(action_spec=action_spec) random_states[name + '_mask'] = mask return random_states",if action_spec['type'] == 'int':
"def _show_option(name=None): if name is None: name = '' filename = peda.getfile() if filename: filename = os.path.basename(filename) else: filename = None for k, v in sorted(config.Option.show(name).items()): <IF_STMT> v = v.replace('#FILENAME#', filename) msg('%s = %s' % (k, repr(v))) return","if filename and isinstance(v, str) and ('#FILENAME#' in v):"
"def _set_posonly_args_def(self, argmts, vals): for v in vals: argmts.posonlyargs.append(v['arg']) d = v['default'] <IF_STMT> argmts.defaults.append(d) elif argmts.defaults: self._set_error('non-default argument follows default argument')",if d is not None:
def get(self): with self._lock: <IF_STMT> self._connection = psycopg2.connect(**self._conn_kwargs) self._connection.autocommit = True self.server_version = self._connection.server_version return self._connection,if not self._connection or self._connection.closed != 0:
"def _Determine_Do(self): if sys.platform == 'darwin': self.applicable = True for opt, optarg in self.chosenOptions: <IF_STMT> self.value = os.path.abspath(optarg) break else: self.applicable = False self.determined = True",if opt == '--' + self.longopt:
"def delete_tags(filenames, v1, v2): for filename in filenames: with _sig.block(): <IF_STMT> print_(u'deleting ID3 tag info in', filename, file=sys.stderr) mutagen.id3.delete(filename, v1, v2)",if verbose:
"def startJail(self, name): with self.__lock: jail = self.__jails[name] if not jail.isAlive(): jail.start() <IF_STMT> logSys.info('Jail %r reloaded', name) del self.__reload_state[name] if jail.idle: jail.idle = False",elif name in self.__reload_state:
"def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith('#'): continue <IF_STMT> if current_repo: repos.append(current_repo) current_repo = {} current_repo['name'] = line[1:-1] if current_repo and '=' in line: key, value = line.split('=', 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos",if line.startswith('['):
"def add_to_auto_transitions(cls, base): result = {} for name, method in base.__dict__.items(): <IF_STMT> for name, transition in method._django_fsm.transitions.items(): if transition.custom.get('auto'): result.update({name: method}) return result","if callable(method) and hasattr(method, '_django_fsm'):"
"def commit(cache): assert cache.is_alive try: <IF_STMT> cache.flush() if cache.in_transaction: assert cache.connection is not None cache.database.provider.commit(cache.connection, cache) cache.for_update.clear() cache.query_results.clear() cache.max_id_cache.clear() cache.immediate = True except: cache.rollback() raise",if cache.modified:
"def block_items(objekt, block, eldict): if objekt not in block: if isinstance(objekt.type, PyType): <IF_STMT> block.append(objekt.type) block.append(objekt) if isinstance(objekt, PyType): others = [p for p in eldict.values() if isinstance(p, PyElement) and p.type[1] == objekt.name] for item in others: if item not in block: block.append(item) return block",if objekt.type not in block:
"def __getattr__(self, item): import pyarrow.lib ret = getattr(plasma, item, None) if ret is None: <IF_STMT> ret = getattr(plasma, 'PlasmaObjectNonexistent', None) or getattr(pyarrow.lib, 'PlasmaObjectNonexistent') elif item == 'PlasmaStoreFull': ret = getattr(pyarrow.lib, item) if ret is not None: setattr(self, item, ret) return ret",if item == 'PlasmaObjectNotFound':
"def clean_str(*args): tdict = {'str': 0, 'bytearray': 1, 'unicode': 2} for obj in args: k = tdict.get(type(obj).__name__) <IF_STMT> raise RuntimeError('Can not clean object: %s' % obj) clean_obj(obj, k)",if k is None:
"def incoming(): while True: m = ws.receive() <IF_STMT> m = str(m) print((m, len(m))) if len(m) == 35: ws.close() break else: break print(('Connection closed!',))",if m is not None:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.add_set_status(d.getVarInt32()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def __init__(self, text, menu): self.text = text self.menu = menu print(text) for i, option in enumerate(menu): menunum = i + 1 match = re.search('0D', option) <IF_STMT> if menunum < 10: print('   %s) %s' % (menunum, option)) else: print('  %s) %s' % (menunum, option)) else: print('\n  99) Return to Main Menu\n') return",if not match:
def take_step(self): with self.walk_lock: peers = self.overlay.get_peers() <IF_STMT> peer = choice(peers) self.overlay.send_random_to(peer),if peers:
"def clear_highlight(self): for doc in self._window.get_documents(): start, end = doc.get_bounds() <IF_STMT> tag = doc.create_tag('result_highlight', foreground='yellow', background='red') doc.remove_tag_by_name('result_highlight', start, end)",if doc.get_tag_table().lookup('result_highlight') == None:
"def impl(self, to_strip=None): mask = get_nan_mask(self._data._data) item_count = len(self._data) res_list = [''] * item_count for it in range(item_count): item = self._data._data[it] <IF_STMT> res_list[it] = usecase(item, to_strip) else: res_list[it] = item str_arr = create_str_arr_from_list(res_list) result = str_arr_set_na_by_mask(str_arr, mask) return pandas.Series(result, self._data._index, name=self._data._name)",if len(item) > 0:
"def modify_subnet_attribute(self): subnet_id = self._get_param('SubnetId') for attribute in ('MapPublicIpOnLaunch', 'AssignIpv6AddressOnCreation'): <IF_STMT> attr_name = camelcase_to_underscores(attribute) attr_value = self.querystring.get('%s.Value' % attribute)[0] self.ec2_backend.modify_subnet_attribute(subnet_id, attr_name, attr_value) return MODIFY_SUBNET_ATTRIBUTE_RESPONSE",if self.querystring.get('%s.Value' % attribute):
"def join(s, *p): path = s for t in p: if not s or isabs(t): path = t continue if t[:1] == ':': t = t[1:] if ':' not in path: path = ':' + path <IF_STMT> path = path + ':' path = path + t return path",if path[-1:] != ':':
"def publish(self): if not self.modules.has_option(self.subscriber_name, 'publish'): return False dest = self.modules.get(self.subscriber_name, 'publish') for name in dest.split(','): self.pubsub.setup_publish(name) while True: message = self.r_temp.spop(self.subscriber_name + 'out') <IF_STMT> time.sleep(1) continue self.pubsub.publish(message)",if message is None:
"def ignore(self, other): if isinstance(other, Suppress): <IF_STMT> super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if other not in self.ignoreExprs:
def recurse(node): for child in node.childNodes: if child.nodeType != child.ELEMENT_NODE: continue if child.nodeName.upper() == 'H1': return child <IF_STMT> return recurse(child),if child not in visited:
"def req(s, poll, msg, expect): do_req = True xid = None while True: if do_req: xid = s.put(msg)['xid'] events = poll.poll(2) for fd, event in events: response = s.get() <IF_STMT> do_req = False continue if response['options']['message_type'] != expect: raise Exception('DHCP protocol error') return response do_req = True",if response['xid'] != xid:
"def close(self, invalidate=False): self.session.transaction = self._parent if self._parent is None: for connection, transaction, autoclose in set(self._connections.values()): if invalidate: connection.invalidate() <IF_STMT> connection.close() else: transaction.close() self._state = CLOSED self.session.dispatch.after_transaction_end(self.session, self) if self._parent is None: if not self.session.autocommit: self.session.begin() self.session = None self._connections = None",if autoclose:
"def visit_loop(self): v = self.vS.top_front() i = self.iS.top_front() num_edges = len(self.graph[v].edges) while i <= num_edges: if i > 0: self.finish_edge(v, i - 1) <IF_STMT> return i += 1 self.finish_visiting(v)","if i < num_edges and self.begin_edge(v, i):"
"def get_objects(self): list_type, id, handles, timestamp = self._obj_list retval = [] for target, handle in handles: _class = map2class(target) <IF_STMT> obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp))) if obj: retval.append(obj) return retval",if _class:
"def __init__(self, config_lists): self.lens = len(config_lists) self.spaces = [] for config_list in config_lists: if isinstance(config_list, tuple): key, config = config_list <IF_STMT> key = config_list config = None else: raise NotImplementedError('the type of config is Error!!! Please check the config information. Receive the type of config is {}'.format(type(config_list))) self.spaces.append(self._get_single_search_space(key, config)) self.init_tokens()","elif isinstance(config_list, str):"
"def fieldset_string_to_field(fieldset_dict, model): if isinstance(fieldset_dict['fields'], tuple): fieldset_dict['fields'] = list(fieldset_dict['fields']) i = 0 for dict_field in fieldset_dict['fields']: <IF_STMT> fieldset_dict['fields'][i] = model._meta.get_field_by_name(dict_field)[0] elif isinstance(dict_field, list) or isinstance(dict_field, tuple): dict_field[1]['recursive'] = True fieldset_string_to_field(dict_field[1], model) i += 1","if isinstance(dict_field, string_types):"
"def _get_directories(config): for directory in config['dump_directories']: for dname in sorted(glob.glob(os.path.join(directory, '*[Aa]*[Xx][XxYy23]'))): <IF_STMT> yield dname",if os.path.isdir(dname):
"def process_event(self, event): super().process_event(event) if event.type == pygame.USEREVENT: <IF_STMT> self.input_op(event.ui_object_id[-1]) return True",if event.user_type == pygame_gui.UI_BUTTON_PRESSED:
"def _restore_std_streams(self): stdout = sys.stdout.getvalue() stderr = sys.stderr.getvalue() close = [sys.stdout, sys.stderr] sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ for stream in close: stream.close() if stdout and stderr: if not stderr.startswith(('*TRACE*', '*DEBUG*', '*INFO*', '*HTML*', '*WARN*')): stderr = '*INFO* %s' % stderr <IF_STMT> stdout += '\n' return self._handle_binary_result(stdout + stderr)",if not stdout.endswith('\n'):
"def _get_attachments(self): if self._attachments is None: alist = [] for a in self._message.get_attachments(): alist.append((AttachmentWidget(a), None)) <IF_STMT> self._attachments = SimpleTree(alist) return self._attachments",if alist:
"def __getattr__(self, name): assert skip_checks or name != 'aval' try: attr = getattr(self.aval, name) except KeyError as err: raise AttributeError('{} has no attribute {}'.format(self.__class__.__name__, name)) from err else: t = type(attr) if t is aval_property: return attr.fget(self) <IF_STMT> return types.MethodType(attr.fun, self) else: return attr",elif t is aval_method:
"def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) <IF_STMT> break if pos > 0 and dn[pos - 1] != '\\': break elif pos > 1 and dn[pos - 1] == '\\': escaped = True for c in dn[pos - 2:0:-1]: if c == '\\': escaped = not escaped else: break if not escaped: break pos += 1 return pos",if pos == -1:
"def test_synopsis(self): self.addCleanup(unlink, TESTFN) for encoding in ('ISO-8859-1', 'UTF-8'): with open(TESTFN, 'w', encoding=encoding) as script: <IF_STMT> print('#coding: {}'.format(encoding), file=script) print('""""""line 1: hé', file=script) print('line 2: hi""""""', file=script) synopsis = pydoc.synopsis(TESTFN, {}) self.assertEqual(synopsis, 'line 1: hé')",if encoding != 'UTF-8':
"def qualify(x): parts = x.split(';', 1) if len(parts) == 2: match = re.match('(^|;)q=(0(\\.\\d{,3})?|1(\\.0{,3})?)(;|$)', parts[1]) <IF_STMT> return (parts[0].strip(), float(match.group(2))) return (parts[0].strip(), 1)",if match:
"def getEndpoints(self): endpoints = self.endpoints[:] for i in range(len(endpoints)): ep = endpoints[i] <IF_STMT> raise TypeError('Not an Endpoint subclass') endpoints[i] = ep(self, self.master) return endpoints","if not issubclass(ep, Endpoint):"
"def __getitem__(self, index): if cfg.RPN.ENABLED: return self.get_rpn_sample(index) elif cfg.RCNN.ENABLED: <IF_STMT> if cfg.RCNN.ROI_SAMPLE_JIT: return self.get_rcnn_sample_jit(index) else: return self.get_rcnn_training_sample_batch(index) else: return self.get_proposal_from_file(index) else: raise NotImplementedError",if self.mode == 'TRAIN':
"def test_data_path(self, filename): repository_dir = self._repository_dir test_data = None if repository_dir: return self.__walk_test_data(dir=repository_dir, filename=filename) elif self.tool_dir: tool_dir = self.tool_dir <IF_STMT> tool_dir = os.path.dirname(self.tool_dir) test_data = self.__walk_test_data(tool_dir, filename=filename) if not test_data: test_data = self.app.test_data_resolver.get_filename(filename) return test_data","if isinstance(self, DataManagerTool):"
"def generate_forwards(cls, attrs): for attr_name, attr in cls._forwards.__dict__.items(): if attr_name.startswith('_') or attr_name in attrs: continue if isinstance(attr, property): cls._forward.append(attr_name) <IF_STMT> wrapper = _forward_factory(cls, attr_name, attr) setattr(cls, attr_name, wrapper) else: raise TypeError(attr_name, type(attr))","elif isinstance(attr, types.FunctionType):"
def summary(result): if not self.options.metadata_to_dict: <IF_STMT> pprint(Fore.CYAN + result['title'] + Fore.RESET) pprint(Fore.CYAN + Style.DIM + result['written_at'] + Style.RESET_ALL + Fore.RESET) pprint(result['body']) writer.write('@title:' + result['title']) writer.write('@written_at:' + result['written_at']) writer.write('@body:' + result['body']) else: if self.options.verbose: pprint(result) writer.write(result),if self.options.verbose:
"def visit_StringConstant(self, node: qlast.StringConstant) -> None: if not _NON_PRINTABLE_RE.search(node.value): for d in (""'"", '""', '$$'): <IF_STMT> if '\\' in node.value and d != '$$': self.write('r', d, node.value, d) else: self.write(d, node.value, d) return self.write(edgeql_quote.dollar_quote_literal(node.value)) return self.write(repr(node.value))",if d not in node.value:
"def get_sql_date_trunc(col, db='default', grouper='hour'): conn = connections[db] engine = get_db_engine(db) if engine.startswith('oracle'): method = DATE_TRUNC_GROUPERS['oracle'].get(grouper, DATE_TRUNC_GROUPERS['default'][grouper]) <IF_STMT> col = '""%s""' % col.upper() else: method = DATE_TRUNC_GROUPERS['default'][grouper] return conn.ops.date_trunc_sql(method, col)","if '""' not in col:"
"def req(s, poll, msg, expect): do_req = True xid = None while True: if do_req: xid = s.put(msg)['xid'] events = poll.poll(2) for fd, event in events: response = s.get() if response['xid'] != xid: do_req = False continue <IF_STMT> raise Exception('DHCP protocol error') return response do_req = True",if response['options']['message_type'] != expect:
"def __init__(self, f): self._refs = {} self._peeled = {} for line in f.readlines(): sha, name = line.rstrip(b'\n').split(b'\t') <IF_STMT> name = name[:-3] if not check_ref_format(name): raise ValueError('invalid ref name %r' % name) self._peeled[name] = sha else: if not check_ref_format(name): raise ValueError('invalid ref name %r' % name) self._refs[name] = sha",if name.endswith(ANNOTATED_TAG_SUFFIX):
"def get_defines(clang_output): import re defines = [] for line in output.splitlines(): m = re.search('#define ([\\w()]+) (.+)', line) <IF_STMT> defines.append('-D{}={}'.format(m.group(1), m.group(2))) else: m = re.search('#define (\\w+)', line) if m is not None: defines.append('-D{}'.format(m.group(1))) _log.debug('Got defines: %s', defines) return defines",if m is not None:
"def clean_rcs_keywords(paragraph, keyword_substitutions): if len(paragraph) == 1 and isinstance(paragraph[0], nodes.Text): textnode = paragraph[0] for pattern, substitution in keyword_substitutions: match = pattern.search(textnode.data) <IF_STMT> textnode.data = pattern.sub(substitution, textnode.data) return",if match:
"def reorder_incremental_state(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order): """"""Reorder buffered internal state (for incremental generation)."""""" input_buffer = self._get_input_buffer(incremental_state) if input_buffer is not None: for k in input_buffer.keys(): <IF_STMT> input_buffer[k] = input_buffer[k].index_select(0, new_order) incremental_state = self._set_input_buffer(incremental_state, input_buffer) return incremental_state",if input_buffer[k] is not None:
"def render(cls) -> str: buf = render_utils.RenderBuffer() buf.write(f'struct {cls.__name__} {{') with buf.indent(): for fieldname, field in cls._fields.items(): <IF_STMT> buf.write_comment(field.doc) field.render_field(fieldname, buf) buf.newline() if buf.lastline() == '': buf.popline() buf.write('};') return str(buf)",if field.doc:
"def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') <IF_STMT> fragment = '<i>%s</i>' % fragment if sty.underline: fragment = '<u>%s</u>' % fragment if sty.strikeout: fragment = '<s>%s</s>' % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub('\n+', '\n', ''.join(body).strip())",if sty.italic:
"def _show_warnings(self): if self._warnings_handled: return self._warnings_handled = True if self._result and (self._result.has_next or not self._result.warning_count): return ws = self._get_db().show_warnings() if ws is None: return for w in ws: msg = w[-1] if PY2: <IF_STMT> msg = msg.encode('utf-8', 'replace') warnings.warn(err.Warning(*w[1:3]), stacklevel=4)","if isinstance(msg, unicode):"
"def scrub_time(self, time): debug('scrub_time: {0}'.format(time)) if time == 0: self.loop_backward() elif time == self.timer_duration: self.loop_forward() elif self.timer_status == TIMER_STATUS_STOPPED: self.timer_status = TIMER_STATUS_PAUSED <IF_STMT> self.timer_status = TIMER_STATUS_PAUSED self.timer_time = time",elif self.timer_status == TIMER_STATUS_EXPIRED:
"def _default_import_run(run, dest, move, copy_resources): if move: log.info('Moving %s', run.id) <IF_STMT> shutil.copytree(run.path, dest) util.safe_rmtree(run.path) else: shutil.move(run.path, dest) else: log.info('Copying %s', run.id) shutil.copytree(run.path, dest, symlinks=not copy_resources)",if copy_resources:
def fn(n): while n < 3: <IF_STMT> yield 'less than zero' elif n == 0: yield 'zero' elif n == 1: yield 'one' else: yield 'more than one' n += 1,if n < 0:
"def _check_dep_names(self): """"""check if user input task_dep or setup_task that doesnt exist"""""" for task in self.tasks.values(): for dep in task.task_dep: <IF_STMT> msg = ""%s. Task dependency '%s' does not exist."" raise InvalidTask(msg % (task.name, dep)) for setup_task in task.setup_tasks: if setup_task not in self.tasks: msg = ""Task '%s': invalid setup task '%s'."" raise InvalidTask(msg % (task.name, setup_task))",if dep not in self.tasks:
"def urls(): for scheme in (b'http', b'https'): for host in (b'example.com',): for port in (None, 100): for path in (b'', b'path'): <IF_STMT> host = host + b':' + networkString(str(port)) yield urlunsplit((scheme, host, path, b'', b''))",if port is not None:
"def split_hashes(cls, line): if '--hash' not in line: return (line, []) split_line = line.split() line_parts = [] hashes = [] for part in split_line: <IF_STMT> param, _, value = part.partition('=') hashes.append(value) else: line_parts.append(part) line = ' '.join(line_parts) return (line, hashes)",if part.startswith('--hash'):
"def part(p, imaginary): s = 'j' if imaginary else '' try: if math.isinf(p): <IF_STMT> return '-1e1000' + s return '1e1000' + s if math.isnan(p): return '(1e1000%s-1e1000%s)' % (s, s) except OverflowError: pass return repr(p) + s",if p < 0:
"def _build_display_args(self, r): args = [] if self.RESULT: <IF_STMT> result = [self.RESULT] else: result = self.RESULT for name in result: value = getattr(r, name) if name == 'offset': value += self.config.base args.append(value) return args",if type(self.RESULT) != type([]):
"def cell_data_statusicon(column, cell, model, row, data): """"""Display text with an icon"""""" try: state = model.get_value(row, data) <IF_STMT> return func_last_value['cell_data_statusicon'] = state icon = ICON_STATE[state] original_filters = warnings.filters[:] warnings.simplefilter('ignore') try: cell.set_property('pixbuf', icon) finally: warnings.filters = original_filters except KeyError: pass",if func_last_value['cell_data_statusicon'] == state:
"def _para_exploit(self, params, part): if len(params) == 0: arr = ['*', 'config'] + self._configs.keys() return suggest(arr, part) if len(params) == 1: arr = [] <IF_STMT> arr = self._configs.keys() if params[0] == '*': arr = ['stopOnFirst'] return suggest(arr, part) return []",if params[0] == 'config':
"def send(self, data, flags=0, timeout=timeout_default): if timeout is timeout_default: timeout = self.timeout try: return self._sock.send(data, flags) except error as ex: if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0: raise sys.exc_clear() self._wait(self._write_event) try: return self._sock.send(data, flags) except error as ex2: <IF_STMT> return 0 raise",if ex2.args[0] == EWOULDBLOCK:
"def server_decode(self, buf): if self.has_recv_header: return (buf, True, False) self.has_recv_header = True crc = binascii.crc32(buf) & 4294967295 if crc != 4294967295: self.has_sent_header = True <IF_STMT> return (b'E' * 2048, False, False) return (buf, True, False) return (b'', False, True)",if self.method == 'random_head':
"def Decode(self, filedesc): while True: chunk = filedesc.Read(4) if not chunk: return <IF_STMT> yield b'NORF' if chunk == b'THUD': yield b'BLARGH'",if chunk == b'QUUX':
"def decProcess(): while 1: yield (clock.posedge, reset.negedge) if reset == ACTIVE_LOW: count.next = 0 elif enable: <IF_STMT> count.next = n - 1 else: count.next = count - 1",if count == -n:
"def set_torrent_path(self, torrent_id, path): try: <IF_STMT> return False self.client.core.set_torrent_move_completed_path(torrent_id, path).get() self.client.core.set_torrent_move_completed(torrent_id, 1).get() except Exception: return False finally: if self.client: self.disconnect() return True",if not self.connect():
"def stale_rec(node, nodes): if node.abspath() in node.ctx.env[Build.CFG_FILES]: return if getattr(node, 'children', []): for x in node.children.values(): <IF_STMT> stale_rec(x, nodes) else: for ext in DYNAMIC_EXT: if node.name.endswith(ext): break else: if not node in nodes: if can_delete(node): Logs.warn('Removing stale file -> %r', node) node.delete()",if x.name != 'c4che':
"def iterate(self, prod_, rule_): newProduction = '' for i in range(len(prod_)): step = self.production[i] if step == 'W': newProduction = newProduction + self.ruleW <IF_STMT> newProduction = newProduction + self.ruleX elif step == 'Y': newProduction = newProduction + self.ruleY elif step == 'Z': newProduction = newProduction + self.ruleZ elif step != 'F': newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction",elif step == 'X':
"def _get_app_params(self): params = self.cfg.params.copy() for key, value in self.__dict__.items(): if key.startswith('_'): continue <IF_STMT> params['parse_console'] = not value else: params[key] = value params['load_config'] = False return params",elif key == 'console_parsed':
"def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False if isinstance(value, dict): value = PseudoNamespace(value) <IF_STMT> value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and (not isinstance(item, PseudoNamespace)): value[i] = PseudoNamespace(item) if tuple_converted: value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)","elif isinstance(value, tuple):"
"def getNextSibling(self, node): if isinstance(node, tuple): node, key = node assert key in ('text', 'tail'), 'Text nodes are text or tail, found %s' % key if key == 'text': <IF_STMT> return node[0] else: return None else: return node.getnext() return (node, 'tail') if node.tail else node.getnext()",if len(node):
"def star_path(path): """"""Replace integers and integer-strings in a path with *"""""" path = list(path) for i, p in enumerate(path): if isinstance(p, int): path[i] = '*' else: <IF_STMT> p = p.decode() if r_is_int.match(p): path[i] = '*' return join_path(path)","if not isinstance(p, text_type):"
"def ensure_popup_selection(self): try: self.__position_at_mouse except AttributeError: path, col = self.get_cursor() <IF_STMT> return False self.scroll_to_cell(path, col) selection = self.get_selection() if not selection.path_is_selected(path): selection.unselect_all() selection.select_path(path) return True",if path is None:
"def release(self): me, lock_count = self.__begin() try: <IF_STMT> return self._count = count = self._count - 1 if not count: self._owner = None self._block.release() finally: self.__end(me, lock_count)",if me is None:
"def date_match(self, date1, date2): if date1.is_empty() or date2.is_empty(): return 0 if date1.is_equal(date2): return 1 if date1.is_compound() or date2.is_compound(): return self.range_compare(date1, date2) if date1.get_year() == date2.get_year(): if date1.get_month() == date2.get_month(): return 0.75 <IF_STMT> return 0.75 else: return -1 else: return -1",if not date1.get_month_valid() or not date2.get_month_valid():
"def onMinimize(self, sender): if self._runDialogListener('onMinimize') is False: return widget = self.child if widget is not None: <IF_STMT> widget.setVisible(False) self.setHeight('') self.setWidth('') if self._maximized: self._minimized = self._maximized self._toggleMaximize() else: self._minimized = None else: if self._minimized is not None: self._toggleMaximize() widget.setVisible(True)",if widget.isVisible():
"def instance_reader(): for epoch_index in range(epoch): <IF_STMT> if shuffle_seed is not None: np.random.seed(shuffle_seed) np.random.shuffle(examples) if phase == 'train': self.current_train_epoch = epoch_index for index, example in enumerate(examples): if phase == 'train': self.current_train_example = index + 1 feature = self.convert_example(index, example, self.get_labels(), self.max_seq_len, self.tokenizer) instance = self.generate_instance(feature) yield instance",if shuffle:
"def _parse_lines(self, linesource): """"""Parse lines of text for functions and classes"""""" functions = [] classes = [] for line in linesource: if line.startswith('def ') and line.count('('): name = self._get_object_name(line) if not name.startswith('_'): functions.append(name) <IF_STMT> name = self._get_object_name(line) if not name.startswith('_'): classes.append(name) else: pass functions.sort() classes.sort() return (functions, classes)",elif line.startswith('class '):
"def get_folder_version(folder): f = os.path.join(code_path, folder, 'version.txt') try: with open(f) as fd: content = fd.read() p = re.compile('([0-9]+)\\.([0-9]+)\\.([0-9]+)') m = p.match(content) <IF_STMT> version = m.group(1) + '.' + m.group(2) + '.' + m.group(3) return version except: return False",if m:
"def __init__(self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None): if builtin and isinstance(builtin, (str, unicode)): builtin = os.path.basename(builtin) for ignore in ('.py', '.pyo', '.pyc'): if builtin.endswith(ignore): builtin = builtin[:-len(ignore)] <IF_STMT> self.LOADED.append(builtin) self.loading_plugin = plugin_name self.loading_builtin = plugin_name and builtin self.builtin = builtin self.deprecated = deprecated self.session = session self.config = config self.manifests = []",if builtin not in self.LOADED:
"def setInt(self, path, value, **kwargs): if value is None: self.set(path, None, **kwargs) return minimum = kwargs.pop('min', None) maximum = kwargs.pop('max', None) try: intValue = int(value) <IF_STMT> intValue = minimum if maximum is not None and intValue > maximum: intValue = maximum except ValueError: self._logger.warning('Could not convert %r to a valid integer when setting option %r' % (value, path)) return self.set(path, intValue, **kwargs)",if minimum is not None and intValue < minimum:
"def __call__(self, session_path): """"""Get raw session object from `session_path`."""""" new_session = copy.deepcopy(self._template) session_keys = new_session.keys() old_session = self._load_file(session_path) for attribute in dir(self): <IF_STMT> target = attribute[4:].capitalize() if target not in session_keys: raise ValueError('Invalid attribute: %r' % attribute) function = getattr(self, attribute) new_session[target] = function(old_session) return new_session",if attribute.startswith('set_'):
"def add_comment_to_directory(args, dir_path): for root, _, files in os.walk(dir_path): for file_name in files: <IF_STMT> continue file_path = os.path.join(root, file_name) add_comment_to_file(args, file_path)","if not re.match('.*(\\.c|\\.h|\\.cpp|\\.hpp|\\.cxx|\\.hxx)$', file_name):"
"def reportMemory(k, options, field=None, isBytes=False): """"""Given k kilobytes, report back the correct format as string."""""" if options.pretty: return prettyMemory(int(k), field=field, isBytes=isBytes) else: <IF_STMT> k /= 1024.0 if field is not None: return '%*dK' % (field - 1, k) else: return '%dK' % int(k)",if isBytes:
"def resolve(self, arguments): positional = [] named = {} for arg in arguments: if self._is_named(arg): self._add_named(arg, named) <IF_STMT> self._raise_positional_after_named() else: positional.append(arg) return (positional, named)",elif named:
"def _load_from_cache(self): if self._cache_key in self._cache: creds = deepcopy(self._cache[self._cache_key]) <IF_STMT> return creds else: logger.debug('Credentials were found in cache, but they are expired.') return None",if not self._is_expired(creds):
"def convertstore(self, inputstore, includefuzzy=False): """"""converts a file to .lang format"""""" thetargetfile = lang.LangStore(mark_active=self.mark_active) for pounit in inputstore.units: if pounit.isheader() or not pounit.istranslatable(): continue newunit = thetargetfile.addsourceunit(pounit.source) if includefuzzy or not pounit.isfuzzy(): newunit.settarget(pounit.target) else: newunit.settarget('') <IF_STMT> newunit.addnote(pounit.getnotes('developer'), 'developer') return thetargetfile",if pounit.getnotes('developer'):
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) for exclude_field in self.context['request'].query_params.getlist('exclude'): p = exclude_field.split('.') <IF_STMT> if len(p) == 1: del self.fields[p[0]] elif len(p) == 2: self.fields[p[0]].child.fields.pop(p[1])",if p[0] in self.fields:
"def __init__(self, fn, args, resources): self.fn = fn self.args = copy.deepcopy(args) self.resources = resources with Task.LOCK: self.task_id = Task.TASK_ID.value <IF_STMT> if isinstance(self.args['args'], (argparse.Namespace, argparse.ArgumentParser)): args_dict = vars(self.args['args']) else: args_dict = self.args['args'] args_dict.update({'task_id': self.task_id}) Task.TASK_ID.value += 1",if 'args' in self.args:
"def _expand_nsplit_by_reduce(splits, reduced): if reduced == 1: return splits out = [] for s in splits: x = s part = max(x / reduced, 1) while x >= 2 * part: out.append(int(part)) x -= int(part) <IF_STMT> out.append(x) assert sum(splits) == sum(out) return tuple(out)",if x:
"def OnDeleteLine(self, items): for n in items: <IF_STMT> name1 = self.items[n][2] name2 = self.items[n][4] del self.items[n] if name1 in self.bindiff.matched1: self.bindiff.matched1.remove(name1) if name2 in self.bindiff.matched2: self.bindiff.matched2.remove(name2) return [Choose.ALL_CHANGED] + items",if n >= 0:
"def _to_str(self, tokens: List[int]) -> str: pos = next((idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1) if pos != -1: tokens = tokens[:pos] vocab_map = self.vocab.id_to_token_map_py words = [vocab_map[t] for t in tokens] if self.encoding is not None and self.perform_decode: <IF_STMT> words = self.bpe_decode(words) elif self.encoding == 'spm': words = self.spm_decode(words) sentence = ' '.join(words) return sentence",if self.encoding == 'bpe':
"def detect(content, **kwargs): headers = kwargs.get('headers', {}) content = str(content) detection_schema = (re.compile('\\Abarra.counter.session(=)?', re.I), re.compile('(\\A|\\b)?barracuda.', re.I), re.compile('barracuda.networks(.)?.inc', re.I)) for detection in detection_schema: <IF_STMT> return True if detection.search(content) is not None: return True","if detection.search(headers.get(HTTP_HEADER.SET_COOKIE, '')) is not None:"
"def _finish_port_forward(self, listener, listen_host, listen_port): """"""Finish processing a TCP/IP port forwarding request"""""" if asyncio.iscoroutine(listener): try: listener = (yield from listener) except OSError: listener = None if listener: <IF_STMT> listen_port = listener.get_port() result = UInt32(listen_port) else: result = True self._local_listeners[listen_host, listen_port] = listener self._report_global_response(result) else: self.logger.debug1('Failed to create TCP listener') self._report_global_response(False)",if listen_port == 0:
"def start(self): """"""Start running the mainloop."""""" with self: result = pa.pa_threaded_mainloop_start(self._pa_threaded_mainloop) <IF_STMT> raise PulseAudioException(0, 'Failed to start PulseAudio mainloop') assert _debug('PulseAudioMainLoop: Started')",if result < 0:
def service(self): try: try: self.start() self.execute() self.finish() except socket.error: self.close_on_finish = True <IF_STMT> raise finally: pass,if self.channel.adj.log_socket_errors:
"def _makepath(self, path): if not self.abspath: try: np = py.path.local().bestrelpath(path) except OSError: return path <IF_STMT> path = np return path",if len(np) < len(str(path)):
"def upload(youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None): body_keys = ','.join(body.keys()) media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True) videos = youtube_resource.videos() request = videos.insert(part=body_keys, body=body, media_body=media) while 1: status, response = request.next_chunk() <IF_STMT> if 'id' in response: return response['id'] else: raise KeyError(""Response has no 'id' field"") elif status and progress_callback: progress_callback(status.total_size, status.resumable_progress)",if response:
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) for exclude_field in self.context['request'].query_params.getlist('exclude'): p = exclude_field.split('.') if p[0] in self.fields: <IF_STMT> del self.fields[p[0]] elif len(p) == 2: self.fields[p[0]].child.fields.pop(p[1])",if len(p) == 1:
"def on_button_press_event(self, iconview, event): if event.button == 3: popup_menu = Gtk.Menu() x = int(event.x) y = int(event.y) time = event.time pathinfo = iconview.get_path_at_pos(x, y) <IF_STMT> iconview.grab_focus() self.do_populate_popup(popup_menu, pathinfo) gtk_popup_at_pointer(popup_menu, event) return True return False",if pathinfo is not None:
"def __rshift__(self, other): if not self.symbolic and type(other) is int: return RegisterOffset(self._bits, self.reg, self._to_signed(self.offset >> other)) el<IF_STMT> return RegisterOffset(self._bits, self.reg, self.offset >> other) else: return RegisterOffset(self._bits, self.reg, ArithmeticExpression(ArithmeticExpression.RShift, (self.offset, other)))",if self.symbolic:
"def _slice_positional_metadata(self, indexable): if self.has_positional_metadata(): <IF_STMT> index = _single_index_to_slice(indexable) else: index = indexable return self.positional_metadata.iloc[index] else: return None",if _is_single_index(indexable):
"def _show_env(name=None): if name is None: name = '' env = peda.execute_redirect('show env') for line in env.splitlines(): k, v = line.split('=', 1) <IF_STMT> msg('%s = %s' % (k, v if is_printable(v) else to_hexstr(v))) return",if k.startswith(name):
"def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == ';': return i elif c == ""'"" or c == '""': i = g.skip_string(s, i) <IF_STMT> i = g.skip_to_end_of_line(s, i) elif g.match(s, i, '/*'): i = g.skip_block_comment(s, i) else: i += 1 return i","elif g.match(s, i, '//'):"
"def filter_iterable(cls, iterable, filterset_class, filters_name, info, **args): filter_input = args.get(filters_name) if filter_input and filterset_class: instance = filterset_class(data=dict(filter_input), queryset=iterable, request=info.context) <IF_STMT> raise GraphQLError(json.dumps(instance.errors.get_json_data())) iterable = instance.qs return iterable",if not instance.is_valid():
"def build(opt): dpath = os.path.join(opt['datapath'], 'self_feeding') version = '3.1' if not build_data.built(dpath, version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version)",if build_data.built(dpath):
"def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type <IF_STMT> token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield (index, token, value)",elif self.c99highlighting and value in self.c99_types:
"def searchOpcode(self, opcode, name=None): to_return = {} if not name: for file in self.__files: to_return[file.loader.fileName] = self.__ropper.searchOpcode(file.loader, opcode) else: fc = self.getFileFor(name) <IF_STMT> raise RopperError('No such file opened: %s' % name) to_return[name] = self.__ropper.searchOpcode(fc.loader, opcode) return self.__filterBadBytes(to_return)",if not fc:
"def logic(): while 1: yield (clock.posedge, reset.negedge) <IF_STMT> count.next = 0 elif enable: if count == -n: count.next = n - 1 else: count.next = count - 1",if reset == ACTIVE_LOW:
"def upgrade_cursor(cursor): count = 0 prefix = pack_be_uint16(cursor) key_len = HASHX_LEN + 2 chunks = util.chunks with self.db.write_batch() as batch: batch_put = batch.put for key, hist in self.db.iterator(prefix=prefix): <IF_STMT> continue count += 1 hist = b''.join((item + b'\x00' for item in chunks(hist, 4))) batch_put(key, hist) self.upgrade_cursor = cursor self.write_state(batch) return count",if len(key) != key_len:
"def fork(receiver: Receiver, func, *args, **kwargs): current_actor = self() send(Fork(current_actor, func, args, kwargs), receiver) while True: message = recv(current_actor) <IF_STMT> return message.new_actor else: send(message, current_actor) return","if isinstance(message, ForkResponse):"
"def history_move(self, n): from ranger.container.history import HistoryEmptyException try: current = self.history.current() except HistoryEmptyException: pass else: <IF_STMT> self.history.modify(self.line) self.history.move(n) current = self.history.current() if self.line != current: self.line = self.history.current() self.pos = len(self.line)",if self.line != current and self.line != self.history.top():
"def fullname(self): if self._fullname is None: pkg_name = namespace.apply_namespace(self.dist.project_name) <IF_STMT> self._fullname = '%s/%s' % (pkg_name, self.name) else: self._fullname = self.name return self._fullname",if pkg_name and pkg_name != '.':
"def do_install(datafilename): ifile = open(datafilename, 'rb') d = pickle.load(ifile) destdir_var = 'DESTDIR' if destdir_var in os.environ: <IF_STMT> subdir = d.prefix[1:] else: subdir = d.prefix d.prefix = os.path.join(os.environ[destdir_var], subdir) install_targets(d) install_headers(d) install_man(d) install_data(d) install_po(d)",if d.prefix[0] == '/':
"def truncate(self, size=None): with self._lock: <IF_STMT> size = self.tell() with self.fs.openbin(self.path) as f: data = f.read(size) with self.fs.openbin(self.path, 'w') as f: f.write(data) if len(data) < size: f.write(b'\x00' * (size - len(data))) return size",if size is None:
"def write(self, expression, location=None): for line in expression.split('\n'): self._phrase.append(line) <IF_STMT> self._phrase_line_begins.append(location) location += len(line) + 1 self.write_command('input', 'allow-incomplete', self._phrase)",if location is not None:
"def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: if 'master' in node['flags']: cursor = '0' while cursor != 0: pieces = [cursor] <IF_STMT> pieces.extend(['MATCH', match]) if count is not None: pieces.extend(['COUNT', count]) response = await self.execute_command_on_nodes([node], 'SCAN', *pieces) cursor, data = list(response.values())[0] for item in data: yield item",if match is not None:
"def communicate(self, input_data=None): """"""Mock subprocess.Popen.communicate."""""" for i in range(2): timeout = execute_time if i == 0 else sigterm_handler_time try: received_signal = self.signal_queue.get(block=True, timeout=timeout) except queue.Empty: continue self.received_signals.append((received_signal, time.time() - self.start_time)) <IF_STMT> break return (output, None)",if received_signal == Signal.KILL:
"def _add_bookmark_breakpoint(self): """"""Add a bookmark or breakpoint to the current file in the editor."""""" editorWidget = self.ide.mainContainer.get_actual_editor() if editorWidget and editorWidget.hasFocus(): if self.ide.mainContainer.actualTab.navigator.operation == 1: editorWidget._sidebarWidget.set_bookmark(editorWidget.textCursor().blockNumber()) <IF_STMT> editorWidget._sidebarWidget.set_breakpoint(editorWidget.textCursor().blockNumber())",elif self.ide.mainContainer.actualTab.navigator.operation == 2:
"def _should_auto_select_container_version(instance_type, distribution): """"""Returns a boolean that indicates whether to use an auto-selected container version."""""" p4d = False if instance_type: match = re.match('^ml[\\._]([a-z\\d]+)\\.?\\w*$', instance_type) <IF_STMT> family = match[1] p4d = family == 'p4d' smdistributed = False if distribution: smdistributed = 'smdistributed' in distribution return p4d or smdistributed",if match:
def _flush_some_if_lockable(self): if self.outbuf_lock.acquire(False): try: self._flush_some() <IF_STMT> self.outbuf_lock.notify() finally: self.outbuf_lock.release(),if self.total_outbufs_len < self.adj.outbuf_high_watermark:
"def add_auth(self, req, **kwargs): if not 'x-amz-content-sha256' in req.headers: <IF_STMT> req.headers['x-amz-content-sha256'] = req.headers.pop('_sha256') else: req.headers['x-amz-content-sha256'] = self.payload(req) req = self.mangle_path_and_params(req) return super(S3HmacAuthV4Handler, self).add_auth(req, **kwargs)",if '_sha256' in req.headers:
"def get_objects(self): list_type, id, handles, timestamp = self._obj_list retval = [] for target, handle in handles: _class = map2class(target) if _class: obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp))) <IF_STMT> retval.append(obj) return retval",if obj:
def toggle_fullscreen_hide_tabbar(self): if self.is_fullscreen(): if self.settings.general.get_boolean('fullscreen-hide-tabbar'): <IF_STMT> self.guake.notebook_manager.set_notebooks_tabbar_visible(False) elif self.guake and self.guake.notebook_manager: v = self.settings.general.get_boolean('window-tabbar') self.guake.notebook_manager.set_notebooks_tabbar_visible(v),if self.guake and self.guake.notebook_manager:
"def __repr__(self): parts = [] if not approx_equal(self.constant, 0.0) or self.is_constant: parts.append(repr(self.constant)) for clv, coeff in sorted(self.terms.items(), key=lambda x: repr(x)): <IF_STMT> parts.append(repr(clv)) else: parts.append(repr(coeff) + '*' + repr(clv)) return ' + '.join(parts)","if approx_equal(coeff, 1.0):"
"def wrapper(*args, **kwds): global bootstrap_logger_enabled if bootstrap_logger_enabled: <IF_STMT> bootstrap_logger.exception(msg=args[0]) else: bootstrap_logger.log(level=level, msg=args[0]) return f(*args, **kwds)",if level == 'EXCEPTION':
"def get_sorted_entry(field, bookid): if field == 'title' or field == 'authors': book = calibre_db.get_filtered_book(bookid) if book: if field == 'title': return json.dumps({'sort': book.sort}) <IF_STMT> return json.dumps({'author_sort': book.author_sort}) return ''",elif field == 'authors':
"def movies_iterator(): for row in self._tuple_iterator(query): id, guid, movie = self._parse(fields, row, offset=2) if parse_guid: <IF_STMT> guids[id] = Guid.parse(guid) guid = guids[id] yield (id, guid, movie)",if id not in guids:
"def update_sockets(self, context): bools = [self.min_list, self.max_list, self.size_list] dims = int(self.dimensions[0]) for i in range(3): for j in range(3): out_index = 4 + j + 3 * i hidden = self.outputs[out_index].hide_safe if bools[i][j] and j < dims: <IF_STMT> self.outputs[out_index].hide_safe = False else: self.outputs[out_index].hide_safe = True updateNode(self, context)",if hidden:
"def broadcast(self, msg, eid): for s in self.subs: if type(self.subs[s].eid) is list: <IF_STMT> self.subs[s].write_message(msg) elif self.subs[s].eid == eid: self.subs[s].write_message(msg)",if eid in self.subs[s].eid:
"def as_create_delta(self: CallableObjectT, schema: s_schema.Schema, context: so.ComparisonContext) -> sd.ObjectCommand[CallableObjectT]: delta = super().as_create_delta(schema, context) new_params = self.get_params(schema).objects(schema) for p in new_params: <IF_STMT> delta.add_prerequisite(p.as_create_delta(schema=schema, context=context)) return delta","if not param_is_inherited(schema, self, p):"
"def set_indentation_params(self, ispythonsource, guess=True): if guess and ispythonsource: i = self.guess_indent() <IF_STMT> self.indentwidth = i if self.indentwidth != self.tabwidth: self.usetabs = False self.set_tabwidth(self.tabwidth)",if 2 <= i <= 8:
"def _test(): """"""Simple test program to disassemble a file."""""" argc = len(sys.argv) if argc != 2: <IF_STMT> fn = __file__ else: sys.stderr.write('usage: %s [-|CPython compiled file]\n' % __file__) sys.exit(2) else: fn = sys.argv[1] disassemble_file(fn)",if argc == 1:
"def set_lineno(self, lineno, override=False): """"""Set the line numbers of the node and children."""""" todo = deque([self]) while todo: node = todo.popleft() <IF_STMT> if node.lineno is None or override: node.lineno = lineno todo.extend(node.iter_child_nodes()) return self",if 'lineno' in node.attributes:
"def _connect(s, address): try: s.connect(address) except socket.error: ty, v = sys.exc_info()[:2] <IF_STMT> v_err = v.errno else: v_err = v[0] if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]: raise v","if hasattr(v, 'errno'):"
"def SurroundedByParens(token): """"""Check if it's an expression surrounded by parentheses."""""" while token: <IF_STMT> return False if token.value == ')': return not token.next_token if token.OpensScope(): token = token.matching_bracket.next_token else: token = token.next_token return False","if token.value == ',':"
"def read_vocab_list(path, max_vocab_size=20000): vocab = {'<eos>': 0, '<unk>': 1} with io.open(path, encoding='utf-8', errors='ignore') as f: for l in f: w = l.strip() if w not in vocab and w: vocab[w] = len(vocab) <IF_STMT> break return vocab",if len(vocab) >= max_vocab_size:
"def _messageHandled(self, resultList): failures = 0 for success, result in resultList: if not success: failures += 1 log.err(result) if failures: msg = 'Could not send e-mail' resultLen = len(resultList) <IF_STMT> msg += ' ({} failures out of {} recipients)'.format(failures, resultLen) self.sendCode(550, networkString(msg)) else: self.sendCode(250, b'Delivery in progress')",if resultLen > 1:
"def test_images_p_is_stochastic_parameter(self): aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3])) seen = [0, 0] for _ in sm.xrange(1000): observed = aug.augment_image(self.image) if np.array_equal(observed, self.image): seen[0] += 1 <IF_STMT> seen[1] += 1 else: assert False assert np.allclose(seen, [700, 300], rtol=0, atol=75)","elif np.array_equal(observed, self.image_flipped):"
"def kill(self): if self.has_kill: try: kill_method = getattr(self.module_class, 'kill') <IF_STMT> kill_method() else: kill_method(self.i3status_thread.json_list, self.config['py3_config']['general']) except Exception: pass",if self.has_kill == self.PARAMS_NEW:
"def remove_topic(self, topic): if topic not in self.messages: return del self.messages[topic] for sub in self.subscribers.get(topic, set()): <IF_STMT> sub._pyroRelease() if hasattr(sub, '_pyroUri'): try: proxy = self.proxy_cache[sub._pyroUri] proxy._pyroRelease() del self.proxy_cache[sub._pyroUri] except KeyError: pass del self.subscribers[topic]","if hasattr(sub, '_pyroRelease'):"
"def run_async(self, source, target, reverse): to_load = target or self.get_next(source, reverse) if not to_load: return view_signature = self.view_signatures[to_load] window = self.view.window() if window: window.run_command(self.commands[to_load]) <IF_STMT> sublime.set_timeout_async(self.view.close)",if not self.view.settings().get(view_signature):
"def eval_operand(assembly, start, stop, prefix=''): imm = assembly[start + 1:stop] try: eval_imm = eval(imm) <IF_STMT> eval_imm = 4294967295 - eval_imm eval_imm += 1 eval_imm = -eval_imm return assembly.replace(prefix + imm, prefix + hex(eval_imm)) except: return assembly",if eval_imm > 2147483648:
"def admin(): if Configuration.loginRequired(): <IF_STMT> return render_template('login.html') else: person = User.get('_dummy_') login_user(person) output = None if os.path.isfile(Configuration.getUpdateLogFile()): with open(Configuration.getUpdateLogFile()) as updateFile: separator = '==========================\n' output = updateFile.read().split(separator)[-2:] output = separator + separator.join(output) return render_template('admin.html', status='default', **adminInfo(output))",if not current_user.is_authenticated():
"def data(self): result = '' for hunk in self._hunks: <IF_STMT> hunk, f = hunk else: f = lambda x: x result += f(hunk.data()) return result","if isinstance(hunk, tuple) and len(hunk) == 2:"
"def not_less_witness(self, other): n = max(self.longest_run_of_spaces(), other.longest_run_of_spaces()) + 1 a = [] for ts in range(1, n + 1): <IF_STMT> a.append((ts, self.indent_level(ts), other.indent_level(ts))) return a",if self.indent_level(ts) >= other.indent_level(ts):
def _validate(self) -> None: indent = self.indent if indent is not None: <IF_STMT> raise CSTValidationError('An indented block must have a non-zero width indent.') if _INDENT_WHITESPACE_RE.fullmatch(indent) is None: raise CSTValidationError('An indent must be composed of only whitespace characters.'),if len(indent) == 0:
"def sanitize_numeric_fields(info): for numeric_field in self._NUMERIC_FIELDS: field = info.get(numeric_field) <IF_STMT> continue report_force_conversion(numeric_field, 'numeric', 'int') info[numeric_field] = int_or_none(field)","if field is None or isinstance(field, compat_numeric_types):"
def count(self): if self._should_cache('count'): <IF_STMT> return len(self._result_cache) return cached_as(self)(lambda: self._no_monkey.count(self))() else: return self._no_monkey.count(self),if self._result_cache is not None:
