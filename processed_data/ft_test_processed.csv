formatted_method,if_statement
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) <IF_STMT> return [] if ignore_non_errors and is_noerr(e): return [] raise",if ignore_timeouts and is_timeout(e):
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: if reuse_len is not None and reuse_len > 0: curr_out = curr_out[:reuse_len] <IF_STMT> new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",if prev_mem is None:
def filtered(gen): for example in gen: example_len = length_fn(example) if max_length is not None: <IF_STMT> continue if min_length is not None: if example_len < min_length: continue yield example,if example_len > max_length:
"def search(self, query): if not query: logger.debug('Empty search query') return [] logger.debug('Searching TuneIn for ""%s""' % query) args = '&query=' + query search_results = self._tunein('Search.ashx', args) results = [] for item in self._flatten(search_results): <IF_STMT> self._stations[item['guide_id']] = item results.append(item) return results","if item.get('type', '') == 'audio':"
"def _check_script(self, script, directive): for var in compile_script(script): <IF_STMT> return False if var.can_contain('.'): reason = 'At least variable ""${var}"" can contain untrusted user input'.format(var=var.name) self.add_issue(directive=[directive] + var.providers, reason=reason) return True return False",if var.must_contain('/'):
"def getAllDataLinkIDs(): linkDataIDs = set() dataType = _forestData.dataTypeBySocket for socketID, linkedIDs in _forestData.linkedSockets.items(): for linkedID in linkedIDs: <IF_STMT> linkDataIDs.add((socketID, linkedID, dataType[socketID], dataType[linkedID])) else: linkDataIDs.add((linkedID, socketID, dataType[linkedID], dataType[socketID])) return linkDataIDs",if socketID[1]:
"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): if curses: curses.setupterm() <IF_STMT> return True elif colorama: if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",if curses.tigetnum('colors') > 0:
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): if isinstance(ty, SimTypeBottom): l.warning('Found a bottom field in struct %s. Ignore and increment the offset using the default element size.', self.name) continue if not self._pack: align = ty.alignment <IF_STMT> offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",if offset_so_far % align != 0:
"def Restore(self): picker, obj = (self._window, self._pObject) value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: if issubclass(picker.__class__, wx.FileDialog): <IF_STMT> value = value[-1] picker.SetPath(value) return True return False",if type(value) == list:
def dt_s_tup_to_string(dt_s_tup): dt_string = dt_s_tup[0] if dt_s_tup[1] > 0: <IF_STMT> dt_string = dt_string[:2] + 's' + dt_string[2:] else: dt_string = 's' + dt_string return dt_string,if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:
"def writer(stream, items): sep = '' for item in items: stream.write(sep) sep = ' ' <IF_STMT> item = str(item) if not PY3K: if not isinstance(item, unicode): item = str(item) stream.write(item) stream.write('\n')","if not isinstance(item, str):"
"def _get_result_keys(self, config): result_key = config.get('result_key') if result_key is not None: <IF_STMT> result_key = [result_key] result_key = [jmespath.compile(rk) for rk in result_key] return result_key","if not isinstance(result_key, list):"
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None: arch = build['arch_tag'] snap_build = self._lp_load_url(build['self_link']) urls = snap_build.getFileUrls() if not urls: logger.error(f'Snap file not available for arch {arch!r}.') return for url in urls: file_name = _get_url_basename(url) self._download_file(url=url, dst=file_name) <IF_STMT> logger.info(f'Snapped {file_name}') else: logger.info(f'Fetched {file_name}')",if file_name.endswith('.snap'):
"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy['Version'] = '2012-10-17' if self.resource_policy.get('Statement') is None: self.resource_policy['Statement'] = custom_statements else: if not isinstance(custom_statements, list): custom_statements = [custom_statements] statement = self.resource_policy['Statement'] if not isinstance(statement, list): statement = [statement] for s in custom_statements: <IF_STMT> statement.append(s) self.resource_policy['Statement'] = statement",if s not in statement:
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] <IF_STMT> message = f'{idx}. {check.message}' else: message = None example = cast(Case, check.example) display_example(example, check.name, message, result.seed) if idx != len(checks): click.echo('\n')",if check.message:
"def build(opt): dpath = os.path.join(opt['datapath'], 'qangaroo') version = 'v1.1' if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def call(self, step_input, states): new_states = [] for i in range(self.num_layers): out, new_state = self.lstm_cells[i](step_input, states[i]) step_input = layers.dropout(out, self.dropout_prob, dropout_implementation='upscale_in_train') <IF_STMT> else out new_states.append(new_state) return (step_input, new_states)",if self.dropout_prob > 0.0
"def jupyter_progress_bar(min=0, max=1.0): """"""Returns an ipywidget progress bar or None if we can't import it"""""" widgets = wandb.util.get_module('ipywidgets') try: <IF_STMT> from IPython.html import widgets assert hasattr(widgets, 'VBox') assert hasattr(widgets, 'Label') assert hasattr(widgets, 'FloatProgress') return ProgressWidget(widgets, min=min, max=max) except (ImportError, AssertionError): return None",if widgets is None:
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) <IF_STMT> if not os.path.exists(path): self.watches.pop(path).close()",if events | pyuv.fs.UV_RENAME:
"def _get_v1_id_from_tags(self, tags_obj, tag): """"""Get image id from array of tags"""""" if isinstance(tags_obj, dict): try: return tags_obj[tag] except KeyError: pass elif isinstance(tags_obj, []): try: for tag_dict in tags_obj: <IF_STMT> return tag_dict['layer'] except KeyError: pass return ''",if tag_dict['name'] == tag:
"def query_lister(domain, query='', max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes(domain, query, attr_names, next_token=next_token) for item in rs: <IF_STMT> if num_results == max_items: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",if max_items:
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError('TypeError', 'callbackfn must be a function') _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: <IF_STMT> kValue = array.get(unicode(k)) if to_boolean(callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",if array.has_property(unicode(k)):
"def every_one_is(self, dst): msg = 'all members of %r should be %r, but the %dth is %r' for index, item in enumerate(self._src): if self._range: if index < self._range[0] or index > self._range[1]: continue error = msg % (self._src, dst, index, item) <IF_STMT> raise AssertionError(error) return True",if item != dst:
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): <IF_STMT> del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if job_id in key:
"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] <IF_STMT> typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",elif item[2]:
"def _read_data_from_all_categories(self, directory, config, categories): lines = [] for category in categories: data_file = os.path.join(directory, _DATASET_VERSION, category, config) <IF_STMT> with open(data_file) as f: ls = f.read().split('\n') for l in ls[::-1]: if not l: ls.remove(l) lines.extend(ls) return lines",if os.path.exists(data_file):
"def find_handlers(self, forms): handlers = {} for form in forms.itervalues(): for action_name, _action_label in form.actions: <IF_STMT> handlers[action_name] = form else: raise HandlerError('More than one form defines the handler %s' % action_name) return handlers",if action_name not in handlers:
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]: action = get_action_with_primary_id(payload) kwargs = {'task_description': action['description']} story_id = action['story_id'] for ref in payload['references']: <IF_STMT> kwargs['name_template'] = STORY_NAME_TEMPLATE.format(name=ref['name'], app_url=ref['app_url']) if action['changes']['complete']['new']: return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs) else: return None",if ref['id'] == story_id:
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): <IF_STMT> continue edge = (nodes[i], nodes[j]) if graph.has_edge(edge): graph.del_edge(edge) graph.add_edge(edge, 1)",if i == j:
"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): <IF_STMT> lift_stmt[-1].append(op) return op.body if op.attr_key == 'virtual_thread': return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError('not reached')",if op.attr_key == 'storage_scope':
"def format_lazy_import(names): """"""Formats lazy import lines"""""" lines = '' for _, name, asname in names: pkg, _, _ = name.partition('.') <IF_STMT> line = '{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n' else: line = '{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n' lines += line.format(pkg=pkg, mod=name, asname=asname) return lines",if asname is None:
"def evaluateWord(self, argument): wildcard_count = argument[0].count('*') if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith('*'): return self.GetWordWildcard(argument[0][1:], method='endswith') if wildcard_count == 1 and argument[0].endswith('*'): return self.GetWordWildcard(argument[0][:-1], method='startswith') else: _regex = argument[0].replace('*', '.+') matched = False for w in self.words: matched = bool(re.search(_regex, w)) <IF_STMT> break return matched return self.GetWord(argument[0])",if matched:
"def setup(self, ir: 'IR', aconf: Config) -> bool: if self.kind == 'ConsulResolver': self.resolve_with = 'consul' <IF_STMT> self.post_error('ConsulResolver is required to have a datacenter') return False elif self.kind == 'KubernetesServiceResolver': self.resolve_with = 'k8s' elif self.kind == 'KubernetesEndpointResolver': self.resolve_with = 'k8s' else: self.post_error(f'Resolver kind {self.kind} unknown') return False return True",if not self.get('datacenter'):
"def get_success_url(self): """"""Continue to the flow index or redirect according `?back` parameter."""""" if 'back' in self.request.GET: back_url = self.request.GET['back'] <IF_STMT> back_url = '/' return back_url return reverse(self.success_url)","if not is_safe_url(url=back_url, allowed_hosts={self.request.get_host()}):"
"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: if url.startswith('https://'): url = url[8:] <IF_STMT> url = 'http://' + url if playlist: download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if not url.startswith('http://'):
def __str__(self): buf = [''] if self.fileName: buf.append(self.fileName + ':') if self.line != -1: <IF_STMT> buf.append('line ') buf.append(str(self.line)) if self.column != -1: buf.append(':' + str(self.column)) buf.append(':') buf.append(' ') return str('').join(buf),if not self.fileName:
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith('win'): output = output.replace('\\\n', '') environ = {} for line in output.splitlines(0): line = line.rstrip() if not line: continue item = _ParseBashEnvStr(line) <IF_STMT> environ[item[0]] = item[1] return environ",if item:
"def remove_selected(self): """"""Removes selected items from list."""""" to_delete = [] for i in range(len(self)): if self[i].selected: to_delete.append(i) to_delete.reverse() for i in to_delete: self.pop(i) if len(to_delete) > 0: first_to_delete = to_delete[-1] <IF_STMT> self[0].selected = True elif first_to_delete > 0: self[first_to_delete - 1].selected = True",if first_to_delete == 0 and len(self) > 0:
"def update(self, update_tracks=True): self.enable_update_metadata_images(False) old_album_title = self.metadata['album'] self.metadata['album'] = config.setting['nat_name'] for track in self.tracks: <IF_STMT> track.metadata['album'] = self.metadata['album'] for file in track.linked_files: track.update_file_metadata(file) self.enable_update_metadata_images(True) super().update(update_tracks)",if old_album_title == track.metadata['album']:
"def on_input(self, target, message): if message.strip() == '': self.panel('No commit message provided') return if target: command = ['git', 'add'] <IF_STMT> command.append('--all') else: command.extend(('--', target)) self.run_command(command, functools.partial(self.add_done, message)) else: self.add_done(message, '')",if target == '*':
"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos <IF_STMT> self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() if position < editor.document().characterCount(): editor.set_cursor_position(position)",if not osp.isfile(filename):
"def returnByType(self, results): new_results = {} for r in results: type_name = r.get('type', 'movie') + 's' <IF_STMT> new_results[type_name] = [] new_results[type_name].append(r) if 'movies' in new_results: new_results['movies'] = self.combineOnIMDB(new_results['movies']) return new_results",if type_name not in new_results:
def cache_sns_topics_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get('environment') == 'prod': cache_sns_topics_for_account.delay(account_id) el<IF_STMT> cache_sns_topics_for_account.delay(account_id) stats.count(f'{function}.success') return True,"if account_id in config.get('celery.test_account_ids', []):"
"def get(self, subject, topic): """"""Handles GET requests."""""" if subject in feconf.AVAILABLE_LANDING_PAGES: <IF_STMT> self.render_template('topic-landing-page.mainpage.html') else: raise self.PageNotFoundException else: raise self.PageNotFoundException",if topic in feconf.AVAILABLE_LANDING_PAGES[subject]:
"def callback(compiled): <IF_STMT> logger.show_tabulated('Compiled', showpath(codepath), 'without writing to file.') else: with univ_open(destpath, 'w') as opened: writefile(opened, compiled) logger.show_tabulated('Compiled to', showpath(destpath), '.') if self.show: print(compiled) if run: if destpath is None: self.execute(compiled, path=codepath, allow_show=False) else: self.execute_file(destpath)",if destpath is None:
"def _find_start_index(self, string, start, end): while True: index = string.find('{', start, end) - 1 if index < 0: return -1 <IF_STMT> return index start = index + 2","if self._start_index_is_ok(string, index):"
"def _get_nlu_target_format(export_path: Text) -> Text: guessed_format = loading.guess_format(export_path) if guessed_format not in {MARKDOWN, RASA, RASA_YAML}: if rasa.shared.data.is_likely_json_file(export_path): guessed_format = RASA elif rasa.shared.data.is_likely_markdown_file(export_path): guessed_format = MARKDOWN <IF_STMT> guessed_format = RASA_YAML return guessed_format",elif rasa.shared.data.is_likely_yaml_file(export_path):
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: if p.v.children: p.moveToFirstChild() el<IF_STMT> p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break p.moveToParent() return p",if p.hasNext():
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) <IF_STMT> continue name = name_fmt % attr if formatter is not None: value = formatter(attr, value) info_add(name, value)",if value is None:
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, 'Description'): <IF_STMT> attr = desc.getAttributeNodeNS(namespace, name) if attr != None: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element","if desc.getAttributeNS(RDF_NAMESPACE, 'about') == aboutUri:"
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() <IF_STMT> dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),if self.timeout is not None:
"def _parse_fixits(message, titer, line): """"""Parses fixit messages."""""" while OutputParser.message_line_re.match(line) is None and OutputParser.note_line_re.match(line) is None: message_text = line.strip() <IF_STMT> message.fixits.append(Note(message.path, message.line, line.find(message_text) + 1, message_text)) line = next(titer) return line",if message_text != '':
"def _connect_db(self, force_reconnect=False): thread_id = thread.get_ident() if force_reconnect and thread_id in ENGINES: del ENGINES[thread_id] conn = None try: engine = ENGINES[thread_id] conn = engine.connect() _test = conn.execute('SELECT 1') _test.fetchall() except (KeyError, MySQLdb.OperationalError): <IF_STMT> conn.close() engine = sqla.create_engine(self.db_url, pool_recycle=3600) ENGINES[thread_id] = engine conn = engine.connect() return conn",if conn:
"def read(self, n): if self.current_frame: data = self.current_frame.read(n) <IF_STMT> self.current_frame = None return self.file_read(n) if len(data) < n: raise UnpicklingError('pickle exhausted before end of frame') return data else: return self.file_read(n)",if not data and n != 0:
"def __setLoadCmd(self): base = self.__rawLoadCmd for _ in range(self.__machHeader.ncmds): command = LOAD_COMMAND.from_buffer_copy(base) <IF_STMT> segment = SEGMENT_COMMAND.from_buffer_copy(base) self.__setSections(segment, base[56:], 32) elif command.cmd == MACHOFlags.LC_SEGMENT_64: segment = SEGMENT_COMMAND64.from_buffer_copy(base) self.__setSections(segment, base[72:], 64) base = base[command.cmdsize:]",if command.cmd == MACHOFlags.LC_SEGMENT:
"def emit_post_sync_signal(created_models, verbosity, interactive, db): for app in models.get_apps(): app_name = app.__name__.split('.')[-2] <IF_STMT> print('Running post-sync handlers for application %s' % app_name) models.signals.post_syncdb.send(sender=app, app=app, created_models=created_models, verbosity=verbosity, interactive=interactive, db=db)",if verbosity >= 2:
"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get('origin', '') if url in repo.remotes: origin = url url = repo.remotes.get(origin) <IF_STMT> repo.pull(origin_uri=url) else: print('No pull URL.') else: print(command_help['git pull'])",if url:
"def version(self): try: return self._version except AttributeError: for line in self._get_metadata(self.PKG_INFO): <IF_STMT> self._version = safe_version(line.split(':', 1)[1].strip()) return self._version else: tmpl = ""Missing 'Version:' header and/or %s file"" raise ValueError(tmpl % self.PKG_INFO, self)",if line.lower().startswith('version:'):
"def increment(self, metric, labels, delta): """"""Increment a value by |delta|."""""" with self._lock: key = self._get_key(metric.name, labels) <IF_STMT> start_time = self._store[key].start_time value = self._store[key].value + delta else: start_time = time.time() value = metric.default_value + delta self._store[key] = _StoreValue(metric, labels, start_time, value)",if key in self._store:
"def get_current_connections(session): """"""Retrieves open connections using the the given session"""""" res = session.sql('SHOW PROCESSLIST').execute() rows = res.fetch_all() connections = {} for row in rows: <IF_STMT> connections[row.get_string('User')] = [row.get_string('Host')] else: connections[row.get_string('User')].append(row.get_string('Host')) return connections",if row.get_string('User') not in connections:
"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: <IF_STMT> etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: if not os.path.exists(fspath + '.spt'): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if env.cache_static:
def thread_loop(self) -> None: while not self.stop_event.is_set(): time.sleep(1) new_trials = self.study.trials with self.lock: need_to_add_callback = self.new_trials is None self.new_trials = new_trials <IF_STMT> self.doc.add_next_tick_callback(self.update_callback),if need_to_add_callback:
"def _cache_db_tables_iterator(tables, cache_alias, db_alias): no_tables = not tables cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,) db_aliases = settings.DATABASES if db_alias is None else (db_alias,) for db_alias in db_aliases: if no_tables: tables = connections[db_alias].introspection.table_names() <IF_STMT> for cache_alias in cache_aliases: yield (cache_alias, db_alias, tables)",if tables:
"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: if hasattr(subscriber, '_pyroRelease'): subscriber._pyroRelease() <IF_STMT> try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)","if hasattr(subscriber, '_pyroUri'):"
"def test_constructor(job_id): with patch('apscheduler.job.Job._modify') as _modify: scheduler_mock = MagicMock(BaseScheduler) job = Job(scheduler_mock, id=job_id) assert job._scheduler is scheduler_mock assert job._jobstore_alias is None modify_kwargs = _modify.call_args[1] <IF_STMT> assert len(modify_kwargs['id']) == 32 else: assert modify_kwargs['id'] == job_id",if job_id is None:
"def get_connection(self): if self.config.proxy_host != '': return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port) el<IF_STMT> return httplib.HTTPSConnection(self.config.simpledb_host) else: return httplib.HTTPConnection(self.config.simpledb_host)",if self.config.use_https:
"def notify_login(self, ipaddress=''): if app.NOTIFY_ON_LOGIN: update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT] title = common.notifyStrings[common.NOTIFY_LOGIN] <IF_STMT> self._notify_pht(title, update_text.format(ipaddress))",if update_text and title and ipaddress:
"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: if item['layout'] == 'horiz': return 0.03 + ctrl.labelHeight * 3 <IF_STMT> return ctrl.labelHeight * len(item['options'])",elif item['layout'] == 'vert':
"def _get_errors_lines(self): """"""Return the number of lines that contains errors to highlight."""""" errors_lines = [] block = self.document().begin() while block.isValid(): user_data = get_user_data(block) <IF_STMT> errors_lines.append(block.blockNumber()) block = block.next() return errors_lines",if user_data.error:
"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _('Processing...')) self.pbar.set_text(progress) if frac > 1: frac = 1.0 <IF_STMT> frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",if frac < 0:
"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [''] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) if os.path.isdir(os.path.join(basedir, filename)): directories.append(filename) <IF_STMT> yield filename","elif os.path.exists(os.path.join(basedir, filename)):"
"def assistive(self): """"""Detects if item can be used as assistance"""""" if self.__assistive is None: assistive = False for effect in self.effects.values(): <IF_STMT> assistive = True break self.__assistive = assistive return self.__assistive",if effect.isAssistance is True:
"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) <IF_STMT> min_dist = dist closest_unseen = (row, col) return closest_unseen",if dist < min_dist:
"def _maybe_has_default_route(self): for route in self.iter_routes(): <IF_STMT> return True for iface in self.iter_interfaces(): for subnet in iface.get('subnets', []): for route in subnet.get('routes', []): if self._is_default_route(route): return True return False",if self._is_default_route(route):
"def data(self, data): if data is None: raise Exception('Data cannot be None') val = [] for d in data: if isinstance(d, str): val.append(bytes(d, 'utf-8')) <IF_STMT> val.append(d) else: raise Exception('Invalid type, data can only be an str or a bytes not {}: {}'.format(type(data), d)) self.__data = val","elif isinstance(d, bytes):"
"def get_one_segment_function(data, context, echoerr): ext = data['ext'] function_name = context[-2][1].get('function') if function_name: module, function_name = get_function_strings(function_name, context, ext) func = import_segment(function_name, data, context, echoerr, module=module) <IF_STMT> yield func",if func:
"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): if isinstance(value, list): for item in value: <IF_STMT> self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)","if isinstance(item, AST):"
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] <IF_STMT> if v['FeatureType'] == 'val': if name.startswith('SCE_'): states.append((name, v['Value'])) elif name.startswith('SCLEX_'): lexers.append((name, v['Value'])) return (lexers, states)",if v['Category'] != 'Deprecated':
"def things(self, query): limit = query.pop('limit', 100) offset = query.pop('offset', 0) keys = set(self.docs) for k, v in query.items(): <IF_STMT> flat = common.flatten_dict(v)[0] k += '.' + web.rstrips(flat[0], '.key') v = flat[1] keys = set((k for k in self.filter_index(self.index, k, v) if k in keys)) keys = sorted(keys) return keys[offset:offset + limit]","if isinstance(v, dict):"
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: return None <IF_STMT> self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: return None",elif self._keys[hash_] == key:
"def test_204_invalid_content_length(self): with ExpectLog(gen_log, '.*Response with code 204 should not have body'): response = self.fetch('/?error=1') <IF_STMT> self.skipTest('requires HTTP/1.x') if self.http_client.configured_class != SimpleAsyncHTTPClient: self.skipTest('curl client accepts invalid headers') self.assertEqual(response.code, 599)",if not self.http1:
"def __str__(self) -> str: text = '\n' for k, r in self.result.items(): text += '{}\n'.format('#' * 40) <IF_STMT> text += '# {} (failed)\n'.format(k) else: text += '# {} (succeeded)\n'.format(k) text += '{}\n'.format('#' * 40) for sub_r in r: text += '**** {}\n'.format(sub_r.name) text += '{}\n'.format(sub_r) return text",if r.failed:
"def DeleteTask(): oid = request.form.get('oid', '') if oid: result = Mongo.coll['Task'].delete_one({'_id': ObjectId(oid)}) <IF_STMT> result = Mongo.coll['Result'].delete_many({'task_id': ObjectId(oid)}) if result: return 'success' return 'fail'",if result.deleted_count > 0:
"def _replace_vars(self, line, extracted, env_variables): for e in extracted: <IF_STMT> value = env_variables.get(e) if isinstance(value, dict) or isinstance(value, list): value = pprint.pformat(value) decorated = self._decorate_var(e) line = line.replace(decorated, str(value)) return line",if e in env_variables:
"def should_include(service): for f in filt: if f == 'status': state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == 'source': source = filt[f] if source == 'image' or source == 'build': <IF_STMT> return False else: raise UserError('Invalid value for source filter: %s' % source) else: raise UserError('Invalid filter: %s' % f) return True",if source not in service.options:
def state_callback_loop(): if usercallback: when = 1 while when and (not self.future_removed.done()) and (not self.session.shutdownstarttime): result = usercallback(self.get_state()) when = await result if iscoroutine(result) else result <IF_STMT> await sleep(when),if when > 0.0 and (not self.session.shutdownstarttime):
"def __get_new_timeout(self, timeout): """"""When using --timeout_multiplier=#.#"""""" self.__check_scope() try: timeout_multiplier = float(self.timeout_multiplier) <IF_STMT> timeout_multiplier = 0.5 timeout = int(math.ceil(timeout_multiplier * timeout)) return timeout except Exception: return timeout",if timeout_multiplier <= 0.5:
"def readexactly(self, n): buf = b'' while n: yield IORead(self.s) res = self.s.read(n) assert res is not None <IF_STMT> yield IOReadDone(self.s) break buf += res n -= len(res) return buf",if not res:
"def contract_rendering_pane(event): """"""Expand the rendering pane."""""" c = event.get('c') if c: vr = c.frame.top.findChild(QtWidgets.QWidget, 'viewrendered_pane') <IF_STMT> vr.contract() else: viewrendered(event)",if vr:
"def translate_headers(self, environ): """"""Translate CGI-environ header names to HTTP header names."""""" for cgiName in environ: <IF_STMT> yield (self.headerNames[cgiName], environ[cgiName]) elif cgiName[:5] == 'HTTP_': translatedHeader = cgiName[5:].replace('_', '-') yield (translatedHeader, environ[cgiName])",if cgiName in self.headerNames:
"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: <IF_STMT> string_value = str(string_value).strip() if string_value != 'NONE': param_value = int(string_value) except ValueError: self.pcluster_config.warn(""Unable to convert the value '{0}' to an Integer. Using default value for parameter '{1}'"".format(string_value, self.key)) return param_value",if string_value is not None:
"def monitor_filter(self): """"""Return filtered service objects list"""""" services = self.client.services.list(filters={'label': 'com.ouroboros.enable'}) monitored_services = [] for service in services: ouro_label = service.attrs['Spec']['Labels'].get('com.ouroboros.enable') <IF_STMT> monitored_services.append(service) self.data_manager.monitored_containers[self.socket] = len(monitored_services) self.data_manager.set(self.socket) return monitored_services","if not self.config.label_enable or ouro_label.lower() in ['true', 'yes']:"
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 <IF_STMT> nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if nei >= len(self._editableChildren):
"def linkify_cm_by_tp(self, timeperiods): for rm in self: mtp_name = rm.modulation_period.strip() mtp = timeperiods.find_by_name(mtp_name) <IF_STMT> err = ""Error: the business impact modulation '%s' got an unknown modulation_period '%s'"" % (rm.get_name(), mtp_name) rm.configuration_errors.append(err) rm.modulation_period = mtp",if mtp_name != '' and mtp is None:
def close_open_fds(keep=None): keep = [maybe_fileno(f) for f in keep or [] if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): <IF_STMT> try: os.close(fd) except OSError as exc: if exc.errno != errno.EBADF: raise,if fd not in keep:
"def _append_child_from_unparsed_xml(father_node, unparsed_xml): """"""Append child xml nodes to a node."""""" dom_tree = parseString(unparsed_xml) if dom_tree.hasChildNodes(): first_child = dom_tree.childNodes[0] <IF_STMT> child_nodes = first_child.childNodes for _ in range(len(child_nodes)): childNode = child_nodes.item(0) father_node.appendChild(childNode) return raise DistutilsInternalError('Could not Append append elements to the Windows msi descriptor.')",if first_child.hasChildNodes():
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) <IF_STMT> body = six.ensure_str(request.body) if old in body: request.body = body.replace(old, new) return request",if is_text_payload(request) and request.body:
"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, 'func_name_highlighting', True) self.disabled_modules = get_list_opt(options, 'disabled_modules', []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._luabuiltins import MODULES for mod, func in MODULES.iteritems(): <IF_STMT> self._functions.update(func) RegexLexer.__init__(self, **options)",if mod not in self.disabled_modules:
"def GetBestSizeForParentSize(self, parentSize): """"""Finds the best width and height given the parent's width and height."""""" if len(self.GetChildren()) == 1: win = self.GetChildren()[0] <IF_STMT> temp_dc = wx.ClientDC(self) childSize = win.GetBestSizeForParentSize(parentSize) clientParentSize = self._art.GetPanelClientSize(temp_dc, self, wx.Size(*parentSize), None) overallSize = self._art.GetPanelSize(temp_dc, self, wx.Size(*clientParentSize), None) return overallSize return self.GetSize()","if isinstance(win, RibbonControl):"
"def pid_from_name(name): processes = [] for pid in os.listdir('/proc'): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) <IF_STMT> return pid if name in cmdline.split(' ', 1)[0]: return pid except: pass raise ProcessException('No process with such name: %s' % name)",if name in pname:
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element <IF_STMT> i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",if element[3] and element[4]:
"def scan_block_scalar_indentation(self): chunks = [] max_indent = 0 end_mark = self.get_mark() while self.peek() in ' \r\n\x85\u2028\u2029': if self.peek() != ' ': chunks.append(self.scan_line_break()) end_mark = self.get_mark() else: self.forward() <IF_STMT> max_indent = self.column return (chunks, max_indent, end_mark)",if self.column > max_indent:
"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' elif col == BARRIER: tmp += '%' <IF_STMT> tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == FOOD:
"def prepare_data(entry): branch_wise_entries = {} gross_pay = 0 for d in entry: gross_pay += d.gross_pay <IF_STMT> branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay else: branch_wise_entries.setdefault(d.branch, {}).setdefault(d.mode_of_payment, d.net_pay) return (branch_wise_entries, gross_pay)",if branch_wise_entries.get(d.branch):
"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs): self.uuid = uuid self.cluster_state = cluster_state if self.cluster_state is not None: self.children = WeakSet((self.cluster_state.tasks.get(task_id) for task_id in children or () <IF_STMT>)) else: self.children = WeakSet() self._serializer_handlers = {'children': self._serializable_children, 'root': self._serializable_root, 'parent': self._serializable_parent} if kwargs: self.__dict__.update(kwargs)",if task_id in self.cluster_state.tasks
"def listdir(self, d): try: return [p for p in os.listdir(d) <IF_STMT>] except OSError: return []","if os.path.basename(p) != 'CVS' and os.path.isdir(os.path.join(d, p))"
"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: <IF_STMT> command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() if len(e.args) == 1: _errno, errmsg = ('UNKNOWN', e.args[0]) else: _errno, errmsg = e.args raise ConnectionError('Error %s while writing to socket. %s.' % (_errno, errmsg)) except Exception: self.disconnect() raise","if isinstance(command, str):"
"def run(self): """"""Start the scanner"""""" logging.info('Dirscanner starting up') self.shutdown = False while not self.shutdown: with self.loop_condition: self.loop_condition.wait(self.dirscan_speed) <IF_STMT> self.scan()",if self.dirscan_speed and (not self.shutdown):
"def __aexit__(self, exc_type: type, exc_value: BaseException, tb: TracebackType) -> None: if exc_type is not None: await self.close() await self._task while not self._receive_queue.empty(): data = await self._receive_queue.get() if isinstance(data, bytes): self.response_data.extend(data) <IF_STMT> raise data","elif not isinstance(data, HTTPDisconnect):"
"def f(msg): text = extractor(msg) for px in prefix: <IF_STMT> chunks = text[len(px):].split(separator) return (chunks[0], (chunks[1:],) if pass_args else ()) return ((None,),)",if text.startswith(px):
def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) <IF_STMT> for ah in item: if type(ah) is not ActionHandle: raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs,"elif type(item) in (list, tuple, dict, set):"
"def find_class(self, module, name): sys.audit('pickle.find_class', module, name) if self.proto < 3 and self.fix_imports: if (module, name) in _compat_pickle.NAME_MAPPING: module, name = _compat_pickle.NAME_MAPPING[module, name] <IF_STMT> module = _compat_pickle.IMPORT_MAPPING[module] __import__(module, level=0) if self.proto >= 4: return _getattribute(sys.modules[module], name)[0] else: return getattr(sys.modules[module], name)",elif module in _compat_pickle.IMPORT_MAPPING:
"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: wr = util.wait_for_write(self.socket, self.socket.gettimeout()) <IF_STMT> raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))",if not wr:
"def __new__(cls, *args, **kwargs): """"""Hack to ensure method defined as async are implemented as such."""""" coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction) for coroutine in coroutines: implemented_method = getattr(cls, coroutine[0]) <IF_STMT> raise RuntimeError('The method %s must be a coroutine' % implemented_method) return super().__new__(cls, *args, **kwargs)",if not inspect.iscoroutinefunction(implemented_method):
"def add_directive(self, name, obj, content=None, arguments=None, **options): if isinstance(obj, clstypes) and issubclass(obj, Directive): <IF_STMT> raise ExtensionError('when adding directive classes, no additional arguments may be given') directives.register_directive(name, directive_dwim(obj)) else: obj.content = content obj.arguments = arguments obj.options = options directives.register_directive(name, obj)",if content or arguments or options:
"def create(self, w): if w.use_eventloop: w.timer = _Timer(max_interval=10.0) else: <IF_STMT> w.timer_cls = w.pool_cls.Timer w.timer = self.instantiate(w.timer_cls, max_interval=w.timer_precision, on_error=self.on_timer_error, on_tick=self.on_timer_tick)",if not w.timer_cls:
"def _config(_molecule_file, request): with open(_molecule_file) as f: d = util.safe_load(f) if hasattr(request, 'param'): <IF_STMT> d2 = util.safe_load(request.getfixturevalue(request.param)) else: d2 = request.getfixturevalue(request.param) d = util.merge_dicts(d, d2) return d","if isinstance(request.getfixturevalue(request.param), str):"
"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): <IF_STMT> new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model","if isinstance(value, tf.keras.layers.Layer):"
"def is_accepted_drag_event(self, event): if event.source() == self.table: return True mime = event.mimeData() if mime.hasUrls(): for url in mime.urls(): <IF_STMT> break filename = url.toLocalFile() extension = os.path.splitext(filename)[1].lower()[1:] if extension not in _dictionary_formats(): break else: return True return False",if not url.isLocalFile():
"def explain(self, other, depth=0): exp = super(UnionType, self).explain(other, depth) for ndx, subtype in enumerate(self.params['allowed_types']): <IF_STMT> exp += '\n{}and'.format(''.join(['\t'] * depth)) exp += '\n' + subtype.explain(other, depth=depth + 1) return exp",if ndx > 0:
"def test_k_is_stochastic_parameter(self): aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) if np.array_equal(observed, self.blur3x3): seen[0] += True <IF_STMT> seen[1] += True else: raise Exception('Unexpected result in MedianBlur@2') if all(seen): break assert np.all(seen)","elif np.array_equal(observed, self.blur5x5):"
def test_get_message(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: message_id = await self._send_message() message = await self.chat_thread_client.get_message(message_id) assert message.id == message_id assert message.type == ChatMessageType.TEXT assert message.content.message == 'hello world' <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():
"def do_write_property(self, device, callback=None): try: iocb = device <IF_STMT> else self.form_iocb(device, request_type='writeProperty') deferred(self.request_io, iocb) self.requests_in_progress.update({iocb: {'callback': callback}}) iocb.add_callback(self.__general_cb) except Exception as error: log.exception('exception: %r', error)","if isinstance(device, IOCB)"
"def fit(self, dataset, force_retrain): if force_retrain: self.sub_unit_1['fitted'] = True self.sub_unit_1['calls'] += 1 self.sub_unit_2['fitted'] = True self.sub_unit_2['calls'] += 1 else: if not self.sub_unit_1['fitted']: self.sub_unit_1['fitted'] = True self.sub_unit_1['calls'] += 1 <IF_STMT> self.sub_unit_2['fitted'] = True self.sub_unit_2['calls'] += 1 return self",if not self.sub_unit_2['fitted']:
"def _insert_with_loop(self): id_list = [] last_id = None return_id_list = self._return_id_list for row in self._rows: last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute() <IF_STMT> id_list.append(last_id) if return_id_list: return id_list else: return last_id",if return_id_list:
"def merge_block(self): """"""merges a block in the map"""""" for i in range(self.block.x): for j in range(self.block.x): c = self.block.get(i, j) <IF_STMT> self.map[i + self.block.pos.x, j + self.block.pos.y] = c",if c:
"def configure_plex(config): core.PLEX_SSL = int(config['Plex']['plex_ssl']) core.PLEX_HOST = config['Plex']['plex_host'] core.PLEX_PORT = config['Plex']['plex_port'] core.PLEX_TOKEN = config['Plex']['plex_token'] plex_section = config['Plex']['plex_sections'] or [] if plex_section: <IF_STMT> plex_section = ','.join(plex_section) plex_section = [tuple(item.split(',')) for item in plex_section.split('|')] core.PLEX_SECTION = plex_section","if isinstance(plex_section, list):"
"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) <IF_STMT> if xlib.XFilterEvent(e, e.xany.window): continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)","if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease):"
"def format_message(self): bits = [self.message] if self.possibilities: <IF_STMT> bits.append('Did you mean %s?' % self.possibilities[0]) else: possibilities = sorted(self.possibilities) bits.append('(Possible options: %s)' % ', '.join(possibilities)) return '  '.join(bits)",if len(self.possibilities) == 1:
"def _collect_logs(model): page_token = None all_logs = [] while True: paginated_logs = model.lookup_logs(now, later, page_token=page_token) page_token = paginated_logs.next_page_token all_logs.extend(paginated_logs.logs) <IF_STMT> break return all_logs",if page_token is None:
"def run(self): while True: context_id_list_tuple = self._inflated_addresses.get(block=True) <IF_STMT> break c_id, inflated_address_list = context_id_list_tuple inflated_value_map = dict(inflated_address_list) if c_id in self._contexts: self._contexts[c_id].set_from_tree(inflated_value_map)",if context_id_list_tuple is _SHUTDOWN_SENTINEL:
"def _setup_prefix(self): path = self.module_path old = None while path != old: <IF_STMT> self.egg_name = os.path.basename(path) self.egg_info = os.path.join(path, 'EGG-INFO') self.egg_root = path break old = path path, base = os.path.split(path)",if path.lower().endswith('.egg'):
"def get_filename(self, prompt): okay = False val = '' while not okay: val = raw_input('%s: %s' % (prompt, val)) val = os.path.expanduser(val) if os.path.isfile(val): okay = True <IF_STMT> path = val val = self.choose_from_list(os.listdir(path)) if val: val = os.path.join(path, val) okay = True else: val = '' else: print('Invalid value: %s' % val) val = '' return val",elif os.path.isdir(val):
"def versions(self, sitename, data): if 'query' in data: q = json.loads(data['query']) itemid = self._get_itemid(q.get('key')) <IF_STMT> key = q['key'] return json.dumps([self.dummy_edit(key)]) return ConnectionMiddleware.versions(self, sitename, data)",if itemid:
"def read_stanza(self): while True: try: stanza_end = self._buffer.index(b'\n') stanza = self.decoder.decode(self._buffer[:stanza_end]) self._buffer = self._buffer[stanza_end + 1:] colon = stanza.index(':') return (stanza[:colon], stanza[colon + 1:]) except ValueError: bytes = self.read_bytes() <IF_STMT> return None else: self._buffer += bytes",if not bytes:
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit <IF_STMT> if attrnames.has_key(mask): names.append(attrnames[mask]) else: names.append(hex(mask)) return names,if attrs & mask:
"def _set_http_cookie(): if conf.cookie: <IF_STMT> conf.http_headers[HTTP_HEADER.COOKIE] = '; '.join(map(lambda x: '='.join(x), conf.cookie.items())) else: conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie","if isinstance(conf.cookie, dict):"
"def __ne__(self, other): if isinstance(other, WeakMethod): <IF_STMT> return self is not other return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref return True",if not self._alive or not other._alive:
"def update_unread(self, order_id, reset=False): conn = Database.connect_database(self.PATH) with conn: cursor = conn.cursor() <IF_STMT> cursor.execute('UPDATE sales SET unread = unread + 1 WHERE id=?;', (order_id,)) else: cursor.execute('UPDATE sales SET unread=0 WHERE id=?;', (order_id,)) conn.commit() conn.close()",if reset is False:
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) <IF_STMT> field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",elif key == 'nw_dst':
"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) <IF_STMT> if keep_nested is not None: if isinstance(v, dict): yield (k, dict(self.nested_filter(v.items(), keep_nested))) else: yield (k, v)",if k in keep_current:
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: <IF_STMT> break elif p: p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",if p and p.isMarked():
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): <IF_STMT> if isinstance(v, NestedSpace): sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret",if k in striped_keys:
"def update_gradients_full(self, dL_dK, X, X2=None): if self.ARD: phi1 = self.phi(X) <IF_STMT> self.variance.gradient = np.einsum('ij,iq,jq->q', dL_dK, phi1, phi1) else: phi2 = self.phi(X2) self.variance.gradient = np.einsum('ij,iq,jq->q', dL_dK, phi1, phi2) else: self.variance.gradient = np.einsum('ij,ij', dL_dK, self._K(X, X2)) * self.beta",if X2 is None or X is X2:
"def post(self): host_json = json.loads(request.data) host_os = host_json.get('os') if host_os: result = get_monkey_executable(host_os.get('type'), host_os.get('machine')) if result: executable_filename = result['filename'] real_path = MonkeyDownload.get_executable_full_path(executable_filename) <IF_STMT> result['size'] = os.path.getsize(real_path) return result return {}",if os.path.isfile(real_path):
"def _encode_data(self, data, content_type): if content_type is MULTIPART_CONTENT: return encode_multipart(BOUNDARY, data) else: match = CONTENT_TYPE_RE.match(content_type) <IF_STMT> charset = match.group(1) else: charset = settings.DEFAULT_CHARSET return force_bytes(data, encoding=charset)",if match:
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while 'e' in tokens[i + 1:]: i = tokens.index('e', i + 1) s = i - 1 e = i + 1 if not re.match('[0-9]', str(tokens[s])): continue if re.match('[+-]', str(tokens[e])): e += 1 <IF_STMT> e += 1 tokens[s:e] = [''.join(tokens[s:e])] i -= 1 return tokens","if re.match('[0-9]', str(tokens[e])):"
"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) if value is not result: <IF_STMT> self[key] = result if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple): result.parent = self result.key = key return result",if replace:
"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: <IF_STMT> if text.strip().lower() == 'none': text = None bp.cond = text break self.RespondDebuggerData()",if id(bp) == item_id:
"def add(self, url: str, future_nzo: NzbObject, when: Optional[int]=None): """"""Add an URL to the URLGrabber queue, 'when' is seconds from now"""""" if future_nzo and when: future_nzo.url_tries += 1 <IF_STMT> self.fail_to_history(future_nzo, url, T('Maximum retries')) return future_nzo.url_wait = time.time() + when self.queue.put((url, future_nzo))",if future_nzo.url_tries > cfg.max_url_retries():
def _is_datetime_string(series): if series.dtype == object: not_numeric = False try: pd.to_numeric(series) except Exception as e: not_numeric = True datetime_col = None <IF_STMT> try: datetime_col = pd.to_datetime(series) except Exception as e: return False if datetime_col is not None: return True return False,if not_numeric:
"def _getEventAndObservers(self, event): if isinstance(event, xpath.XPathQuery): observers = self._xpathObservers el<IF_STMT> observers = self._eventObservers else: event = xpath.internQuery(event) observers = self._xpathObservers return (event, observers)",if self.prefix == event[:len(self.prefix)]:
"def test_wildcard_import(): bonobo = __import__('bonobo') assert bonobo.__version__ for name in dir(bonobo): if name.startswith('_'): continue attr = getattr(bonobo, name) <IF_STMT> continue assert name in bonobo.__all__",if inspect.ismodule(attr):
"def relint_views(wid=None): windows = [sublime.Window(wid)] if wid else sublime.windows() for window in windows: for view in window.views(): <IF_STMT> hit(view, 'relint_views')",if view.buffer_id() in persist.assigned_linters and view.is_primary():
def _check_for_unknown_gender(self): if self.obj.get_gender() == Person.UNKNOWN: d = GenderDialog(parent=self.window) gender = d.run() d.destroy() <IF_STMT> self.obj.set_gender(gender),if gender >= 0:
"def add_to_path(self, fnames): """"""Add fnames to path"""""" indexes = [] for path in fnames: project = self.get_source_project(path) <IF_STMT> self.parent_widget.emit(SIGNAL('pythonpath_changed()')) indexes.append(self.get_index(path)) if indexes: self.reset_icon_provider() for index in indexes: self.update(index)",if project.add_to_pythonpath(path):
"def validate(self, value): if value.grid_id is not None: if not isinstance(value, self.proxy_class): self.error('FileField only accepts GridFSProxy values') <IF_STMT> self.error('Invalid GridFSProxy value')","if not isinstance(value.grid_id, ObjectId):"
"def shortcut(self, input, ch_out, stride, name, if_first=False): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <IF_STMT> return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) else: return input",if if_first:
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) <IF_STMT> ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",elif code == Path.CURVE4:
"def _get_build_status(self, job_name, build_number): try: build_info = self.server.get_build_info(job_name, build_number) <IF_STMT> return 'building' else: return 'built' except jenkins.NotFoundException: return 'not found'",if build_info['building']:
"def _parse_param_value(name, datatype, default): if datatype == 'bool': if default.lower() == 'true': return True elif default.lower() == 'false': return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': <IF_STMT> return default else: return float(default) else: return str(default)",if type(default) == float:
"def get_fills(self, exchange_order_id): async with aiohttp.ClientSession() as client: response: aiohttp.ClientResponse = await client.get(f'{BASE_URL}{FILLS_ROUTE}', params={'orderId': exchange_order_id, 'limit': 100}) <IF_STMT> try: msg = await response.json() except ValueError: msg = await response.text() raise DydxAsyncAPIError(response.status, msg) return await response.json()",if response.status >= 300:
"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() for key, value in list(semanticTags.items()): if not isinstance(key, int): raise TypeError('At least one key is not a valid int position') if not isinstance(value, list): raise TypeError('At least one value of the provided dict is not a list of string') for x in value: <IF_STMT> raise TypeError('At least one value of the provided dict is not a list of string') self.__semanticTags = semanticTags","if not isinstance(x, str):"
"def start_cutting_tool(self, event, axis, direction): toggle = event.EventObject self.cutting = toggle.Value if toggle.Value: for child in self.cutsizer.Children: child = child.Window <IF_STMT> child.Value = False self.cutting_axis = axis self.cutting_direction = direction else: self.cutting_axis = None self.cutting_direction = None self.cutting_dist = None",if child != toggle:
"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) if patching.attribute_name is not None: keywargs.update(arg) <IF_STMT> extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",elif patching.new is DEFAULT:
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit if attrs & mask: <IF_STMT> names.append(attrnames[mask]) else: names.append(hex(mask)) return names,if attrnames.has_key(mask):
def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/params'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:
"def handle_socket(self, request): conn = request.connection while True: chunk = conn.recv(4) <IF_STMT> break slen = struct.unpack('>L', chunk)[0] chunk = conn.recv(slen) while len(chunk) < slen: chunk = chunk + conn.recv(slen - len(chunk)) obj = pickle.loads(chunk) record = logging.makeLogRecord(obj) self.log_output += record.msg + '\n' self.handled.release()",if len(chunk) < 4:
"def on_source_foreach(self, model, path, iter, id): m_id = model.get_value(iter, self.COLUMN_ID) if m_id == id: if self._foreach_mode == 'get': self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED) <IF_STMT> self._foreach_take = iter",elif self._foreach_mode == 'set':
"def parts(): for l in lists.leaves: head_name = l.get_head_name() if head_name == 'System`List': yield l.leaves <IF_STMT> raise MessageException('Catenate', 'invrp', l)",elif head_name != 'System`Missing':
"def __fill_counter_values(self, command: str): result = [] regex = '(item[0-9]+\\.counter_value)' for token in re.split(regex, command): <IF_STMT> try: result.append(str(self.simulator_config.item_dict[token].value)) except (KeyError, ValueError, AttributeError): logger.error('Could not get counter value for ' + token) else: result.append(token) return ''.join(result)","if re.match(regex, token) is not None:"
"def IMPORTFROM(self, node): <IF_STMT> if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == '*': self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if node.module == '__future__':
"def _split_batch_list(args, batch_list): new_list = [] for batch in batch_list.batches: new_list.append(batch) <IF_STMT> yield batch_pb2.BatchList(batches=new_list) new_list = [] if new_list: yield batch_pb2.BatchList(batches=new_list)",if len(new_list) == args.batch_size_limit:
"def get_branch_or_use_upstream(branch_name, arg, repo): if not branch_name: current_b = repo.current_branch upstream_b = current_b.upstream <IF_STMT> raise ValueError('No {0} branch specified and the current branch has no upstream branch set'.format(arg)) ret = current_b.upstream else: ret = get_branch(branch_name, repo) return ret",if not upstream_b:
"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): <IF_STMT> setattr(self, name, value) for name, value in settings.items(): if name not in default_settings: raise ImproperlyConfigured(""Invalid setting '{}' for {}"".format(name, self.__class__.__name__)) setattr(self, name, value)","if not hasattr(self, name):"
"def _declare(self, name, obj, included=False, quals=0): if name in self._declarations: prevobj, prevquals = self._declarations[name] if prevobj is obj and prevquals == quals: return <IF_STMT> raise api.FFIError('multiple declarations of %s (for interactive usage, try cdef(xx, override=True))' % (name,)) assert '__dotdotdot__' not in name.split() self._declarations[name] = (obj, quals) if included: self._included_declarations.add(obj)",if not self._override:
"def include_file(name, fdir=tmp_dir, b64=False): try: if fdir is None: fdir = '' <IF_STMT> with io.open(os.path.join(fdir, name), 'rb') as f: return base64.b64encode(f.read()).decode('utf-8') else: with io.open(os.path.join(fdir, name), 'r', encoding='utf-8') as f: return f.read() except (OSError, IOError) as e: logger.error(""Could not include file '{}': {}"".format(name, e))",if b64:
"def to_raw_json(self): parts = {} for p in self.parts: <IF_STMT> parts[p[0]] = [] parts[p[0]].append({'value': p[2], 'parameters': p[1]}) children = [x.to_raw_json() for x in self.children] return {'type': self.__class__.__name__, 'children': children, 'parts': parts}",if p[0] not in parts:
"def process_output(output: str, filename: str, start_line: int) -> Tuple[Optional[str], bool]: error_found = False for line in output.splitlines(): t = get_revealed_type(line, filename, start_line) <IF_STMT> return (t, error_found) elif 'error:' in line: error_found = True return (None, True)",if t:
"def __init__(self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3): if row_width > self.max_row_keys: <IF_STMT> logger.error('Telegram does not support reply keyboard row width over %d.' % self.max_row_keys) row_width = self.max_row_keys self.resize_keyboard = resize_keyboard self.one_time_keyboard = one_time_keyboard self.selective = selective self.row_width = row_width self.keyboard = []",if not DISABLE_KEYLEN_ERROR:
"def realizeElementExpressions(innerElement): elementHasBeenRealized = False for exp in innerElement.expressions: if not hasattr(exp, 'realize'): continue before, during, after = exp.realize(innerElement) elementHasBeenRealized = True for n in before: newStream.append(n) <IF_STMT> newStream.append(during) for n in after: newStream.append(n) if elementHasBeenRealized is False: newStream.append(innerElement)",if during is not None:
"def lex_number(self, pos): start = pos found_dot = False while pos < len(self.string) and (self.string[pos].isdigit() or self.string[pos] == '.'): <IF_STMT> if found_dot is True: raise ValueError(""Invalid number. Found multiple '.'"") found_dot = True pos += 1 val = self.string[start:pos] return Token(TokenType.LNUM, val, len(val))",if self.string[pos] == '.':
"def rename(src, dst): if _rename(src, dst): return try: os.rename(src, dst) except OSError as e: <IF_STMT> raise old = '%s-%08x' % (dst, random.randint(0, sys.maxsize)) os.rename(dst, old) os.rename(src, dst) try: os.unlink(old) except Exception: pass",if e.errno != errno.EEXIST:
"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, '__call__'): if num > 1: args = [point, index] else: args = [point] <IF_STMT> args.append(widget) try_callback(callback, *args) return",if pass_widget:
def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({'test': 'insert'}) error = False except: <IF_STMT> raise if self.expect_exception: assert error,if not self.expect_exception:
"def handle(self, *args: Any, **options: Any) -> None: realm = self.get_realm(options) if options['all']: <IF_STMT> raise CommandError('You must specify a realm if you choose the --all option.') self.fix_all_users(realm) return self.fix_emails(realm, options['emails'])",if realm is None:
"def recv_tdi(self, nbits, pos): bits = 0 for n in range(nbits * 2): yield from self._wait_for_tck() <IF_STMT> bits = bits << 1 | (yield self.tdi.o) return bits",if (yield self.tck.o) == pos:
"def _split_head(self): if not hasattr(self, '_severed_head'): <IF_STMT> tree = self._tree.copy() head = tree.get_heading_text() tree.remove_heading() self._severed_head = (head, tree) else: self._severed_head = (None, None) return self._severed_head",if self._tree:
"def buildSearchTrie(self, choices): searchtrie = trie.Trie() for choice in choices: for token in self.tokenizeChoice(choice): <IF_STMT> searchtrie[token] = [] searchtrie[token].append(choice) return searchtrie",if not searchtrie.has_key(token):
"def format_sql(sql, params): rv = [] if isinstance(params, dict): conv = _FormatConverter(params) if params: sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): <IF_STMT> rv.append('NULL') param = safe_repr(param) rv.append(param) return (sql, rv)",if param is None:
def on_completed2(): doner[0] = True if not qr: if len(ql) > 0: observer.on_next(False) observer.on_completed() <IF_STMT> observer.on_next(True) observer.on_completed(),elif donel[0]:
"def notify_digest(self, frequency, changes): notifications = defaultdict(list) users = {} for change in changes: for user in self.get_users(frequency, change): <IF_STMT> notifications[user.pk].append(change) users[user.pk] = user for user in users.values(): self.send_digest(user.profile.language, user.email, notifications[user.pk], subscription=user.current_subscription)",if change.project is None or user.can_access_project(change.project):
"def _any_listener_using(self, target_group_arn): for load_balancer in self.load_balancers.values(): for listener in load_balancer.listeners.values(): for rule in listener.rules: for action in rule.actions: <IF_STMT> return True return False",if action.data.get('target_group_arn') == target_group_arn:
"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) for p, _ in ctr.most_common(): w, pos, l = p if (w, pos) not in self.composite_dict: self.composite_dict[w, pos] = l <IF_STMT> self.word_dict[w] = l return",if w not in self.word_dict:
"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, 'config'), 'r') as f: for line in f: line = line.strip() <IF_STMT> section = line[1:-1].strip() config[section] = dict() elif section: key, value = line.replace(' ', '').split('=') config[section][key] = value return config",if line.startswith('['):
"def send_signal(self, pid, signum): if pid in self.processes: process = self.processes[pid] hook_result = self.call_hook('before_signal', pid=pid, signum=signum) <IF_STMT> logger.debug(""before_signal hook didn't return True => signal %i is not sent to %i"" % (signum, pid)) else: process.send_signal(signum) self.call_hook('after_signal', pid=pid, signum=signum) else: logger.debug('process %s does not exist' % pid)",if signum != signal.SIGKILL and (not hook_result):
"def validate_pos_return(self): if self.is_pos and self.is_return: total_amount_in_payments = 0 for payment in self.payments: total_amount_in_payments += payment.amount invoice_total = self.rounded_total or self.grand_total <IF_STMT> frappe.throw(_(""Total payments amount can't be greater than {}"").format(-invoice_total))",if total_amount_in_payments < invoice_total:
"def delete(key, inner_key=None): if inner_key is not None: try: del cache[key][inner_key] del use_count[key][inner_key] <IF_STMT> del cache[key] del use_count[key] wrapper.cache_size -= 1 except KeyError: return False else: return True else: try: wrapper.cache_size -= len(cache[key]) del cache[key] del use_count[key] except KeyError: return False else: return True",if not cache[key]:
"def insertionsort(array): size = array.getsize() array.reset('Insertion sort') for i in range(1, size): j = i - 1 while j >= 0: <IF_STMT> break array.swap(j, j + 1) j = j - 1 array.message('Sorted')","if array.compare(j, j + 1) <= 0:"
"def publish_state(cls, payload, state): try: if isinstance(payload, LiveActionDB): <IF_STMT> cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)",if state == action_constants.LIVEACTION_STATUS_REQUESTED:
"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: if type(submob) == AnnularSector: sectors.append(submob) for r, submob in zip(np.arange(0, self.radius, dr), sectors): <IF_STMT> continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",if type(submob) != AnnularSector:
"def is_suppressed_warning(type: str, subtype: str, suppress_warnings: List[str]) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: <IF_STMT> target, subtarget = warning_type.split('.', 1) else: target, subtarget = (warning_type, None) if target == type: if subtype is None or subtarget is None or subtarget == subtype or (subtarget == '*'): return True return False",if '.' in warning_type:
"def set_many(self, mapping, timeout=None): timeout = self._normalize_timeout(timeout) pipe = self._client.pipeline(transaction=False) for key, value in _items(mapping): dump = self.dump_object(value) <IF_STMT> pipe.set(name=self.key_prefix + key, value=dump) else: pipe.setex(name=self.key_prefix + key, value=dump, time=timeout) return pipe.execute()",if timeout == -1:
"def maybe_relative_path(path): if not os.path.isabs(path): return path dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) if dir == prevdir or not dir: return path names.append(name) try: <IF_STMT> names.reverse() return os.path.join(*names) except OSError: pass","if samefile(dir, os.curdir):"
"def word_range(word): for ind in range(len(word)): temp = word[ind] for c in [chr(x) for x in range(ord('a'), ord('z') + 1)]: <IF_STMT> yield (word[:ind] + c + word[ind + 1:])",if c != temp:
def validate(self): self.update_soil_edit('sand_composition') for soil_type in self.soil_types: <IF_STMT> frappe.throw(_('{0} should be a value between 0 and 100').format(soil_type)) if sum((self.get(soil_type) for soil_type in self.soil_types)) != 100: frappe.throw(_('Soil compositions do not add up to 100')),if self.get(soil_type) > 100 or self.get(soil_type) < 0:
"def on_click(self, event): run = self._is_running() if event['button'] == self.button_activate: self.py3.command_run(['xscreensaver-command', '-activate']) if event['button'] == self.button_toggle: <IF_STMT> self.py3.command_run(['xscreensaver-command', '-exit']) else: Popen(['xscreensaver', '-no-splash', '-no-capture-stderr'], stdout=PIPE, stderr=PIPE, preexec_fn=setpgrp)",if run:
"def maybe_relative_path(path): if not os.path.isabs(path): return path dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) <IF_STMT> return path names.append(name) try: if samefile(dir, os.curdir): names.reverse() return os.path.join(*names) except OSError: pass",if dir == prevdir or not dir:
"def _format_micros(self, datestring): parts = datestring[:-1].split('.') if len(parts) == 1: <IF_STMT> return datestring[:-1] + '.000000Z' else: return datestring + '.000000Z' else: micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1] return '.'.join(parts[:-1] + ['{:06d}'.format(int(micros))]) + 'Z'",if datestring.endswith('Z'):
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] <IF_STMT> if line[0] == ' ': line = line[1:] f2.writelines(line + '\n')",if line != ' ' and line != '':
"def set(self, item, data): if not type(item) is slice: item = slice(item, item + len(data), None) virt_item = self.item2virtitem(item) if not virt_item: return off = 0 for s, n_item in virt_item: <IF_STMT> i = slice(off, n_item.stop + off - n_item.start, n_item.step) data_slice = data.__getitem__(i) s.content.__setitem__(n_item, data_slice) off = i.stop else: raise ValueError('TODO XXX') return","if isinstance(s, ProgBits):"
"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): if part.get_content_maintype() == 'multipart': continue ctype = part.get_content_type() if ctype is None: ctype = OCTET_TYPE filename = part.get_filename() <IF_STMT> filename = PART_FN_TPL % partnum headers = dict(part) LOG.debug(headers) headers['Content-Type'] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",if not filename:
"def _run_wes(args): """"""Run CWL using a Workflow Execution Service (WES) endpoint"""""" main_file, json_file, project_name = _get_main_and_json(args.directory) main_file = _pack_cwl(main_file) if args.host and 'stratus' in args.host: _run_wes_stratus(args, main_file, json_file) else: opts = ['--no-wait'] <IF_STMT> opts += ['--host', args.host] if args.auth: opts += ['--auth', args.auth] cmd = ['wes-client'] + opts + [main_file, json_file] _run_tool(cmd)",if args.host:
"def insertTestData(self, rows): for row in rows: if isinstance(row, Worker): self.workers[row.id] = dict(id=row.id, name=row.name, paused=0, graceful=0, info=row.info) <IF_STMT> row.id = row.buildermasterid * 10000 + row.workerid self.configured[row.id] = dict(buildermasterid=row.buildermasterid, workerid=row.workerid) elif isinstance(row, ConnectedWorker): self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)","elif isinstance(row, ConfiguredWorker):"
"def local_shape_to_shape_i(node): if node.op == T.shape: <IF_STMT> return shape_feature = node.fgraph.shape_feature ret = shape_feature.make_vector_shape(node.inputs[0]) copy_stack_trace(node.outputs[0], ret) return [ret]","if not hasattr(node.fgraph, 'shape_feature'):"
"def get_config(): """"""Get INI parser with version.ini data."""""" ini_path = os.path.join(THIS_DIRECTORY, 'version.ini') <IF_STMT> ini_path = os.path.join(THIS_DIRECTORY, '../../version.ini') if not os.path.exists(ini_path): raise RuntimeError(""Couldn't find version.ini"") config = configparser.ConfigParser() config.read(ini_path) return config",if not os.path.exists(ini_path):
"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): <IF_STMT> kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) else: raise TypeError('pretrained must be a str or None')","if isinstance(m, nn.Conv2d):"
"def isValidDateString(config_param_name, value, valid_value): try: <IF_STMT> return value day, month, year = value.split('-') if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if value == 'DD-MM-YYYY':
"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError('py_obj must be a wandb.Image') else: <IF_STMT> box_keys = list(py_obj._boxes.keys()) else: box_keys = [] if hasattr(py_obj, 'masks') and py_obj.masks: mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)","if hasattr(py_obj, '_boxes') and py_obj._boxes:"
"def _path_type(st, lst): parts = [] if st: if stat.S_ISREG(st.st_mode): parts.append('file') <IF_STMT> parts.append('dir') else: parts.append('other') if lst: if stat.S_ISLNK(lst.st_mode): parts.append('link') return ' '.join(parts)",elif stat.S_ISDIR(st.st_mode):
"def is_destructive(queries): """"""Returns if any of the queries in *queries* is destructive."""""" keywords = ('drop', 'shutdown', 'delete', 'truncate', 'alter') for query in sqlparse.split(queries): if query: <IF_STMT> return True elif query_starts_with(query, ['update']) is True and (not query_has_where_clause(query)): return True return False","if query_starts_with(query, keywords) is True:"
"def _store_gsuite_membership_post(self): """"""Flush storing gsuite memberships."""""" if not self.member_cache: return self.session.flush() if self.membership_items: <IF_STMT> for item in self.membership_items: stmt = self.dao.TBL_MEMBERSHIP.insert(item) self.session.execute(stmt) else: stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items) self.session.execute(stmt)",if get_sql_dialect(self.session) == 'sqlite':
"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): <IF_STMT> block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) if i < 2: route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",if i > 0:
"def deep_dict(self, root=None): if root is None: root = self result = {} for key, value in root.items(): <IF_STMT> result[key] = self.deep_dict(root=self.__class__._get_next(key, root)) else: result[key] = value return result","if isinstance(value, dict):"
"def _parse_param_list(self, content): r = Reader(content) params = [] while not r.eof(): header = r.read().strip() <IF_STMT> arg_name, arg_type = header.split(' : ')[:2] else: arg_name, arg_type = (header, '') desc = r.read_to_next_unindented_line() desc = dedent_lines(desc) params.append((arg_name, arg_type, desc)) return params",if ' : ' in header:
"def _ungroup(sequence, groups=None): for v in sequence: <IF_STMT> if groups is not None: groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v","if isinstance(v, (list, tuple)):"
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if 'resourcegroup' not in [x.lower() for x in obj.keys()]: if obj['id']: obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: <IF_STMT> _add_resource_group(obj[item_key])",if item_key != 'sourceVault':
"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: <IF_STMT> ret = fvalue.haslayer(cls) if ret: return ret return self.payload.haslayer(cls)","if isinstance(fvalue, Packet):"
"def _post_attachment(self, message, channel, color, sub_fields=None): if channel is None: message_channels = self.channels else: message_channels = [channel] for message_channel in message_channels: attachment = {'fallback': message, 'text': message, 'color': color} <IF_STMT> attachment['fields'] = sub_fields self.slack_client.api_call('chat.postMessage', channel=message_channel, attachments=[attachment], as_user=True)",if sub_fields is not None:
"def create(cls, repository, args): key = cls() passphrase = os.environ.get('ATTIC_PASSPHRASE') if passphrase is not None: passphrase2 = passphrase else: passphrase, passphrase2 = (1, 2) while passphrase != passphrase2: passphrase = getpass('Enter passphrase: ') <IF_STMT> print('Passphrase must not be blank') continue passphrase2 = getpass('Enter same passphrase again: ') if passphrase != passphrase2: print('Passphrases do not match') key.init(repository, passphrase) if passphrase: print('Remember your passphrase. Your data will be inaccessible without it.') return key",if not passphrase:
"def _generate_create_date(self): if self.timezone is not None: tzinfo = tz.gettz(self.timezone) <IF_STMT> tzinfo = tz.gettz(self.timezone.upper()) if tzinfo is None: raise util.CommandError(""Can't locate timezone: %s"" % self.timezone) create_date = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo) else: create_date = datetime.datetime.now() return create_date",if tzinfo is None:
"def _read_header_lines(fp): """"""Read lines with headers until the start of body"""""" lines = deque() for line in fp: if is_empty(line): break <IF_STMT> fp.seek(fp.tell() - len(line)) break lines.append(line) return lines",if not _RE_HEADER.match(line):
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple <IF_STMT> if utils.is_media_file(uri) == True: files.append(uri) if len(files) == 0: return open_dropped_files(files)",if os.path.exists(uri) == True:
"def remove_importlib(frame, options): if frame is None: return None for child in frame.children: remove_importlib(child, options=options) <IF_STMT> frame.self_time += child.self_time frame.add_children(child.children, after=child) child.remove_from_parent() return frame",if '<frozen importlib._bootstrap' in child.file_path:
"def __call__(self, graph): for layer_name, data in self.params: <IF_STMT> node = graph.get_node(layer_name) node.data = self.adjust_parameters(node, data) else: print_stderr('Ignoring parameters for non-existent layer: %s' % layer_name) return graph",if layer_name in graph:
"def test_with_three_points(self): cba = ia.Polygon([(1, 2), (3, 4), (5, 5)]) for i, xy in enumerate(cba): assert i in [0, 1, 2] if i == 0: assert np.allclose(xy, (1, 2)) <IF_STMT> assert np.allclose(xy, (3, 4)) elif i == 2: assert np.allclose(xy, (5, 5)) assert i == 2",elif i == 1:
"def _serve(self): self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain) conn = MsgPackMessages(self._conn) while self.active: request = conn.recv() if not request: logger.warning('DNS: Recieved empty request. Shutdown') self.stop() break now = time.time() response = self.handler.process(request) if not response: response = [] used = time.time() - now <IF_STMT> logger.warning('DNS: Slow processing speed (%s)s', used) conn.send(response)",if used > 1:
"def read(cls, fp, **kwargs): major_version, minor_version, count = read_fmt('2HI', fp) items = [] for _ in range(count): length = read_fmt('I', fp)[0] - 4 <IF_STMT> with io.BytesIO(fp.read(length)) as f: items.append(Annotation.read(f)) return cls(major_version=major_version, minor_version=minor_version, items=items)",if length > 0:
"def save_uploaded_files(): files = [] unzip = bool(request.form.get('unzip') in ['true', 'on']) for uploaded_file in request.files.getlist('files'): <IF_STMT> with zipfile.ZipFile(uploaded_file, 'r') as zf: for info in zf.infolist(): name = info.filename size = info.file_size data = zf.read(name) if size > 0: files.append(save_file(data, filename=name.split('/')[-1])) else: files.append(save_file(uploaded_file)) return files",if unzip and zipfile.is_zipfile(uploaded_file):
"def analyze_string_content(self, string, line_num, filename): output = {} if self.keyword_exclude and self.keyword_exclude.search(string): return output for identifier in self.secret_generator(string, filetype=determine_file_type(filename)): <IF_STMT> continue secret = PotentialSecret(self.secret_type, filename, identifier, line_num) output[secret] = secret return output",if self.is_secret_false_positive(identifier):
"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition['spec'] if 'DefaultValue' in spec: self.hyperparam_dict[name] = spec['DefaultValue'] <IF_STMT> raise ValueError('Required hyperparameter: %s is not set' % name)",elif 'IsRequired' in spec and spec['IsRequired']:
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() <IF_STMT> self.code = self._get_delegate().get_code() return self.code",elif mod_type == imp.PKG_DIRECTORY:
"def eigh_abstract_eval(operand, lower): if isinstance(operand, ShapedArray): <IF_STMT> raise ValueError('Argument to symmetric eigendecomposition must have shape [..., n, n],got shape {}'.format(operand.shape)) batch_dims = operand.shape[:-2] n = operand.shape[-1] v = ShapedArray(batch_dims + (n, n), operand.dtype) w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype)) else: v, w = (operand, operand) return (v, w)",if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) <IF_STMT> return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if not param_match:
"def load_weights_from_unsupervised(self, unsupervised_model): update_state_dict = copy.deepcopy(self.network.state_dict()) for param, weights in unsupervised_model.network.state_dict().items(): if param.startswith('encoder'): new_param = 'tabnet.' + param else: new_param = param <IF_STMT> update_state_dict[new_param] = weights self.network.load_state_dict(update_state_dict)",if self.network.state_dict().get(new_param) is not None:
"def viewer_setup(self): for key, value in DEFAULT_CAMERA_CONFIG.items(): <IF_STMT> getattr(self.viewer.cam, key)[:] = value else: setattr(self.viewer.cam, key, value)","if isinstance(value, np.ndarray):"
"def colormap_changed(change): if change['new']: cmap_colors = [color[1:] for color in cmap.step.__dict__['_schemes'][colormap.value]] palette.value = ', '.join(cmap_colors) colorbar = getattr(cmap.step, colormap.value) colorbar_output = self.colorbar_widget with colorbar_output: colorbar_output.clear_output() display(colorbar) <IF_STMT> labels = [f'Class {i + 1}' for i in range(len(palette.value.split(',')))] legend_labels.value = ', '.join(labels)","if len(palette.value) > 0 and ',' in palette.value:"
"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [br for br in self.blockRenderers if br.layer is Layer.Blocks or br.layer not in layers] <IF_STMT> self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",if len(blockRenderers) < len(self.blockRenderers):
"def fromstring(cls, input): productions = [] for linenum, line in enumerate(input.split('\n')): line = line.strip() <IF_STMT> continue try: productions += _read_dependency_production(line) except ValueError: raise ValueError('Unable to parse line %s: %s' % (linenum, line)) if len(productions) == 0: raise ValueError('No productions found!') return DependencyGrammar(productions)",if line.startswith('#') or line == '':
"def repl(m, base_path, rel_path=None): if m.group('comments'): tag = m.group('comments') else: tag = m.group('open') <IF_STMT> tag += RE_TAG_LINK_ATTR.sub(lambda m2: repl_absolute(m2, base_path), m.group('attr')) else: tag += RE_TAG_LINK_ATTR.sub(lambda m2: repl_relative(m2, base_path, rel_path), m.group('attr')) tag += m.group('close') return tag",if rel_path is None:
"def encode(path): if isinstance(path, str_cls): try: path = path.encode(fs_encoding, 'strict') except UnicodeEncodeError: <IF_STMT> raise path = path.encode(fs_fallback_encoding, 'strict') return path",if not platform.is_linux():
"def __iter__(self): base_iterator = super(ProcessIterable, self).__iter__() if getattr(self.queryset, '_coerced', False): for process in base_iterator: <IF_STMT> process = coerce_to_related_instance(process, process.flow_class.process_class) yield process else: for process in base_iterator: yield process","if isinstance(process, self.queryset.model):"
"def footnotes_under(n: Element) -> Iterator[nodes.footnote]: if isinstance(n, nodes.footnote): yield n else: for c in n.children: <IF_STMT> continue elif isinstance(c, nodes.Element): yield from footnotes_under(c)","if isinstance(c, addnodes.start_of_file):"
"def _process_submissions(self) -> None: """"""Process all submissions which have not been processed yet."""""" while self._to_be_processed: job = self._to_be_processed[0] job.process() <IF_STMT> heapq.heappush(self._steady_priority_queue, OrderedJobs(job.release_time, self._order, job)) self._to_be_processed.popleft() self._order += 1",if not self.batch_mode:
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) <IF_STMT> continue if strip_delimiters: if ',' in line or ';' in line: continue yield line",if match:
"def _get_payload_hash(self, method, data=None): if method in ('POST', 'PUT'): if data: <IF_STMT> return UNSIGNED_PAYLOAD return _hash(data) else: return UNSIGNED_PAYLOAD else: return _hash('')","if hasattr(data, 'next') or hasattr(data, '__next__'):"
"def get_download_info(self): try: download_info = self.api.get_download_info(self.game) result = True except NoDownloadLinkFound as e: print(e) <IF_STMT> Config.unset('current_download') GLib.idle_add(self.parent.parent.show_error, _('Download error'), _('There was an error when trying to fetch the download link!\n{}'.format(e))) download_info = False result = False return (result, download_info)",if Config.get('current_download') == self.game.id:
"def find_id(self, doc_id): self._lock.acquire() try: doc = self._docs.get(doc_id) <IF_STMT> doc = copy.deepcopy(doc) doc['id'] = doc_id return doc finally: self._lock.release()",if doc:
"def assign_art(self, session, task): """"""Place the discovered art in the filesystem."""""" if task in self.art_candidates: candidate = self.art_candidates.pop(task) self._set_art(task.album, candidate, not self.src_removed) <IF_STMT> task.prune(candidate.path)",if self.src_removed:
"def _replace_named(self, named, replace_scalar): for item in named: for name, value in self._get_replaced_named(item, replace_scalar): <IF_STMT> raise DataError('Argument names must be strings.') yield (name, value)",if not is_string(name):
"def qtTypeIdent(conn, *args): res = None value = None for val in args: if not hasattr(val, '__len__'): val = str(val) if len(val) == 0: continue value = val <IF_STMT> value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res","if Driver.needsQuoting(val, True):"
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: <IF_STMT> continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): if any((c.key not in chunk_result for c in tiled_n.chunks)): continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple((sum(nsplit) for nsplit in new_nsplits))) tiled_n._nsplits = new_nsplits",if n.op in failed_ops:
"def _read_filter(self, data): if data: <IF_STMT> self.inner_sha.update(data) if self.expected_inner_md5sum: self.inner_md5.update(data) return data",if self.expected_inner_sha256:
"def find_previous_editable(self, *args): if self.editw == 0: if self._active_page > 0: self.switch_page(self._active_page - 1) if not self.editw == 0: for n in range(self.editw - 1, -1, -1): <IF_STMT> self.editw = n break",if self._widgets__[n].editable and (not self._widgets__[n].hidden):
"def _get_event_for_message(self, message_id): with self.event_lock: <IF_STMT> raise RuntimeError('Event for message[{}] should have been created before accessing'.format(message_id)) return self._events[message_id]",if message_id not in self._events:
"def _get_deepest(self, t): if isinstance(t, list): if len(t) == 1: return t[0] else: for part in t: res = self._get_deepest(part) <IF_STMT> return res return None return None",if res:
"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: <IF_STMT> task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify elif self._chain_notify: return self._chain_notify return None",if action_node.notify:
"def __init__(self, centered=None, shape_params=()): assert centered is None or isinstance(centered, (float, torch.Tensor)) assert isinstance(shape_params, (tuple, list)) assert all((isinstance(name, str) for name in shape_params)) if is_validation_enabled(): if isinstance(centered, float): assert 0 <= centered and centered <= 1 <IF_STMT> assert (0 <= centered).all() assert (centered <= 1).all() else: assert centered is None self.centered = centered self.shape_params = shape_params","elif isinstance(centered, torch.Tensor):"
"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host['host'], squid_host['port']) if fulldata is not None: fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) <IF_STMT> self.publish_counter('%s.%s' % (nickname, matches.group(1)), float(matches.group(2)))",if matches:
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <IF_STMT> bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 9:
"def wait_for_initial_conf(self, timeout=1.0): logger.info('Waiting for initial configuration') cur_timeout = timeout while not self.new_conf and (not self.interrupted): elapsed, _, _ = self.handleRequests(cur_timeout) if elapsed: cur_timeout -= elapsed <IF_STMT> continue cur_timeout = timeout sys.stdout.write('.') sys.stdout.flush()",if cur_timeout > 0:
"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: if self.query_id == -1: self.query_id = query.query_id el<IF_STMT> raise ValueError('query in list must be same query_id')",if self.query_id != query.query_id:
"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: if matchSelf: yield s <IF_STMT> yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if recurseInAnon:
"def get_default_params(problem_type: str, penalty: str): if problem_type == REGRESSION: default_params = {'C': None, 'random_state': 0, 'fit_intercept': True} <IF_STMT> default_params['solver'] = 'auto' else: default_params = {'C': None, 'random_state': 0, 'solver': _get_solver(problem_type), 'n_jobs': -1, 'fit_intercept': True} model_params = list(default_params.keys()) return (model_params, default_params)",if penalty == L2:
"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str): """"""Upload the contents of a local directory to a GCS Bucket."""""" for file_name in os.listdir(local_dir): path = os.path.join(local_dir, file_name) <IF_STMT> logging.info(""Skipping %s as it's not a file."", path) continue logging.info('Uploading: %s', path) gcs_blob = gcs_bucket.blob(f'{gcs_dir}/{file_name}') gcs_blob.upload_from_filename(path)",if not os.path.isfile(path):
"def decode_query_ids(self, trans, conditional): if conditional.operator == 'and': self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split('.')[0] if left_base in self.FIELDS: field = self.FIELDS[left_base] <IF_STMT> conditional.right = trans.security.decode_id(conditional.right)",if field.id_decode:
"def data_dir(self) -> Path: try: from appdirs import user_data_dir except ImportError: path = Path.home() / '.local' / 'share' <IF_STMT> return path / 'dephell' path = Path.home() / 'Library' / 'Application Support' if path.exists(): return path / 'dephell' self.pip_main(['install', 'appdirs']) from appdirs import user_data_dir return Path(user_data_dir('dephell'))",if path.exists():
"def setGameCard(self, isGameCard=False): if isGameCard: targetValue = 1 else: targetValue = 0 for nca in self: if isinstance(nca, Nca): <IF_STMT> continue Print.info('writing isGameCard for %s, %d' % (str(nca._path), targetValue)) nca.header.setIsGameCard(targetValue)",if nca.header.getIsGameCard() == targetValue:
"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' <IF_STMT> if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",elif mode == 'key':
"def register_aggregate_groups(conn, *groups): seen = set() for group in groups: klasses = AGGREGATE_COLLECTION[group] for klass in klasses: name = getattr(klass, 'name', klass.__name__) <IF_STMT> seen.add(name) conn.create_aggregate(name, -1, klass)",if name not in seen:
"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: if isinstance(inputs[1], int): axis = int(inputs[1]) <IF_STMT> axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",elif _is_int_seq(inputs[1]):
"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip('\n').split(' ')] bucket.append(walk) if len(bucket) == batch_size: yield bucket bucket = [] <IF_STMT> yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",if len(bucket):
"def _calculate_runtimes(states): results = {'runtime': 0.0, 'num_failed_states': 0, 'num_passed_states': 0} for state, resultset in states.items(): <IF_STMT> if resultset['result']: results['num_passed_states'] += 1 else: results['num_failed_states'] += 1 results['runtime'] += resultset['duration'] log.debug('Parsed state metrics: {}'.format(results)) return results","if isinstance(resultset, dict) and 'duration' in resultset:"
"def _replicator_primary_device() -> snt_replicator.Replicator: for device_type in ('TPU', 'GPU', 'CPU'): devices = tf.config.experimental.list_logical_devices(device_type=device_type) <IF_STMT> devices = [d.name for d in devices] logging.info('Replicating over %s', devices) return snt_replicator.Replicator(devices=devices) assert False, 'No TPU/GPU or CPU found'",if devices:
"def get_tag_values(self, event): http = event.interfaces.get('sentry.interfaces.Http') if not http: return [] if not http.headers: return [] headers = http.headers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: <IF_STMT> continue ua = Parse(value) if not ua: continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",if key != 'User-Agent':
"def general(metadata, value): if metadata.get('commands') and value: <IF_STMT> v = quote(value) else: v = value return u'{0} {1}'.format(metadata['commands'][0], v) elif not value: return None elif not metadata.get('nargs'): return quote(value) else: return value",if not metadata.get('nargs'):
"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: if self.action_input.selected_index == 0: self.back_to_parent() elif self.action_input.selected_index == 1: self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) <IF_STMT> self._apply_prefs() self.back_to_parent()",elif self.action_input.selected_index == 2:
def logic(): if reset == 1: lfsr.next = 1 el<IF_STMT> lfsr.next = lfsr << 1 lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16],if enable:
"def action_delete(self, request, attachments): deleted_attachments = [] desynced_posts = [] for attachment in attachments: <IF_STMT> deleted_attachments.append(attachment.pk) desynced_posts.append(attachment.post_id) if desynced_posts: with transaction.atomic(): for post in Post.objects.filter(id__in=desynced_posts): self.delete_from_cache(post, deleted_attachments) for attachment in attachments: attachment.delete() message = _('Selected attachments have been deleted.') messages.success(request, message)",if attachment.post:
"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) if feature: feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] <IF_STMT> indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]","elif isinstance(index, slice):"
"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) <IF_STMT> continue elif self._pos == start: return b'' else: chunk = chunk[start - self._pos:] if stop is not None and self._pos > stop: chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",if self._pos < start:
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: <IF_STMT> continue if 'qemux86copy-' in root or 'qemux86-' in root: continue if 'do_build' not in name and 'do_populate_sdk' not in name: f.append(os.path.join(root, name)) return f",if 'meta-environment' in root or 'cross-canadian' in root:
"def _load_windows_store_certs(self, storename, purpose): certs = bytearray() try: for cert, encoding, trust in enum_certificates(storename): <IF_STMT> if trust is True or purpose.oid in trust: certs.extend(cert) except PermissionError: warnings.warn('unable to enumerate Windows certificate store') if certs: self.load_verify_locations(cadata=certs) return certs",if encoding == 'x509_asn':
"def test_tokenizer_identifier_with_correct_config(self): for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]: tokenizer = tokenizer_class.from_pretrained('wietsedv/bert-base-dutch-cased') self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast)) <IF_STMT> self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False) else: self.assertEqual(tokenizer.do_lower_case, False) self.assertEqual(tokenizer.model_max_length, 512)","if isinstance(tokenizer, BertTokenizer):"
"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() if path_id is None: continue self.lock.acquire() self.modules[path_id] = module self.lock.release() <IF_STMT> resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",if resolve:
"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == 'SaltLoggingClass': for base in bases: if hasattr(base, 'trace'): include_trace = False <IF_STMT> include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr(base, 'garbage'):"
"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_owner_: res += prefix + 'owner: %s\n' % self.DebugFormatString(self.owner_) cnt = 0 for e in self.entries_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'entries%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 return res",if printElemNumber:
def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): if escaped: buf.append(c) elif c == '\\': escaped = True <IF_STMT> return ''.join(buf) else: buf.append(c) raise Exception('Unclosed tag ' + ''.join(buf)),elif c == '>':
"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True): if shuffle: random.shuffle(train_nodes) total = train_nodes.shape[0] for i in range(0, total, batch_size): <IF_STMT> cur_nodes = train_nodes[i:i + batch_size] cur_labels = train_labels[cur_nodes] yield (cur_nodes, cur_labels)",if i + batch_size <= total:
def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() if len(splitrow) > 0: <IF_STMT> infos.append(' '.join(splitrow[1:])) return infos,if splitrow[0] == 'INFO:':
"def _validate_client_public_key(self, username, key_data): """"""Validate a client public key for the specified user"""""" try: key = decode_ssh_public_key(key_data) except KeyImportError: return None options = None if self._client_keys: options = self._client_keys.validate(key, self._peer_addr) if options is None: result = self._owner.validate_public_key(username, key) if asyncio.iscoroutine(result): result = (yield from result) <IF_STMT> return None options = {} self._key_options = options return key",if not result:
"def attach_related_versions(addons, addon_dict=None): if addon_dict is None: addon_dict = {addon.id: addon for addon in addons} all_ids = set(filter(None, (addon._current_version_id for addon in addons))) versions = list(Version.objects.filter(id__in=all_ids).order_by()) for version in versions: try: addon = addon_dict[version.addon_id] except KeyError: log.info('Version %s has an invalid add-on id.' % version.id) continue <IF_STMT> addon._current_version = version version.addon = addon",if addon._current_version_id == version.id:
"def move_view(obj, evt): position = obj.GetCurrentCursorPosition() for other_axis, axis_number in self._axis_names.iteritems(): <IF_STMT> continue ipw3d = getattr(self, 'ipw_3d_%s' % other_axis) ipw3d.ipw.slice_position = position[axis_number]",if other_axis == axis_name:
"def func_wrapper(*args, **kwargs): warnings.simplefilter('always', DeprecationWarning) for old, new in arg_mapping.items(): <IF_STMT> warnings.warn(f""Keyword argument '{old}' has been deprecated in favour of '{new}'. '{old}' will be removed in a future version."", category=DeprecationWarning, stacklevel=2) val = kwargs.pop(old) kwargs[new] = val warnings.simplefilter('default', DeprecationWarning) return func(*args, **kwargs)",if old in kwargs:
"def inner_connection_checker(self, *args, **kwargs): LOG.debug('in _connection_checker') for attempts in range(5): try: return func(self, *args, **kwargs) except exception.VolumeBackendAPIException as e: pattern = re.compile('.*Session id expired$') matches = pattern.match(six.text_type(e)) if matches: <IF_STMT> LOG.debug('Session might have expired. Trying to relogin') self._login() continue LOG.error('Re-throwing Exception %s', e) raise",if attempts < 4:
"def set(self, pcount): """"""Set channel prefetch_count setting."""""" if pcount != self.prev: new_value = pcount <IF_STMT> logger.warning('QoS: Disabled: prefetch_count exceeds %r', PREFETCH_COUNT_MAX) new_value = 0 logger.debug('basic.qos: prefetch_count->%s', new_value) self.callback(prefetch_count=new_value) self.prev = pcount return pcount",if pcount > PREFETCH_COUNT_MAX:
"def _build_gcs_object_key(self, key): if self.platform_specific_separator: <IF_STMT> gcs_object_key = os.path.join(self.prefix, self._convert_key_to_filepath(key)) else: gcs_object_key = self._convert_key_to_filepath(key) elif self.prefix: gcs_object_key = '/'.join((self.prefix, self._convert_key_to_filepath(key))) else: gcs_object_key = self._convert_key_to_filepath(key) return gcs_object_key",if self.prefix:
"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): <IF_STMT> name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)",if name not in skip:
def isCurveMonotonic(set_): for i in range(len(set_) - 1): <IF_STMT> return False if set_[i][1] >= set_[i + 1][1]: return False return True,if set_[i][0] >= set_[i + 1][0]:
"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color('Miscellaneous Topics:', 'yellow')) for pp in PAGEPATHS: <IF_STMT> continue content = os.listdir(pp) for pn in content: if '.' in pn: name = pn[:pn.index('.')] else: name = pn print(name)",if not os.path.isdir(pp):
"def test_send_error(self): allow_transfer_encoding_codes = (205, 304) for code in (101, 102, 204, 205, 304): self.con.request('SEND_ERROR', '/{}'.format(code)) res = self.con.getresponse() self.assertEqual(code, res.status) self.assertEqual(None, res.getheader('Content-Length')) self.assertEqual(None, res.getheader('Content-Type')) <IF_STMT> self.assertEqual(None, res.getheader('Transfer-Encoding')) data = res.read() self.assertEqual(b'', data)",if code not in allow_transfer_encoding_codes:
"def _length_hint(obj): """"""Returns the length hint of an object."""""" try: return len(obj) except (AttributeError, TypeError): try: get_hint = type(obj).__length_hint__ except AttributeError: return None try: hint = get_hint(obj) except TypeError: return None <IF_STMT> return None return hint","if hint is NotImplemented or not isinstance(hint, int_types) or hint < 0:"
"def _rmtree(self, path): for name in self._listdir(path): fullname = self._path_join(path, name) try: isdir = self._isdir(fullname) except self._os_error: isdir = False <IF_STMT> self._rmtree(fullname) else: try: self._remove(fullname) except self._os_error: pass try: self._rmdir(path) except self._os_error: pass",if isdir:
"def get_sources(self, sources=None): """"""Returns all sources from this provider."""""" self._load() if sources is None: sources = list(self.data.keys()) elif not isinstance(sources, (list, tuple)): sources = [sources] for source in sources: <IF_STMT> raise KeyError('Invalid data key: {}. Valid keys are: {}'.format(source, ', '.join((str(k) for k in self.data)))) return {k: self.data[k] for k in sources}",if source not in self.data:
"def do_shorts(opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != '': opt, optstring = (optstring[0], optstring[1:]) if short_has_arg(opt, shortopts): <IF_STMT> if not args: raise GetoptError('option -%s requires argument' % opt, opt) optstring, args = (args[0], args[1:]) optarg, optstring = (optstring, '') else: optarg = '' opts.append(('-' + opt, optarg)) return (opts, args)",if optstring == '':
"def _sanitize_dict(self, config_dict, allow_val_change=None, ignore_keys: set=None): sanitized = {} for k, v in six.iteritems(config_dict): <IF_STMT> continue k, v = self._sanitize(k, v, allow_val_change) sanitized[k] = v return sanitized",if ignore_keys and k in ignore_keys:
def x(data): count = 0 while count < 10: data.start_example(SOME_LABEL) b = data.draw_bits(1) <IF_STMT> count += 1 data.stop_example(discard=not b) data.mark_interesting(),if b:
def prompt_for_resume(config): logger = logging.getLogger('changeme') logger.error('A previous scan was interrupted. Type R to resume or F to start a fresh scan') answer = '' while not (answer == 'R' or answer == 'F'): prompt = '(R/F)> ' answer = '' try: answer = raw_input(prompt) except NameError: answer = input(prompt) if answer.upper() == 'F': logger.debug('Forcing a fresh scan') <IF_STMT> logger.debug('Resuming previous scan') config.resume = True return config.resume,elif answer.upper() == 'R':
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): <IF_STMT> results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results","if isinstance(in_arrays, tuple):"
"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: <IF_STMT> raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))","if not util.wait_for_write(self.socket, self.socket.gettimeout()):"
"def _read_jtl_chunk(self, jtl): data = jtl.read(1024 * 1024 * 10) if data: parts = data.rsplit('\n', 1) <IF_STMT> ready_chunk = self.buffer + parts[0] + '\n' self.buffer = parts[1] df = string_to_df(ready_chunk) self.stat_queue.put(df) return df else: self.buffer += parts[0] else: if self.jmeter_finished: self.agg_finished = True jtl.readline() return None",if len(parts) > 1:
"def __new__(mcl, classname, bases, dictionary): slots = list(dictionary.get('__slots__', [])) for getter_name in [key for key in dictionary if key.startswith('get_')]: name = getter_name slots.append('__' + name) getter = dictionary.pop(getter_name) setter = dictionary.get(setter_name, None) <IF_STMT> del dictionary[setter_name] dictionary[name] = property(getter.setter) dictionary['__slots__'] = tuple(slots) return super().__new__(mcl, classname, bases, dictionary)","if setter is not None and isinstance(setter, collections.Callable):"
"def tex_coords(self): """"""Array of texture coordinate data."""""" if 'multi_tex_coords' not in self.domain.attribute_names: <IF_STMT> domain = self.domain attribute = domain.attribute_names['tex_coords'] self._tex_coords_cache = attribute.get_region(attribute.buffer, self.start, self.count) self._tex_coords_cache_version = domain._version region = self._tex_coords_cache region.invalidate() return region.array else: return None",if self._tex_coords_cache_version != self.domain._version:
"def index(self, sub, start=0): """"""Returns the index of the closing bracket"""""" br = '([{<'[')]}>'.index(sub)] count = 0 for i in range(start, len(self.string)): char = self.string[i] <IF_STMT> count += 1 elif char == sub: if count > 0: count -= 1 else: return i err = 'Closing bracket {!r} missing in string {!r}'.format(sub, ''.join(self.original)) raise ParseError(err)",if char == br:
"def test_createFile(self): text = 'This is a test!' path = tempfile.mktemp() try: koDoc = self._koDocFromPath(path, load=False) koDoc.buffer = text koDoc.save(0) del koDoc koDoc2 = self._koDocFromPath(path) assert koDoc2.buffer == text finally: <IF_STMT> os.unlink(path)",if os.path.exists(path):
"def __editScopeHasEdit(self, attributeHistory): with attributeHistory.context: tweak = GafferScene.EditScopeAlgo.acquireParameterEdit(attributeHistory.scene.node(), attributeHistory.context['scene:path'], attributeHistory.attributeName, IECoreScene.ShaderNetwork.Parameter('', self.__parameter), createIfNecessary=False) <IF_STMT> return False return tweak['enabled'].getValue()",if tweak is None:
"def mail_migrator(app, schema_editor): Event_SettingsStore = app.get_model('pretixbase', 'Event_SettingsStore') for ss in Event_SettingsStore.objects.filter(key__in=['mail_text_order_approved', 'mail_text_order_placed', 'mail_text_order_placed_require_approval']): chgd = ss.value.replace('{date}', '{expire_date}') <IF_STMT> ss.value = chgd ss.save() cache.delete('hierarkey_{}_{}'.format('event', ss.object_id))",if chgd != ss.value:
"def __get_limits(self): dimension = len(self.__tree.get_root().data) nodes = self.__get_all_nodes() max, min = ([float('-inf')] * dimension, [float('+inf')] * dimension) for node in nodes: for d in range(dimension): if max[d] < node.data[d]: max[d] = node.data[d] <IF_STMT> min[d] = node.data[d] return (min, max)",if min[d] > node.data[d]:
"def get_complete_position(self, context: UserContext) -> int: for prefix_pattern in convert2list(self.get_filetype_var(context['filetype'], 'prefix_patterns')): m = re.search(self._object_pattern + prefix_pattern + '\\w*$', context['input']) <IF_STMT> continue self._prefix = re.sub('\\w*$', '', m.group(0)) m = re.search('\\w*$', context['input']) if m: return m.start() return -1",if m is None or prefix_pattern == '':
"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): if curses: curses.setupterm() if curses.tigetnum('colors') > 0: return True <IF_STMT> if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",elif colorama:
"def setLabelColumnWidth(self, panel, width): for child in panel.GetChildren(): <IF_STMT> size = child.GetSize() size[0] = width child.SetBestSize(size)","if isinstance(child, wx.lib.stattext.GenStaticText):"
"def update(self, other): if other.M is None: <IF_STMT> self.items.update(other.items) else: for i in other.items: self.add(i) return if self.M is None: self.convert() self.M = array.array('B', list(map(max, list(zip(self.M, other.M)))))",if self.M is None:
"def on_end_epoch(self, state): if self.write_epoch_metrics: <IF_STMT> self.writer.add_text('epoch', '<h4>Epoch {}</h4>'.format(state[torchbearer.EPOCH]) + self.table_formatter(str(state[torchbearer.METRICS])), 1) else: self.writer.add_text('epoch', self.table_formatter(str(state[torchbearer.METRICS])), state[torchbearer.EPOCH])",if self.visdom:
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get('events', []))): <IF_STMT> return False elif e.get('event') == ActionExecuted.type_name: return e.get('name') == ACTION_LISTEN_NAME return False",if e.get('event') == UserUttered.type_name:
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id <IF_STMT> continue if nw_id_ == nw_id: ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",if port.port_no == in_port:
"def next_month(billing_cycle_anchor: datetime, dt: datetime) -> datetime: estimated_months = round((dt - billing_cycle_anchor).days * 12.0 / 365) for months in range(max(estimated_months - 1, 0), estimated_months + 2): proposed_next_month = add_months(billing_cycle_anchor, months) <IF_STMT> return proposed_next_month raise AssertionError(f'Something wrong in next_month calculation with billing_cycle_anchor: {billing_cycle_anchor}, dt: {dt}')",if 20 < (proposed_next_month - dt).days < 40:
"def wait_complete(self): """"""Wait for futures complete done."""""" for future in concurrent.futures.as_completed(self._futures.keys()): try: error = future.exception() except concurrent.futures.CancelledError: break name = self._futures[future] <IF_STMT> err_msg = 'Extracting ""{0}"", got: {1}'.format(name, error) logger.error(err_msg)",if error is not None:
"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): if issubclass(target, mapperlib.Mapper): return target else: mapper = _mapper_or_none(target) <IF_STMT> return mapper else: return _MapperEventsHold(target) else: return target",if mapper is not None:
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: <IF_STMT> gvariant += ' {}'.format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f' {arg}' elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","if isinstance(arg, bool):"
"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) <IF_STMT> if support.match_test(test): print(test.id())","elif isinstance(test, unittest.TestCase):"
def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: <IF_STMT> all_disambiguations.extend(page.relations.disambiguation_links_norm) if page.relations.disambiguation_links is not None: all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),if page.relations.disambiguation_links_norm is not None:
"def test_decode_invalid(self): testcases = [(b'xn--w&', 'strict', UnicodeError()), (b'xn--w&', 'ignore', 'xn-')] for puny, errors, expected in testcases: with self.subTest(puny=puny, errors=errors): <IF_STMT> self.assertRaises(UnicodeError, puny.decode, 'punycode', errors) else: self.assertEqual(puny.decode('punycode', errors), expected)","if isinstance(expected, Exception):"
"def find_globs(walker, patterns, matches): for root, dirs, files in walker: for d in dirs: d = join(root, d) for pattern in patterns: for p in Path(d).glob(pattern): matches.add(str(p)) sub_files = set() for p in matches: <IF_STMT> for f in files: sub_files.add(join(root, f)) matches.update(sub_files)",if root.startswith(p):
"def parse_stack_trace(self, it, line): """"""Iterate over lines and parse stack traces."""""" events = [] stack_traces = [] while self.stack_trace_re.match(line): event = self.parse_stack_trace_line(line) <IF_STMT> events.append(event) stack_traces.append(line) line = get_next(it) events.reverse() return (stack_traces, events, line)",if event:
"def process(self): """"""Do processing necessary, storing result in feature."""""" summation = 0 histo = self.data['flat.notes.quarterLengthHistogram'] if not histo: raise NativeFeatureException('input lacks notes') maxKey = 0 for key in histo: if histo[key] > 0: summation += histo[key] <IF_STMT> maxKey = histo[key] self.feature.vector[0] = maxKey / summation",if histo[key] >= maxKey:
"def load_resource(name): """"""return file contents for files within the package root folder"""""" try: <IF_STMT> return sublime.load_resource('Packages/Markdown Preview/{0}'.format(name)) else: filename = os.path.join(sublime.packages_path(), INSTALLED_DIRECTORY, os.path.normpath(name)) return load_utf8(filename) except: print(""Error while load_resource('%s')"" % name) traceback.print_exc() return ''",if is_ST3():
"def get_password(self, service, repo_url): if self.is_unlocked: asyncio.set_event_loop(asyncio.new_event_loop()) collection = secretstorage.get_default_collection(self.connection) attributes = {'application': 'Vorta', 'service': service, 'repo_url': repo_url} items = list(collection.search_items(attributes)) logger.debug('Found %i passwords matching repo URL.', len(items)) <IF_STMT> return items[0].get_secret().decode('utf-8') return None",if len(items) > 0:
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) if fname == 'output': continue if fname == 'PureMVC_Python_1_0': continue if fname[-4:] == '.pyc': continue <IF_STMT> get_dir(p) else: res.append(p) return res",if os.path.isdir(p):
"def test_nic_names(self): p = subprocess.Popen(['ipconfig', '/all'], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: <IF_STMT> continue if nic not in out: self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)","if 'pseudo-interface' in nic.replace(' ', '-').lower():"
"def vexop_to_simop(op, extended=True, fp=True): res = operations.get(op) if res is None and extended: attrs = op_attrs(op) <IF_STMT> raise UnsupportedIROpError('Operation not implemented') res = SimIROp(op, **attrs) if res is None: raise UnsupportedIROpError('Operation not implemented') if res._float and (not fp): raise UnsupportedIROpError('Floating point support disabled') return res",if attrs is None:
"def rule_builder_add_value(self, value, screenshot_name=None): rule_builder = self.components.rule_builder rule_builder.menu_button_column.wait_for_and_click() with self.rule_builder_rule_editor('add-column-value') as editor_element: filter_input = editor_element.find_element_by_css_selector(""input[type='text']"") filter_input.clear() filter_input.send_keys(value) <IF_STMT> self.screenshot(screenshot_name)",if screenshot_name:
"def make_open_socket(self): s = socket.socket() try: s.bind(DEFAULT_BIND_ADDR_TUPLE) <IF_STMT> s.listen(1) self.assert_open(s, s.fileno()) except: s.close() s = None raise return s",if WIN or greentest.LINUX:
"def handle_ray_task_error(e): for s in e.traceback_str.split('\n')[::-1]: <IF_STMT> try: raise getattr(builtins, s.split(':')[0])(''.join(s.split(':')[1:])) except AttributeError as att_err: if 'module' in str(att_err) and builtins.__name__ in str(att_err): pass else: raise att_err raise e",if 'Error' in s or 'Exception' in s:
"def compare_multiple_events(i, expected_results, actual_results): events_in_a_row = [] j = i while j < len(expected_results) and isinstance(actual_results[j], actual_results[i].__class__): events_in_a_row.append(actual_results[j]) j += 1 message = '' for event in events_in_a_row: for k in range(i, j): passed, message = compare_events(expected_results[k], event) <IF_STMT> expected_results[k] = None break else: return (i, False, message) return (j, True, '')",if passed:
"def ListSubscriptions(self, params): queryreturn = sqlQuery('SELECT label, address, enabled FROM subscriptions') data = '{""subscriptions"":[' for row in queryreturn: label, address, enabled = row label = shared.fixPotentiallyInvalidUTF8Data(label) <IF_STMT> data += ',' data += json.dumps({'label': label.encode('base64'), 'address': address, 'enabled': enabled == 1}, indent=4, separators=(',', ': ')) data += ']}' return data",if len(data) > 20:
"def compile(self, args): compiled_args = {} for key, value in six.iteritems(args): <IF_STMT> compiled_args[key] = str(value) else: compiled_args[key] = sjson_dumps(value) return self._minified_code % compiled_args",if key in self.clean_args:
"def insert(self, pack_id, data): if pack_id not in self.queue and pack_id > self.begin_id: self.queue[pack_id] = PacketInfo(data) if self.end_id == pack_id: self.end_id = pack_id + 1 <IF_STMT> eid = self.end_id while eid < pack_id: self.miss_queue.add(eid) eid += 1 self.end_id = pack_id + 1 else: self.miss_queue.remove(pack_id)",elif self.end_id < pack_id:
"def _target_generator(self): if self._internal_target_generator is None: <IF_STMT> return None from ....model_zoo.ssd.target import SSDTargetGenerator self._internal_target_generator = SSDTargetGenerator(iou_thresh=self._iou_thresh, stds=self._box_norm, negative_mining_ratio=-1, **self._kwargs) return self._internal_target_generator else: return self._internal_target_generator",if self._anchors_none:
"def test_heapsort(self): for trial in range(100): size = random.randrange(50) data = [random.randrange(25) for i in range(size)] <IF_STMT> heap = data[:] self.module.heapify(heap) else: heap = [] for item in data: self.module.heappush(heap, item) heap_sorted = [self.module.heappop(heap) for i in range(size)] self.assertEqual(heap_sorted, sorted(data))",if trial & 1:
"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) <IF_STMT> code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",if res == _subprocess.WAIT_OBJECT_0:
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break <IF_STMT> continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",elif value is None:
"def isnotsurplus(self, item: T) -> bool: if not self.matchers: <IF_STMT> self.mismatch_description.append_text('not matched: ').append_description_of(item) return False return True",if self.mismatch_description:
"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): <IF_STMT> return environ.get(list(config.values())[0]) elif list(config.keys()) == ['$file']: return open(list(config.values())[0]).read() else: return {key: resolve_env_secrets(value, environ) for key, value in config.items()} elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",if list(config.keys()) == ['$env']:
"def __open__(filename, *args, **kwargs): if os.path.isfile(filename): return __realopen__(filename, *args, **kwargs) if not os.path.isabs(filename): datafilename = __papplet__.dataPath(filename) <IF_STMT> return __realopen__(datafilename, *args, **kwargs) sketchfilename = __papplet__.sketchPath(filename) if os.path.isfile(sketchfilename): return __realopen__(sketchfilename, *args, **kwargs) return __realopen__(filename, *args, **kwargs)",if os.path.isfile(datafilename):
def run(self): while not self.completed: <IF_STMT> time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),if self.block:
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: <IF_STMT> raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if error_on_path:
"def structured_dot_grad(sparse_A, dense_B, ga): if sparse_A.type.format in ('csc', 'csr'): <IF_STMT> sdgcsx = sdg_csc CSx = CSC else: sdgcsx = sdg_csr CSx = CSR g_A_data = sdgcsx(csm_indices(sparse_A), csm_indptr(sparse_A), dense_B, ga) return CSx(g_A_data, csm_indices(sparse_A), csm_indptr(sparse_A), csm_shape(sparse_A)) else: raise NotImplementedError()",if sparse_A.type.format == 'csc':
"def step_async(self, actions): listify = True try: <IF_STMT> listify = False except TypeError: pass if not listify: self.actions = actions else: assert self.num_envs == 1, f'actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments' self.actions = [actions]",if len(actions) == self.num_envs:
"def tempFailureRetry(func, *args, **kwargs): while True: try: return func(*args, **kwargs) except (os.error, IOError) as ex: <IF_STMT> continue else: raise",if ex.errno == errno.EINTR:
"def test_learning_always_changes_generation(chars, order): learner = LStar(lambda s: len(s) == 1 and s[0] in chars) for c in order: prev = learner.generation s = bytes([c]) <IF_STMT> learner.learn(s) assert learner.generation > prev",if learner.dfa.matches(s) != learner.member(s):
"def test_costs_5D_noisy_names(signal_bkps_5D_noisy, cost_name): signal, bkps = signal_bkps_5D_noisy cost = cost_factory(cost_name) cost.fit(signal) cost.error(0, 100) cost.error(100, signal.shape[0]) cost.error(10, 50) cost.sum_of_costs(bkps) with pytest.raises(NotEnoughPoints): <IF_STMT> cost.min_size = 4 cost.error(1, 2) else: cost.error(1, 2)",if cost_name == 'cosine':
"def remove_empty_dirs(dirname): logger.debug(""remove_empty_dirs '%s'"" % dirname) try: <IF_STMT> dirname = dirname.encode('utf-8') os.removedirs(dirname) logger.debug(""remove_empty_dirs '%s' done"" % dirname) except OSError as exc: if exc.errno == errno.ENOTEMPTY: logger.debug(""remove_empty_dirs '%s' not empty"" % dirname) pass else: raise except Exception as e: logger.exception(e) logger.error('remove_empty_dirs exception: ' + dirname) raise e","if not isinstance(dirname, str):"
"def get_unique_attribute(self, name: str): feat = None for f in self.features: <IF_STMT> if feat is not None: raise RuntimeError('The attribute was not unique.') feat = f if feat is None: raise RuntimeError('The attribute did not exist') return getattr(feat, name)","if self._return_feature(f) and hasattr(f, name):"
"def get_allocated_address(self, config: ActorPoolConfig, allocated: allocated_type) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): if strategy == self: occupied = True break <IF_STMT> return addr raise NoIdleSlot(f'No idle slot for creating actor with label {self.label}, mark {self.mark}')",if not occupied:
"def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result for key, value in self.__dict__.items(): <IF_STMT> setattr(result, key, copy.copy(value)) else: setattr(result, key, copy.deepcopy(value, memo)) return result",if key in cls.dynamic_methods:
def restore_forward(model): for child in model.children(): <IF_STMT> child.forward = child.old_forward child.old_forward = None else: restore_forward(child),"if is_leaf(child) and hasattr(child, 'old_forward'):"
"def add(self, obj, allow_duplicates=False): if allow_duplicates or obj not in self._constants: self._constant_pool.append(obj) self._constants[obj] = len(self) <IF_STMT> self._constant_pool.append(None)","if obj.__class__ in (Double, Long):"
def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: idx = l.lower().find('copyright') if idx < 0: continue copyright = l[idx + 9:].strip() if not copyright: continue copyright = sanitise(copyright) <IF_STMT> continue ret.add(copyright) return ret,if not copyright.find('200') >= 0 and (not copyright.find('199') >= 0):
"def callback(lexer, match, context): text = match.group() extra = '' if start: context.next_indent = len(text) if context.next_indent < context.indent: while context.next_indent < context.indent: context.indent = context.indent_stack.pop() <IF_STMT> extra = text[context.indent:] text = text[:context.indent] else: context.next_indent += len(text) if text: yield (match.start(), TokenClass, text) if extra: yield (match.start() + len(text), TokenClass.Error, extra) context.pos = match.end()",if context.next_indent > context.indent:
"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) if not cmd.check(f'docker check for {self.path.k8s}'): if not cmd.stdout.strip(): log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) <IF_STMT> print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if log_cmd.check(f'docker logs for {self.path.k8s}'):
"def nodes(self): if not self._nodes: nodes = self.cluster_group.instances() self._nodes = [] master = self.master_node nodeid = 1 for node in nodes: if node.state not in ['pending', 'running']: continue <IF_STMT> self._nodes.insert(0, master) continue self._nodes.append(Node(node, self.key_location, 'node%.3d' % nodeid)) nodeid += 1 else: for node in self._nodes: log.debug('refreshing instance %s' % node.id) node.update() return self._nodes",if node.id == master.id:
"def match(cls, agent_name, guid, uri, media=None): agent = Agents.get(agent_name) if agent is None: <IF_STMT> log.warn('Unsupported metadata agent: %s' % agent_name) unsupported_agents[agent_name] = True return False log.warn('Unsupported metadata agent: %s' % agent_name, extra={'duplicate': True}) return False return agent.fill(guid, uri, media)",if agent_name not in unsupported_agents:
"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)): plug.setInput(randomNode['outFloat']) <IF_STMT> plug.setInput(randomNode['outColor']) GafferUI.NodeEditor.acquire(randomNode)","elif isinstance(plug, Gaffer.Color3fPlug):"
"def post_arrow(self, arr: pa.Table, graph_type: str, opts: str=''): dataset_id = self.dataset_id tok = self.token sub_path = f'api/v2/upload/datasets/{dataset_id}/{graph_type}/arrow' try: resp = self.post_arrow_generic(sub_path, tok, arr, opts) out = resp.json() <IF_STMT> raise Exception('No success indicator in server response') return out except Exception as e: logger.error('Failed to post arrow to %s', sub_path, exc_info=True) raise e",if not 'success' in out or not out['success']:
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): <IF_STMT> child = dict_to_XML('layer', val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: if tag == 'config': child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",if tag == 'layers':
def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) <IF_STMT> error('node not found: ' + str(dir)) continue if not node in lst: lst.append(node) self.bld_incpaths_lst.append(node),if not node:
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): if isinstance(val, compat.string_types): return '  %s' % val elif val < 1024 ** 2: return '  %.1f KB' % (val / 1024.0 ** 1) <IF_STMT> return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val",elif val < 1024 ** 3:
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <IF_STMT> curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",if reuse_len is not None and reuse_len > 0:
"def GROUP_CONCAT(builder, distinct, expr, sep=None): assert distinct in (None, True, False) result = (distinct and 'GROUP_CONCAT(DISTINCT ' or 'GROUP_CONCAT(', builder(expr)) if sep is not None: <IF_STMT> result = (result, ' SEPARATOR ', builder(sep)) else: result = (result, ', ', builder(sep)) return (result, ')')",if builder.provider.dialect == 'MySQL':
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.custom_fields = [] self.obj_type = ContentType.objects.get_for_model(self.model) custom_fields = CustomField.objects.filter(content_types=self.obj_type) for cf in custom_fields: <IF_STMT> self.nullable_fields.append(cf.name) self.fields[cf.name] = cf.to_form_field(set_initial=False, enforce_required=False) self.custom_fields.append(cf.name)",if not cf.required:
"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: <IF_STMT> return True if possible_child_hash not in self.items: return False possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash == item_hash:
"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) <IF_STMT> self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) if not self.use_parallel_executor: self.assertTrue((in_data == out_data).all())",if self.use_parallel_executor and (not self.use_double_buffer):
"def add_cells(self, cells): for cell in cells: <IF_STMT> id = len(self.cell_id_map) self.cell_id_map[cell] = id self.id_cell_map[id] = cell",if cell not in self.cell_id_map:
"def _verify_out(marker='>>'): if shared: self.assertIn('libapp_lib.dylib', self.client.out) el<IF_STMT> self.assertIn('libapp_lib.a', self.client.out) else: self.assertIn('Built target app_lib', self.client.out) out = str(self.client.out).splitlines() for k, v in vals.items(): self.assertIn('%s %s: %s' % (marker, k, v), out)",if marker == '>>':
"def Visit_expr(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance(child, pytree.Leaf) and child.value == '|':"
"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) if not data: return <IF_STMT> self.limit = 0 self.after = Object(id=int(data[-1]['user']['id'])) for element in reversed(data): await self.members.put(self.create_member(element))",if len(data) < 1000:
"def assert_warns(expected): with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always') yield if sys.version_info >= (3, 0): <IF_STMT> try: exc_name = expected.__name__ except AttributeError: exc_name = str(expected) raise AssertionError('%s not triggerred' % exc_name)","if not any((isinstance(m.message, expected) for m in w)):"
"def __init__(self, measures): """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class"""""" self.__class__.__name__ = 'Contingency' + measures.__class__.__name__ for k in dir(measures): <IF_STMT> continue v = getattr(measures, k) if not k.startswith('_'): v = self._make_contingency_fn(measures, v) setattr(self, k, v)",if k.startswith('__'):
"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: omit = elem.tag == 'kw' and elem.get('type') != 'teardown' start = event == 'start' <IF_STMT> omitted_kws += 1 if not omitted_kws: yield (event, elem) elif not start: elem.clear() if omit and (not start): omitted_kws -= 1",if omit and start:
"def read_block(buffer, i): offset = i * BLOCK_LENGTH % config.CAPTURE_BUFFER while True: if buffer[offset] == BLOCK_MARKER.END: return None while buffer[offset] == BLOCK_MARKER.WRITE: time.sleep(SHORT_SENSOR_SLEEP_TIME) buffer[offset] = BLOCK_MARKER.READ buffer.seek(offset + 1) length = struct.unpack('=H', buffer.read(2))[0] retval = buffer.read(length) <IF_STMT> break buffer[offset] = BLOCK_MARKER.NOP return retval",if buffer[offset] == BLOCK_MARKER.READ:
def _start(self): try: instance_info = self._get_instance_info() <IF_STMT> self._multipass_cmd.start(instance_name=self.instance_name) except errors.ProviderInfoError as instance_error: raise errors.ProviderInstanceNotFoundError(instance_name=self.instance_name) from instance_error,if not instance_info.is_running():
"def _river_driver(self): if self._cached_river_driver: return self._cached_river_driver else: <IF_STMT> self._cached_river_driver = MsSqlDriver(self.workflow, self.wokflow_object_class, self.field_name) else: self._cached_river_driver = OrmDriver(self.workflow, self.wokflow_object_class, self.field_name) return self._cached_river_driver",if app_config.IS_MSSQL:
"def __LazyMap__(self, attr): try: <IF_STMT> debug_attr_print('%s.__LazyMap__(%s) added something' % (self._username_, attr)) return 1 except AttributeError: return 0",if self._LazyAddAttr_(attr):
"def prepare(self, data=None, user=None): """"""Prepare activation for execution."""""" super(ManagedStartViewActivation, self).prepare.original() self.task.owner = user management_form_class = self.get_management_form_class() self.management_form = management_form_class(data=data, instance=self.task) if data: <IF_STMT> raise FlowRuntimeError('Activation metadata is broken {}'.format(self.management_form.errors)) self.task = self.management_form.save(commit=False)",if not self.management_form.is_valid():
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 <IF_STMT> RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",if not RegionSizeGuid:
"def _get_completion(self, document): try: completion_header = document.xpath(""//div[@id='complete_day']"")[0] completion_message = completion_header.getchildren()[0] <IF_STMT> return False elif 'day_complete_message' in completion_message.classes: return True except IndexError: return False",if 'day_incomplete_message' in completion_message.classes:
"def run(self): DISPATCH_SYNC = components.interfaces.nsIEventTarget.DISPATCH_SYNC try: <IF_STMT> return for match in findlib2.find_all_matches(self.regex, self.text): if self._stopped: return self.target.dispatch(lambda: self.callback(match), DISPATCH_SYNC) if self._stopped: return self.target.dispatch(lambda: self.callback(None), DISPATCH_SYNC) finally: self.callback = None self.target = None",if self._stopped:
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) <IF_STMT> return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)",elif 'regex' in literal_or_identifier:
"def process_image_pre_creation(sender, instance: Image, **kwargs): if instance.pk is not None: return for plugin in _plugin_instances: process_fn = getattr(plugin, 'process_image_pre_creation', None) <IF_STMT> continue try: process_fn(django_settings=settings, image_instance=instance) except Exception: logging.exception(""Error occurs while trying to access plugin's pin_pre_save for plugin %s"" % plugin)",if process_fn is None:
def check_screenshots(self): if self.interactive: self._commit_screenshots() el<IF_STMT> self._validate_screenshots() self._commit_screenshots() elif self.allow_missing_screenshots: warnings.warn('No committed reference screenshots available. Ignoring.') else: self.fail('No committed reference screenshots available. Run interactive first.'),if self._has_reference_screenshots():
"def on_task_abort(self, task, config): if 'abort' in config: <IF_STMT> return log.debug('sending abort notification') self.send_notification(config['abort']['title'], config['abort']['message'], config['abort']['via'], template_renderer=task.render)",if task.silent_abort:
"def block_users(self, user_ids): broken_items = [] self.logger.info('Going to block %d users.' % len(user_ids)) for user_id in tqdm(user_ids): <IF_STMT> self.error_delay() broken_items = user_ids[user_ids.index(user_id):] break self.logger.info('DONE: Total blocked %d users.' % self.total['blocks']) return broken_items",if not self.block(user_id):
"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] parent = self['editor'] for c in parent.get_children(): if hasattr(c, 'get_id'): if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) <IF_STMT> return r return None",if not r is None:
"def addClasses(self, name): for n in name.split(): try: k, method = n.split('.') except ValueError: k = n method = None self.classes[k] = 1 <IF_STMT> self.methods.setdefault(k, {})[method] = 1",if method is not None:
"def Read(self, lex_mode): while True: t = self._Read(lex_mode) self.was_line_cont = t.id == Id.Ignored_LineCont <IF_STMT> break return t",if t.id != Id.Ignored_LineCont:
"def _dir_guildfile(dir, ctx): from guild import guildfile try: return guildfile.for_dir(dir) except guildfile.NoModels: <IF_STMT> help_suffix = "" or '%s' for help"" % click_util.cmd_help(ctx) else: help_suffix = '' cli.error('%s does not contain a Guild file (guild.yml)\nTry specifying a project path or package name%s.' % (cwd_desc(dir), help_suffix)) except guildfile.GuildfileError as e: cli.error(str(e))",if ctx:
"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: if not line.strip(): continue <IF_STMT> return elif line.startswith(b'Benutzer/Passwort Fehler'): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",if line.startswith(b'OK'):
"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f'Unexpected response type: `{type(response)}`') <IF_STMT> homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)",if stat.S_ISDIR(int(response.st_mode)):
"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: <IF_STMT> x = np.add(x, self.bias[spk]) if self.norm_vars: x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",if self.norm_means:
"def hasFixtures(self, ctx_callback=None): context = self.context if context is None: return False if self.implementsAnyFixture(context, ctx_callback=ctx_callback): return True factory = self.factory if factory: ancestors = factory.context.get(self, []) for ancestor in ancestors: <IF_STMT> return True return False","if self.implementsAnyFixture(ancestor, ctx_callback=ctx_callback):"
def UpdateControlState(self): active = self.demoModules.GetActiveID() for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] if moduleID == active: btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) <IF_STMT> self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),if moduleID == modModified:
"def ignore_proxy_host(self): """"""Check if self.host is in the $no_proxy ignore list."""""" if urllib.proxy_bypass(self.host): return True no_proxy = os.environ.get('no_proxy') if no_proxy: entries = [parse_host_port(x) for x in no_proxy.split(',')] for host, port in entries: <IF_STMT> return True return False",if host.lower() == self.host and port == self.port:
"def run(self, _): view = self.view if not view.settings().get('terminus_view'): return terminal = Terminal.from_id(view.id()) if terminal: terminal.close() panel_name = terminal.panel_name <IF_STMT> window = panel_window(view) if window: window.destroy_output_panel(panel_name) else: view.close()",if panel_name:
"def get_docname_for_node(self, node: Node) -> str: while node: <IF_STMT> return self.env.path2doc(node['source']) elif isinstance(node, addnodes.start_of_file): return node['docname'] else: node = node.parent return None","if isinstance(node, nodes.document):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.add_version(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _maybe_female(self, path_elements, female, strict): if female: if self.has_gender_differences: elements = path_elements + ['female'] try: return self._get_file(elements, '.png', strict=strict) except ValueError: <IF_STMT> raise elif strict: raise ValueError('Pokemon %s has no gender differences' % self.species_id) return self._get_file(path_elements, '.png', strict=strict)",if strict:
"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() elif event.GetKeyCode() == wx.WXK_RETURN: self._update_value() <IF_STMT> self.SetValue('') if event.GetKeyCode() != wx.WXK_RETURN: event.Skip()",elif event.GetKeyCode() == wx.WXK_DELETE:
"def sync_up_to_new_location(self, worker_ip): if worker_ip != self.worker_ip: logger.debug('Setting new worker IP to %s', worker_ip) self.set_worker_ip(worker_ip) self.reset() <IF_STMT> logger.warning('Sync up to new location skipped. This should not occur.') else: logger.warning('Sync attempted to same IP %s.', worker_ip)",if not self.sync_up():
"def _get_download_link(self, url, download_type='torrent'): links = {'torrent': '', 'magnet': ''} try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find('div', {'class': 'download'}) <IF_STMT> for download in downloads.findAll('a'): link = download['href'] if link.startswith('magnet'): links['magnet'] = link else: links['torrent'] = urljoin(self.urls['base_url'], link) except Exception: pass return links[download_type]",if downloads:
"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): if '::1' in line: newline = re.sub('\\slocalhost\\s', ' ', line) <IF_STMT> logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), 'w') for line in lines: f.write(line) f.close()",if line != newline:
"def prepare(self): if hasattr(self, 'prepared') and (not self.prepared): self.data = SafeUnpickler.loads(self.data) <IF_STMT> self.data['instance_id'] = self.instance_id self.prepared = True","if hasattr(self, 'instance_id'):"
"def _test_compute_q0(self): sigma = 15 order = 250 logqs = np.arange(-290, -270, 1) count = 0 for logq in logqs: count += 1 sys.stdout.write('\t%0.5g: %0.10g' % (logq, pate.rdp_gaussian(logq, sigma, order))) sys.stdout.flush() <IF_STMT> print('')",if count % 5 == 0:
"def valid_fieldnames(fieldnames): """"""check if fieldnames are valid"""""" for fieldname in fieldnames: <IF_STMT> return True elif fieldname in fieldname_map and fieldname_map[fieldname] == 'source': return True return False",if fieldname in canonical_field_names and fieldname == 'source':
"def ns_provide(self, id_): global controllers, layouts if id_ == '_leo_viewrendered': c = self.c vr = controllers.get(c.hash()) or ViewRenderedController(c) h = c.hash() controllers[h] = vr <IF_STMT> layouts[h] = c.db.get('viewrendered_default_layouts', (None, None)) return vr",if not layouts.get(h):
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config <IF_STMT> defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if defaults is None:
"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): if k not in query: query[k] = {} <IF_STMT> query[k] = v else: query[k].update(v) return query","if isinstance(v, list):"
"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: <IF_STMT> data, passon = (data[:self.size], data[self.size:]) else: passon = b'' if data: self.data.append(data) return passon",if self.size:
"def updateVar(name, data, mode=None): if mode: if mode == 'append': core.config.globalVariables[name].append(data) <IF_STMT> core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",elif mode == 'add':
"def vi_pos_back_short(line, index=0, count=1): line = vi_list(line) try: for i in range(count): index -= 1 while vi_is_space(line[index]): index -= 1 in_word = vi_is_word(line[index]) <IF_STMT> while vi_is_word(line[index]): index -= 1 else: while not vi_is_word_or_space(line[index]): index -= 1 return index + 1 except IndexError: return 0",if in_word:
"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) <IF_STMT> for key, max_len in len_map.items(): example_len = example[key].shape if example_len > max_len: example[key] = np.resize(example[key], max_len) yield tuple(example)",if len_map is not None:
def decorate(f): f.__wrapped__ = Obj.__init__ f._uses_signature = Obj if Obj.__doc__: doclines = Obj.__doc__.splitlines() <IF_STMT> doc = f.__doc__ + '\n'.join(doclines[1:]) else: doc = '\n'.join(doclines) try: f.__doc__ = doc except AttributeError: pass return f,if f.__doc__:
"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in ('toplevel', 'detailed'): if self.counting != 'detailed': category = category.split('/')[0] <IF_STMT> self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",if category not in self.errors_by_category:
"def _delete_fields(self, data): data = self._del(data, ['speaker_ids', 'track_id', 'microlocation_id', 'session_type_id']) for _ in ['start_time_tz', 'end_time_tz']: <IF_STMT> data[_] = SESSION_POST[_[0:-3]].from_str(data[_]) data[_[0:-3]] = data.pop(_) return data",if _ in data:
"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = '' strings = [] for char in word: if char in char_set: letters += char count += 1 else: <IF_STMT> strings.append(letters) letters = '' count = 0 if count > threshold: strings.append(letters) return strings",if count > threshold:
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == '{': length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit <IF_STMT> break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",if token.ClosesScope():
"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break <IF_STMT> raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",elif s.startswith('Proc-Type') and 'ENCRYPTED' in s:
def main(self): self.model.clear() self.callman.unregister_all() active_handle = self.get_active('Person') if active_handle: active = self.dbstate.db.get_person_from_handle(active_handle) <IF_STMT> self.callman.register_obj(active) self.display_citations(active) else: self.set_has_data(False) else: self.set_has_data(False),if active:
"def _validate(self) -> None: super(Tuple, self)._validate() if len(self.elements) == 0: <IF_STMT> raise CSTValidationError('A zero-length tuple must be wrapped in parentheses.')",if len(self.lpar) == 0:
"def _session_from_arg(self, session_obj, lock_type=None): if not isinstance(session_obj, self.ISession): vm = self._machine_from_arg(session_obj) lock_type = lock_type or self.LockType.null <IF_STMT> return vm.create_session(lock_type) return None return session_obj",if vm:
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue <IF_STMT> if not private and name.startswith('_'): continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",if name != '__init__':
"def pdb(message=''): """"""Fall into pdb."""""" import pdb if app and (not app.useIpython): try: import PyQt5.QtCore as QtCore except ImportError: try: import PyQt4.QtCore as QtCore except ImportError: QtCore = None <IF_STMT> QtCore.pyqtRemoveInputHook() if message: print(message) pdb.set_trace()",if QtCore:
"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get('Logging'): <IF_STMT> if b['Name'] != b['Logging']['TargetBucket']: continue yield (b['Logging']['TargetBucket'], b['Logging']['TargetPrefix']) if not self_log and b['Name'].startswith('cf-templates-'): yield (b['Name'], '')",if self_log:
"def prepare_fields(self): for k, v in self.fields.items(): v._required = v.required v.required = False v.widget.is_required = False <IF_STMT> v._required = v.one_required v.one_required = False v.widget.enabled_locales = self.locales","if isinstance(v, I18nFormField):"
"def __pack__(self): new_values = [] for i in xrange(len(self.__unpacked_data_elms__)): for key in self.__keys__[i]: new_val = getattr(self, key) old_val = self.__unpacked_data_elms__[i] <IF_STMT> break new_values.append(new_val) return struct.pack(self.__format__, *new_values)",if new_val != old_val:
"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join(constant.profile['APPDATA'], u'..', u'Local', u'Microsoft', u'Vault') <IF_STMT> for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault(os.path.join(main_vault_directory, vault_directory)) if cred: pwd_found.append(cred) return pwd_found",if os.path.exists(main_vault_directory):
"def on_revision_plugin_revision_pre_save(**kwargs): instance = kwargs['instance'] if kwargs.get('created', False): update_previous_revision = not instance.previous_revision and instance.plugin and instance.plugin.current_revision and (instance.plugin.current_revision != instance) <IF_STMT> instance.previous_revision = instance.plugin.current_revision if not instance.revision_number: try: previous_revision = instance.plugin.revision_set.latest() instance.revision_number = previous_revision.revision_number + 1 except RevisionPluginRevision.DoesNotExist: instance.revision_number = 1",if update_previous_revision:
"def __setattr__(self, name, value): super().__setattr__(name, value) field = self._fields.get(name) if field: self.check_field_type(field, value) <IF_STMT> raise TypeError(f'cannot set immutable {name} on {self!r}')",if name in self.__ast_frozen_fields__:
"def _check_for_req_data(data): required_args = ['columns'] for arg in required_args: <IF_STMT> return (True, make_json_response(status=400, success=0, errormsg=gettext('Could not find required parameter ({}).').format(arg))) return (False, '')","if arg not in data or (isinstance(data[arg], list) and len(data[arg]) < 1):"
"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) for p, _ in ctr.most_common(): w, pos, l = p <IF_STMT> self.composite_dict[w, pos] = l if w not in self.word_dict: self.word_dict[w] = l return","if (w, pos) not in self.composite_dict:"
"def render(type_, obj, context): if type_ == 'foreign_key': return None if type_ == 'column': if obj.name == 'y': return None <IF_STMT> return False else: return 'col(%s)' % obj.name if type_ == 'type' and isinstance(obj, MySpecialType): context.imports.add('from mypackage import MySpecialType') return 'MySpecialType()' return 'render:%s' % type_",elif obj.name == 'q':
"def test_knows_when_stepping_back_possible(self): iterator = bidirectional_iterator.BidirectionalIterator([0, 1, 2, 3]) commands = [0, 1, 0, 0, 1, 1, 0, 0, 0, 0] command_count = 0 results = [] for _ in iterator: <IF_STMT> iterator.step_back_on_next_iteration() results.append(iterator.can_step_back()) command_count += 1 assert results == [False, True, False, True, True, True, False, True, True, True]",if commands[command_count]:
"def flask_debug_true(context): if context.is_module_imported_like('flask'): if context.call_function_name_qual.endswith('.run'): <IF_STMT> return bandit.Issue(severity=bandit.HIGH, confidence=bandit.MEDIUM, text='A Flask app appears to be run with debug=True, which exposes the Werkzeug debugger and allows the execution of arbitrary code.', lineno=context.get_lineno_for_call_arg('debug'))","if context.check_call_arg_value('debug', 'True'):"
"def __exit__(self, exc_type, exc_val, exc_tb): if self._should_meta_profile: end_time = timezone.now() exception_raised = exc_type is not None if exception_raised: Logger.error('Exception when performing meta profiling, dumping trace below') traceback.print_exception(exc_type, exc_val, exc_tb) request = getattr(DataCollector().local, 'request', None) <IF_STMT> curr = request.meta_time or 0 request.meta_time = curr + _time_taken(self.start_time, end_time)",if request:
"def get_job_offer(ja_list): ja_joff_map = {} offers = frappe.get_all('Job Offer', filters=[['job_applicant', 'IN', ja_list]], fields=['name', 'job_applicant', 'status', 'offer_date', 'designation']) for offer in offers: <IF_STMT> ja_joff_map[offer.job_applicant] = [offer] else: ja_joff_map[offer.job_applicant].append(offer) return ja_joff_map",if offer.job_applicant not in ja_joff_map.keys():
"def _get_deepest(self, t): if isinstance(t, list): <IF_STMT> return t[0] else: for part in t: res = self._get_deepest(part) if res: return res return None return None",if len(t) == 1:
"def test_main(self): root = os.path.dirname(mutagen.__path__[0]) skip = [os.path.join(root, 'docs'), os.path.join(root, 'venv')] for dirpath, dirnames, filenames in os.walk(root): <IF_STMT> continue for filename in filenames: if filename.endswith('.py'): path = os.path.join(dirpath, filename) self._check_encoding(path)",if any((dirpath.startswith(s + os.sep) or s == dirpath for s in skip)):
"def xview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.hsb.get() elif mode == 'moveto': frameWidth = self.innerframe.winfo_reqwidth() self._startX = value * float(frameWidth) else: clipperWidth = self._clipper.winfo_width() <IF_STMT> jump = int(clipperWidth * self._jfraction) else: jump = clipperWidth self._startX = self._startX + value * jump self.reposition()",if units == 'units':
"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model(DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config='data/test_config/max_hist_config.yml', kwargs={}) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: <IF_STMT> if type(policy) == FormPolicy: assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5","if hasattr(policy.featurizer, 'max_history'):"
"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base <IF_STMT> compare_entry = compare_entry.lower() compare_base = compare_base.lower() if self.compare_entries(compare_entry, compare_base): if entry not in sugg: sugg.append(entry) return sugg",if self.settings.get(IGNORE_CASE_SETTING):
"def marker_expr(remaining): if remaining and remaining[0] == '(': result, remaining = marker(remaining[1:].lstrip()) <IF_STMT> raise SyntaxError('unterminated parenthesis: %s' % remaining) remaining = remaining[1:].lstrip() else: lhs, remaining = marker_var(remaining) while remaining: m = MARKER_OP.match(remaining) if not m: break op = m.groups()[0] remaining = remaining[m.end():] rhs, remaining = marker_var(remaining) lhs = {'op': op, 'lhs': lhs, 'rhs': rhs} result = lhs return (result, remaining)",if remaining[0] != ')':
"def __repr__(self): """"""Dump the class data in the format of a .netrc file."""""" rep = '' for host in self.hosts.keys(): attrs = self.hosts[host] rep = rep + 'machine ' + host + '\n\tlogin ' + repr(attrs[0]) + '\n' <IF_STMT> rep = rep + 'account ' + repr(attrs[1]) rep = rep + '\tpassword ' + repr(attrs[2]) + '\n' for macro in self.macros.keys(): rep = rep + 'macdef ' + macro + '\n' for line in self.macros[macro]: rep = rep + line rep = rep + '\n' return rep",if attrs[1]:
"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, 'id') controls_ids = required_key(item, 'controls') <IF_STMT> if controls_ids != 'all': msg = 'Policy {id_} contains invalid controls list {controls}.'.format(id_=id_, controls=str(controls_ids)) raise ValueError(msg) self.policies[id_] = controls_ids","if not isinstance(controls_ids, list):"
"def __set__(self, obj, value): if value is not None and self.field._currency_field.null and (not isinstance(value, MONEY_CLASSES + (Decimal,))): raise ValueError('Missing currency value') if isinstance(value, BaseExpression): <IF_STMT> value = self.prepare_value(obj, value.value) elif not isinstance(value, Func): validate_money_expression(obj, value) prepare_expression(value) else: value = self.prepare_value(obj, value) obj.__dict__[self.field.name] = value","if isinstance(value, Value):"
"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): is_list, property_type, is_strong = attributes[0:3] <IF_STMT> if not is_list: children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",if is_strong and property in self._properties:
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction <IF_STMT> start = 0",if start < 0:
def setup_displace(self): self.displace_mod = None self.displace_strength = 0.02 for mod in self.obj.modifiers: <IF_STMT> self.displace_mod = mod self.displace_strength = mod.strength if not self.displace_mod: bpy.ops.object.modifier_add(type='DISPLACE') self.displace_mod = self.obj.modifiers[-1] self.displace_mod.show_expanded = False self.displace_mod.strength = self.displace_strength self.displace_mod.show_render = False self.displace_mod.show_viewport = False,if mod.type == 'DISPLACE':
"def set_json_body(cls, request_builder): old_body = request_builder.info.pop('data', {}) if isinstance(old_body, abc.Mapping): body = request_builder.info.setdefault('json', {}) for path in old_body: <IF_STMT> cls._sequence_path_resolver(path, old_body[path], body) else: body[path] = old_body[path] else: request_builder.info.setdefault('json', old_body)","if isinstance(path, tuple):"
"def build(opt): dpath = os.path.join(opt['datapath'], 'DBLL') version = None if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def test_prefix_lm(self): num_tries = 100 original = 'This is a long test with lots of words to see if it works ok.' dataset = tf.data.Dataset.from_tensor_slices({'text': [original] * num_tries}) dataset = prep.prefix_lm(dataset) for data in test_utils.dataset_as_text(dataset): inputs = data['inputs'].replace('prefix: ', '') targets = data['targets'] reconstructed = ''.join(inputs) <IF_STMT> reconstructed += ' ' reconstructed += ''.join(targets) self.assertEqual(reconstructed, original)",if inputs:
"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): if c in legal_indent_chars: leading_ws.append(c) else: break <IF_STMT> self.indchar = c elif c != self.indchar: self.strict_err_or_warn('found mixing of tabs and spaces', inputstring, i) return ''.join(leading_ws)",if self.indchar is None:
"def __init__(self, text): self.mappings = {} self.attributes = collections.defaultdict(set) for stanza in _ParseTextProperties(text): processor_id, single_values, multiple_values = self._ParseStanza(stanza) if processor_id is None: continue <IF_STMT> logging.warn('Processor id %s seen twice in %s', processor_id, text) continue self.mappings[processor_id] = single_values for key, value in multiple_values.items(): self.attributes[key].add(value)",if processor_id in self.mappings:
def __iter__(self): for chunk in self.source: <IF_STMT> self.wait_counter = 0 yield chunk elif self.wait_counter < self.wait_cntr_max: self.wait_counter += 1 else: logger.warning('Data poller has been receiving no data for {} seconds.\nClosing data poller'.format(self.wait_cntr_max * self.poll_period)) break time.sleep(self.poll_period),if chunk is not None:
"def download(self, prefetch=False): while self.running: try: <IF_STMT> path, start, end = self.prefetch_queue.get(True, 1) else: path, start, end = self.download_queue.get(True, 1) self.download_data(path, start, end) if prefetch: self.prefetch_queue.task_done() else: self.download_queue.task_done() except Queue.Empty: pass",if prefetch:
"def process_messages(self, found_files, messages): for message in messages: <IF_STMT> message.to_absolute_path(self.config.workdir) else: message.to_relative_path(self.config.workdir) if self.config.blending: messages = blender.blend(messages) filepaths = found_files.iter_module_paths(abspath=False) return postfilter.filter_messages(filepaths, self.config.workdir, messages)",if self.config.absolute_paths:
"def set_indentation_params(self, ispythonsource, guess=1): if guess and ispythonsource: i = self.guess_indent() <IF_STMT> self.indentwidth = i if self.indentwidth != self.tabwidth: self.usetabs = 0 self.editwin.set_tabwidth(self.tabwidth)",if 2 <= i <= 8:
"def to_tree(self, tagname=None, value=None, namespace=None): namespace = getattr(self, 'namespace', namespace) if value is not None: <IF_STMT> tagname = '{%s}%s' % (namespace, tagname) el = Element(tagname) el.text = safe_string(value) return el",if namespace is not None:
"def execute(self, argv: List) -> bool: if not argv: print('ERROR: You must give at least one module to download.') return False for _arg in argv: result = module_server.search_module(_arg) CacheUpdater('hub_download', _arg).start() <IF_STMT> url = result[0]['url'] with log.ProgressBar('Download {}'.format(url)) as bar: for file, ds, ts in utils.download_with_progress(url): bar.update(float(ds) / ts) else: print('ERROR: Could not find a HubModule named {}'.format(_arg)) return True",if result:
"def visit_type_type(self, t: TypeType) -> ProperType: if isinstance(self.s, TypeType): typ = self.meet(t.item, self.s.item) <IF_STMT> typ = TypeType.make_normalized(typ, line=t.line) return typ elif isinstance(self.s, Instance) and self.s.type.fullname == 'builtins.type': return t elif isinstance(self.s, CallableType): return self.meet(t, self.s) else: return self.default(self.s)","if not isinstance(typ, NoneType):"
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.name()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:
"def get_icon(self): if self.icon is not None: <IF_STMT> try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass icon_name, extension = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() if theme.has_icon(icon_name): return theme.load_icon(icon_name, 24, 0)",if os.path.exists(self.icon):
"def setup_logger(): """"""Set up logger and add stdout handler"""""" logging.setLoggerClass(IPDLogger) logger = logging.getLogger('icloudpd') has_stdout_handler = False for handler in logger.handlers: <IF_STMT> has_stdout_handler = True if not has_stdout_handler: formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S') stdout_handler = logging.StreamHandler(stream=sys.stdout) stdout_handler.setFormatter(formatter) stdout_handler.name = 'stdoutLogger' logger.addHandler(stdout_handler) return logger",if handler.name == 'stdoutLogger':
"def process_extra_fields(self): if self.instance.pk is not None: if self.cleaned_data.get('initialize', None): self.instance.initialize() <IF_STMT> self.instance.update_from_templates()","if self.cleaned_data.get('update', None) or not self.instance.stores.count():"
"def testFunctions(self): from zim.formats.wiki import match_url, is_url for input, input_is_url, tail in self.examples: if input_is_url: <IF_STMT> self.assertEqual(match_url(input), input[:-len(tail)]) self.assertFalse(is_url(input)) else: self.assertEqual(match_url(input), input) self.assertTrue(is_url(input)) else: self.assertEqual(match_url(input), None) self.assertFalse(is_url(input))",if tail:
"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings['room']['name'] if 'room' in settings else None file_ = settings['file'] if 'file' in settings else None <IF_STMT> if 'joined' in settings['event']: self._client.userlist.addUser(username, room, file_) elif 'left' in settings['event']: self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",if 'event' in settings:
"def restoreTerminals(self, state): for name in list(self.terminals.keys()): <IF_STMT> self.removeTerminal(name) for name, opts in state.items(): if name in self.terminals: term = self[name] term.setOpts(**opts) continue try: opts = strDict(opts) self.addTerminal(name, **opts) except: printExc('Error restoring terminal %s (%s):' % (str(name), str(opts)))",if name not in state:
"def htmlify(path, text): fname = os.path.basename(path) if any((fnmatch.fnmatchcase(fname, p) for p in _patterns)): sql = 'SELECT files.id FROM files WHERE path = ? LIMIT 1' row = _conn.execute(sql, (path,)).fetchone() <IF_STMT> return ClangHtmlifier(_tree, _conn, path, text, row[0]) return None",if row:
"def autoformat_filter_conv2d(fsize, in_depth, out_depth): if isinstance(fsize, int): return [fsize, fsize, in_depth, out_depth] elif isinstance(fsize, (tuple, list, tf.TensorShape)): <IF_STMT> return [fsize[0], fsize[1], in_depth, out_depth] else: raise Exception('filter length error: ' + str(len(fsize)) + ', only a length of 2 is supported.') else: raise Exception('filter format error: ' + str(type(fsize)))",if len(fsize) == 2:
def _rle_encode(string): new = b'' count = 0 for cur in string: <IF_STMT> count += 1 else: if count: new += b'\x00' + bytes([count]) count = 0 new += bytes([cur]) return new,if not cur:
"def is_clean(self): acceptable_statuses = {'external', 'unversioned'} root = self._capture_output('status', '--quiet') for elem in root.findall('./target/entry'): status = elem.find('./wc-status') <IF_STMT> continue log.debug('Path %s is %s', elem.get('path'), status.get('item')) return False return True","if status.get('item', None) in acceptable_statuses:"
"def process(self, body, message): try: <IF_STMT> raise TypeError('Received an unexpected type ""%s"" for payload.' % type(body)) response = self._handler.pre_ack_process(body) self._dispatcher.dispatch(self._process_message, response) except: LOG.exception('%s failed to process message: %s', self.__class__.__name__, body) finally: message.ack()","if not isinstance(body, self._handler.message_type):"
"def page_file(self, page): try: page = self.notebook.get_page(page) <IF_STMT> return page.source else: return None except PageNotFoundError: return None","if hasattr(page, 'source') and isinstance(page.source, File):"
"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): <IF_STMT> pass elif silhouette <= best_silhouette: break best_silhouette = silhouette best_a = a best_k = k return (best_a, best_silhouette, best_k)",if best_silhouette is None:
"def _cancel_tasks_for_partitions(self, to_cancel_partitions): with self._lock: _LOGGER.debug('EventProcessor %r tries to cancel partitions %r', self._id, to_cancel_partitions) for partition_id in to_cancel_partitions: <IF_STMT> self._consumers[partition_id].stop = True _LOGGER.info('EventProcessor %r has cancelled partition %r', self._id, partition_id)",if partition_id in self._consumers:
"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): <IF_STMT> result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) if refine: solid = solid.removeSplitter() return solid",if result is None:
"def geli_detach(self, pool, clear=False): failed = 0 for ed in self.middleware.call_sync('datastore.query', 'storage.encrypteddisk', [('encrypted_volume', '=', pool['id'])]): dev = ed['encrypted_provider'] try: self.geli_detach_single(dev) except Exception as ee: self.logger.warn(str(ee)) failed += 1 <IF_STMT> try: self.geli_clear(dev) except Exception as e: self.logger.warn('Failed to clear %s: %s', dev, e) return failed",if clear:
def compute_lengths(batch_sizes): tmp_batch_sizes = np.copy(batch_sizes) lengths = [] while True: c = np.count_nonzero(tmp_batch_sizes > 0) <IF_STMT> break lengths.append(c) tmp_batch_sizes = np.array([b - 1 for b in tmp_batch_sizes]) return np.array(lengths),if c == 0:
"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: <IF_STMT> flatten_items.append(b'') elif isinstance(item, bytes): flatten_items.append(item) elif isinstance(item, int): flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b'\n'.join(flatten_items)",if item is None:
"def update(self, new_config): jsonschema.validate(new_config, self.schema) config = {} for k, v in new_config.items(): <IF_STMT> config[k] = self[k] else: config[k] = v self._config = config self.changed()","if k in self.schema.get('secret', []) and v == SECRET_PLACEHOLDER:"
"def _encode_numpy(values, uniques=None, encode=False, check_unknown=True): if uniques is None: if encode: uniques, encoded = np.unique(values, return_inverse=True) return (uniques, encoded) else: return np.unique(values) if encode: <IF_STMT> diff = _encode_check_unknown(values, uniques) if diff: raise ValueError('y contains previously unseen labels: %s' % str(diff)) encoded = np.searchsorted(uniques, values) return (uniques, encoded) else: return uniques",if check_unknown:
"def restore_dtype_and_merge(arr, input_dtype): if isinstance(arr, list): arr = [restore_dtype_and_merge(arr_i, input_dtype) for arr_i in arr] shapes = [arr_i.shape for arr_i in arr] <IF_STMT> arr = np.array(arr) if ia.is_np_array(arr): arr = iadt.restore_dtypes_(arr, input_dtype) return arr",if len(set(shapes)) == 1:
"def proc_minute(d): if expanded[0][0] != '*': diff_min = nearest_diff_method(d.minute, expanded[0], 60) if diff_min is not None and diff_min != 0: <IF_STMT> d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return (True, d) return (False, d)",if is_prev:
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): <IF_STMT> self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","if isinstance(v, dict):"
"def __createItemAttribute(self, item, function, preload): """"""Create the new widget, add it, and remove the old one"""""" try: self.__stack.addWidget(function(item, preload)) <IF_STMT> oldWidget = self.__stack.widget(0) self.__stack.removeWidget(oldWidget) oldWidget.setParent(QtWidgets.QWidget()) except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if self.__stack.count() > 1:
"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: <IF_STMT> url = url[8:] if not url.startswith('http://'): url = 'http://' + url if playlist: download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if url.startswith('https://'):
"def add_enc_zero(obj, enc_zero): if isinstance(obj, np.ndarray): return obj + enc_zero elif isinstance(obj, Iterable): return type(obj)((EncryptModeCalculator.add_enc_zero(o, enc_zero) <IF_STMT> else o + enc_zero for o in obj)) else: return obj + enc_zero","if isinstance(o, Iterable)"
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[w, pos] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred <IF_STMT> lemma = w lemmas.append(lemma) return lemmas",if lemma is None:
"def replace_to_6hex(color): """"""Validate and replace 3hex colors to 6hex ones."""""" if match('^#(?:[0-9a-fA-F]{3}){1,2}$', color): <IF_STMT> color = '#{0}{0}{1}{1}{2}{2}'.format(color[1], color[2], color[3]) return color else: exit(_('Invalid color {}').format(color))",if len(color) == 4:
"def computeMachineName(self): """"""Return the name of the current machine, i.e, HOSTNAME."""""" try: import os name = os.getenv('HOSTNAME') <IF_STMT> name = os.getenv('COMPUTERNAME') if not name: import socket name = socket.gethostname() except Exception: name = '' return name",if not name:
"def _git_dirty_working_directory(q, include_untracked): try: cmd = ['git', 'status', '--porcelain'] if include_untracked: cmd += ['--untracked-files=normal'] else: cmd += ['--untracked-files=no'] status = _run_git_cmd(cmd) <IF_STMT> q.put(bool(status)) else: q.put(None) except (subprocess.CalledProcessError, OSError, FileNotFoundError): q.put(None)",if status is not None:
"def runAndWaitWork(server, work): work.touch() thr = threading.Thread(target=workThread, args=(server, work)) thr.setDaemon(True) thr.start() while True: if work.isTimedOut(): break if not thr.isAlive(): break <IF_STMT> break time.sleep(2)",if sys.stdin.closed:
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) if ignore_timeouts and is_timeout(e): return [] <IF_STMT> return [] raise",if ignore_non_errors and is_noerr(e):
"def PrintHeader(self): if self.draw == False: return for val in self.parent.header: self.SetPrintFont(val['Font']) header_indent = val['Indent'] * self.pwidth text = val['Text'] htype = val['Type'] <IF_STMT> addtext = self.GetDate() elif htype == 'Date & Time': addtext = self.GetDateTime() else: addtext = '' self.OutTextPageWidth(text + addtext, self.pheader_margin, val['Align'], header_indent, True)",if htype == 'Date':
"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): if result is None: result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) <IF_STMT> solid = solid.removeSplitter() return solid",if refine:
"def captured_updateNode(self, context): if not self.updating_name_from_pointer: font_datablock = self.get_bpy_data_from_name(self.fontname, bpy.data.fonts) <IF_STMT> self.font_pointer = font_datablock updateNode(self, context)",if font_datablock:
"def __add__(self, other): if isinstance(other, Vector2): <IF_STMT> _class = Vector2 else: _class = Point2 return _class(self.x + other.x, self.y + other.y) else: assert hasattr(other, '__len__') and len(other) == 2 return Vector2(self.x + other[0], self.y + other[1])",if self.__class__ is other.__class__:
"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: <IF_STMT> setting_values.update(self._flatten_settings_from_form(settings, field, form_values[field._name])) elif field._name in settings: setting_values[field._name] = form_values[field._name] return setting_values","if isinstance(field, _ContainerMixin):"
"def add_include_dirs(self, args): ids = [] for a in args: if hasattr(a, 'includedirs'): a = a.includedirs <IF_STMT> raise InvalidArguments('Include directory to be added is not an include directory object.') ids.append(a) self.include_dirs += ids","if not isinstance(a, IncludeDirs):"
"def _clip_array(array, config): if 'threshold' in config.keys(): threshold = config['threshold'] else: abs_array = np.max(np.abs(array)) <IF_STMT> return array threshold = np.percentile(np.abs(array), 99.99) return np.clip(array, -threshold, threshold)",if abs_array < 1.0:
def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: <IF_STMT> yield from dfs(w) elif w not in identified: while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v]:]) del stack[index[v]:] identified.update(scc) yield scc,if w not in index:
"def create_balancer(self, name, members, protocol='http', port=80, algorithm=DEFAULT_ALGORITHM): balancer = self.ex_create_balancer_nowait(name, members, protocol, port, algorithm) timeout = 60 * 20 waittime = 0 interval = 2 * 15 if balancer.id is not None: return balancer else: while waittime < timeout: balancers = self.list_balancers() for i in balancers: <IF_STMT> return i waittime += interval time.sleep(interval) raise Exception('Failed to get id')",if i.name == balancer.name and i.id is not None:
"def handle(self, scope: Scope, receive: Receive, send: Send) -> None: if self.methods and scope['method'] not in self.methods: <IF_STMT> raise HTTPException(status_code=405) else: response = PlainTextResponse('Method Not Allowed', status_code=405) await response(scope, receive, send) else: await self.app(scope, receive, send)",if 'app' in scope:
"def convert(data): result = [] for d in data: if isinstance(d, tuple) and len(d) == 2: result.append((d[0], None, d[1])) <IF_STMT> result.append(d) return result","elif isinstance(d, basestring):"
"def register_adapters(): global adapters_registered if adapters_registered is True: return try: import pkg_resources packageDir = pkg_resources.resource_filename('pyamf', 'adapters') except: packageDir = os.path.dirname(__file__) for f in glob.glob(os.path.join(packageDir, '*.py')): mod = os.path.basename(f).split(os.path.extsep, 1)[0] <IF_STMT> continue try: register_adapter(mod[1:].replace('_', '.'), PackageImporter(mod)) except ImportError: pass adapters_registered = True",if mod == '__init__' or not mod.startswith('_'):
"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue if hasattr(module, 'aliases'): for alias in module.aliases(): <IF_STMT> modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()",if alias not in excluded_aliases:
"def clean_items(event, items, variations): for item in items: <IF_STMT> raise ValidationError(_('One or more items do not belong to this event.')) if item.has_variations: if not any((var.item == item for var in variations)): raise ValidationError(_('One or more items has variations but none of these are in the variations list.'))",if event != item.event:
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: <IF_STMT> return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",if idx == num:
"def check(chip, xeddb, chipdb): all_inst = [] undoc = [] for inst in xeddb.recs: <IF_STMT> if inst.undocumented: undoc.append(inst) else: all_inst.append(inst) return (all_inst, undoc)",if inst.isa_set in chipdb[chip]:
"def get_all_topic_src_files(self): """"""Retrieves the file paths of all the topics in directory"""""" topic_full_paths = [] topic_names = os.listdir(self.topic_dir) for topic_name in topic_names: <IF_STMT> topic_full_path = os.path.join(self.topic_dir, topic_name) if topic_full_path != self.index_file: topic_full_paths.append(topic_full_path) return topic_full_paths",if not topic_name.startswith('.'):
"def _get_element(dom_msi, tag_name, name=None, id_=None): """"""Get a xml element defined on Product."""""" product = dom_msi.getElementsByTagName('Product')[0] elements = product.getElementsByTagName(tag_name) for element in elements: <IF_STMT> if element.getAttribute('Name') == name and element.getAttribute('Id') == id_: return element elif id_: if element.getAttribute('Id') == id_: return element",if name and id_:
"def __init__(self, *models): super().__init__() self.models = ModuleList(models) for m in models: <IF_STMT> raise ValueError('IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)') self.likelihood = LikelihoodList(*[m.likelihood for m in models])","if not hasattr(m, 'likelihood'):"
"def _sniff(filename, oxlitype): try: with open(filename, 'rb') as fileobj: header = fileobj.read(4) <IF_STMT> fileobj.read(1) ftype = fileobj.read(1) if binascii.hexlify(ftype) == oxlitype: return True return False except OSError: return False",if header == b'OXLI':
"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) if '/' not in key: key += '/tcp' <IF_STMT> result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result","if isinstance(v, list):"
"def input_data(self): gen = self.config.generator if gen and (not self.config['out'] or not self.config['in']): <IF_STMT> self._run_generator(gen, args=self.config.generator_args) if self._generated[0]: return self._generated[0] return self._normalize(self.problem.problem_data[self.config['in']]) if self.config['in'] else b''",if self._generated is None:
"def __new__(cls, *tasks, **kwargs): if not kwargs and tasks: <IF_STMT> tasks = tasks[0] if len(tasks) == 1 else tasks return reduce(operator.or_, tasks) return super(chain, cls).__new__(cls, *tasks, **kwargs)",if len(tasks) != 1 or is_list(tasks[0]):
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists('file_sources.json'): file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) <IF_STMT> ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if file_sources is None:
"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) <IF_STMT> self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE if c == curr: self._colourSelection = i",if c.IsOk():
"def convert_obj_into_marshallable(self, obj): if isinstance(obj, self.marshalable_types): return obj if isinstance(obj, array.array): if obj.typecode == 'c': return obj.tostring() <IF_STMT> return obj.tounicode() return obj.tolist() return self.class_to_dict(obj)",if obj.typecode == 'u':
"def run(self): self.run_command('egg_info') from glob import glob for pattern in self.match: pattern = self.distribution.get_name() + '*' + pattern files = glob(os.path.join(self.dist_dir, pattern)) files = [(os.path.getmtime(f), f) for f in files] files.sort() files.reverse() log.info('%d file(s) matching %s', len(files), pattern) files = files[self.keep:] for t, f in files: log.info('Deleting %s', f) <IF_STMT> os.unlink(f)",if not self.dry_run:
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: if token.token_type == TOKEN_TEXT: result.append(token.contents.replace('%', '%%')) <IF_STMT> result.append('%%(%s)s' % token.contents) vars.append(token.contents) return (''.join(result), vars)",elif token.token_type == TOKEN_VAR:
"def _handle_raise(self, values, is_NAs, origins): for is_NA, origin in zip(is_NAs, origins): <IF_STMT> msg = 'Missing values detected. If you want rows with missing values to be automatically deleted in a list-wise manner (not recommended), please set dropna=True in the Bambi Model initialization.' raise PatsyError(msg, origin) return values",if np.any(is_NA):
"def add_node_data(node_array, ntwk): node_ntwk = nx.Graph() newdata = {} for idx, data in ntwk.nodes(data=True): <IF_STMT> newdata['value'] = node_array[int(idx) - 1] data.update(newdata) node_ntwk.add_node(int(idx), **data) return node_ntwk",if not int(idx) == 0:
"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: if ';' in date_hdr: date_hdr = date_hdr.split(';')[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) <IF_STMT> return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",if msg_ts > time.time() + 24 * 3600 or msg_ts < 1:
"def _route_db(self, model, **hints): chosen_db = None for router in self.routers: try: method = getattr(router, action) except AttributeError: pass else: chosen_db = method(model, **hints) <IF_STMT> return chosen_db try: return hints['instance']._state.db or DEFAULT_DB_ALIAS except KeyError: return DEFAULT_DB_ALIAS",if chosen_db:
"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): if not ignore_first_level: keys = [x.split('(')[0] for x in struct.keys()] res.extend(keys) for key in struct: <IF_STMT> logging.debug('Ignored: %s: %s', key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",if key in IGNORED_KEYS:
"def launch_app(self, fs_id): if fs_id in self.app_infos: row = self.get_row_by_fsid(fs_id) <IF_STMT> return app_info = self.app_infos[fs_id] filepath = os.path.join(row[SAVEDIR_COL], row[SAVENAME_COL]) gfile = Gio.File.new_for_path(filepath) app_info.launch([gfile], None) self.app_infos.pop(fs_id, None)",if not row:
"def create_skipfile(files_changed, skipfile): json_pattern = re.compile('^\\{.*\\}') for line in files_changed.readlines(): <IF_STMT> for filename in json.loads(line): if '/COMMIT_MSG' in filename: continue skipfile.write('+*/%s\n' % filename) skipfile.write('-*\n')","if re.match(json_pattern, line):"
"def zscore(self, client, request, N): check_input(request, N != 2) key = request[1] db = client.db value = db.get(key) if value is None: client.reply_bulk(None) elif not isinstance(value, self.zset_type): client.reply_wrongtype() else: score = value.score(request[2], None) <IF_STMT> score = str(score).encode('utf-8') client.reply_bulk(score)",if score is not None:
"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) elif isinstance(test, unittest.TestCase): <IF_STMT> print(test.id())",if support.match_test(test):
"def Run(self): """"""The main run method of the client."""""" for thread in self._threads.values(): thread.start() logging.info(START_STRING) while True: dead_threads = [tn for tn, t in self._threads.items() if not t.isAlive()] <IF_STMT> raise FatalError('These threads are dead: %r. Shutting down...' % dead_threads) time.sleep(10)",if dead_threads:
"def _slice_queryset(queryset, order_by, per_page, start): page_len = int(per_page) + 1 if start: <IF_STMT> filter_name = '%s__lte' % order_by[1:] else: filter_name = '%s__gte' % order_by return queryset.filter(**{filter_name: start})[:page_len] return queryset[:page_len]",if order_by.startswith('-'):
"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 <IF_STMT> break else: dt = t2 - previous if dt <= 0.0: continue if precision is not None: precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",if 0 < dt:
"def findWorkingDir(): frozen = getattr(sys, 'frozen', '') if not frozen: path = os.path.dirname(__file__) elif frozen in ('dll', 'console_exe', 'windows_exe', 'macosx_app'): path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))) elif frozen: <IF_STMT> path = getattr(sys, '_MEIPASS', '') else: path = os.path.dirname(sys.executable) else: path = '' return path","if getattr(sys, '_MEIPASS', '') is not None:"
"def CreateDataType(vmodlName, wsdlName, parent, version, props): with _lazyLock: dic = [vmodlName, wsdlName, parent, version, props] names = vmodlName.split('.') <IF_STMT> vmodlName = '.'.join((name[0].lower() + name[1:] for name in names)) _AddToDependencyMap(names) typeNs = GetWsdlNamespace(version) _dataDefMap[vmodlName] = dic _wsdlDefMap[typeNs, wsdlName] = dic _wsdlTypeMapNSs.add(typeNs)",if _allowCapitalizedNames:
"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f'Unexpected response type: `{type(response)}`') if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) <IF_STMT> yield rdf_client.User(username=username, homedir=homedir)",if username not in self._ignore_users:
def process_question(qtxt): question = '' skip = False for letter in qtxt: if letter == '<': skip = True if letter == '>': skip = False if skip: continue <IF_STMT> if letter == ' ': letter = '_' question += letter.lower() return question,if letter.isalnum() or letter == ' ':
"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: if gap: self.write('') self.process(line) <IF_STMT> gap = True return 0",if not is_command(line):
"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() if time_now - record['update'] > self.ttl: record = None except KeyError: record = None <IF_STMT> record = {'r': 'unknown', 'dns': {}, 'g': 1, 'query_count': 0} return record",if not record:
"def gen_constant_folding(cw): types = ['Int32', 'Double', 'BigInteger', 'Complex'] for cur_type in types: cw.enter_block('if (constLeft.Value.GetType() == typeof(%s))' % (cur_type,)) cw.enter_block('switch (_op)') for op in ops: gen = getattr(op, 'genConstantFolding', None) <IF_STMT> gen(cw, cur_type) cw.exit_block() cw.exit_block()",if gen is not None:
"def unreferenced_dummy(self): for g, base in zip(self.evgroups, self.evbases): for ind, j in enumerate(g): <IF_STMT> debug_print('replacing unreferenced %d %s with dummy' % (base + ind, g[ind])) g[ind] = 'dummy' self.evnum[base + ind] = 'dummy'",if not self.indexobj[base + ind]:
"def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]: for cls in self.__class__.__mro__: <IF_STMT> warnings.warn('PyDecoratorMixin is deprecated. Please check the implementation of %s' % cls, RemovedInSphinx50Warning, stacklevel=2) break else: warnings.warn('PyDecoratorMixin is deprecated', RemovedInSphinx50Warning, stacklevel=2) ret = super().handle_signature(sig, signode) signode.insert(0, addnodes.desc_addname('@', '@')) return ret",if cls.__name__ != 'DirectiveAdapter':
"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get('next', {}).get('url', None) <IF_STMT> break n += 1 if n > max_next: vd.warning(f'stopping at max {max_next} pages') break vd.status(f'fetching next page from {src}') response = requests.get(src, stream=True)",if not src:
"def ordered_indices(self): with data_utils.numpy_seed(self.seed, self.epoch): indices = [np.random.permutation(len(dataset)) for dataset in self.datasets.values()] counters = [0 for _ in self.datasets] sampled_indices = [self._sample(indices, counters) for _ in range(self.total_num_instances)] <IF_STMT> sampled_indices.sort(key=lambda i: self.num_tokens(i)) return np.array(sampled_indices, dtype=np.int64)",if self.sort_indices:
"def _build_columns(self): self.columns = [Column() for col in self.keys] for row in self: for col_idx, col_val in enumerate(row): col = self.columns[col_idx] col.append(col_val) <IF_STMT> col.is_quantity = False for idx, key_name in enumerate(self.keys): self.columns[idx].name = key_name self.x = Column() self.ys = []",if col_val is not None and (not is_quantity(col_val)):
"def tearDown(self): subprocess_list = self.subprocess_list processes = subprocess_list.processes self.schedule.reset() del self.schedule for proc in processes: <IF_STMT> terminate_process(proc.pid, kill_children=True, slow_stop=True) subprocess_list.cleanup() processes = subprocess_list.processes if processes: for proc in processes: if proc.is_alive(): terminate_process(proc.pid, kill_children=True, slow_stop=False) subprocess_list.cleanup() processes = subprocess_list.processes if processes: log.warning('Processes left running: %s', processes)",if proc.is_alive():
"def colorNetwork(cls, network, nodesInNetwork, nodeByID=None): for node in nodesInNetwork: node.use_custom_color = True neededCopies = sum((socket.execution.neededCopies for socket in node.outputs)) <IF_STMT> color = (0.7, 0.9, 0.7) else: color = (1.0, 0.3, 0.3) node.color = color",if neededCopies == 0:
"def _init_warmup_scheduler(self, optimizer, states): updates_so_far = states.get('number_training_updates', 0) if self.warmup_updates > 0 and (updates_so_far <= self.warmup_updates or self.hard_reset): self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, self._warmup_lr) <IF_STMT> self.warmup_scheduler.load_state_dict(states['warmup_scheduler']) else: self.warmup_scheduler = None",if states.get('warmup_scheduler'):
"def inner(self, *iargs, **ikwargs): try: return getattr(super(VEXResilienceMixin, self), func)(*iargs, **ikwargs) except excs as e: for exc, handler in zip(excs, handlers): if isinstance(e, exc): v = getattr(self, handler)(*iargs, **ikwargs) <IF_STMT> raise return v assert False, 'this should be unreachable if Python is working correctly'",if v is raiseme:
"def unwrap_envelope(self, data, many): if many: if data['items']: <IF_STMT> self.context['total'] = len(data) return data else: self.context['total'] = data['total'] else: self.context['total'] = 0 data = {'items': []} return data['items'] return data","if isinstance(data, InstrumentedList) or isinstance(data, list):"
"def __subclasscheck__(self, cls): if self.__origin__ is not None: <IF_STMT> raise TypeError('Parameterized generics cannot be used with class or instance checks') return False if self is Generic: raise TypeError('Class %r cannot be used with class or instance checks' % self) return super().__subclasscheck__(cls)","if sys._getframe(1).f_globals['__name__'] not in ['abc', 'functools']:"
"def __init__(self, pyversions, coverage_service): build_matrix = '' for version in pyversions: build_matrix += '\n{},'.format(version <IF_STMT> else 'py{}'.format(''.join(version.split('.')))) coverage_package = '' if coverage_service: coverage_package += '\n{}'.format(coverage_service.package) coverage_package += '\n' super(Tox, self).__init__('tox.ini', TEMPLATE.format(build_matrix=build_matrix, coverage_package=coverage_package))",if version.startswith('pypy')
"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks except AttributeError: tasks = self.tasks if len(tasks): app = tasks[0]._app <IF_STMT> app = body._app return app if app is not None else current_app",if app is None and body is not None:
"def logic(): for v in [True, False, None, 0, True, None, None, 1]: yield clk.posedge xd.next = v <IF_STMT> yd.next = zd.next = None elif v: yd.next = zd.next = 11 else: yd.next = zd.next = 0",if v is None:
"def run(self): eid = self.start_episode() obs = self.env.reset() while True: <IF_STMT> action = self.env.action_space.sample() self.log_action(eid, obs, action) else: action = self.get_action(eid, obs) obs, reward, done, info = self.env.step(action) self.log_returns(eid, reward, info=info) if done: self.end_episode(eid, obs) obs = self.env.reset() eid = self.start_episode()",if random.random() < self.off_pol_frac:
"def tearDown(self): os.chdir(self.orig_working_dir) sys.argv = self.orig_argv sys.stdout = self.orig_stdout sys.stderr = self.orig_stderr for dirname in ['lv_LV', 'ja_JP']: locale_dir = os.path.join(self.datadir, 'project', 'i18n', dirname) <IF_STMT> shutil.rmtree(locale_dir)",if os.path.isdir(locale_dir):
"def sentry_set_scope(process_context, entity, project, email=None, url=None): with sentry_sdk.hub.GLOBAL_HUB.configure_scope() as scope: scope.set_tag('process_context', process_context) scope.set_tag('entity', entity) scope.set_tag('project', project) <IF_STMT> scope.user = {'email': email} if url: scope.set_tag('url', url)",if email:
"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) <IF_STMT> continue if c.getYAxis() == Y_AXIS: nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",if not c.isVisible():
"def handle_starttag(self, tag, attrs): if tag == 'link' and ('rel', 'icon') in attrs or ('rel', 'shortcut icon') in attrs: href = None icon_type = None for attr, value in attrs: if attr == 'href': href = value elif attr == 'type': icon_type = value <IF_STMT> try: mimetype = extension_to_mimetype(href.rpartition('.')[2]) except KeyError: pass else: icon_type = mimetype if icon_type: self.icons.append((href, icon_type))",if href:
"def get_version(version_file=STATIC_VERSION_FILE): version_info = get_static_version_info(version_file) version = version_info['version'] if version == '__use_git__': version = get_version_from_git() <IF_STMT> version = get_version_from_git_archive(version_info) if not version: version = Version('unknown', None, None) return pep440_format(version) else: return version",if not version:
"def _Sleep(self, seconds): if threading.current_thread() is not self._worker_thread: return self._original_sleep(seconds) self._time += seconds self._budget -= seconds while self._budget < 0: self._worker_thread_turn.clear() self._owner_thread_turn.set() self._worker_thread_turn.wait() <IF_STMT> raise FakeTimeline._WorkerThreadExit()",if self._worker_thread_done:
def validate_attributes(self): if not (self.has_variants or self.variant_of): return if not self.variant_based_on: self.variant_based_on = 'Item Attribute' if self.variant_based_on == 'Item Attribute': attributes = [] <IF_STMT> frappe.throw(_('Attribute table is mandatory')) for d in self.attributes: if d.attribute in attributes: frappe.throw(_('Attribute {0} selected multiple times in Attributes Table').format(d.attribute)) else: attributes.append(d.attribute),if not self.attributes:
"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get('Authorization'): credentails = parse_authorization_header(request.headers.get('Authorization')) if not credentails: return response_hash = response(credentails, passwd, dict(uri=request.script_root + request.path, body=request.data, method=request.method)) <IF_STMT> return True return False",if credentails.get('response') == response_hash:
"def _get_index_type(return_index_type, ctx): if return_index_type is None: if ctx.running_mode == RunningMode.local: return_index_type = 'object' <IF_STMT> return_index_type = 'filename' else: return_index_type = 'bytes' return return_index_type",elif ctx.running_mode == RunningMode.local_cluster:
"def iter_event_handlers(self, resource: resources_.Resource, event: bodies.RawEvent) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn('SimpleRegistry.iter_event_handlers() is deprecated; use ResourceWatchingRegistry.iter_handlers().', DeprecationWarning) cause = _create_watching_cause(resource, event) for handler in self._handlers: if not isinstance(handler, handlers.ResourceWatchingHandler): pass <IF_STMT> yield handler","elif registries.match(handler=handler, cause=cause, ignore_fields=True):"
"def subprocess_post_check(completed_process: subprocess.CompletedProcess, raise_error: bool=True) -> None: if completed_process.returncode: <IF_STMT> print(completed_process.stdout, file=sys.stdout, end='') if completed_process.stderr is not None: print(completed_process.stderr, file=sys.stderr, end='') if raise_error: raise PipxError(f""{' '.join([str(x) for x in completed_process.args])!r} failed"") else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process.stdout is not None:
"def __pow__(self, power): if power == 1: return self if power == -1: from cirq.devices import line_qubit decomposed = protocols.decompose_once_with_qubits(self, qubits=line_qubit.LineQid.for_gate(self), default=None) <IF_STMT> return NotImplemented inverse_decomposed = protocols.inverse(decomposed, None) if inverse_decomposed is None: return NotImplemented return _InverseCompositeGate(self) return NotImplemented",if decomposed is None:
"def tearDown(self): """"""Close the application after tests"""""" self.old_pos = self.dlg.rectangle self.dlg.menu_select('File->Exit') try: <IF_STMT> self.app.UntitledNotepad[""Do&n't Save""].click() self.app.UntitledNotepad.wait_not('visible') except Exception: pass finally: self.app.kill()","if self.app.UntitledNotepad[""Do&n't Save""].exists():"
"def terminate_subprocess(proc, timeout=0.1, log=None): <IF_STMT> if log: log.info('Sending SIGTERM to %r', proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info('Sending SIGKILL to %r', proc) proc.kill() return proc.returncode",if proc.poll() is None:
"def validate(self, detection, expectation): config = SigmaConfiguration() self.basic_rule['detection'] = detection with patch('yaml.safe_load_all', return_value=[self.basic_rule]): parser = SigmaCollectionParser('any sigma io', config, None) backend = SQLiteBackend(config, self.table) assert len(parser.parsers) == 1 for p in parser.parsers: <IF_STMT> self.assertEqual(expectation, backend.generate(p)) elif isinstance(expectation, Exception): self.assertRaises(type(expectation), backend.generate, p)","if isinstance(expectation, str):"
"def makelist(d): """"""Convert d into a list if all the keys of d are integers."""""" if isinstance(d, dict): <IF_STMT> return [makelist(d[k]) for k in sorted(d, key=int)] else: return web.storage(((k, makelist(v)) for k, v in d.items())) else: return d",if all((isint(k) for k in d)):
"def __share_local_dir(self, lpath, rpath, fast): result = const.ENoError for walk in self.__walk_normal_file(lpath): dirpath, dirnames, filenames = walk for filename in filenames: rpart = os.path.relpath(dirpath, lpath) if rpart == '.': rpart = '' subr = self.__share_local_file(joinpath(dirpath, filename), posixpath.join(rpath, rpart, filename), fast) <IF_STMT> result = subr return result",if subr != const.ENoError:
"def _targets(self, sigmaparser): targets = set() for condfield in self.conditions: <IF_STMT> rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: if condvalue in rulefieldvalues: targets.update(self.conditions[condfield][condvalue]) return targets",if condfield in sigmaparser.values:
"def _wrapped_view(request, *args, **kwargs): user = request.user if user.is_authenticated(): obj = _resolve_lookup(obj_lookup, kwargs) perm_obj = _resolve_lookup(perm_obj_lookup, kwargs) granted = access.has_perm_or_owns(user, perm, obj, perm_obj, owner_attr) <IF_STMT> return view_func(request, *args, **kwargs) return HttpResponseForbidden()",if granted or user.has_perm(perm):
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier['part'] earlier_step = earlier['step'] found = False for current in current_parts: <IF_STMT> found = True break if not found: cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",if earlier_part == current['part'] and earlier_step == current['step']:
"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: if self.named_windows[wnd_name] == 0: self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) <IF_STMT> self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print('show_image: named_window ', wnd_name, ' not found.')",if wnd_name in self.capture_mouse_windows:
"def readlines(self, hint=None): body = self._get_body() rest = body[self.position:] self.position = len(body) result = [] while 1: next = rest.find('\r\n') <IF_STMT> result.append(rest) break result.append(rest[:next + 2]) rest = rest[next + 2:] return result",if next == -1:
"def __lt__(self, other): olen = len(other) for i in range(olen): try: c = self[i] < other[i] except IndexError: return True if c: return c <IF_STMT> return False return len(self) < olen",elif other[i] < self[i]:
"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: if user and social.user != user: msg = 'This account is already in use.' raise AuthAlreadyAssociated(backend, msg) <IF_STMT> user = social.user return {'social': social, 'user': user, 'is_new': user is None, 'new_association': social is None}",elif not user:
"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: if self.utexc is False: self.utexc = np.zeros(ntexco, dtype=bool) <IF_STMT> self.utexc[indices] = True",if self.utexc is not True:
"def destination(self, type, name, arglist): classname = 'ResFunction' listname = 'functions' if arglist: t, n, m = arglist[0] <IF_STMT> classname = 'ResMethod' listname = 'resmethods' return (classname, listname)",if t == 'Handle' and m == 'InMode':
"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) if register: to_store.append(self.view.substr(self.view.full_line(r))) if register: text = ''.join(to_store) <IF_STMT> text = text + '\n' state = State(self.view) state.registers[register] = [text]",if not text.endswith('\n'):
"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue <IF_STMT> return b'' else: chunk = chunk[start - self._pos:] if stop is not None and self._pos > stop: chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",elif self._pos == start:
def start(self): self.on_config_change() self.start_config_watch() try: if self.config['MITMf']['DNS']['tcp'].lower() == 'on': self.startTCP() else: self.startUDP() except socket.error as e: <IF_STMT> shutdown('\n[DNS] Unable to start DNS server on port {}: port already in use'.format(self.config['MITMf']['DNS']['port'])),if 'Address already in use' in e:
"def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super(ParseElementEnhance, self).ignore(other) <IF_STMT> self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if self.expr is not None:
def test_relative_deploy_path_override(): s = Site(TEST_SITE_ROOT) s.load() res = s.content.resource_from_relative_path('blog/2010/december/merry-christmas.html') res.relative_deploy_path = 'blog/2010/december/happy-holidays.html' for page in s.content.walk_resources(): <IF_STMT> assert page.relative_deploy_path == 'blog/2010/december/happy-holidays.html' else: assert page.relative_deploy_path == Folder(page.relative_path),if res.source_file == page.source_file:
"def _parser(cls, buf): tlvs = [] while buf: tlv_type = LLDPBasicTLV.get_type(buf) tlv = cls._tlv_parsers[tlv_type](buf) tlvs.append(tlv) offset = LLDP_TLV_SIZE + tlv.len buf = buf[offset:] <IF_STMT> break assert len(buf) > 0 lldp_pkt = cls(tlvs) assert lldp_pkt._tlvs_len_valid() assert lldp_pkt._tlvs_valid() return (lldp_pkt, None, buf)",if tlv.tlv_type == LLDP_TLV_END:
"def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures): try: output = self.client.pull(repo, **pull_kwargs) if silent: with open(os.devnull, 'w') as devnull: yield from stream_output(output, devnull) else: yield from stream_output(output, sys.stdout) except (StreamOutputError, NotFound) as e: <IF_STMT> raise else: log.error(str(e))",if not ignore_pull_failures:
def _collect_bytecode(ordered_code): bytecode_blocks = [] stack = [ordered_code] while stack: code = stack.pop() bytecode_blocks.append(code.co_code) for const in code.co_consts: <IF_STMT> stack.append(const) return bytecode_blocks,"if isinstance(const, blocks.OrderedCode):"
"def displayhook(value): if value is None: return builtins = modules['builtins'] builtins._ = None text = repr(value) try: local_stdout = stdout except NameError as e: raise RuntimeError('lost sys.stdout') from e try: local_stdout.write(text) except UnicodeEncodeError: bytes = text.encode(local_stdout.encoding, 'backslashreplace') <IF_STMT> local_stdout.buffer.write(bytes) else: text = bytes.decode(local_stdout.encoding, 'strict') local_stdout.write(text) local_stdout.write('\n') builtins._ = value","if hasattr(local_stdout, 'buffer'):"
"def _analyze(self): lines = open(self.log_path, 'r').readlines() prev_line = None for line in lines: if line.startswith('ERROR:') and prev_line and prev_line.startswith('='): self.errors.append(line[len('ERROR:'):].strip()) <IF_STMT> self.failures.append(line[len('FAIL:'):].strip()) prev_line = line",elif line.startswith('FAIL:') and prev_line and prev_line.startswith('='):
"def _flush(self): if self._data: if self._last is not None: text = ''.join(self._data) <IF_STMT> assert self._last.tail is None, 'internal error (tail)' self._last.tail = text else: assert self._last.text is None, 'internal error (text)' self._last.text = text self._data = []",if self._tail:
"def write(self, chunk): consumer = self._current_consumer server_side = consumer.server_side if server_side: server_side.data_received(chunk) else: consumer.message += chunk assert consumer.in_parser.execute(chunk, len(chunk)) == len(chunk) <IF_STMT> consumer.finished()",if consumer.in_parser.is_message_complete():
"def _api_change_cat(name, output, kwargs): """"""API: accepts output, value(=nzo_id), value2(=category)"""""" value = kwargs.get('value') value2 = kwargs.get('value2') if value and value2: nzo_id = value cat = value2 <IF_STMT> cat = None result = sabnzbd.NzbQueue.change_cat(nzo_id, cat) return report(output, keyword='status', data=bool(result > 0)) else: return report(output, _MSG_NO_VALUE)",if cat == 'None':
"def get_allocated_address(self, config: ActorPoolConfig, allocated: allocated_type) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): <IF_STMT> occupied = True break if not occupied: return addr raise NoIdleSlot(f'No idle slot for creating actor with label {self.label}, mark {self.mark}')",if strategy == self:
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: <IF_STMT> with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if delete:
"def quick_load(tool_file, async_load=True): try: tool = self.load_tool(tool_file, tool_cache_data_dir) self.__add_tool(tool, load_panel_dict, elems) key = 'tool_%s' % str(tool.id) integrated_elems[key] = tool <IF_STMT> self._load_tool_panel() self._save_integrated_tool_panel() return tool.id except Exception: log.exception('Failed to load potential tool %s.', tool_file) return None",if async_load:
"def _get_default_ordering(self): try: ordering = super(DocumentChangeList, self)._get_default_ordering() except AttributeError: ordering = [] if self.model_admin.ordering: ordering = self.model_admin.ordering <IF_STMT> ordering = self.lookup_opts.ordering return ordering",elif self.lookup_opts.ordering:
"def names(self, persistent=None): u = set() result = [] for s in [self.__storage(None), self.__storage(self.__category)]: for b in s: <IF_STMT> continue if b.name.startswith('__'): continue if b.name not in u: result.append(b.name) u.add(b.name) return result",if persistent is not None and b.persistent != persistent:
"def common_check_get_messages_query(self, query_params: Dict[str, object], expected: str) -> None: user_profile = self.example_user('hamlet') request = POSTRequestMock(query_params, user_profile) with queries_captured() as queries: get_messages_backend(request, user_profile) for query in queries: <IF_STMT> sql = str(query['sql']).replace(' /* get_messages */', '') self.assertEqual(sql, expected) return raise AssertionError('get_messages query not found')",if '/* get_messages */' in query['sql']:
"def _activate_only_current_top_active(): for i in range(0, len(current_sequence().tracks) - 1): <IF_STMT> current_sequence().tracks[i].active = True else: current_sequence().tracks[i].active = False gui.tline_column.widget.queue_draw()",if i == current_sequence().get_first_active_track().id:
"def http_wrapper(self, url, postdata={}): try: if postdata != {}: f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) <IF_STMT> logging.error('with post data') else: logging.error('without post data') logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = '' return response",if postdata:
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None <IF_STMT> if hasattr(sys, 'getswitchinterval'): interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): if hasattr(sys, 'setswitchinterval'): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)",if not sys.platform.startswith('java'):
"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() <IF_STMT> if block_end: queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f","if f is not None and f.type in ('or', 'and', 'not'):"
"def smartsplit(code): """"""Split `code` at "" symbol, only if it is not escaped."""""" strings = [] pos = 0 while pos < len(code): <IF_STMT> word = '' pos += 1 while pos < len(code): if code[pos] == '""': break if code[pos] == '\\': word += '\\' pos += 1 word += code[pos] pos += 1 strings.append('""%s""' % word) pos += 1 return strings","if code[pos] == '""':"
"def get_folder_content(cls, name): """"""Return (folders, files) for the given folder in the root dir."""""" folders = set() files = set() for path in cls.LAYOUT: <IF_STMT> continue parts = path.split('/') if len(parts) == 2: files.add(parts[1]) else: folders.add(parts[1]) folders = list(folders) folders.sort() files = list(files) files.sort() return (folders, files)",if not path.startswith(name + '/'):
"def array_for(self, i): if 0 <= i < self._cnt: <IF_STMT> return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[i >> level & 31] level -= 5 assert isinstance(node, Node) return node._array affirm(False, u'Index out of Range')",if i >= self.tailoff():
"def __or__(self, other) -> 'MultiVector': """"""``self | other``, the inner product :math:`M \\cdot N`"""""" other, mv = self._checkOther(other) if mv: newValue = self.layout.imt_func(self.value, other.value) else: <IF_STMT> obj = self.__array__() return obj | other return self._newMV(dtype=np.result_type(self.value.dtype, other)) return self._newMV(newValue)","if isinstance(other, np.ndarray):"
"def parse_bzr_stats(status): stats = RepoStats() statustype = 'changed' for statusline in status: if statusline[:2] == '  ': setattr(stats, statustype, getattr(stats, statustype) + 1) <IF_STMT> statustype = 'staged' elif statusline == 'unknown:': statustype = 'new' else: statustype = 'changed' return stats",elif statusline == 'added:':
"def write(self, timestamps, actualValues, predictedValues, predictionStep=1): assert len(timestamps) == len(actualValues) == len(predictedValues) for index in range(len(self.names)): timestamp = timestamps[index] actual = actualValues[index] prediction = predictedValues[index] writer = self.outputWriters[index] <IF_STMT> outputRow = [timestamp, actual, prediction] writer.writerow(outputRow) self.lineCounts[index] += 1",if timestamp is not None:
"def clean(self): """"""Delete old files in ""tmp""."""""" now = time.time() for entry in os.listdir(os.path.join(self._path, 'tmp')): path = os.path.join(self._path, 'tmp', entry) <IF_STMT> os.remove(path)",if now - os.path.getatime(path) > 129600:
"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen([self._find_binary(), 'info', os.path.realpath(path)], stdout=PIPE, stderr=PIPE).communicate() except OSError: pass else: <IF_STMT> for line in stdout.splitlines(): line = u(line).split(': ', 1) if len(line) == 2: info[line[0]] = line[1] return info",if stdout:
"def add(meta_list, info_list=None): if not info_list: info_list = meta_list if not isinstance(meta_list, (list, tuple)): meta_list = (meta_list,) if not isinstance(info_list, (list, tuple)): info_list = (info_list,) for info_f in info_list: <IF_STMT> for meta_f in meta_list: metadata[meta_f] = info[info_f] break",if info.get(info_f) is not None:
"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): <IF_STMT> log_r_term = model_site['log_prob'] if not model_site['is_observed']: log_r_term = log_r_term - guide_trace.nodes[name]['log_prob'] log_r.add((stacks[name], log_r_term.detach())) return log_r",if model_site['type'] == 'sample':
"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break if prog.match(line): text = line[len(key) + 1:] while 1: line = f.readline() <IF_STMT> break text = text + line return text.strip() return None",if not line or not line[0].isspace():
"def build_iterator(data, infinite=True): """"""Build the iterator for inputs."""""" index = 0 size = len(data[0]) while True: if index + batch_size > size: <IF_STMT> index = 0 else: return yield (data[0][index:index + batch_size], data[1][index:index + batch_size]) index += batch_size",if infinite:
"def checkall(g, bg, dst_nodes, include_dst_in_src=True): for etype in g.etypes: ntype = g.to_canonical_etype(etype)[2] <IF_STMT> check(g, bg, ntype, etype, dst_nodes[ntype], include_dst_in_src) else: check(g, bg, ntype, etype, None, include_dst_in_src)",if dst_nodes is not None and ntype in dst_nodes:
"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: if issubclass(n, m) and m is not n: break else: <IF_STMT> candidates.remove(m) candidates.append(m) return candidates",if m in candidates:
"def __keep_songs_enable(self, enabled): config.set('memory', 'queue_keep_songs', enabled) if enabled: self.queue.set_first_column_type(CurrentColumn) else: for col in self.queue.get_columns(): <IF_STMT> self.queue.set_first_column_type(None) break","if isinstance(col, CurrentColumn):"
"def outlineView_heightOfRowByItem_(self, tree, item) -> float: default_row_height = self.rowHeight if item is self: return default_row_height heights = [default_row_height] for column in self.tableColumns: value = getattr(item.attrs['node'], str(column.identifier)) <IF_STMT> heights.append(value._impl.native.intrinsicContentSize().height) return max(heights)","if isinstance(value, toga.Widget):"
def condition(self): if self.__condition is None: <IF_STMT> self.__condition = self.flat_conditions[0] elif len(self.flat_conditions) == 0: self.__condition = lambda _: True else: self.__condition = lambda x: all((cond(x) for cond in self.flat_conditions)) return self.__condition,if len(self.flat_conditions) == 1:
"def _find_delimiter(f, block_size=2 ** 16): delimiter = b'\n' if f.tell() == 0: return 0 while True: b = f.read(block_size) if not b: return f.tell() <IF_STMT> return f.tell() - len(b) + b.index(delimiter) + 1",elif delimiter in b:
"def serialize(self, name=None): data = super(SimpleText, self).serialize(name) data['contentType'] = self.contentType data['content'] = self.content if self.width: <IF_STMT> raise InvalidWidthException(self.width) data['inputOptions'] = {} data['width'] = self.width return data","if self.width not in [100, 50, 33, 25]:"
"def inference(self): self.attention_weight_dim = self.input_dims[0][-1] if self.keep_dim: self.output_dim = copy.deepcopy(self.input_dims[0]) else: self.output_dim = [] for idx, dim in enumerate(self.input_dims[0]): <IF_STMT> self.output_dim.append(dim) super(LinearAttentionConf, self).inference()",if idx != len(self.input_dims[0]) - 2:
"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): if status == MemcacheDeleteResponse.DELETED: result.append(DELETE_SUCCESSFUL) <IF_STMT> result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",elif status == MemcacheDeleteResponse.NOT_FOUND:
def identify_page_at_cursor(self): for region in self.view.sel(): text_on_cursor = None pos = region.begin() scope_region = self.view.extract_scope(pos) <IF_STMT> text_on_cursor = self.view.substr(scope_region) return text_on_cursor.strip(string.punctuation) return None,if not scope_region.empty():
"def from_elem(cls, parent, when_elem): """"""Loads the proper when by attributes of elem"""""" when_value = when_elem.get('value', None) <IF_STMT> return ValueToolOutputActionConditionalWhen(parent, when_elem, when_value) else: when_value = when_elem.get('datatype_isinstance', None) if when_value is not None: return DatatypeIsInstanceToolOutputActionConditionalWhen(parent, when_elem, when_value) raise TypeError('When type not implemented')",if when_value is not None:
"def test_insert_entity_empty_string_rk(self, tables_cosmos_account_name, tables_primary_cosmos_account_key): await self._set_up(tables_cosmos_account_name, tables_primary_cosmos_account_key) try: entity = {'PartitionKey': 'pk', 'RowKey': ''} with pytest.raises(HttpResponseError): await self.table.create_entity(entity=entity) finally: await self._tear_down() <IF_STMT> sleep(SLEEP_DELAY)",if self.is_live:
"def provider_uris(self): login_urls = {} continue_url = self.request.get('continue_url') for provider in self.provider_info: <IF_STMT> login_url = self.uri_for('social-login', provider_name=provider, continue_url=continue_url) else: login_url = self.uri_for('social-login', provider_name=provider) login_urls[provider] = login_url return login_urls",if continue_url:
"def expand_extensions(existing): for name in extension_names: ext = im('lizard_ext.lizard' + name.lower()).LizardExtension() <IF_STMT> else name existing.insert(len(existing) if not hasattr(ext, 'ordering_index') else ext.ordering_index, ext) return existing","if isinstance(name, str)"
"def wrapper(self, *args, **kwargs): if not self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path + '/' <IF_STMT> uri += '?' + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)",if self.request.query:
"def subword_map_by_joiner(subwords, marker=SubwordMarker.JOINER): """"""Return word id for each subword token (annotate by joiner)."""""" flags = [0] * len(subwords) for i, tok in enumerate(subwords): <IF_STMT> flags[i] = 1 if tok.startswith(marker): assert i >= 1 and flags[i - 1] != 1, 'Sentence `{}` not correct!'.format(' '.join(subwords)) flags[i - 1] = 1 marker_acc = list(accumulate([0] + flags[:-1])) word_group = [i - maker_sofar for i, maker_sofar in enumerate(marker_acc)] return word_group",if tok.endswith(marker):
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue <IF_STMT> break i += direction if start < 0: start = 0",if self.select(i):
"def get_config(cls): config = {} profile_path = dingdangpath.config('profile.yml') if os.path.exists(profile_path): with open(profile_path, 'r') as f: profile = yaml.safe_load(f) <IF_STMT> if 'vid' in profile['iflytek_yuyin']: config['vid'] = profile['iflytek_yuyin']['vid'] return config",if 'iflytek_yuyin' in profile:
def get_signed_in_user(test_case): playback = not (test_case.is_live or test_case.in_recording) if playback: return MOCKED_USER_NAME else: account_info = test_case.cmd('account show').get_output_in_json() <IF_STMT> return account_info['user']['name'] return None,if account_info['user']['type'] != 'servicePrincipal':
"def rename_project(self, project, new_name): """"""Rename project, update the related projects if necessary"""""" old_name = project.name for proj in self.projects: relproj = proj.get_related_projects() <IF_STMT> relproj[relproj.index(old_name)] = new_name proj.set_related_projects(relproj) project.rename(new_name) self.save()",if old_name in relproj:
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True if len(a) != b.len: return False <IF_STMT> return True return memcmp(a.ptr, b.ptr, a.len) == 0",if a.ptr == b.ptr:
def parse_variable(self): begin = self._pos while True: ch = self.read() <IF_STMT> return ScriptVariable(self._text[begin:self._pos - 1]) elif ch is None: self.__raise_eof() elif not isidentif(ch) and ch != ':': self.__raise_char(ch),if ch == '%':
"def h_file(self): filename = self.abspath() st = os.stat(filename) cache = self.ctx.hashes_md5_tstamp if filename in cache and cache[filename][0] == st.st_mtime: return cache[filename][1] if STRONGEST: ret = Utils.h_file(filename) else: <IF_STMT> raise IOError('Not a file') ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest() cache[filename] = (st.st_mtime, ret) return ret",if stat.S_ISDIR(st[stat.ST_MODE]):
"def add_widgets(self, *widgets_or_spacings): """"""Add widgets/spacing to dialog vertical layout"""""" layout = self.layout() for widget_or_spacing in widgets_or_spacings: <IF_STMT> layout.addSpacing(widget_or_spacing) else: layout.addWidget(widget_or_spacing)","if isinstance(widget_or_spacing, int):"
"def _str_index(self): idx = self['index'] out = [] if len(idx) == 0: return out out += ['.. index:: %s' % idx.get('default', '')] for section, references in idx.iteritems(): <IF_STMT> continue elif section == 'refguide': out += ['   single: %s' % ', '.join(references)] else: out += ['   %s: %s' % (section, ','.join(references))] return out",if section == 'default':
"def dictify_CPPDEFINES(env): cppdefines = env.get('CPPDEFINES', {}) if cppdefines is None: return {} if SCons.Util.is_Sequence(cppdefines): result = {} for c in cppdefines: <IF_STMT> result[c[0]] = c[1] else: result[c] = None return result if not SCons.Util.is_Dict(cppdefines): return {cppdefines: None} return cppdefines",if SCons.Util.is_Sequence(c):
"def decoder(s): r = [] decode = [] for c in s: if c == '&' and (not decode): decode.append('&') elif c == '-' and decode: if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] <IF_STMT> decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",elif decode:
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): <IF_STMT> continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if not Placeholder.check_resolved(v.size):
"def one_gpr_reg_one_mem_scalable(ii): n, r = (0, 0) for op in _gen_opnds(ii): if op_agen(op) or (op_mem(op) and op.oc2 in ['v']): n += 1 <IF_STMT> r += 1 else: return False return n == 1 and r == 1",elif op_gprv(op):
"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in(['fasta', 'base'], refs) if seq_file and os.path.exists(seq_file): return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), 'genomes', '*', gid)) <IF_STMT> return gdirs[0]",if len(gdirs) == 1 and os.path.exists(gdirs[0]):
"def __modules(self): raw_output = self.__module_avail_output().decode('utf-8') for line in StringIO(raw_output): line = line and line.strip() if not line or line.startswith('-'): continue line_modules = line.split() for module in line_modules: <IF_STMT> module = module[0:-len(self.default_indicator)].strip() module_parts = module.split('/') module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield (module_name, module_version)",if module.endswith(self.default_indicator):
"def save(self): updates = self.cinder_obj_get_changes() if updates: <IF_STMT> metadata = updates.pop('metadata', None) self.metadata = db.backup_metadata_update(self._context, self.id, metadata, True) updates.pop('parent', None) db.backup_update(self._context, self.id, updates) self.obj_reset_changes()",if 'metadata' in updates:
"def test_set_tag(association_obj, sagemaker_session): tag = {'Key': 'foo', 'Value': 'bar'} association_obj.set_tag(tag) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags(ResourceArn=association_obj.source_arn)['Tags'] <IF_STMT> break time.sleep(5) assert len(actual_tags) > 0 assert actual_tags[0] == tag",if actual_tags:
"def test_error_stream(environ, start_response): writer = start_response('200 OK', []) wsgi_errors = environ['wsgi.errors'] error_msg = None for method in ['flush', 'write', 'writelines']: if not hasattr(wsgi_errors, method): error_msg = ""wsgi.errors has no '%s' attr"" % method <IF_STMT> error_msg = 'wsgi.errors.%s attr is not callable' % method if error_msg: break return_msg = error_msg or 'success' writer(return_msg) return []","if not error_msg and (not callable(getattr(wsgi_errors, method))):"
"def current_dict(cursor_offset, line): """"""If in dictionary completion, return the dict that should be used"""""" for m in current_dict_re.finditer(line): <IF_STMT> return LinePart(m.start(1), m.end(1), m.group(1)) return None",if m.start(2) <= cursor_offset and m.end(2) >= cursor_offset:
"def show_file_browser(self): """"""Show/hide the file browser."""""" if self.show_file_browser_action.isChecked(): sizes = self.panel.sizes() <IF_STMT> sizes[0] = sum(sizes) // 4 self.panel.setSizes(sizes) self.file_browser.show() else: self.file_browser.hide()",if sizes[0] == 0:
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.nameEncoded()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): <IF_STMT> tag.text = '' if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if not tag.text:
"def getLabel(self, address=None): if address is None: address = self.address label = address if shared.config.has_section(address): label = shared.config.get(address, 'label') queryreturn = sqlQuery('select label from addressbook where address=?', address) <IF_STMT> for row in queryreturn: label, = row else: queryreturn = sqlQuery('select label from subscriptions where address=?', address) if queryreturn != []: for row in queryreturn: label, = row return label",if queryreturn != []:
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') <IF_STMT> self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",if 'momentum' in self.args:
"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) if 'encoding' in new_kwargs: del new_kwargs['encoding'] <IF_STMT> del new_kwargs['errors'] return quote(*args, **new_kwargs)",if 'errors' in kwargs:
"def setNextFormPrevious(self, backup=STARTING_FORM): try: if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]: self._FORM_VISIT_LIST.pop() <IF_STMT> self.setNextForm(self._FORM_VISIT_LIST.pop()) except IndexError: self.setNextForm(backup)",if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM:
"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char['text'].isspace(): if current_word: yield current_word current_word = [] <IF_STMT> yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word","elif current_word and self.char_begins_new_word(current_word, char):"
"def get(self): """"""return a secret by name"""""" results = self._get('secrets', self.name) results['decoded'] = {} results['exists'] = False if results['returncode'] == 0 and results['results'][0]: results['exists'] = True <IF_STMT> if 'data' in results['results'][0]: for sname, value in results['results'][0]['data'].items(): results['decoded'][sname] = base64.b64decode(value) if results['returncode'] != 0 and '""%s"" not found' % self.name in results['stderr']: results['returncode'] = 0 return results",if self.decode:
"def insert_use(self, edit): if self.is_first_use(): for location in ['^\\s*namespace\\s+[\\w\\\\]+[;{]', '<\\?php']: inserted = self.insert_first_use(location, edit) <IF_STMT> break else: self.insert_use_among_others(edit)",if inserted:
"def _new_rsa_key(spec): if 'name' not in spec: <IF_STMT> head, tail = os.path.split(spec['key']) spec['path'] = head spec['name'] = tail else: spec['name'] = spec['key'] return rsa_init(spec)",if '/' in spec['key']:
"def mimeData(self, indexes): if len(indexes) == 1: index = indexes[0] model = song = index.data(Qt.UserRole) <IF_STMT> try: model = song.album except (ProviderIOError, Exception): model = None return ModelMimeData(model)",if index.column() == Column.album:
"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: if url.endswith('ping') and self._first_ping: self._first_ping = False return EmptyCapabilitiesResponse() <IF_STMT> return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",elif 'Hello0' in url and '1.2.1' in url and ('v1' in url):
"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): if hasattr(model, '_create_content_base'): self.validate_base_model(model) <IF_STMT> self.validate_content_type(model)","if hasattr(model, '_feincms_content_models'):"
"def test_rules_widget(self): subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit) widgets = subreddit.widgets with self.use_cassette('TestSubredditWidgets.fetch_widgets'): rules = None for widget in widgets.sidebar: <IF_STMT> rules = widget break assert isinstance(rules, RulesWidget) assert rules == rules assert rules.id == rules assert rules.display assert len(rules) > 0 assert subreddit == rules.subreddit","if isinstance(widget, RulesWidget):"
"def __init__(self, exception): message = str(exception) with contextlib.suppress(IndexError): underlying_exception = exception.args[0] <IF_STMT> message = 'maximum retries exceeded trying to reach the store.\nCheck your network connection, and check the store status at {}'.format(_STORE_STATUS_URL) super().__init__(message=message)","if isinstance(underlying_exception, urllib3.exceptions.MaxRetryError):"
"def wrapped(self, request): try: return self._finished except AttributeError: if self.node_ids: <IF_STMT> log.debug('%s is still going to be used, not terminating it. Still in use on:\n%s', self, pprint.pformat(list(self.node_ids))) return log.debug('Finish called on %s', self) try: return func(request) finally: self._finished = True",if not request.session.shouldfail and (not request.session.shouldstop):
"def get_min_vertical_scroll() -> int: used_height = 0 prev_lineno = ui_content.cursor_position.y for lineno in range(ui_content.cursor_position.y, -1, -1): used_height += get_line_height(lineno) <IF_STMT> return prev_lineno else: prev_lineno = lineno return 0",if used_height > height - scroll_offsets_bottom:
"def cookies(self): cookies = flask.Request.cookies.__get__(self) result = {} desuffixed = {} for key, value in cookies.items(): <IF_STMT> desuffixed[key[:-len(self.cookie_suffix)]] = value else: result[key] = value result.update(desuffixed) return result",if key.endswith(self.cookie_suffix):
"def update_vars(state1, state2): ops = [] for name in state1._fields: state1_vs = getattr(state1, name) <IF_STMT> ops += [tf.assign(_v1, _v2) for _v1, _v2 in zip(state1_vs, getattr(state2, name))] else: ops += [tf.assign(state1_vs, getattr(state2, name))] return tf.group(*ops)","if isinstance(state1_vs, list):"
"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: <IF_STMT> return {} mtime = self.getmtime(self.manifest_path) if self._mtime is None or mtime > self._mtime: self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",if not self.exists(self.manifest_path):
"def csvtitle(self): if isinstance(self.name, six.string_types): return '""' + self.name + '""' + char['sep'] * (len(self.nick) - 1) else: ret = '' for i, name in enumerate(self.name): ret = ret + '""' + name + '""' + char['sep'] * (len(self.nick) - 1) <IF_STMT> ret = ret + char['sep'] return ret",if i + 1 != len(self.name):
"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): <IF_STMT> if final_dst is not None: raise ValueError('Multiple destinations!') final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",if dst.is_id('IRDst'):
"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition('.') if dot: <IF_STMT> if module_name in dependencies: dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: logging.warning('Empty package name: %s', name)",if module_name:
def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == '[': m = -1 aas.append('') elif i == ']': m = 0 continue elif m == -1: aas[-1] = aas[-1] + i <IF_STMT> aas.append(i) return aas,elif m == 0:
"def logic(): count = intbv(0, min=0, max=MAXVAL + 1) while True: yield (clock.posedge, reset.posedge) if reset == 1: count[:] = 0 else: flag.next = 0 <IF_STMT> flag.next = 1 count[:] = 0 else: count += 1",if count == MAXVAL:
"def _history_define_metric(self, hkey: str) -> Optional[wandb_internal_pb2.MetricRecord]: """"""check for hkey match in glob metrics, return defined metric."""""" if hkey.startswith('_'): return None for k, mglob in six.iteritems(self._metric_globs): if k.endswith('*'): <IF_STMT> m = wandb_internal_pb2.MetricRecord() m.CopyFrom(mglob) m.ClearField('glob_name') m.name = hkey return m return None",if hkey.startswith(k[:-1]):
"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_(beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment) <IF_STMT> model.half() if use_cuda: model.cuda()",if args.fp16:
"def _Dynamic_Rollback(self, transaction, transaction_response): txid = transaction.handle() self.__local_tx_lock.acquire() try: <IF_STMT> raise apiproxy_errors.ApplicationError(datastore_pb.Error.BAD_REQUEST, 'Transaction %d not found.' % (txid,)) txdata = self.__transactions[txid] assert txdata.thread_id == thread.get_ident(), 'Transactions are single-threaded.' del self.__transactions[txid] finally: self.__local_tx_lock.release()",if txid not in self.__transactions:
"def get_job_dirs(path): regex = re.compile('[1-9][0-9]*-') jobdirs = [] for d in os.listdir(path): <IF_STMT> continue d = os.path.join(options.resultsdir, d) if os.path.isdir(d) and (not os.path.exists(os.path.join(d, PUBLISH_FLAGFILE))): jobdirs.append(d) return jobdirs",if not regex.match(d):
"def traverse(node, functions=[]): if hasattr(node, 'grad_fn'): node = node.grad_fn if hasattr(node, 'variable'): node = graph.nodes_by_id.get(id(node.variable)) if node: node.functions = list(functions) del functions[:] if hasattr(node, 'next_functions'): functions.append(type(node).__name__) for f in node.next_functions: <IF_STMT> functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, 'saved_tensors'): for t in node.saved_tensors: traverse(t)",if f[0]:
"def get_all_snap_points(self, forts): points = [] radius = Constants.MAX_DISTANCE_FORT_IS_REACHABLE for i in range(0, len(forts)): for j in range(i + 1, len(forts)): c1, c2 = self.get_enclosing_circles(forts[i], forts[j], radius) <IF_STMT> points.append((c1, c2, forts[i], forts[j])) return points",if c1 and c2:
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: <IF_STMT> continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))","if not isinstance(grandchild, minidom.Element):"
"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: if ch == ' ': w += 1 <IF_STMT> w += abs(tab_width) - w % abs(tab_width) else: break return w",elif ch == '\t':
"def test_avg_group_by(self): ret = await Book.annotate(avg=Avg('rating')).group_by('author_id').values('author_id', 'avg') for item in ret: author_id = item.get('author_id') avg = item.get('avg') <IF_STMT> self.assertEqual(avg, 4.5) elif author_id == self.a2.pk: self.assertEqual(avg, 2.0)",if author_id == self.a1.pk:
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() <IF_STMT> expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if stored_session:
"def setInnerHTML(self, html): log.HTMLClassifier.classify(log.ThugLogging.url if log.ThugOpts.local else log.last_url, html) self.tag.clear() for node in bs4.BeautifulSoup(html, 'html.parser').contents: self.tag.append(node) name = getattr(node, 'name', None) if name is None: continue handler = getattr(log.DFT, 'handle_%s' % (name,), None) <IF_STMT> handler(node)",if handler:
def get_supported_period_type_map(cls): if cls.supported_period_map is None: cls.supported_period_map = {} cls.supported_period_map.update(cls.period_type_map) try: from dateutil import relativedelta <IF_STMT> cls.supported_period_map.update(cls.optional_period_type_map) except Exception: pass return cls.supported_period_map,if relativedelta is not None:
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: <IF_STMT> if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()","if self._decide_whether_to_process(compare_id, redo, compares_done):"
"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if ':' in line: left, right = line.split(':', 1) left = left.strip().lower() right = right.strip() if left == field_name and len(right) > 0: if cant_be_number: <IF_STMT> return right else: return right return None",if not right.isdigit():
"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) if not s.consume('\\'): break <IF_STMT> pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return ''.join(res)",if s.consume_re(_newline_esc_re):
"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == '550': if self.isfile(path): raise errors.DirectoryExpected(path) <IF_STMT> raise errors.DirectoryNotEmpty(path) raise",if not self.isempty(path):
"def _normalize_store_path(self, resource_store): if resource_store['type'] == 'filesystem': <IF_STMT> resource_store['base_directory'] = os.path.join(self.root_directory, resource_store['base_directory']) return resource_store",if not os.path.isabs(resource_store['base_directory']):
"def _apply_nested(name, val, nested): parts = name.split('.') cur = nested for i in range(0, len(parts) - 1): cur = cur.setdefault(parts[i], {}) <IF_STMT> conflicts_with = '.'.join(parts[0:i + 1]) raise ValueError('%r cannot be nested: conflicts with {%r: %s}' % (name, conflicts_with, cur)) cur[parts[-1]] = val","if not isinstance(cur, dict):"
"def build_packages(targeted_packages, distribution_directory, is_dev_build=False): for package_root in targeted_packages: service_hierarchy = os.path.join(os.path.basename(package_root)) <IF_STMT> verify_update_package_requirement(package_root) print('Generating Package Using Python {}'.format(sys.version)) run_check_call([sys.executable, build_packing_script_location, '--dest', os.path.join(distribution_directory, service_hierarchy), package_root], root_dir)",if is_dev_build:
"def resolve_root_node_address(self, root_node): if '[' in root_node: name, numbers = root_node.split('[', maxsplit=1) number = numbers.split(',', maxsplit=1)[0] <IF_STMT> number = number.split('-')[0] number = re.sub('[^0-9]', '', number) root_node = name + number return root_node",if '-' in number:
"def _map_args(maps: dict, **kwargs): output = {} for name, val in kwargs.items(): if name in maps: assert isinstance(maps[name], str) output.update({maps[name]: val}) else: output.update({name: val}) for keys in maps.keys(): <IF_STMT> pass return output",if keys not in output.keys():
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break <IF_STMT> i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",if i >= len(self.items):
"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: <IF_STMT> if self.KEY in function.context: continue self._explore(function.entry_point, []) function.context[self.KEY] = True",if function.is_implemented:
"def load_model(self): if not os.path.exists(self.get_filename(absolute=True)): <IF_STMT> return ({}, {}) error('Model file with pre-trained convolution layers not found. Download it here...', 'https://github.com/alexjc/neural-enhance/releases/download/v%s/%s' % (__version__, self.get_filename())) print('  - Loaded file `{}` with trained model.'.format(self.get_filename())) return pickle.load(bz2.open(self.get_filename(), 'rb'))",if args.train:
"def get_nonexisting_check_definition_extends(definition, indexed_oval_defs): for extdefinition in definition.findall('.//{%s}extend_definition' % oval_ns): extdefinitionref = extdefinition.get('definition_ref') referreddefinition = indexed_oval_defs.get(extdefinitionref) <IF_STMT> return extdefinitionref return None",if referreddefinition is None:
"def pause(self): if self.is_playing: self.state = MusicPlayerState.PAUSED <IF_STMT> self._current_player.pause() self.emit('pause', player=self, entry=self.current_entry) return elif self.is_paused: return raise ValueError('Cannot pause a MusicPlayer in state %s' % self.state)",if self._current_player:
"def setNextFormPrevious(self, backup=STARTING_FORM): try: <IF_STMT> self._FORM_VISIT_LIST.pop() if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM: self.setNextForm(self._FORM_VISIT_LIST.pop()) except IndexError: self.setNextForm(backup)",if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]:
"def get_expr_referrers(schema: s_schema.Schema, obj: so.Object) -> Dict[so.Object, str]: """"""Return schema referrers with refs in expressions."""""" refs = schema.get_referrers_ex(obj) result = {} for (mcls, fn), referrers in refs.items(): field = mcls.get_field(fn) <IF_STMT> result.update({ref: fn for ref in referrers}) return result","if issubclass(field.type, (Expression, ExpressionList)):"
"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: <IF_STMT> continue requires_index = any((field.index, field.unique, isinstance(field, ForeignKeyField))) if requires_index: fields.append(field) return fields",if field.primary_key:
"def ident_values(self): value = self._ident_values if value is False: value = None if not self.orig_prefix: wrapped = self.wrapped idents = getattr(wrapped, 'ident_values', None) <IF_STMT> value = [self._wrap_hash(ident) for ident in idents] self._ident_values = value return value",if idents:
def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) if not node: error('node not found: ' + str(dir)) continue <IF_STMT> lst.append(node) self.bld_incpaths_lst.append(node),if not node in lst:
"def application_openFiles_(self, nsapp, filenames): for filename in filenames: logging.info('[osx] receiving from macOS : %s', filename) if os.path.exists(filename): <IF_STMT> sabnzbd.add_nzbfile(filename, keep=True)",if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES:
"def check(self, xp, nout): input = xp.asarray(self.x).astype(numpy.float32) with warnings.catch_warnings(): if self.ignore_warning: warnings.simplefilter('ignore', self.ignore_warning) <IF_STMT> self.check_positive(xp, self.func, input, self.eps, nout) else: self.check_negative(xp, self.func, input, self.eps, nout)",if self.result:
"def _set_scheme(url, newscheme): scheme = _get_scheme(url) newscheme = newscheme or '' newseparator = ':' if newscheme in COLON_SEPARATED_SCHEMES else '://' if scheme == '': url = '%s:%s' % (newscheme, url) elif scheme is None and url: url = ''.join([newscheme, newseparator, url]) elif scheme: remainder = url[len(scheme):] <IF_STMT> remainder = remainder[3:] elif remainder.startswith(':'): remainder = remainder[1:] url = ''.join([newscheme, newseparator, remainder]) return url",if remainder.startswith('://'):
"def parquet(tables, data_directory, ignore_missing_dependency, **params): try: import pyarrow as pa import pyarrow.parquet as pq except ImportError: msg = 'PyArrow dependency is missing' <IF_STMT> logger.warning('Ignored: %s', msg) return 0 else: raise click.ClickException(msg) data_directory = Path(data_directory) for table, df in read_tables(tables, data_directory): arrow_table = pa.Table.from_pandas(df) target_path = data_directory / '{}.parquet'.format(table) pq.write_table(arrow_table, str(target_path))",if ignore_missing_dependency:
"def h2i(self, pkt, s): t = () if type(s) is str: t = time.strptime(s) t = t[:2] + t[2:-3] el<IF_STMT> y, m, d, h, min, sec, rest, rest, rest = time.gmtime(time.time()) t = (y, m, d, h, min, sec) else: t = s return t",if not s:
"def filter_episodes(self, batch, cross_entropy): """"""Filter the episodes for the cross_entropy method"""""" accumulated_reward = [sum(rewards) for rewards in batch['rewards']] percentile = cross_entropy * 100 reward_bound = np.percentile(accumulated_reward, percentile) result = {k: [] for k in self.data_keys} episode_kept = 0 for i in range(len(accumulated_reward)): <IF_STMT> for k in self.data_keys: result[k].append(batch[k][i]) episode_kept += 1 return result",if accumulated_reward[i] >= reward_bound:
"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] if envvar in os.environ: value = os.environ[envvar] <IF_STMT> value = value.decode('utf8') return value else: raise InvalidConfigException(""{} - environment variable '{}' not set"".format(msg, var)) else: raise InvalidConfigException(""{} - environment variable name '{}' does not match pattern '{}'"".format(msg, var, _ENV_VAR_PAT_STR))",if six.PY2:
def _allocate_nbd(self): if not os.path.exists('/sys/block/nbd0'): self.error = _('nbd unavailable: module not loaded') return None while True: if not self._DEVICES: self.error = _('No free nbd devices') return None device = self._DEVICES.pop() <IF_STMT> break return device,if not os.path.exists('/sys/block/%s/pid' % os.path.basename(device)):
"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() <IF_STMT> keys.add(k) dep = self.target_database[k] if 'generate_java' in dep.attr: dep.attr['generate_java'] = True queue.extend(dep.deps)",if k not in keys:
"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get('partitions', [])) scanners = {} for part_name, part_scanner in list(syntax.get('scanner', {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get('formats', {}).items()): if isinstance(fstyle, basestring): <IF_STMT> key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return (partition_scanner, scanners, formats)",if fstyle.startswith('%(') and fstyle.endswith(')s'):
"def rollback(self): for operation, values in self.current_transaction_state[::-1]: <IF_STMT> values.remove() elif operation == 'update': old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",if operation == 'insert':
"def _buildOffsets(offsetDict, localeData, indexStart): o = indexStart for key in localeData: <IF_STMT> for k in key.split('|'): offsetDict[k] = o else: offsetDict[key] = o o += 1",if '|' in key:
"def _check_start_pipeline_execution_errors(graphene_info, execution_params, execution_plan): if execution_params.step_keys: for step_key in execution_params.step_keys: <IF_STMT> raise UserFacingGraphQLError(graphene_info.schema.type_named('InvalidStepError')(invalid_step_key=step_key))",if not execution_plan.has_step(step_key):
"def __setattr__(self, option_name, option_value): if option_name in self._options: sort = self.OPTIONS[self.arch.name][option_name][0] <IF_STMT> self._options[option_name] = option_value else: raise ValueError('Value for option ""%s"" must be of type %s' % (option_name, sort)) else: super(CFGArchOptions, self).__setattr__(option_name, option_value)","if sort is None or isinstance(option_value, sort):"
"def value(self): quote = False if self.defects: quote = True else: for x in self: <IF_STMT> quote = True if quote: pre = post = '' if self[0].token_type == 'cfws' or self[0][0].token_type == 'cfws': pre = ' ' if self[-1].token_type == 'cfws' or self[-1][-1].token_type == 'cfws': post = ' ' return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",if x.token_type == 'quoted-string':
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: <IF_STMT> filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories","if isinstance(filename_data, list):"
"def _evaluateStack(s): op = s.pop() if op in '+-*/@^': op2 = _evaluateStack(s) op1 = _evaluateStack(s) result = opn[op](op1, op2) <IF_STMT> print(result) return result else: return op",if debug_flag:
"def reconnect_user(self, user_id, host_id, server_id): if host_id == settings.local.host_id: return if server_id and self.server.id != server_id: return for client in self.clients.find({'user_id': user_id}): self.clients.update_id(client['id'], {'ignore_routes': True}) <IF_STMT> self.instance.disconnect_wg(client['id']) else: self.instance_com.client_kill(client['id'])",if len(client['id']) > 32:
"def _get_library(self, name, args): library_database = self._library_manager.get_new_connection_to_library_database() try: last_updated = library_database.get_library_last_updated(name, args) if last_updated: <IF_STMT> self._library_manager.fetch_keywords(name, args, self._libraries_need_refresh_listener) return library_database.fetch_library_keywords(name, args) return self._library_manager.get_and_insert_keywords(name, args) finally: library_database.close()",if time.time() - last_updated > 10.0:
"def get_paths(self, path, commit): """"""Return a generator of all filepaths under path at commit."""""" _check_path_is_repo_relative(path) git_path = _get_git_path(path) tree = self.gl_repo.git_repo[commit.tree[git_path].id] assert tree.type == pygit2.GIT_OBJ_TREE for tree_entry in tree: tree_entry_path = os.path.join(path, tree_entry.name) <IF_STMT> for fp in self.get_paths(tree_entry_path, commit): yield fp else: yield tree_entry_path",if tree_entry.type == 'tree':
"def scan_resource_conf(self, conf): if 'properties' in conf: if 'attributes' in conf['properties']: <IF_STMT> if conf['properties']['attributes']['exp']: return CheckResult.PASSED return CheckResult.FAILED",if 'exp' in conf['properties']['attributes']:
"def _set_parse_context(self, tag, tag_attrs): if not self._wb_parse_context: if tag == 'style': self._wb_parse_context = 'style' elif tag == 'script': <IF_STMT> self._wb_parse_context = 'script'",if self._allow_js_type(tag_attrs):
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): <IF_STMT> continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",if item is None:
def preorder(root): res = [] if not root: return res stack = [] stack.append(root) while stack: root = stack.pop() res.append(root.val) <IF_STMT> stack.append(root.right) if root.left: stack.append(root.left) return res,if root.right:
"def create(exported_python_target): if exported_python_target not in created: self.context.log.info('Creating setup.py project for {}'.format(exported_python_target)) subject = self.derived_by_original.get(exported_python_target, exported_python_target) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir if self._recursive: for dep in dependencies: <IF_STMT> create(dep)",if is_exported_python_target(dep):
"def test_array_interface(self, data): result = np.array(data) np.testing.assert_array_equal(result[0], data[0]) result = np.array(data, dtype=object) expected = np.array(list(data), dtype=object) for a1, a2 in zip(result, expected): <IF_STMT> assert np.isnan(a1) and np.isnan(a2) else: tm.assert_numpy_array_equal(a2, a1)",if np.isscalar(a1):
"def valueChanged(plug): changed = plug.getInput() is not None if not changed and isinstance(plug, Gaffer.ValuePlug): <IF_STMT> changed = not Gaffer.NodeAlgo.isSetToUserDefault(plug) else: changed = not plug.isSetToDefault() return changed",if Gaffer.NodeAlgo.hasUserDefault(plug):
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) <IF_STMT> major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return (company, major, minor, arch, exe, args)",if version is not None:
"def __iter__(self): for name, value in self.__class__.__dict__.items(): if isinstance(value, alias_flag_value): continue <IF_STMT> yield (name, self._has_flag(value.flag))","if isinstance(value, flag_value):"
"def connect(self): self.sock = sockssocket() self.sock.setproxy(*proxy_args) if type(self.timeout) in (int, float): self.sock.settimeout(self.timeout) self.sock.connect((self.host, self.port)) if isinstance(self, compat_http_client.HTTPSConnection): <IF_STMT> self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host) else: self.sock = ssl.wrap_socket(self.sock)","if hasattr(self, '_context'):"
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith('java'): if hasattr(sys, 'getswitchinterval'): interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): <IF_STMT> sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)","if hasattr(sys, 'setswitchinterval'):"
"def vars(self): ret = [] if op.intlist: varlist = op.intlist else: varlist = self.discover for name in varlist: if name in ('0', '1', '2', '8', 'CPU0', 'ERR', 'LOC', 'MIS', 'NMI'): varlist.remove(name) if not op.full and len(varlist) > 3: varlist = varlist[-3:] for name in varlist: if name in self.discover: ret.append(name) <IF_STMT> ret.append(self.intmap[name.lower()]) return ret",elif name.lower() in self.intmap:
"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) <IF_STMT> count = len(inst) toReturn.append(gadget) added = True if callback: callback(gadget, added, float(i + 1) / len_gadgets) added = False return toReturn",if len(inst) > count:
"def ident(self): value = self._ident if value is False: value = None if not self.orig_prefix: wrapped = self.wrapped ident = getattr(wrapped, 'ident', None) <IF_STMT> value = self._wrap_hash(ident) self._ident = value return value",if ident is not None:
"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: if isinstance(field, _ContainerMixin): setting_values.update(self._flatten_settings_from_form(settings, field, form_values[field._name])) <IF_STMT> setting_values[field._name] = form_values[field._name] return setting_values",elif field._name in settings:
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != '__init__': <IF_STMT> continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",if not private and name.startswith('_'):
"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) <IF_STMT> return False if not b1: return True",if b1 != b2:
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError <IF_STMT> self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",if self.ctl:
"def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]: self.invoke_threads() total_links = 0 for hyperlink in hyperlinks.values(): <IF_STMT> yield CheckResult(hyperlink.uri, hyperlink.docname, hyperlink.lineno, 'ignored', '', 0) else: self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False) total_links += 1 done = 0 while done < total_links: yield self.rqueue.get() done += 1 self.shutdown_threads()",if self.is_ignored_uri(hyperlink.uri):
"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: <IF_STMT> subscriber._pyroRelease() if hasattr(subscriber, '_pyroUri'): try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)","if hasattr(subscriber, '_pyroRelease'):"
"def delete_arc(collection, document, origin, target, type): directory = collection real_dir = real_directory(directory) mods = ModificationTracker() projectconf = ProjectConfiguration(real_dir) document = path_join(real_dir, document) with TextAnnotations(document) as ann_obj: <IF_STMT> raise AnnotationsIsReadOnlyError(ann_obj.get_document()) _delete_arc_with_ann(origin, target, type, mods, ann_obj, projectconf) mods_json = mods.json_response() mods_json['annotations'] = _json_from_ann(ann_obj) return mods_json",if ann_obj._read_only:
"def _select_from(self, parent_path, is_dir, exists, listdir): if not is_dir(parent_path): return with _cached(listdir) as listdir: yielded = set() try: successor_select = self.successor._select_from for starting_point in self._iterate_directories(parent_path, is_dir, listdir): for p in successor_select(starting_point, is_dir, exists, listdir): <IF_STMT> yield p yielded.add(p) finally: yielded.clear()",if p not in yielded:
"def _fractional_part(self, n, expr, evaluation): n_sympy = n.to_sympy() if n_sympy.is_constant(): <IF_STMT> positive_integer_part = Expression('Floor', n).evaluate(evaluation).to_python() result = n - positive_integer_part else: negative_integer_part = Expression('Ceiling', n).evaluate(evaluation).to_python() result = n - negative_integer_part else: return expr return from_python(result)",if n_sympy >= 0:
"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: if geometry[0] > 180 or geometry[0] < -180: raise ValueError('Longitude is out of bounds, check your JSON format or data') <IF_STMT> raise ValueError('Latitude is out of bounds, check your JSON format or data')",if geometry[1] > 90 or geometry[1] < -90:
"def get_absolute_path(self, root, path): self.root = self.roots[0] for root in self.roots: abspath = os.path.abspath(os.path.join(root, path)) <IF_STMT> self.root = root break return abspath",if os.path.exists(abspath):
"def do_setflow(self, l=''): try: <IF_STMT> l = str(self.flow_slider.GetValue()) else: l = l.lower() flow = int(l) if self.p.online: self.p.send_now('M221 S' + l) self.log(_('Setting print flow factor to %d%%.') % flow) else: self.logError(_('Printer is not online.')) except Exception as x: self.logError(_('You must enter a flow. (%s)') % (repr(x),))","if not isinstance(l, str) or not len(l):"
def sources(): for d in os.listdir(base): <IF_STMT> continue if d == 'indcat': continue if not os.path.isdir(base + d): continue yield d,if d.endswith('old'):
"def create_accumulator(self) -> tf_metric_accumulators.TFCompilableMetricsAccumulator: configs = zip(self._metric_configs, self._loss_configs) padding_options = None if self._eval_config is not None: model_spec = model_util.get_model_spec(self._eval_config, self._model_name) <IF_STMT> padding_options = model_spec.padding_options return tf_metric_accumulators.TFCompilableMetricsAccumulator(padding_options, [len(m) + len(l) for m, l in configs], desired_batch_size=self._desired_batch_size)",if model_spec is not None and model_spec.HasField('padding_options'):
"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): <IF_STMT> if self.expr.resultsName: tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return (loc, tokens)",if self.defaultValue is not self.__optionalNotMatched:
"def handleConnection(self): try: <IF_STMT> return True self.csock.close() except: ex_t, ex_v, ex_tb = sys.exc_info() tb = util.formatTraceback(ex_t, ex_v, ex_tb) log.warning('error during connect/handshake: %s; %s', ex_v, '\n'.join(tb)) self.csock.close() return False",if self.daemon._handshake(self.csock):
"def getProc(su, innerTarget): if len(su) == 1: proc = ('first', 'last') el<IF_STMT> proc = ('first', 'last') elif su.isFirst(innerTarget): proc = ('first',) elif su.isLast(innerTarget): proc = ('last',) else: proc = () return proc",if su.isFirst(innerTarget) and su.isLast(innerTarget):
"def get_color_dtype(data, column_names): has_color = all((column in data['points'] for column in column_names)) if has_color: color_data_types = [data['points'][column_name].dtype for column_name in column_names] <IF_STMT> raise TypeError(f'Data types of color values are inconsistent: got {color_data_types}') color_data_type = color_data_types[0] else: color_data_type = None return color_data_type",if len(set(color_data_types)) > 1:
"def close(self): children = [] for children_part, line_offset, last_line_offset_leaf in self.children_groups: <IF_STMT> try: _update_positions(children_part, line_offset, last_line_offset_leaf) except _PositionUpdatingFinished: pass children += children_part self.tree_node.children = children for node in children: node.parent = self.tree_node",if line_offset != 0:
"def get_multi(self, keys, index=None): with self._lmdb.begin() as txn: result = [] for key in keys: packed = txn.get(key.encode()) <IF_STMT> result.append((key, cbor.loads(packed))) return result",if packed is not None:
"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: if p[0] != '.': subp = os.path.join(pth, p) p = os.path.join(prefix, p) <IF_STMT> res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",if recursive and os.path.isdir(subp):
"def __schedule(self, workflow_scheduler_id, workflow_scheduler): invocation_ids = self.__active_invocation_ids(workflow_scheduler_id) for invocation_id in invocation_ids: log.debug('Attempting to schedule workflow invocation [%s]', invocation_id) self.__attempt_schedule(invocation_id, workflow_scheduler) <IF_STMT> return",if not self.monitor_running:
"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: if self.size: data, passon = (data[:self.size], data[self.size:]) else: passon = b'' <IF_STMT> self.data.append(data) return passon",if data:
"def __getstate__(self): try: store_func, load_func = (self.store_function, self.load_function) self.store_function, self.load_function = (None, None) d = dict(((k, v) for k, v in self.__dict__.items() <IF_STMT> not in {'analyses'})) return d finally: self.store_function, self.load_function = (store_func, load_func)",if k
"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local <IF_STMT> self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",if self.scroll_up_rect().collidepoint(p):
"def on_api_command(self, command, data): if command == 'select': if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, 'Insufficient permissions') if self._prompt is None: return flask.abort(409, 'No active prompt') choice = data['choice'] <IF_STMT> return flask.abort(400, '{!r} is not a valid value for choice'.format(choice)) self._answer_prompt(choice)","if not isinstance(choice, int) or not self._prompt.validate_choice(choice):"
"def register_predictors(self, model_data_arr): for integration in self._get_integrations(): <IF_STMT> integration.register_predictors(model_data_arr) else: logger.warning(f""There is no connection to {integration.name}. predictor wouldn't be registred."")",if integration.check_connection():
"def _pack_shears(shearData): shears = list() vidxs = list() for e_idx, entry in enumerate(shearData): <IF_STMT> shears.extend([float('nan'), float('nan')]) vidxs.extend([0, 0]) else: vidx1, vidx2, shear1, shear2 = entry shears.extend([shear1, shear2]) vidxs.extend([vidx1, vidx2]) return (np.asarray(shears, dtype=np.float32), np.asarray(vidxs, dtype=np.uint32))",if entry is None:
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): <IF_STMT> continue yield (cog_name, cog_id)","if not isinstance(inner, dict):"
"def subFeaName(m, newNames, state): try: int(m[3], 16) except: return m[0] name = m[2] if name in newNames: <IF_STMT> print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4])) state['didChange'] = True return m[1] + newNames[name] + m[4] return m[0]",if name == 'uni0402':
"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: if input_array is None: input_array = model.example_input_array <IF_STMT> input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn('Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given', UserWarning)",if input_array is not None:
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) <IF_STMT> return True if not event.get_date_object(): return True return False",if not event.get_place_handle():
"def format(m): if m > 1000: <IF_STMT> return (str(int(m / 1000)), 'km') else: return (str(round(m / 1000, 1)), 'km') return (str(m), 'm')",if m % 1000 == 0:
"def previous(self): try: idx = _jump_list_index next_index = idx + 1 <IF_STMT> next_index = 100 next_index = min(len(_jump_list) - 1, next_index) _jump_list_index = next_index return _jump_list[next_index] except (IndexError, KeyError) as e: return None",if next_index > 100:
"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition['spec'] <IF_STMT> self.hyperparam_dict[name] = spec['DefaultValue'] elif 'IsRequired' in spec and spec['IsRequired']: raise ValueError('Required hyperparameter: %s is not set' % name)",if 'DefaultValue' in spec:
"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: if self.action_input.selected_index == 0: self.back_to_parent() <IF_STMT> self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) elif self.action_input.selected_index == 2: self._apply_prefs() self.back_to_parent()",elif self.action_input.selected_index == 1:
"def _split_anonymous_function(s): if s[:1] == '[' and s[-1:] == ']' and (':' in s): try: l = yaml_util.decode_yaml(s) except Exception: return (None, s[1:-1]) else: <IF_STMT> return (None, s[1:-1]) return None","if len(l) == 1 and isinstance(l[0], (six.string_types, int)):"
"def test_source_address(self): for addr, is_ipv6 in VALID_SOURCE_ADDRESSES: <IF_STMT> warnings.warn('No IPv6 support: skipping.', NoIPv6Warning) continue pool = HTTPConnectionPool(self.host, self.port, source_address=addr, retries=False) self.addCleanup(pool.close) r = pool.request('GET', '/source_address') self.assertEqual(r.data, b(addr[0]))",if is_ipv6 and (not HAS_IPV6_AND_DNS):
"def vim_G(self): """"""Put the cursor on the last character of the file."""""" if self.is_text_wrapper(self.w): <IF_STMT> self.do('end-of-buffer-extend-selection') else: self.do('end-of-buffer') self.done() else: self.quit()",if self.state == 'visual':
"def backend_supported(module, manager, **kwargs): if CollectionNodeModule.backend_supported(module, manager, **kwargs): if 'tid' not in kwargs: return True conn = manager.connection(did=kwargs['did']) template_path = 'partitions/sql/{0}/#{0}#{1}#'.format(manager.server_type, manager.version) SQL = render_template('/'.join([template_path, 'backend_support.sql']), tid=kwargs['tid']) status, res = conn.execute_scalar(SQL) <IF_STMT> return internal_server_error(errormsg=res) return res",if not status:
"def _get_regex_config(self, data_asset_name: Optional[str]=None) -> dict: regex_config: dict = copy.deepcopy(self._default_regex) asset: Optional[Asset] = None if data_asset_name: asset = self._get_asset(data_asset_name=data_asset_name) if asset is not None: <IF_STMT> regex_config['pattern'] = asset.pattern if asset.group_names: regex_config['group_names'] = asset.group_names return regex_config",if asset.pattern:
"def resolve(self, other): if other == ANY_TYPE: return self elif isinstance(other, ComplexType): f = self.first.resolve(other.first) s = self.second.resolve(other.second) <IF_STMT> return ComplexType(f, s) else: return None elif self == ANY_TYPE: return other else: return None",if f and s:
"def collect_pages(app): new_images = {} for full_path, basename in app.builder.images.iteritems(): base, ext = os.path.splitext(full_path) retina_path = base + '@2x' + ext <IF_STMT> new_images[retina_path] = app.env.images[retina_path][1] app.builder.images.update(new_images) return []",if retina_path in app.env.images:
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): <IF_STMT> return True if linenum > 0 and line[0] not in '\t ': return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",if not line:
"def reader(): try: imgs = mp4_loader(video_path, seg_num, seglen, mode) <IF_STMT> logger.error('{} frame length {} less than 1.'.format(video_path, len(imgs))) yield (None, None) except: logger.error('Error when loading {}'.format(mp4_path)) yield (None, None) imgs_ret = imgs_transform(imgs, mode, seg_num, seglen, short_size, target_size, img_mean, img_std) label_ret = video_path yield (imgs_ret, label_ret)",if len(imgs) < 1:
"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) <IF_STMT> for separator in (' & ', '; ', ' and ', ' vs. ', ' with ', ' y '): if separator in sortname: parts = sortname.split(separator) break else: parts = [sortname] separator = '' return separator.join(map(_reverse_sortname, parts)) return name",if ctg[0] == 'L' and unicodedata.name(c).find('LATIN') == -1:
"def _to_local_path(path): """"""Convert local path to SFTP path"""""" if sys.platform == 'win32': path = os.fsdecode(path) <IF_STMT> path = path[1:] path = path.replace('/', '\\') return path",if path[:1] == '/' and path[2:3] == ':':
"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == 'tacotron': text = tacotron_cleaner.cleaners.custom_english_cleaners(text) <IF_STMT> text = jaconv.normalize(text) elif t == 'vietnamese': if vietnamese_cleaners is None: raise RuntimeError('Please install underthesea') text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f'Not supported: type={t}') return text",elif t == 'jaconv':
"def cb_syncthing_system_data(self, daemon, mem, cpu, d_failed, d_total): if self.daemon.get_my_id() in self.devices: device = self.devices[self.daemon.get_my_id()] device['ram'] = sizeof_fmt(mem) device['cpu'] = '%3.2f%%' % cpu <IF_STMT> device['announce'] = _('disabled') else: device['announce'] = '%s/%s' % (d_total - d_failed, d_total)",if d_total == 0:
"def update_kls(self, sampled_kls): for i, kl in enumerate(sampled_kls): <IF_STMT> self.kl_coeff_val[i] *= 0.5 elif kl > 1.5 * self.kl_target: self.kl_coeff_val[i] *= 2.0 return self.kl_coeff_val",if kl < self.kl_target / 1.5:
"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): if self.FindItemAtPosition((r, c)) is not None: f = False <IF_STMT> cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]",if f:
"def get_session(self): if self._session is None: session = super(ChildResourceManager, self).get_session() <IF_STMT> session = session.get_session_for_resource(self.resource_type.resource) self._session = session return self._session",if self.resource_type.resource != constants.RESOURCE_ACTIVE_DIRECTORY:
"def _get_master_authorized_networks_config(self, raw_cluster): if raw_cluster.get('masterAuthorizedNetworksConfig'): config = raw_cluster.get('masterAuthorizedNetworksConfig') config['includes_public_cidr'] = False for block in config['cidrBlocks']: <IF_STMT> config['includes_public_cidr'] = True return config else: return {'enabled': False, 'cidrBlocks': [], 'includes_public_cidr': False}",if block['cidrBlock'] == '0.0.0.0/0':
"def scan_folder(folder): scanned_files = [] for root, dirs, files in os.walk(folder): dirs[:] = [d for d in dirs if d != '__pycache__'] relative_path = os.path.relpath(root, folder) for f in files: <IF_STMT> continue relative_name = os.path.normpath(os.path.join(relative_path, f)).replace('\\', '/') scanned_files.append(relative_name) return sorted(scanned_files)",if f.endswith('.pyc'):
def read_progress(self): while True: processed_file = self.queue.get() self.threading_completed.append(processed_file) total_number = len(self.file_list) completed_number = len(self.threading_completed) if _progress_emitter: _progress_emitter.update_progress(completed_number * 100 // total_number) <IF_STMT> break,if total_number == completed_number:
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): <IF_STMT> return False continue if line.startswith('def ') or line.startswith('class '): return True if line.startswith(('#', '@', ' ', ')')): continue return False return False",if i > 0 and (not lines[i - 1].strip()):
def __next__(self): try: data = next(self.iter_loader) except StopIteration: self._epoch += 1 <IF_STMT> self._dataloader.sampler.set_epoch(self._epoch) self.iter_loader = iter(self._dataloader) data = next(self.iter_loader) return data,"if hasattr(self._dataloader.sampler, 'set_epoch'):"
"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: if dt is not None: <IF_STMT> return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)","if isinstance(dt, dgl.DGLGraph):"
"def f(self, info): for k in keys: <IF_STMT> for k2 in list(info.keys()): if k(k2): info.pop(k2) else: info.pop(k, None)",if callable(k):
"def create(path, binary=False): for i in range(10): try: os.makedirs(os.path.dirname(path), exist_ok=True) <IF_STMT> return open(path, 'wb') else: return open(path, 'w', encoding='utf-8') if i > 0: log(True, f'Created {path} at attempt {i + 1}') except: time.sleep(0.5) else: raise Error(f'Failed to create {path}')",if binary:
"def validate_update(self, update_query): structure = DotCollapsedDict(self.doc_class.structure) for op, fields in update_query.iteritems(): for field in fields: if op != '$unset' and op != '$rename': <IF_STMT> raise UpdateQueryError(""'%s' not found in %s's structure"" % (field, self.doc_class.__name__))",if field not in structure:
"def check_enums_ATLAS_ISAEXT(lines): for i, isaext in enumerate(ATLAS_ISAEXT): got = lines.pop(0).strip() <IF_STMT> expect = 'none: 1' else: expect = '{0}: {1}'.format(isaext, 1 << i) if got != expect: raise RuntimeError('ATLAS_ISAEXT mismatch at position ' + str(i) + ': got >>' + got + '<<, expected >>' + expect + '<<')",if i == 0:
"def _test_export_session_csv(self, test_session=None): with self.app.test_request_context(): <IF_STMT> test_session = SessionFactory() field_data = export_sessions_csv([test_session]) session_row = field_data[1] self.assertEqual(session_row[0], 'example (accepted)') self.assertEqual(session_row[9], 'accepted')",if not test_session:
"def get_report_to_platform(self, args, scan_reports): if self.bc_api_key: <IF_STMT> repo_id = self.get_repository(args) self.setup_bridgecrew_credentials(bc_api_key=self.bc_api_key, repo_id=repo_id) if self.is_integration_configured(): self._upload_run(args, scan_reports)",if args.directory:
"def test_fvalue(self): if not getattr(self, 'skip_f', False): rtol = getattr(self, 'rtol', 1e-10) assert_allclose(self.res1.fvalue, self.res2.F, rtol=rtol) <IF_STMT> assert_allclose(self.res1.f_pvalue, self.res2.Fp, rtol=rtol) else: raise pytest.skip('TODO: document why this test is skipped')","if hasattr(self.res2, 'Fp'):"
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): if e.value is None: e.value = [] <IF_STMT> e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",elif type(e.value) is not list:
"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) if not node.isdir: raise AssertionError('Not a folder: %s' % self.path) <IF_STMT> self.watcher.emit('created', self)",if self.watcher:
"def __init__(self, _inf=None, _tzinfos=None): if _inf: self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = _inf else: _tzinfos = {} self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = self._transition_info[0] _tzinfos[self._transition_info[0]] = self for inf in self._transition_info[1:]: <IF_STMT> _tzinfos[inf] = self.__class__(inf, _tzinfos)",if not _tzinfos.has_key(inf):
"def test_sample_output(): comment = 'SAMPLE OUTPUT' skip_files = ['__init__.py'] errors = [] for _file in sorted(MODULE_PATH.iterdir()): <IF_STMT> with _file.open() as f: if comment not in f.read(): errors.append((comment, _file)) if errors: line = 'Missing sample error(s) detected!\n\n' for error in errors: line += '`{}` is not in module `{}`\n'.format(*error) print(line[:-1]) assert False",if _file.suffix == '.py' and _file.name not in skip_files:
"def http_get(url, target): req = requests.get(url, stream=True) content_length = req.headers.get('Content-Length') total = int(content_length) if content_length is not None else None progress = tqdm(unit='B', total=total) with open(target, 'wb') as target_file: for chunk in req.iter_content(chunk_size=1024): <IF_STMT> progress.update(len(chunk)) target_file.write(chunk) progress.close()",if chunk:
"def _elements_to_datasets(self, elements, level=0): for element in elements: extra_kwds = {'identifier_%d' % level: element['name']} <IF_STMT> for inner_element in self._elements_to_datasets(element['elements'], level=level + 1): dataset = extra_kwds.copy() dataset.update(inner_element) yield dataset else: dataset = extra_kwds extra_kwds.update(element) yield extra_kwds",if 'elements' in element:
"def update_dict(a, b): for key, value in b.items(): if value is None: continue <IF_STMT> a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",if key not in a:
"def scan(self, targets): for target in targets: target.print_infos() if self.is_interesting(target): self.target['other'].append(target) <IF_STMT> return target return None",if self.match(target):
"def printConnections(switches): """"""Compactly print connected nodes to each switch"""""" for sw in switches: output('%s: ' % sw) for intf in sw.intfList(): link = intf.link <IF_STMT> intf1, intf2 = (link.intf1, link.intf2) remote = intf1 if intf1.node != sw else intf2 output('%s(%s) ' % (remote.node, sw.ports[intf])) output('\n')",if link:
"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P) begin, nexti = (0, 0) for i, char in enumerate(sentence): pos = pos_list[i] if pos == 'B': begin = i <IF_STMT> yield sentence[begin:i + 1] nexti = i + 1 elif pos == 'S': yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",elif pos == 'E':
"def check_files(self, paths=None): """"""Run all checks on the paths."""""" if paths is None: paths = self.paths report = self.options.report runner = self.runner report.start() try: for path in paths: <IF_STMT> self.input_dir(path) elif not self.excluded(path): runner(path) except KeyboardInterrupt: print('... stopped') report.stop() return report",if os.path.isdir(path):
"def verts_of_loop(edge_loop): verts = [] for e0, e1 in iter_pairs(edge_loop, False): <IF_STMT> v0 = e0.shared_vert(e1) verts += [e0.other_vert(v0), v0] verts += [e1.other_vert(verts[-1])] if len(verts) > 1 and verts[0] == verts[-1]: return verts[:-1] return verts",if not verts:
"def generator(self, data): for task in data: <IF_STMT> continue for bucket in task.bash_hash_entries(): yield (0, [int(task.p_pid), str(task.p_comm), int(bucket.times_found), str(bucket.key), str(bucket.data.path)])",if not (self._config.SCAN_ALL or str(task.p_comm) == 'bash'):
"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() if w: aList = w.sizes() <IF_STMT> n1, n2 = aList ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",if len(aList) == 2:
"def geterrors(self): """"""Get all error messages."""""" notes = self.getnotes(origin='translator').split('\n') errordict = {} for note in notes: <IF_STMT> error = note.replace('(pofilter) ', '') errorname, errortext = error.split(': ', 1) errordict[errorname] = errortext return errordict",if '(pofilter) ' in note:
"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: if d in ['.', '..']: continue d_path = ''.join([path, '/', d]) d_new_path = ''.join([new_path, '/', d]) attr = self.getattr(d_path) <IF_STMT> self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)",if stat.S_ISDIR(attr['st_mode']):
"def index(self, url_id: int) -> FlaskResponse: url = db.session.query(models.Url).get(url_id) if url and url.url: explore_url = '//superset/explore/?' <IF_STMT> explore_url += f'r={url_id}' return redirect(explore_url[1:]) return redirect(url.url[1:]) flash('URL to nowhere...', 'danger') return redirect('/')",if url.url.startswith(explore_url):
"def testShortCircuit(self): """"""Test that creation short-circuits to reuse existing references"""""" sd = {} for s in self.ss: sd[s] = 1 for t in self.ts: <IF_STMT> self.assertTrue(sd.has_key(safeRef(t.x))) self.assertTrue(safeRef(t.x) in sd) else: self.assertTrue(sd.has_key(safeRef(t))) self.assertTrue(safeRef(t) in sd)","if hasattr(t, 'x'):"
"def wrapped(request, *args, **kwargs): if not request.user.is_authenticated(): request.session['_next'] = request.get_full_path() <IF_STMT> redirect_uri = reverse('sentry-auth-organization', args=[kwargs['organization_slug']]) else: redirect_uri = get_login_url() return HttpResponseRedirect(redirect_uri) return func(request, *args, **kwargs)",if 'organization_slug' in kwargs:
"def read_info(reader, dump=None): line_number_table_length = reader.read_u2() <IF_STMT> reader.debug('' * dump, 'Line numbers (%s total):' % line_number_table_length) line_numbers = [] for i in range(0, line_number_table_length): start_pc = reader.read_u2() line_number = reader.read_u2() if dump is not None: reader.debug('' * (dump + 1), '%s: %s' % (start_pc, line_number)) line_numbers.append((start_pc, line_number)) return LineNumberTable(line_numbers)",if dump is not None:
"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 if 0 < dt: break else: dt = t2 - previous if dt <= 0.0: continue <IF_STMT> precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",if precision is not None:
def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() <IF_STMT> pass else: lineno = self.expr2.get_hi_lineno() if self.expr3 is None: pass else: lineno = self.expr3.get_hi_lineno() return lineno,if self.expr2 is None:
"def validate_cluster_resource_group(cmd, namespace): if namespace.cluster_resource_group is not None: client = get_mgmt_service_client(cmd.cli_ctx, ResourceType.MGMT_RESOURCE_RESOURCES) <IF_STMT> raise InvalidArgumentValueError(""Invalid --cluster-resource-group '%s': resource group must not exist."" % namespace.cluster_resource_group)",if client.resource_groups.check_existence(namespace.cluster_resource_group):
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: <IF_STMT> done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",if left == 0:
"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] if len(set(lengths)) > 1: raise ValueError('Not all values of X are of equal length.') x_len = lengths[0] else: x_len = len(X) if y is not None: <IF_STMT> raise ValueError('X and y are not of equal length.') if self.regression and y is not None and (y.ndim == 1): y = y.reshape(-1, 1) return (X, y)",if len(y) != x_len:
"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): if node.tag_type == OPEN_TAG: open_tags += 1 <IF_STMT> open_tags -= 1 elif isinstance(node, HtmlDataFragment) and node.is_text_content and (open_tags == 0): text.append(html_body[node.start:node.end]) return text",elif node.tag_type == CLOSE_TAG:
"def _get_spyne_type(cls_name, k, v): try: v = NATIVE_MAP.get(v, v) except TypeError: return try: subc = issubclass(v, ModelBase) or issubclass(v, SelfReference) except: subc = False if subc: if issubclass(v, Array) and len(v._type_info) != 1: raise Exception('Invalid Array definition in %s.%s.' % (cls_name, k)) <IF_STMT> raise Exception('Please specify the number of dimensions') return v","elif issubclass(v, Point) and v.Attributes.dim is None:"
"def customize(cls, **kwargs): """"""return a class with some existing attributes customized"""""" for name, value in kwargs.iteritems(): <IF_STMT> raise TransportError('you cannot customize the protected attribute %s' % name) if not hasattr(cls, name): raise TransportError('Transport has no attribute %s' % name) NewSubClass = type('Customized_{}'.format(cls.__name__), (cls,), kwargs) return NewSubClass","if name in ['cookie', 'circuit', 'upstream', 'downstream', 'stream']:"
"def test_UNrelativize(self): import URIlib relative = self.relative + self.full_relativize for base, rel, fullpath, common in relative: URI = uriparse.UnRelativizeURL(base, rel) fullURI = URIlib.URIParser(URI) <IF_STMT> fullpath = fullpath[:-1] self.failUnlessSamePath(os.path.normcase(fullURI.path), os.path.normcase(fullpath))","if fullpath[-1] in ('/', '\\'):"
"def get_release_info(file_path=RELEASE_FILE): RELEASE_TYPE_REGEX = re.compile('^[Rr]elease [Tt]ype: (major|minor|patch)$') with open(file_path, 'r') as f: line = f.readline() match = RELEASE_TYPE_REGEX.match(line) <IF_STMT> print('The file RELEASE.md should start with `Release type` and specify one of the following values: major, minor or patch.') sys.exit(1) type_ = match.group(1) changelog = ''.join([line for line in f.readlines()]).strip() return (type_, changelog)",if not match:
"def _get_next_history_entry(self): if self._history: hist_len = len(self._history) - 1 self.history_index = min(hist_len, self.history_index + 1) index = self.history_index <IF_STMT> self.history_index += 1 return self._history[index] return ''",if self.history_index == hist_len:
"def star_op(self): """"""Put a '*' op, with special cases for *args."""""" val = '*' if self.paren_level: i = len(self.code_list) - 1 if self.code_list[i].kind == 'blank': i -= 1 token = self.code_list[i] <IF_STMT> self.op_no_blanks(val) elif token.value == ',': self.blank() self.add_token('op-no-blanks', val) else: self.op(val) else: self.op(val)",if token.kind == 'lt':
"def get_safe_settings(): """"""Returns a dictionary of the settings module, with sensitive settings blurred out."""""" settings_dict = {} for k in dir(settings): <IF_STMT> if HIDDEN_SETTINGS.search(k): settings_dict[k] = '********************' else: settings_dict[k] = getattr(settings, k) return settings_dict",if k.isupper():
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): <IF_STMT> cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if ref in self._editableChildren:
"def _handle_dependents_type(types, type_str, type_name, rel_name, row): if types[type_str[0]] is None: <IF_STMT> type_name = 'index' rel_name = row['indname'] + ' ON ' + rel_name elif type_str[0] == 'o': type_name = 'operator' rel_name = row['relname'] else: type_name = types[type_str[0]] return (type_name, rel_name)",if type_str[0] == 'i':
"def streamErrorHandler(self, conn, error): name, text = ('error', error.getData()) for tag in error.getChildren(): <IF_STMT> if tag.getName() == 'text': text = tag.getData() else: name = tag.getName() if name in stream_exceptions.keys(): exc = stream_exceptions[name] else: exc = StreamError raise exc((name, text))",if tag.getNamespace() == NS_XMPP_STREAMS:
"def _validate_names(self, settings: _SettingsType) -> None: """"""Make sure all settings exist."""""" unknown = [] for name in settings: <IF_STMT> unknown.append(name) if unknown: errors = [configexc.ConfigErrorDesc('While loading options', 'Unknown option {}'.format(e)) for e in sorted(unknown)] raise configexc.ConfigFileErrors('autoconfig.yml', errors)",if name not in configdata.DATA:
"def can_haz(self, target, credentials): """"""Check whether key-values in target are present in credentials."""""" for requirement in target: key, match = requirement.split(':', 1) check = credentials.get(key) <IF_STMT> check = [check] if match in check: return True","if check is None or isinstance(check, basestring):"
"def _recursive_fx_apply(input: dict, fx): for k, v in input.items(): <IF_STMT> v = torch.tensor(v) if isinstance(v, torch.Tensor): v = fx(v.float()) input[k] = v else: _recursive_fx_apply(v, fx)","if isinstance(v, list):"
"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: <IF_STMT> self._first_ping = False return EmptyCapabilitiesResponse() elif 'Hello0' in url and '1.2.1' in url and ('v1' in url): return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",if url.endswith('ping') and self._first_ping:
"def server_thread_fn(): server_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) server_ctx.load_cert_chain('trio-test-1.pem') server = server_ctx.wrap_socket(server_sock, server_side=True, suppress_ragged_eofs=False) while True: data = server.recv(4096) print('server got:', data) <IF_STMT> print('server waiting for client to finish everything') client_done.wait() print('server attempting to send back close-notify') server.unwrap() print('server ok') break server.sendall(data)",if not data:
"def find_hostnames(data): hostnames = [] for i in re.finditer(hostname_regex, data): h = string.lower(i.group(1)) tld = h.split('.')[-1:][0] <IF_STMT> hostnames.append(h) return hostnames",if tld in tlds:
"def Validate(self, win): textCtrl = self.GetWindow() text = textCtrl.GetValue().strip() sChar = Character.getInstance() try: <IF_STMT> raise ValueError(_t('You must supply a name for the Character!')) elif text in [x.name for x in sChar.getCharacterList()]: raise ValueError(_t('Character name already in use, please choose another.')) return True except ValueError as e: pyfalog.error(e) wx.MessageBox('{}'.format(e), _t('Error')) textCtrl.SetFocus() return False",if len(text) == 0:
def get_random_user_agent(agent_list=UA_CACHE): if not len(agent_list): ua_file = file(UA_FILE) for line in ua_file: line = line.strip() <IF_STMT> agent_list.append(line) ua = random.choice(UA_CACHE) return ua,if line:
"def _validate_action_like_for_prefixes(self, key): for statement in self._statements: <IF_STMT> if isinstance(statement[key], string_types): self._validate_action_prefix(statement[key]) else: for action in statement[key]: self._validate_action_prefix(action)",if key in statement:
"def predict(self, X): if self.regression: return self.predict_proba(X) else: y_pred = np.argmax(self.predict_proba(X), axis=1) <IF_STMT> y_pred = self.enc_.inverse_transform(y_pred) return y_pred",if self.use_label_encoder:
"def _threaded_request_tracker(self, builder): while True: event_type = self._read_q.get() <IF_STMT> return payload = {'body': b''} request_id = builder.build_record(event_type, payload, '') self._write_q.put_nowait(request_id)",if event_type is False:
"def __call__(self, value): try: super(EmailValidator, self).__call__(value) except ValidationError as e: <IF_STMT> parts = value.split('@') try: parts[-1] = parts[-1].encode('idna').decode('ascii') except UnicodeError: raise e super(EmailValidator, self).__call__('@'.join(parts)) else: raise",if value and '@' in value:
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 <IF_STMT> if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",elif not IfList:
"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): if new_id != -1: palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: <IF_STMT> palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",if self.PALETTE is None:
"def Visit_star_expr(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR) _AppendTokenSubtype(child, format_token.Subtype.VARARGS_STAR)","if isinstance(child, pytree.Leaf) and child.value == '*':"
"def create_if_compatible(cls, typ: Type, *, root: 'RootNode') -> Optional['Node']: if cls.compatible_types: target_type: Type = typ <IF_STMT> target_type = getattr(typ, '__origin__', None) or typ if cls._issubclass(target_type, cls.compatible_types): return cls(typ, root=root) return None",if cls.use_origin:
"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 if tokentype != 'id': continue while i + 1 < len(tokens) and tokens[i] == ('op', '.') and (tokens[i + 1][0] == 'id'): token += '.' + tokens[i + 1][1] i += 2 <IF_STMT> continue if token in pykeywords: continue if token[0] in '.0123456789': continue yield token",if token == '':
"def create_config_filepath(cls, visibility=None): if cls.is_local(visibility): base_path = os.path.join('.') <IF_STMT> base_path = os.path.join(base_path, '.polyaxon') cls._create_dir(base_path) elif cls.CONFIG_PATH: pass else: base_path = polyaxon_user_path() cls._create_dir(base_path)",if cls.IS_POLYAXON_DIR:
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 <IF_STMT> bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 3:
"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: <IF_STMT> records = as_dict(path + item, version, section) if records: result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",if item.endswith('/'):
"def api_read(self): result = {} files = ['my.cnf', 'debian.cnf'] directory_list = self.exec_payload('mysql_config_directory')['directory'] for _file in files: for directory in directory_list: mysql_conf = directory + _file content = self.shell.read(mysql_conf) <IF_STMT> result[mysql_conf] = content return result",if content:
"def generate(self, count=100): self.pre_generate() counter = iter(range(count)) created = 0 while True: batch = list(islice(counter, self.batch_size)) <IF_STMT> break self.do_generate(batch, self.batch_size) from_size = created created += len(batch) print('Generate %s: %s-%s' % (self.resource, from_size, created)) self.after_generate()",if not batch:
"def _normalize_fields(self, document, loader): for d in list(document.keys()): d2 = loader.expand_url(d, u'', scoped_id=False, vocab_term=True) <IF_STMT> document[d2] = document[d] del document[d]",if d != d2:
"def load_cache(filename, get_key=mangle_key): cache = {} if not os.path.exists(filename): return cache f = open(filename, 'rb') l = 0 for line in f.readlines(): l += 1 fields = line.split(b' ') <IF_STMT> sys.stderr.write('Invalid file format in [%s], line %d\n' % (filename, l)) continue cache[get_key(fields[0][1:])] = fields[1].split(b'\n')[0] f.close() return cache",if fields == None or not len(fields) == 2 or fields[0][0:1] != b':':
"def __lshift__(self, other): if not self.symbolic and type(other) is int: return RegisterOffset(self._bits, self.reg, self._to_signed(self.offset << other)) el<IF_STMT> return RegisterOffset(self._bits, self.reg, self.offset << other) else: return RegisterOffset(self._bits, self.reg, ArithmeticExpression(ArithmeticExpression.LShift, (self.offset, other)))",if self.symbolic:
"def SaveSettings(self, force=False): if self.config is not None: frame.ShellFrameMixin.SaveSettings(self) <IF_STMT> frame.Frame.SaveSettings(self, self.config) self.shell.SaveSettings(self.config)",if self.autoSaveSettings or force:
"def _parse_gene(element): for genename_element in element: if 'type' in genename_element.attrib: ann_key = 'gene_%s_%s' % (genename_element.tag.replace(NS, ''), genename_element.attrib['type']) <IF_STMT> self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",if genename_element.attrib['type'] == 'primary':
"def _write_pkg_file(self, file): with TemporaryFile(mode='w+') as tmpfd: _write_pkg_file_orig(self, tmpfd) tmpfd.seek(0) for line in tmpfd: <IF_STMT> file.write('Metadata-Version: 2.1\n') elif line.startswith('Description: '): file.write('Description-Content-Type: %s; charset=UTF-8\n' % long_description_content_type) file.write(line) else: file.write(line)",if line.startswith('Metadata-Version: '):
"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: <IF_STMT> return self.value getcurrent().throw(*self._exception) else: if self.greenlet is not None: raise ConcurrentObjectUseError('This Waiter is already used by %r' % (self.greenlet,)) self.greenlet = getcurrent() try: return self.hub.switch() finally: self.greenlet = None",if self._exception is None:
"def connect(self, *args): """"""connects to the dropbox. args[0] is the username."""""" if len(args) != 1: return 'expected one argument!' try: dbci = get_dropbox_client(args[0], False, None, None) except Exception as e: return e.message else: <IF_STMT> return ""No Dropbox configured for '{u}'."".format(u=args[0]) else: self.client = dbci return True",if dbci is None:
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: <IF_STMT> text = text.replace('\n', '<br>') return text",if '\n' in text:
def t(ret): with IPDB() as ipdb: with ipdb.eventqueue() as evq: for msg in evq: <IF_STMT> ret.append(msg) return,if msg.get_attr('IFLA_IFNAME') == 'test1984':
"def check_stmt(self, stmt): if is_future(stmt): for name, asname in stmt.names: <IF_STMT> self.found[name] = 1 else: raise SyntaxError('future feature %s is not defined' % name) stmt.valid_future = 1 return 1 return 0",if name in self.features:
"def process_pypi_option(option, option_str, option_value, parser): if option_str.startswith('--no'): setattr(parser.values, option.dest, []) else: indexes = getattr(parser.values, option.dest, []) <IF_STMT> indexes.append(_PYPI) setattr(parser.values, option.dest, indexes)",if _PYPI not in indexes:
"def modify_address(self, name, address, domain): if not self.get_entries_by_name(name, domain): raise exception.NotFound infile = open(self.filename, 'r') outfile = tempfile.NamedTemporaryFile('w', delete=False) for line in infile: entry = self.parse_line(line) <IF_STMT> outfile.write('%s   %s   %s\n' % (address, self.qualify(name, domain), entry['type'])) else: outfile.write(line) infile.close() outfile.close() shutil.move(outfile.name, self.filename)","if entry and entry['name'].lower() == self.qualify(name, domain).lower():"
"def tms_to_quadkey(self, tms, google=False): quadKey = '' x, y, z = tms if not google: y = 2 ** z - 1 - y for i in range(z, 0, -1): digit = 0 mask = 1 << i - 1 if x & mask != 0: digit += 1 <IF_STMT> digit += 2 quadKey += str(digit) return quadKey",if y & mask != 0:
"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: if _typ == typ and key is _key: flag = 0 break <IF_STMT> self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",if flag:
"def scan_error(self): """"""A string describing why the last scan failed, or None if it didn't."""""" self.acquire_lock() try: <IF_STMT> try: self._load_buf_data_once() except NotFoundInDatabase: pass return self._scan_error_cache finally: self.release_lock()",if self._scan_error_cache is None:
"def _query(self): if self._mongo_query is None: self._mongo_query = self._query_obj.to_query(self._document) <IF_STMT> if '_cls' in self._mongo_query: self._mongo_query = {'$and': [self._cls_query, self._mongo_query]} else: self._mongo_query.update(self._cls_query) return self._mongo_query",if self._cls_query:
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 <IF_STMT> n += 1 return n",if self.HasPinButton():
"def testBind(self): try: with socket.socket(socket.PF_CAN, socket.SOCK_DGRAM, socket.CAN_J1939) as s: addr = (self.interface, socket.J1939_NO_NAME, socket.J1939_NO_PGN, socket.J1939_NO_ADDR) s.bind(addr) self.assertEqual(s.getsockname(), addr) except OSError as e: <IF_STMT> self.skipTest('network interface `%s` does not exist' % self.interface) else: raise",if e.errno == errno.ENODEV:
"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes('\x00\x00\x01', self.current_size, self.current_size + 1024 * 1024 * 8) if pos is not None: padsize = pos - self.current_size <IF_STMT> yield PaddingBytes(self, 'pad[]', padsize // 8) chunk = Chunk(self, 'chunk[]') try: chunk['content/data'] except: pass yield chunk",if padsize:
"def index_modulemd_files(repo_path): merger = Modulemd.ModuleIndexMerger() for fn in sorted(os.listdir(repo_path)): <IF_STMT> continue yaml_path = os.path.join(repo_path, fn) mmd = Modulemd.ModuleIndex() mmd.update_from_file(yaml_path, strict=True) merger.associate_index(mmd, 0) return merger.resolve()",if not fn.endswith('.yaml'):
"def set_visible(self, visible=True): self._visible = visible if self._nswindow is not None: <IF_STMT> self.dispatch_event('on_resize', self._width, self._height) self.dispatch_event('on_show') self.dispatch_event('on_expose') self._nswindow.makeKeyAndOrderFront_(None) else: self._nswindow.orderOut_(None)",if visible:
"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True if self.is_computed(): status = 'computed, ' if self.error() is None: <IF_STMT> status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.value() is self:
"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == 'val': for key, value in segment.items(): if key == index[0]: return value <IF_STMT> if key.text == index[0]: return value raise Exception('Invalid state') elif index_type == 'index': return segment[index] elif index_type == 'textslice': return segment[index[0]:index[1]] elif index_type == 'key': return index[1] if strictdoc else index[0] else: raise Exception('Invalid state')","if hasattr(key, 'text'):"
"def _makeSafeAbsoluteURI(base, rel=None): if not ACCEPTABLE_URI_SCHEMES: return _urljoin(base, rel or u'') if not base: return rel or u'' if not rel: try: scheme = urlparse.urlparse(base)[0] except ValueError: return u'' <IF_STMT> return base return u'' uri = _urljoin(base, rel) if uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES: return u'' return uri",if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:
"def _write_packet(self, packet): try: for listener in self.early_outgoing_packet_listeners: listener.call_packet(packet) <IF_STMT> packet.write(self.socket, self.options.compression_threshold) else: packet.write(self.socket) for listener in self.outgoing_packet_listeners: listener.call_packet(packet) except IgnorePacket: pass",if self.options.compression_enabled:
"def rangelist_to_set(rangelist): result = set() if not rangelist: return result for x in rangelist.split(','): <IF_STMT> result.add(int(x)) continue m = re.match('^(\\d+)-(\\d+)$', x) if m: start = int(m.group(1)) end = int(m.group(2)) result.update(set(range(start, end + 1))) continue msg = 'Cannot understand data input: %s %s' % (x, rangelist) raise ValueError(msg) return result","if re.match('^(\\d+)$', x):"
"def test_device_property_logfile_isinstance(self): mock = MagicMock() with patch(builtin_string + '.open', mock): <IF_STMT> builtin_file = 'io.TextIOWrapper' else: builtin_file = builtin_string + '.file' with patch(builtin_file, MagicMock): handle = open('filename', 'r') self.dev.logfile = handle self.assertEqual(self.dev.logfile, handle)",if sys.version > '3':
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 if not start: start = stmt end = stmt <IF_STMT> pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",elif start:
"def reset_parameters(self): initialize = layers.get_initializer(self._hparams.initializer) if initialize is not None: for name, param in self.named_parameters(): <IF_STMT> initialize(param)",if name.split('.')[-1] == 'weight' and 'layer_norm' not in name:
"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import RequiredArgumentMissingError, MutuallyExclusiveArgumentError valid_combs = 'only --account-name, --name / --name / --name, --by-subscription is valid' if namespace.account_name is not None: if namespace.by_subscription is not None: raise MutuallyExclusiveArgumentError(valid_combs) <IF_STMT> raise RequiredArgumentMissingError('--name is also required') if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError('--name is also required')",if namespace.name is None:
"def DeleteDocuments(self, document_ids, response): """"""Deletes documents for the given document_ids."""""" for document_id in document_ids: <IF_STMT> document = self._documents[document_id] self._inverted_index.RemoveDocument(document) del self._documents[document_id] delete_status = response.add_status() delete_status.set_code(search_service_pb.SearchServiceError.OK)",if document_id in self._documents:
"def generate_new_element(items, prefix, numeric=False): """"""Creates a random string with prefix, that is not in 'items' list."""""" while True: <IF_STMT> candidate = prefix + generate_random_numeric(8) else: candidate = prefix + generate_random_alphanumeric(8) if not candidate in items: return candidate LOG.debug('Random collision on %s' % candidate)",if numeric:
"def generate_text_for_vocab(self, data_dir, tmp_dir): for i, sample in enumerate(self.generate_samples(data_dir, tmp_dir, problem.DatasetSplit.TRAIN)): if self.has_inputs: yield sample['inputs'] yield sample['targets'] <IF_STMT> break",if self.max_samples_for_vocab and i + 1 >= self.max_samples_for_vocab:
"def _get_ccp(config=None, config_path=None, saltenv='base'): """""" """""" if config_path: config = __salt__['cp.get_file_str'](config_path, saltenv=saltenv) <IF_STMT> raise SaltException('{} is not available'.format(config_path)) if isinstance(config, six.string_types): config = config.splitlines() ccp = ciscoconfparse.CiscoConfParse(config) return ccp",if config is False:
"def rpush(key, *vals, **kwargs): ttl = kwargs.get('ttl') cap = kwargs.get('cap') if not ttl and (not cap): _client.rpush(key, *vals) else: pipe = _client.pipeline() pipe.rpush(key, *vals) <IF_STMT> pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",if cap:
"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): <IF_STMT> if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",if mode == 'start':
"def _add_communication_type(apps, schema_editor, communication_type): Worker = apps.get_model('orchestra', 'Worker') CommunicationPreference = apps.get_model('orchestra', 'CommunicationPreference') for worker in Worker.objects.all(): communication_preference, created = CommunicationPreference.objects.get_or_create(worker=worker, communication_type=communication_type) <IF_STMT> communication_preference.methods.slack = True communication_preference.methods.email = True communication_preference.save()",if created:
def get_postgresql_driver_name(): try: driver = os.getenv('CODECHECKER_DB_DRIVER') <IF_STMT> return driver try: import psycopg2 return 'psycopg2' except Exception: import pg8000 return 'pg8000' except Exception as ex: LOG.error(str(ex)) LOG.error('Failed to import psycopg2 or pg8000 module.') raise,if driver:
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, '_viewcode_modules', {}) for modname, entry in list(modules.items()): if entry is False: continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) <IF_STMT> modules.pop(modname)",if len(used) == 0:
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: <IF_STMT> continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if not is_iterable(value):
"def _get_bucket_for_key(self, key: bytes) -> Optional[_DBValueTuple]: dbs: Iterable[PartitionDB] try: partition = self._key_index[key] dbs = [PartitionDB(partition, self._dbs[partition])] except KeyError: dbs = cast(Iterable[PartitionDB], self._dbs.items()) for partition, db in dbs: if db.key_may_exist(key)[0]: value = db.get(key) <IF_STMT> self._key_index[key] = partition return _DBValueTuple(db, value) return None",if value is not None:
def _clean(self): logger.info('Cleaning up...') if self._process is not None: if self._process.poll() is None: for _ in range(3): self._process.terminate() time.sleep(0.5) <IF_STMT> break else: self._process.kill() self._process.wait() logger.error('KILLED') if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info('Cleanup complete'),if self._process.poll() is not None:
"def _calculate_runtimes(states): results = {'runtime': 0.0, 'num_failed_states': 0, 'num_passed_states': 0} for state, resultset in states.items(): if isinstance(resultset, dict) and 'duration' in resultset: <IF_STMT> results['num_passed_states'] += 1 else: results['num_failed_states'] += 1 results['runtime'] += resultset['duration'] log.debug('Parsed state metrics: {}'.format(results)) return results",if resultset['result']:
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer if max != -1 and spaces > max: return LintProblem(token.start_mark.line + 1, next.start_mark.column, max_desc) <IF_STMT> return LintProblem(token.start_mark.line + 1, next.start_mark.column + 1, min_desc)",elif min != -1 and spaces < min:
"def getfileinfo(name): finfo = FInfo() with io.open(name, 'rb') as fp: data = fp.read(512) <IF_STMT> finfo.Type = 'TEXT' fp.seek(0, 2) dsize = fp.tell() dir, file = os.path.split(name) file = file.replace(':', '-', 1) return (file, finfo, dsize, 0)",if 0 not in data:
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == 'layers': child = dict_to_XML('layer', val, name=key) <IF_STMT> child = dict_to_XML(key, val) else: if tag == 'config': child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem","elif isinstance(val, MutableMapping):"
"def _read_bytes(self, length): buffer = b'' while length: chunk = self.request.recv(length) <IF_STMT> log.debug('Connection closed') return False length -= len(chunk) buffer += chunk return buffer",if chunk == b'':
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) <IF_STMT> continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: if init_service and init_service in dep_cnt['_deps']: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if not dep_cnts:
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: <IF_STMT> if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",if type(e) is Argument or (type(e) is Option and e.argcount):
"def do_cli(manager, options): header = ['Name', 'Description'] table_data = [header] for filter_name, filter in get_filters(): <IF_STMT> continue filter_doc = inspect.getdoc(filter) or '' table_data.append([filter_name, filter_doc]) try: table = TerminalTable(options.table_type, table_data) except TerminalTableError as e: console('ERROR: %s' % str(e)) else: console(table.output)",if options.name and (not options.name in filter_name):
"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) if b1 != b2: return False <IF_STMT> return True",if not b1:
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() <IF_STMT> return True if not mother_handle: return True return False",if not father_handle:
"def caesar_cipher(s, k): result = '' for char in s: n = ord(char) if 64 < n < 91: n = (n - 65 + k) % 26 + 65 <IF_STMT> n = (n - 97 + k) % 26 + 97 result = result + chr(n) return result",if 96 < n < 123:
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name if composite_file.description: rval = '{} ({})'.format(rval, composite_file.description) <IF_STMT> rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if composite_file.optional:
"def __str__(self): t = '' if self._name != 'root': r = f'{t * (self._level - 1)}{self._name}:\n' else: r = '' level = self._level for i, (k, v) in enumerate(self._pointer.items()): <IF_STMT> r += f'{t * self._level}{v}\n' self._level += 1 else: r += f'{t * self._level}{k}: {v} ({type(v).__name__})\n' self._level = level return r[:-1]","if isinstance(v, Config):"
"def __get_securitygroups(vm_): vm_securitygroups = config.get_cloud_config_value('securitygroups', vm_, __opts__, search_global=False) if not vm_securitygroups: return [] securitygroups = list_securitygroups() for i in range(len(vm_securitygroups)): vm_securitygroups[i] = six.text_type(vm_securitygroups[i]) <IF_STMT> raise SaltCloudNotFound(""The specified securitygroups '{0}' could not be found."".format(vm_securitygroups[i])) return vm_securitygroups",if vm_securitygroups[i] not in securitygroups:
"def assert_walk_snapshot(self, field, filespecs_or_globs, paths, ignore_patterns=None, prepare=None): with self.mk_project_tree(ignore_patterns=ignore_patterns) as project_tree: scheduler = self.mk_scheduler(rules=create_fs_rules(), project_tree=project_tree) <IF_STMT> prepare(project_tree) result = self.execute(scheduler, Snapshot, self.specs(filespecs_or_globs))[0] self.assertEqual(sorted(getattr(result, field)), sorted(paths))",if prepare:
"def _parse_rowids(self, rowids): xploded = [] rowids = [x.strip() for x in rowids.split(',')] for rowid in rowids: try: <IF_STMT> start = int(rowid.split('-')[0].strip()) end = int(rowid.split('-')[-1].strip()) xploded += range(start, end + 1) else: xploded.append(int(rowid)) except ValueError: continue return sorted(list(set(xploded)))",if '-' in rowid:
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p <IF_STMT> lemma = self.composite_dict[w, pos] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas","if (w, pos) in self.composite_dict:"
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: <IF_STMT> self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True if remove and (not add): selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if box == self.level.bounds:
"def _ensure_max_size(cls, image, max_size, interpolation): if max_size is not None: size = max(image.shape[0], image.shape[1]) <IF_STMT> resize_factor = max_size / size new_height = int(image.shape[0] * resize_factor) new_width = int(image.shape[1] * resize_factor) image = ia.imresize_single_image(image, (new_height, new_width), interpolation=interpolation) return image",if size > max_size:
"def _1_0_cloud_ips(self, method, url, body, headers): if method == 'GET': return self.test_response(httplib.OK, self.fixtures.load('list_cloud_ips.json')) elif method == 'POST': <IF_STMT> body = json.loads(body) node = json.loads(self.fixtures.load('create_cloud_ip.json')) if 'reverse_dns' in body: node['reverse_dns'] = body['reverse_dns'] return self.test_response(httplib.ACCEPTED, json.dumps(node))",if body:
"def get_formatted_stats(self): """"""Get percentage or number of rar's done"""""" if self.cur_setname and self.cur_setname in self.total_volumes: <IF_STMT> return '%02d/%02d' % (self.cur_volume, self.total_volumes[self.cur_setname]) return self.cur_volume",if self.total_volumes[self.cur_setname] >= self.cur_volume and self.cur_volume:
"def wdayset(self, year, month, day): dset = [None] * (self.yearlen + 7) i = datetime.date(year, month, day).toordinal() - self.yearordinal start = i for j in range(7): dset[i] = i i += 1 <IF_STMT> break return (dset, start, i)",if self.wdaymask[i] == self.rrule._wkst:
"def do_acquire_read_lock(self, wait=True): self.condition.acquire() try: <IF_STMT> while self.current_sync_operation is not None: self.condition.wait() elif self.current_sync_operation is not None: return False self.asynch += 1 finally: self.condition.release() if not wait: return True",if wait:
"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): <IF_STMT> return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y","if not isinstance(y, (dict, OrderedDict)):"
"def update_forum_nums_topic_post(modeladmin, request, queryset): for forum in queryset: forum.num_topics = forum.count_nums_topic() forum.num_posts = forum.count_nums_post() <IF_STMT> forum.last_post = forum.topic_set.order_by('-last_reply_on')[0].last_post else: forum.last_post = '' forum.save()",if forum.num_topics:
"def get_docname_for_node(self, node: Node) -> str: while node: if isinstance(node, nodes.document): return self.env.path2doc(node['source']) <IF_STMT> return node['docname'] else: node = node.parent return None","elif isinstance(node, addnodes.start_of_file):"
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) <IF_STMT> selected_machines.append(machine) return selected_machines",if self.locations and machine.location in self.locations:
"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: if value is True: return ['-%s' % name] <IF_STMT> if split_single_char_options: return ['-%s' % name, '%s' % value] else: return ['-%s%s' % (name, value)] elif value is True: return ['--%s' % dashify(name)] elif value is not False and value is not None: return ['--%s=%s' % (dashify(name), value)] return []","elif value not in (False, None):"
"def indent(elem, level=0): i = '\n' + level * '  ' if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + '  ' if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i el<IF_STMT> elem.tail = i",if level and (not elem.tail or not elem.tail.strip()):
"def _run_instances_op(self, op, instance_ids, **kwargs): while instance_ids: try: return self.manager.retry(op, InstanceIds=instance_ids, **kwargs) except ClientError as e: <IF_STMT> instance_ids.remove(extract_instance_id(e)) raise",if e.response['Error']['Code'] == 'IncorrectInstanceState':
"def runTest(self): self.poco(text='wait UI').click() bomb_count = 0 while True: blue_fish = self.poco('fish_emitter').child('blue') yellow_fish = self.poco('fish_emitter').child('yellow') bomb = self.poco('fish_emitter').child('bomb') fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) <IF_STMT> bomb_count += 1 if bomb_count > 3: return else: fish.click() time.sleep(2.5)",if fish is bomb:
"def lineWidth(self, lw=None): """"""Set/get width of mesh edges. Same as `lw()`."""""" if lw is not None: <IF_STMT> self.GetProperty().EdgeVisibilityOff() self.GetProperty().SetRepresentationToSurface() return self self.GetProperty().EdgeVisibilityOn() self.GetProperty().SetLineWidth(lw) else: return self.GetProperty().GetLineWidth() return self",if lw == 0:
"def _current_date_updater(doc, field_name, value): if isinstance(doc, dict): <IF_STMT> doc[field_name] = helpers.get_current_timestamp() else: doc[field_name] = mongomock.utcnow()",if value == {'$type': 'timestamp'}:
"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) <IF_STMT> return if len(data) < 1000: self.limit = 0 self.after = Object(id=int(data[-1]['user']['id'])) for element in reversed(data): await self.members.put(self.create_member(element))",if not data:
"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract(page, start_index, end_index, self.template.ignored_regions) for item in arg_to_iter(extracted): if item: <IF_STMT> item[u'_template'] = self.template.id items.append(item) return items","if isinstance(item, (ItemProcessor, dict)):"
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] if node_type not in self.available_node_types: raise ValueError(f'Unknown node type tag: {node_type}.') node_specific_config = self.available_node_types[node_type] <IF_STMT> fields = node_specific_config[fields_key] return fields",if fields_key in node_specific_config:
"def _write_all(self, writer): """"""Writes messages and insert comments here and there."""""" for msg, comment in zip_longest(self.original_messages, self.original_comments, fillvalue=None): <IF_STMT> print('writing comment: ', comment) writer.log_event(comment) if msg is not None: print('writing message: ', msg) writer(msg)",if comment is not None:
"def run_tests(): x = 5 with switch(x) as case: <IF_STMT> print('zero') print('zero') elif case(1, 2): print('one or two') elif case(3, 4): print('three or four') else: print('default') print('another')",if case(0):
"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: if isinstance(value, datetime.date): ret = value.strftime('%d/%m/%y') <IF_STMT> ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.time): ret = value.strftime('%H:%M:%S') else: ret = value return ret","elif isinstance(value, datetime.datetime):"
def database_app(request): if request.param == 'postgres_app': if not which('initdb'): pytest.skip('initdb must be on PATH for postgresql fixture') <IF_STMT> pytest.skip('psycopg2 must be installed for postgresql fixture') if request.param == 'sqlite_rabbitmq_app': if not os.environ.get('GALAXY_TEST_AMQP_INTERNAL_CONNECTION'): pytest.skip('rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset') return request.getfixturevalue(request.param),if not psycopg2:
"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) <IF_STMT> deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) if timeout < 0: n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",if timeout >= 0:
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: if name is None: all_plugins += [plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated] else: plugin = self.plugins[name] <IF_STMT> all_plugins.append(plugin) return all_plugins",if plugin.is_activated:
"def get_expected_sql(self): sql_base_path = path.join(path.dirname(path.realpath(__file__)), 'sql') for version_mapping in get_version_mapping_directories(self.server['type']): <IF_STMT> continue complete_path = path.join(sql_base_path, version_mapping['name']) if not path.exists(complete_path): continue break data_sql = '' with open(path.join(complete_path, 'test_sql_output.sql')) as fp: data_sql = fp.read() return data_sql",if version_mapping['number'] > self.server_information['server_version']:
"def _validate_headers(self, headers): if headers is None: return headers res = {} for key, value in headers.items(): if isinstance(value, (int, float)): value = str(value) <IF_STMT> raise ScriptError({'message': 'headers must be a table with strings as keys and values.Header: `{!r}:{!r}` is not valid'.format(key, value)}) res[key] = value return res","if not isinstance(key, (bytes, str)) or not isinstance(value, (bytes, str)):"
"def _get_literal_value(self, pyval): if pyval == self.vm.lookup_builtin('builtins.True'): return True elif pyval == self.vm.lookup_builtin('builtins.False'): return False elif isinstance(pyval, str): prefix, value = parser_constants.STRING_RE.match(pyval).groups()[:2] value = value[1:-1] <IF_STMT> value = compat.bytestring(value) elif 'u' in prefix and self.vm.PY2: value = compat.UnicodeType(value) return value else: return pyval",if 'b' in prefix and (not self.vm.PY2):
"def decode_query_ids(self, trans, conditional): if conditional.operator == 'and': self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split('.')[0] <IF_STMT> field = self.FIELDS[left_base] if field.id_decode: conditional.right = trans.security.decode_id(conditional.right)",if left_base in self.FIELDS:
"def testLastPhrases(self): for day in (11, 12, 13, 14, 15, 16, 17): start = datetime.datetime(2012, 11, day, 9, 0, 0) yr, mth, dy, _, _, _, wd, yd, isdst = start.timetuple() n = 4 - wd <IF_STMT> n -= 7 target = start + datetime.timedelta(days=n) self.assertExpectedResult(self.cal.parse('last friday', start.timetuple()), (target.timetuple(), 1), dateOnly=True)",if n >= 0:
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: <IF_STMT> nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",if nbChars[0] is not None:
"def getpystone(): maxpystone = 0 for pyseed in [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]: duration, pystonefloat = pystones(pyseed) maxpystone = max(maxpystone, int(pystonefloat)) <IF_STMT> break return maxpystone",if duration > 0.1:
"def _append_to_io_queue(self, data, stream_name): parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: <IF_STMT> for block in re.split('(.{%d,})' % (self._get_squeeze_threshold() + 1), part): if block: self._queued_io_events.append((block, stream_name))",if part:
"def qtTypeIdent(conn, *args): res = None value = None for val in args: <IF_STMT> val = str(val) if len(val) == 0: continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res","if not hasattr(val, '__len__'):"
"def SetVerbose(self, level): """"""Sets the verbose level."""""" try: <IF_STMT> level = int(level) if level >= 0 and level <= 3: self._verbose = level return except ValueError: pass self.Error('Verbose level (%s) must be between 0 and 3 inclusive.' % level)",if type(level) != types.IntType:
"def step(self) -> None: """"""Performs a single optimization step."""""" for group in self.param_groups: for p in group['params']: <IF_STMT> continue p.add_(p.grad, alpha=-group['lr'] * self.num_data) return None",if p.grad is None:
"def fill(self, values): if lupa.lua_type(values) != 'table': raise ScriptError({'argument': 'values', 'message': 'element:fill values is not a table', 'splash_method': 'fill'}) for key, value in values.items(): <IF_STMT> _mark_table_as_array(self.lua, value) values = self.lua.lua2python(values) return self.element.fill(values)",if lupa.lua_type(value) == 'table':
"def _gen_repr(self, buf): (print >> buf, 'def __repr__(self):') if self.argnames: fmt = COMMA.join(['%s'] * self.nargs) <IF_STMT> fmt = '(%s)' % fmt vals = ['repr(self.%s)' % name for name in self.argnames] vals = COMMA.join(vals) if self.nargs == 1: vals = vals + ',' (print >> buf, 'return ""%s(%s)"" %% (%s)' % (self.name, fmt, vals)) else: (print >> buf, 'return ""%s()""' % self.name)",if '(' in self.args:
"def render_observation(self): x = self.read_head_position label = 'Observation Grid: ' x_str = '' for j in range(-1, self.rows + 1): <IF_STMT> x_str += ' ' * len(label) for i in range(-2, self.input_width + 2): if i == x[0] and j == x[1]: x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += '\n' x_str = label + x_str return x_str",if j != -1:
"def get_module_comment(self, attrname: str) -> Optional[List[str]]: try: analyzer = ModuleAnalyzer.for_module(self.modname) analyzer.analyze() key = ('', attrname) <IF_STMT> return list(analyzer.attr_docs[key]) except PycodeError: pass return None",if key in analyzer.attr_docs:
"def tms_to_quadkey(self, tms, google=False): quadKey = '' x, y, z = tms if not google: y = 2 ** z - 1 - y for i in range(z, 0, -1): digit = 0 mask = 1 << i - 1 <IF_STMT> digit += 1 if y & mask != 0: digit += 2 quadKey += str(digit) return quadKey",if x & mask != 0:
"def test_enumerate(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i * 4)) async for i, value in stream.enumerate(): current_event = stream.current_event assert i == current_event.key assert value == i * 4 <IF_STMT> break assert await channel_empty(stream.channel)",if i >= 99:
"def print_messages(self): output_reports = self.config.get_output_report() for report in output_reports: output_format, output_files = report self.summary['formatter'] = output_format formatter = FORMATTERS[output_format](self.summary, self.messages, self.config.profile) <IF_STMT> self.write_to(formatter, sys.stdout) for output_file in output_files: with open(output_file, 'w+') as target: self.write_to(formatter, target)",if not output_files:
"def eval_metrics(self): for task in self.task_list: <IF_STMT> return [metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY, metrics.Metrics.ROUGE_2_F, metrics.Metrics.ROUGE_L_F] return [metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY]",if 'summarize' in task.name:
"def _getBuildRequestForBrdict(self, brdict): breq = self.breqCache.get(brdict['buildrequestid']) if not breq: breq = (yield BuildRequest.fromBrdict(self.master, brdict)) <IF_STMT> self.breqCache[brdict['buildrequestid']] = breq defer.returnValue(breq)",if breq:
"def _stash_splitter(states): keep, split = ([], []) if state_func is not None: for s in states: ns = state_func(s) <IF_STMT> split.append(ns) elif isinstance(ns, (list, tuple, set)): split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return (keep, split)","if isinstance(ns, SimState):"
"def sequence_to_text(sequence): """"""Converts a sequence of IDs back to a string"""""" result = '' for symbol_id in sequence: <IF_STMT> s = _id_to_symbol[symbol_id] if len(s) > 1 and s[0] == '@': s = '{%s}' % s[1:] result += s return result.replace('}{', ' ')",if symbol_id in _id_to_symbol:
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] <IF_STMT> source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",if mod_type == imp.PY_SOURCE:
"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info('Checking for %s' % wafvendor) if self.wafdetections[wafvendor](self): detected.append(wafvendor) <IF_STMT> break self.knowledge['wafname'] = detected return detected",if not findall:
"def SessionId(self): """"""Returns the Session ID of the process"""""" if self.Session.is_valid(): process_space = self.get_process_address_space() <IF_STMT> return obj.Object('_MM_SESSION_SPACE', offset=self.Session, vm=process_space).SessionId return obj.NoneObject('Cannot find process session')",if process_space:
"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] if c == '$' and s[i + 1] in '0123456789': s[i] = '\\' <IF_STMT> s[i] = '' i += 1 i += 1 return pattern[:0].join(s)",elif c == '\\' and s[i + 1] == '$':
"def __init__(self, coverage): self.coverage = coverage self.config = self.coverage.config self.source_paths = set() if self.config.source: for src in self.config.source: <IF_STMT> if not self.config.relative_files: src = files.canonical_filename(src) self.source_paths.add(src) self.packages = {} self.xml_out = None",if os.path.exists(src):
"def populate_vol_format(self): rhel6_file_whitelist = ['raw', 'qcow2', 'qed'] model = self.widget('vol-format').get_model() model.clear() formats = self.vol_class.formats if hasattr(self.vol_class, 'create_formats'): formats = getattr(self.vol_class, 'create_formats') if self.vol_class == Storage.FileVolume and (not self.conn.rhel6_defaults_caps()): newfmts = [] for f in rhel6_file_whitelist: <IF_STMT> newfmts.append(f) formats = newfmts for f in formats: model.append([f, f])",if f in formats:
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None <IF_STMT> file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if os.path.exists('file_sources.json'):
"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] <IF_STMT> result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y",if len(x) > len(y):
def copy_dicts(dct): if '_remote_data' in dct: dsindex = dct['_remote_data']['_content'].dsindex newdct = dct.copy() newdct['_remote_data'] = {'_content': dsindex} return list(newdct.items()) elif '_data' in dct: newdct = dct.copy() newdata = copy_dicts(dct['_data']) <IF_STMT> newdct['_data'] = newdata return list(newdct.items()) return None,if newdata:
"def _import_epic_activity(self, project_data, taiga_epic, epic, options): offset = 0 while True: activities = self._client.get('/projects/{}/epics/{}/activity'.format(project_data['id'], epic['id']), {'envelope': 'true', 'limit': 300, 'offset': offset}) offset += 300 for activity in activities['data']: self._import_activity(taiga_epic, activity, options) <IF_STMT> break",if len(activities['data']) < 300:
"def __get__(self, instance, instance_type=None): if instance: <IF_STMT> rel_obj = self.get_obj(instance) if rel_obj: instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",if self.att_name not in instance._obj_cache:
"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: if url.startswith('https://'): url = url[8:] if not url.startswith('http://'): url = 'http://' + url <IF_STMT> download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if playlist:
"def _mksubs(self): self._subs = {} commit_dir = CommitDir(self, '.commit') self._subs['.commit'] = commit_dir tag_dir = TagDir(self, '.tag') self._subs['.tag'] = tag_dir for name, sha in git.list_refs(): <IF_STMT> name = name[11:] date = git.rev_get_date(sha.encode('hex')) n1 = BranchList(self, name, sha) n1.ctime = n1.mtime = date self._subs[name] = n1",if name.startswith('refs/heads/'):
"def readAtOffset(self, offset, size, shortok=False): ret = b'' self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) <IF_STMT> if not shortok: return None return ret ret += x return ret",if x == b'':
"def remove_indent(self): """"""Remove one tab-width of blanks from the previous token."""""" w = abs(self.tab_width) if self.result: s = self.result[-1] <IF_STMT> self.result.pop() s = s.replace('\t', ' ' * w) if s.startswith('\n'): s2 = s[1:] self.result.append('\n' + s2[:-w]) else: self.result.append(s[:-w])",if s.isspace():
"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: if kwargs.get('in_place', False): self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: <IF_STMT> self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()",if '_create_temporary' in traceback.format_exc():
def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree['activities'].values(): <IF_STMT> _collect_manual_intervention_nodes(act['pipeline']) elif act['component']['code'] in MANUAL_INTERVENTION_COMP_CODES: manual_intervention_nodes.add(act['id']),if act['type'] == 'SubProcess':
"def banned(): if request.endpoint == 'views.themes': return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() <IF_STMT> return (render_template('errors/403.html', error='You have been banned from this CTF'), 403) if team and team.banned: return (render_template('errors/403.html', error='Your team has been banned from this CTF'), 403)",if user and user.banned:
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == 'ANY': if v == 'ANY': self._definition = [] <IF_STMT> self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",elif v in self._definition:
"def save(self, learner, file_name): """"""Save the model to location specified in file_name."""""" with open(file_name, 'wb') as f: <IF_STMT> learner.inference_cache_, tmp = (None, learner.inference_cache_) pickle.dump(learner, f, -1) learner.inference_cache_ = tmp else: pickle.dump(learner, f, -1)","if hasattr(learner, 'inference_cache_'):"
"def __init__(self, exprs, savelist=False): super(ParseExpression, self).__init__(savelist) if isinstance(exprs, _generatorType): exprs = list(exprs) if isinstance(exprs, basestring): self.exprs = [ParserElement._literalStringClass(exprs)] elif isinstance(exprs, collections.Iterable): exprs = list(exprs) <IF_STMT> exprs = map(ParserElement._literalStringClass, exprs) self.exprs = list(exprs) else: try: self.exprs = list(exprs) except TypeError: self.exprs = [exprs] self.callPreparse = False","if all((isinstance(expr, basestring) for expr in exprs)):"
"def find(self, back=False): flags = 0 <IF_STMT> flags = QTextDocument.FindBackward if self.csBox.isChecked(): flags = flags | QTextDocument.FindCaseSensitively text = self.searchEdit.text() if not self.findMain(text, flags): if text in self.editBoxes[self.ind].toPlainText(): cursor = self.editBoxes[self.ind].textCursor() if back: cursor.movePosition(QTextCursor.End) else: cursor.movePosition(QTextCursor.Start) self.editBoxes[self.ind].setTextCursor(cursor) self.findMain(text, flags)",if back:
"def _load_storage(self): self._storage = {} for row in self('SELECT object, resource, amount FROM storage'): ownerid = int(row[0]) <IF_STMT> self._storage[ownerid].append(row[1:]) else: self._storage[ownerid] = [row[1:]]",if ownerid in self._storage:
"def parse_chunked(self, unreader): size, rest = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() <IF_STMT> raise NoMoreData() yield rest[:size] rest = rest[size:] while len(rest) < 2: rest += unreader.read() if rest[:2] != b'\r\n': raise ChunkMissingTerminator(rest[:2]) size, rest = self.parse_chunk_size(unreader, data=rest[2:])",if not rest:
"def _augment_batch_(self, batch, random_state, parents, hooks): for column in batch.columns: <IF_STMT> for i, cbaoi in enumerate(column.value): column.value[i] = cbaoi.clip_out_of_image_() return batch","if column.name in ['keypoints', 'bounding_boxes', 'polygons', 'line_strings']:"
"def to_nim(self): if self.is_pointer == 2: s = 'cstringArray' if self.type == 'GLchar' else 'ptr pointer' else: s = self.type <IF_STMT> default = 'ptr ' + s s = self.NIM_POINTER_MAP.get(s, default) return s",if self.is_pointer == 1:
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) <IF_STMT> self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",if os.path.isfile(file) or os.path.islink(file):
"def remove(self, event): try: self._events_current_sweep.remove(event) <IF_STMT> assert event.in_sweep == True assert event.other.in_sweep == True event.in_sweep = False event.other.in_sweep = False return True except KeyError: if USE_DEBUG: assert event.in_sweep == False assert event.other.in_sweep == False return False",if USE_DEBUG:
"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue <IF_STMT> attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrname == 'salt_version':
"def _init_auxiliary_head(self, auxiliary_head): """"""Initialize ``auxiliary_head``"""""" if auxiliary_head is not None: <IF_STMT> self.auxiliary_head = nn.ModuleList() for head_cfg in auxiliary_head: self.auxiliary_head.append(builder.build_head(head_cfg)) else: self.auxiliary_head = builder.build_head(auxiliary_head)","if isinstance(auxiliary_head, list):"
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) <IF_STMT> parts.append(param.type) out += [' : '.join(parts)] if param.desc and ''.join(param.desc).strip(): out += self._str_indent(param.desc) out += [''] return out",if param.type:
"def _set_handler(self, name, handle=None, obj=None, constructor_args=(), constructor_kwds={}): if handle is None: handle = obj is not None if handle: handler_class = self.handler_classes[name] <IF_STMT> newhandler = handler_class(obj) else: newhandler = handler_class(*constructor_args, **constructor_kwds) else: newhandler = None self._replace_handler(name, newhandler)",if obj is not None:
"def _extract_subtitles(src): subtitles = {} for caption in try_get(src, lambda x: x['captions'], list) or []: subtitle_url = url_or_none(caption.get('uri')) <IF_STMT> lang = caption.get('language', 'deu') subtitles.setdefault(lang, []).append({'url': subtitle_url}) return subtitles",if subtitle_url:
"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): <IF_STMT> keys = [x.split('(')[0] for x in struct.keys()] res.extend(keys) for key in struct: if key in IGNORED_KEYS: logging.debug('Ignored: %s: %s', key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",if not ignore_first_level:
"def create_dir(path): curr_path = None for p in path: if curr_path is None: curr_path = os.path.abspath(p) else: curr_path = os.path.join(curr_path, p) <IF_STMT> os.mkdir(curr_path)",if not os.path.exists(curr_path):
"def dataToDumpFile(dumpFile, data): try: dumpFile.write(data) dumpFile.flush() except IOError as ex: if 'No space left' in getUnicode(ex): errMsg = 'no space left on output device' logger.error(errMsg) <IF_STMT> errMsg = 'permission denied when flushing dump data' logger.error(errMsg) else: errMsg = ""error occurred when writing dump data to file ('%s')"" % getUnicode(ex) logger.error(errMsg)",elif 'Permission denied' in getUnicode(ex):
"def elements(self, top): res = [] for part in self.parts: <IF_STMT> res.append(name_or_ref(part, top)) else: if isinstance(part, Extension): res.append(part.base) res.extend(part.elements(top)) return res","if isinstance(part, Element):"
"def _parse_param_value(name, datatype, default): if datatype == 'bool': if default.lower() == 'true': return True <IF_STMT> return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': if type(default) == float: return default else: return float(default) else: return str(default)",elif default.lower() == 'false':
"def dvmethod(c, dx, doAST=False): for m in c.get_methods(): mx = dx.get_method(m) ms = DvMethod(mx) ms.process(doAST=doAST) <IF_STMT> assert ms.get_ast() is not None assert isinstance(ms.get_ast(), dict) assert 'body' in ms.get_ast() else: assert ms.get_source() is not None",if doAST:
"def _repr_pretty_(self, p, cycle): if cycle: return '{{...}' with p.group(2, '{', '}'): p.breakable('') for idx, key in enumerate(self._items): <IF_STMT> p.text(',') p.breakable() value = self._items[key] p.pretty(key) p.text(': ') if isinstance(value, bytes): value = trimmed_repr(value) p.pretty(value) p.breakable('')",if idx:
"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean('browsers', 'rating_confirm_multiple'): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _('_Remove Rating'), count, None) <IF_STMT> return reset = [] for song in songs: if '~#rating' in song: del song['~#rating'] reset.append(song) librarian.changed(reset)",if dialog.run() != Gtk.ResponseType.YES:
"def get_or_create_place(self, place_name): """"""Return the requested place object tuple-packed with a new indicator."""""" LOG.debug('get_or_create_place: looking for: %s', place_name) for place_handle in self.db.iter_place_handles(): place = self.db.get_place_from_handle(place_handle) place_title = place_displayer.display(self.db, place) <IF_STMT> return (0, place) place = Place() place.set_title(place_name) place.name = PlaceName(value=place_name) self.db.add_place(place, self.trans) return (1, place)",if place_title == place_name:
def _skip_trivial(constraint_data): if skip_trivial_constraints: <IF_STMT> if constraint_data.variables is None: return True elif constraint_data.body.polynomial_degree() == 0: return True return False,"if isinstance(constraint_data, LinearCanonicalRepn):"
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): <IF_STMT> del data[i] if is_tuple: return tuple(data) else: return data else: return None",if i < len(data) and i > -1:
"def test_case_insensitivity(self): with support.EnvironmentVarGuard() as env: env.set('PYTHONCASEOK', '1') <IF_STMT> self.skipTest('os.environ changes not reflected in _os.environ') loader = self.find_module() self.assertTrue(hasattr(loader, 'load_module'))",if b'PYTHONCASEOK' not in _bootstrap._os.environ:
def field_spec(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.field_spec_ is None: self.field_spec_ = FieldSpec() finally: self.lazy_init_lock_.release() return self.field_spec_,if self.field_spec_ is None:
"def reduce(self, f, init): for x in range(self._idx, rt.count(self._w_array)): <IF_STMT> return rt.deref(init) init = f.invoke([init, rt.nth(self._w_array, rt.wrap(x))]) return init",if rt.reduced_QMARK_(init):
def _find(event: E) -> None: values = list(self.values) for value in values[self._selected_index + 1:] + values: text = fragment_list_to_text(to_formatted_text(value[1])).lower() <IF_STMT> self._selected_index = self.values.index(value) return,if text.startswith(event.data.lower()):
def check_permissions(): if platform_os() != 'Windows': <IF_STMT> print(localization.lang_check_permissions['permissions_granted']) else: print(localization.lang_check_permissions['permissions_denied']) exit() else: print(localization.lang_check_permissions['windows_warning']) exit(),if getuid() == 0:
"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition('.') if dot: if module_name: <IF_STMT> dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: logging.warning('Empty package name: %s', name)",if module_name in dependencies:
"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) if content == 'Salted__': data = StringIO() <IF_STMT> self.encryptor.decrypt(db, data) else: raise EncryptionError('Encrpyted credential storage: {}'.format(self.db)) return json.loads(data.getvalue()) else: return json.load(db) except: return {'creds': []}",if self.encryptor:
"def _parse(self, stream, context): obj = [] try: context_for_subcon = context if self.subcon.conflags & self.FLAG_COPY_CONTEXT: context_for_subcon = context.__copy__() while True: subobj = self.subcon._parse(stream, context_for_subcon) <IF_STMT> break obj.append(subobj) except ConstructError as ex: raise ArrayError('missing terminator', ex) return obj","if self.predicate(subobj, context):"
"def is_active_for_user(self, user): is_active = super(AbstractUserFlag, self).is_active_for_user(user) if is_active: return is_active user_ids = self._get_user_ids() if hasattr(user, 'pk') and user.pk in user_ids: return True if hasattr(user, 'groups'): group_ids = self._get_group_ids() if group_ids: user_groups = set(user.groups.all().values_list('pk', flat=True)) <IF_STMT> return True return None",if group_ids.intersection(user_groups):
"def lookup_member(self, member_name): document_choices = self.choices or [] for document_choice in document_choices: doc_and_subclasses = [document_choice] + document_choice.__subclasses__() for doc_type in doc_and_subclasses: field = doc_type._fields.get(member_name) <IF_STMT> return field",if field:
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True <IF_STMT> return True return False",if not mother_handle:
"def init_weights(self): for m in self.modules(): if isinstance(m, nn.Linear): normal_init(m, std=0.01) if isinstance(m, nn.Conv3d): xavier_init(m, distribution='uniform') <IF_STMT> constant_init(m, 1)","if isinstance(m, nn.BatchNorm3d):"
def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == 'resnet_tf': <IF_STMT> lrn_rate = hparams.mom_lrn elif step_num < 30000: lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate,if step_num < hparams.lrn_step:
"def token_producer(source): token = source.read_uint8() while token is not None: if is_push_data_token(token): yield DataToken(read_data(token, source)) <IF_STMT> yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",elif is_small_integer(token):
"def user_info(oicsrv, userdb, sub, client_id='', user_info_claims=None): identity = userdb[sub] if user_info_claims: result = {} for key, restr in user_info_claims['claims'].items(): try: result[key] = identity[key] except KeyError: <IF_STMT> raise Exception(""Missing property '%s'"" % key) else: result = identity return OpenIDSchema(**result)",if restr == {'essential': True}:
"def _helpSlot(self, *args): help_text = 'Filters are applied to packets in both direction.\n\n' filter_nb = 0 for filter in self._filters: help_text += '{}: {}'.format(filter['name'], filter['description']) filter_nb += 1 <IF_STMT> help_text += '\n\n' QtWidgets.QMessageBox.information(self, 'Help for filters', help_text)",if len(self._filters) != filter_nb:
"def find_user_theme(self, name: str) -> Theme: """"""Find a theme named as *name* from latex_theme_path."""""" for theme_path in self.theme_paths: config_path = path.join(theme_path, name, 'theme.conf') <IF_STMT> try: return UserTheme(name, config_path) except ThemeError as exc: logger.warning(exc) return None",if path.isfile(config_path):
"def decompress(self, value): if value: <IF_STMT> if value.country_code and value.national_number: return ['+%d' % value.country_code, national_significant_number(value)] else: return value.split('.') return [None, '']",if type(value) == PhoneNumber:
"def update_prevdoc_status(self, flag): for quotation in list(set([d.prevdoc_docname for d in self.get('items')])): <IF_STMT> doc = frappe.get_doc('Quotation', quotation) if doc.docstatus == 2: frappe.throw(_('Quotation {0} is cancelled').format(quotation)) doc.set_status(update=True) doc.update_opportunity()",if quotation:
"def map(item): if item.deleted: return exploration = exp_fetchers.get_exploration_from_model(item) for state_name, state in exploration.states.items(): hints_length = len(state.interaction.hints) <IF_STMT> exp_and_state_key = '%s %s' % (item.id, state_name.encode('utf-8')) yield (python_utils.UNICODE(hints_length), exp_and_state_key)",if hints_length > 0:
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) <IF_STMT> selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines","if self.tags and self._tags_match(machine.tags, self.tags):"
"def _ripple_trim_compositors_move(self, delta): comp_ids = self.multi_data.moved_compositors_destroy_ids tracks_compositors = _get_tracks_compositors_list() track_moved = self.multi_data.track_affected for i in range(1, len(current_sequence().tracks) - 1): if not track_moved[i - 1]: continue track_comps = tracks_compositors[i - 1] for comp in track_comps: <IF_STMT> comp.move(delta)",if comp.destroy_id in comp_ids:
def stream_docker_log(log_stream): async for line in log_stream: if 'stream' in line and line['stream'].strip(): logger.debug(line['stream'].strip()) <IF_STMT> logger.debug(line['status'].strip()) elif 'error' in line: logger.error(line['error'].strip()) raise DockerBuildError,elif 'status' in line:
"def create_keyfile(self, keyfile, size=64, force=False): if force or not os.path.exists(keyfile): keypath = os.path.dirname(keyfile) <IF_STMT> os.makedirs(keypath) subprocess.run(['dd', 'if=/dev/random', f'of={keyfile}', f'bs={size}', 'count=1'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)",if not os.path.exists(keypath):
"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num elif self.op == '-': self.current -= num <IF_STMT> self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '*':
"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = (expr.real, expr.imag) <IF_STMT> real = Integer(0) if -delta < imag.get_float_value() < delta: imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",if -delta < real.get_float_value() < delta:
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists('file_sources.json'): file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) <IF_STMT> file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if file_sources_as_dict is not None:
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): <IF_STMT> if tag.user: tts[name] = '%ssort' % name if tag.internal: tts['~%s' % name] = '~%ssort' % name return tts",if tag.has_sort:
"def __init__(self, **kwargs): if self.name is None: raise RuntimeError('RenderPrimitive cannot be used directly') self.option_values = {} for key, val in kwargs.items(): <IF_STMT> raise ValueError(""primitive `{0}' has no option `{1}'"".format(self.name, key)) self.option_values[key] = val for name, (description, default) in self.options.items(): if not name in self.option_values: self.option_values[name] = default",if not key in self.options:
"def modify_bottle_params(self, output_stride=None): if output_stride is not None and output_stride % 2 != 0: raise Exception('output stride must to be even number') if output_stride is None: return else: stride = 2 for i, _cfg in enumerate(self.cfg): stride = stride * _cfg[-1] <IF_STMT> s = 1 self.cfg[i][-1] = s",if stride > output_stride:
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) <IF_STMT> ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",elif is_iterable(value):
"def make_shares(self, plaintext): share_arrays = [] for i, p in enumerate(plaintext): share_array = self.make_byte_shares(p) for sa in share_array: <IF_STMT> share_arrays.append(array.array('H')) current_share_array = sa current_share_array.append(sa) return share_arrays",if i == 0:
"def populate(self, item): path = self.getItemPath(item) value = self.getValue(path) for name in sorted(value.__dict__.keys()): <IF_STMT> continue child = getattr(value, name, None) if hasattr(child, '__dict__'): item.addChild(name, True) else: item.addChild(name, False)",if name[:2] == '__' and name[-2:] == '__':
def __repr__(self): try: if self._semlock._is_mine(): name = current_process().name <IF_STMT> name += '|' + threading.current_thread().name elif self._semlock._get_value() == 1: name = 'None' elif self._semlock._count() > 0: name = 'SomeOtherThread' else: name = 'SomeOtherProcess' except Exception: name = 'unknown' return '<Lock(owner=%s)>' % name,if threading.current_thread().name != 'MainThread':
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): """"""Add data to be displayed in the buffer."""""" self.values.extend(lines) if scroll_end: <IF_STMT> self.start_display_at = len(self.values) - len(self._my_widgets) elif scroll_if_editing: self.start_display_at = len(self.values) - len(self._my_widgets)",if not self.editing:
"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): if dep.repo is None: continue <IF_STMT> continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())","if not isinstance(dep.repo, WarehouseBaseRepo):"
"def _apply_flag_attrs(src_flag, dest_flag): baseline_flag = FlagDef('', {}, None) for name in dir(src_flag): <IF_STMT> continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) if dest_val == baseline_val: setattr(dest_flag, name, getattr(src_flag, name))",if name[:1] == '_':
"def out(parent, attr, indent=0): val = getattr(parent, attr) prefix = '%s%s:' % (' ' * indent, attr.replace('_', '-')) if val is None: cli.out(prefix) else: <IF_STMT> val = [flag_util.encode_flag_val(c.value) for c in val] cli.out('%s %s' % (prefix, flag_util.encode_flag_val(val)))",if attr == 'choices':
"def add_cand_to_check(cands): for cand in cands: x = cand.creator if x is None: continue <IF_STMT> heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",if x not in fan_out:
"def task_tree_lines(task=None): if task is None: task = current_root_task() rendered_children = [] nurseries = list(task.child_nurseries) while nurseries: nursery = nurseries.pop() nursery_children = _rendered_nursery_children(nursery) <IF_STMT> nested = _render_subtree('(nested nursery)', rendered_children) nursery_children.append(nested) rendered_children = nursery_children return _render_subtree(task.name, rendered_children)",if rendered_children:
"def lock_workspace(build_dir): _BUILDING_LOCK_FILE = '.blade.building.lock' lock_file_fd, ret_code = lock_file(os.path.join(build_dir, _BUILDING_LOCK_FILE)) if lock_file_fd == -1: <IF_STMT> console.fatal('There is already an active building in current workspace.') else: console.fatal('Lock exception, please try it later.') return lock_file_fd",if ret_code == errno.EAGAIN:
"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data['features']), 2) for feature in response.data['features']: self.assertIn('bbox', feature) fid = feature['id'] if fid == 1: self.assertEqual(feature['bbox'], self.bl1.bbox_geometry.extent) <IF_STMT> self.assertEqual(feature['bbox'], self.bl2.bbox_geometry.extent) else: self.fail('Unexpected id: {0}'.format(fid)) BoxedLocation.objects.all().delete()",elif fid == 2:
"def result(): R, V = (rays, virtual_rays) if V is not None: if normalize: V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() <IF_STMT> raise ValueError('virtual rays must be linearly independent and with other rays span the ambient space.') return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if len(V) != d - R.dim() or (R + V).dim() != d:
"def search_host(self, search_string): results = [] for host_entry in self.config_data: if host_entry.get('type') != 'entry': continue if host_entry.get('host') == '*': continue searchable_information = host_entry.get('host') for key, value in six.iteritems(host_entry.get('options')): <IF_STMT> value = ' '.join(value) if isinstance(value, int): value = str(value) searchable_information += ' ' + value if search_string in searchable_information: results.append(host_entry) return results","if isinstance(value, list):"
"def test_async_iterator(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i)) received = 0 async for value in stream: assert value == received received += 1 <IF_STMT> break assert await channel_empty(stream.channel)",if received >= 100:
def has_google_credentials(): global _HAS_GOOGLE_CREDENTIALS if _HAS_GOOGLE_CREDENTIALS is None: provider = Provider('google') <IF_STMT> _HAS_GOOGLE_CREDENTIALS = False else: _HAS_GOOGLE_CREDENTIALS = True return _HAS_GOOGLE_CREDENTIALS,if provider.get_access_key() is None or provider.get_secret_key() is None:
"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() if a < b: return -1 <IF_STMT> return 0 else: raise TypeError('expected date or datetime object') return 1",elif a == b:
"def validate_weight(self, weight): try: add_acl_to_obj(self.context['user_acl'], self.category) except AttributeError: return weight if weight > self.category.acl.get('can_pin_threads', 0): <IF_STMT> raise ValidationError(_(""You don't have permission to pin threads globally in this category."")) else: raise ValidationError(_(""You don't have permission to pin threads in this category."")) return weight",if weight == 2:
"def effective(line): for b in line: if not b.cond: return else: try: val = 5 <IF_STMT> if b.ignore: b.ignore -= 1 else: return (b, True) except: return (b, False) return",if val:
"def wheelEvent(self, event): """"""Handle a wheel event."""""" if QtCore.Qt.ControlModifier & event.modifiers(): d = {'c': self.leo_c} if isQt5: point = event.angleDelta() delta = point.y() or point.x() else: delta = event.delta() <IF_STMT> zoom_out(d) else: zoom_in(d) event.accept() return QtWidgets.QTextBrowser.wheelEvent(self, event)",if delta < 0:
"def test_evname_in_mp_events_testcases(): ok = True for evname in ins.mp_events: if evname == 'version': continue for i, args in enumerate(ins.mp_events[evname]['test_cases']): <IF_STMT> msg = 'Error, for evname %s the testase #%d does not match evname' print(msg % (evname, i)) ok = False if ok: print('test_evname_in_mp_events_testcases: passed')",if evname != args[0]:
def check_database(): if len(EmailAddress.objects.all()) > 0: print('Are you sure you want to wipe the existing development database and reseed it? (Y/N)') <IF_STMT> destroy_database() else: return False else: return True,if raw_input().lower() == 'y':
"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if self._requested_namespaces is not None and self._requested_namespaces != []: for requested_namespace in self._requested_namespaces: if requested_namespace[0] is '*': return [] <IF_STMT> requested_databases.append(requested_namespace[0]) return requested_databases",elif requested_namespace[0] not in IGNORE_DBS:
"def decorated(self, *args, **kwargs): start_time = time.perf_counter() stderr = '' saved_exception = None try: yield from fn(self, *args, **kwargs) except GitSavvyError as e: stderr = e.stderr saved_exception = e finally: end_time = time.perf_counter() util.debug.log_git(args, None, '<SNIP>', stderr, end_time - start_time) <IF_STMT> raise saved_exception from None",if saved_exception:
"def is_suppressed_warning(type: str, subtype: str, suppress_warnings: List[str]) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: if '.' in warning_type: target, subtarget = warning_type.split('.', 1) else: target, subtarget = (warning_type, None) <IF_STMT> if subtype is None or subtarget is None or subtarget == subtype or (subtarget == '*'): return True return False",if target == type:
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) if j == -1: attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) <IF_STMT> return r",if reply == '!done':
"def encrypt(self, plaintext): encrypted = [] for p in _string_to_bytes(plaintext): <IF_STMT> self._remaining_block = self._aes.encrypt(self._last_precipherblock) self._last_precipherblock = [] precipherbyte = self._remaining_block.pop(0) self._last_precipherblock.append(precipherbyte) cipherbyte = p ^ precipherbyte encrypted.append(cipherbyte) return _bytes_to_string(encrypted)",if len(self._remaining_block) == 0:
"def find_symbol(self, r, globally=False): query = self.view.substr(self.view.word(r)) fname = self.view.file_name().replace('\\', '/') locations = self.view.window().lookup_symbol_in_index(query) if not locations: return try: <IF_STMT> location = [hit[2] for hit in locations if fname.endswith(hit[1])][0] return (location[0] - 1, location[1] - 1) else: return locations[0] except IndexError: return",if not globally:
"def __getslice__(self, i, j): try: <IF_STMT> j = len(self) if i < 0 or j < 0: raise dns.exception.FormError if i != j: super(WireData, self).__getitem__(i) super(WireData, self).__getitem__(j - 1) return WireData(super(WireData, self).__getslice__(i, j)) except IndexError: raise dns.exception.FormError",if j == sys.maxint:
"def main(): r = redis.StrictRedis() curr_memory = prev_memory = r.info()['used_memory'] while True: <IF_STMT> print('Delta Memory : %d, Total Memory : %d' % (curr_memory - prev_memory, curr_memory)) time.sleep(1) prev_memory = curr_memory curr_memory = r.info()['used_memory']",if prev_memory != curr_memory:
"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: <IF_STMT> self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if fname not in self._flags:
"def urls(self, version=None): """"""Returns all URLS that are mapped to this interface"""""" urls = [] for _base_url, routes in self.api.http.routes.items(): for url, methods in routes.items(): for _method, versions in methods.items(): for interface_version, interface in versions.items(): <IF_STMT> if not url in urls: urls.append(('/v{0}'.format(version) if version else '') + url) return urls",if interface_version == version and interface == self:
"def _handle_data(self, text): if self._translate: <IF_STMT> self._data.append(text) else: self._translate = False self._data = [] self._comments = []",if not text.startswith('gtk-'):
"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): <IF_STMT> continue log.info('changing mode of %s to %o', dirpath, mode) if not self.dry_run: os.chmod(dirpath, mode)",if os.path.islink(dirpath):
"def language(self): if self.lang_data: lang_data = [s if s != 'None' else None for s in self.lang_data] <IF_STMT> return Language(lang_data[0], country=lang_data[1], script=lang_data[2])",if lang_data[0]:
"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 if col == nCol: for col in range(0, nCol, 2): <IF_STMT> break if col + 2 == nCol: col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)","if not self.layout.itemAt(row, col):"
"def align_comments(tlist): tidx, token = tlist.token_next_by(i=sql.Comment) while token: pidx, prev_ = tlist.token_prev(tidx) <IF_STMT> tlist.group_tokens(sql.TokenList, pidx, tidx, extend=True) tidx = pidx tidx, token = tlist.token_next_by(i=sql.Comment, idx=tidx)","if isinstance(prev_, sql.TokenList):"
"def hook_GetVariable(ql, address, params): if params['VariableName'] in ql.env: var = ql.env[params['VariableName']] read_len = read_int64(ql, params['DataSize']) if params['Attributes'] != 0: write_int64(ql, params['Attributes'], 0) write_int64(ql, params['DataSize'], len(var)) <IF_STMT> return EFI_BUFFER_TOO_SMALL if params['Data'] != 0: ql.mem.write(params['Data'], var) return EFI_SUCCESS return EFI_NOT_FOUND",if read_len < len(var):
"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) if self._CheckMySQLConnection(): print('Successfully connected to MySQL with the given configuration.') return else: print('Error: Could not connect to MySQL with the given configuration.') retry = RetryBoolQuestion('Do you want to retry MySQL configuration?', True) <IF_STMT> raise ConfigInitError()",if not retry:
"def split_long_line_with_indent(line, max_per_line, indent): """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines."""""" words = line.split(' ') lines = [] current_line = words[0] for word in words[1:]: <IF_STMT> lines.append(current_line) current_line = ' ' * indent + word else: current_line = f'{current_line} {word}' lines.append(current_line) return '\n'.join(lines)",if len(f'{current_line} {word}') > max_per_line:
"def gen_cli(docs_dir): with open(os.path.join(docs_dir, 'CLI_template.md'), 'r') as cli_temp_file: temp_lines = cli_temp_file.readlines() lines = [] for line in temp_lines: matched = re.match('{onnx-tf.*}', line) <IF_STMT> command = matched.string.strip()[1:-1] output = subprocess.check_output(command.split(' ')).decode('UTF-8') lines.append(output) else: lines.append(line) with open(os.path.join(docs_dir, 'CLI.md'), 'w') as cli_file: cli_file.writelines(lines)",if matched:
"def read(self, size=None): if size == 0: return '' data = list() while size is None or size > 0: line = self.readline(size or -1) if not line: break <IF_STMT> size -= len(line) data.append(line) return ''.join(data)",if size is not None:
"def _get_format_and_pattern(file_path): file_path = Path(file_path) with file_path.open() as f: first_line = f.readline().strip() match = re.match('format *: *(.+)', first_line) <IF_STMT> return ('gztar', first_line, 1) return (match.group(1), f.readline().strip(), 2)",if match is None:
def remove_old_snapshot(install_dir): logging.info('Removing any old files in {}'.format(install_dir)) for file in glob.glob('{}/*'.format(install_dir)): try: if os.path.isfile(file): os.unlink(file) <IF_STMT> shutil.rmtree(file) except Exception as error: logging.error('Error: {}'.format(error)) sys.exit(1),elif os.path.isdir(file):
"def _test_forever(self, tests): while True: for test_name in tests: yield test_name if self.bad: return <IF_STMT> return",if self.ns.fail_env_changed and self.environment_changed:
"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): if line.startswith('#include') or line.startswith('%include'): line = line.split(' ')[1].strip('\'""\r\n') <IF_STMT> dep.append(line) return [i for i in dep if os.path.exists(i)]",if not ('<' in line or line in dep):
"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() <IF_STMT> key.name = name if metadata is not None: key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",if name is not None:
"def range(self, dimension, data_range=True, dimension_range=True): if self.nodes and dimension in self.nodes.dimensions(): node_range = self.nodes.range(dimension, data_range, dimension_range) <IF_STMT> path_range = self._edgepaths.range(dimension, data_range, dimension_range) return max_range([node_range, path_range]) return node_range return super(Graph, self).range(dimension, data_range, dimension_range)",if self._edgepaths:
"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) <IF_STMT> break chan.send(data) if chan in r: data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",if len(data) == 0:
"def output_layer(self, features, **kwargs): """"""Project features to the vocabulary size."""""" if self.adaptive_softmax is None: <IF_STMT> return F.linear(features, self.embed_tokens.weight) else: return F.linear(features, self.embed_out) else: return features",if self.share_input_output_embed:
"def generate(self, dest, vars): util.ensure_dir(dest) for relpath, src, template in self._file_templates: file_dest = os.path.join(dest, relpath) util.ensure_dir(os.path.dirname(file_dest)) <IF_STMT> shutil.copyfile(src, file_dest) else: _render_template(template, vars, file_dest)",if template is None:
"def _py_matching_callback(self, context, result, sender, device): d = HIDDevice.get_device(c_void_p(device)) if d not in self.devices: self.devices.add(d) for x in self.matching_observers: <IF_STMT> x.device_discovered(d)","if hasattr(x, 'device_discovered'):"
"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) <IF_STMT> del new_kwargs['encoding'] if 'errors' in kwargs: del new_kwargs['errors'] return quote(*args, **new_kwargs)",if 'encoding' in new_kwargs:
"def Set(self, attr, value): hook = getattr(self, '_set_%s' % attr, None) if hook: <IF_STMT> raise ValueError('Can only update attribute %s using the context manager.' % attr) if attr not in self._pending_hooks: self._pending_hooks.append(attr) self._pending_parameters[attr] = value else: super(Configuration, self).Set(attr, value)",if self._lock > 0:
"def on_profiles_loaded(self, profiles): cb = self.builder.get_object('cbProfile') model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() if name.endswith('.mod'): continue <IF_STMT> name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",if name.endswith('.sccprofile'):
"def get_eval_task(self, worker_id): """"""Return next evaluation (task_id, Task) tuple"""""" with self._lock: <IF_STMT> return (-1, None) self._task_id += 1 task = self._eval_todo.pop() self._doing[self._task_id] = (worker_id, task, time.time()) return (self._task_id, task)",if not self._eval_todo:
"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) <IF_STMT> if not cmd.stdout.strip(): log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) if log_cmd.check(f'docker logs for {self.path.k8s}'): print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if not cmd.check(f'docker check for {self.path.k8s}'):
"def disjoined(data): data_disjoined = None dim = len(data.shape) for d in range(dim): axes = list(range(dim)) axes.remove(d) data1d = multisum(data, axes) shape = [1 for k in range(dim)] shape[d] = len(data1d) data1d = data1d.reshape(tuple(shape)) <IF_STMT> data_disjoined = data1d else: data_disjoined = data_disjoined * data1d return data_disjoined",if d == 0:
def safe_repr(val): try: <IF_STMT> val = _obj_with_safe_repr(val) ret = repr(val) if six.PY2: ret = ret.decode('utf-8') except UnicodeEncodeError: ret = red('a %r that cannot be represented' % type(val)) else: ret = green(ret) return ret,"if isinstance(val, dict):"
"def wrapper(*args, **kwargs): resp = view_func(*args, **kwargs) if isinstance(resp, dict): ctx_params = request.environ.get('webrec.template_params') <IF_STMT> resp.update(ctx_params) template = self.jinja_env.jinja_env.get_or_select_template(template_name) return template.render(**resp) else: return resp",if ctx_params:
"def post(self, request, *args, **kwargs): contact_id = kwargs.get('pk') self.object = get_object_or_404(Contact, id=contact_id) if self.request.user.role != 'ADMIN' and (not self.request.user.is_superuser) and (self.request.user != self.object.created_by) or self.object.company != self.request.company: raise PermissionDenied else: if self.object.address_id: self.object.address.delete() self.object.delete() <IF_STMT> return JsonResponse({'error': False}) return redirect('contacts:list')",if self.request.is_ajax():
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') <IF_STMT> text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text",if '<' in text:
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': <IF_STMT> return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True",if not everythingIsUnicode(v):
"def fill(self): try: while not self.stopping.wait(self.sample_wait) and len(self.queue) < self.queue.maxlen: self.queue.append(self.parent._read()) <IF_STMT> self.parent._fire_events() self.full.set() while not self.stopping.wait(self.sample_wait): self.queue.append(self.parent._read()) if isinstance(self.parent, EventsMixin): self.parent._fire_events() except ReferenceError: pass","if self.partial and isinstance(self.parent, EventsMixin):"
"def _SetListviewTextItems(self, items): self.listview.DeleteAllItems() index = -1 for item in items: index = self.listview.InsertItem(index + 1, item[0]) data = item[1] <IF_STMT> data = '' self.listview.SetItemText(index, 1, data)",if data is None:
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: body = six.ensure_str(request.body) <IF_STMT> request.body = body.replace(old, new) return request",if old in body:
"def serialize(cls, value, *args, **kwargs): if value is None: return '' value_as_string = six.text_type(value) if SHOULD_NOT_USE_LOCALE: return value_as_string else: grouping = kwargs.get('grouping', None) has_decimal_places = value_as_string.find('.') != -1 <IF_STMT> string_format = '%d' else: decimal_places = len(value_as_string.split('.')[1]) string_format = '%.{}f'.format(decimal_places) return locale.format(string_format, value, grouping=grouping)",if not has_decimal_places:
"def review_link(request, path_obj): try: <IF_STMT> if check_permission('translate', request): text = _('Review Suggestions') else: text = _('View Suggestions') return {'href': dispatch.translate(request, path_obj.pootle_path, matchnames=['hassuggestion']), 'text': text} except IOError: pass",if path_obj.has_suggestions():
"def _migrate_key(self, key): """"""migrate key from old .dat file"""""" key_path = os.path.join(self.home_path, 'keys.dat') if os.path.exists(key_path): try: key_data = json.loads(open(key_path, 'rb').read()) <IF_STMT> self.add_key(key, key_data.get(key)) except: self.error(f""Corrupt key file. Manual migration of '{key}' required."")",if key_data.get(key):
"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: <IF_STMT> root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: if root_obj: args = self.get_callback_args(root_obj, callback) if args: yield args","if not isinstance(root_objs, Iterable):"
"def GetDefFile(self, gyp_to_build_path): """"""Returns the .def file from sources, if any.  Otherwise returns None."""""" spec = self.spec if spec['type'] in ('shared_library', 'loadable_module', 'executable'): def_files = [s for s in spec.get('sources', []) if s.endswith('.def')] <IF_STMT> return gyp_to_build_path(def_files[0]) elif len(def_files) > 1: raise Exception('Multiple .def files') return None",if len(def_files) == 1:
"def _validate_gallery(images): for image in images: image_path = image.get('image_path', '') if image_path: if not isfile(image_path): raise TypeError(f'{image_path!r} is not a valid image path.') else: raise TypeError(""'image_path' is required."") <IF_STMT> raise TypeError('Caption must be 180 characters or less.')","if not len(image.get('caption', '')) <= 180:"
"def VType(self): if 'DW_AT_type' in self.attributes: target = self.types[self.type_id] target_type = target.VType() <IF_STMT> target_type = [target_type, None] return ['Pointer', dict(target=target_type[0], target_args=target_type[1])] return ['Pointer', dict(target='Void')]","if not isinstance(target_type, list):"
"def addInPlace(self, value1, value2): for group in value2: for key in value2[group]: <IF_STMT> value1[group][key] = value2[group][key] else: value1[group][key] += value2[group][key] return value1",if key not in value1[group]:
"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): <IF_STMT> query[k] = {} if isinstance(v, list): query[k] = v else: query[k].update(v) return query",if k not in query:
"def _handled_eventtype(self, eventtype, handler): if eventtype not in known_events: log.error('The event ""%s"" is not known', eventtype) return False if known_events[eventtype].__module__.startswith('deluge.event'): <IF_STMT> return True log.error('You cannot register custom notification providers for built-in event types.') return False return True",if handler.__self__ is self:
"def get_ax_arg(uri): if not ax_ns: return u'' prefix = 'openid.' + ax_ns + '.type.' ax_name = None for name, values in self.request.arguments.iteritems(): <IF_STMT> part = name[len(prefix):] ax_name = 'openid.' + ax_ns + '.value.' + part break if not ax_name: return u'' return self.get_argument(ax_name, u'')",if values[-1] == uri and name.startswith(prefix):
"def handle_starttag(self, tag, attrs): if tag == 'base': self.base_url = dict(attrs).get('href') if self.scan_tag(tag): for attr, value in attrs: <IF_STMT> if self.strip: value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",if self.scan_attr(attr):
"def test_long_steadystate_queue_popright(self): for size in (0, 1, 2, 100, 1000): d = deque(reversed(range(size))) append, pop = (d.appendleft, d.pop) for i in range(size, BIG): append(i) x = pop() <IF_STMT> self.assertEqual(x, i - size) self.assertEqual(list(reversed(list(d))), list(range(BIG - size, BIG)))",if x != i - size:
"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) <IF_STMT> self.on_message(msg) return True self.close() except socket.error as err: if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK): pass else: self.on_error(err) return False",if msg:
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = '' <IF_STMT> root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if len(root) > 0:
"def cmp(self, other): v_is_ptr = not isinstance(self, CTypesGenericPrimitive) w_is_ptr = isinstance(other, CTypesData) and (not isinstance(other, CTypesGenericPrimitive)) if v_is_ptr and w_is_ptr: return cmpfunc(self._convert_to_address(None), other._convert_to_address(None)) elif v_is_ptr or w_is_ptr: return NotImplemented else: if isinstance(self, CTypesGenericPrimitive): self = self._value <IF_STMT> other = other._value return cmpfunc(self, other)","if isinstance(other, CTypesGenericPrimitive):"
"def get_external_addresses(self, label=None) -> List[str]: result = [] for c in self._conf['pools'].values(): <IF_STMT> if label == c['label']: result.append(c['external_address'][0]) else: result.append(c['external_address'][0]) return result",if label is not None:
"def coerce_text(v): if not isinstance(v, basestring_): <IF_STMT> attr = '__unicode__' else: attr = '__str__' if hasattr(v, attr): return unicode(v) else: return bytes(v) return v",if sys.version_info[0] < 3:
"def check_localhost(self): """"""Warn if any socket_host is 'localhost'. See #711."""""" for k, v in cherrypy.config.items(): <IF_STMT> warnings.warn(""The use of 'localhost' as a socket host can cause problems on newer systems, since 'localhost' can map to either an IPv4 or an IPv6 address. You should use '127.0.0.1' or '[::1]' instead."")",if k == 'server.socket_host' and v == 'localhost':
"def add_songs(self, filenames, library): changed = [] for i in range(len(self)): <IF_STMT> song = library[self._list[i]] self._list[i] = song changed.append(song) if changed: self._emit_changed(changed, msg='add') return bool(changed)","if isinstance(self[i], str) and self._list[i] in filenames:"
"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() if k not in keys: keys.add(k) dep = self.target_database[k] <IF_STMT> dep.attr['generate_java'] = True queue.extend(dep.deps)",if 'generate_java' in dep.attr:
"def get(self): name = request.args.get('filename') if name is not None: opts = dict() opts['type'] = 'episode' result = guessit(name, options=opts) res = dict() <IF_STMT> res['episode'] = result['episode'] else: res['episode'] = 0 if 'season' in result: res['season'] = result['season'] else: res['season'] = 0 if 'subtitle_language' in result: res['subtitle_language'] = str(result['subtitle_language']) return jsonify(data=res) else: return ('', 400)",if 'episode' in result:
def _get_error_file(self) -> Optional[str]: error_file = None min_timestamp = sys.maxsize for replicas in self.role_replicas.values(): for replica in replicas: <IF_STMT> continue mtime = os.path.getmtime(replica.error_file) if mtime < min_timestamp: min_timestamp = mtime error_file = replica.error_file return error_file,if not os.path.exists(replica.error_file):
"def findChapterNameForPosition(self, p): """"""Return the name of a chapter containing p or None if p does not exist."""""" cc, c = (self, self.c) if not p or not c.positionExists(p): return None for name in cc.chaptersDict: <IF_STMT> theChapter = cc.chaptersDict.get(name) if theChapter.positionIsInChapter(p): return name return 'main'",if name != 'main':
"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) if os.path.isfile(f_path): extension = os.path.splitext(f_path)[1] <IF_STMT> os.remove(f_path)",if extension in file_extensions:
"def execute_uncomment(self, event): cursor = self._editor.GetCurrentPos() line, pos = self._editor.GetCurLine() spaces = ' ' * self._tab_size comment = 'Comment' + spaces cpos = cursor - len(comment) lenline = len(line) if lenline > 0: idx = 0 while idx < lenline and line[idx] == ' ': idx += 1 <IF_STMT> self._editor.DeleteRange(cursor - pos + idx, len(comment)) self._editor.SetCurrentPos(cpos) self._editor.SetSelection(cpos, cpos) self.store_position()",if line[idx:len(comment) + idx].lower() == comment.lower():
"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell(critical_suite_with_citations, empty_data_context): obs = SuiteEditNotebookRenderer.from_data_context(empty_data_context).render(critical_suite_with_citations, {'path': './foo/data'}) assert isinstance(obs, dict) found_expected = False for cell in obs['cells']: <IF_STMT> source_code = cell['source'] if 'batch_kwargs = {""path"": ""../.././foo/data""}' in source_code: found_expected = True break assert found_expected",if cell['cell_type'] == 'code':
"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile(max_size=self._storage.max_memory_size, suffix='.S3Boto3StorageFile', dir=setting('FILE_UPLOAD_TEMP_DIR')) if 'r' in self._mode: self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) <IF_STMT> self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",if self._storage.gzip and self.obj.content_encoding == 'gzip':
"def _parse_filters(f_strs): filters = [] if not f_strs: return filters for f_str in f_strs: <IF_STMT> fname, fopts = f_str.split(':', 1) filters.append((fname, _parse_options([fopts]))) else: filters.append((f_str, {})) return filters",if ':' in f_str:
"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = ', '.join(orig_text.replace(', ', ',').split(',')[:-1]) tags = [] for tag in self.tags_list: <IF_STMT> if orig_text[-1] not in (',', ' '): tags.append('%s,%s' % (text, tag)) tags.append('%s, %s' % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)","if ',' in orig_text:"
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == '__pycache__': continue <IF_STMT> names.add(name.split('.')[0]) elif path.is_dir() and '.' not in name: names.add(name) if packages: packages = {package.lower().replace('-', '_') for package in packages} if len(names & packages) == len(packages): return packages return names",if name.endswith('.py'):
"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): <IF_STMT> if cred.kind == self.source.replace('ec2', 'aws'): credential = cred break elif cred.credential_type.kind != 'vault': credential = cred break return credential",if self.source in CLOUD_PROVIDERS:
"def newickize(clade): """"""Convert a node tree to a Newick tree string, recursively."""""" label = clade.name or '' if label: unquoted_label = re.match(token_dict['unquoted node label'], label) <IF_STMT> label = ""'%s'"" % label.replace('\\', '\\\\').replace(""'"", ""\\'"") if clade.is_terminal(): return label + make_info_string(clade, terminal=True) else: subtrees = (newickize(sub) for sub in clade) return '(%s)%s' % (','.join(subtrees), label + make_info_string(clade))",if not unquoted_label or unquoted_label.end() < len(label):
"def __iter__(self): for name, value in self._vars.store.data.items(): source = self._sources[name] prefix = self._get_prefix(value) name = u'{0}{{{1}}}'.format(prefix, name) <IF_STMT> yield ArgumentInfo(name, value) else: yield VariableInfo(name, value, source)",if source == self.ARGUMENT_SOURCE:
"def filepath_enumerate(paths): """"""Enumerate the file paths of all subfiles of the list of paths"""""" out = [] for path in paths: <IF_STMT> out.append(path) else: for root, dirs, files in os.walk(path): for name in files: out.append(os.path.normpath(os.path.join(root, name))) return out",if os.path.isfile(path):
"def del_(self, key): hash_ = self.hash(key) node_ = self._table[hash_] pre_node = None while node_ is not None: <IF_STMT> if pre_node is None: self._table[hash_] = node_.next else: pre_node.next = node_.next self._len -= 1 pre_node = node_ node_ = node_.next",if node_.key == key:
"def _recurse(self, base_path, rel_source, rel_zip): submodules_path = Path(base_path) / 'submodules' if not submodules_path.is_dir(): return for submodule in submodules_path.iterdir(): source_path = submodule / rel_source <IF_STMT> continue output_path = submodule / rel_zip self._build_lambdas(source_path, output_path) self._recurse(submodule, rel_source, rel_zip)",if not source_path.is_dir():
"def find_test_functions(collections): if not isinstance(collections, list): collections = [collections] functions = [] for collection in collections: if not isinstance(collection, dict): collection = vars(collection) keys = collection.keys() keys.sort() for key in keys: value = collection[key] <IF_STMT> functions.append(value) return functions","if isinstance(value, types.FunctionType) and hasattr(value, 'unittest'):"
"def __init__(self, classifier, layer_name=None, transpose=None, distance=None, copy_weights=True): super().__init__() self.copy_weights = copy_weights if layer_name is not None: self.set_weights(getattr(classifier, layer_name)) else: for x in self.possible_layer_names: layer = getattr(classifier, x, None) <IF_STMT> self.set_weights(layer) break self.distance = classifier.distance if distance is None else distance self.transpose = transpose",if layer is not None:
def multi_dev_generator(self): for data in self._data_loader(): if len(self._tail_data) < self._base_number: self._tail_data += data <IF_STMT> yield self._tail_data self._tail_data = [],if len(self._tail_data) == self._base_number:
"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) if answer == Gtk.ResponseType.YES: value = setting.Value.split('|') value.remove(edge) setting.Value = '|'.join(value) if updater: updater.UpdateSetting(setting) <IF_STMT> return False return True",if answer == Gtk.ResponseType.NO:
"def _post_process_ttl(zone): for name in zone: for record_type in zone[name]: records = zone[name][record_type] if isinstance(records, list): ttl = min([x['ttl'] for x in records]) for record in records: <IF_STMT> logger.warning('Using lowest TTL {} for the record set. Ignoring value {}'.format(ttl, record['ttl'])) record['ttl'] = ttl",if record['ttl'] != ttl:
"def __init__(self, cmds, env, cleanup=[]): self.handle = None self.cmds = cmds self.env = env if cleanup: <IF_STMT> cleanup = [cleanup] else: try: cleanup = [c for c in cleanup if callable(c)] except: cleanup = [] self.cleanup = cleanup",if callable(cleanup):
"def _parse_data_of_birth(cls, data_of_birth_string): if data_of_birth_string: format = '%m/%d/%Y' try: parsed_date = datetime.datetime.strptime(data_of_birth_string, format) return parsed_date except ValueError: <IF_STMT> raise",if data_of_birth_string.count('/') != 1:
"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == 'QTCORE': continue value = env['LIBPATH_' + var] if value: core = env[coreval] accu = [] for lib in value: <IF_STMT> continue accu.append(lib) env['LIBPATH_' + var] = accu",if lib in core:
"def throttle_status(server=None): result = AmonStruct() result.allow = False last_check = server.get('last_check') server_check_period = server.get('check_every', 60) if last_check: period_since_last_check = unix_utc_now() - last_check period_since_last_check = period_since_last_check + 15 <IF_STMT> result.allow = True else: result.allow = True return result",if period_since_last_check >= server_check_period:
"def fetch_scatter_outputs(self, task): scatteroutputs = [] for var in task['body']: if var.startswith('call'): <IF_STMT> for output in self.tasks_dictionary[task['body'][var]['task']]['outputs']: scatteroutputs.append({'task': task['body'][var]['alias'], 'output': output[0]}) return scatteroutputs",if 'outputs' in self.tasks_dictionary[task['body'][var]['task']]:
"def _add_constant_node(self, source_node): parent_ids = range(len(source_node.in_edges)) for idx in parent_ids: parent_node = self.tf_graph.get_node(source_node.in_edges[idx]) <IF_STMT> self._rename_Const(parent_node)",if parent_node.type == 'Const':
"def enableCtrls(self): for data in self.storySettingsData: name = data['name'] if name in self.ctrls: <IF_STMT> set = self.getSetting(data['requires']) for i in self.ctrls[name]: i.Enable(set not in ['off', 'false', '0'])",if 'requires' in data:
"def update_realtime(self, stdout='', stderr='', delete=False): wooey_cache = wooey_settings.WOOEY_REALTIME_CACHE if delete == False and wooey_cache is None: self.stdout = stdout self.stderr = stderr self.save() elif wooey_cache is not None: cache = django_cache[wooey_cache] <IF_STMT> cache.delete(self.get_realtime_key()) else: cache.set(self.get_realtime_key(), json.dumps({'stdout': stdout, 'stderr': stderr}))",if delete:
"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x['description'] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in(('metadata', 'batch'), x) if batches: if not isinstance(batches, (list, tuple)): batches = [batches] for batch in batches: <IF_STMT> dups.add(batch) if len(dups) > 0: raise ValueError('Batch names must be unique from sample descriptions.\nClashing batch names: %s' % sorted(list(dups)))",if batch in names:
"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 <IF_STMT> self.split = self.save elif self.split < 0.5: self.split = self.min else: self.split = self.max self.placeChilds()",if self.split <= self.min or self.split >= self.max:
"def can_read(self): if hasattr(self.file, '__iter__'): iterator = iter(self.file) head = next(iterator, None) <IF_STMT> self.repaired = [] return True if isinstance(head, str): self.repaired = itertools.chain([head], iterator) return True else: raise IOSourceError('Could not open source: %r (mode: %r)' % (self.file, self.options['mode'])) return False",if head is None:
"def _print_message_content(self, peer, data): inheaders = 1 lines = data.splitlines() for line in lines: if inheaders and (not line): peerheader = 'X-Peer: ' + peer[0] <IF_STMT> peerheader = repr(peerheader.encode('utf-8')) print(peerheader) inheaders = 0 if not isinstance(data, str): line = repr(line) print(line)","if not isinstance(data, str):"
"def connect(self): try: <IF_STMT> connection = pymysql.connect(read_default_file='/etc/mysql/conf.d/my.cnf') else: connection = pymysql.connect(read_default_file='~/.my.cnf') return connection except ValueError as e: Log.debug(self, str(e)) raise MySQLConnectionError except pymysql.err.InternalError as e: Log.debug(self, str(e)) raise MySQLConnectionError",if os.path.exists('/etc/mysql/conf.d/my.cnf'):
"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) if dest.exists(): logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() <IF_STMT> shutil.copy(src, dest)",if src.exists():
"def update(self, x, who=None, metadata=None): self._retain_refs(metadata) y = self._get_key(x) if self.keep == 'last': self._buffer.pop(y, None) self._metadata_buffer.pop(y, None) self._buffer[y] = x self._metadata_buffer[y] = metadata el<IF_STMT> self._buffer[y] = x self._metadata_buffer[y] = metadata return self.last",if y not in self._buffer:
"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k['c7n:match-type'] == 'credential': c_date = parse_date(k['last_rotated']) for ak in keys: if c_date == ak['CreateDate']: ak = dict(ak) ak['c7n:match-type'] = 'access' <IF_STMT> res.append(ak) elif k not in res: res.append(k) return res",if ak not in res:
"def _apply_flag_attrs(src_flag, dest_flag): baseline_flag = FlagDef('', {}, None) for name in dir(src_flag): if name[:1] == '_': continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) <IF_STMT> setattr(dest_flag, name, getattr(src_flag, name))",if dest_val == baseline_val:
"def _ws_keep_reading(self): import websockets.exceptions while not self._reader_stopped: try: data = await self._ws.recv() <IF_STMT> data = data.encode('UTF-8') if len(data) == 0: self._error = 'EOF' break except websockets.exceptions.ConnectionClosedError: self._error = 'EOF' break self.num_bytes_received += len(data) self._make_output_available(data, block=False)","if isinstance(data, str):"
"def to_dict(self) -> Dict[str, Any]: result = {} for field_name in self.API_FIELDS: <IF_STMT> result['stream_id'] = self.id continue elif field_name == 'date_created': result['date_created'] = datetime_to_timestamp(self.date_created) continue result[field_name] = getattr(self, field_name) result['is_announcement_only'] = self.stream_post_policy == Stream.STREAM_POST_POLICY_ADMINS return result",if field_name == 'id':
"def all_masks(cls, images, run, run_key, step): all_mask_groups = [] for image in images: <IF_STMT> mask_group = {} for k in image._masks: mask = image._masks[k] mask_group[k] = mask.to_json(run) all_mask_groups.append(mask_group) else: all_mask_groups.append(None) if all_mask_groups and (not all((x is None for x in all_mask_groups))): return all_mask_groups else: return False",if image._masks:
"def disconnect_all(listener): """"""Disconnect from all signals"""""" for emitter in listener._signal_data.emitters: for signal in emitter._signal_data.listeners: emitter._signal_data.listeners[signal] = [i for i in emitter._signal_data.listeners[signal] <IF_STMT>]","if getattr(i, '__self__', None) != listener"
"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) <IF_STMT> code = -signal.SIGTERM self.returncode = code return self.returncode",if code == TERMINATE:
"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _('Processing...')) self.pbar.set_text(progress) <IF_STMT> frac = 1.0 if frac < 0: frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",if frac > 1:
def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == '[': m = -1 aas.append('') <IF_STMT> m = 0 continue elif m == -1: aas[-1] = aas[-1] + i elif m == 0: aas.append(i) return aas,elif i == ']':
"def link(token, base_url): """"""Validation for ``link``."""""" if get_keyword(token) == 'none': return 'none' parsed_url = get_url(token, base_url) if parsed_url: return parsed_url function = parse_function(token) if function: name, args = function prototype = (name, [a.type for a in args]) args = [getattr(a, 'value', a) for a in args] <IF_STMT> return ('attr()', args[0])","if prototype == ('attr', ['ident']):"
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) <IF_STMT> return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",elif self.current_provider.kind == directory.Provider.PROVIDER_URL:
"def test_handle_single(self): self.skipTest(""Pops up windows and needs user input.. so disabled.Still worth keeping whilst we don't have unit tests for all plugins."") for id_, plugin in self.plugins.items(): <IF_STMT> self.h.plugin_enable(plugin, None) self.h.handle(id_, self.lib, self.parent, SONGS) self.h.plugin_disable(plugin)",if self.h.plugin_handle(plugin):
"def __repr__(self): attrs = [] for k in self._keydata: <IF_STMT> attrs.append('p(%d)' % (self.size() + 1,)) elif hasattr(self, k): attrs.append(k) if self.has_private(): attrs.append('private') return '<%s @0x%x %s>' % (self.__class__.__name__, id(self), ','.join(attrs))",if k == 'p':
"def apply(self, node, code, required): yield 'try:' yield from self.iterIndented(code) yield 'pass' yield 'except {}:'.format(self.exceptionString) outputVariables = node.getOutputSocketVariables() for i, s in enumerate(node.outputs): <IF_STMT> if hasattr(s, 'getDefaultValueCode'): yield f'{outputVariables[s.identifier]} = {s.getDefaultValueCode()}' else: yield f'{outputVariables[s.identifier]} = self.outputs[{i}].getDefaultValue()' yield 'pass'",if s.identifier in required:
"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: if '*' in fromlist: fromlist = list(fromlist) fromlist.remove('*') fromlist.extend(getattr(module, '__all__', [])) for x in fromlist: <IF_STMT> from_name = '{}.{}'.format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module","if isinstance(getattr(module, x, None), types.ModuleType):"
"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() <IF_STMT> r = r.decode('utf-8') msg = json.loads(r) stream = msg.get('stream') if stream is not None: await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())","if isinstance(r, bytes):"
"def add_source(self, source, name=None): """"""Adds a new data source to an existing provider."""""" if self.randomize: <IF_STMT> raise ValueError('Cannot add a non-shuffleable source to an already shuffled provider.') super().add_source(source, name=name) if self.randomize is True: self._shuffle_len = self.entries",if not source.can_shuffle():
def __str__(self): buf = [''] if self.fileName: buf.append(self.fileName + ':') if self.line != -1: if not self.fileName: buf.append('line ') buf.append(str(self.line)) <IF_STMT> buf.append(':' + str(self.column)) buf.append(':') buf.append(' ') return str('').join(buf),if self.column != -1:
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): if not line: return True if linenum > 0 and line[0] not in '\t ': return True if _has_newline(line): return True <IF_STMT> return True return False",if len(line.strip()) == 0:
"def scanHexEscape(self, prefix): code = 0 leng = 4 if prefix == 'u' else 2 for i in xrange(leng): <IF_STMT> ch = self.source[self.index] self.index += 1 code = code * 16 + HEX_CONV[ch] else: return '' return unichr(code)",if self.index < self.length and isHexDigit(self.source[self.index]):
"def _get_table_info(self, table_name): table_addr = self.addr_space.profile.get_symbol(table_name) table_size = self._get_table_info_distorm() <IF_STMT> table_size = self._get_table_info_other(table_addr, table_name) if table_size == 0: debug.error('Unable to get system call table size') return [table_addr, table_size]",if table_size == 0:
"def format_file_path(filepath): """"""Formats a path as absolute and with the correct platform separator."""""" try: is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN.match(filepath) filepath = os.path.realpath(os.path.abspath(filepath)) filepath = re.sub(BACKSLASH_REPLACE_PATTERN, '/', filepath) is_windows_drive = WINDOWS_DRIVE_PATTERN.match(filepath) <IF_STMT> filepath = filepath.capitalize() if is_windows_network_mount: filepath = '/' + filepath except: pass return filepath",if is_windows_drive:
"def _match(self, cre, s): self.mo = cre.match(s) if __debug__: <IF_STMT> self._mesg(""\tmatched r'%r' => %r"" % (cre.pattern, self.mo.groups())) return self.mo is not None",if self.mo is not None and self.debug >= 5:
"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): if not line.startswith('#'): self.sanitize_allowlist.append(line.strip()) except OSError: <IF_STMT> log.warning(""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file)",if explicit:
"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): if not self.flags.forc: raise RuntimeError('only contiguous arrays may be used as arguments to this operation') <IF_STMT> order = 'F' else: order = 'C' result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call(self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size) return result else: return self",if self.flags.f_contiguous:
"def scan_spec_conf(self, conf): if 'metadata' in conf: if 'annotations' in conf['metadata'] and conf['metadata'].get('annotations'): for annotation in conf['metadata']['annotations']: for key in annotation: <IF_STMT> if 'docker/default' in annotation[key] or 'runtime/default' in annotation[key]: return CheckResult.PASSED return CheckResult.FAILED",if 'seccomp.security.alpha.kubernetes.io/defaultProfileName' in key:
"def test_error_through_destructor(self): rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy if not IOBASE_EMITS_UNRAISABLE: self.assertIsNone(cm.unraisable) <IF_STMT> self.assertEqual(cm.unraisable.exc_type, OSError)",elif cm.unraisable is not None:
"def _dumpf(frame): if frame is None: return '<None>' else: addn = '(with trace!)' <IF_STMT> addn = ' **No Trace Set **' return 'Frame at %d, file %s, line: %d%s' % (id(frame), frame.f_code.co_filename, frame.f_lineno, addn)",if frame.f_trace is None:
"def containsBadbytes(self, value, bytecount=4): for b in self.badbytes: tmp = value <IF_STMT> b = ord(b) for i in range(bytecount): if tmp & 255 == b: return True tmp >>= 8 return False",if type(b) == str:
"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: <IF_STMT> peer.status = PEER_BAD elif peer.last_good > cutoff: peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",if peer.bad:
"def afterTest(self, test): try: self.driver.quit() except AttributeError: pass except Exception: pass if self.options.headless: <IF_STMT> try: self.display.stop() except AttributeError: pass except Exception: pass",if self.headless_active:
"def _written_variables_in_proxy(self, contract): variables = [] if contract.is_upgradeable: variables_name_written_in_proxy = self._variable_written_in_proxy() <IF_STMT> variables_in_contract = [contract.get_state_variable_from_name(v) for v in variables_name_written_in_proxy] variables_in_contract = [v for v in variables_in_contract if v] variables += variables_in_contract return list(set(variables))",if variables_name_written_in_proxy:
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') if name.startswith(expr): <IF_STMT> found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",if name not in found_names:
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): <IF_STMT> if not name.startswith('wait_until'): if is_resource_action(member): resource_methods[name] = member return resource_methods",if not name[0].isupper():
def UpdateControlState(self): active = self.demoModules.GetActiveID() for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] <IF_STMT> btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) if moduleID == modModified: self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),if moduleID == active:
"def test_controlcharacters(self): for i in range(128): c = chr(i) testString = 'string containing %s' % c if i >= 32 or c in '\r\n\t': data = plistlib.dumps(testString, fmt=plistlib.FMT_XML) <IF_STMT> self.assertEqual(plistlib.loads(data), testString) else: with self.assertRaises(ValueError): plistlib.dumps(testString, fmt=plistlib.FMT_XML) plistlib.dumps(testString, fmt=plistlib.FMT_BINARY)",if c != '\r':
"def remove_usernames(self, username: SLT[str]) -> None: with self.__lock: <IF_STMT> raise RuntimeError(f""Can't set {self.username_name} in conjunction with (already set) {self.chat_id_name}s."") parsed_username = self._parse_username(username) self._usernames -= parsed_username",if self._chat_ids:
"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize <IF_STMT> size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","elif isinstance(elem, int):"
"def before_step(self, step, feed_dict): if step == 0: for _type, mem in self.memories.items(): <IF_STMT> self.gan.session.run(tf.assign(mem['var'], mem['source']))",if 'var' in mem and 'source' in mem:
"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] <IF_STMT> if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if self.bitnum == 8:
"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for parameter_range_key, parameter_range_value in parameter_range.__dict__.items(): if parameter_range_key == 'scaling_type': continue <IF_STMT> for categorical_value in parameter_range_value: value_hp.validate(categorical_value) else: value_hp.validate(parameter_range_value)","if isinstance(parameter_range_value, list):"
"def _trackA(self, tracks): try: track, start, end = self.featureA assert track in tracks return track except TypeError: for track in tracks: for feature_set in track.get_sets(): if hasattr(feature_set, 'features'): <IF_STMT> return track return None",if self.featureA in feature_set.features.values():
"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any((fnmatch(name, pattern) for pattern in basename_ignore)): continue path = path_so_far + '/' + name if path_so_far else name if any((fnmatch(path, pattern) for pattern in path_ignore)): continue full_name = os.path.join(directory, name) <IF_STMT> for file_path in walk(full_name, path): yield file_path elif os.path.isfile(full_name): yield path",if os.path.isdir(full_name):
"def _poll_ipc_requests(self) -> None: try: <IF_STMT> return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: if os.path.isfile(filename): self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception('Problem processing ipc request', exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",if self._ipc_requests.empty():
"def test_read1(self): self.test_write() blocks = [] nread = 0 with gzip.GzipFile(self.filename, 'r') as f: while True: d = f.read1() <IF_STMT> break blocks.append(d) nread += len(d) self.assertEqual(f.tell(), nread) self.assertEqual(b''.join(blocks), data1 * 50)",if not d:
"def _target_generator(self): if self._internal_target_generator is None: <IF_STMT> return None from ....model_zoo.rcnn.rpn.rpn_target import RPNTargetGenerator self._internal_target_generator = RPNTargetGenerator(num_sample=self._num_sample, pos_iou_thresh=self._pos_iou_thresh, neg_iou_thresh=self._neg_iou_thresh, pos_ratio=self._pos_ratio, stds=self._box_norm, **self._kwargs) return self._internal_target_generator else: return self._internal_target_generator",if self._net_none:
"def time_left(self): """"""Return how many seconds are left until the timeout expires"""""" if self.is_non_blocking: return 0 elif self.is_infinite: return None else: delta = self.target_time - self.TIME() <IF_STMT> self.target_time = self.TIME() + self.duration return self.duration else: return max(0, delta)",if delta > self.duration:
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != '__init__': if not private and name.startswith('_'): continue <IF_STMT> continue setattr(cls, name, decorator(meth)) return cls",if name in butnot:
"def load_vocab(vocab_file: str) -> List: """"""Loads a vocabulary file into a dictionary."""""" vocab = collections.OrderedDict() with io.open(vocab_file, 'r', encoding='UTF-8') as file: for num, line in enumerate(file): items = convert_to_unicode(line.strip()).split('\t') <IF_STMT> break token = items[0] index = items[1] if len(items) == 2 else num token = token.strip() vocab[token] = int(index) return vocab",if len(items) > 2:
"def slice_fill(self, slice_): """"""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"""""" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): <IF_STMT> new_slice_.append(0) elif offset < len(slice_): new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",if self.squeeze_dims[i]:
"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) if remote_version > local_version: <IF_STMT> update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",if auto:
"def iter_content(self, chunk_size_bytes): while True: try: data = self._fp.read(chunk_size_bytes) except IOError as e: raise Fetcher.PermanentError('Problem reading chunk from {}: {}'.format(self._fp.name, e)) <IF_STMT> break yield data",if not data:
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: if isinstance(arg, bool): gvariant += ' {}'.format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f' {arg}' <IF_STMT> gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","elif isinstance(arg, str):"
"def _element_keywords(cls, backend, elements=None): """"""Returns a dictionary of element names to allowed keywords"""""" if backend not in Store.loaded_backends(): return {} mapping = {} backend_options = Store.options(backend) elements = elements if elements is not None else backend_options.keys() for element in elements: <IF_STMT> continue element = element if isinstance(element, tuple) else (element,) element_keywords = [] options = backend_options['.'.join(element)] for group in Options._option_groups: element_keywords.extend(options[group].allowed_keywords) mapping[element[0]] = element_keywords return mapping",if '.' in element:
"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print('V', value) <IF_STMT> param_node.selected_mode = 'int' param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = 'float' param_node.float_ = value","if isinstance(value, int):"
"def _get_oshape(indices_shape, depth, axis): oshape = [] true_axis = len(indices_shape) if axis == -1 else axis ndim = len(indices_shape) + 1 indices_index = 0 for i in range(0, ndim): <IF_STMT> oshape.append(depth) else: oshape.append(indices_shape[indices_index]) indices_index += 1 return oshape",if i == true_axis:
"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split('_', 1)[1]): <IF_STMT> value = value[len(prefix):] value = value.lstrip('_') if hasattr(self.group, value): return getattr(self.group, value) else: raise ValueError('No such constant: %s_%s' % (self.prefix, value)) else: return value",if value.startswith(prefix):
"def shuffle_unison_inplace(list_of_lists, random_state=None): if list_of_lists: assert all((len(l) == len(list_of_lists[0]) for l in list_of_lists)) <IF_STMT> random_state.permutation(len(list_of_lists[0])) else: p = np.random.permutation(len(list_of_lists[0])) return [l[p] for l in list_of_lists] return None",if random_state is not None:
"def _load_module(self): spec = self.default_module_spec module_identifier = self.module_identifier if module_identifier: impls = self.get_module_implementation_map() <IF_STMT> raise ModuleNotFound('Invalid module identifier %r in %s' % (module_identifier, force_ascii(repr(self)))) spec = impls[module_identifier] cls = load(spec, context_explanation='Loading module for %s' % force_ascii(repr(self))) options = getattr(self, self.module_options_field, None) or {} return cls(self, options)",if module_identifier not in impls:
"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): <IF_STMT> if isinstance(request, numbers.Integral): data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)",if shapes[i] is not None:
"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k['c7n:match-type'] == 'credential': c_date = parse_date(k['last_rotated']) for ak in keys: <IF_STMT> ak = dict(ak) ak['c7n:match-type'] = 'access' if ak not in res: res.append(ak) elif k not in res: res.append(k) return res",if c_date == ak['CreateDate']:
"def _is_legacy_mode(self, node): """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode."""""" script_mode = False py_version = 'py2' for kw in node.keywords: <IF_STMT> script_mode = bool(kw.value.value) if isinstance(kw.value, ast.NameConstant) else True if kw.arg == 'py_version': py_version = kw.value.s if isinstance(kw.value, ast.Str) else 'py3' return not (py_version.startswith('py3') or script_mode)",if kw.arg == 'script_mode':
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue <IF_STMT> continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) if status: statuses_by_refs[entity_ref].append(status) return statuses_by_refs",if entity_ref not in statuses_by_refs:
"def items(self): dict = {} for userdir in self.XDG_DIRS.keys(): prefix = self.get(userdir).strip('""').split('/')[0] <IF_STMT> path = os.getenv('HOME') + '/' + '/'.join(self.get(userdir).strip('""').split('/')[1:]) else: path = self.get(userdir).strip('""') dict[userdir] = path return dict.items()",if prefix:
"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: <IF_STMT> prefix_words_are_adj = False if prefix_words_are_adj: return (words[-1:], words[:-1]) else: return ([string], []) else: return ([string], [])",if att not in common_attributes:
"def extract_custom(extractor, *args, **kw): for match in extractor(*args, **kw): msg = match[2] <IF_STMT> unused = '<unused singular (hash=%s)>' % md5(msg[1].encode('utf8')).hexdigest() msg = (unused, msg[1], msg[2]) match = (match[0], match[1], msg, match[3]) yield match","if isinstance(msg, tuple) and msg[0] == '':"
"def test_convex_decomposition(self): mesh = g.get_mesh('quadknot.obj') engines = [('vhacd', g.trimesh.interfaces.vhacd.exists)] for engine, exists in engines: <IF_STMT> g.log.warning('skipping convex decomposition engine %s', engine) continue g.log.info('Testing convex decomposition with engine %s', engine) meshes = mesh.convex_decomposition(engine=engine) self.assertTrue(len(meshes) > 1) for m in meshes: self.assertTrue(m.is_watertight) g.log.info('convex decomposition succeeded with %s', engine)",if not exists:
"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(' , ') else: hasConst = not (self._const.__class__ in native_numeric_types and self._const == 0) if hasConst: idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ <IF_STMT> ostream.write(' - ') else: ostream.write(' + ')",if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0):
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] <IF_STMT> return tuple(data) else: return data else: return None",if is_tuple:
"def process_error(self, data): if data.get('error'): <IF_STMT> raise AuthCanceled(self, data.get('error_description', '')) raise AuthFailed(self, data.get('error_description') or data['error']) elif 'denied' in data: raise AuthCanceled(self, data['denied'])",if 'denied' in data['error'] or 'cancelled' in data['error']:
"def retry_http_digest_auth(self, req, auth): token, challenge = auth.split(' ', 1) chal = parse_keqv_list(parse_http_list(challenge)) auth = self.get_authorization(req, chal) if auth: auth_val = 'Digest %s' % auth <IF_STMT> return None req.add_unredirected_header(self.auth_header, auth_val) resp = self.parent.open(req) return resp","if req.headers.get(self.auth_header, None) == auth_val:"
"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() if type(sockname) is str: <IF_STMT> os.remove(sockname) self.sock = None",if os.path.exists(sockname):
"def to_nurbs(self, curves): result = [] for i, c in enumerate(curves): nurbs = SvNurbsCurve.to_nurbs(c) <IF_STMT> raise Exception(f'Curve #{i} - {c} - can not be converted to NURBS!') result.append(nurbs) return result",if nurbs is None:
"def handle_1_roomid_raffle(self, i): if i[1] in ['handle_1_room_TV', 'handle_1_room_captain']: <IF_STMT> await self.notify('post_watching_history', i[0]) await self.notify(i[1], i[0], i[2]) else: print('hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh', i)","if await self.notify('check_if_normal_room', i[0], -1):"
"def init_ps_var_partition(self): ps_vars = {} for v in self._non_embed_vars.values(): if v.name not in self._var_to_ps: self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num) ps_id = self._var_to_ps[v.name] <IF_STMT> ps_vars[ps_id] = [v] else: ps_vars[ps_id].append(v) self._ps_vars = ps_vars",if ps_id not in ps_vars:
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if 'meta-environment' in root or 'cross-canadian' in root: continue if 'qemux86copy-' in root or 'qemux86-' in root: continue <IF_STMT> f.append(os.path.join(root, name)) return f",if 'do_build' not in name and 'do_populate_sdk' not in name:
"def setSelectedLabelState(self, p): c = self.c if p and c.edit_widget(p): <IF_STMT> g.trace(self.trace_n, c.edit_widget(p), p) self.trace_n += 1 self.setDisabledHeadlineColors(p)",if 0:
"def filter_tasks(self, task_types=None, task_states=None, task_text=None): tasks = self.api.tasks(self.id).get('tasks', {}) if tasks and tasks.get('task'): return [Task(self, task) for task in tasks.get('task', []) if (not task_types or task['type'].lower() in task_types) and (not task_states or task['state'].lower() in task_states) and (not task_text or task_text.lower() in str(task).lower())] else: return []",if not task_types or task['type'].lower() in task_types
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): <IF_STMT> if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if vector:
def _transmit_from_storage(self) -> None: for blob in self.storage.gets(): if blob.lease(self._timeout + 5): envelopes = [TelemetryItem(**x) for x in blob.get()] result = self._transmit(list(envelopes)) <IF_STMT> blob.lease(1) else: blob.delete(),if result == ExportResult.FAILED_RETRYABLE:
"def load_dictionary(file): oui = {} with open(file, 'r') as f: for line in f: <IF_STMT> data = line.split('(hex)') key = data[0].replace('-', ':').lower().strip() company = data[1].strip() oui[key] = company return oui",if '(hex)' in line:
"def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True): indices = np.arange(data_ary.shape[0]) if shuffle: indices = rgen.permutation(indices) if n_batches > 1: remainder = data_ary.shape[0] % n_batches <IF_STMT> minis = np.array_split(indices[:-remainder], n_batches) minis[-1] = np.concatenate((minis[-1], indices[-remainder:]), axis=0) else: minis = np.array_split(indices, n_batches) else: minis = (indices,) for idx_batch in minis: yield idx_batch",if remainder:
"def canonical_custom_headers(self, headers): hoi = [] custom_headers = {} for key in headers: lk = key.lower() if headers[key] is not None: <IF_STMT> custom_headers[lk] = ','.join((v.strip() for v in headers.get_all(key))) sorted_header_keys = sorted(custom_headers.keys()) for key in sorted_header_keys: hoi.append('%s:%s' % (key, custom_headers[key])) return '\n'.join(hoi)",if lk.startswith('x-amz-'):
"def validate(self, data): if not data.get('reason'): message = data.get('message') if not message: if 'message' not in data: msg = serializers.Field.default_error_messages['required'] <IF_STMT> msg = serializers.Field.default_error_messages['null'] else: msg = serializers.CharField.default_error_messages['blank'] raise serializers.ValidationError({'message': [msg]}) return data",elif message is None:
def tearDown(self): try: os.chdir(self.cwd) <IF_STMT> os.remove(self.pythonexe) test_support.rmtree(self.parent_dir) finally: BaseTestCase.tearDown(self),if self.pythonexe != sys.executable:
"def update(self, value, label): if self._disabled: return try: self._progress.value = value self._label.value = label <IF_STMT> self._displayed = True display_widget(self._widget) except Exception as e: self._disabled = True logger.exception(e) wandb.termwarn('Unable to render progress bar, see the user log for details')",if not self._displayed:
"def GetBinaryOperationBinder(self, op): with self._lock: <IF_STMT> return self._binaryOperationBinders[op] b = runtime.SymplBinaryOperationBinder(op) self._binaryOperationBinders[op] = b return b",if self._binaryOperationBinders.ContainsKey(op):
"def apply(self, l, b, evaluation): """"""FromDigits[l_, b_]"""""" if l.get_head_name() == 'System`List': value = Integer(0) for leaf in l.leaves: value = Expression('Plus', Expression('Times', value, b), leaf) return value elif isinstance(l, String): value = FromDigits._parse_string(l.get_string_value(), b) <IF_STMT> evaluation.message('FromDigits', 'nlst') else: return value else: evaluation.message('FromDigits', 'nlst')",if value is None:
"def hsconn_sender(self): while not self.stop_event.is_set(): try: request = self.send_queue.get(True, 6.0) if self.socket is not None: self.socket.sendall(request) <IF_STMT> self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",if self.send_queue is not None:
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: <IF_STMT> return False elif not rline.endswith(eline): return False return True",if eline not in rline:
"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): <IF_STMT> normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): if isinstance(m, nn.Conv2d): normal_init(m, std=0.001, bias=0)","if isinstance(m, nn.ConvTranspose2d):"
"def filter_rel_attrs(field_name, **rel_attrs): clean_dict = {} for k, v in rel_attrs.items(): <IF_STMT> splitted_key = k.split('__') key = '__'.join(splitted_key[1:]) clean_dict[key] = v else: clean_dict[k] = v return clean_dict",if k.startswith(field_name + '__'):
"def cancel(self): with self._condition: <IF_STMT> self._squash(state_root=self._previous_state_hash, context_ids=[self._previous_context_id], persist=False, clean_up=True) self._cancelled = True self._condition.notify_all()",if not self._cancelled and (not self._final) and self._previous_context_id:
"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): <IF_STMT> level_ref += len(levels) if not 0 <= level_ref < len(levels): raise PatsyError('specified level %r is out of range' % (level_ref,)) return level_ref raise PatsyError('specified level %r not found' % (level_ref,))",if level_ref < 0:
"def parse_node(self, node, alias_map=None, conv=None): sql, params, unknown = self._parse(node, alias_map, conv) if unknown and conv and params: params = [conv.db_value(i) for i in params] if isinstance(node, Node): if node._negated: sql = 'NOT %s' % sql <IF_STMT> sql = ' '.join((sql, 'AS', node._alias)) if node._ordering: sql = ' '.join((sql, node._ordering)) return (sql, params)",if node._alias:
"def parse_object_id(_, values): if values: for key in values: <IF_STMT> val = values[key] if len(val) > 10: try: values[key] = utils.ObjectIdSilent(val) except: values[key] = None",if key.endswith('_id'):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_max_rows(d.getVarInt32()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if 'identifiers' in rule and rule['identifiers'] is not None: for i_type, i_value in rule['identifiers'].items(): <IF_STMT> if not checks.is_cce_value_valid('CCE-' + str(i_value)): return True return False",if i_type[0:3] == 'cce':
"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield (simple_colorize(fromdesc, 'description'), simple_colorize(todesc, 'description')) for i, line in enumerate(diffs): if line is None: <IF_STMT> yield (simple_colorize('---', 'separator'), simple_colorize('---', 'separator')) else: yield line",if i > 0:
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if '%' not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'): template = DateEpoch(lineBeginOnly=key != 'EPOCH') <IF_STMT> template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template","elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'):"
"def ref_max_pooling_2d(x, kernel, stride, ignore_border, pad): y = [] for xx in x.reshape((-1,) + x.shape[-3:]): <IF_STMT> xx = xx[np.newaxis] y += [refs.pooling_2d(xx, 'max', kernel, stride, pad, ignore_border)[np.newaxis]] y = np.vstack(y) if x.ndim == 2: y = np.squeeze(y, 1) return y.reshape(x.shape[:-3] + y.shape[1:])",if xx.ndim == 2:
"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color('Miscellaneous Topics:', 'yellow')) for pp in PAGEPATHS: if not os.path.isdir(pp): continue content = os.listdir(pp) for pn in content: <IF_STMT> name = pn[:pn.index('.')] else: name = pn print(name)",if '.' in pn:
"def justify_toggle_auto(self, event=None): c = self if c.editCommands.autojustify == 0: c.editCommands.autojustify = abs(c.config.getInt('autojustify') or 0) <IF_STMT> g.es('Autojustify on, @int autojustify == %s' % c.editCommands.autojustify) else: g.es('Set @int autojustify in @settings') else: c.editCommands.autojustify = 0 g.es('Autojustify off')",if c.editCommands.autojustify:
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <IF_STMT> result.append(token.contents.replace('%', '%%')) elif token.token_type == TOKEN_VAR: result.append('%%(%s)s' % token.contents) vars.append(token.contents) return (''.join(result), vars)",if token.token_type == TOKEN_TEXT:
"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: if operation['type'] == 'crop': width = operation['right'] - operation['left'] height = operation['bottom'] - operation['top'] <IF_STMT> width = operation['width'] height = operation['height'] return (width, height)",if operation['type'] == 'resize':
"def get_eval_matcher(self): if isinstance(self.data['match'], str): <IF_STMT> values = ['explicitDeny', 'implicitDeny'] else: values = ['allowed'] vf = ValueFilter({'type': 'value', 'key': 'EvalDecision', 'value': values, 'op': 'in'}) else: vf = ValueFilter(self.data['match']) vf.annotate = False return vf",if self.data['match'] == 'denied':
"def test_training(self): if not self.model_tester.is_training: return config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common() config.return_dict = True for model_class in self.all_model_classes: <IF_STMT> continue model = model_class(config) model.to(torch_device) model.train() inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True) loss = model(**inputs).loss loss.backward()",if model_class in MODEL_MAPPING.values():
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: <IF_STMT> reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == 'amd64': reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",if self.arch == 'i386':
"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len('hello'), len(base64mime.encode('hello', eol=''))) for size in range(15): <IF_STMT> bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len('x' * size), bsize)",if size == 0:
"def __new__(cls, dependencies): deps = check.list_param(dependencies, 'dependencies', of_type=DependencyDefinition) seen = {} for dep in deps: key = dep.solid + ':' + dep.output <IF_STMT> raise DagsterInvalidDefinitionError('Duplicate dependencies on solid ""{dep.solid}"" output ""{dep.output}"" used in the same MultiDependencyDefinition.'.format(dep=dep)) seen[key] = True return super(MultiDependencyDefinition, cls).__new__(cls, deps)",if key in seen:
"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) if len(a) == 1: return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: <IF_STMT> raise pass elif self.strict > 1: raise PermError('Empty domain-spec on exp=') return None",if self.strict > 1:
"def build(self): if self.args.get('sle_id'): self.process_sle_against_current_voucher() else: entries_to_fix = self.get_future_entries_to_fix() i = 0 while i < len(entries_to_fix): sle = entries_to_fix[i] i += 1 self.process_sle(sle) <IF_STMT> self.get_dependent_entries_to_fix(entries_to_fix, sle) if self.exceptions: self.raise_exceptions() self.update_bin()",if sle.dependant_sle_voucher_detail_no:
"def ValidateStopLatitude(self, problems): if self.stop_lat is not None: value = self.stop_lat try: if not isinstance(value, (float, int)): self.stop_lat = util.FloatStringToFloat(value, problems) except (ValueError, TypeError): problems.InvalidValue('stop_lat', value) del self.stop_lat else: <IF_STMT> problems.InvalidValue('stop_lat', value)",if self.stop_lat > 90 or self.stop_lat < -90:
"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" 'the ignore method' ignore = getattr(self, 'ignore') for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) <IF_STMT> for k1 in self.combinations[k]: if not hasattr(self, k1): setattr(self, k1, ignore)",if k in self.combinations:
"def split(self, duration, include_remainder=True): duration = _seconds_or_timedelta(duration) if duration <= timedelta(seconds=0): raise ValueError('cannot call split with a non-positive timedelta') start = self.start while start < self.end: if start + duration <= self.end: yield MayaInterval(start, start + duration) <IF_STMT> yield MayaInterval(start, self.end) start += duration",elif include_remainder:
"def get_first_field(layout, clz): for layout_object in layout.fields: if issubclass(layout_object.__class__, clz): return layout_object <IF_STMT> gf = get_first_field(layout_object, clz) if gf: return gf","elif hasattr(layout_object, 'get_field_names'):"
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if '%' not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: <IF_STMT> template = DateEpoch(lineBeginOnly=key != 'EPOCH') elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'): template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template","if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'):"
"def findOwningViewController(self, object): while object: <IF_STMT> description = fb.evaluateExpressionValue(object).GetObjectDescription() print('Found the owning view controller.\n{}'.format(description)) cmd = 'echo {} | tr -d ""\n"" | pbcopy'.format(object) os.system(cmd) return else: object = self.nextResponder(object) print('Could not find an owning view controller')",if self.isViewController(object):
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) <IF_STMT> return i idx = i else: idx += 1 return idx","if not isinstance(i, int):"
"def promtool(**kwargs): key = 'prometheus:promtool' try: path = pathlib.Path(util.setting(key)) except TypeError: yield checks.Warning('Missing setting for %s in %s ' % (key, settings.PROMGEN_CONFIG_FILE), id='promgen.W001') else: <IF_STMT> yield checks.Warning('Unable to execute file %s' % path, id='promgen.W003')","if not os.access(path, os.X_OK):"
"def parse_config(schema, config): schemaparser = ConfigParser() schemaparser.readfp(StringIO(schema)) cfgparser = ConfigParser() cfgparser.readfp(StringIO(config)) result = {} for section in cfgparser.sections(): result_section = {} schema = {} <IF_STMT> schema = dict(schemaparser.items(section)) for key, value in cfgparser.items(section): converter = converters[schema.get(key, 'string')] result_section[key] = converter(value) result[section] = result_section return result",if section in schemaparser.sections():
"def validate_arguments(args): if args.num_pss < 1: print('Value error: must have ore than one parameter servers.') exit(1) if not GPU_IDS: num_cpus = multiprocessing.cpu_count() <IF_STMT> print('Value error: there are %s available CPUs but you are requiring %s.' % (num_cpus, args.cpu_trainers)) exit(1) if not os.path.isfile(args.file): print('Value error: model trainning file does not exist') exit(1)",if args.cpu_trainers > num_cpus:
"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return 'raw' elif IndexedDataset.exists(path): with open(index_file_path(path), 'rb') as f: magic = f.read(8) if magic == IndexedDataset._HDR_MAGIC: return 'cached' <IF_STMT> return 'mmap' else: return None elif FastaDataset.exists(path): return 'fasta' else: return None",elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if 'resourcegroup' not in [x.lower() for x in obj.keys()]: <IF_STMT> obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != 'sourceVault': _add_resource_group(obj[item_key])",if obj['id']:
"def reformatBody(self, event=None): """"""Reformat all paragraphs in the body."""""" c, p = (self, self.p) undoType = 'reformat-body' w = c.frame.body.wrapper c.undoer.beforeChangeGroup(p, undoType) w.setInsertPoint(0) while 1: progress = w.getInsertPoint() c.reformatParagraph(event, undoType=undoType) ins = w.getInsertPoint() s = w.getAllText() w.setInsertPoint(ins) <IF_STMT> break c.undoer.afterChangeGroup(p, undoType)",if ins <= progress or ins >= len(s):
"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) <IF_STMT> content.append(project.readme.to_rst().path.name) path = project.package.path for fname in ('setup.cfg', 'setup.py'): if (path / fname).exists(): content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append('/'.join(fpath.parts)) return '\n'.join(content)",if project.readme.markup != 'rst':
"def __init__(self, response): error = '{} {}'.format(response.status_code, response.reason) extra = [] try: response_json = response.json() <IF_STMT> error = ' '.join((error['message'] for error in response_json['error_list'])) extra = [error['extra'] for error in response_json['error_list'] if 'extra' in error] except JSONDecodeError: pass super().__init__(response=response, error=error, extra=extra)",if 'error_list' in response_json:
"def handle_event(self, fileno=None, events=None): if self._state == RUN: <IF_STMT> self._it = self._process_result(0) try: next(self._it) except (StopIteration, CoroStop): self._it = None",if self._it is None:
"def find_query(self, needle, haystack): try: import pinyin haystack_py = pinyin.get_initial(haystack, '') needle_len = len(needle) start = 0 result = [] while True: found = haystack_py.find(needle, start) <IF_STMT> break result.append((found, needle_len)) start = found + needle_len return result except: return None",if found < 0:
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if 'Last-Modified' not in rv.headers: try: result = date if callable(result): result = result(rv) if not isinstance(result, basestring): from werkzeug.http import http_date result = http_date(result) <IF_STMT> rv.headers['Last-Modified'] = result except Exception: logging.getLogger(__name__).exception('Error while calculating the lastmodified value for response {!r}'.format(rv)) return rv",if result:
"def check_require(require_modules, require_lines): for require_module in require_modules: st = try_import(require_module) if st == 0: continue <IF_STMT> print('installed {}: {}\n'.format(require_module, require_lines[require_module])) elif st == 2: print('failed installed {}: {}\n'.format(require_module, require_lines[require_module]))",elif st == 1:
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith('.'): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, '__init__.py')): self.bundle_package(itempath) <IF_STMT> self.bundle_module(itempath)",elif nm.endswith('.py'):
"def _find_root(): test_dirs = ['Src', 'Build', 'Package', 'Tests', 'Util'] root = os.getcwd() test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) while not test: last_root = root root = os.path.dirname(root) <IF_STMT> raise Exception('Root not found') test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) return root",if root == last_root:
"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = (c.rootPosition(), [], []) while p: if p.v in seen: p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, '@ignore'): p.moveToNodeAfterTree() <IF_STMT> result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",elif p.h.startswith('@mark-for-unit-tests'):
"def startTagFrameset(self, token): self.parser.parseError('unexpected-start-tag', {'name': 'frameset'}) if len(self.tree.openElements) == 1 or self.tree.openElements[1].name != 'body': assert self.parser.innerHTML elif not self.parser.framesetOK: pass else: <IF_STMT> self.tree.openElements[1].parent.removeChild(self.tree.openElements[1]) while self.tree.openElements[-1].name != 'html': self.tree.openElements.pop() self.tree.insertElement(token) self.parser.phase = self.parser.phases['inFrameset']",if self.tree.openElements[1].parent:
"def try_split(self, split_text: List[str]): ret = [] for i in split_text: <IF_STMT> continue val = int(i, 2) if val > 255 or val < 0: return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f'binary successful, returning {ret.__repr__()}') return ret",if len(i) == 0:
"def generator(self, data): for sock in data: <IF_STMT> offset = sock.obj_offset else: offset = sock.obj_vm.vtop(sock.obj_offset) yield (0, [Address(offset), int(sock.Pid), int(sock.LocalPort), int(sock.Protocol), str(protos.protos.get(sock.Protocol.v(), '-')), str(sock.LocalIpAddress), str(sock.CreateTime)])",if not self._config.PHYSICAL_OFFSET:
"def __init__(self, num_bits=4, always_apply=False, p=0.5): super(Posterize, self).__init__(always_apply, p) if isinstance(num_bits, (list, tuple)): <IF_STMT> self.num_bits = [to_tuple(i, 0) for i in num_bits] else: self.num_bits = to_tuple(num_bits, 0) else: self.num_bits = to_tuple(num_bits, num_bits)",if len(num_bits) == 3:
"def tearDown(self): """"""Just in case yn00 creates some junk files, do a clean-up."""""" del_files = [self.out_file, '2YN.dN', '2YN.dS', '2YN.t', 'rst', 'rst1', 'rub'] for filename in del_files: <IF_STMT> os.remove(filename) if os.path.exists(self.working_dir): for filename in os.listdir(self.working_dir): filepath = os.path.join(self.working_dir, filename) os.remove(filepath) os.rmdir(self.working_dir)",if os.path.exists(filename):
"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [(idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) <IF_STMT>] else: res = [(idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) if line.startswith(searchfor)] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return ''",if line.lstrip().startswith(searchfor.lstrip())
"def ComboBoxDroppedHeightTest(windows): """"""Check if each combobox height is the same as the reference"""""" bugs = [] for win in windows: if not win.ref: continue <IF_STMT> continue if win.DroppedRect().height() != win.ref.DroppedRect().height(): bugs.append(([win], {}, testname, 0)) return bugs",if win.Class() != 'ComboBox' or win.ref.Class() != 'ComboBox':
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() <IF_STMT> if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if start < last_end:
"def unpickle_from_file(file_path, gzip=False): """"""Unpickle obj from file_path with gzipping."""""" with tf.io.gfile.GFile(file_path, 'rb') as f: <IF_STMT> obj = pickle.load(f) else: with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf: obj = pickle.load(gzipf) return obj",if not gzip:
"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, 'user', None) result = {'ip_address': request.META['REMOTE_ADDR']} if user and user.is_authenticated(): result.update({'email': user.email, 'id': user.id}) <IF_STMT> result['name'] = user.name else: result = {} return mark_safe(json.dumps(result))",if user.name:
"def get_item_address(self, item): """"""Get an item's address as a collection of names"""""" result = [] while True: name = self.tree_ctrl.GetItemPyData(item) <IF_STMT> break else: result.insert(0, name) item = self.tree_ctrl.GetItemParent(item) return result",if name is None:
"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: <IF_STMT> dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen",if self.map[row][col] == UNSEEN:
"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: <IF_STMT> input_array = model.example_input_array if input_array is not None: input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn('Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given', UserWarning)",if input_array is None:
"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: if not len(scene_exception) == 2: continue scene_name, scene_season = scene_exception.split('|') <IF_STMT> scene_exceptions.append(scene_name) return scene_exceptions",if season == scene_season:
def _clean_temp_files(): for pattern in _temp_files: for path in glob.glob(pattern): <IF_STMT> os.remove(path) else: shutil.rmtree(path),if os.path.islink(path) or os.path.isfile(path):
"def wait_for_completion(self, job_id, offset, max_results, start_time, timeout): """"""Wait for job completion and return the first page."""""" while True: result = self.get_query_results(job_id=job_id, page_token=None, start_index=offset, max_results=max_results) <IF_STMT> return result if time.time() - start_time > timeout: raise Exception(""Timeout: the query doesn't finish within %d seconds."" % timeout) time.sleep(1)",if result['jobComplete']:
"def get_data(self, element, ranges, style): <IF_STMT> groups = element.groupby(element.kdims).items() else: groups = [(element.label, element)] plots = [] axis = 'x' if self.invert_axes else 'y' for key, group in groups: if element.kdims: label = ','.join([d.pprint_value(v) for d, v in zip(element.kdims, key)]) else: label = key data = {axis: group.dimension_values(group.vdims[0]), 'name': label} plots.append(data) return plots",if element.kdims:
"def get_files(self, dirname): if not self._data.has_key(dirname): self._create(dirname) else: new_time = self._changed(dirname) <IF_STMT> self._update(dirname, new_time) dcLog.debug('==> ' + '\t\n'.join(self._data[dirname]['flist'])) return self._data[dirname]['flist']",if new_time:
"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): <IF_STMT> self.module_names.add(name[:-3]) elif '.' not in name: self.module_names.add(name)",if name.endswith('.py'):
"def logic(): for i in range(100): yield (clock.posedge, reset.negedge) <IF_STMT> count.next = 0 elif enable: count.next = (count + 1) % n raise StopSimulation",if reset == ACTIVE_LOW:
"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): if k[:1] == '-': keynames[i] = k[1:] negate.add(k[1:])  def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): <IF_STMT> composite[i] = -v return composite return getit",if k in negate:
"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: <IF_STMT> self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) if wnd_name in self.capture_mouse_windows: self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print('show_image: named_window ', wnd_name, ' not found.')",if self.named_windows[wnd_name] == 0:
def check_action_permitted(self): if self._action == 'sts:GetCallerIdentity': return True policies = self._access_key.collect_policies() permitted = False for policy in policies: iam_policy = IAMPolicy(policy) permission_result = iam_policy.is_action_permitted(self._action) if permission_result == PermissionResult.DENIED: self._raise_access_denied() <IF_STMT> permitted = True if not permitted: self._raise_access_denied(),elif permission_result == PermissionResult.PERMITTED:
"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key]['inverse'] is True: <IF_STMT> value = datetime.now() - limit elif datetime.now() + limit < value: value = datetime.now() + limit elif value > limit: value = limit return value",if datetime.now() - limit > value:
"def replace_dataset_ids(path, key, value): """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job."""""" current_case = input_values if key == 'id': for i, p in enumerate(path): if isinstance(current_case, (list, dict)): current_case = current_case[p] <IF_STMT> return (key, translate_values.get(current_case['id'], value)) return (key, value)",if src == current_case.get('src'):
"def load_ext(name, funcs): ExtModule = namedtuple('ExtModule', funcs) ext_list = [] lib_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) for fun in funcs: <IF_STMT> ext_list.append(extension.load(fun, name, lib_dir=lib_root).op) else: ext_list.append(extension.load(fun, name, lib_dir=lib_root).op_) return ExtModule(*ext_list)","if fun in ['nms', 'softnms']:"
"def execute_action(self): selected_actions = self.model_action.get_selected_results_with_index() if selected_actions and self.args_for_action: for name, _, act_idx in selected_actions: try: action = self.actions[act_idx] <IF_STMT> action.act([arg for arg, _, _ in self.args_for_action], self) except Exception as e: debug.log('execute_action', e)",if action:
"def __getattr__(self, attr): proxy = self.__proxy if proxy and hasattr(proxy, attr): return getattr(proxy, attr) attrmap = self.__attrmap if attr in attrmap: source = attrmap[attr] <IF_STMT> value = source() else: value = _import_object(source) setattr(self, attr, value) self.__log.debug('loaded lazy attr %r: %r', attr, value) return value raise AttributeError(""'module' object has no attribute '%s'"" % (attr,))",if callable(source):
"def forward(self, x): x = x.unsqueeze(1) for conv in self.conv_layers: residual = x x = conv(x) <IF_STMT> tsz = x.size(2) r_tsz = residual.size(2) residual = residual[..., ::r_tsz // tsz][..., :tsz] x = (x + residual) * self.residual_scale if self.log_compression: x = x.abs() x = x + 1 x = x.log() return x",if self.skip_connections and x.size(1) == residual.size(1):
"def __Prefix_Step2a(self, token): for prefix in self.__prefix_step2a: <IF_STMT> token = token[len(prefix):] self.prefix_step2a_success = True break return token",if token.startswith(prefix) and len(token) > 5:
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False elif isinstance(s, np.ndarray) and s.size == 0: return False <IF_STMT> return False return True","elif isinstance(s, collections.abc.Sequence) and len(s) == 0:"
"def get_all_comments(self, gallery_id, post_no, comment_cnt): comment_page_cnt = (comment_cnt - 1) // self.options.comments_per_page + 1 comments = [] headers = {'X-Requested-With': 'XMLHttpRequest'} data = {'ci_t': self._session.cookies['ci_c'], 'id': gallery_id, 'no': post_no} for i in range(comment_page_cnt): data['comment_page'] = i + 1 response = self.request_comment(headers, data) batch = self.parse_comments(response.text) <IF_STMT> break comments = batch + comments return comments",if not batch:
def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: <IF_STMT> if e.no_match_group_specs or e.error_group_specs: raise e if e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS: raise e logger.error(str(e)),if self.base.conf.strict:
"def find_field_notnull_differ(self, meta, table_description, table_name): if not self.can_detect_notnull_differ: return for field in all_local_fields(meta): attname = field.db_column or field.attname <IF_STMT> continue null = self.get_field_db_nullable(field, table_name) if field.null != null: action = field.null and 'DROP' or 'SET' self.add_difference('notnull-differ', table_name, attname, action)","if (table_name, attname) in self.new_db_fields:"
"def _change_moving_module(self, changes, dest): if not self.source.is_folder(): pymodule = self.pycore.resource_to_pyobject(self.source) source = self.import_tools.relatives_to_absolutes(pymodule) pymodule = self.tools.new_pymodule(pymodule, source) source = self._change_occurrences_in_module(dest, pymodule) source = self.tools.new_source(pymodule, source) <IF_STMT> changes.add_change(ChangeContents(self.source, source))",if source != self.source.read():
"def get(quality_name): """"""Returns a quality object based on canonical quality name."""""" found_components = {} for part in quality_name.lower().split(): component = _registry.get(part) <IF_STMT> raise ValueError('`%s` is not a valid quality string' % part) if component.type in found_components: raise ValueError('`%s` cannot be defined twice in a quality' % component.type) found_components[component.type] = component if not found_components: raise ValueError('No quality specified') result = Quality() for type, component in found_components.items(): setattr(result, type, component) return result",if not component:
def _unselected(self): selected = self._selected k = 0 z = selected[k] k += 1 for i in range(self._n): if i == z: <IF_STMT> z = selected[k] k += 1 else: z = -1 else: yield i,if k < len(selected):
"def render_headers(self) -> bytes: if not hasattr(self, '_headers'): parts = [b'Content-Disposition: form-data; ', format_form_param('name', self.name)] <IF_STMT> filename = format_form_param('filename', self.filename) parts.extend([b'; ', filename]) if self.content_type is not None: content_type = self.content_type.encode() parts.extend([b'\r\nContent-Type: ', content_type]) parts.append(b'\r\n\r\n') self._headers = b''.join(parts) return self._headers",if self.filename:
"def app_middleware(next, root, info, **kwargs): app_auth_header = 'HTTP_AUTHORIZATION' prefix = 'bearer' request = info.context if request.path == API_PATH: if not hasattr(request, 'app'): request.app = None auth = request.META.get(app_auth_header, '').split() <IF_STMT> auth_prefix, auth_token = auth if auth_prefix.lower() == prefix: request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",if len(auth) == 2:
"def _shortest_hypernym_paths(self, simulate_root): if self.offset == '00000000': return {self: 0} queue = deque([(self, 0)]) path = {} while queue: s, depth = queue.popleft() <IF_STMT> continue path[s] = depth depth += 1 queue.extend(((hyp, depth) for hyp in s._hypernyms())) if simulate_root: root = Synset(self._wordnet_corpus_reader, None, self.pos(), '00000000', '') path[root] = max(path.values()) + 1 return path",if s in path:
"def _populate_class_variables(): lookup = {} reverse_lookup = {} characters_for_re = [] for codepoint, name in list(codepoint2name.items()): character = chr(codepoint) <IF_STMT> characters_for_re.append(character) lookup[character] = name reverse_lookup[name] = character re_definition = '[%s]' % ''.join(characters_for_re) return (lookup, reverse_lookup, re.compile(re_definition))",if codepoint != 34:
"def prepare_data_status(self, view: sublime.View, data: Dict[str, Any]) -> Any: """"""Prepare the returned data for status"""""" if data['success'] and 'No docstring' not in data['doc'] and (data['doc'] != 'list\n'): self.signature = data['doc'] <IF_STMT> return try: self.signature = self.signature.splitlines()[2] except KeyError: return return self._show_status(view)",if self._signature_excluded(self.signature):
def _setup_once_tables(cls): if cls.run_define_tables == 'once': cls.define_tables(cls.metadata) <IF_STMT> cls.metadata.create_all(cls.bind) cls.tables.update(cls.metadata.tables),if cls.run_create_tables == 'once':
"def _send_recursive(self, files): for base in files: <IF_STMT> self._send_files([base]) continue last_dir = asbytes(base) for root, dirs, fls in os.walk(base): self._chdir(last_dir, asbytes(root)) self._send_files([os.path.join(root, f) for f in fls]) last_dir = asbytes(root) for i in range(len(os.path.split(last_dir))): self._send_popd()",if not os.path.isdir(base):
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) if not is_registered(self.model): inline_fields = () for inline in self.inlines: inline_model, follow_field = self._reversion_introspect_inline_admin(inline) if inline_model: self._reversion_autoregister(inline_model, ()) <IF_STMT> inline_fields += (follow_field,) self._reversion_autoregister(self.model, inline_fields)",if follow_field:
"def dispatch_hook(key, hooks, hook_data, **kwargs): """"""Dispatches a hook dictionary on a given piece of data."""""" hooks = hooks or dict() hooks = hooks.get(key) if hooks: if hasattr(hooks, '__call__'): hooks = [hooks] for hook in hooks: _hook_data = hook(hook_data, **kwargs) <IF_STMT> hook_data = _hook_data return hook_data",if _hook_data is not None:
"def __call__(self, image, crop=True): if isinstance(image, PTensor): return self.crop_to_output(numpy_to_paddle(self(paddle_to_numpy(image), crop=False))) else: warp = cv.warpAffine(image, self.transform_matrix, image.shape[1::-1], borderMode=cv.BORDER_REPLICATE) <IF_STMT> return self.crop_to_output(warp) else: return warp",if crop:
"def _analyze(self): lines = open(self.log_path, 'r').readlines() prev_line = None for line in lines: <IF_STMT> self.errors.append(line[len('ERROR:'):].strip()) elif line.startswith('FAIL:') and prev_line and prev_line.startswith('='): self.failures.append(line[len('FAIL:'):].strip()) prev_line = line",if line.startswith('ERROR:') and prev_line and prev_line.startswith('='):
"def end(self, name): self.soup.endData() completed_tag = self.soup.tagStack[-1] namespace, name = self._getNsTag(name) nsprefix = None if namespace is not None: for inverted_nsmap in reversed(self.nsmaps): <IF_STMT> nsprefix = inverted_nsmap[namespace] break self.soup.handle_endtag(name, nsprefix) if len(self.nsmaps) > 1: self.nsmaps.pop()",if inverted_nsmap is not None and namespace in inverted_nsmap:
"def _bind_parameters(operation, parameters): string_parameters = {} for name, value in parameters.iteritems(): if value is None: string_parameters[name] = 'NULL' <IF_STMT> string_parameters[name] = ""'"" + _escape(value) + ""'"" else: string_parameters[name] = str(value) return operation % string_parameters","elif isinstance(value, basestring):"
"def plugin_on_song_ended(self, song, skipped): if song is not None: rating = song('~#rating') invrating = 1.0 - rating delta = min(rating, invrating) / 2.0 <IF_STMT> rating -= delta else: rating += delta song['~#rating'] = rating",if skipped:
"def on_activated_async(self, view): if settings['modified_lines_only']: self.freeze_last_version(view) if settings['enabled']: match_trailing_spaces(view) <IF_STMT> active_views[view.id()] = view.visible_region() self.update_on_region_change(view)",if not view.id() in active_views:
"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term):] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u('%s is contained here:') % py.io.saferepr(term, maxsize=42)] for line in diff: <IF_STMT> continue if line.startswith(u('- ')): continue if line.startswith(u('+ ')): newdiff.append(u('  ') + line[2:]) else: newdiff.append(line) return newdiff",if line.startswith(u('Skipping')):
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) elif fn.endswith('.png'): os.remove(fn_full) <IF_STMT> os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",elif fn.endswith('.md'):
"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = (0, 0) for ts in self.time_steps: if ts.raw_reward is not None: raw_rewards += ts.raw_reward <IF_STMT> processed_rewards += ts.processed_reward return (raw_rewards, processed_rewards)",if ts.processed_reward is not None:
"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] <IF_STMT> s = s.decode(encoding) if withyear: s = '%s %s' % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if encoding is not None:
"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get('Authorization'): credentails = parse_authorization_header(request.headers.get('Authorization')) <IF_STMT> return response_hash = response(credentails, passwd, dict(uri=request.script_root + request.path, body=request.data, method=request.method)) if credentails.get('response') == response_hash: return True return False",if not credentails:
"def wrapped(self, request): try: return self._finished except AttributeError: <IF_STMT> if not request.session.shouldfail and (not request.session.shouldstop): log.debug('%s is still going to be used, not terminating it. Still in use on:\n%s', self, pprint.pformat(list(self.node_ids))) return log.debug('Finish called on %s', self) try: return func(request) finally: self._finished = True",if self.node_ids:
"def run_tests(): x = 5 with switch(x) as case: if case(0): print('zero') print('zero') elif case(1, 2): print('one or two') <IF_STMT> print('three or four') else: print('default') print('another')","elif case(3, 4):"
def task_done(self): with self._cond: <IF_STMT> raise ValueError('task_done() called too many times') if self._unfinished_tasks._semlock._is_zero(): self._cond.notify_all(),if not self._unfinished_tasks.acquire(False):
"def _set_uid(self, val): if val is not None: <IF_STMT> self.bus.log('pwd module not available; ignoring uid.', level=30) val = None elif isinstance(val, text_or_bytes): val = pwd.getpwnam(val)[2] self._uid = val",if pwd is None:
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) <IF_STMT> exe, args = exe_data return (company, major, minor, arch, exe, args)",if exe_data is not None:
"def run(algs): for alg in algs: vcs = alg.get('variantcaller') if vcs: if isinstance(vcs, dict): vcs = reduce(operator.add, vcs.values()) <IF_STMT> vcs = [vcs] return any((vc.startswith(prefix) for vc in vcs if vc))","if not isinstance(vcs, (list, tuple)):"
"def wrapper(self, *args, **kwargs): if not self.request.path.endswith('/'): <IF_STMT> uri = self.request.path + '/' if self.request.query: uri += '?' + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)","if self.request.method in ('GET', 'HEAD'):"
"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: <IF_STMT> continue if line.startswith(b'OK'): return elif line.startswith(b'Benutzer/Passwort Fehler'): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",if not line.strip():
"def Walk(self, hMenu=None): if not hMenu: hMenu = self.handle n = user32.GetMenuItemCount(hMenu) mi = MENUITEMINFO() for i in range(n): mi.fMask = 2 user32.GetMenuItemInfoA(hMenu, i, 1, byref(mi)) handle = user32.GetSubMenu(hMenu, i) <IF_STMT> yield (handle, self.ListItems(handle)) for i in self.Walk(handle): yield i",if handle:
"def setSelection(self, labels): input = self.__validateInput(labels) if len(input) == 0 and (not self.__allowEmptySelection): return if self.__allowMultipleSelection: self.__selectedLabels[:] = input self.__selectionChanged() el<IF_STMT> raise RuntimeError('Parameter must be single item or a list with one element.') else: self.__selectedLabels[:] = input self.__selectionChanged() self.__validateState()",if len(input) > 1:
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) <IF_STMT> raise ParsingError('""momentum"" must be numeric.')","if not isinstance(self.momentum, (int, float)):"
"def get_order(self, aBuf): if not aBuf: return (-1, 1) first_char = wrap_ord(aBuf[0]) if 129 <= first_char <= 159 or 224 <= first_char <= 252: charLen = 2 else: charLen = 1 if len(aBuf) > 1: second_char = wrap_ord(aBuf[1]) <IF_STMT> return (second_char - 159, charLen) return (-1, charLen)",if first_char == 202 and 159 <= second_char <= 241:
"def saveSpecial(self, **kwargs): for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST: item = config.get_config('misc', kw) value = kwargs.get(kw) msg = item.set(value) <IF_STMT> return badParameterResponse(msg) config.save_config() raise Raiser(self.__root)",if msg:
"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): <IF_STMT> kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: if isinstance(kwargs.get('event_data', {}).get(key), str): if len(kwargs['event_data'][key]) > 1024: kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)",if key not in valid_keys:
"def toggleFactorReload(self, value=None): self.serviceFittingOptions['useGlobalForceReload'] = value if value is not None else not self.serviceFittingOptions['useGlobalForceReload'] fitIDs = set() for fit in set(self._loadedFits): <IF_STMT> continue if fit.calculated: fit.factorReload = self.serviceFittingOptions['useGlobalForceReload'] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",if fit is None:
"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): <IF_STMT> if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen","if filter is None or (row, col) not in filter:"
"def getAlphaClone(lookfor, eager=None): if isinstance(lookfor, int): <IF_STMT> item = get_gamedata_session().query(AlphaClone).get(lookfor) else: item = get_gamedata_session().query(AlphaClone).options(*processEager(eager)).filter(AlphaClone.ID == lookfor).first() else: raise TypeError('Need integer as argument') return item",if eager is None:
def _rle_encode(string): new = b'' count = 0 for cur in string: if not cur: count += 1 else: <IF_STMT> new += b'\x00' + bytes([count]) count = 0 new += bytes([cur]) return new,if count:
def result_iterator(): try: for future in fs: <IF_STMT> yield future.result() else: yield future.result(end_time - time.time()) finally: for future in fs: future.cancel(),if timeout is None:
"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == 'val': for key, value in segment.items(): <IF_STMT> return value if hasattr(key, 'text'): if key.text == index[0]: return value raise Exception('Invalid state') elif index_type == 'index': return segment[index] elif index_type == 'textslice': return segment[index[0]:index[1]] elif index_type == 'key': return index[1] if strictdoc else index[0] else: raise Exception('Invalid state')",if key == index[0]:
"def _reset_sequences(self, db_name): conn = connections[db_name] if conn.features.supports_sequence_reset: sql_list = conn.ops.sequence_reset_by_name_sql(no_style(), conn.introspection.sequence_list()) <IF_STMT> try: cursor = conn.cursor() for sql in sql_list: cursor.execute(sql) except Exception: transaction.rollback_unless_managed(using=db_name) raise transaction.commit_unless_managed(using=db_name)",if sql_list:
"def translate_to_statements(self, statements, conditional_write_vars): lines = [] for stmt in statements: <IF_STMT> self.temporary_vars.add((stmt.var, stmt.dtype)) line = self.translate_statement(stmt) if stmt.var in conditional_write_vars: subs = {} condvar = conditional_write_vars[stmt.var] lines.append('if %s:' % condvar) lines.append(indent(line)) else: lines.append(line) return lines",if stmt.op == ':=' and (not stmt.var in self.variables):
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: ext = os.path.splitext(os.path.normcase(py_file))[1] if ext != PYTHON_SOURCE_EXTENSION: continue <IF_STMT> bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if self.compile:
"def logic(): for i in range(100): yield (clock.posedge, reset.negedge) if reset == ACTIVE_LOW: count.next = 0 el<IF_STMT> count.next = (count + 1) % n raise StopSimulation",if enable:
"def _is_subnet_of(a, b): try: <IF_STMT> raise TypeError('%s and %s are not of the same version' % (a, b)) return b.network_address <= a.network_address and b.broadcast_address >= a.broadcast_address except AttributeError: raise TypeError('Unable to test subnet containment between %s and %s' % (a, b))",if a._version != b._version:
"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: if item.endswith('/') and (not is_dir): continue match = path if item.startswith('/') else basename <IF_STMT> return True return False","if fnmatch.fnmatch(match, item.strip('/')):"
"def __recv_null(self): """"""Receive a null byte."""""" while 1: c = self.sock.recv(1) if c == '': self.close() raise EOFError('Socket Closed') <IF_STMT> return",if c == '\x00':
"def onMessage(self, payload, isBinary): if isBinary: self.result = 'Expected text message with payload, but got binary.' el<IF_STMT> self.result = 'Expected text message with payload of length %d, but got %d.' % (self.DATALEN, len(payload)) else: self.behavior = Case.OK self.result = 'Received text message of length %d.' % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len(payload) != self.DATALEN:
"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: <IF_STMT> continue d_path = ''.join([path, '/', d]) d_new_path = ''.join([new_path, '/', d]) attr = self.getattr(d_path) if stat.S_ISDIR(attr['st_mode']): self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)","if d in ['.', '..']:"
"def dir_box_click(self, double): if double: name = self.list_box.get_selected_name() path = os.path.join(self.directory, name) suffix = os.path.splitext(name)[1] <IF_STMT> self.directory = path else: self.double_click_file(name) self.update()",if suffix not in self.suffixes and os.path.isdir(path):
"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: if value is not None: <IF_STMT> return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError('Module %r has no mapped classes registered under the name %r' % (self.__parent.name, key))","if isinstance(value, _ModuleMarker):"
"def poll_thread(): time.sleep(0.5) if process.wait() and process_state: time.sleep(0.25) <IF_STMT> stdout, stderr = process._communicate(None) logger.error('Web server process exited unexpectedly', 'app', stdout=stdout, stderr=stderr) time.sleep(1) restart_server(1)",if not check_global_interrupt():
"def apply_dateparser_timezone(utc_datetime, offset_or_timezone_abb): for name, info in _tz_offsets: <IF_STMT> tz = StaticTzInfo(name, info['offset']) return utc_datetime.astimezone(tz)",if info['regex'].search(' %s' % offset_or_timezone_abb):
"def _load_wordlist(filename): if filename is None: return {} path = None for dir in (CONFIG_DIR, ASSETS_DIR): path = os.path.realpath(os.path.join(dir, filename)) <IF_STMT> break words = {} with open(path, encoding='utf-8') as f: pairs = [word.strip().rsplit(' ', 1) for word in f] pairs.sort(reverse=True, key=lambda x: int(x[1])) words = {p[0]: int(p[1]) for p in pairs} return words",if os.path.exists(path):
"def terminate_processes_matching_names(match_strings, kill=False): """"""Terminates processes matching particular names (case sensitive)."""""" if isinstance(match_strings, str): match_strings = [match_strings] for process in psutil.process_iter(): try: process_info = process.as_dict(attrs=['name', 'pid']) process_name = process_info['name'] except (psutil.AccessDenied, psutil.NoSuchProcess, OSError): continue <IF_STMT> terminate_process(process_info['pid'], kill)",if any((x == process_name for x in match_strings)):
"def has_scheme(self, inp): if '://' in inp: return True else: authority = inp.replace('/', '#').replace('?', '#').split('#')[0] <IF_STMT> _, host_or_port = authority.split(':', 1) if re.match('^\\d+$', host_or_port): return False else: return False return True",if ':' in authority:
def close(self): with BrowserContext._BROWSER_LOCK: BrowserContext._BROWSER_REFCNT -= 1 <IF_STMT> logger.info('Destroying browser main loop') BrowserContext._BROWSER_LOOP.destroy() BrowserContext._BROWSER_LOOP = None,if BrowserContext._BROWSER_REFCNT == 0:
"def _mock_get_merge_ticks(self, order_book_id_list, trading_date, last_dt=None): for tick in self._ticks: <IF_STMT> continue if self.env.data_proxy.get_future_trading_date(tick.datetime).date() != trading_date.date(): continue if last_dt and tick.datetime <= last_dt: continue yield tick",if tick.order_book_id not in order_book_id_list:
"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' <IF_STMT> source += '[branch %s] ' % ss['branch'] if ss['revision']: source += str(ss['revision']) else: source += 'HEAD' if ss['patch'] is not None: source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['branch']:
"def test_open_read_bytes(self, sftp): """"""Test reading bytes from a file"""""" f = None try: self._create_file('file', 'xxx') f = (yield from sftp.open('file', 'rb')) self.assertEqual((yield from f.read()), b'xxx') finally: <IF_STMT> yield from f.close() remove('file')",if f:
"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) if len(data) == 0: break chan.send(data) <IF_STMT> data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",if chan in r:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search(""url\\('/ks-waf-error\\.png'\\)"", page, re.I) is not None <IF_STMT> break return retval",if retval:
"def __init__(self, raw): ticker_ticks = {} for tick in raw['results']: <IF_STMT> ticker_ticks[tick['T']].append(tick) else: ticker_ticks[tick['T']] = [tick] super().__init__({ticker: Aggsv2({'results': ticks}) for ticker, ticks in ticker_ticks.items()})",if ticker_ticks.get(tick['T']):
"def _makefiles(self, f): if isinstance(f, dict): for k, v in list(f.items()): <IF_STMT> self.makedir(dirname=k, content=v) elif isinstance(v, str): self.make_file(filename=k, content=v) else: raise ValueError('Unexpected:', k, v) elif isinstance(f, str): self._make_empty_file(f) elif isinstance(f, list): self.make_list(f) else: raise ValueError('Unknown type:', f)","if isinstance(v, list):"
"def migrate_command_storage(apps, schema_editor): model = apps.get_model('terminal', 'CommandStorage') init_storage_data(model) setting = get_setting(apps, schema_editor, 'TERMINAL_COMMAND_STORAGE') if not setting: return values = get_storage_data(setting) for name, meta in values.items(): tp = meta.pop('TYPE') <IF_STMT> continue model.objects.create(name=name, type=tp, meta=meta)","if not tp or name in ['default', 'null']:"
"def build_vertices(self, ulines): vertex_idx = 0 vertices = collections.OrderedDict() for line in ulines: for vt in line: <IF_STMT> continue new_vertex = (vt.u, vt.v, 0.0) if new_vertex in vertices: continue vt.index = vertex_idx vertex_idx += 1 vertices[new_vertex] = 1 return (vertex_idx, list(vertices.keys()))",if vt.replacement is not None:
"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {'objects': 0, 'containers': 0, 'accounts': 0} qdir = 'quarantined' for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) if os.path.exists(qtgt): linkcount = os.lstat(qtgt).st_nlink <IF_STMT> qcounts[qtype] += linkcount - 2 return qcounts",if linkcount > 2:
"def _format_arg(self, name, trait_spec, value): if name == 'mask_file': return '' if name == 'op_string': <IF_STMT> if isdefined(self.inputs.mask_file): return self.inputs.op_string % self.inputs.mask_file else: raise ValueError('-k %s option in op_string requires mask_file') return super(ImageStats, self)._format_arg(name, trait_spec, value)",if '-k %s' in self.inputs.op_string:
"def _update_theme_style(self, *args): self.line_color_normal = self.theme_cls.divider_color if not any([self.error, self._text_len_error]): if not self.focus: self._current_hint_text_color = self.theme_cls.disabled_hint_text_color self._current_right_lbl_color = self.theme_cls.disabled_hint_text_color <IF_STMT> self._current_error_color = self.theme_cls.disabled_hint_text_color",if self.helper_text_mode == 'persistent':
"def createFields(self): for item in self.format: <IF_STMT> yield item[0](self, *item[1:-1], **item[-1]) else: yield item[0](self, *item[1:])","if isinstance(item[-1], dict):"
"def execute(self, statement, arguments=None): while True: try: <IF_STMT> self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: if 'locked' not in getSafeExString(ex): raise else: break if statement.lstrip().upper().startswith('SELECT'): return self.cursor.fetchall()",if arguments:
"def set_income_account_for_fixed_assets(self): disposal_account = depreciation_cost_center = None for d in self.get('items'): <IF_STMT> if not disposal_account: disposal_account, depreciation_cost_center = get_disposal_account_and_cost_center(self.company) d.income_account = disposal_account if not d.cost_center: d.cost_center = depreciation_cost_center",if d.is_fixed_asset:
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 <IF_STMT> nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",if nbChars[1] is not None:
"def _get_service_full_name(self, name, help_command_table): if help_command_table and name not in self._NON_SERVICE_COMMANDS: <IF_STMT> return self._HIGH_LEVEL_SERVICE_FULL_NAMES[name] service = help_command_table.get(name) if service: return service.service_model.metadata['serviceFullName']",if name in self._HIGH_LEVEL_SERVICE_FULL_NAMES:
"def print_addresses(self): p = 3 tmp_str = '[' if self.get_len() >= 7: while 1: <IF_STMT> tmp_str += '#' tmp_str += self.get_ip_address(p) p += 4 if p >= self.get_len(): break else: tmp_str += ', ' tmp_str += '] ' if self.get_ptr() % 4: tmp_str += 'nonsense ptr field: %d ' % self.get_ptr() return tmp_str",if p + 1 == self.get_ptr():
def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({'test': 'insert'}) error = False except: if not self.expect_exception: raise <IF_STMT> assert error,if self.expect_exception:
"def create_composite_mounter_by_args(args): """"""Creates a CompositeMounter by the images in given args."""""" logging.info('Mount images...') mounter = composite_mounter.CompositeMounter() for partition in composite_mounter.SUPPORTED_PARTITIONS: image_source = vars(args)[partition] <IF_STMT> logging.info('  %s=%s', partition, image_source) mounter.add_by_mount_target(partition, image_source) if mounter.is_empty(): raise RuntimeError('Must give at least one image source.') return mounter",if image_source:
"def _get_containing_class(self, pyname): if isinstance(pyname, pynames.DefinedName): scope = pyname.get_object().get_scope() parent = scope.parent <IF_STMT> return parent.pyobject",if parent is not None and parent.get_kind() == 'Class':
"def test_chunkcoding(self): tstring_lines = [] for b in self.tstring: lines = b.split(b'\n') last = lines.pop() assert last == b'' lines = [line + b'\n' for line in lines] tstring_lines.append(lines) for native, utf8 in zip(*tstring_lines): u = self.decode(native)[0] self.assertEqual(u, utf8.decode('utf-8')) <IF_STMT> self.assertEqual(native, self.encode(u)[0])",if self.roundtriptest:
"def set_default_variants(apps, schema_editor): Product = apps.get_model('product', 'Product') for product in Product.objects.iterator(): first_variant = product.variants.first() <IF_STMT> product.default_variant = first_variant product.save(update_fields=['default_variant', 'updated_at'])",if first_variant:
def json(self): try: if self.is_json(): raw_data = self.raw_data() <IF_STMT> raw_data = raw_data.decode('utf-8') return json.loads(raw_data) except ValueError: pass,"if not isinstance(raw_data, text_type):"
"def clear_react(self, message: discord.Message, emoji: MutableMapping=None) -> None: try: await message.clear_reactions() except discord.Forbidden: <IF_STMT> return with contextlib.suppress(discord.HTTPException): async for key in AsyncIter(emoji.values(), delay=0.2): await message.remove_reaction(key, self.bot.user) except discord.HTTPException: return",if not emoji:
"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split('_', 1)[1]): if value.startswith(prefix): value = value[len(prefix):] value = value.lstrip('_') <IF_STMT> return getattr(self.group, value) else: raise ValueError('No such constant: %s_%s' % (self.prefix, value)) else: return value","if hasattr(self.group, value):"
"def value(self): quote = False if self.defects: quote = True else: for x in self: if x.token_type == 'quoted-string': quote = True if quote: pre = post = '' if self[0].token_type == 'cfws' or self[0][0].token_type == 'cfws': pre = ' ' <IF_STMT> post = ' ' return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",if self[-1].token_type == 'cfws' or self[-1][-1].token_type == 'cfws':
"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: if root_path: config_root_path = drive.get('root_path') if config_root_path and root_path == config_root_path: return drive <IF_STMT> config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",elif volume_guid_path:
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) <IF_STMT> edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if parsed_drawing:
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' <IF_STMT> return 'null' else: return unicode(k)",elif k is None:
"def find_multiple_stats(stats, name, _found=None, _on_found=None): if _found is None: _found = [] for child_stats in stats: if child_stats.name == name: _found.append(child_stats) <IF_STMT> _on_found(_found) find_multiple_stats(child_stats, name, _found) return _found",if callable(_on_found):
"def _run_generated_code(self, code, globs, locs, fails_under_py3k=True): import warnings from zope.interface._compat import PYTHON3 with warnings.catch_warnings(record=True) as log: warnings.resetwarnings() <IF_STMT> exec(code, globs, locs) self.assertEqual(len(log), 0) return True else: try: exec(code, globs, locs) except TypeError: return False else: if fails_under_py3k: self.fail(""Didn't raise TypeError"")",if not PYTHON3:
"def _get_node(self, node_id): self.non_terminated_nodes({}) with self.lock: <IF_STMT> return self.cached_nodes[node_id] instance = self.compute.instances().get(project=self.provider_config['project_id'], zone=self.provider_config['availability_zone'], instance=node_id).execute() return instance",if node_id in self.cached_nodes:
def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok['text'] <IF_STMT> if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break,if self.opHash.has_key(tval):
"def _create_or_get_helper(self, infer_mode: Optional[bool]=None, **kwargs) -> Helper: prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) if is_training and self._train_helper is None: self._train_helper = helper <IF_STMT> self._infer_helper = helper return helper",elif not is_training and self._infer_helper is None:
"def get_ldset(self, ldsets): ldset = None if self._properties['ldset_name'] == '': nldset = len(ldsets) if nldset == 0: msg = _('Logical Disk Set could not be found.') raise exception.NotFound(msg) else: ldset = None else: <IF_STMT> msg = _('Logical Disk Set `%s` could not be found.') % self._properties['ldset_name'] raise exception.NotFound(msg) ldset = ldsets[self._properties['ldset_name']] return ldset",if self._properties['ldset_name'] not in ldsets:
"def calc_fractal_serial(q, maxiter): z = np.zeros(q.shape, complex) output = np.resize(np.array(0), q.shape) for i in range(len(q)): for iter in range(maxiter): z[i] = z[i] * z[i] + q[i] <IF_STMT> output[i] = iter break return output",if abs(z[i]) > 2.0:
"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) <IF_STMT> if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))","if isinstance(inst, (_Block, _Instantiator)):"
"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip('\n').split(' ')] bucket.append(walk) <IF_STMT> yield bucket bucket = [] if len(bucket): yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",if len(bucket) == batch_size:
def _traverse(op): if op in visited: return visited.add(op) if tag.is_injective(op.tag): if op not in s.outputs: s[op].compute_inline() for tensor in op.input_tensors: <IF_STMT> _traverse(tensor.op) callback(op),"if isinstance(tensor.op, tvm.te.ComputeOp):"
"def unwatch_run(self, run_id, handler): with self._dict_lock: <IF_STMT> self._handlers_dict[run_id] = [(start_cursor, callback) for start_cursor, callback in self._handlers_dict[run_id] if callback != handler] if not self._handlers_dict[run_id]: del self._handlers_dict[run_id] run_id_dict = self._run_id_dict del run_id_dict[run_id] self._run_id_dict = run_id_dict",if run_id in self._run_id_dict:
"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) <IF_STMT> print('Successfully connected to MySQL with the given configuration.') return else: print('Error: Could not connect to MySQL with the given configuration.') retry = RetryBoolQuestion('Do you want to retry MySQL configuration?', True) if not retry: raise ConfigInitError()",if self._CheckMySQLConnection():
"def get_courses_without_topic(topic): data = [] for entry in frappe.db.get_all('Course'): course = frappe.get_doc('Course', entry.name) topics = [t.topic for t in course.topics] <IF_STMT> data.append(course.name) return data",if not topics or topic not in topics:
"def _error_handler(action, **keywords): if keywords: file_type = keywords.get('file_type', None) if file_type: raise exceptions.FileTypeNotSupported(constants.FILE_TYPE_NOT_SUPPORTED_FMT % (file_type, action)) else: <IF_STMT> keywords.pop('on_demand') msg = 'Please check if there were typos in ' msg += 'function parameters: %s. Otherwise ' msg += 'unrecognized parameters were given.' raise exceptions.UnknownParameters(msg % keywords) else: raise exceptions.UnknownParameters('No parameters found!')",if 'on_demand' in keywords:
"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) <IF_STMT> to_store.append(self.view.substr(self.view.full_line(r))) if register: text = ''.join(to_store) if not text.endswith('\n'): text = text + '\n' state = State(self.view) state.registers[register] = [text]",if register:
"def has_actor(self, message: HasActorMessage) -> ResultMessage: actor_ref = message.actor_ref for address, item in self._allocated_actors.items(): ref = create_actor_ref(address, actor_ref.uid) <IF_STMT> return ResultMessage(message.message_id, True, protocol=message.protocol) return ResultMessage(message.message_id, False, protocol=message.protocol)",if ref in item:
"def toggleMetaButton(self, event): """"""Process clicks on toggle buttons"""""" clickedBtn = event.EventObject if wx.GetMouseState().GetModifiers() == wx.MOD_CONTROL: activeBtns = [btn for btn in self.metaButtons if btn.GetValue()] <IF_STMT> clickedBtn.setUserSelection(clickedBtn.GetValue()) self.itemView.filterItemStore() else: clickedBtn.setUserSelection(True) else: for btn in self.metaButtons: btn.setUserSelection(btn == clickedBtn) self.itemView.filterItemStore()",if activeBtns:
"def __init__(self, hub=None): if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() if config.resolver_nameservers: _resolver.network_resolver.nameservers[:] = config.resolver_nameservers <IF_STMT> _resolver.network_resolver.lifetime = config.resolver_timeout assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",if config.resolver_timeout:
"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] <IF_STMT> m = RE_CHECKBOX.match(first.text) if m is not None: first.text = self.markdown.htmlStash.store(get_checkbox(m.group('state')), safe=True) + m.group('line') found = True return found",if first.tag == 'p' and first.text is not None:
"def _check_mswin_locale(locale): msloc = None try: msloc = _LOCALE_NAMES[locale[:5]][:2] locale = locale[:5] except KeyError: try: msloc = _LOCALE_NAMES[locale[:2]][:2] locale = locale[:2] except KeyError: <IF_STMT> return ('en_GB', '1252') return (None, None) return (locale, msloc)",if locale[:2] == 'en' and locale[:5] != 'en_US':
"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = (self.c, self, self.w) if w: <IF_STMT> g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) if protect: k.mb_prefix = s","if hasattr(g.app.gui, 'set_minibuffer_label'):"
"def getProc(su, innerTarget): if len(su) == 1: proc = ('first', 'last') elif su.isFirst(innerTarget) and su.isLast(innerTarget): proc = ('first', 'last') <IF_STMT> proc = ('first',) elif su.isLast(innerTarget): proc = ('last',) else: proc = () return proc",elif su.isFirst(innerTarget):
def await_test_end(self): iterations = 0 while True: if iterations > 100: self.log.debug('Await: iteration limit reached') return status = self.master.get_status() <IF_STMT> return iterations += 1 time.sleep(1.0),if status.get('status') == 'ENDED':
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): <IF_STMT> text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","if isinstance(text, CodeViewText):"
"def validate_party_details(self): if self.party: <IF_STMT> frappe.throw(_('Invalid {0}: {1}').format(self.party_type, self.party)) if self.party_account and self.party_type in ('Customer', 'Supplier'): self.validate_account_type(self.party_account, [erpnext.get_party_account_type(self.party_type)])","if not frappe.db.exists(self.party_type, self.party):"
"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): <IF_STMT> pieces.append(force_text(getattr(self, piece)())) elif piece: pieces.append(re_escaped.sub('\\1', piece)) return ''.join(pieces)",if i % 2:
"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] <IF_STMT> s[i] = '\\' elif c == '\\' and s[i + 1] == '$': s[i] = '' i += 1 i += 1 return pattern[:0].join(s)",if c == '$' and s[i + 1] in '0123456789':
"def download(self, url, filename, **kwargs): try: r = self.get(url, timeout=10, stream=True, **kwargs) <IF_STMT> return False with open(filename, 'wb') as f: for chunk in r.iter_content(chunk_size=1024): if chunk: f.write(chunk) helpers.chmod_as_parent(filename) except Exception as e: sickrage.app.log.debug('Failed to download file from {} - ERROR: {}'.format(url, e)) if os.path.exists(filename): os.remove(filename) return False return True",if r.status_code >= 400:
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedFilesWithExtension('js'): items.append('<script type=""text/javascript"" src=""' + item.pathAbsoluteFromProjectEncoded() + '""></script>') if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:
def work(self): while True: timeout = self.timeout if idle.is_set(): timeout = self.idle_timeout log.debug('Wait for {}'.format(timeout)) fetch.wait(timeout) <IF_STMT> log.info('Stop fetch worker') break self.fetch(),if shutting_down.is_set():
"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': <IF_STMT> mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s:
"def compare_lists(self, l1, l2, key): l2_lookup = {o.get(key): o for o in l2} for obj1 in l1: obj2 = l2_lookup.get(obj1.get(key)) for k in obj1: <IF_STMT> self.assertEqual(obj1.get(k), obj2.get(k))",if k not in 'id' and obj1.get(k):
"def before_get_object(self, view_kwargs): if view_kwargs.get('id') is not None: try: user_favourite_event = find_user_favourite_event_by_id(event_id=view_kwargs['id']) except NoResultFound: raise ObjectNotFound({'source': '/data/relationships/event'}, 'Object: not found') else: <IF_STMT> view_kwargs['id'] = user_favourite_event.id else: view_kwargs['id'] = None",if user_favourite_event is not None:
"def close(self): super().close() if not sys.is_finalizing(): for sig in list(self._signal_handlers): self.remove_signal_handler(sig) el<IF_STMT> warnings.warn(f'Closing the loop {self!r} on interpreter shutdown stage, skipping signal handlers removal', ResourceWarning, source=self) self._signal_handlers.clear()",if self._signal_handlers:
"def install_script(self, script, install_options=None): try: fname = utils.do_script(script, python_exe=osp.join(self.target, 'python.exe'), architecture=self.architecture, verbose=self.verbose, install_options=install_options) except RuntimeError: <IF_STMT> print('Failed!') raise",if not self.verbose:
"def GetRouterForUser(self, username): """"""Returns a router corresponding to a given username."""""" for index, router in enumerate(self.routers): router_id = str(index) <IF_STMT> logging.debug('Matched router %s to user %s', router.__class__.__name__, username) return router logging.debug('No router ACL rule match for user %s. Using default router %s', username, self.default_router.__class__.__name__) return self.default_router","if self.auth_manager.CheckPermissions(username, router_id):"
"def charset(self): """"""The charset from the content type."""""" header = self.environ.get('CONTENT_TYPE') if header: ct, options = parse_options_header(header) charset = options.get('charset') <IF_STMT> if is_known_charset(charset): return charset return self.unknown_charset(charset) return self.default_charset",if charset:
def isFinished(self): if self.count > self.epiLen: self.res() return True else: if self.count == 1: self.pertGlasPos(0) <IF_STMT> self.env.reset() self.pertGlasPos(1) self.count += 1 return False,if self.count == self.epiLen / 2 + 1:
"def mtimes_of_files(dirnames: List[str], suffix: str) -> Iterator[float]: for dirname in dirnames: for root, dirs, files in os.walk(dirname): for sfile in files: <IF_STMT> try: yield path.getmtime(path.join(root, sfile)) except OSError: pass",if sfile.endswith(suffix):
"def get_all_hashes(self): event_hashes = [] sample_hashes = [] for a in self.event.attributes: h = None if a.type in ('md5', 'sha1', 'sha256'): h = a.value event_hashes.append(h) elif a.type in ('filename|md5', 'filename|sha1', 'filename|sha256'): h = a.value.split('|')[1] event_hashes.append(h) <IF_STMT> h = a.value.split('|')[1] sample_hashes.append(h) return (event_hashes, sample_hashes)",elif a.type == 'malware-sample':
"def _validate(self, event): if self.type is None: return new = self.value if not isinstance(new, self.type) and new is not None: <IF_STMT> self.value = event.old types = repr(self.type) if isinstance(self.type, tuple) else self.type.__name__ raise ValueError('LiteralInput expected %s type but value %s is of type %s.' % (types, new, type(new).__name__))",if event:
"def update_dict(a, b): for key, value in b.items(): <IF_STMT> continue if key not in a: a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",if value is None:
"def on_pre_save(self, view): extOrClause = '|'.join(s.get('format_on_save_extensions')) extRegex = '\\.(' + extOrClause + ')$' if s.get('format_on_save') and re.search(extRegex, view.file_name()): lints_regions = ['lint-keyword-underline', 'lint-keyword-outline'] for linter in lints_regions: <IF_STMT> return view.run_command('js_format')",if len(view.get_regions(linter)):
"def readMemory(self, va, size): for mva, mmaxva, mmap, mbytes in self._map_defs: if mva <= va < mmaxva: mva, msize, mperms, mfname = mmap <IF_STMT> raise envi.SegmentationViolation(va) offset = va - mva return mbytes[offset:offset + size] raise envi.SegmentationViolation(va)",if not mperms & MM_READ:
"def assertFilepathsEqual(self, p1, p2): if sys.platform == 'win32': <IF_STMT> p1 = [normcase(normpath(x)) for x in p1] p2 = [normcase(normpath(x)) for x in p2] else: assert isinstance(p1, (str, unicode)) p1 = normcase(normpath(p1)) p2 = normcase(normpath(p2)) self.assertEqual(p1, p2)","if isinstance(p1, (list, tuple)):"
"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) if isdir(path): paths = add_directory_csv_files(path, paths) <IF_STMT> paths.append(path) return paths",elif isfile(path) and path.endswith('.csv'):
"def _verifySubs(self): for inst in self.subs: <IF_STMT> raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))","if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):"
"def __annotations_bytes(self): if self.annotations: a = [] for k, v in self.annotations.items(): if len(k) != 4: raise errors.ProtocolError('annotation key must be of length 4') <IF_STMT> k = k.encode('ASCII') a.append(struct.pack('!4sH', k, len(v))) a.append(v) return b''.join(a) return b''","if sys.version_info >= (3, 0):"
"def session(self, profile: str='default', region: str=None) -> boto3.Session: region = self._get_region(region, profile) try: session = self._cache_lookup(self._session_cache, [profile, region], self._boto3.Session, [], {'region_name': region, 'profile_name': profile}) except ProfileNotFound: <IF_STMT> raise session = self._boto3.Session(region_name=region) self._cache_set(self._session_cache, [profile, region], session) return session",if profile != 'default':
"def spans_score(gold_spans, system_spans): correct, gi, si = (0, 0, 0) while gi < len(gold_spans) and si < len(system_spans): if system_spans[si].start < gold_spans[gi].start: si += 1 <IF_STMT> gi += 1 else: correct += gold_spans[gi].end == system_spans[si].end si += 1 gi += 1 return Score(len(gold_spans), len(system_spans), correct)",elif gold_spans[gi].start < system_spans[si].start:
"def to_api(tag, raw_value): try: api_tag, converter = _QL_TO_SC[tag] if tag else ('q', None) except KeyError: <IF_STMT> raise self.error(""Unsupported '%s' tag. Try: %s"" % (tag, ', '.join(SUPPORTED))) return (None, None) else: value = str(converter(raw_value) if converter else raw_value) return (api_tag, value)",if tag not in SUPPORTED:
"def unpack(self, buf): dpkt.Packet.unpack(self, buf) buf = buf[self.__hdr_len__:] if self.type & 128: self.len = 0 self.data = b'' else: <IF_STMT> self.len = struct.unpack('>H', buf[:2])[0] buf = buf[2:] else: self.len = struct.unpack('B', buf[:1])[0] buf = buf[1:] self.data = buf[:self.len]",if self.type == USER_TO_USER:
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): <IF_STMT> return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:
"def _text(bitlist): out = '' for typ, text in bitlist: if not typ: out += text <IF_STMT> out += '\\fI%s\\fR' % text elif typ in ['strong', 'code']: out += '\\fB%s\\fR' % text else: raise ValueError('unexpected tag %r inside text' % (typ,)) out = out.strip() out = re.sub(re.compile('^\\s+', re.M), '', out) return out",elif typ == 'em':
"def process(self, buckets): with self.executor_factory(max_workers=3) as w: futures = {} results = [] for b in buckets: futures[w.submit(self.process_bucket, b)] = b for f in as_completed(futures): <IF_STMT> b = futures[f] self.log.error('error modifying bucket:%s\n%s', b['Name'], f.exception()) results += filter(None, [f.result()]) return results",if f.exception():
"def check_settings(self): if self.settings_dict['TIME_ZONE'] is not None: <IF_STMT> raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because USE_TZ is False."" % self.alias) elif self.features.supports_timezones: raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because its engine handles time zones conversions natively."" % self.alias)",if not settings.USE_TZ:
"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: if each: <IF_STMT> key, value = each.split('=', 1) else: key, value = (each, '') result[key] = value namespace.webhook_properties = result",if '=' in each:
"def _expand_query_values(original_query_list): query_list = [] for key, value in original_query_list: <IF_STMT> query_list.append((key, value)) else: key_fmt = key + '[%s]' value_list = _to_kv_list(value) query_list.extend(((key_fmt % k, v) for k, v in value_list)) return query_list","if isinstance(value, basestring):"
"def tags(): """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}."""""" tags = {} for n, c in list_refs(): if n.startswith('refs/tags/'): name = n[10:] <IF_STMT> tags[c] = [] tags[c].append(name) return tags",if not c in tags:
"def test_colorspiral(self): """"""Set of 625 colours, with jitter, using get_colors()."""""" boxedge = 20 boxes_per_row = 25 rows = 0 for i, c in enumerate(get_colors(625)): self.c.setFillColor(c) x1 = boxedge * (i % boxes_per_row) y1 = rows * boxedge self.c.rect(x1, y1, boxedge, boxedge, fill=1, stroke=0) <IF_STMT> rows += 1 self.finish()",if not (i + 1) % boxes_per_row:
"def oldest_pending_update_in_days(): """"""Return the datestamp of the oldest pending update"""""" pendingupdatespath = os.path.join(prefs.pref('ManagedInstallDir'), 'UpdateNotificationTracking.plist') try: pending_updates = FoundationPlist.readPlist(pendingupdatespath) except FoundationPlist.NSPropertyListSerializationException: return 0 oldest_date = now = NSDate.date() for category in pending_updates: for name in pending_updates[category]: this_date = pending_updates[category][name] <IF_STMT> oldest_date = this_date return now.timeIntervalSinceDate_(oldest_date) / (24 * 60 * 60)",if this_date < oldest_date:
"def _try_read_gpg(path): path = os.path.expanduser(path) cmd = _gpg_cmd() + [path] log.debug('gpg cmd: %s', cmd) try: p = subprocess.Popen(cmd, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE) except OSError as e: log.error(""cannot decode %s with command '%s' (%s)"", path, ' '.join(cmd), e) else: out, err = p.communicate() <IF_STMT> log.error(err.decode(errors='replace').strip()) return None return out.decode(errors='replace')",if p.returncode != 0:
"def sort_nested_dictionary_lists(d): for k, v in d.items(): <IF_STMT> for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v, list):"
"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, '__call__'): <IF_STMT> args = [point, index] else: args = [point] if pass_widget: args.append(widget) try_callback(callback, *args) return",if num > 1:
"def _add_cs(master_cs, sub_cs, prefix, delimiter='.', parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) <IF_STMT> new_parameter.name = prefix elif not prefix == '': new_parameter.name = '{}{}{}'.format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",if new_parameter.name == '':
"def tearDown(self): """"""Shutdown the server."""""" try: <IF_STMT> self.server.stop() if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:
"def app_uninstall_all(self, excludes=[], verbose=False): """"""Uninstall all apps"""""" our_apps = ['com.github.uiautomator', 'com.github.uiautomator.test'] output, _ = self.shell(['pm', 'list', 'packages', '-3']) pkgs = re.findall('package:([^\\s]+)', output) pkgs = set(pkgs).difference(our_apps + excludes) pkgs = list(pkgs) for pkg_name in pkgs: <IF_STMT> print('uninstalling', pkg_name, ' ', end='', flush=True) ok = self.app_uninstall(pkg_name) if verbose: print('OK' if ok else 'FAIL') return pkgs",if verbose:
"def httpapi(self, arg, opts): sc = HttpAPIStatsCollector() headers = ['#Item', 'Value'] table = [] for k, v in sc.get().getStats().items(): if isinstance(v, dict): v = json.dumps(v) row = [] row.append('#%s' % k) <IF_STMT> row.append(formatDateTime(v)) else: row.append(v) table.append(row) self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))",if k[-3:] == '_at':
"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = '' for line in string.split(entry, '\n'): <IF_STMT> GN = string.strip(line[5:]) if GN[-1] == '.': GN = GN[0:-1] return GN if line[0:2] == '//': break return GN",if line[0:5] == 'GN   ':
"def replace_dir_vars(path, d): """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})"""""" dirvars = {} for var in sorted(list(d.keys()), key=len): if var.endswith('dir') and var.lower() == var: value = d.getVar(var) <IF_STMT> dirvars[value] = var for dirpath in sorted(list(dirvars.keys()), reverse=True): path = path.replace(dirpath, '${%s}' % dirvars[dirpath]) return path",if value.startswith('/') and (not '\n' in value) and (value not in dirvars):
"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, 'r') as f: lines = f.readlines() if len(lines) < 1: return with open(source, 'w') as f: <IF_STMT> f.write(lines[0]) for line in lines[1:]: f.write(line)",if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: <IF_STMT> all_plugins += [plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",if name is None:
"def test_query_level(self): """"""Tests querying at a level other than max"""""" l2 = set() for p in self.tile_paths: l2.add(p[0:2]) for path in iterate_base4(2): <IF_STMT> self.assertTrue(self.tree.query_path(path)) else: self.assertFalse(self.tree.query_path(path)) self.assertTrue(self.tree.query_path((0,))) self.assertTrue(self.tree.query_path((1,))) self.assertTrue(self.tree.query_path((2,))) self.assertFalse(self.tree.query_path((3,)))",if path in l2:
"def program_exists(name): paths = (os.getenv('PATH') or os.defpath).split(os.pathsep) for p in paths: fn = '%s/%s' % (p, name) <IF_STMT> return not os.path.isdir(fn) and os.access(fn, os.X_OK)",if os.path.exists(fn):
"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) <IF_STMT> keywargs.update(arg) elif patching.new is DEFAULT: extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",if patching.attribute_name is not None:
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) <IF_STMT> rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.ENABLED:
"def calcUniqueStates(self): self.uniqueStates = {} for k in self.holdUniqueStates.keys(): v = self.holdUniqueStates[k] <IF_STMT> self.uniqueStates[k] = v.keys()[0] log.debug('Map style [%s] to state [%s]', k, v.keys()[0]) log.debug('Style [%s] maps to states [%s]', k, ', '.join(v.keys())) self.holdUniqueStates = None",if len(v.keys()) == 1:
"def init_logger(): configured_loggers = [log_config.get('root', {})] + [logger for logger in log_config.get('loggers', {}).values()] used_handlers = {handler for log in configured_loggers for handler in log.get('handlers', [])} for handler_id, handler in list(log_config['handlers'].items()): <IF_STMT> del log_config['handlers'][handler_id] elif 'filename' in handler.keys(): filename = handler['filename'] logfile_path = Path(filename).expanduser().resolve() handler['filename'] = str(logfile_path) logging.config.dictConfig(log_config)",if handler_id not in used_handlers:
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: <IF_STMT> selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines",if self._args.host and self._args.host == machine.name:
def init(self): r = self.get_redis() if r: key = 'pocsuite_target' info_msg = '[PLUGIN] try fetch targets from redis...' logger.info(info_msg) targets = r.get(key) count = 0 if targets: for target in targets: <IF_STMT> count += 1 info_msg = '[PLUGIN] get {0} target(s) from redis'.format(count) logger.info(info_msg),if self.add_target(target):
def tearDown(self): suffix = str(os.getgid()) cli = monitoring_v3.MetricServiceClient() for md in cli.list_metric_descriptors('projects/{}'.format(PROJECT)): <IF_STMT> try: cli.delete_metric_descriptor(md.name) except Exception: pass,if 'OpenCensus' in md.name and suffix in md.name:
"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) if c.IsOk(): self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE <IF_STMT> self._colourSelection = i",if c == curr:
"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) <IF_STMT> feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",if feature:
"def _get_data_from_buffer(obj): try: view = memoryview(obj) except TypeError: <IF_STMT> view = memoryview(buffer(obj)) warnings.warn('using old buffer interface to unpack %s; this leads to unpacking errors if slicing is used and will be removed in a future version' % type(obj), RuntimeWarning, stacklevel=3) else: raise if view.itemsize != 1: raise ValueError('cannot unpack from multi-byte object') return view",if PY2:
"def import_modules(modules, safe=True): """"""Safely import a list of *modules*"""""" all = [] for mname in modules: if mname.endswith('.*'): to_load = expand_star(mname) else: to_load = [mname] for module in to_load: try: all.append(import_module(module)) except ImportError: <IF_STMT> raise return all",if not safe:
"def pack(types, *args): if len(types) != len(args): raise Exception('number of arguments does not match format string') port = StringIO() for type, value in zip(types, args): if type == 'V': write_vuint(port, value) <IF_STMT> write_vint(port, value) elif type == 's': write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",elif type == 'v':
"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): <IF_STMT> os.mkdir(folder) init_path = join(folder, '__init__.py') if not exists(init_path): create_file(init_path)",if not exists(folder):
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] <IF_STMT> raise ValueError(f'Unknown node type tag: {node_type}.') node_specific_config = self.available_node_types[node_type] if fields_key in node_specific_config: fields = node_specific_config[fields_key] return fields",if node_type not in self.available_node_types:
"def _maybe_fix_sequence_in_union(aliases: List[Alias], typecst: cst.SubscriptElement) -> cst.SubscriptElement: slc = typecst.slice if isinstance(slc, cst.Index): val = slc.value <IF_STMT> return cst.ensure_type(typecst.deep_replace(val, _get_clean_type_from_subscript(aliases, val)), cst.SubscriptElement) return typecst","if isinstance(val, cst.Subscript):"
"def cancel_download(self, downloads): if isinstance(downloads, Download): downloads = [downloads] for download in downloads: <IF_STMT> self.cancel_current_download() else: self.__paused = True new_queue = queue.Queue() while not self.__queue.empty(): queued_download = self.__queue.get() if download == queued_download: download.cancel() else: new_queue.put(queued_download) self.__queue = new_queue self.__paused = False",if download == self.__current_download:
"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) if account.discriminator == 'easaccount': create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) <IF_STMT> set_labels_for_imapuids(account, db_session) db_session.commit()",if account.discriminator == 'gmailaccount':
"def __init__(self, fmt=None, *args): if not isinstance(fmt, BaseException): Error.__init__(self, fmt, *args) else: e = fmt cls = e.__class__ fmt = '%s.%s: %s' % (cls.__module__, cls.__name__, e) tb = sys.exc_info()[2] <IF_STMT> fmt += '\n' fmt += ''.join(traceback.format_tb(tb)) Error.__init__(self, fmt)",if tb:
"def setLabel(self, label): if label is None: <IF_STMT> self.label.scene().removeItem(self.label) self.label = None else: if self.label is None: self.label = TextItem() self.label.setParentItem(self) self.label.setText(label) self._updateLabel()",if self.label is not None:
"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) <IF_STMT> self.handle_request() if self.event is not None and self.event.is_set(): break",if rd:
"def generateCompressedFile(inputfile, outputfile, formatstring): try: <IF_STMT> in_file = open(inputfile, 'rb') in_data = in_file.read() out_file = open(inputfile + '.xz', 'wb') out_file.write(xz.compress(in_data)) in_file.close() out_file.close() else: tarout = tarfile.open(outputfile, formatstring) tarout.add(inputfile, arcname=os.path.basename(inputfile)) tarout.close() except Exception as e: print(e) return False return True",if formatstring == 'w:xz':
"def _datastore_get_handler(signal, sender, keys, **kwargs): txn = current_transaction() if txn: for key in keys: <IF_STMT> raise PreventedReadError('Attempted to read key (%s:%s) inside a transaction where it was marked protected' % (key.kind(), key.id_or_name())) txn._fetched_keys.update(set(keys))",if key in txn._protected_keys:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_access_token(d.getPrefixedString()) continue if tt == 16: self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def write_vuint(port, x): if x < 0: raise Exception('vuints must not be negative') elif x == 0: port.write('\x00') else: while x: seven_bits = x & 127 x >>= 7 <IF_STMT> port.write(chr(128 | seven_bits)) else: port.write(chr(seven_bits))",if x:
"def _expand_srcs(self): """"""Expand src to [(src, full_path)]"""""" result = [] for src in self.srcs: full_path = self._source_file_path(src) <IF_STMT> full_path = self._target_file_path(src) result.append((src, full_path)) return result",if not os.path.exists(full_path):
def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/ops'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:
"def set_shape(self, shape): """"""Sets a shape."""""" if self._shape is not None: logger.warning('Modifying the shape of Placeholder ""%s"".', self.name) if not isinstance(shape, (list, tuple)): shape = (shape,) shape = tuple((x if x != 'None' else None for x in shape)) for x in shape: <IF_STMT> raise ParsingError('All entries in ""shape"" must be integers, or in special cases None. Shape is: {}'.format(shape)) self._shape = shape","if not isinstance(x, (int, type(None))):"
"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if ':' in line: left, right = line.split(':', 1) left = left.strip().lower() right = right.strip() <IF_STMT> if cant_be_number: if not right.isdigit(): return right else: return right return None",if left == field_name and len(right) > 0:
"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) if value is None: <IF_STMT> raise ValidationError(_('%(attr)s attribute cannot be blank') % {'attr': attribute.code}) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError(_('%(attr)s attribute %(err)s') % {'attr': attribute.code, 'err': e})",if attribute.required:
"def append(self, s): buf = self.buf if buf is None: strbuf = self.strbuf <IF_STMT> self.strbuf = strbuf + s return buf = self._create_buffer() buf.append(s) sz = buf.__len__() if not self.overflowed: if sz >= self.overflow: self._set_large_buffer()",if len(strbuf) + len(s) < STRBUF_LIMIT:
"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import RequiredArgumentMissingError, MutuallyExclusiveArgumentError valid_combs = 'only --account-name, --name / --name / --name, --by-subscription is valid' if namespace.account_name is not None: <IF_STMT> raise MutuallyExclusiveArgumentError(valid_combs) if namespace.name is None: raise RequiredArgumentMissingError('--name is also required') if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError('--name is also required')",if namespace.by_subscription is not None:
"def Handle(self, args, context=None): for client_id in args.client_ids: cid = str(client_id) data_store.REL_DB.RemoveClientLabels(cid, context.username, args.labels) labels_to_remove = set(args.labels) existing_labels = data_store.REL_DB.ReadClientLabels(cid) for label in existing_labels: labels_to_remove.discard(label.name) <IF_STMT> idx = client_index.ClientIndex() idx.RemoveClientLabels(cid, labels_to_remove)",if labels_to_remove:
"def delete_snapshot(self, snapshot): snap_name = self._get_snap_name(snapshot['id']) LOG.debug('Deleting snapshot (%s)', snapshot['id']) self.client_login() try: self.client.delete_snapshot(snap_name, self.backend_type) except exception.DotHillRequestError as ex: <IF_STMT> return LOG.exception('Deleting snapshot %s failed', snapshot['id']) raise exception.Invalid(ex) finally: self.client_logout()",if 'The volume was not found on this system.' in ex.args:
def jobs(self): total_processed = 0 for jobEntity in self.jobItems.query_entities(): yield AzureJob.fromEntity(jobEntity) total_processed += 1 <IF_STMT> logger.debug('Processed %d total jobs' % total_processed) logger.debug('Processed %d total jobs' % total_processed),if total_processed % 1000 == 0:
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() <IF_STMT> self.stop(),if self.counter == self.count:
"def get_instance(cls, pool_size=None): if cls._instance is not None: return cls._instance with cls._SINGLETON_LOCK: <IF_STMT> cls._instance = cls(ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size) return cls._instance",if cls._instance is None:
"def set_state(self, state): if self._inhibit_play: <IF_STMT> self.__set_inhibit_play(False) self.bin.set_state(state) return self._wanted_state = state else: self.bin.set_state(state)","if state not in (Gst.State.PLAYING, Gst.State.PAUSED):"
"def seen_add(options): seen_name = options.add_value if is_imdb_url(seen_name): console('IMDB url detected, try to parse ID') imdb_id = extract_id(seen_name) <IF_STMT> seen_name = imdb_id else: console('Could not parse IMDB ID') db.add(seen_name, 'cli_add', {'cli_add': seen_name}) console('Added %s as seen. This will affect all tasks.' % seen_name)",if imdb_id:
"def test_204_invalid_content_length(self): with ExpectLog(gen_log, '.*Response with code 204 should not have body'): response = self.fetch('/?error=1') if not self.http1: self.skipTest('requires HTTP/1.x') <IF_STMT> self.skipTest('curl client accepts invalid headers') self.assertEqual(response.code, 599)",if self.http_client.configured_class != SimpleAsyncHTTPClient:
"def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> None: src_class = target.cls_model id_ = target.datasource_id if id_: ds = db.session.query(src_class).filter_by(id=int(id_)).first() <IF_STMT> target.perm = ds.perm target.schema_perm = ds.schema_perm",if ds:
"def on_modified_async(self, view): if self.is_command_line(view): <IF_STMT> view.run_command('text_pastry_selection_preview')","if view.size() > 6 and view.substr(sublime.Region(0, 6)).lower() == 'search':"
"def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text): """"""Returns tokenized answer spans that better match the annotated answer."""""" tok_answer_text = ' '.join(tokenizer.tokenize(orig_answer_text)) for new_start in range(input_start, input_end + 1): for new_end in range(input_end, new_start - 1, -1): text_span = ' '.join(doc_tokens[new_start:new_end + 1]) <IF_STMT> return (new_start, new_end) return (input_start, input_end)",if text_span == tok_answer_text:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_url(d.getPrefixedString()) continue if tt == 18: self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: <IF_STMT> if obj['id']: obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != 'sourceVault': _add_resource_group(obj[item_key])",if 'resourcegroup' not in [x.lower() for x in obj.keys()]:
"def build(opt): dpath = os.path.join(opt['datapath'], DECODE) version = DECODE_VERSION if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def toterminal(self, tw): last_style = None for i, entry in enumerate(self.reprentries): <IF_STMT> tw.line('') entry.toterminal(tw) if i < len(self.reprentries) - 1: next_entry = self.reprentries[i + 1] if entry.style == 'long' or (entry.style == 'short' and next_entry.style == 'long'): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",if entry.style == 'long':
"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): <IF_STMT> found += 1 if found == 2: if division in '\n'.join(lines): break lines.insert(i + 1, '') lines.insert(i + 2, division) break return '\n'.join(lines)","if line.startswith('""""""'):"
def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: if self.base.conf.strict: <IF_STMT> raise e if e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS: raise e logger.error(str(e)),if e.no_match_group_specs or e.error_group_specs:
"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len('hello'), len(base64mime.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <IF_STMT> bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len('x' * size), bsize)",elif size <= 9:
"def is_valid(self): """"""Determines whether file is valid for this reader"""""" blocklist = self.open() valid = True for line in blocklist: line = decode_bytes(line) <IF_STMT> try: start, end = self.parse(line) if not re.match('^(\\d{1,3}\\.){4}$', start + '.') or not re.match('^(\\d{1,3}\\.){4}$', end + '.'): valid = False except Exception: valid = False break blocklist.close() return valid",if not self.is_ignored(line):
"def next(self): while self.index < len(self.data): uid = self._read_next_word() dont_care = self._read_next_word() entry = self._read_next_string() total_size = int(4 + 4 + len(entry)) count = int(total_size / self.SIZE) if count == 0: mod = self.SIZE - total_size else: mod = self.SIZE - int(total_size - count * self.SIZE) <IF_STMT> remainder = self._read_next_block(mod) yield (uid, entry)",if mod > 0:
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) if param.type: parts.append(param.type) out += [' : '.join(parts)] <IF_STMT> out += self._str_indent(param.desc) out += [''] return out",if param.desc and ''.join(param.desc).strip():
"def assert_backend(self, expected_translated, language='cs'): """"""Check that backend has correct data."""""" translation = self.get_translation(language) translation.commit_pending('test', None) store = translation.component.file_format_cls(translation.get_filename(), None) messages = set() translated = 0 for unit in store.content_units: id_hash = unit.id_hash self.assertFalse(id_hash in messages, 'Duplicate string in in backend file!') <IF_STMT> translated += 1 self.assertEqual(translated, expected_translated, 'Did not found expected number of translations ({} != {}).'.format(translated, expected_translated))",if unit.is_translated():
"def status(self, name, error='No matching script logs found'): with self.script_lock: <IF_STMT> return self.script_running[1:] elif self.script_last and self.script_last[1] == name: return self.script_last[1:] else: raise ValueError(error)",if self.script_running and self.script_running[1] == name:
"def dict_no_value_from_proto_list(obj_list): d = dict() for item in obj_list: possible_dict = json.loads(item.value_json) <IF_STMT> logger.warning(""key '{}' has no 'value' attribute"".format(item.key)) continue d[item.key] = possible_dict['value'] return d","if not isinstance(possible_dict, dict) or 'value' not in possible_dict:"
"def visit(self, node): """"""dispatcher on node's class/bases name."""""" cls = node.__class__ try: visitmethod = self.cache[cls] except KeyError: for subclass in cls.__mro__: visitmethod = getattr(self, subclass.__name__, None) <IF_STMT> break else: visitmethod = self.__object self.cache[cls] = visitmethod visitmethod(node)",if visitmethod is not None:
"def _get_adapter(mcls, reversed_mro: Tuple[type, ...], collection: Dict[Any, Dict[type, Adapter]], kwargs: Dict[str, Any]) -> Optional[Adapter]: registry_key = mcls.get_registry_key(kwargs) adapters = collection.get(registry_key) if adapters is None: return None result = None seen: Set[Adapter] = set() for base in reversed_mro: for adaptee, adapter in adapters.items(): found = mcls._match_adapter(base, adaptee, adapter) <IF_STMT> result = found seen.add(found) return result",if found and found not in seen:
"def test_pt_BR_rg(self): for _ in range(100): to_test = self.fake.rg() <IF_STMT> assert re.search('^\\d{8}X', to_test) else: assert re.search('^\\d{9}$', to_test)",if 'X' in to_test:
"def get_user_extra_data_by_client_id(self, client_id, username): extra_data = {} current_client = self.clients.get(client_id, None) if current_client: for readable_field in current_client.get_readable_fields(): attribute = list(filter(lambda f: f['Name'] == readable_field, self.users.get(username).attributes)) <IF_STMT> extra_data.update({attribute[0]['Name']: attribute[0]['Value']}) return extra_data",if len(attribute) > 0:
"def augment(self, resources): super().augment(resources) for r in resources: md = r.get('SAMLMetadataDocument') <IF_STMT> continue root = sso_metadata(md) r['IDPSSODescriptor'] = root['IDPSSODescriptor'] return resources",if not md:
"def __init__(self, mode=0, decode=None): self.regex = self.REGEX[mode] self.decode = decode if decode: self.header = _('### This log has been decoded with automatic search pattern\n### If some paths are not decoded you can manually decode them with:\n') self.header += ""### 'backintime --quiet "" <IF_STMT> self.header += '--profile ""%s"" ' % decode.config.profileName() self.header += ""--decode <path>'\n\n"" else: self.header = ''",if int(decode.config.currentProfile()) > 1:
"def _get_dynamic_attr(self, attname, obj, default=None): try: attr = getattr(self, attname) except AttributeError: return default if callable(attr): try: code = six.get_function_code(attr) except AttributeError: code = six.get_function_code(attr.__call__) <IF_STMT> return attr(obj) else: return attr() return attr",if code.co_argcount == 2:
"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 <IF_STMT> continue while i + 1 < len(tokens) and tokens[i] == ('op', '.') and (tokens[i + 1][0] == 'id'): token += '.' + tokens[i + 1][1] i += 2 if token == '': continue if token in pykeywords: continue if token[0] in '.0123456789': continue yield token",if tokentype != 'id':
"def _add_disk_config(self, context, images): for image in images: metadata = image['metadata'] <IF_STMT> raw_value = metadata[INTERNAL_DISK_CONFIG] value = utils.bool_from_str(raw_value) image[API_DISK_CONFIG] = disk_config_to_api(value)",if INTERNAL_DISK_CONFIG in metadata:
"def test_edgeql_expr_valid_setop_07(self): expected_error_msg = 'cannot be applied to operands' for val in get_test_values(): query = f'SELECT 1 IF {val} ELSE 2;' <IF_STMT> await self.assert_query_result(query, [1]) else: with self.assertRaisesRegex(edgedb.QueryError, expected_error_msg, msg=query): async with self.con.transaction(): await self.con.execute(query)",if val == '<bool>True':
"def get_all_url_infos() -> Dict[str, UrlInfo]: """"""Returns dict associating URL to UrlInfo."""""" url_infos = {} for path in _checksum_paths().values(): dataset_url_infos = load_url_infos(path) for url, url_info in dataset_url_infos.items(): <IF_STMT> raise AssertionError('URL {} is registered with 2+ distinct size/checksum tuples. {} vs {}'.format(url, url_info, url_infos[url])) url_infos.update(dataset_url_infos) return url_infos","if url_infos.get(url, url_info) != url_info:"
"def global_fixes(): """"""Yield multiple (code, function) tuples."""""" for function in list(globals().values()): <IF_STMT> arguments = _get_parameters(function) if arguments[:1] != ['source']: continue code = extract_code_from_function(function) if code: yield (code, function)",if inspect.isfunction(function):
"def createSocket(self): skt = Port.createSocket(self) if self.listenMultiple: skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) <IF_STMT> skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) return skt","if hasattr(socket, 'SO_REUSEPORT'):"
"def _asStringList(self, sep=''): out = [] for item in self._toklist: if out and sep: out.append(sep) <IF_STMT> out += item._asStringList() else: out.append(str(item)) return out","if isinstance(item, ParseResults):"
"def parse_c_comments(lexer, tok, ntok): if tok != '/' or ntok != '*': return False quotes = lexer.quotes lexer.quotes = '' while True: tok = lexer.get_token() ntok = lexer.get_token() <IF_STMT> lexer.quotes = quotes break else: lexer.push_token(ntok) return True",if tok == '*' and ntok == '/':
"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) <IF_STMT> v.append(sibling) if sibling.getFirstChild(): sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",if c2:
"def __view_beside(self, onsideof, **kwargs): bounds = self.info['bounds'] min_dist, found = (-1, None) for ui in UiObject(self.session, Selector(**kwargs)): dist = onsideof(bounds, ui.info['bounds']) <IF_STMT> min_dist, found = (dist, ui) return found",if dist >= 0 and (min_dist < 0 or dist < min_dist):
"def __eq__(self, other): if isinstance(other, numeric_range): empty_self = not bool(self) empty_other = not bool(other) <IF_STMT> return empty_self and empty_other else: return self._start == other._start and self._step == other._step and (self._get_by_index(-1) == other._get_by_index(-1)) else: return False",if empty_self or empty_other:
"def _buffered_generator(self, size): buf = [] c_size = 0 push = buf.append while 1: try: while c_size < size: c = next(self._gen) push(c) if c: c_size += 1 except StopIteration: <IF_STMT> return yield concat(buf) del buf[:] c_size = 0",if not c_size:
"def connect(self): with self._conn_lock: <IF_STMT> raise Exception('Error, database not properly initialized before opening connection') with self.exception_wrapper(): self.__local.conn = self._connect(self.database, **self.connect_kwargs) self.__local.closed = False self.initialize_connection(self.__local.conn)",if self.deferred:
"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): <IF_STMT> subst[name] = var elif subst[name] is not var: subst[name].PasteVariable(var) return subst",if name not in subst:
"def remove(self, tag): """"""Removes a tag recursively from all containers."""""" new_contents = [] self.content_size = 0 for element in self.contents: if element.name != tag: new_contents.append(element) <IF_STMT> element.remove(tag) self.content_size += element.size() self.contents = new_contents","if isinstance(element, Container):"
"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): if isinstance(prop_value, dict) and prop_value: func_name = list(prop_value.keys())[0] <IF_STMT> func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props",if func_name.startswith('_'):
"def visit_try_stmt(self, o: 'mypy.nodes.TryStmt') -> str: a = [o.body] for i in range(len(o.vars)): a.append(o.types[i]) <IF_STMT> a.append(o.vars[i]) a.append(o.handlers[i]) if o.else_body: a.append(('Else', o.else_body.body)) if o.finally_body: a.append(('Finally', o.finally_body.body)) return self.dump(a, o)",if o.vars[i]:
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False <IF_STMT> return False elif isinstance(v, _bytes): return False return True","elif isinstance(i, _bytes):"
"def msg_ser(inst, sformat, lev=0): if sformat in ['urlencoded', 'json']: if isinstance(inst, Message): res = inst.serialize(sformat, lev) else: res = inst elif sformat == 'dict': if isinstance(inst, Message): res = inst.serialize(sformat, lev) <IF_STMT> res = inst elif isinstance(inst, str): res = inst else: raise MessageException('Wrong type: %s' % type(inst)) else: raise PyoidcError('Unknown sformat', inst) return res","elif isinstance(inst, dict):"
"def start_container_if_stopped(self, container, attach_logs=False, quiet=False): if not container.is_running: <IF_STMT> log.info('Starting %s' % container.name) if attach_logs: container.attach_log_stream() return self.start_container(container)",if not quiet:
"def layer_op(self, input_image, mask=None): if not isinstance(input_image, dict): self._set_full_border(input_image) input_image = np.pad(input_image, self.full_border, mode=self.mode) return (input_image, mask) for name, image in input_image.items(): self._set_full_border(image) <IF_STMT> tf.logging.warning('could not pad, dict name %s not in %s', name, self.image_name) continue input_image[name] = np.pad(image, self.full_border, mode=self.mode) return (input_image, mask)",if name not in self.image_name:
"def __Suffix_Noun_Step2b(self, token): for suffix in self.__suffix_noun_step2b: <IF_STMT> token = token[:-2] self.suffix_noun_step2b_success = True break return token",if token.endswith(suffix) and len(token) >= 5:
"def __projectBookmark(widget, location): script = None while widget is not None: <IF_STMT> script = widget.scriptNode() if isinstance(script, Gaffer.ScriptNode): break widget = widget.parent() if script is not None: p = script.context().substitute(location) if not os.path.exists(p): try: os.makedirs(p) except OSError: pass return p else: return os.getcwd()","if hasattr(widget, 'scriptNode'):"
"def events_to_str(event_field, all_events): result = [] for flag, string in all_events: c_flag = flag if event_field & c_flag: result.append(string) event_field = event_field & ~c_flag <IF_STMT> break if event_field: result.append(hex(event_field)) return '|'.join(result)",if not event_field:
"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get('Logging'): if self_log: if b['Name'] != b['Logging']['TargetBucket']: continue yield (b['Logging']['TargetBucket'], b['Logging']['TargetPrefix']) <IF_STMT> yield (b['Name'], '')",if not self_log and b['Name'].startswith('cf-templates-'):
"def extract_file(tgz, tarinfo, dst_path, buffer_size=10 << 20, log_function=None): """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'."""""" src = tgz.extractfile(tarinfo) if src is None: return dst = tf.compat.v1.gfile.GFile(dst_path, 'wb') while 1: buf = src.read(buffer_size) if not buf: break dst.write(buf) <IF_STMT> log_function(len(buf)) dst.close() src.close()",if log_function is not None:
"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): if k in ('lccn', 'oclc', 'isbn'): fields[k] = v continue <IF_STMT> fields['title'] = [read_short_title(v)] return fields",if k == 'full_title':
def disconnect_application(self): if not self.is_app_running(self.APP_BACKDROP): self.socket.send(commands.CloseCommand(destination_id=False)) start_time = time.time() while not self.is_app_running(None): try: self.socket.send_and_wait(commands.StatusCommand()) except cast_socket.ConnectionTerminatedException: break current_time = time.time() <IF_STMT> raise TimeoutException() time.sleep(self.WAIT_INTERVAL) else: logger.debug('Closing not necessary. Backdrop is running ...'),if current_time - start_time > self.timeout:
"def matches(self, cursor_offset, line, **kwargs): cs = lineparts.current_string(cursor_offset, line) if cs is None: return None matches = set() username = cs.word.split(os.path.sep, 1)[0] user_dir = os.path.expanduser(username) for filename in self.safe_glob(os.path.expanduser(cs.word)): if os.path.isdir(filename): filename += os.path.sep <IF_STMT> filename = username + filename[len(user_dir):] matches.add(filename) return matches",if cs.word.startswith('~'):
"def eventFilter(self, obj, event): if event.type() == QEvent.MouseButtonPress: button = event.button() <IF_STMT> self._app.browser.back() return True elif button == Qt.ForwardButton: self._app.browser.forward() return True return False",if button == Qt.BackButton:
"def reset_parameters(self): for m in self.modules(): if isinstance(m, nn.Embedding): continue <IF_STMT> nn.init.constant_(m.weight, 0.1) nn.init.constant_(m.bias, 0) else: for p in m.parameters(): nn.init.normal_(p, 0, 0.1)","elif isinstance(m, nn.LayerNorm):"
"def get_scalding_core(self): lib_dir = os.path.join(self.scalding_home, 'lib') for j in os.listdir(lib_dir): <IF_STMT> p = os.path.join(lib_dir, j) logger.debug('Found scalding-core: %s', p) return p raise luigi.contrib.hadoop.HadoopJobError('Could not find scalding-core.')",if j.startswith('scalding-core-'):
"def save(self): """"""Saves a new set of golden output frames to disk."""""" for pixels, (relative_to_assets, filename) in zip(self.iter_render(), self._iter_paths()): full_directory_path = os.path.join(self._ASSETS_DIR, relative_to_assets) <IF_STMT> os.makedirs(full_directory_path) path = os.path.join(full_directory_path, filename) _save_pixels(pixels, path)",if not os.path.exists(full_directory_path):
"def _fix_var_naming(operators, names, mod='input'): new_names = [] map = {} for op in operators: <IF_STMT> iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name if len(map) == len(names): break for name in names: new_names.append(map[name]) return new_names",if mod == 'input':
"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] <IF_STMT> typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",elif item[1]:
"def init_errorhandler(): for ex in default_exceptions: if ex < 500: app.register_error_handler(ex, error_http) <IF_STMT> app.register_error_handler(ex, internal_error) if services.ldap:  @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug('LDAP server not accessible while trying to login to opds feed') return error_http(FailedDependency())",elif ex == 500:
"def decode(self, ids): ids = pad_decr(ids) tokens = [] for int_id in ids: <IF_STMT> tokens.append(self._vocab_list[int_id]) else: tokens.append(self._oov_token) return self._decode_token_separator.join(tokens)",if int_id < len(self._vocab_list):
"def remove_contest(contest_id): with SessionGen() as session: contest = session.query(Contest).filter(Contest.id == contest_id).first() if not contest: print('No contest with id %s found.' % contest_id) return False contest_name = contest.name <IF_STMT> print(""Not removing contest `%s'."" % contest_name) return False session.delete(contest) session.commit() print(""Contest `%s' removed."" % contest_name) return True",if not ask(contest):
def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() if self.expr2 is None: pass else: lineno = self.expr2.get_hi_lineno() <IF_STMT> pass else: lineno = self.expr3.get_hi_lineno() return lineno,if self.expr3 is None:
"def _send_internal(self, bytes_): if self.pendings: self.pendings += bytes_ bytes_ = self.pendings try: self._reconnect() self.socket.sendall(bytes_) self.pendings = None except Exception: self._close() <IF_STMT> self.pendings = None else: self.pendings = bytes_",if self.pendings and len(self.pendings) > self.bufmax:
"def _unpack(self, fmt, byt): d = unpack(self._header['byteorder'] + fmt, byt)[0] if fmt[-1] in self.MISSING_VALUES: nmin, nmax = self.MISSING_VALUES[fmt[-1]] if d < nmin or d > nmax: <IF_STMT> return StataMissingValue(nmax, d) else: return None return d",if self._missing_values:
"def tuple_iter(self): for x in range(self.center.x - self.max_radius, self.center.x + self.max_radius + 1): for y in range(self.center.y - self.max_radius, self.center.y + self.max_radius + 1): <IF_STMT> yield (x, y)","if self.min_radius <= self.center.distance((x, y)) <= self.max_radius:"
"def _parse_gene(element): for genename_element in element: <IF_STMT> ann_key = 'gene_%s_%s' % (genename_element.tag.replace(NS, ''), genename_element.attrib['type']) if genename_element.attrib['type'] == 'primary': self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",if 'type' in genename_element.attrib:
"def invalidateDependentSlices(self, iFirstCurve): if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType): c.invalidate() <IF_STMT> break",elif i == iFirstCurve:
"def gen_app_versions(self): for app_config in apps.get_app_configs(): name = app_config.verbose_name app = app_config.module version = self.get_app_version(app) <IF_STMT> yield (app.__name__, name, version)",if version:
"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError('Empty chown path') checkpath = root parts = path.split(os.sep) for part in parts: if part in ('.', '..'): raise PackagerError('. and .. is not allowed in chown path') checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1:] <IF_STMT> raise PackagerError(f'chown path {relpath} does not exist') if os.path.islink(checkpath): raise PackagerError(f'chown path {relpath} is a soft link')",if not os.path.exists(checkpath):
"def create_or_update_tag_at_scope(cmd, resource_id=None, tags=None, tag_name=None): rcf = _resource_client_factory(cmd.cli_ctx) if resource_id is not None: <IF_STMT> raise IncorrectUsageError('Tags could not be empty.') Tags = cmd.get_models('Tags') tag_obj = Tags(tags=tags) return rcf.tags.create_or_update_at_scope(scope=resource_id, properties=tag_obj) return rcf.tags.create_or_update(tag_name=tag_name)",if not tags:
"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base if self.settings.get(IGNORE_CASE_SETTING): compare_entry = compare_entry.lower() compare_base = compare_base.lower() <IF_STMT> if entry not in sugg: sugg.append(entry) return sugg","if self.compare_entries(compare_entry, compare_base):"
"def createFields(self): yield String(self, 'dict_start', 2) while not self.eof: addr = self.absolute_address + self.current_size <IF_STMT> for field in parsePDFType(self): yield field else: break yield String(self, 'dict_end', 2)","if self.stream.readBytes(addr, 2) != '>>':"
"def Visit_and_test(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance(child, pytree.Leaf) and child.value == 'and':"
"def getfiledata(directories): columns = None data = [] counter = 1 for directory in directories: for f in os.listdir(directory): if not os.path.isfile(os.path.join(directory, f)): continue counter += 1 st = os.stat(os.path.join(directory, f)) <IF_STMT> columns = ['rowid', 'name', 'directory'] + [x for x in dir(st) if x.startswith('st_')] data.append([counter, f, directory] + [getattr(st, x) for x in columns[3:]]) return (columns, data)",if columns is None:
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) if value is None: continue name = name_fmt % attr <IF_STMT> value = formatter(attr, value) info_add(name, value)",if formatter is not None:
"def main(args): ap = argparse.ArgumentParser() ap.add_argument('job_ids', nargs='+', type=int, help='ID of a running job') ns = ap.parse_args(args) _stash = globals()['_stash'] ':type : StaSh' for job_id in ns.job_ids: <IF_STMT> print('killing job {} ...'.format(job_id)) worker = _stash.runtime.worker_registry.get_worker(job_id) worker.kill() time.sleep(1) else: print('error: no such job with id: {}'.format(job_id)) break",if job_id in _stash.runtime.worker_registry:
"def _check_choice(self): if self.type == 'choice': if self.choices is None: raise OptionError(""must supply a list of choices for type 'choice'"", self) <IF_STMT> raise OptionError(""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self) elif self.choices is not None: raise OptionError('must not supply choices for type %r' % self.type, self)","elif type(self.choices) not in (types.TupleType, types.ListType):"
"def add_file(pipe, srcpath, tgtpath): with open(srcpath, 'rb') as handle: <IF_STMT> write(pipe, enc('M 100755 inline %s\n' % tgtpath)) else: write(pipe, enc('M 100644 inline %s\n' % tgtpath)) data = handle.read() write(pipe, enc('data %d\n' % len(data))) write(pipe, enc(data)) write(pipe, enc('\n'))","if os.access(srcpath, os.X_OK):"
"def cdf(self, x): if x == numpy.inf: return 1.0 else: <IF_STMT> raise RuntimeError('Invalid value.') c = 0.0 for i in xrange(x + 1): c += self.probability(i) return c",if x != int(x):
"def convert_to_strings(self, out, seq_len): results = [] for b, batch in enumerate(out): utterances = [] for p, utt in enumerate(batch): size = seq_len[b][p] <IF_STMT> transcript = ''.join(map(lambda x: self.int_to_char[x.item()], utt[0:size])) else: transcript = '' utterances.append(transcript) results.append(utterances) return results",if size > 0:
"def get_date_range(self): if not hasattr(self, 'start') or not hasattr(self, 'end'): args = (self.today.year, self.today.month) form = self.get_form() <IF_STMT> args = (int(form.cleaned_data['year']), int(form.cleaned_data['month'])) self.start = self.get_start(*args) self.end = self.get_end(*args) return (self.start, self.end)",if form.is_valid():
"def save_stats(self): LOGGER.info('Saving task-level statistics.') has_headers = os.path.isfile(paths.TABLE_COUNT_PATH) with open(paths.TABLE_COUNT_PATH, 'a') as csvfile: headers = ['start_time', 'database_name', 'number_tables'] writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\n', fieldnames=headers) <IF_STMT> writer.writeheader() writer.writerow({'start_time': self.start_time, 'database_name': self.database_name, 'number_tables': self.count})",if not has_headers:
"def _CheckCanaryCommand(self): <IF_STMT> return with self._lock: if OpenStackVirtualMachine.command_works: return logging.info('Testing OpenStack CLI command is installed and working') cmd = os_utils.OpenStackCLICommand(self, 'image', 'list') stdout, stderr, _ = cmd.Issue() if stderr: raise errors.Config.InvalidValue('OpenStack CLI test command failed. Please make sure the OpenStack CLI client is installed and properly configured') OpenStackVirtualMachine.command_works = True",if OpenStackVirtualMachine.command_works:
"def test_windows_hidden(self): if not sys.platform == 'win32': self.skipTest('sys.platform is not windows') return hidden_mask = 2 with tempfile.NamedTemporaryFile() as f: success = ctypes.windll.kernel32.SetFileAttributesW(f.name, hidden_mask) <IF_STMT> self.skipTest('unable to set file attributes') self.assertTrue(hidden.is_hidden(f.name))",if not success:
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <IF_STMT> y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return b''.join(y)",elif r:
def _is_xml(accepts): if accepts.startswith(b'application/'): has_xml = accepts.find(b'xml') <IF_STMT> semicolon = accepts.find(b';') if semicolon < 0 or has_xml < semicolon: return True return False,if has_xml > 0:
"def times(self, value: int): if value is None: self._times = None else: try: candidate = int(value) except ValueError: raise BarException(f'cannot set repeat times to: {value!r}') if candidate < 0: raise BarException(f'cannot set repeat times to a value less than zero: {value}') <IF_STMT> raise BarException('cannot set repeat times on a start Repeat') self._times = candidate",if self.direction == 'start':
"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): if 'self' in stack[0].f_locals: layer = stack[0].f_locals['self'] <IF_STMT> log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) return out",if layer in layer_names:
"def do_begin(self, byte): if byte.isspace(): return if byte != '<': <IF_STMT> self._leadingBodyData = byte return 'bodydata' self._parseError(""First char of document [{!r}] wasn't <"".format(byte)) return 'tagstart'",if self.beExtremelyLenient:
"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) <IF_STMT> r = r.replace('*/', '\\x2a/') return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): if comment: return '%s /* %s */' % (n, self.pretty(int(n))) else: return '%s (%s)' % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",if not comment:
"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model(DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config='data/test_config/max_hist_config.yml', kwargs={}) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: if hasattr(policy.featurizer, 'max_history'): <IF_STMT> assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5",if type(policy) == FormPolicy:
"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + '  --->>  ' + _distro_dir) user_input = read_input_uninstall() <IF_STMT> for index, _distro_dir in enumerate(distro_list): if index == user_input: config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log('No distro installed on ' + config.usb_disk)",if user_input is not False:
"def set_random_avatar(user): galleries = get_available_galleries(include_default=True) if not galleries: raise RuntimeError('no avatar galleries are set') avatars_list = [] for gallery in galleries: <IF_STMT> avatars_list = gallery['images'] break else: avatars_list += gallery['images'] random_avatar = random.choice(avatars_list) store.store_new_avatar(user, Image.open(random_avatar.image))",if gallery['name'] == DEFAULT_GALLERY:
"def make_query(self, key, filters): meta = self.get_meta(key) q = {meta.facet_key: self.normalize_key(meta.path)} if filters: if filters.get('has_fulltext') == 'true': q['has_fulltext'] = 'true' <IF_STMT> q['publish_year'] = filters['publish_year'] return q",if filters.get('publish_year'):
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == 'likelihood.noise_covar.raw_noise': self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) <IF_STMT> self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == 'covar_module.base_kernel.raw_lengthscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive)",elif name == 'mean_module.constant':
"def _test_pooling(input_shape, **kwargs): _test_pooling_iteration(input_shape, **kwargs) if is_gpu_available(): <IF_STMT> input_shape = [input_shape[ii] for ii in (0, 3, 1, 2)] kwargs['data_format'] = 'NCHW' _test_pooling_iteration(input_shape, **kwargs)",if len(input_shape) == 4:
def init(self): r = self.get_redis() if r: key = 'pocsuite_target' info_msg = '[PLUGIN] try fetch targets from redis...' logger.info(info_msg) targets = r.get(key) count = 0 <IF_STMT> for target in targets: if self.add_target(target): count += 1 info_msg = '[PLUGIN] get {0} target(s) from redis'.format(count) logger.info(info_msg),if targets:
"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs['setting'] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, '') value = kwargs['value'] if setting in DEFAULTS.keys(): if value is not None: setattr(json_api_settings, setting, value) <IF_STMT> delattr(json_api_settings, setting)","elif hasattr(json_api_settings, setting):"
"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) <IF_STMT> try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass","elif hasattr(self.metadata, attrname):"
"def test_02_looking_at_listdir_path_(name): for dline in listdir.json(): <IF_STMT> assert dline['type'] in ('DIRECTORY', 'FILE'), listdir.text assert dline['uid'] == 0, listdir.text assert dline['gid'] == 0, listdir.text assert dline['name'] == name, listdir.text break else: raise AssertionError(f'/{path}/{name} not found')",if dline['path'] == f'{path}/{name}':
"def DeletePlugin(): oid = request.form.get('oid', '') if oid: result = Mongo.coll['Plugin'].find_one_and_delete({'_id': ObjectId(oid)}, remove=True) <IF_STMT> result['filename'] = result['filename'] + '.py' if os.path.exists(file_path + result['filename']): os.remove(file_path + result['filename']) return 'success' return 'fail'",if not result['filename'].find('.') > -1:
"def iterparent(self, node): """"""Iterator wrapper to get allowed parent and child all at once."""""" for child in node: <IF_STMT> yield (node, child) yield from self.iterparent(child)","if not self.header_rgx.match(child.tag) and child.tag not in ['pre', 'code']:"
def _get_matched_layout(command): cmd = command.script.split(' ') for source_layout in source_layouts: is_all_match = True for cmd_part in cmd: if not all([ch in source_layout or ch in '-_' for ch in cmd_part]): is_all_match = False break <IF_STMT> return source_layout,if is_all_match:
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: if n.op in failed_ops: continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): <IF_STMT> continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple((sum(nsplit) for nsplit in new_nsplits))) tiled_n._nsplits = new_nsplits",if any((c.key not in chunk_result for c in tiled_n.chunks)):
"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: if len(all_items) < target: raise ItemNotFoundError('insufficient items with name %r' % name) else: raise AttributeError('insufficient non-disabled items with name %s' % name) on = [] off = [] for o in items: <IF_STMT> on.append(o) else: off.append(o) return (on, off)",if o.selected:
def parse_flow_sequence_entry_mapping_value(self): if self.check_token(ValueToken): token = self.get_token() <IF_STMT> self.states.append(self.parse_flow_sequence_entry_mapping_end) return self.parse_flow_node() else: self.state = self.parse_flow_sequence_entry_mapping_end return self.process_empty_scalar(token.end_mark) else: self.state = self.parse_flow_sequence_entry_mapping_end token = self.peek_token() return self.process_empty_scalar(token.start_mark),"if not self.check_token(FlowEntryToken, FlowSequenceEndToken):"
"def serialize_config(self, session, key, tid, language): cache_key = gen_cache_key(key, tid, language) cache_obj = None if cache_key not in self.cache: <IF_STMT> cache_obj = db_admin_serialize_node(session, tid, language) elif key == 'notification': cache_obj = db_get_notification(session, tid, language) self.cache[cache_key] = cache_obj return self.cache[cache_key]",if key == 'node':
"def get_lldp_neighbors(self): commands = ['show lldp neighbors'] output = self.device.run_commands(commands)[0]['lldpNeighbors'] lldp = {} for n in output: <IF_STMT> lldp[n['port']] = [] lldp[n['port']].append({'hostname': n['neighborDevice'], 'port': n['neighborPort']}) return lldp",if n['port'] not in lldp.keys():
def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name if self.option('full-path'): name = str(venv.path) <IF_STMT> self.line('<info>{} (Activated)</info>'.format(name)) continue self.line(name),if venv == current_env:
"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): if list(config.keys()) == ['$env']: return environ.get(list(config.values())[0]) <IF_STMT> return open(list(config.values())[0]).read() else: return {key: resolve_env_secrets(value, environ) for key, value in config.items()} elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",elif list(config.keys()) == ['$file']:
"def _is_valid_16bit_as_path(cls, buf): two_byte_as_size = struct.calcsize('!H') while buf: type_, num_as = struct.unpack_from(cls._SEG_HDR_PACK_STR, six.binary_type(buf)) if type_ is not cls._AS_SET and type_ is not cls._AS_SEQUENCE: return False buf = buf[struct.calcsize(cls._SEG_HDR_PACK_STR):] <IF_STMT> return False buf = buf[num_as * two_byte_as_size:] return True",if len(buf) < num_as * two_byte_as_size:
"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: <IF_STMT> newParent._element.text = '' if self._element.text is not None: newParent._element.text += self._element.text self._element.text = '' base.Node.reparentChildren(self, newParent)",if not newParent._element.text:
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: if operation: return None operation = definition <IF_STMT> return definition return operation",elif definition.name and definition.name.value == operation_name:
"def reprSmart(vw, item): ptype = type(item) if ptype is int: <IF_STMT> return str(item) elif vw.isValidPointer(item): return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) elif ptype is dict: return '{%s}' % ','.join(['%s:%s' % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]) else: return repr(item)",if -1024 < item < 1024:
"def cleanDataCmd(cmd): newcmd = 'AbracadabrA ** <?php ' if cmd[:6] != 'php://': <IF_STMT> cmds = cmd.split('&') for c in cmds: if len(c) > 0: newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += '?> **' return newcmd",if reverseConn not in cmd:
"def render_tasks(self) -> List: results = [] for task in self.tasks.values(): job_entry = self.jobs.get(task.job_id) <IF_STMT> if not self.should_render_job(job_entry): continue files = self.get_file_counts([task]) entry = (task.job_id, task.task_id, task.state, task.type.name, task.target, files, task.pool, task.end_time) results.append(entry) return results",if job_entry:
"def __call__(self, environ, start_response): for key in ('REQUEST_URL', 'REQUEST_URI', 'UNENCODED_URL'): if key not in environ: continue request_uri = unquote(environ[key]) script_name = unquote(environ.get('SCRIPT_NAME', '')) <IF_STMT> environ['PATH_INFO'] = request_uri[len(script_name):].split('?', 1)[0] break return self.app(environ, start_response)",if request_uri.startswith(script_name):
"def _add_role_information(self, function_dict, role_id): function_dict['role_arn'] = role_id role_name = role_id.split('/')[-1] function_dict['execution_role'] = await self.facade.awslambda.get_role_with_managed_policies(role_name) if function_dict.get('execution_role'): statements = [] for policy in function_dict['execution_role'].get('policies'): <IF_STMT> statements += policy['Document']['Statement'] function_dict['execution_role']['policy_statements'] = statements",if 'Document' in policy and 'Statement' in policy['Document']:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_ts(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def format_counts(results, json_output=False, human_readable=False): if json_output: for result in results: yield json.dumps(result) else: for result in results: space_consumed = result.get('spaceConsumed') <IF_STMT> space_consumed = _sizeof_fmt(int(result.get('spaceConsumed'))) yield ('%12s %12s %18s %s' % (result.get('directoryCount'), result.get('fileCount'), space_consumed, result.get('path')))",if human_readable:
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) <IF_STMT> bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if bbox is None:
"def __getitem__(self, k) -> 'SimMemView': if isinstance(k, slice): if k.step is not None: raise ValueError('Slices with strides are not supported') <IF_STMT> raise ValueError('Must specify start index') elif k.stop is not None: raise ValueError('Slices with stop index are not supported') else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",elif k.start is None:
"def _parse(self, stream, context): obj = [] try: if self.subcon.conflags & self.FLAG_COPY_CONTEXT: while True: subobj = self.subcon._parse(stream, context.__copy__()) obj.append(subobj) <IF_STMT> break else: while True: subobj = self.subcon._parse(stream, context) obj.append(subobj) if self.predicate(subobj, context): break except ConstructError as ex: raise ArrayError('missing terminator', ex) return obj","if self.predicate(subobj, context):"
"def before_run(self, run_context): if 'featurizer' in self.model_portion and (self.need_to_refresh or self.refresh_base_model): <IF_STMT> self.refresh_base_model = True self.init_fn(None, run_context.session, self.model_portion, self.refresh_base_model) self.need_to_refresh = False self.refresh_base_model = False",if self.model_portion == 'whole_featurizer':
"def run(self): while True: task = self.requestQueue.get() if task is None: break try: <IF_STMT> raise SCons.Errors.BuildError(task.targets[0], errstr=interrupt_msg) task.execute() except: task.exception_set() ok = False else: ok = True self.resultsQueue.put((task, ok))",if self.interrupted():
"def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]: """"""Returns all documents that have need had a recent evergreen notification."""""" documents = db_session.query(Document).filter(Document.evergreen == True).all() overdue_documents = [] now = datetime.utcnow() for d in documents: next_reminder = d.evergreen_last_reminder_at + timedelta(days=d.evergreen_reminder_interval) <IF_STMT> overdue_documents.append(d) return overdue_documents",if now > next_reminder:
"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): if not exists(folder): os.mkdir(folder) init_path = join(folder, '__init__.py') <IF_STMT> create_file(init_path)",if not exists(init_path):
"def generate(): for leaf in u.leaves: <IF_STMT> val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector","if isinstance(leaf, Integer):"
"def replace(self, old, new): v_m = self.var_map size = v_m[self.size] if not (size.is_const() or size.is_ident()): size.replace(old, new) el<IF_STMT> v_m[new.value()] = new self.size = new.value() else: v_m[old] = new",if new.is_ident():
def method_for_doctype(doctype): method = 'xhtml' if doctype: if doctype.startswith('html'): method = 'html' <IF_STMT> method = 'xhtml' elif doctype.startswith('svg'): method = 'xml' else: method = 'xhtml' return method,elif doctype.startswith('xhtml'):
"def delete(self, trans, **kwd): idnum = kwd[self.tagged_item_id] item = self._get_item_from_id(trans, idnum, check_writable=True) if item is not None: ex_obj = self.get_item_extended_metadata_obj(trans, item) <IF_STMT> self.unset_item_extended_metadata_obj(trans, item) self.delete_extended_metadata(trans, ex_obj)",if ex_obj is not None:
"def check_testv(self, testv): test_good = True f = open(self.home, 'rb+') for offset, length, operator, specimen in testv: data = self._read_share_data(f, offset, length) <IF_STMT> test_good = False break f.close() return test_good","if not testv_compare(data, operator, specimen):"
"def get_history_user(self, instance): """"""Get the modifying user from instance or middleware."""""" try: return instance._history_user except AttributeError: request = None try: <IF_STMT> request = self.thread.request except AttributeError: pass return self.get_user(instance=instance, request=request)",if self.thread.request.user.is_authenticated:
"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) <IF_STMT> strlen *= size if strlen < MAX_LEN: data = 'A' * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",if size:
"def __setattr__(self, name, value): if name == 'path': if value and value != '': if value[0] != '/': raise ValueError('The page path should always start with a slash (""/"").') elif name == 'load_time': <IF_STMT> raise ValueError('Page load time must be specified in integer milliseconds.') object.__setattr__(self, name, value)","if value and (not isinstance(value, int)):"
"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True if self.is_computed(): status = 'computed, ' <IF_STMT> if self.value() is self: status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.error() is None:
"def _exclude_node(self, name): if 'exclude_nodes' in self.node_filters: <IF_STMT> self.loggit.info('Excluding node ""{0}"" due to node_filters'.format(name)) return True return False",if name in self.node_filters['exclude_nodes']:
"def enumerate_projects(): """"""List projects in _DEFAULT_APP_DIR."""""" src_path = os.path.join(_DEFAULT_APP_DIR, 'src') projects = {} for project in os.listdir(src_path): projects[project] = [] project_path = os.path.join(src_path, project) for file in os.listdir(project_path): <IF_STMT> projects[project].append(file[:-8]) return projects",if file.endswith('.gwt.xml'):
"def zip_readline_read_test(self, f, compression): self.make_test_archive(f, compression) with zipfile.ZipFile(f, 'r') as zipfp, zipfp.open(TESTFN) as zipopen: data = b'' while True: read = zipopen.readline() <IF_STMT> break data += read read = zipopen.read(100) if not read: break data += read self.assertEqual(data, self.data)",if not read:
"def f(view, s): if mode == modes.NORMAL: return sublime.Region(0) elif mode == modes.VISUAL: <IF_STMT> return sublime.Region(s.a + 1, 0) else: return sublime.Region(s.a, 0) elif mode == modes.INTERNAL_NORMAL: return sublime.Region(view.full_line(s.b).b, 0) elif mode == modes.VISUAL_LINE: if s.a < s.b: return sublime.Region(0, s.b) else: return sublime.Region(0, s.a) return s",if s.a < s.b:
def response(self): try: response = requests.get(str(self)) rjson = response.json() <IF_STMT> raise Exception(response.text) return rjson except Exception as e: raise ResponseFanartError(str(e)),"if not isinstance(rjson, dict):"
"def __get_type(self, cexpr): """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument"""""" child = cexpr for p in reversed(self.parents): assert p, 'Failed to get type at ' + helper.to_hex(self.__function_address) if p.cexpr.op == idaapi.cot_call: return 'Arg' if not p.is_expr(): return 'R' if p.cexpr.op == idaapi.cot_asg: <IF_STMT> return 'W' return 'R' child = p.cexpr",if p.cexpr.x == child:
"def _extract_lemma(self, parse: Parse) -> str: special_feats = [x for x in self.SPECIAL_FEATURES if x in parse.tag] if len(special_feats) == 0: return parse.normal_form for other in parse.lexeme: tag = other.tag <IF_STMT> continue if tag.case == 'nomn' and tag.gender == parse.tag.gender and (tag.number == 'sing'): return other.word return parse.normal_form",if any((x not in tag for x in special_feats)):
"def evaluateWord(self, argument): wildcard_count = argument[0].count('*') if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith('*'): return self.GetWordWildcard(argument[0][1:], method='endswith') <IF_STMT> return self.GetWordWildcard(argument[0][:-1], method='startswith') else: _regex = argument[0].replace('*', '.+') matched = False for w in self.words: matched = bool(re.search(_regex, w)) if matched: break return matched return self.GetWord(argument[0])",if wildcard_count == 1 and argument[0].endswith('*'):
def getAllEntries(self): entries = [] for bucket in self.buckets: last = None for entry in bucket.entries: if last is not None: last.size = entry.virtualOffset - last.virtualOffset last = entry entries.append(entry) <IF_STMT> entries[-1].size = bucket.endOffset - entries[-1].virtualOffset return entries,if len(entries) != 0:
def clean(self): if self._ctx: <IF_STMT> libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx) else: libcrypto.EVP_CIPHER_CTX_reset(self._ctx) libcrypto.EVP_CIPHER_CTX_free(self._ctx),"if hasattr(libcrypto, 'EVP_CIPHER_CTX_cleanup'):"
"def _addTab(self, name, label, idx=None): label = getLanguageString(label) tab = Tab(self, name, label) tab.idx = self._makeTab(tab, idx) if idx != None: newIdxList = {} for tIdx, t in list(self._tabs_by_idx.items()): <IF_STMT> t.idx += 1 newIdxList[t.idx] = t self._tabs_by_idx = newIdxList self._tabs_by_idx[tab.idx] = tab self._tabs_by_name[tab.name] = tab return tab",if int(tIdx) >= idx:
"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) <IF_STMT> self.users[current_user.id] = dict(session_count=1, key=_key) else: if _new_login: user['session_count'] += 1 user['key'] = _key",if user is None:
"def stop(self): try: self.rpcserver.stop() <IF_STMT> self.backend_rpcserver.stop() if self.cluster_rpcserver: self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",if self.backend_rpcserver:
"def __genmenuOnlyAllocated(menu): for submenu in menu.Submenus: __genmenuOnlyAllocated(submenu) if menu.OnlyUnallocated == True: tmp['cache'].addMenuEntries(menu.AppDirs) menuentries = [] for rule in menu.Rules: menuentries = rule.do(tmp['cache'].getMenuEntries(menu.AppDirs), rule.Type, 2) for menuentry in menuentries: <IF_STMT> menuentry.Parents.append(menu) menu.MenuEntries.append(menuentry)",if menuentry.Add == True:
"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, 'func_name_highlighting', True) self.disabled_modules = get_list_opt(options, 'disabled_modules', []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._lua_builtins import MODULES for mod, func in iteritems(MODULES): <IF_STMT> self._functions.update(func) RegexLexer.__init__(self, **options)",if mod not in self.disabled_modules:
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() <IF_STMT> break elif r: y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return ''.join(y)",if r is None:
"def get_menu_items(node): aList = [] for child in node.children: for tag in ('@menu', '@item'): if child.h.startswith(tag): name = child.h[len(tag) + 1:].strip() <IF_STMT> aList.append(('%s %s' % (tag, name), get_menu_items(child), None)) else: b = g.splitLines(''.join(child.b)) aList.append((tag, name, b[0] if b else '')) break return aList",if tag == '@menu':
"def import_suffix_generator(a_block, datatype=False): if datatype is False: for name, suffix in iteritems(a_block.component_map(Suffix)): <IF_STMT> yield (name, suffix) else: for name, suffix in iteritems(a_block.component_map(Suffix)): if suffix.import_enabled() is True and suffix.get_datatype() is datatype: yield (name, suffix)",if suffix.import_enabled() is True:
"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError('Empty chown path') checkpath = root parts = path.split(os.sep) for part in parts: if part in ('.', '..'): raise PackagerError('. and .. is not allowed in chown path') checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1:] if not os.path.exists(checkpath): raise PackagerError(f'chown path {relpath} does not exist') <IF_STMT> raise PackagerError(f'chown path {relpath} is a soft link')",if os.path.islink(checkpath):
"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get('partitions', [])) scanners = {} for part_name, part_scanner in list(syntax.get('scanner', {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get('formats', {}).items()): <IF_STMT> if fstyle.startswith('%(') and fstyle.endswith(')s'): key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return (partition_scanner, scanners, formats)","if isinstance(fstyle, basestring):"
"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or '' for line in commit_msg.splitlines(): parts = line.strip('# ').split(':', 1) key, val = parts if len(parts) > 1 else (parts[0], '') <IF_STMT> ci_names = val.replace(',', ' ').lower().split() if val else [] if len(ci_names) == 0 or ci.lower() in ci_names: result = True return result",if key == 'CI_KEEP_ALIVE':
"def get_note_title_file(note): mo = note_title_re.match(note.get('content', '')) if mo: fn = mo.groups()[0] fn = fn.replace(' ', '_') fn = fn.replace('/', '_') if not fn: return '' <IF_STMT> fn = unicode(fn, 'utf-8') else: fn = unicode(fn) if note_markdown(note): fn += '.mkdn' else: fn += '.txt' return fn else: return ''","if isinstance(fn, str):"
"def post(self, orgname, teamname): if _syncing_setup_allowed(orgname): try: team = model.team.get_organization_team(orgname, teamname) except model.InvalidTeamException: raise NotFound() config = request.get_json() status, err = authentication.check_group_lookup_args(config) <IF_STMT> raise InvalidRequest('Could not sync to group: %s' % err) model.team.set_team_syncing(team, authentication.federated_service, config) return team_view(orgname, team) raise Unauthorized()",if not status:
"def _marshalData(self): if self._cache == None: d = self._data s = '' s = time.strftime('%H:%M:%S', (0, 0, 0) + d + (0, 0, -1)) f = d[2] - int(d[2]) <IF_STMT> s += ('%g' % f)[1:] s += 'Z' self._cache = s return self._cache",if f != 0:
"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): if level_ref < 0: level_ref += len(levels) <IF_STMT> raise PatsyError('specified level %r is out of range' % (level_ref,)) return level_ref raise PatsyError('specified level %r not found' % (level_ref,))",if not 0 <= level_ref < len(levels):
"def iterfieldselect(source, field, where, complement, missing): it = iter(source) hdr = next(it) yield tuple(hdr) indices = asindices(hdr, field) getv = operator.itemgetter(*indices) for row in it: try: v = getv(row) except IndexError: v = missing <IF_STMT> yield tuple(row)",if bool(where(v)) != complement:
"def _test_wait_read_invalid_switch(self, sleep): sock1, sock2 = socket.socketpair() try: p = gevent.spawn(util.wrap_errors(AssertionError, socket.wait_read), sock1.fileno()) gevent.get_hub().loop.run_callback(switch_None, p) <IF_STMT> gevent.sleep(sleep) result = p.get() assert isinstance(result, AssertionError), result assert 'Invalid switch' in str(result), repr(str(result)) finally: sock1.close() sock2.close()",if sleep is not None:
"def train(config, args): gan = setup_gan(config, inputs, args) test_batches = [] for i in range(args.steps): gan.step() <IF_STMT> correct_prediction = 0 total = 0 for x, y in gan.inputs.testdata(): prediction = gan.generator(x) correct_prediction += (torch.argmax(prediction, 1) == torch.argmax(y, 1)).sum() total += y.shape[0] accuracy = float(correct_prediction) / total * 100 print('accuracy: ', accuracy) return sum_metrics",if i % args.sample_every == 0 and i > 0:
"def process_response(self, request, response, spider): if not response.body: return response for fmt, func in six.iteritems(self._formats): new_response = func(response) <IF_STMT> logger.debug('Decompressed response with format: %(responsefmt)s', {'responsefmt': fmt}, extra={'spider': spider}) return new_response return response",if new_response:
"def detect_ssl_option(self): for option in self.ssl_options(): <IF_STMT> for other_option in self.ssl_options(): if option != other_option: if scan_argv(self.argv, other_option) is not None: raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option","if scan_argv(self.argv, option) is not None:"
"def load(cls, storefile, template_store): if not hasattr(storefile, 'read'): storefile = open(storefile, 'rb') store = cls.convertfile(storefile, template_store) for unit in store.units: <IF_STMT> continue if cls.needs_target_sync: unit.target = unit.source unit.rich_target = unit.rich_source return store",if unit.isheader():
"def _pre_get_table(self, _ctx, table_name): vsctl_table = self._get_table(table_name) schema_helper = self.schema_helper schema_helper.register_table(vsctl_table.table_name) for row_id in vsctl_table.row_ids: <IF_STMT> schema_helper.register_table(row_id.table) if row_id.name_column: schema_helper.register_columns(row_id.table, [row_id.name_column]) if row_id.uuid_column: schema_helper.register_columns(row_id.table, [row_id.uuid_column]) return vsctl_table",if row_id.table:
"def __init__(self, pin=None, pull_up=False): super(InputDevice, self).__init__(pin) try: self.pin.function = 'input' pull = 'up' if pull_up else 'down' <IF_STMT> self.pin.pull = pull except: self.close() raise self._active_state = False if pull_up else True self._inactive_state = True if pull_up else False",if self.pin.pull != pull:
"def _increment_operations_count(self, operation, executed): with self._lock: <IF_STMT> self._executed_operations += 1 self._executed[operation.job_type] += 1 else: self._skipped[operation.job_type] += 1",if executed:
"def emit(self, type, info=None): ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: <IF_STMT> self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev]) elif type in self.__event_types_at_proxy: self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev])",if type in self.__proxy_properties__:
def validate_pull_secret(namespace): if namespace.pull_secret is None: warning = 'No --pull-secret provided: cluster will not include samples or operators from ' + 'Red Hat or from certified partners.' logger.warning(warning) else: try: <IF_STMT> raise Exception() except: raise InvalidArgumentValueError('Invalid --pull-secret.'),"if not isinstance(json.loads(namespace.pull_secret), dict):"
"def pack(types, *args): if len(types) != len(args): raise Exception('number of arguments does not match format string') port = StringIO() for type, value in zip(types, args): if type == 'V': write_vuint(port, value) elif type == 'v': write_vint(port, value) <IF_STMT> write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",elif type == 's':
"def data(self): if self._data is not None: return self._data el<IF_STMT> with open(self.path, 'rb') as jsonfile: data = jsonfile.read().decode('utf8') data = json.loads(data) self._data = data return self._data else: return dict()",if os.path.exists(self.path):
"def interact(self): self.output.write('\n') while True: try: request = self.getline('help> ') <IF_STMT> break except (KeyboardInterrupt, EOFError): break request = strip(request) if len(request) > 2 and request[0] == request[-1] in (""'"", '""') and (request[0] not in request[1:-1]): request = request[1:-1] if lower(request) in ('q', 'quit'): break self.help(request)",if not request:
"def api_attachment_metadata(self): resp = [] for part in self.parts: <IF_STMT> continue k = {'content_type': part.block.content_type, 'size': part.block.size, 'filename': part.block.filename, 'id': part.block.public_id} content_id = part.content_id if content_id: if content_id[0] == '<' and content_id[-1] == '>': content_id = content_id[1:-1] k['content_id'] = content_id resp.append(k) return resp",if not part.is_attachment:
"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term):] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u('%s is contained here:') % py.io.saferepr(term, maxsize=42)] for line in diff: if line.startswith(u('Skipping')): continue <IF_STMT> continue if line.startswith(u('+ ')): newdiff.append(u('  ') + line[2:]) else: newdiff.append(line) return newdiff",if line.startswith(u('- ')):
"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: <IF_STMT> API_CACHE = {} if API_CACHE.get(url) is None: API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",if API_CACHE is None:
"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_index_name_: res += prefix + 'index_name: %s\n' % self.DebugFormatString(self.index_name_) cnt = 0 for e in self.prefix_value_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'prefix_value%s: %s\n' % (elm, self.DebugFormatString(e)) cnt += 1 if self.has_value_prefix_: res += prefix + 'value_prefix: %s\n' % self.DebugFormatBool(self.value_prefix_) return res",if printElemNumber:
"def add_group(x, nl, in_group, mw): if len(x) == 0: return x if len(x) > 1 and (not in_group): <IF_STMT> return ['[['] + x + [']]'] mw.warn('Equation will multiplex and may produce inaccurate results (see manual)') return ['['] + x + [']']","if supports_group(x, nl):"
def unfulfilled_items(self): unfulfilled_items = 0 for order_item in self.items.all(): <IF_STMT> aggr = order_item.deliver_item.aggregate(delivered=Sum('quantity')) unfulfilled_items += order_item.quantity - (aggr['delivered'] or 0) return unfulfilled_items,if not order_item.canceled:
"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): <IF_STMT> data = self.tagged_blocks.get_data(key) for pattern in data: if pattern.pattern_id == pattern_id: return pattern return None",if key in self.tagged_blocks:
"def query_lister(domain, query='', max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes(domain, query, attr_names, next_token=next_token) for item in rs: if max_items: <IF_STMT> raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",if num_results == max_items:
"def find_deprecated_settings(source): from celery.utils import deprecated for name, opt in flatten(NAMESPACES): <IF_STMT> deprecated.warn(description='The {0!r} setting'.format(name), deprecation=opt.deprecate_by, removal=opt.remove_by, alternative='Use the {0.alt} instead'.format(opt)) return source","if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):"
"def tearDown(self): """"""Shutdown the server."""""" try: <IF_STMT> self.server.stop(2.0) if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:
"def broadcast_events(self, events): LOGGER.debug('Broadcasting events: %s', events) with self._subscribers_cv: subscribers = {conn: sub.copy() for conn, sub in self._subscribers.items()} if subscribers: for connection_id, subscriber in subscribers.items(): <IF_STMT> subscriber_events = [event for event in events if subscriber.is_subscribed(event)] event_list = EventList(events=subscriber_events) self._send(connection_id, event_list.SerializeToString())",if subscriber.is_listening():
"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen([self._find_binary(), 'info', os.path.realpath(path)], stdout=PIPE, stderr=PIPE).communicate() except OSError: pass else: if stdout: for line in stdout.splitlines(): line = u(line).split(': ', 1) <IF_STMT> info[line[0]] = line[1] return info",if len(line) == 2:
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False <IF_STMT> return True if len(a) != b.len: return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",if a is None:
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: <IF_STMT> raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs",if type(ah) is not ActionHandle:
"def startElement(self, name, attrs, connection): if name == 'Parameter': <IF_STMT> self[self._current_param.name] = self._current_param self._current_param = Parameter(self) return self._current_param",if self._current_param:
"def _find_class_in_descendants(self, search_key): for cls in self.primitive_classes: cls_key = (cls.__name__, cls.__module__) self.class_cache[cls_key] = cls <IF_STMT> return cls",if cls_key == search_key:
"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) if c2: v.append(sibling) <IF_STMT> sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",if sibling.getFirstChild():
"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): if i > 0: block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) <IF_STMT> route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",if i < 2:
"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: <IF_STMT> continue match = path if item.startswith('/') else basename if fnmatch.fnmatch(match, item.strip('/')): return True return False",if item.endswith('/') and (not is_dir):
"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): if line.startswith('""""""'): found += 1 if found == 2: <IF_STMT> break lines.insert(i + 1, '') lines.insert(i + 2, division) break return '\n'.join(lines)",if division in '\n'.join(lines):
"def buildImage(opt): dpath = os.path.join(opt['datapath'], 'COCO-IMG-2015') version = '1' if not build_data.built(dpath, version_string=version): print('[building image data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES[:1]: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def colorformat(text): if text[0:1] == '#': col = text[1:] <IF_STMT> return col elif len(col) == 3: return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == '': return '' assert False, 'wrong color format %r' % text",if len(col) == 6:
"def tree_print(tree): for key in tree: print(key, end=' ') tree_element = tree[key] for subElem in tree_element: print(' -> ', subElem, end=' ') <IF_STMT> print('\n ') print()",if type(subElem) != str:
"def is_dse_cluster(path): try: with open(os.path.join(path, 'CURRENT'), 'r') as f: name = f.readline().strip() cluster_path = os.path.join(path, name) filename = os.path.join(cluster_path, 'cluster.conf') with open(filename, 'r') as f: data = yaml.load(f) <IF_STMT> return True except IOError: return False",if 'dse_dir' in data:
"def delete_old_target_output_files(classpath_prefix): """"""Delete existing output files or symlinks for target."""""" directory, basename = os.path.split(classpath_prefix) pattern = re.compile('^{basename}(([0-9]+)(\\.jar)?|classpath\\.txt)$'.format(basename=re.escape(basename))) files = [filename for filename in os.listdir(directory) if pattern.match(filename)] for rel_path in files: path = os.path.join(directory, rel_path) <IF_STMT> safe_delete(path)",if os.path.islink(path) or os.path.isfile(path):
"def test_files(self): dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): if n.endswith('.py') and (not n.startswith('bad')): names.append(os.path.join(test_dir, n)) for filename in names: <IF_STMT> print('Testing %s' % filename) source = read_pyfile(filename) self.check_roundtrip(source)",if test_support.verbose:
"def __str__(self): if self.HasError(): return self.ErrorAsStr() else: string = self._action if self._target is not None: string += ' ""{target}""'.format(target=self._target) <IF_STMT> path = self._filename if self._lineno is not None: path += ':{lineno}'.format(lineno=self._lineno) string += ' ({path})'.format(path=path) return string",if self._filename is not None:
"def extra_action_out(self, input_dict, state_batches, model, action_dist): with self._no_grad_context(): <IF_STMT> stats_dict = extra_action_out_fn(self, input_dict, state_batches, model, action_dist) else: stats_dict = parent_cls.extra_action_out(self, input_dict, state_batches, model, action_dist) return self._convert_to_non_torch_type(stats_dict)",if extra_action_out_fn:
"def _retract_bindings(fstruct, inv_bindings, fs_class, visited): if id(fstruct) in visited: return visited.add(id(fstruct)) if _is_mapping(fstruct): items = fstruct.items() elif _is_sequence(fstruct): items = enumerate(fstruct) else: raise ValueError('Expected mapping or sequence') for fname, fval in items: if isinstance(fval, fs_class): <IF_STMT> fstruct[fname] = inv_bindings[id(fval)] _retract_bindings(fval, inv_bindings, fs_class, visited)",if id(fval) in inv_bindings:
"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): <IF_STMT> continue if not isinstance(dep.repo, WarehouseBaseRepo): continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())",if dep.repo is None:
"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) <IF_STMT> continue deval = detyper(val) if deval is None: continue ctx[key] = deval self._detyped = ctx return ctx",if detyper is None:
"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: <IF_STMT> field.delete() return if isinstance(self.data, FileStorage) and (not is_empty(self.data.stream)): if not field.grid_id: func = field.put else: func = field.replace func(self.data.stream, filename=self.data.filename, content_type=self.data.content_type)",if self._should_delete:
"def _load(container): if isinstance(container, str): <IF_STMT> with open(container, 'rb') as f: return pickle.load(f) else: return pickle.loads(container) elif isinstance(container, IOBase): return pickle.load(container) else: l.error('Cannot unpickle container of type %s', type(container)) return None",if all((c in string.printable for c in container)) and os.path.exists(container):
"def append_row(self, row): self.allocate_future_payments(row) self.set_invoice_details(row) self.set_party_details(row) self.set_ageing(row) if self.filters.get('group_by_party'): self.update_sub_total_row(row, row.party) <IF_STMT> self.append_subtotal_row(self.previous_party) self.previous_party = row.party self.data.append(row)",if self.previous_party and self.previous_party != row.party:
def gg1(): while 1: tt = 3 while tt > 0: trace.append(tt) val = (yield) <IF_STMT> tt = 10 trace.append('breaking early...') break tt -= 1 trace.append('try!'),if val is not None:
"def migrate_common_facts(facts): """"""Migrate facts from various roles into common"""""" params = {'node': 'portal_net', 'master': 'portal_net'} if 'common' not in facts: facts['common'] = {} for role in params.keys(): if role in facts: for param in params[role]: <IF_STMT> facts['common'][param] = facts[role].pop(param) return facts",if param in facts[role]:
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] <IF_STMT> if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] if self.do_manders: results += ['Manders'] if self.do_rwc: results += ['RWC'] if self.do_costes: results += ['Costes'] return results return []",if self.do_corr_and_slope:
"def access_modes(self): """"""access_modes property"""""" if self._access_modes is None: self._access_modes = self.get_access_modes() <IF_STMT> self._access_modes = list(self._access_modes) return self._access_modes","if not isinstance(self._access_modes, list):"
"def unwrap_envelope(self, data, many): if many: <IF_STMT> if isinstance(data, InstrumentedList) or isinstance(data, list): self.context['total'] = len(data) return data else: self.context['total'] = data['total'] else: self.context['total'] = 0 data = {'items': []} return data['items'] return data",if data['items']:
"def to_string(self, fmt='{:.4f}'): result_str = '' for key in self.measures: result = self.m_dict[key][0]() result_str += ','.join((fmt.format(x) for x in result)) <IF_STMT> else fmt.format(result) result_str += ',' return result_str[:-1]","if isinstance(result, tuple)"
"def on_torrent_created(self, result): if not result: return self.dialog_widget.btn_create.setEnabled(True) self.dialog_widget.edit_channel_create_torrent_progress_label.setText('Created torrent') if 'torrent' in result: self.create_torrent_notification.emit({'msg': 'Torrent successfully created'}) <IF_STMT> self.add_torrent_to_channel(result['torrent']) self.close_dialog()",if self.dialog_widget.add_to_channel_checkbox.isChecked():
"def save(self): for var_name in self.default_config: <IF_STMT> if var_name in self.file_config: del self.file_config[var_name] else: self.file_config[var_name] = getattr(self, var_name) with open(self.config_path, 'w') as f: f.write(json.dumps(self.file_config, indent=2))","if getattr(self, var_name, None) == self.default_config[var_name]:"
"def get_class_parameters(kwarg): ret = {'attrs': []} for key in ('rsc', 'fsc', 'usc'): <IF_STMT> ret['attrs'].append(['TCA_HFSC_%s' % key.upper(), {'m1': get_rate(kwarg[key].get('m1', 0)), 'd': get_time(kwarg[key].get('d', 0)), 'm2': get_rate(kwarg[key].get('m2', 0))}]) return ret",if key in kwarg:
"def forward(self, x): f_x = x if self.exp: f_x = self.exp_swish(self.exp_bn(self.exp(f_x))) f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x))) f_x = self.se(f_x) f_x = self.lin_proj_bn(self.lin_proj(f_x)) if self.has_skip: <IF_STMT> f_x = drop_connect(f_x, effnet_cfg.EN.DC_RATIO) f_x = x + f_x return f_x",if self.training and effnet_cfg.EN.DC_RATIO > 0.0:
"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + '  --->>  ' + _distro_dir) user_input = read_input_uninstall() if user_input is not False: for index, _distro_dir in enumerate(distro_list): <IF_STMT> config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log('No distro installed on ' + config.usb_disk)",if index == user_input:
"def IMPORTFROM(self, node): if node.module == '__future__': <IF_STMT> self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == '*': self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if not self.futuresAllowed:
"def _split_and_load(batch, ctx_list): """"""Split data to 1 batch each device."""""" new_batch = [] for _, data in enumerate(batch): <IF_STMT> new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)] else: new_data = [data.as_in_context(ctx_list[0])] new_batch.append(new_data) return new_batch","if isinstance(data, (list, tuple)):"
"def wait_success(self, timeout=60 * 10): for i in range(timeout // 10): time.sleep(10) status = self.query_job() print('job {} status is {}'.format(self.job_id, status)) <IF_STMT> return True if status and status in [StatusSet.CANCELED, StatusSet.TIMEOUT, StatusSet.FAILED]: return False return False",if status and status == StatusSet.SUCCESS:
"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): <IF_STMT> rel_root = os.path.relpath(src_root, src_dir) else: rel_root = '' if skip_variables and rel_root.startswith('variables'): continue dst_root = os.path.join(dst_dir, rel_root) if not os.path.exists(dst_root): os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if src_root != src_dir:
"def _make_padded_shapes(self, dataset, decoders): padded_shapes = dataset.output_shapes for i, hparams_i in enumerate(self._hparams.datasets): <IF_STMT> continue if not hparams_i['pad_to_max_seq_length']: continue text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(dataset, hparams_i, decoders[i], self.text_name(i), self.text_id_name(i)) padded_shapes.update(text_and_id_shapes) return padded_shapes",if not _is_text_data(hparams_i['data_type']):
"def format_errors(messages): errors = {} for k, v in messages.items(): key = camelize(k, uppercase_first_letter=False) <IF_STMT> errors[key] = format_errors(v) elif isinstance(v, list): errors[key] = v[0] return errors","if isinstance(v, dict):"
"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): <IF_STMT> for item in value: if isinstance(item, AST): self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)","if isinstance(value, list):"
"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get('allow_css_overrides'): filename = self.view.file_name() filetypes = self.settings.get('markdown_filetypes') if filename and filetypes: for filetype in filetypes: <IF_STMT> css_filename = filename.rpartition(filetype)[0] + '.css' if os.path.isfile(css_filename): return u'<style>%s</style>' % load_utf8(css_filename) return ''",if filename.endswith(filetype):
"def clean(self): super().clean() if self.cluster.site is not None: for device in self.cleaned_data.get('devices', []): <IF_STMT> raise ValidationError({'devices': '{} belongs to a different site ({}) than the cluster ({})'.format(device, device.site, self.cluster.site)})",if device.site != self.cluster.site:
"def _setProcessPriority(process, nice_val, disable_gc): org_nice_val = Computer._process_original_nice_value try: process.nice(nice_val) Computer.in_high_priority_mode = nice_val != org_nice_val <IF_STMT> gc.disable() else: gc.enable() return True except psutil.AccessDenied: print2err('WARNING: Could not set process {} priority to {}'.format(process.pid, nice_val)) return False",if disable_gc:
"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <IF_STMT> warnings.warn('{}: setting results name {!r} on {} expression may only return a single token for an And alternative, in future will return the full list of tokens'.format('warn_multiple_tokens_in_named_alternation', name, type(self).__name__), stacklevel=3) return super()._setResultsName(name, listAllMatches)","if any((isinstance(e, And) for e in self.exprs)):"
"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) if project.readme.markup != 'rst': content.append(project.readme.to_rst().path.name) path = project.package.path for fname in ('setup.cfg', 'setup.py'): <IF_STMT> content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append('/'.join(fpath.parts)) return '\n'.join(content)",if (path / fname).exists():
"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): if j < segmentCount: <IF_STMT> controlPointIndices[index] = 1 index += s[1] return controlPointIndices",if glyph.contours[i].segments[j].type == 'line':
"def MergeFrom(self, other): if self.message_class is not None: if other.Parse(self.message_class): self.message.MergeFrom(other.message) elif other.message_class is not None: <IF_STMT> self.message = other.message_class() self.message_class = other.message_class self.message.MergeFrom(other.message) else: self.message += other.message",if not self.Parse(other.message_class):
def remove_old_snapshot(install_dir): logging.info('Removing any old files in {}'.format(install_dir)) for file in glob.glob('{}/*'.format(install_dir)): try: <IF_STMT> os.unlink(file) elif os.path.isdir(file): shutil.rmtree(file) except Exception as error: logging.error('Error: {}'.format(error)) sys.exit(1),if os.path.isfile(file):
"def writexml(self, stream, indent='', addindent='', newl='', strip=0, nsprefixes={}, namespace=''): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue if not isinstance(v, str): v = str(v) <IF_STMT> v = ' '.join(v.split()) val = escape(v) w(val)",if strip:
"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) <IF_STMT> if attribute.required: raise ValidationError(_('%(attr)s attribute cannot be blank') % {'attr': attribute.code}) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError(_('%(attr)s attribute %(err)s') % {'attr': attribute.code, 'err': e})",if value is None:
"def PyJsHoisted_BinaryExpression_(node, parent, this, arguments, var=var): var = Scope({u'node': node, u'this': this, u'arguments': arguments, u'parent': parent}, var) var.registers([u'node', u'parent']) if PyJsStrictEq(var.get(u'node').get(u'operator'), Js(u'in')): <IF_STMT> return var.get(u'true') if var.get(u't').callprop(u'isFor', var.get(u'parent')): return var.get(u'true') return Js(False)","if var.get(u't').callprop(u'isVariableDeclarator', var.get(u'parent')):"
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) <IF_STMT> raise TypeError('on must be a name or field, not: {0}'.format(n)) elif n not in fields: raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))","if not isinstance(n, _strtypes):"
"def encode(self, msg): """"""Encodes the message to the stream encoding."""""" stream = self.stream rv = msg + '\n' if PY2 and is_unicode(rv) or not (PY2 or is_unicode(rv) or _is_text_stream(stream)): enc = self.encoding <IF_STMT> enc = getattr(stream, 'encoding', None) or 'utf-8' rv = rv.encode(enc, 'replace') return rv",if enc is None:
"def color_convert(self, to_color_space, preserve_alpha=True): if to_color_space == self.color_space and preserve_alpha: return self else: pixels = pixels_as_float(self.pixels) converted = convert_color(pixels, self.color_space, to_color_space, preserve_alpha) <IF_STMT> return None return Image(converted, to_color_space)",if converted is None:
"def seek(self, pos): if self.closed: raise IOError('Cannot seek on a closed file') for n, idx in enumerate(self._indexes[::-1]): if idx.offset <= pos: <IF_STMT> self._idxiter = iter(self._indexes[-(n + 1):]) self._nextidx() break else: raise Exception('Cannot seek to pos') self._curfile.seek(pos - self._curidx.offset)",if idx != self._curidx:
"def load_from_json(self, node_data: dict, import_version: float): if import_version <= 0.08: self.image_pointer = unpack_pointer_property_name(bpy.data.images, node_data, 'image_name') <IF_STMT> proposed_name = node_data.get('image_name') self.info(f'image data not found in current {proposed_name}')",if not self.image_pointer:
"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == 'Average': self._local_aggregators.append(_AverageAggregator()) <IF_STMT> self._local_aggregators.append(_CountAggregator()) elif operator == 'Max': self._local_aggregators.append(_MaxAggregator()) elif operator == 'Min': self._local_aggregators.append(_MinAggregator()) elif operator == 'Sum': self._local_aggregators.append(_SumAggregator())",elif operator == 'Count':
"def attrgetter(item): items = [None] * len(attribute) for i, attribute_part in enumerate(attribute): item_i = item for part in attribute_part: item_i = environment.getitem(item_i, part) <IF_STMT> item_i = postprocess(item_i) items[i] = item_i return items",if postprocess is not None:
def work(self): while True: timeout = self.timeout <IF_STMT> timeout = self.idle_timeout log.debug('Wait for {}'.format(timeout)) fetch.wait(timeout) if shutting_down.is_set(): log.info('Stop fetch worker') break self.fetch(),if idle.is_set():
"def testCoreInterfaceIntInputData(): result_testing = False for _ in range(10): hsyncnet_instance = hsyncnet([[1], [2], [3], [20], [21], [22]], 2, initial_type.EQUIPARTITION, ccore=True) analyser = hsyncnet_instance.process() <IF_STMT> result_testing = True break assert result_testing",if len(analyser.allocate_clusters(0.1)) == 2:
"def _gen(): buf = [] iterable = dataset() try: while len(buf) < buffer_size: buf.append(next(iterable)) while 1: i = random.randint(0, buffer_size - 1) n = next(iterable) yield buf[i] buf[i] = n except StopIteration: <IF_STMT> random.shuffle(buf) for i in buf: yield i",if len(buf):
"def debug_tree(tree): l = [] for elt in tree: if isinstance(elt, (int, long)): l.append(_names.get(elt, elt)) <IF_STMT> l.append(elt) else: l.append(debug_tree(elt)) return l","elif isinstance(elt, str):"
"def reverse_code(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None: PreregistrationUser = apps.get_model('zerver', 'PreregistrationUser') for user in PreregistrationUser.objects.all(): <IF_STMT> user.invited_as_admin = True else: user.invited_as_admin = False user.save(update_fields=['invited_as_admin'])",if user.invited_as == 2:
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith('>>%s' % section_name): in_section = True elif in_section: <IF_STMT> break out.append(line.rstrip('\r\n')) return out",if line.startswith('>>END'):
"def determine_block_hints(self, text): hints = '' if text: if text[0] in ' \n\x85\u2028\u2029': hints += str(self.best_indent) <IF_STMT> hints += '-' elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029': hints += '+' return hints",if text[-1] not in '\n\x85\u2028\u2029':
def database_app(request): if request.param == 'postgres_app': if not which('initdb'): pytest.skip('initdb must be on PATH for postgresql fixture') if not psycopg2: pytest.skip('psycopg2 must be installed for postgresql fixture') if request.param == 'sqlite_rabbitmq_app': <IF_STMT> pytest.skip('rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset') return request.getfixturevalue(request.param),if not os.environ.get('GALAXY_TEST_AMQP_INTERNAL_CONNECTION'):
"def do_rollout(agent, env, num_steps, render=False): total_rew = 0 ob = env.reset() for t in range(num_steps): a = agent.act(ob) ob, reward, done, _info = env.step(a) total_rew += reward <IF_STMT> env.render() if done: break return (total_rew, t + 1)",if render and t % 3 == 0:
"def _handle_subrepos(self, ctx, dirty_trees): substate = util.parse_hgsubstate(ctx['.hgsubstate'].data().splitlines()) sub = util.OrderedDict() if '.hgsub' in ctx: sub = util.parse_hgsub(ctx['.hgsub'].data().splitlines()) for path, sha in substate.iteritems(): <IF_STMT> continue d = os.path.dirname(path) dirty_trees.add(d) tree = self._dirs.setdefault(d, dulobjs.Tree()) tree.add(os.path.basename(path), dulobjs.S_IFGITLINK, sha)",if path in sub and (not sub[path].startswith('[git]')):
"def get_property_file_image_choices(self, pipeline): columns = pipeline.get_measurement_columns() image_names = [] for column in columns: object_name, feature, coltype = column[:3] choice = feature[len(C_FILE_NAME) + 1:] <IF_STMT> image_names.append(choice) return image_names",if object_name == 'Image' and feature.startswith(C_FILE_NAME):
"def check_all_decorator_order(): """"""Check that in all test files, the slow decorator is always last."""""" errors = [] for fname in os.listdir(PATH_TO_TESTS): <IF_STMT> filename = os.path.join(PATH_TO_TESTS, fname) new_errors = check_decorator_order(filename) errors += [f'- {filename}, line {i}' for i in new_errors] if len(errors) > 0: msg = '\n'.join(errors) raise ValueError(f'The parameterized decorator (and its variants) should always be first, but this is not the case in the following files:\n{msg}')",if fname.endswith('.py'):
"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: if col and col.get_title() == _('Active'): <IF_STMT> client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",if self.watchdirs[watchdir_id]['enabled']:
"def get_conv_output_size(input_size, kernel_size, stride, padding, dilation): ndim = len(input_size) output_size = [] for i in range(ndim): size = (input_size[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) // stride[i] + 1 <IF_STMT> output_size.append(1) else: output_size.append(size) return output_size",if kernel_size[i] == -1:
"def from_location(cls, location, basename, metadata=None, **kw): project_name, version, py_version, platform = [None] * 4 basename, ext = os.path.splitext(basename) if ext.lower() in ('.egg', '.egg-info'): match = EGG_NAME(basename) <IF_STMT> project_name, version, py_version, platform = match.group('name', 'ver', 'pyver', 'plat') return cls(location, metadata, project_name=project_name, version=version, py_version=py_version, platform=platform, **kw)",if match:
"def __new__(metacls, typename, bases, namespace): annotations = namespace.get('__annotations__', {}) for t in annotations.values(): <IF_STMT> for ut in t.__args__: _assert_tensorizer_type(ut) else: _assert_tensorizer_type(t) return super().__new__(metacls, typename, bases, namespace)","if getattr(t, '__origin__', '') is Union:"
"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get('content-type') if ct: ct, options = parse_options_header(ct) charset = options.get('charset') <IF_STMT> return self.json(charset) elif ct.startswith('text/'): return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",if ct in JSON_CONTENT_TYPES:
"def get_full_path(path): if '://' not in path: path = os.path.join(self.AUTO_COLL_TEMPL, path, '') <IF_STMT> path = os.path.join(abs_path, path) return path",if abs_path:
"def __getitem__(self, name_or_path): if isinstance(name_or_path, integer_types): return list.__getitem__(self, name_or_path) elif isinstance(name_or_path, tuple): try: val = self for fid in name_or_path: <IF_STMT> raise KeyError val = val[fid] return val except (KeyError, IndexError): raise KeyError(name_or_path) else: raise TypeError(self._INDEX_ERROR % name_or_path)","if not isinstance(val, FeatStruct):"
"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: if isinstance(s, (tree.Scope, tree.Flow)): return scan(s) or s <IF_STMT> return scan(s) return None","elif s.type in ('suite', 'decorated'):"
"def _get_key(self): if not self.key: self._channel.send(u'pake', self.msg1) pake_msg = self._channel.get(u'pake') self.key = self.sp.finish(pake_msg) self.verifier = self.derive_key(u'wormhole:verifier') <IF_STMT> return confkey = self.derive_key(u'wormhole:confirmation') nonce = os.urandom(CONFMSG_NONCE_LENGTH) confmsg = make_confmsg(confkey, nonce) self._channel.send(u'_confirm', confmsg)",if not self._send_confirm:
"def executeScript(self, script): if len(script) > 0: commands = [] for l in script: extracted = self.extract_command(l) <IF_STMT> commands.append(extracted) for command in commands: cmd, argv = command self.dispatch_command(cmd, argv)",if extracted:
"def create_path(n, fullname, meta): if meta: meta.create_path(fullname) else: unlink(fullname) if stat.S_ISDIR(n.mode): mkdirp(fullname) <IF_STMT> os.symlink(n.readlink(), fullname)",elif stat.S_ISLNK(n.mode):
def get_cycle(self): if self.has_cycle(): cross_node = self.path[-1] <IF_STMT> return self.path[self.path.index(cross_node):] else: return self.path return [],if self.path.count(cross_node) > 1:
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): <IF_STMT> depth += 1 elif str_in[pos] == end_tag: depth -= 1 if depth == 0: break sel = str_in[start_pos + 1:pos] return sel",if str_in[pos] == start_tag:
"def device(self): """"""Device on which the data array of this variable reside."""""" if self._device is None: <IF_STMT> self._device = backend.CpuDevice() else: self._device = backend.get_device_from_array(self._data[0]) return self._device",if self._data[0] is None:
"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException('Item does not exist!') <IF_STMT> raise ItemNotFoundException(e.get_dbus_message()) if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED): raise SecretServiceNotAvailableException(e.get_dbus_message()) raise",if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT:
"def run(self): """"""Continual loop evaluating when_statements"""""" while len(self.library) > 0: for name, expression in self.library.items(): <IF_STMT> del self.library[name] else: expression.evaluate() sleep(0.01) return",if expression.remove_me == True:
"def _source_target_path(source, source_path, source_location): target_path_attr = source.target_path or source.resdef.target_path if source.preserve_path: <IF_STMT> log.warning(""target-path '%s' specified with preserve-path - ignoring"", target_path_attr) return os.path.relpath(os.path.dirname(source_path), source_location) else: return target_path_attr or source.resdef.target_path or ''",if target_path_attr:
"def _load_user_from_header(self, header): if self._header_callback: user = self._header_callback(header) <IF_STMT> app = current_app._get_current_object() user_loaded_from_header.send(app, user=user) return user return None",if user is not None:
"def setup(cls): """"""Check dependencies and warn about firewalling"""""" pathCheck('brctl', moduleName='bridge-utils') for table in ('arp', 'ip', 'ip6'): cmd = 'sysctl net.bridge.bridge-nf-call-%stables' % table out = quietRun(cmd).strip() <IF_STMT> warn('Warning: Linux bridge may not work with', out, '\n')",if out.endswith('1'):
"def _browse_your_music(web_client, variant): if not web_client.logged_in: return [] if variant in ('tracks', 'albums'): items = flatten([page.get('items', []) for page in web_client.get_all(f'me/{variant}', params={'market': 'from_token', 'limit': 50}) if page]) <IF_STMT> return list(translator.web_to_track_refs(items)) else: return list(translator.web_to_album_refs(items)) else: return []",if variant == 'tracks':
"def reset_styling(self): for edge in self.fsm_graph.edges_iter(): style_attr = self.fsm_graph.style_attributes.get('edge', {}).get('default') edge.attr.update(style_attr) for node in self.fsm_graph.nodes_iter(): <IF_STMT> style_attr = self.fsm_graph.style_attributes.get('node', {}).get('inactive') node.attr.update(style_attr) for sub_graph in self.fsm_graph.subgraphs_iter(): style_attr = self.fsm_graph.style_attributes.get('graph', {}).get('default') sub_graph.graph_attr.update(style_attr)",if 'point' not in node.attr['shape']:
"def set_message_type_visibility(self, message_type: MessageType): try: rows = {i for i, msg in enumerate(self.proto_analyzer.messages) <IF_STMT>} if message_type.show: self.ui.tblViewProtocol.show_rows(rows) else: self.ui.tblViewProtocol.hide_rows(rows) except Exception as e: logger.exception(e)",if msg.message_type == message_type
"def POP(cpu, *regs): for reg in regs: val = cpu.stack_pop(cpu.address_bit_size // 8) <IF_STMT> cpu._set_mode_by_val(val) val = val & ~1 reg.write(val)","if reg.reg in ('PC', 'R15'):"
"def processMovie(self, atom): for field in atom: if 'track' in field: self.processTrack(field['track']) <IF_STMT> self.processMovieHeader(field['movie_hdr'])",if 'movie_hdr' in field:
"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) <IF_STMT> if auto: update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",if remote_version > local_version:
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 <IF_STMT> values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",if i >= 0 and i < len(selections):
"def find_int_identifiers(directory): results = find_rules(directory, has_int_identifier) print('Number of rules with integer identifiers: %d' % len(results)) for result in results: rule_path = result[0] product_yaml_path = result[1] product_yaml = None <IF_STMT> product_yaml = yaml.open_raw(product_yaml_path) fix_file(rule_path, product_yaml, fix_int_identifier)",if product_yaml_path is not None:
def condition(self): if self.__condition is None: if len(self.flat_conditions) == 1: self.__condition = self.flat_conditions[0] <IF_STMT> self.__condition = lambda _: True else: self.__condition = lambda x: all((cond(x) for cond in self.flat_conditions)) return self.__condition,elif len(self.flat_conditions) == 0:
"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: <IF_STMT> continue scene_name, scene_season = scene_exception.split('|') if season == scene_season: scene_exceptions.append(scene_name) return scene_exceptions",if not len(scene_exception) == 2:
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): <IF_STMT> break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",if idx >= len(selections):
"def to_tool_path(self, path_or_uri_like, **kwds): if '://' not in path_or_uri_like: path = path_or_uri_like else: uri_like = path_or_uri_like <IF_STMT> raise Exception('Invalid URI passed to get_tool_source') scheme, rest = uri_like.split(':', 2) if scheme not in self.resolver_classes: raise Exception('Unknown tool scheme [{}] for URI [{}]'.format(scheme, uri_like)) path = self.resolver_classes[scheme]().get_tool_source_path(uri_like) return path",if ':' not in path_or_uri_like:
def mainWindow(): global MW if not MW: for i in qApp.topLevelWidgets(): <IF_STMT> MW = i return MW return None else: return MW,if i.objectName() == 'MainWindow':
"def async_get_service(hass, config, discovery_info=None): """"""Get the demo notification service."""""" for account, account_dict in hass.data[DATA_ALEXAMEDIA]['accounts'].items(): for key, _ in account_dict['devices']['media_player'].items(): <IF_STMT> _LOGGER.debug('%s: Media player %s not loaded yet; delaying load', hide_email(account), hide_serial(key)) return False return AlexaNotificationService(hass)",if key not in account_dict['entities']['media_player']:
"def _migrate_bool(self, name: str, true_value: str, false_value: str) -> None: if name not in self._settings: return values = self._settings[name] if not isinstance(values, dict): return for scope, val in values.items(): <IF_STMT> new_value = true_value if val else false_value self._settings[name][scope] = new_value self.changed.emit()","if isinstance(val, bool):"
"def send(self, data, flags=0): self._checkClosed() if self._sslobj: <IF_STMT> raise ValueError('non-zero flags not allowed in calls to send() on %s' % self.__class__) return self._sslobj.write(data) else: return socket.send(self, data, flags)",if flags != 0:
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) <IF_STMT> if init_service and init_service in dep_cnt['_deps']: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if dep_cnt:
"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith('/'): records = as_dict(path + item, version, section) if records: result[item[:-1]] = records <IF_STMT> idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",elif is_dict.match(item):
"def PrintColGroup(col_names, schema): """"""Print HTML colgroup element, used for JavaScript sorting."""""" print('  <colgroup>') for i, col in enumerate(col_names): if col.endswith('_HREF'): continue <IF_STMT> css_class = 'number' else: css_class = 'case-insensitive' print('<col id=""{}"" type=""{}"" />'.format(col, css_class)) print('  </colgroup>')",if schema.IsNumeric(col):
"def check_region(self, region): for other in self.regions: <IF_STMT> continue if other.start < region.start < other.end or other.start < region.end < other.end: raise Exception('%r overlaps with %r' % (region, other))",if other is region:
"def _write_value(self, rng, value, scalar): if rng.api and value: <IF_STMT> value = value[0][0] else: rng = rng.resize(len(value), len(value[0])) rng.raw_value = value",if scalar:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): sv_faces = [] for i, face in enumerate(dcel_mesh.faces): <IF_STMT> 'Face ({}) has inner components! Sverchok cant show polygons with holes.'.format(i) if not face.outer or del_flag in face.flags: continue if only_select and (not face.select): continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",if face.inners and face.outer:
"def _get_x_for_y(self, xValue, x, y): x_value = str(xValue) for anime in self.xmlMap.findall('anime'): try: <IF_STMT> return int(anime.get(y, 0)) except ValueError as e: continue return 0","if anime.get(x, False) == x_value:"
"def dir_copy(src_dir, dest_dir, merge_if_exists=True): try: if not os.path.exists(dest_dir): shutil.copytree(src_dir, dest_dir) <IF_STMT> merge_dir(src_dir, dest_dir) except OSError as e: if e.errno == errno.ENOTDIR: shutil.copy(src_dir, dest_dir) else: logging.error('Could not copy %s to %s', src_dir, dest_dir)",elif merge_if_exists:
"def mapping(self): m = {} if getGdriveCredentialsFile() is not None: m['gdrive'] = '' unknown = 0 for f in self.scan: bits = f.split('#', 2) if len(bits) == 1: label = os.path.basename(f) else: label = bits[1] <IF_STMT> label = 'L' + str(unknown) unknown += 1 m[label] = bits[0] return m",if not label or len(label) == 0 or label == '':
"def get_tag_values(self, event): http = event.interfaces.get('sentry.interfaces.Http') if not http: return [] if not http.headers: return [] headers = http.headers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: if key != 'User-Agent': continue ua = Parse(value) <IF_STMT> continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",if not ua:
"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): if g: <IF_STMT> t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield (k, tuple(t))",if i in direct_upstreams:
def process_question(qtxt): question = '' skip = False for letter in qtxt: <IF_STMT> skip = True if letter == '>': skip = False if skip: continue if letter.isalnum() or letter == ' ': if letter == ' ': letter = '_' question += letter.lower() return question,if letter == '<':
"def _module_repr_from_spec(spec): """"""Return the repr to use for the module."""""" name = '?' if spec.name is None else spec.name if spec.origin is None: if spec.loader is None: return '<module {!r}>'.format(name) else: return '<module {!r} ({!r})>'.format(name, spec.loader) el<IF_STMT> return '<module {!r} from {!r}>'.format(name, spec.origin) else: return '<module {!r} ({})>'.format(spec.name, spec.origin)",if spec.has_location:
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) if self.any_match: if result: return not self.inverse el<IF_STMT> return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if not result:
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith('java'): <IF_STMT> interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): if hasattr(sys, 'setswitchinterval'): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)","if hasattr(sys, 'getswitchinterval'):"
"def record_expected_exportable_production(self, ticks): """"""Record the amount of production that should be transferred to other islands."""""" for (quota_holder, resource_id), amount in self._low_priority_requests.items(): <IF_STMT> self._settlement_manager_id[quota_holder] = WorldObject.get_object_by_id(int(quota_holder[1:].split(',')[0])).settlement_manager.worldid self.trade_storage[self._settlement_manager_id[quota_holder]][resource_id] += ticks * amount",if quota_holder not in self._settlement_manager_id:
"def _method_events_callback(self, values): try: previous_echoed = values['child_result_list'][-1].decode().split('\n')[-2].strip() <IF_STMT> return 'echo foo2\n' elif previous_echoed.endswith('foo2'): return 'echo foo3\n' elif previous_echoed.endswith('foo3'): return 'exit\n' else: raise Exception('Unexpected output {0!r}'.format(previous_echoed)) except IndexError: return 'echo foo1\n'",if previous_echoed.endswith('foo1'):
"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None): if cluster_identifier: cluster_snapshots = [] for snapshot in self.snapshots.values(): <IF_STMT> cluster_snapshots.append(snapshot) if cluster_snapshots: return cluster_snapshots if snapshot_identifier: if snapshot_identifier in self.snapshots: return [self.snapshots[snapshot_identifier]] raise ClusterSnapshotNotFoundError(snapshot_identifier) return self.snapshots.values()",if snapshot.cluster.cluster_identifier == cluster_identifier:
def get_snippet_edit_handler(model): if model not in SNIPPET_EDIT_HANDLERS: <IF_STMT> edit_handler = model.edit_handler else: panels = extract_panel_definitions_from_model_class(model) edit_handler = ObjectList(panels) SNIPPET_EDIT_HANDLERS[model] = edit_handler.bind_to(model=model) return SNIPPET_EDIT_HANDLERS[model],"if hasattr(model, 'edit_handler'):"
"def start(): if os.environ.get('RUN_MAIN') != 'true': try: exit_code = restart_with_reloader() <IF_STMT> os.kill(os.getpid(), -exit_code) else: sys.exit(exit_code) except KeyboardInterrupt: pass",if exit_code < 0:
"def discover(self, *objlist): ret = [] for l in self.splitlines(): if len(l) < 5: continue <IF_STMT> continue try: int(l[2]) int(l[3]) except: continue ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",if l[0] == 'Filename':
"def ipfs_publish(self, lib): with tempfile.NamedTemporaryFile() as tmp: self.ipfs_added_albums(lib, tmp.name) try: <IF_STMT> cmd = 'ipfs add --nocopy -q '.split() else: cmd = 'ipfs add -q '.split() cmd.append(tmp.name) output = util.command_output(cmd) except (OSError, subprocess.CalledProcessError) as err: msg = 'Failed to publish library. Error: {0}'.format(err) self._log.error(msg) return False self._log.info('hash of library: {0}', output)",if self.config['nocopy']:
"def spends(self): spends = defaultdict(list) utxos = self.mempool_utxos() for tx_hash, tx in self.txs.items(): for n, input in enumerate(tx.inputs): if input.is_generation(): continue prevout = (input.prev_hash, input.prev_idx) <IF_STMT> hashX, value = utxos.pop(prevout) else: hashX, value = self.db_utxos[prevout] spends[hashX].append(prevout) return spends",if prevout in utxos:
"def terminate(self): if self.returncode is None: try: os.kill(self.pid, TERM_SIGNAL) except OSError as exc: if getattr(exc, 'errno', None) != errno.ESRCH: <IF_STMT> raise",if self.wait(timeout=0.1) is None:
def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value <IF_STMT> thisDynamic = thisDynamic[1:] if thisDynamic[-1] == 'z': thisDynamic = thisDynamic[:-1] if thisDynamic in dynamicStrToScalar: return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None],if 's' in thisDynamic:
"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): if '.' in valname: realvalname, key = valname.split('.', 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: <IF_STMT> self.__dict__[name] = config[name] del self._raw_config",if name in self.values:
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary <IF_STMT> continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",if dictionary.path in paths:
"def __getitem__(self, key, _get_mode=False): if not _get_mode: <IF_STMT> return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)","if isinstance(key, (int, long)):"
"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: <IF_STMT> raise ItemNotFoundError('insufficient items with name %r' % name) else: raise AttributeError('insufficient non-disabled items with name %s' % name) on = [] off = [] for o in items: if o.selected: on.append(o) else: off.append(o) return (on, off)",if len(all_items) < target:
"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in(['fasta', 'base'], refs) <IF_STMT> return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), 'genomes', '*', gid)) if len(gdirs) == 1 and os.path.exists(gdirs[0]): return gdirs[0]",if seq_file and os.path.exists(seq_file):
"def _PrintFuncs(self, names): status = 0 for name in names: <IF_STMT> print(name) else: status = 1 return status",if name in self.funcs:
"def package_files(self): seen_package_directories = () directories = self.distribution.package_dir or {} empty_directory_exists = '' in directories packages = self.distribution.packages or [] for package in packages: if package in directories: package_directory = directories[package] elif empty_directory_exists: package_directory = os.path.join(directories[''], package) else: package_directory = package <IF_STMT> seen_package_directories += (package_directory + '.',) yield package_directory",if not package_directory.startswith(seen_package_directories):
"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) <IF_STMT> with open(conf_file) as conf_handle: value = fn(conf_handle) if value: return value return None",if conf_file:
"def on_text(self, text): if text != self.chosen_text: self.fail_test('Expected ""{}"", received ""{}""'.format(self.chosen_text, text)) else: self.checks_passed += 1 <IF_STMT> self.pass_test() else: self._select_next_text()",if self.checks_passed >= self.number_of_checks:
"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): <IF_STMT> continue x = item() if isinstance(x, ast.AST): self.assertEqual(type(x._fields), tuple)",if name == 'Index':
"def apply(self, response): updated_headers = self.update_headers(response) if updated_headers: response.headers.update(updated_headers) warning_header_value = self.warning(response) <IF_STMT> response.headers.update({'Warning': warning_header_value}) return response",if warning_header_value is not None:
"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) if self.use_parallel_executor and (not self.use_double_buffer): self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) <IF_STMT> self.assertTrue((in_data == out_data).all())",if not self.use_parallel_executor:
def finalize(self): if self._started: <IF_STMT> self._queue.put(None) self._queue.join() self._consumer.join() self._started = False self._finalized = True,if not self._finalized:
"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: <IF_STMT> if e.code == 405: return 3 if e.code == 501: return 1 raise return 2","if hasattr(e, 'code'):"
"def _check_data(self, source, expected_bytes, expected_duration): received_bytes = 0 received_seconds = 0.0 bytes_to_read = 1024 while True: data = source.get_audio_data(bytes_to_read) <IF_STMT> break received_bytes += data.length received_seconds += data.duration self.assertEqual(data.length, len(data.data)) self.assertAlmostEqual(expected_duration, received_seconds, places=1) self.assertAlmostEqual(expected_bytes, received_bytes, delta=5)",if data is None:
"def __randomize_interval_task(self): for job in self.aps_scheduler.get_jobs(): <IF_STMT> self.aps_scheduler.modify_job(job.id, next_run_time=datetime.now() + timedelta(seconds=randrange(job.trigger.interval.total_seconds() * 0.75, job.trigger.interval.total_seconds())))","if isinstance(job.trigger, IntervalTrigger):"
"def find_approximant(x): c = 0.0001 it = sympy.ntheory.continued_fraction_convergents(sympy.ntheory.continued_fraction_iterator(x)) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 <IF_STMT> return i if tol < machine_epsilon: break return x",if abs(i - x) <= tol:
"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): if line.endswith('\r\n'): lines[i] = line[:-2] + '\n' <IF_STMT> lines[i] = line[:-1] + '\n'",elif line.endswith('\r'):
"def payment_control_render(self, request: HttpRequest, payment: OrderPayment): template = get_template('pretixplugins/paypal/control.html') sale_id = None for trans in payment.info_data.get('transactions', []): for res in trans.get('related_resources', []): <IF_STMT> sale_id = res['sale']['id'] ctx = {'request': request, 'event': self.event, 'settings': self.settings, 'payment_info': payment.info_data, 'order': payment.order, 'sale_id': sale_id} return template.render(ctx)",if 'sale' in res and 'id' in res['sale']:
"def for_name(self, name): try: name_resources = self._resources[name] except KeyError: raise LookupError(name) else: for res in name_resources: try: inst = res.inst() except Exception as e: <IF_STMT> log.exception('error initializing %s', res) else: log.error('error initializing %s: %s', res, e) else: yield inst",if log.getEffectiveLevel() <= logging.DEBUG:
"def describe(self, done=False): description = ShellCommand.describe(self, done) if done: <IF_STMT> description = ['compile'] description.append('%d projects' % self.getStatistic('projects', 0)) description.append('%d files' % self.getStatistic('files', 0)) warnings = self.getStatistic('warnings', 0) if warnings > 0: description.append('%d warnings' % warnings) errors = self.getStatistic('errors', 0) if errors > 0: description.append('%d errors' % errors) return description",if not description:
"def parse_list(tl): ls = [] nm = [] while True: term, nmt, tl = parse_term(tl) ls.append(term) <IF_STMT> nm.append(nmt) if tl[0] != ',': break tl = tl[1:] return (ls, nm, tl)",if nmt is not None:
"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return 'raw' elif IndexedDataset.exists(path): with open(index_file_path(path), 'rb') as f: magic = f.read(8) <IF_STMT> return 'cached' elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]: return 'mmap' else: return None elif FastaDataset.exists(path): return 'fasta' else: return None",if magic == IndexedDataset._HDR_MAGIC:
"def _get(self): fut = item = None with self._mutex: <IF_STMT> fut = Future() fut.add_done_callback(lambda f: self._get_complete() if not f.cancelled() else None) self._getters.append(fut) else: item = self._get_item() self._get_complete() return (item, fut)",if not self._queue or self._getters:
"def validate(self): dates = [] for d in self.get('leave_block_list_dates'): <IF_STMT> frappe.msgprint(_('Date is repeated') + ':' + d.block_date, raise_exception=1) dates.append(d.block_date)",if d.block_date in dates:
"def on_choose_watch_dir_clicked(self): if self.window().watchfolder_enabled_checkbox.isChecked(): previous_watch_dir = self.window().watchfolder_location_input.text() or '' watch_dir = QFileDialog.getExistingDirectory(self.window(), 'Please select the watch folder', previous_watch_dir, QFileDialog.ShowDirsOnly) <IF_STMT> return self.window().watchfolder_location_input.setText(watch_dir)",if not watch_dir:
"def log_generator(self, limit=6000, **kwargs): skip = 0 while True: logs = self.log(limit=limit, skip=skip, **kwargs) if not logs: break for entry in logs: yield entry <IF_STMT> break skip = skip + limit",if len(logs) < limit:
"def _setUpClass(cls): global solver import pyomo.environ from pyomo.solvers.tests.io.writer_test_cases import testCases for test_case in testCases: <IF_STMT> solver[test_case.name, test_case.io] = True","if (test_case.name, test_case.io) in solver and test_case.available:"
"def _get_file_data(self, normpath, normrev): data = self.client.cat(normpath, normrev) if has_expanded_svn_keywords(data): keywords = self.client.propget('svn:keywords', normpath, normrev, recurse=True) <IF_STMT> data = collapse_svn_keywords(data, force_bytes(keywords[normpath])) return data",if normpath in keywords:
"def add_controller_list(path): if not os.path.exists(os.path.join(path, '__init__.py')): bb.fatal('Controllers directory %s exists but is missing __init__.py' % path) files = sorted([f for f in os.listdir(path) if f.endswith('.py') and (not f.startswith('_'))]) for f in files: module = 'oeqa.controllers.' + f[:-3] <IF_STMT> controllerslist.append(module) else: bb.warn('Duplicate controller module found for %s, only one added. Layers should create unique controller module names' % module)",if module not in controllerslist:
"def on_session2(event): new_xmpp.get_roster() new_xmpp.send_presence() logging.info(roster[0]) data = roster[0]['roster']['items'] logging.info(data) for jid, item in data.items(): <IF_STMT> new_xmpp.send_presence(ptype='subscribe', pto=jid) new_xmpp.update_roster(jid, name=item['name'], groups=item['groups']) new_xmpp.disconnect()",if item['subscription'] != 'none':
"def _parse_class_simplified(symbol): results = {} name = symbol.name + '(' name += ', '.join([analyzer.expand_attribute(base) for base in symbol.bases]) name += ')' for sym in symbol.body: <IF_STMT> result = _parse_function_simplified(sym, symbol.name) results.update(result) elif isinstance(sym, ast.ClassDef): result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, 'c') return results","if isinstance(sym, ast.FunctionDef):"
"def check_args(args): """"""Checks that the args are coherent."""""" check_args_has_attributes(args) if args.v: non_version_attrs = [v for k, v in args.__dict__.items() if k != 'v'] print('non_version_attrs', non_version_attrs) <IF_STMT> fail('Cannot show the version number with another command.') return if args.i is None: fail('Cannot draw ER diagram of no database.') if args.o is None: fail('Cannot draw ER diagram with no output file.')",if len([v for v in non_version_attrs if v is not None]) != 0:
"def handle(self, *args, **options): if not settings.ST_BASE_DIR.endswith('spirit'): raise CommandError('settings.ST_BASE_DIR is not the spirit root folder, are you overriding it?') for root, dirs, files in os.walk(settings.ST_BASE_DIR): <IF_STMT> continue with utils.pushd(root): call_command('makemessages', stdout=self.stdout, stderr=self.stderr, **options) self.stdout.write('ok')",if 'locale' not in dirs:
"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: <IF_STMT> return scan(s) or s elif s.type in ('suite', 'decorated'): return scan(s) return None","if isinstance(s, (tree.Scope, tree.Flow)):"
def run_sync(self): count = 0 while count < self.args.num_messages: batch = self.receiver.fetch_next(max_batch_size=self.args.num_messages - count) <IF_STMT> for msg in batch: msg.complete() count += len(batch),if self.args.peeklock:
"def __getitem__(self, item): if self._datas is not None: ret = [] for data in self._datas: <IF_STMT> ret.append(data[self._offset]) else: ret.append(data.iloc[self._offset]) self._offset += 1 return ret else: return self._get_data(item)","if isinstance(data, np.ndarray):"
"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == '550': <IF_STMT> raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise",if self.isfile(path):
"def replaces_in_file(file, replacement_list): rs = [(re.compile(regexp), repl) for regexp, repl in replacement_list] file_tmp = file + '.' + str(os.getpid()) + '.tmp' with open(file, 'r') as f: with open(file_tmp, 'w') as f_tmp: for line in f: for r, replace in rs: match = r.search(line) <IF_STMT> line = replace + '\n' f_tmp.write(line) shutil.move(file_tmp, file)",if match:
"def _get_path_check_mem(self, i, size): if size > 0: <IF_STMT> p = self._get_path(i, -1) else: p = self._get_path(i, size) if p.startswith('/dev/shm'): env.meminfo.add(size) else: p = self._get_path(i, size) return p",if env.meminfo.rss + size > env.meminfo.mem_limit_soft:
"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] parent = self['editor'] for c in parent.get_children(): <IF_STMT> if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) if not r is None: return r return None","if hasattr(c, 'get_id'):"
"def _deserialize(cls, io): flags = VideoFlags() flags.byte = U8.read(io) if flags.bit.type == VIDEO_FRAME_TYPE_COMMAND_FRAME: data = VideoCommandFrame.deserialize(io) el<IF_STMT> data = AVCVideoData.deserialize(io) else: data = io.read() return cls(flags.bit.type, flags.bit.codec, data)",if flags.bit.codec == VIDEO_CODEC_ID_AVC:
"def asciiLogData(data, maxlen=64, replace=False): ellipses = ' ...' try: <IF_STMT> dd = data[:maxlen] + ellipses else: dd = data return dd.decode('utf8', errors='replace' if replace else 'strict') except: return '0x' + binLogData(data, maxlen)",if len(data) > maxlen - len(ellipses):
"def _check_units(self, new_unit_system): if self.unit_system is None: self.unit_system = new_unit_system el<IF_STMT> raise ValueError('Unit system mismatch %d v. %d' % (self.unit_system, new_unit_system))",if self.unit_system != new_unit_system:
"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) <IF_STMT> print('%s: %s' % (filename, visitor.get_stats())) if fix: print('Fixing: %s' % filename) fix_file(filename)",if visitor.needs_fix():
"def assign_attributes_to_variants(variant_attributes): for value in variant_attributes: pk = value['pk'] defaults = value['fields'] defaults['variant_id'] = defaults.pop('variant') defaults['assignment_id'] = defaults.pop('assignment') assigned_values = defaults.pop('values') assoc, created = AssignedVariantAttribute.objects.update_or_create(pk=pk, defaults=defaults) <IF_STMT> assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))",if created:
"def _info(self, userlist): for strng in userlist: group_matched = False for env in self.base.comps.environments_by_pattern(strng): self.output.display_groups_in_environment(env) group_matched = True for group in self.base.comps.groups_by_pattern(strng): self.output.display_pkgs_in_groups(group) group_matched = True <IF_STMT> logger.error(_('Warning: Group %s does not exist.'), strng) return (0, [])",if not group_matched:
def parse_implements_interfaces(parser): types = [] if parser.token.value == 'implements': advance(parser) while True: types.append(parse_named_type(parser)) <IF_STMT> break return types,"if not peek(parser, TokenKind.NAME):"
"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 <IF_STMT> yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",elif leaf == SymbolFalse:
"def update_gstin(context): dirty = False for key, value in iteritems(frappe.form_dict): if key != 'party': address_name = frappe.get_value('Address', key) <IF_STMT> address = frappe.get_doc('Address', address_name) address.gstin = value.upper() address.save(ignore_permissions=True) dirty = True if dirty: frappe.db.commit() context.updated = True",if address_name:
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): <IF_STMT> if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True","if isinstance(v, dict) and k != 'headers':"
"def check_graph(graph): for c in graph: <IF_STMT> raise RuntimeError('cannot have fuse') for inp in c.inputs: if isinstance(inp.op, Fuse): raise RuntimeError('cannot have fuse')","if isinstance(c.op, Fuse):"
"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: <IF_STMT> if isinstance(value, _ModuleMarker): return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError('Module %r has no mapped classes registered under the name %r' % (self.__parent.name, key))",if value is not None:
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue <IF_STMT> ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",if nw_id_ == nw_id:
"def _parse(self, contents): entries = [] for line in contents.splitlines(): if not len(line.strip()): entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') <IF_STMT> entries.append(('all_comment', [line])) continue entries.append(('option', [head.split(None), tail])) return entries",if not len(head):
"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, '_UNDOCUMENTED', False): continue <IF_STMT> continue if getattr(command, 'positional_arg', False): continue names.append(key) return names",if startswith is not None and (not key.startswith(startswith)):
"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] <IF_STMT> val = tf.sparse.to_dense(val) if val.dtype == tf.int64: val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",if tf.keras.backend.is_sparse(val):
"def _get_lang_zone(self, lang): if lang not in self._lang_zone_from_lang: <IF_STMT> self._lang_zone_from_lang[lang] = MultiLangZone(self.mgr, lang) else: self._lang_zone_from_lang[lang] = LangZone(self.mgr, lang) return self._lang_zone_from_lang[lang]",if self.mgr.is_multilang(lang):
"def dispatch(self, request, *args, **kwargs): try: return super(Handler, self).dispatch(request, *args, **kwargs) except Http404 as e: <IF_STMT> try: request.original_path_info = request.path_info request.path_info = settings.FEINCMS_CMS_404_PAGE response = super(Handler, self).dispatch(request, *args, **kwargs) response.status_code = 404 return response except Http404: raise e else: raise",if settings.FEINCMS_CMS_404_PAGE:
"def _maybe_update_dropout(self, step): for i in range(len(self.dropout_steps)): <IF_STMT> self.model.update_dropout(self.dropout[i]) logger.info('Updated dropout to %f from step %d' % (self.dropout[i], step))",if step > 1 and step == self.dropout_steps[i] + 1:
"def bulk_move(*args, **kwargs): for arg in args: <IF_STMT> raise PopupException(_('Source path and destination path cannot be same')) request.fs.rename(urllib.unquote(arg['src_path']), urllib.unquote(arg['dest_path']))",if arg['src_path'] == arg['dest_path']:
"def asisWrite(self, root): at, c = (self, self.c) try: c.endEditing() c.init_error_dialogs() fileName = at.initWriteIvars(root, root.atAsisFileNodeName()) <IF_STMT> at.addToOrphanList(root) return at.openOutputStream() for p in root.self_and_subtree(copy=False): at.writeAsisNode(p) contents = at.closeOutputStream() at.replaceFile(contents, at.encoding, fileName, root) except Exception: at.writeException(fileName, root)","if not at.precheck(fileName, root):"
"def next_event(it): """"""read an event from an eventstream"""""" while True: try: line = next(it) except StopIteration: return <IF_STMT> return json.loads(line.split(':', 1)[1])",if line.startswith('data:'):
"def process_formdata(self, valuelist): if valuelist: if valuelist[0] == '__None': self.data = None else: <IF_STMT> self.data = None return try: obj = self.queryset.get(pk=valuelist[0]) self.data = obj except DoesNotExist: self.data = None",if self.queryset is None:
"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <IF_STMT> warnings.warn('{}: setting results name {!r} on {} expression will return a list of all parsed tokens in an And alternative, in prior versions only the first token was returned'.format('warn_multiple_tokens_in_named_alternation', name, type(self).__name__), stacklevel=3) return super()._setResultsName(name, listAllMatches)","if any((isinstance(e, And) for e in self.exprs)):"
"def add(request): form_type = 'servers' if request.method == 'POST': form = BookMarkForm(request.POST) if form.is_valid(): form_type = form.save() messages.add_message(request, messages.INFO, 'Bookmark created') else: messages.add_message(request, messages.INFO, form.errors) <IF_STMT> url = reverse('servers') else: url = reverse('metrics') return redirect(url) else: return redirect(reverse('servers'))",if form_type == 'server':
"def __init__(self, post_id, artist, page, tzInfo=None, dateFormat=None): self.imageUrls = list() self.imageResizedUrls = list() self.imageId = int(post_id) self._tzInfo = tzInfo self.dateFormat = dateFormat if page is not None: post_json = demjson.decode(page) <IF_STMT> artist_id = post_json['data']['item']['user']['id'] self.artist = SketchArtist(artist_id, page, tzInfo, dateFormat) else: self.artist = artist self.parse_post(post_json['data']['item'])",if artist is None:
"def _create_batch_iterator(self, mark_as_delete: Callable[[Any], None], to_key: Callable[[Any], Any], to_value: Callable[[Any], Any], batch: Iterable[EventT]) -> Iterable[Tuple[Any, Any]]: for event in batch: key = to_key(event.key) <IF_STMT> mark_as_delete(key) continue yield (key, to_value(event.value))",if event.message.value is None:
"def test_lc_numeric_nl_langinfo(self): tested = False for loc in candidate_locales: try: setlocale(LC_NUMERIC, loc) setlocale(LC_CTYPE, loc) except Error: continue for li, lc in ((RADIXCHAR, 'decimal_point'), (THOUSEP, 'thousands_sep')): <IF_STMT> tested = True if not tested: self.skipTest('no suitable locales')","if self.numeric_tester('nl_langinfo', nl_langinfo(li), lc, loc):"
def _level_up_logging(self): for handler in self.log.handlers: <IF_STMT> if handler.level != logging.DEBUG: handler.setLevel(logging.DEBUG) self.log.debug('Leveled up log file verbosity'),"if issubclass(handler.__class__, logging.FileHandler):"
def _show_axes_changed(self): marker = self.marker if self._vtk_control is not None and marker is not None: <IF_STMT> marker.interactor = None marker.enabled = False else: marker.interactor = self.interactor marker.enabled = True self.render(),if not self.show_axes:
"def handle_keypress(self, rawKey, modifiers, key, *args): if self.recordKeyboard and self.__delayPassed(): <IF_STMT> self.insideKeys = True self.targetParent.start_key_sequence() modifierCount = len(modifiers) if modifierCount > 1 or (modifierCount == 1 and Key.SHIFT not in modifiers) or (Key.SHIFT in modifiers and len(rawKey) > 1): self.targetParent.append_hotkey(rawKey, modifiers) elif key not in MODIFIERS: self.targetParent.append_key(key)",if not self.insideKeys:
"def transform(self, data): with timer('transform %s' % self.name, logging.DEBUG): if self.operator in {'lat', 'latitude'}: return self.series(data).apply(GeoIP.get_latitude) <IF_STMT> return self.series(data).apply(GeoIP.get_longitude) elif self.operator in {'acc', 'accuracy'}: return self.series(data).apply(GeoIP.get_accuracy) raise NameError('Unknown GeoIP operator [lat, lon, acc]: %s' % self.operator)","elif self.operator in {'lon', 'longitude'}:"
def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = 'bl_%s' % self.businessline_id <IF_STMT> sidebar_selected += '_s_%s' % self.service_id if self.environment_id: sidebar_selected += '_env_%s' % self.environment_id return sidebar_selected,if self.service_id:
"def _run_response_middleware(self, request, response, request_name=None): named_middleware = self.named_response_middleware.get(request_name, deque()) applicable_middleware = self.response_middleware + named_middleware if applicable_middleware: for middleware in applicable_middleware: _response = middleware(request, response) <IF_STMT> _response = await _response if _response: response = _response break return response",if isawaitable(_response):
"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: if self._should_delete: field.delete() return <IF_STMT> if not field.grid_id: func = field.put else: func = field.replace func(self.data.stream, filename=self.data.filename, content_type=self.data.content_type)","if isinstance(self.data, FileStorage) and (not is_empty(self.data.stream)):"
"def _import_hash(self, operator): for key in sorted(operator.import_hash.keys()): module_list = ', '.join(sorted(operator.import_hash[key])) <IF_STMT> exec('from {} import {}'.format(key[4:], module_list)) else: exec('from {} import {}'.format(key, module_list)) for var in operator.import_hash[key]: self.operators_context[var] = eval(var)",if key.startswith('tpot.'):
"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) <IF_STMT> extension = os.path.splitext(f_path)[1] if extension in file_extensions: os.remove(f_path)",if os.path.isfile(f_path):
def clearBuffer(self): if self.shouldLose == -1: return if self.producer: self.producer.resumeProducing() if self.buffer: <IF_STMT> self.logFile.write('loopback receiving %s\n' % repr(self.buffer)) buffer = self.buffer self.buffer = b'' self.target.dataReceived(buffer) if self.shouldLose == 1: self.shouldLose = -1 self.target.connectionLost(failure.Failure(main.CONNECTION_DONE)),if self.logFile:
"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + ': ') if bytes: sys.stderr.write(data.decode('utf8')) else: sys.stderr.write(data) <IF_STMT> self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",if data[-1] == '\n':
def stop(self): self.queue_com.state_lock.acquire() try: <IF_STMT> self.queue_com.state = STOPPED self.remove() return True return False finally: self.queue_com.state_lock.release(),if self.queue_com.state == RUNNING and self.stop_task():
"def _handle_special_args(self, pyobjects): if len(pyobjects) == len(self.arguments.args): if self.arguments.vararg: pyobjects.append(rope.base.builtins.get_list()) <IF_STMT> pyobjects.append(rope.base.builtins.get_dict())",if self.arguments.kwarg:
"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos if not osp.isfile(filename): self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() <IF_STMT> editor.set_cursor_position(position)",if position < editor.document().characterCount():
"def _create_sentence_objects(self): """"""Returns a list of Sentence objects from the raw text."""""" sentence_objects = [] sent_tokenizer = SentenceTokenizer(locale=self.language.code) seq = Sequence(self.raw) seq = sent_tokenizer.transform(seq) for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]): sent = seq.text[start_index:end_index].strip() <IF_STMT> continue s = Sentence(sent, start_index=start_index, end_index=end_index) s.detected_languages = self.detected_languages sentence_objects.append(s) return sentence_objects",if not sent:
"def to_json_schema(self, parent=None): schema = {} if not parent: schema['title'] = self.title <IF_STMT> schema['description'] = self.description if self.has_default: schema['default'] = self.default schema['_required_'] = self.required if self.null: schema['type'] = ['string', 'null'] else: schema['type'] = 'string' if self.enum is not None: schema['enum'] = self.enum return schema",if self.description:
def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return for f in os.listdir(dirname): <IF_STMT> continue path = dirname + os.sep + f if os.path.isdir(path): rmdir(path) else: os.unlink(path) os.rmdir(dirname),"if f in ('.', '..'):"
"def convert_whole_dir(path=Path('marian_ckpt/')): for subdir in tqdm(list(path.ls())): dest_dir = f'marian_converted/{subdir.name}' <IF_STMT> continue convert(source_dir, dest_dir)",if (dest_dir / 'pytorch_model.bin').exists():
"def colorformat(text): if text[0:1] == '#': col = text[1:] if len(col) == 6: return col <IF_STMT> return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == '': return '' assert False, 'wrong color format %r' % text",elif len(col) == 3:
"def _init_rel_seek(self): """"""Sets the file object's position to the relative location set above."""""" rs, fo = (self._rel_seek, self._file_obj) if rs == 0.0: fo.seek(0, os.SEEK_SET) else: fo.seek(0, os.SEEK_END) size = fo.tell() <IF_STMT> self._cur_pos = size else: target = int(size * rs) fo.seek(target, os.SEEK_SET) self._align_to_newline() self._cur_pos = fo.tell()",if rs == 1.0:
"def parse_command_line(self, argv=None): """"""Parse the command line"""""" if self.config: parser = argparse.ArgumentParser(add_help=False) self.settings['config'].add_argument(parser) opts, _ = parser.parse_known_args(argv) if opts.config is not None: self.set('config', opts.config) self.params.update(self.import_from_module()) parser = self.parser() opts = parser.parse_args(argv) for k, v in opts.__dict__.items(): <IF_STMT> continue self.set(k.lower(), v)",if v is None:
"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client('shield', region_name='us-east-1') protections = get_type_protections(client, self.manager.get_model()) protected_resources = {p['ResourceArn'] for p in protections} state = self.data.get('state', False) results = [] for arn, r in zip(self.manager.get_arns(resources), resources): r['c7n:ShieldProtected'] = shielded = arn in protected_resources <IF_STMT> results.append(r) elif not shielded and (not state): results.append(r) return results",if shielded and state:
"def removeTrailingWs(self, aList): i = 0 while i < len(aList): <IF_STMT> j = i i = self.skip_ws(aList, i) assert j < i if i >= len(aList) or aList[i] == '\n': del aList[j:i] i = j else: i += 1",if self.is_ws(aList[i]):
"def predict(request: Request): form = await request.form() files, entry = convert_input(form) try: <IF_STMT> return JSONResponse(ALL_FEATURES_PRESENT_ERROR, status_code=400) try: resp = model.predict(data_dict=[entry]).to_dict('records')[0] return JSONResponse(resp) except Exception as e: logger.error('Error: {}'.format(str(e))) return JSONResponse(COULD_NOT_RUN_INFERENCE_ERROR, status_code=500) finally: for f in files: os.remove(f.name)",if entry.keys() & input_features != input_features:
"def reset(self): logger.debug('Arctic.reset()') with self._lock: <IF_STMT> self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): if hasattr(l, '_reset') and callable(l._reset): logger.debug('Library reset() %s' % l) l._reset()",if self.__conn is not None:
"def read(self): if op.isfile(self.fileName): with textfile_open(self.fileName, 'rt') as fid: items = json.load(fid) <IF_STMT> items = dict() else: items = dict() self._items.clear() self._items.update(items) self._haveReadData = True",if items is None:
"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): if text[end - 2:end] == '#}': return text[i:end] <IF_STMT> unclosed_end = end end += 1 raise TokenizationException('Unclosed comment', text[i:unclosed_end])",if not unclosed_end and text[end] == '<':
"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: value_ = self._late_binding_returnvalues.get(key, KeyError) <IF_STMT> value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) if schema is not None: value_ = schema.validate(value_) self._late_binding_returnvalues[key] = value_ return value_ else: return value",if value_ is KeyError:
"def connect(*args, **ckwargs): if 'give_content_type' in kwargs: <IF_STMT> kwargs['give_content_type'](args[6]['content-type']) else: kwargs['give_content_type']('') if 'give_connect' in kwargs: kwargs['give_connect'](*args, **ckwargs) status = code_iter.next() etag = etag_iter.next() timestamp = timestamps_iter.next() if status == -1: raise HTTPException() return FakeConn(status, etag, body=kwargs.get('body', ''), timestamp=timestamp)",if len(args) >= 7 and 'content_type' in args[6]:
"def _reset(self): self._handle_connect() if self.rewarder_session: <IF_STMT> env_id = random.choice(self._sample_env_ids) logger.info('Randomly sampled env_id={}'.format(env_id)) else: env_id = None self.rewarder_session.reset(env_id=env_id) else: logger.info('No rewarder session exists, so cannot send a reset via the rewarder channel') self._reset_mask() return [None] * self.n",if self._sample_env_ids:
"def _create_architecture_list(architectures, current_arch): if not architectures: return [_Architecture(build_on=[current_arch])] build_architectures: List[str] = [] architecture_list: List[_Architecture] = [] for item in architectures: if isinstance(item, str): build_architectures.append(item) <IF_STMT> architecture_list.append(_Architecture(build_on=item.get('build-on'), run_on=item.get('run-on'))) if build_architectures: architecture_list.append(_Architecture(build_on=build_architectures)) return architecture_list","if isinstance(item, dict):"
"def inspect(self, pokemon): for caught_pokemon in self.cache: same_latitude = '{0:.4f}'.format(pokemon['latitude']) == '{0:.4f}'.format(caught_pokemon['latitude']) same_longitude = '{0:.4f}'.format(pokemon['longitude']) == '{0:.4f}'.format(caught_pokemon['longitude']) <IF_STMT> return if len(self.cache) >= 200: self.cache.pop(0) self.cache.append(pokemon)",if same_latitude and same_longitude:
"def parley(self): for x in [0, 1]: a = self.agents[x].act() <IF_STMT> if '[DONE]' in a['text']: self.agents[x - 1].observe({'id': 'World', 'text': 'The other agent has ended the chat.'}) self.episodeDone = True else: self.agents[x - 1].observe(a)",if a is not None:
"def _prepare_subset(full_data: torch.Tensor, full_targets: torch.Tensor, num_samples: int, digits: Sequence): classes = {d: 0 for d in digits} indexes = [] for idx, target in enumerate(full_targets): label = target.item() if classes.get(label, float('inf')) >= num_samples: continue indexes.append(idx) classes[label] += 1 <IF_STMT> break data = full_data[indexes] targets = full_targets[indexes] return (data, targets)",if all((classes[k] >= num_samples for k in classes)):
"def get_work_root(self, flags): _flags = flags.copy() _flags['is_toplevel'] = True target = self._get_target(_flags) if target: _flags['target'] = target.name tool = self.get_tool(_flags) <IF_STMT> return target.name + '-' + tool else: raise SyntaxError('Failed to determine work root. Could not resolve tool for target ' + target.name) else: raise SyntaxError('Failed to determine work root. Could not resolve target')",if tool:
"def run_command(self, data): """"""Run editor commands."""""" parts = data.split(' ') cmd = parts[0].lower() if cmd in self.operations.keys(): return self.run_operation(cmd) args = ' '.join(parts[1:]) self.logger.debug(""Looking for command '{0}'"".format(cmd)) if cmd in self.modules.modules.keys(): self.logger.debug(""Trying to run command '{0}'"".format(cmd)) self.get_editor().store_action_state(cmd) <IF_STMT> return False else: self.set_status(""Command '{0}' not found."".format(cmd)) return False return True","if not self.run_module(cmd, args):"
"def get_main_chain_layers(self): """"""Return a list of layer IDs in the main chain."""""" main_chain = self.get_main_chain() ret = [] for u in main_chain: for v, layer_id in self.adj_list[u]: <IF_STMT> ret.append(layer_id) return ret",if v in main_chain and u in main_chain:
"def hash(self, context): with context: <IF_STMT> return IECore.MurmurHash() h = GafferDispatch.TaskNode.hash(self, context) h.append(self['fileName'].hash()) h.append(self['in'].hash()) h.append(self.__parameterHandler.hash()) return h",if not self['fileName'].getValue() or self['in'].source() == self['in']:
"def consume_buf(): ty = state['ty'] - 1 for i in xrange(state['buf'].shape[1] // N): tx = x // N + i src = state['buf'][:, i * N:(i + 1) * N, :] <IF_STMT> with self.tile_request(tx, ty, readonly=False) as dst: mypaintlib.tile_convert_rgba8_to_rgba16(src, dst, self.EOTF) if state['progress']: try: state['progress'].completed(ty - ty0) except Exception: logger.exception('Progress.completed() failed') state['progress'] = None","if src[:, :, 3].any():"
"def check_permissions(self, obj): request = self.context.get('request') for Perm in permissions: perm = Perm() if not perm.has_permission(request, self): return False <IF_STMT> return False return True","if not perm.has_object_permission(request, self, obj):"
"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): if op.attr_key == 'storage_scope': lift_stmt[-1].append(op) return op.body <IF_STMT> return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError('not reached')",if op.attr_key == 'virtual_thread':
def task_done(self): with self._cond: if not self._unfinished_tasks.acquire(False): raise ValueError('task_done() called too many times') <IF_STMT> self._cond.notify_all(),if self._unfinished_tasks._semlock._is_zero():
"def get_json(self): if not hasattr(self, '_json'): self._json = None <IF_STMT> self._json = json.loads(self.request.body) return self._json","if self.request.headers.get('Content-Type', '').startswith('application/json'):"
"def userfullname(): """"""Get the user's full name."""""" global _userfullname <IF_STMT> uid = os.getuid() entry = pwd_from_uid(uid) if entry: _userfullname = entry[4].split(',')[0] or entry[0] if not _userfullname: _userfullname = 'user%d' % uid return _userfullname",if not _userfullname:
"def test_scatter(self): for rank in range(self.world_size): tensor = [] <IF_STMT> tensor = [torch.tensor(i) for i in range(self.world_size)] result = comm.get().scatter(tensor, rank, size=()) self.assertTrue(torch.is_tensor(result)) self.assertEqual(result.item(), self.rank)",if self.rank == rank:
"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) if method is None: throw(DecompileError('Unsupported operation: %s' % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) <IF_STMT> decompiler.stack.append(x)",if x is not None:
"def print_scenario_ran(self, scenario): if scenario.passed: self.wrt('OK') elif scenario.failed: reason = self.scenarios_and_its_fails[scenario] <IF_STMT> self.wrt('FAILED') else: self.wrt('ERROR') self.wrt('\n')","if isinstance(reason.exception, AssertionError):"
"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): if option != other_option: <IF_STMT> raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option","if scan_argv(self.argv, other_option) is not None:"
"def print_po_snippet(en_loc_old_lists, context): for m, localized, old in zip(*en_loc_old_lists): if m == '': continue <IF_STMT> localized = old print('#: {file}:{line}\nmsgid ""{context}{en_month}""\nmsgstr ""{localized_month}""\n'.format(context=context, file=filename, line=print_po_snippet.line, en_month=m, localized_month=localized)) print_po_snippet.line += 1",if m == localized:
"def set_status(self, dict_new): for i, value in dict_new.items(): self.dict_bili[i] = value <IF_STMT> self.dict_bili['pcheaders']['cookie'] = value self.dict_bili['appheaders']['cookie'] = value",if i == 'cookie':
"def makeSomeFiles(pathobj, dirdict): pathdict = {} for key, value in dirdict.items(): child = pathobj.child(key) <IF_STMT> pathdict[key] = child child.setContent(value) elif isinstance(value, dict): child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError('only strings and dicts allowed as values') return pathdict","if isinstance(value, bytes):"
"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) if len_map is not None: for key, max_len in len_map.items(): example_len = example[key].shape <IF_STMT> example[key] = np.resize(example[key], max_len) yield tuple(example)",if example_len > max_len:
"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: if 'dir' in kw: return not kw['dir'] ^ isdir(self.strpath) <IF_STMT> return not kw['file'] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",if 'file' in kw:
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): if i > 0 and (not lines[i - 1].strip()): return False continue if line.startswith('def ') or line.startswith('class '): return True <IF_STMT> continue return False return False","if line.startswith(('#', '@', ' ', ')')):"
"def askCheckReadFile(self, localFile, remoteFile): if not kb.bruteMode: message = ""do you want confirmation that the remote file '%s' "" % remoteFile message += 'has been successfully downloaded from the back-end ' message += 'DBMS file system? [Y/n] ' <IF_STMT> return self._checkFileLength(localFile, remoteFile, True) return None","if readInput(message, default='Y', boolean=True):"
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) <IF_STMT> exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return (company, major, minor, arch, exe, args)",if arch is not None:
"def _get_matching_bracket(self, s, pos): if s[pos] != '{': return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == '{': depth += 1 <IF_STMT> depth -= 1 if depth == 0: break pos += 1 if pos < end and s[pos] == '}': return pos return None",elif c == '}':
"def pred(field, value, item): for suffix, p in _BUILTIN_PREDS.iteritems(): if field.endswith(suffix): f = field[:field.index(suffix)] <IF_STMT> return False return p(getattr(item, f), value) if not hasattr(item, field) or getattr(item, field) is None: return False if isinstance(value, type(lambda x: x)): return value(getattr(item, field)) return getattr(item, field) == value","if not hasattr(item, f) or getattr(item, f) is None:"
"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): if isinstance(m, nn.ConvTranspose2d): normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): <IF_STMT> normal_init(m, std=0.001, bias=0)","if isinstance(m, nn.Conv2d):"
"def test_byteswap(self): if self.typecode == 'u': example = '\U00100100' else: example = self.example a = array.array(self.typecode, example) self.assertRaises(TypeError, a.byteswap, 42) if a.itemsize in (1, 2, 4, 8): b = array.array(self.typecode, example) b.byteswap() <IF_STMT> self.assertEqual(a, b) else: self.assertNotEqual(a, b) b.byteswap() self.assertEqual(a, b)",if a.itemsize == 1:
"def _remove_blocks_from_variables(variables): new_variables = [] for name, variable in variables: <IF_STMT> new_variables.extend(variable.locals) new_variables.append((name, variable.result)) else: new_variables.append((name, variable)) return new_variables",if variable.is_block():
def scope(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.scope_ is None: self.scope_ = Scope() finally: self.lazy_init_lock_.release() return self.scope_,if self.scope_ is None:
"def translate(): assert Lex.next() is AttributeList reader.read() attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: if k == 'attrlist': v = subs_attrs(v) <IF_STMT> parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",if v:
"def parse(self, response): try: content = response.content.decode('utf-8', 'ignore') content = json.loads(content, strict=False) except: self.logger.error('Fail to parse the response in json format') return for item in content['data']: <IF_STMT> img_url = self._decode_url(item['objURL']) elif 'hoverURL' in item: img_url = item['hoverURL'] else: continue yield dict(file_url=img_url)",if 'objURL' in item:
"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() if name == 'MOV': if instr.mnemonic.startswith('lsr'): return 'LSR' elif instr.mnemonic.startswith('lsl'): return 'LSL' <IF_STMT> return 'ASR' return OP_NAME_MAP.get(name, name)",elif instr.mnemonic.startswith('asr'):
"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) <IF_STMT> if isinstance(target, six.string_types) and os.path.isfile(target): target = _load_regions(target) else: target = [target] return target",if target:
def reader_leaves(self): self.mutex.acquire() try: self.active_readers -= 1 <IF_STMT> self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() finally: self.mutex.release(),if self.active_readers == 0 and self.waiting_writers != 0:
"def _bpe_to_words(sentence, delimiter='@@'): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = '' delimiter_len = len(delimiter) for subwords in sentence: <IF_STMT> word += subwords[:-delimiter_len] else: word += subwords words.append(word) word = '' return words",if len(subwords) >= delimiter_len and subwords[-delimiter_len:] == delimiter:
"def _make_var_names(exog): if hasattr(exog, 'name'): var_names = exog.name elif hasattr(exog, 'columns'): var_names = exog.columns else: raise ValueError('exog is not a Series or DataFrame or is unnamed.') try: var_names = ' '.join(var_names) except TypeError: from statsmodels.base.data import _make_exog_names <IF_STMT> var_names = 'x1' else: var_names = ' '.join(_make_exog_names(exog)) return var_names",if exog.ndim == 1:
"def __start_element_handler(self, name, attrs): if name == 'mime-type': <IF_STMT> for extension in self.extensions: self[extension] = self.type self.type = attrs['type'].lower() self.extensions = [] elif name == 'glob': pattern = attrs['pattern'] if pattern.startswith('*.'): self.extensions.append(pattern[1:].lower())",if self.type:
"def nodes(self, id=None, name=None): for node_dict in self.node_ls(id=id, name=name): node_id = node_dict['ID'] node = DockerNode(self, node_id, inspect=node_dict) <IF_STMT> continue yield node",if self._node_prefix and (not node.name.startswith(self._node_prefix)):
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() <IF_STMT> e.value = 0 return self",if type(e) is Command or (type(e) is Option and e.argcount == 0):
"def vi_search(self, rng): for i in rng: line_history = self._history.history[i] pos = line_history.get_line_text().find(self._vi_search_text) <IF_STMT> self._history.history_cursor = i self.l_buffer.line_buffer = list(line_history.line_buffer) self.l_buffer.point = pos self.vi_undo_restart() return True self._bell() return False",if pos >= 0:
"def visitIf(self, node, scope): for test, body in node.tests: <IF_STMT> if type(test.value) in self._const_types: if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)","if isinstance(test, ast.Const):"
"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host['host'], squid_host['port']) <IF_STMT> fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) if matches: self.publish_counter('%s.%s' % (nickname, matches.group(1)), float(matches.group(2)))",if fulldata is not None:
"def convert(x, base, exponents): out = [] for e in exponents: d = int(x / base ** e) x -= d * base ** e out.append(digits[d]) <IF_STMT> break return out",if x == 0 and e < 0:
"def print_doc(manager, options): plugin_name = options.doc plugin = plugins.get(plugin_name, None) if plugin: <IF_STMT> console('Plugin %s does not have documentation' % plugin_name) else: console('') console(trim(plugin.instance.__doc__)) console('') else: console('Could not find plugin %s' % plugin_name)",if not plugin.instance.__doc__:
"def _set_attrs(self, attrs): for attr in self.ATTRS: if attr in attrs: setattr(self, attr, attrs[attr]) del attrs[attr] el<IF_STMT> setattr(self, attr, NO_DEFAULT) else: setattr(self, attr, None) if attrs: attrs = sorted(attrs.keys()) raise OptionError('invalid keyword arguments: %s' % ', '.join(attrs), self)",if attr == 'default':
"def _get_set_scope(ir_set: irast.Set, scope_tree: irast.ScopeTreeNode) -> irast.ScopeTreeNode: if ir_set.path_scope_id: new_scope = scope_tree.root.find_by_unique_id(ir_set.path_scope_id) <IF_STMT> raise errors.InternalServerError(f'dangling scope pointer to node with uid:{ir_set.path_scope_id} in {ir_set!r}') else: new_scope = scope_tree return new_scope",if new_scope is None:
"def test_leave_one_out(self): correct = 0 k = 3 model = kNN.train(xs, ys, k) predictions = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1] for i in range(len(predictions)): model = kNN.train(xs[:i] + xs[i + 1:], ys[:i] + ys[i + 1:], k) prediction = kNN.classify(model, xs[i]) self.assertEqual(prediction, predictions[i]) <IF_STMT> correct += 1 self.assertEqual(correct, 13)",if prediction == ys[i]:
"def import_files(self, files): """"""Import a list of MORE (.csv) files."""""" c = self.c if files: changed = False self.tab_width = c.getTabWidth(c.p) for fileName in files: g.setGlobalOpenDir(fileName) p = self.import_file(fileName) <IF_STMT> p.contract() p.setDirty() c.setChanged(True) changed = True if changed: c.redraw(p)",if p:
"def getPageTemplate(payload, place): retVal = (kb.originalPage, kb.errorIsNone) if payload and place: <IF_STMT> page, _, _ = Request.queryPage(payload, place, content=True, raise404=False) kb.pageTemplates[payload, place] = (page, kb.lastParserStatus is None) retVal = kb.pageTemplates[payload, place] return retVal","if (payload, place) not in kb.pageTemplates:"
"def _skip_trivial(constraint_data): if skip_trivial_constraints: if isinstance(constraint_data, LinearCanonicalRepn): if constraint_data.variables is None: return True el<IF_STMT> return True return False",if constraint_data.body.polynomial_degree() == 0:
"def get_unique_attribute(self, name: str): feat = None for f in self.features: if self._return_feature(f) and hasattr(f, name): <IF_STMT> raise RuntimeError('The attribute was not unique.') feat = f if feat is None: raise RuntimeError('The attribute did not exist') return getattr(feat, name)",if feat is not None:
"def hideEvent(self, event): """"""Reimplement Qt method"""""" if not self.light: for plugin in self.widgetlist: <IF_STMT> plugin.visibility_changed(True) QMainWindow.hideEvent(self, event)",if plugin.isAncestorOf(self.last_focused_widget):
"def move_stdout_to_stderr(self): to_remove = [] to_add = [] for consumer_level, consumer in self.consumers: <IF_STMT> to_remove.append((consumer_level, consumer)) to_add.append((consumer_level, sys.stderr)) for item in to_remove: self.consumers.remove(item) self.consumers.extend(to_add)",if consumer == sys.stdout:
"def create(exported_python_target): if exported_python_target not in created: self.context.log.info('Creating setup.py project for {}'.format(exported_python_target)) subject = self.derived_by_original.get(exported_python_target, exported_python_target) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir <IF_STMT> for dep in dependencies: if is_exported_python_target(dep): create(dep)",if self._recursive:
"def __add__(self, other): other = ArithmeticExpression.try_unpack_const(other) if not self.symbolic and type(other) is int: return SpOffset(self._bits, self._to_signed(self.offset + other)) el<IF_STMT> return SpOffset(self._bits, self.offset + other) else: return SpOffset(self._bits, ArithmeticExpression(ArithmeticExpression.Add, (self.offset, other)))",if self.symbolic:
"def check_connection(conn): tables = [r[0] for r in conn.execute(""select name from sqlite_master where type='table'"").fetchall()] for table in tables: try: conn.execute(f'PRAGMA table_info({escape_sqlite(table)});') except sqlite3.OperationalError as e: <IF_STMT> raise SpatialiteConnectionProblem(e) else: raise ConnectionProblem(e)",if e.args[0] == 'no such module: VirtualSpatialIndex':
"def _get_github_client(self) -> 'Github': from github import Github if self.access_token_secret is not None: access_token = Secret(self.access_token_secret).get() else: access_token = prefect.context.get('secrets', {}).get('GITHUB_ACCESS_TOKEN') <IF_STMT> access_token = os.getenv('GITHUB_ACCESS_TOKEN') return Github(access_token)",if access_token is None:
"def make_tab(lists): if hasattr(lists, 'tolist'): lists = lists.tolist() ut = [] for rad in lists: <IF_STMT> ut.append('\t'.join(['%s' % x for x in rad])) else: ut.append('%s' % rad) return '\n'.join(ut)","if type(rad) in [list, tuple]:"
"def _ensure_ffi_initialized(cls): with cls._init_lock: <IF_STMT> cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES) cls._lib_loaded = True cls.lib.SSL_library_init() cls.lib.OpenSSL_add_all_algorithms() cls.lib.SSL_load_error_strings() cls._register_osrandom_engine()",if not cls._lib_loaded:
def writer_leaves(self): self.mutex.acquire() try: self.active_writers -= 1 if self.waiting_writers != 0: self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() <IF_STMT> t = self.waiting_readers self.waiting_readers = 0 self.active_readers += t while t > 0: self.can_read.release() t -= 1 finally: self.mutex.release(),elif self.waiting_readers != 0:
"def _spans(self, operands): spans = {} k = 0 j = 0 for mode in (self.FLOAT, self.MPMATH): for i, operand in enumerate(operands[k:]): if operand[0] > mode: break j = i + k + 1 <IF_STMT> j = 0 spans[mode] = slice(k, j) k = j spans[self.SYMBOLIC] = slice(k, len(operands)) return spans",if k == 0 and j == 1:
"def _report_error(self, completion_routine, response=None, message=None): if response: <IF_STMT> status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: if completion_routine: return completion_routine(status) raise IOError(response.text) elif completion_routine: completion_routine(status) return location.Status(200, response.content)",if not response.ok:
"def readinto(self, buf): if self.current_frame: n = self.current_frame.readinto(buf) if n == 0 and len(buf) != 0: self.current_frame = None n = len(buf) buf[:] = self.file_read(n) return n <IF_STMT> raise UnpicklingError('pickle exhausted before end of frame') return n else: n = len(buf) buf[:] = self.file_read(n) return n",if n < len(buf):
"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] <IF_STMT> return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent if mydict is None: return",if value is not None:
"def _handle_Mul(self, expr): arg0, arg1 = expr.args expr_0 = self._expr(arg0) if expr_0 is None: return None expr_1 = self._expr(arg1) if expr_1 is None: return None try: <IF_STMT> mask = (1 << expr.result_size(self.tyenv)) - 1 return expr_0 * expr_1 & mask else: return expr_0 * expr_1 except TypeError as e: self.l.warning(e) return None","if isinstance(expr_0, int) and isinstance(expr_1, int):"
"def end_request(self, request_id): """"""Removes the information associated with given request_id."""""" with self._lock: del self._request_wsgi_environ[request_id] del self._request_id_to_server_configuration[request_id] <IF_STMT> del self._request_id_to_instance[request_id]",if request_id in self._request_id_to_instance:
def generate(): <IF_STMT> decoder = zlib.decompressobj(16 + zlib.MAX_WBITS) while True: chunk = self.raw.read(chunk_size) if not chunk: break if self._gzipped: chunk = decoder.decompress(chunk) yield chunk,if self._gzipped:
def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name <IF_STMT> name = str(venv.path) if venv == current_env: self.line('<info>{} (Activated)</info>'.format(name)) continue self.line(name),if self.option('full-path'):
"def addAggregators(sheet, cols, aggrnames): """"""Add each aggregator in list of *aggrnames* to each of *cols*."""""" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: <IF_STMT> c.aggregators = [] if aggr and aggr not in c.aggregators: c.aggregators += [aggr]","if not hasattr(c, 'aggregators'):"
"def on_pre_output_coercion(directive_args: Dict[str, Any], next_directive: Callable, value: Any, ctx: Optional[Any], info: 'ResolveInfo'): value = await next_directive(value, ctx, info) if value is None: return value try: py_enum = _ENUM_MAP[directive_args['name']] <IF_STMT> return [None if item is None else py_enum(item).name for item in value] return py_enum(value).name except Exception: pass return value","if isinstance(value, list):"
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: <IF_STMT> for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,if re_han.match(blk):
"def refresh_archive_action(self): archive_name = self.selected_archive_name() if archive_name is not None: params = BorgInfoArchiveThread.prepare(self.profile(), archive_name) <IF_STMT> thread = BorgInfoArchiveThread(params['cmd'], params, parent=self.app) thread.updated.connect(self._set_status) thread.result.connect(self.refresh_archive_result) self._toggle_all_buttons(False) thread.start()",if params['ok']:
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): if not name[0].isupper(): if not name.startswith('wait_until'): <IF_STMT> resource_methods[name] = member return resource_methods",if is_resource_action(member):
"def _get_compressor(compress_type, compresslevel=None): if compress_type == ZIP_DEFLATED: <IF_STMT> return zlib.compressobj(compresslevel, zlib.DEFLATED, -15) return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15) elif compress_type == ZIP_BZIP2: if compresslevel is not None: return bz2.BZ2Compressor(compresslevel) return bz2.BZ2Compressor() elif compress_type == ZIP_LZMA: return LZMACompressor() else: return None",if compresslevel is not None:
"def parse_header(plyfile, ext): line = [] properties = [] num_points = None while b'end_header' not in line and line != b'': line = plyfile.readline() if b'element' in line: line = line.split() num_points = int(line[2]) <IF_STMT> line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return (num_points, properties)",elif b'property' in line:
"def download_release_artifacts(self, version): try: os.mkdir(self.artifacts_dir) except FileExistsError: pass for job_name in self.build_ids: build_number = self.build_ids.get(job_name) build_status = self._get_build_status(job_name, build_number) <IF_STMT> self._download_job_artifact(job_name, build_number, version) else: print('Build for {} is not fininished'.format(job_name)) print(""\tRun 'build' action to check status of {}"".format(job_name))",if build_status == 'built':
"def update_metadata(self): for attrname in dir(self): <IF_STMT> continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrname.startswith('__'):
"def check_heuristic_in_sql(): heurs = set() excluded = ['Equal assembly or pseudo-code', 'All or most attributes'] for heur in HEURISTICS: name = heur['name'] if name in excluded: continue sql = heur['sql'] <IF_STMT> print('SQL command not correctly associated to %s' % repr(name)) print(sql) assert sql.find(name) != -1 heurs.add(name) print('Heuristics:') import pprint pprint.pprint(heurs)",if sql.lower().find(name.lower()) == -1:
def gettext(rv): for child in rv.childNodes: <IF_STMT> yield child.nodeValue if child.nodeType == child.ELEMENT_NODE: for item in gettext(child): yield item,if child.nodeType == child.TEXT_NODE:
"def update(self): """"""Update properties over dbus."""""" self._check_dbus() _LOGGER.info('Updating service information') self._services.clear() try: systemd_units = await self.sys_dbus.systemd.list_units() for service_data in systemd_units[0]: <IF_STMT> continue self._services.add(ServiceInfo.read_from(service_data)) except (HassioError, IndexError): _LOGGER.warning(""Can't update host service information!"")",if not service_data[0].endswith('.service') or service_data[2] != 'loaded':
"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: <IF_STMT> comment = source[0, source.index('*/') + 2] elif re.search('^\\s*\\/\\/', source): comment = re.search('^\\s*\\/\\/', source).group(0) else: comment = None if comment: source = re.sub('^\\s+', '', source[len(comment):]) trailing_comments.append(comment) return '\n'.join(trailing_comments) + source","if re.search('^\\s*\\/\\*', source):"
"def _getSourceStamp_sync(self, ssid): if ssid in self.sourcestamps: ssdict = self.sourcestamps[ssid].copy() ssdict['ssid'] = ssid patchid = ssdict['patchid'] <IF_STMT> ssdict.update(self.patches[patchid]) ssdict['patchid'] = patchid else: ssdict['patch_body'] = None ssdict['patch_level'] = None ssdict['patch_subdir'] = None ssdict['patch_author'] = None ssdict['patch_comment'] = None return ssdict else: return None",if patchid:
"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): if self.defaultValue is not self.__optionalNotMatched: <IF_STMT> tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return (loc, tokens)",if self.expr.resultsName:
"def _find_exceptions(): for _name, obj in iteritems(globals()): try: is_http_exception = issubclass(obj, HTTPException) except TypeError: is_http_exception = False if not is_http_exception or obj.code is None: continue __all__.append(obj.__name__) old_obj = default_exceptions.get(obj.code, None) <IF_STMT> continue default_exceptions[obj.code] = obj","if old_obj is not None and issubclass(obj, old_obj):"
"def generator(self, data): for proc_as, key_buf_ptr in data: key_buf = proc_as.read(key_buf_ptr, 24) <IF_STMT> continue key = ''.join(('%02X' % ord(k) for k in key_buf)) yield (0, [str(key)])",if not key_buf:
"def calculateEnableMargins(self): self.cnc.resetEnableMargins() for block in self.blocks: <IF_STMT> CNC.vars['xmin'] = min(CNC.vars['xmin'], block.xmin) CNC.vars['ymin'] = min(CNC.vars['ymin'], block.ymin) CNC.vars['zmin'] = min(CNC.vars['zmin'], block.zmin) CNC.vars['xmax'] = max(CNC.vars['xmax'], block.xmax) CNC.vars['ymax'] = max(CNC.vars['ymax'], block.ymax) CNC.vars['zmax'] = max(CNC.vars['zmax'], block.zmax)",if block.enable:
"def __init__(self, client, job_id, callback=None): self.client = client self.job_id = job_id with client._jobs_lock: job = client._jobs.get(job_id) self.event = None <IF_STMT> self.event = job.get('__ready') if self.event is None: self.event = job['__ready'] = Event() job['__callback'] = callback",if job:
"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: <IF_STMT> tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if not os.path.exists(fspath + '.spt'):
"def set_conf(): """"""Collapse all object_trail config into cherrypy.request.config."""""" base = cherrypy.config.copy() for name, obj, conf, segleft in object_trail: base.update(conf) <IF_STMT> base['tools.staticdir.section'] = '/' + '/'.join(fullpath[0:fullpath_len - segleft]) return base",if 'tools.staticdir.dir' in conf:
"def __init__(self): self.setLayers(None, None) self.interface = None self.event_callbacks = {} self.__stack = None self.lock = threading.Lock() members = inspect.getmembers(self, predicate=inspect.ismethod) for m in members: <IF_STMT> fname = m[0] fn = m[1] self.event_callbacks[fn.event_callback] = getattr(self, fname)","if hasattr(m[1], 'event_callback'):"
def multi_dev_generator(self): for data in self._data_loader(): <IF_STMT> self._tail_data += data if len(self._tail_data) == self._base_number: yield self._tail_data self._tail_data = [],if len(self._tail_data) < self._base_number:
"def replace_field_to_value(layout, cb): for i, lo in enumerate(layout.fields): if isinstance(lo, Field) or issubclass(lo.__class__, Field): layout.fields[i] = ShowField(cb, *lo.fields, attrs=lo.attrs, wrapper_class=lo.wrapper_class) elif isinstance(lo, basestring): layout.fields[i] = ShowField(cb, lo) <IF_STMT> replace_field_to_value(lo, cb)","elif hasattr(lo, 'get_field_names'):"
"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException('Item does not exist!') if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT: raise ItemNotFoundException(e.get_dbus_message()) <IF_STMT> raise SecretServiceNotAvailableException(e.get_dbus_message()) raise","if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED):"
"def results_iter(self): if self.connection.ops.oracle: from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] <IF_STMT> date = typecast_timestamp(str(date)) yield date",elif needs_string_cast:
"def handle_label(self, path, **options): verbosity = int(options.get('verbosity', 1)) result = finders.find(path, all=options['all']) path = smart_unicode(path) if result: if not isinstance(result, (list, tuple)): result = [result] output = u'\n  '.join((smart_unicode(os.path.realpath(path)) for path in result)) self.stdout.write(smart_str(u""Found '%s' here:\n  %s\n"" % (path, output))) el<IF_STMT> self.stderr.write(smart_str(""No matching file found for '%s'.\n"" % path))",if verbosity >= 1:
"def name(self): """"""Get the enumeration name of this storage class."""""" if self._name_map is None: self._name_map = {} for key, value in list(StorageClass.__dict__.items()): <IF_STMT> self._name_map[value] = key return self._name_map[self]","if isinstance(value, StorageClass):"
"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) if r == self._zero: return int(q) el<IF_STMT> q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError('{} is not in numeric range'.format(value))",if self._start >= value > self._stop:
"def extract_cookie(cookie_header, cookie_name): inx = cookie_header.find(cookie_name) if inx >= 0: end_inx = cookie_header.find(';', inx) <IF_STMT> value = cookie_header[inx:end_inx] else: value = cookie_header[inx:] return value return ''",if end_inx > 0:
"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) <IF_STMT> size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","elif isinstance(elem, np.ndarray):"
"def createFields(self): size = self.size / 8 if size > 2: <IF_STMT> yield UInt8(self, 'cs', '10ms units, values from 0 to 199') yield Bits(self, '2sec', 5, 'seconds/2') yield Bits(self, 'min', 6, 'minutes') yield Bits(self, 'hour', 5, 'hours') yield Bits(self, 'day', 5, '(1-31)') yield Bits(self, 'month', 4, '(1-12)') yield Bits(self, 'year', 7, '(0 = 1980, 127 = 2107)')",if size > 4:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('incap_ses|visid_incap', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None retval |= re.search('Incapsula', headers.get('X-CDN', ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def _get_order_information(self, node_id, timeout=1200, check_interval=5): mask = {'billingItem': '', 'powerState': '', 'operatingSystem': {'passwords': ''}, 'provisionDate': ''} for i in range(0, timeout, check_interval): res = self.connection.request('SoftLayer_Virtual_Guest', 'getObject', id=node_id, object_mask=mask).object <IF_STMT> return res time.sleep(check_interval) raise SoftLayerException('Timeout on getting node details')","if res.get('provisionDate', None):"
"def _process_param_change(self, msg): msg = super(Select, self)._process_param_change(msg) labels, values = (self.labels, self.values) if 'value' in msg: msg['value'] = [labels[indexOf(v, values)] for v in msg['value'] if isIn(v, values)] if 'options' in msg: msg['options'] = labels <IF_STMT> self.value = [v for v in self.value if isIn(v, values)] return msg","if any((not isIn(v, values) for v in self.value)):"
"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip('\\') for a, o in self.objects.items(): if not o.name: continue if o.name.lower() == name.lower(): return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] <IF_STMT> name = m[0] return self.get_object_from_name(name, False)",if m:
"def run(self): for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'User': if v['email'] == '': v['email'] = None if v['ip'] == '0.0.0.0': v['ip'] = None return self.objs",if k.startswith('_'):
"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): if descriptor in ent_desc: <IF_STMT> pass else: res.append(ent_id) return res",if ent_id in res:
"def test_add_participant(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: share_history_time = datetime.utcnow() share_history_time = share_history_time.replace(tzinfo=TZ_UTC) new_participant = ChatThreadParticipant(user=self.new_user, display_name='name', share_history_time=share_history_time) await self.chat_thread_client.add_participant(new_participant) <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id)",if not self.is_playback():
"def url(regex, view, kwargs=None, name=None, prefix=''): if isinstance(view, (list, tuple)): urlconf_module, app_name, namespace = view return RegexURLResolver(regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace) else: if isinstance(view, basestring): if not view: raise ImproperlyConfigured('Empty URL pattern view name not permitted (for pattern %r)' % regex) <IF_STMT> view = prefix + '.' + view return RegexURLPattern(regex, view, kwargs, name)",if prefix:
"def tx(): while not sub_ready.ready(): pub.send(b'test BEGIN') eventlet.sleep(0.005) for i in range(1, 101): msg = 'test {0}'.format(i).encode() <IF_STMT> pub.send(msg) else: pub.send(b'test LAST') sub_last.wait() eventlet.sleep(0.001) pub.send(b'done DONE')",if i != 50:
"def remove_tmp_snapshot_file(self, files): for filepath in files: path = Path(filepath) if path.is_dir() and path.exists(): shutil.rmtree(path) <IF_STMT> path.unlink()",elif path.is_file() and path.exists():
"def f(view, s): if mode == modes.INTERNAL_NORMAL: if count == 1: <IF_STMT> eol = view.line(s.b).b return R(s.b, eol) return s return s",if view.line(s.b).size() > 0:
"def get_ids(self, **kwargs): id = [] if 'id' in kwargs: id = kwargs['id'] <IF_STMT> id = id.split(',') try: id = list(map(int, id)) except Exception: decorators.error('Invalid id') return id","if not isinstance(id, list):"
def param_value(self): for token in self: <IF_STMT> return token.stripped_value if token.token_type == 'quoted-string': for token in token: if token.token_type == 'bare-quoted-string': for token in token: if token.token_type == 'value': return token.stripped_value return '',if token.token_type == 'value':
"def get_all_start_methods(self): if sys.platform == 'win32': return ['spawn'] else: methods = ['spawn', 'fork'] if sys.platform == 'darwin' else ['fork', 'spawn'] <IF_STMT> methods.append('forkserver') return methods",if reduction.HAVE_SEND_HANDLE:
"def _process_watch(self, watched_event): logger.debug('process_watch: %r', watched_event) with handle_exception(self._tree._error_listeners): <IF_STMT> assert self._parent is None, 'unexpected CREATED on non-root' self.on_created() elif watched_event.type == EventType.DELETED: self.on_deleted() elif watched_event.type == EventType.CHANGED: self._refresh_data() elif watched_event.type == EventType.CHILD: self._refresh_children()",if watched_event.type == EventType.CREATED:
"def assert_open(self, sock, *rest): if isinstance(sock, fd_types): self.__assert_fd_open(sock) else: fileno = sock.fileno() assert isinstance(fileno, fd_types), fileno sockname = sock.getsockname() assert isinstance(sockname, tuple), sockname <IF_STMT> self.__assert_fd_open(fileno) else: self._assert_sock_open(sock) if rest: self.assert_open(rest[0], *rest[1:])",if not WIN:
"def detype(self): """"""De-types the instance, allowing it to be exported to the environment."""""" style = self.style if self._detyped is None: self._detyped = ':'.join([key + '=' + ';'.join([LsColors.target_value <IF_STMT> else ansi_color_name_to_escape_code(v, cmap=style) for v in val]) for key, val in sorted(self._d.items())]) return self._detyped",if key in self._targets
"def gather_metrics(dry_run=False): today = datetime.date.today() first = today.replace(day=1) last_month = first - datetime.timedelta(days=1) filename = 'form_types_{}.csv'.format(last_month.strftime('%Y-%m')) with connection.cursor() as cursor: cursor.execute(REGISTRATION_METRICS_SQL) <IF_STMT> for row in cursor.fetchall(): logger.info(encode_row(row)) else: write_raw_data(cursor=cursor, filename=filename)",if dry_run:
"def cat(tensors, dim=0): assert isinstance(tensors, list), 'input to cat must be a list' if len(tensors) == 1: return tensors[0] from .autograd_cryptensor import AutogradCrypTensor if any((isinstance(t, AutogradCrypTensor) for t in tensors)): <IF_STMT> tensors[0] = AutogradCrypTensor(tensors[0], requires_grad=False) return tensors[0].cat(*tensors[1:], dim=dim) else: return get_default_backend().cat(tensors, dim=dim)","if not isinstance(tensors[0], AutogradCrypTensor):"
"def is_installed(self, dlc_title='') -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info('version', dlc_title) installed = True if dlc_version else False if not installed: status = self.legacy_get_dlc_status(dlc_title) installed = True if status in ['installed', 'updatable'] else False el<IF_STMT> installed = True return installed",if self.install_dir and os.path.exists(self.install_dir):
"def on_copy(self): source_objects = self.__getSelection() for source in source_objects: <IF_STMT> new_obj = model.Phrase('', '') else: new_obj = model.Script('', '') new_obj.copy(source) self.cutCopiedItems.append(new_obj)","if isinstance(source, model.Phrase):"
"def FetchFn(type_name): """"""Fetches all hunt results of a given type."""""" offset = 0 while True: results = data_store.REL_DB.ReadHuntResults(hunt_id, offset=offset, count=self._RESULTS_PAGE_SIZE, with_type=type_name) <IF_STMT> break for r in results: msg = r.AsLegacyGrrMessage() msg.source_urn = source_urn yield msg offset += self._RESULTS_PAGE_SIZE",if not results:
"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: <IF_STMT> return 'TINYBLOB' if length <= self.LENGTH_LIMIT_BLOB: return 'BLOB' if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_TINYBLOB:
"def decode(cls, data): while data: length, atype = unpack(cls.Header.PACK, data[:cls.Header.LEN]) <IF_STMT> raise AttributesError('Buffer underrun %d < %d' % (len(data), length)) payload = data[cls.Header.LEN:length] yield (atype, payload) data = data[int((length + 3) / 4) * 4:]",if len(data) < length:
"def test_join_diffs(db, series_of_diffs, expected): diffs = [] for changes in series_of_diffs: tracker = DBDiffTracker() for key, val in changes.items(): <IF_STMT> del tracker[key] else: tracker[key] = val diffs.append(tracker.diff()) DBDiff.join(diffs).apply_to(db) assert db == expected",if val is None:
"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' elif col == BARRIER: tmp += '%' elif col == FOOD: tmp += '*' <IF_STMT> tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == UNSEEN:
"def _report_error(self, completion_routine, response=None, message=None): if response: if not response.ok: status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: <IF_STMT> return completion_routine(status) raise IOError(response.text) elif completion_routine: completion_routine(status) return location.Status(200, response.content)",if completion_routine:
"def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None): """"""Yields examples."""""" with tf.io.gfile.GFile(src_path) as f_d, tf.io.gfile.GFile(tgt_path) as f_s: for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)): <IF_STMT> yield (i, {_DOCUMENT: doc_text.strip().replace('<unk>', 'UNK'), _SUMMARY: sum_text.strip().replace('<unk>', 'UNK')}) else: yield (i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()})",if replace_unk:
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') <IF_STMT> if '\n' in text: text = text.replace('\n', '<br>') return text",if newline:
"def _handle_url_click(self, event): url = _extract_click_text(self.info_text, event, 'url') if url is not None: <IF_STMT> import webbrowser webbrowser.open(url) elif os.path.sep in url: os.makedirs(url, exist_ok=True) open_path_in_system_file_manager(url) else: self._start_show_package_info(url)",if url.startswith('http:') or url.startswith('https:'):
"def SConsignFile(self, name='.sconsign', dbm_module=None): if name is not None: name = self.subst(name) <IF_STMT> name = os.path.join(str(self.fs.SConstruct_dir), name) if name: name = os.path.normpath(name) sconsign_dir = os.path.dirname(name) if sconsign_dir and (not os.path.exists(sconsign_dir)): self.Execute(SCons.Defaults.Mkdir(sconsign_dir)) SCons.SConsign.File(name, dbm_module)",if not os.path.isabs(name):
"def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None: super().on_train_start(trainer, pl_module) submodule_dict = dict(pl_module.named_modules()) self._hook_handles = [] for name in self._get_submodule_names(pl_module): <IF_STMT> rank_zero_warn(f'{name} is not a valid identifier for a submodule in {pl_module.__class__.__name__}, skipping this key.') continue handle = self._register_hook(name, submodule_dict[name]) self._hook_handles.append(handle)",if name not in submodule_dict:
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): super().validate_configuration(configuration) if configuration is None: configuration = self.configuration try: assert 'value_set' in configuration.kwargs, 'value_set is required' assert isinstance(configuration.kwargs['value_set'], (list, set, dict)), 'value_set must be a list or a set' <IF_STMT> assert '$PARAMETER' in configuration.kwargs['value_set'], 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key.' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True","if isinstance(configuration.kwargs['value_set'], dict):"
"def check_refcounts(expected, timeout=10): start = time.time() while True: try: _check_refcounts(expected) break except AssertionError as e: <IF_STMT> raise e else: time.sleep(0.1)",if time.time() - start > timeout:
"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() <IF_STMT> break if prog.match(line): text = line[len(key) + 1:] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",if not line:
def _is_perf_file(file_path): f = get_file(file_path) for line in f: <IF_STMT> continue r = event_regexp.search(line) if r: f.close() return True f.close() return False,if line[0] == '#':
"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for page, soup in soups.items(): for a in soup.find_all('a'): <IF_STMT> continue pantsref = a['pantsref'] if pantsref not in precomputed.pantsref: raise TaskError(f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it') a['href'] = rel_href(page, precomputed.pantsref[pantsref])",if not a.has_attr('pantsref'):
"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: <IF_STMT> self.query_id = query.query_id elif self.query_id != query.query_id: raise ValueError('query in list must be same query_id')",if self.query_id == -1:
"def _draw_number(screen, x_offset, y_offset, number, token=Token.Clock, transparent=False): """"""Write number at position."""""" fg = Char(' ', token) bg = Char(' ', Token) for y, row in enumerate(_numbers[number]): screen_row = screen.data_buffer[y + y_offset] for x, n in enumerate(row): <IF_STMT> screen_row[x + x_offset] = fg elif not transparent: screen_row[x + x_offset] = bg",if n == '#':
"def init(self): self.sock.setblocking(True) if self.parser is None: <IF_STMT> self.sock = ssl.wrap_socket(self.sock, server_side=True, **self.cfg.ssl_options) self.parser = http.RequestParser(self.cfg, self.sock)",if self.cfg.is_ssl:
"def intersect_face(pt): nonlocal vis_faces2D for f, vs in vis_faces2D: v0 = vs[0] for v1, v2 in iter_pairs(vs[1:], False): <IF_STMT> return f return None","if intersect_point_tri_2d(pt, v0, v1, v2):"
"def IMPORTFROM(self, node): if node.module == '__future__': if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: <IF_STMT> self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if alias.name == '*':
"def PyObject_Bytes(obj): if type(obj) == bytes: return obj if hasattr(obj, '__bytes__'): res = obj.__bytes__() <IF_STMT> raise TypeError('__bytes__ returned non-bytes (type %s)' % type(res).__name__) return PyBytes_FromObject(obj)","if not isinstance(res, bytes):"
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) <IF_STMT> return self.current_provider.on_file(query)",elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:
"def remove(self, name): for s in [self.__storage(self.__category), self.__storage(None)]: for i, b in enumerate(s): <IF_STMT> del s[i] if b.persistent: self.__save() return raise KeyError(name)",if b.name == name:
"def _wrapper(data, axis=None, keepdims=False): if not keepdims: return func(data, axis=axis) else: <IF_STMT> axis = axis if isinstance(axis, int) else axis[0] out_shape = list(data.shape) out_shape[axis] = 1 else: out_shape = [1 for _ in range(len(data.shape))] return func(data, axis=axis).reshape(out_shape)",if axis is not None:
"def authn_info(self): res = [] for astat in self.assertion.authn_statement: context = astat.authn_context try: authn_instant = astat.authn_instant except AttributeError: authn_instant = '' <IF_STMT> try: aclass = context.authn_context_class_ref.text except AttributeError: aclass = '' try: authn_auth = [a.text for a in context.authenticating_authority] except AttributeError: authn_auth = [] res.append((aclass, authn_auth, authn_instant)) return res",if context:
"def _persist_metadata(self, dirname, filename): metadata_path = '{0}/{1}.json'.format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: <IF_STMT> self.merge_json({'GraphImages': self.posts}, metadata_path) else: self.save_json({'GraphImages': self.posts}, metadata_path) if self.stories: if self.latest: self.merge_json({'GraphStories': self.stories}, metadata_path) else: self.save_json({'GraphStories': self.stories}, metadata_path)",if self.latest:
"def update_record_image_detail(input_image_record, updated_image_detail, session=None): if not session: session = db.Session image_record = {} image_record.update(input_image_record) image_record.pop('created_at', None) image_record.pop('last_updated', None) if image_record['image_type'] == 'docker': for tag_record in updated_image_detail: <IF_STMT> image_record['image_detail'].append(tag_record) return update_record(image_record, session=session) return image_record",if tag_record not in image_record['image_detail']:
"def backup(self): for ds in [('activedirectory', 'AD'), ('ldap', 'LDAP'), ('nis', 'NIS')]: <IF_STMT> try: ds_cache = self.middleware.call_sync('cache.get', f'{ds[1]}_cache') with open(f'/var/db/system/.{ds[1]}_cache_backup', 'wb') as f: pickle.dump(ds_cache, f) except KeyError: self.logger.debug('No cache exists for directory service [%s].', ds[0])",if self.middleware.call_sync(f'{ds[0]}.config')['enable']:
"def parse_setup_cfg(self): if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: if six.PY2: contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) <IF_STMT> return {} return parsed return {}",if not parsed:
"def parts(): for l in lists.leaves: head_name = l.get_head_name() <IF_STMT> yield l.leaves elif head_name != 'System`Missing': raise MessageException('Catenate', 'invrp', l)",if head_name == 'System`List':
"def _get_callback_and_order(self, hook): if callable(hook): return (hook, None) elif isinstance(hook, tuple) and len(hook) == 2: callback, order = hook <IF_STMT> raise ValueError('Hook callback is not a callable') try: int(order) except ValueError: raise ValueError('Hook order is not a number') return (callback, order) else: raise ValueError('Invalid hook definition, neither a callable nor a 2-tuple (callback, order): {!r}'.format(hook))",if not callable(callback):
"def _resize_masks(self, results): """"""Resize masks with ``results['scale']``"""""" for key in results.get('mask_fields', []): if results[key] is None: continue <IF_STMT> results[key] = results[key].rescale(results['scale']) else: results[key] = results[key].resize(results['img_shape'][:2])",if self.keep_ratio:
"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) if not c.isVisible(): continue <IF_STMT> nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",if c.getYAxis() == Y_AXIS:
"def _check_token(self): if settings.app.sso_client_cache and self.server_auth_token: doc = self.sso_client_cache_collection.find_one({'user_id': self.user.id, 'server_id': self.server.id, 'device_id': self.device_id, 'device_name': self.device_name, 'auth_token': self.server_auth_token}) <IF_STMT> self.has_token = True",if doc:
"def parse_header(plyfile, ext): line = [] properties = [] num_points = None while b'end_header' not in line and line != b'': line = plyfile.readline() <IF_STMT> line = line.split() num_points = int(line[2]) elif b'property' in line: line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return (num_points, properties)",if b'element' in line:
"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = (self.pyflakes_enabled, self.pep8_enabled) for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() <IF_STMT> if current_finfo is not finfo: finfo.run_code_analysis(run_pyflakes, run_pep8)",if (run_pyflakes or run_pep8) and current_finfo is not None:
"def __modules(self): raw_output = self.__module_avail_output().decode('utf-8') for line in StringIO(raw_output): line = line and line.strip() <IF_STMT> continue line_modules = line.split() for module in line_modules: if module.endswith(self.default_indicator): module = module[0:-len(self.default_indicator)].strip() module_parts = module.split('/') module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield (module_name, module_version)",if not line or line.startswith('-'):
"def _set_trailing_size(self, size): if self.is_free(): next_chunk = self.next_chunk() <IF_STMT> self.state.memory.store(next_chunk.base, size, self.state.arch.bytes)",if next_chunk is not None:
"def _execute_for_all_tables(self, app, bind, operation, skip_tables=False): app = self.get_app(app) if bind == '__all__': binds = [None] + list(app.config.get('SQLALCHEMY_BINDS') or ()) elif isinstance(bind, string_types) or bind is None: binds = [bind] else: binds = bind for bind in binds: extra = {} <IF_STMT> tables = self.get_tables_for_bind(bind) extra['tables'] = tables op = getattr(self.Model.metadata, operation) op(bind=self.get_engine(app, bind), **extra)",if not skip_tables:
"def getFileName(): extension = '.json' file = '%s-stats' % self.clusterName counter = 0 while True: suffix = str(counter).zfill(3) + extension fullName = os.path.join(self.statsPath, file + suffix) <IF_STMT> return fullName counter += 1",if not os.path.exists(fullName):
def logic(): if goRight == ACTIVE: dir.next = DirType.RIGHT run.next = True elif goLeft == ACTIVE: dir.next = DirType.LEFT run.next = True if stop == ACTIVE: run.next = False if run: <IF_STMT> q.next[4:1] = q[3:] q.next[0] = not q[3] else: q.next[3:] = q[4:1] q.next[3] = not q[0],if dir == DirType.LEFT:
"def test_broadcast(self): """"""Test example broadcast functionality."""""" self.create_lang_connection('1000000000', 'en') self.create_lang_connection('1000000001', 'en') self.create_lang_connection('1000000002', 'en') self.create_lang_connection('1000000003', 'es') self.create_lang_connection('1000000004', 'es') app.lang_broadcast() self.assertEqual(2, len(self.outbound)) for message in self.outbound: if message.text == 'hello': self.assertEqual(3, len(message.connections)) <IF_STMT> self.assertEqual(2, len(message.connections))",elif message.text == 'hola':
"def get_ovf_env(dirname): env_names = ('ovf-env.xml', 'ovf_env.xml', 'OVF_ENV.XML', 'OVF-ENV.XML') for fname in env_names: full_fn = os.path.join(dirname, fname) <IF_STMT> try: contents = util.load_file(full_fn) return (fname, contents) except Exception: util.logexc(LOG, 'Failed loading ovf file %s', full_fn) return (None, False)",if os.path.isfile(full_fn):
"def _calc_offsets_children(self, offset, is_last): if self.elems: elem_last = self.elems[-1] for elem in self.elems: offset = elem._calc_offsets(offset, elem is elem_last) offset += _BLOCK_SENTINEL_LENGTH elif not self.props or self.id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL: <IF_STMT> offset += _BLOCK_SENTINEL_LENGTH return offset",if not is_last:
"def publish_state(cls, payload, state): try: <IF_STMT> if state == action_constants.LIVEACTION_STATUS_REQUESTED: cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)","if isinstance(payload, LiveActionDB):"
"def log_predictive_density(self, x_test, y_test, Y_metadata=None): if isinstance(x_test, list): x_test, y_test, ind = util.multioutput.build_XY(x_test, y_test) <IF_STMT> Y_metadata = {'output_index': ind, 'trials': np.ones(ind.shape)} return super(MultioutputGP, self).log_predictive_density(x_test, y_test, Y_metadata)",if Y_metadata is None:
"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: <IF_STMT> break else: if m in candidates: candidates.remove(m) candidates.append(m) return candidates","if issubclass(n, m) and m is not n:"
"def apply(self, operations, rotations=None, **kwargs): rotations = rotations or [] for i, operation in enumerate(operations): <IF_STMT> raise DeviceError('Operation {} cannot be used after other Operations have already been applied on a {} device.'.format(operation.name, self.short_name)) for operation in operations: self._apply_operation(operation) self._pre_rotated_state = self._state for operation in rotations: self._apply_operation(operation)","if i > 0 and isinstance(operation, (QubitStateVector, BasisState)):"
"def __str__(self): txt = str(self._called) if self.call_gas or self.call_value: gas = f'gas: {self.call_gas}' if self.call_gas else '' value = f'value: {self.call_value}' if self.call_value else '' salt = f'salt: {self.call_salt}' if self.call_salt else '' <IF_STMT> options = [gas, value, salt] txt += '{' + ','.join([o for o in options if o != '']) + '}' return txt + '(' + ','.join([str(a) for a in self._arguments]) + ')'",if gas or value or salt:
"def pop(self): """"""Pop a nonterminal.  (Internal)"""""" popdfa, popstate, popnode = self.stack.pop() newnode = self.convert(self.grammar, popnode) if newnode is not None: <IF_STMT> dfa, state, node = self.stack[-1] node.children.append(newnode) else: self.rootnode = newnode",if self.stack:
"def pollpacket(self, wait): self._stage0() if len(self.buffer) < self.bufneed: r, w, x = select.select([self.sock.fileno()], [], [], wait) <IF_STMT> return None try: s = self.sock.recv(BUFSIZE) except socket.error: raise EOFError if len(s) == 0: raise EOFError self.buffer += s self._stage0() return self._stage1()",if len(r) == 0:
"def increaseToolReach(self): if self.draggingFace is not None: d = (1, -1)[self.draggingFace & 1] <IF_STMT> d = -d self.draggingY += d x, y, z = self.editor.mainViewport.cameraPosition pos = [x, y, z] pos[self.draggingFace >> 1] += d self.editor.mainViewport.cameraPosition = tuple(pos) else: self.cloneCameraDistance = self.editor._incrementReach(self.cloneCameraDistance) return True",if self.draggingFace >> 1 != 1:
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True <IF_STMT> selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if remove and (not add):
"def __init__(self, *args, **kwargs): super(ProjectForm, self).__init__(*args, **kwargs) if self.instance.id: <IF_STMT> self.fields['localfiletype'].widget.attrs['disabled'] = True self.fields['localfiletype'].required = False if self.instance.treestyle != 'auto' and self.instance.translationproject_set.count() and (self.instance.treestyle == self.instance._detect_treestyle()): self.fields['treestyle'].widget.attrs['disabled'] = True self.fields['treestyle'].required = False",if Store.objects.filter(translation_project__project=self.instance).count():
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): <IF_STMT> raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if return_type is str:
"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) if len(inst) > count: count = len(inst) toReturn.append(gadget) added = True <IF_STMT> callback(gadget, added, float(i + 1) / len_gadgets) added = False return toReturn",if callback:
"def send_all(self, data: bytes): with self._conflict_detector: <IF_STMT> raise _core.ClosedResourceError('this pipe is already closed') if not data: await _core.checkpoint() return try: written = await _core.write_overlapped(self._handle_holder.handle, data) except BrokenPipeError as ex: raise _core.BrokenResourceError from ex assert written == len(data)",if self._handle_holder.closed:
"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': <IF_STMT> value = self.sv_get()[0][0] print('V', value) if isinstance(value, int): param_node.selected_mode = 'int' param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = 'float' param_node.float_ = value",if self.use_prop or self.get_prop_name():
"def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map): active_inst_idx_list = [] for inst_idx, inst_position in inst_idx_to_position_map.items(): is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position]) <IF_STMT> active_inst_idx_list += [inst_idx] return active_inst_idx_list",if not is_inst_complete:
"def compare_member_req_resp_without_key(self, request, response): for user_response in resp_json(response)['data']: for user_request in request: <IF_STMT> assert user_request['role'] == user_response['role']",if user_request['user_id'] == user_response['user_id']:
"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): if name.endswith('.py'): self.module_names.add(name[:-3]) <IF_STMT> self.module_names.add(name)",elif '.' not in name:
"def _read_filter(self, data): if data: if self.expected_inner_sha256: self.inner_sha.update(data) <IF_STMT> self.inner_md5.update(data) return data",if self.expected_inner_md5sum:
"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) <IF_STMT> break if s.consume_re(_newline_esc_re): pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return ''.join(res)",if not s.consume('\\'):
"def process_response(self, request, response): if response.status_code == 404 and request.path_info.endswith('/') and (not is_valid_path(request.path_info)) and is_valid_path(request.path_info[:-1]): newurl = request.path[:-1] <IF_STMT> with safe_query_string(request): newurl += '?' + request.META.get('QUERY_STRING', '') return HttpResponsePermanentRedirect(newurl) else: return response",if request.GET:
"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] <IF_STMT> k = dumps(k) if Types.KEYS not in obj: obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj","if not (isinstance(k, str) or isinstance(k, unicode)):"
"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True <IF_STMT> status = 'computed, ' if self.error() is None: if self.value() is self: status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.is_computed():
"def allocate_network(ipv='ipv4'): global dtcd_uuid global network_pool global allocations network = None try: cx = httplib.HTTPConnection('localhost:7623') cx.request('POST', '/v1/network/%s/' % ipv, body=dtcd_uuid) resp = cx.getresponse() <IF_STMT> network = netaddr.IPNetwork(resp.read().decode('utf-8')) cx.close() except Exception: pass if network is None: network = network_pool[ipv].pop() allocations[network] = True return network",if resp.status == 200:
def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith(' '): ind += 1 else: if start < ind: ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: <IF_STMT> lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if ':' in line and len(line) > 0:
"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: <IF_STMT> print('killing %s' % member) proc = hosts[member]['proc'] if sys.platform in ('win32', 'cygwin'): os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print('%s already dead?' % member)",if ha_tools_debug:
"def check(self): for path in self.paths: response = self.http_request(method='GET', path=path) <IF_STMT> continue if any(map(lambda x: x in response.text, ['report.db.server.name', 'report.db.server.sa.pass', 'report.db.server.user.pass'])): self.valid = path return True return False",if response is None:
"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) if r.ok: run_logs = r.json()['data']['records'] result.extend([i['logs'][0]['stats']['id'] for i in run_logs]) last_date = r.json()['data']['lastTimestamp'] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f'pares keep ids data since {since_time}') time.sleep(1) <IF_STMT> break return result",if not last_date:
"def button_press_cb(self, tdw, event): self._update_zone_and_cursors(tdw, event.x, event.y) if self._zone in (_EditZone.CREATE_FRAME, _EditZone.REMOVE_FRAME): button = event.button <IF_STMT> self._click_info = (button, self._zone) return False return super(FrameEditMode, self).button_press_cb(tdw, event)",if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS:
"def first_timestep(): assignment = self.has_previous.assign(value=tf_util.constant(value=True, dtype='bool'), read_value=False) with tf.control_dependencies(control_inputs=(assignment,)): <IF_STMT> current = x else: current = tf.expand_dims(input=x, axis=self.axis + 1) multiples = tuple((self.length if dims == self.axis + 1 else 1 for dims in range(self.output_spec().rank + 1))) return tf.tile(input=current, multiples=multiples)",if self.concatenate:
"def main() -> None: onefuzz = Onefuzz() jobs = onefuzz.jobs.list() for job in jobs: print('job:', str(job.job_id)[:8], ':'.join([job.config.project, job.config.name, job.config.build])) for task in onefuzz.tasks.list(job_id=job.job_id): <IF_STMT> continue print('', str(task.task_id)[:8], task.config.task.type, task.config.task.target_exe)","if task.state in ['stopped', 'stopping']:"
"def update_stack(self, full_name, template_url, parameters, tags): """"""Updates an existing stack in CloudFormation."""""" try: logger.info('Attempting to update stack %s.', full_name) self.conn.cloudformation.update_stack(full_name, template_url=template_url, parameters=parameters, tags=tags, capabilities=['CAPABILITY_IAM']) return SUBMITTED except BotoServerError as e: <IF_STMT> logger.info('Stack %s did not change, not updating.', full_name) return SKIPPED raise",if 'No updates are to be performed.' in e.message:
"def header_tag_files(env, files, legal_header, script_files=False): """"""Apply the legal_header to the list of files"""""" try: import apply_legal_header except: xbc.cdie('XED ERROR: mfile.py could not find scripts directory') for g in files: print('G: ', g) for f in mbuild.glob(g): print('F: ', f) <IF_STMT> apply_legal_header.apply_header_to_data_file(legal_header, f) else: apply_legal_header.apply_header_to_source_file(legal_header, f)",if script_files:
"def cleanDataCmd(cmd): newcmd = 'AbracadabrA ** <?php ' if cmd[:6] != 'php://': if reverseConn not in cmd: cmds = cmd.split('&') for c in cmds: <IF_STMT> newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += '?> **' return newcmd",if len(c) > 0:
"def test_form(self): n_qubits = 6 random_operator = get_fermion_operator(random_interaction_operator(n_qubits)) chemist_operator = chemist_ordered(random_operator) for term, _ in chemist_operator.terms.items(): <IF_STMT> pass else: self.assertTrue(term[0][1]) self.assertTrue(term[2][1]) self.assertFalse(term[1][1]) self.assertFalse(term[3][1]) self.assertTrue(term[0][0] > term[2][0]) self.assertTrue(term[1][0] > term[3][0])",if len(term) == 2 or not len(term):
"def do(server, handler, config, modargs): data = [] clients = server.get_clients(handler.default_filter) if not clients: return for client in clients: tags = config.tags(client.node()) <IF_STMT> tags.remove(*modargs.remove) if modargs.add: tags.add(*modargs.add) data.append({'ID': client.node(), 'TAGS': tags}) config.save(project=modargs.write_project, user=modargs.write_user) handler.display(Table(data))",if modargs.remove:
"def validate(self): if self.data.get('state') == 'enabled': <IF_STMT> raise PolicyValidationError('redshift logging enablement requires `bucket` and `prefix` specification on %s' % (self.manager.data,)) return self",if 'bucket' not in self.data:
"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return exists, contents = read_sysconfig_file(fn) updated_am = 0 for k, v in adjustments.items(): if v is None: continue v = str(v) if len(v) == 0 and (not allow_empty): continue contents[k] = v updated_am += 1 if updated_am: lines = [str(contents)] <IF_STMT> lines.insert(0, util.make_header()) util.write_file(fn, '\n'.join(lines) + '\n', 420)",if not exists:
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, 'Description'): if desc.getAttributeNS(RDF_NAMESPACE, 'about') == aboutUri: attr = desc.getAttributeNodeNS(namespace, name) <IF_STMT> yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",if attr != None:
"def get_store_name_from_connection_string(connection_string): if is_valid_connection_string(connection_string): segments = dict((seg.split('=', 1) for seg in connection_string.split(';'))) endpoint = segments.get('Endpoint') <IF_STMT> return endpoint.split('//')[1].split('.')[0] return None",if endpoint:
"def insertLoopTemplate(self, layout): col = layout.column(align=True) for socket in self.activeNode.outputs: <IF_STMT> props = col.operator('an.insert_loop_for_iterator', text='Loop through {}'.format(repr(socket.getDisplayedName())), icon='MOD_ARRAY') props.nodeIdentifier = self.activeNode.identifier props.socketIndex = socket.getIndex()",if not socket.hide and isList(socket.bl_idname):
"def do_task(self, task): self.running_task += 1 result = (yield gen.Task(self.fetcher.fetch, task)) type, task, response = result.args self.processor.on_task(task, response) while not self.processor.inqueue.empty(): _task, _response = self.processor.inqueue.get() self.processor.on_task(_task, _response) while not self.processor.result_queue.empty(): _task, _result = self.processor.result_queue.get() <IF_STMT> self.result_worker.on_result(_task, _result) self.running_task -= 1",if self.result_worker:
"def _parse_config_result(data): command_list = ' ; '.join([x.strip() for x in data[0]]) config_result = data[1] if isinstance(config_result, list): result = '' <IF_STMT> for key in config_result[0]: result += config_result[0][key] config_result = result else: config_result = config_result[0] return [command_list, config_result]","if isinstance(config_result[0], dict):"
"def load_api_handler(self, mod_name): for name, hdl in API_HANDLERS: name = name.lower() <IF_STMT> handler = self.mods.get(name) if not handler: handler = hdl(self.emu) self.mods.update({name: handler}) return handler return None",if mod_name and name == mod_name.lower():
def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: proc = PipelineProcess.objects.get(id=proc_id) if not proc.is_alive or proc.is_frozen: continue for dr in self.doctors: <IF_STMT> dr.cure(proc) break,if dr.confirm(proc):
"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError('You can either use {} with one positional argument or with keyword arguments, not both.'.format(cls.__name__)) <IF_STMT> return super().__new__(cls) if isinstance(args[0], cls): return cls return super().__new__(cls, *args, **kwargs)",if not args[0]:
"def __lt__(self, other): try: A, B = (self[0], other[0]) <IF_STMT> if A == B: return self[2] < other[2] return A < B return self[1] < other[1] except IndexError: return NotImplemented",if A and B:
"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): if str.lower(key) == str.lower(resource_provider): <IF_STMT> return GeneralPrivateEndpointClient(key, value['api_version'], value['support_list_or_not'], value['resource_get_api_version']) return value() raise CLIError('Resource type must be one of {}'.format(', '.join(rp_mapping.keys())))","if isinstance(value, dict):"
"def test_progressbar_format_pos(runner, pos, length): with _create_progress(length, length_known=length != 0, pos=pos) as progress: result = progress.format_pos() <IF_STMT> assert result == f'{pos}/{length}' else: assert result == str(pos)",if progress.length_known:
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue <IF_STMT> flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if not v.has_attribute(SplitTarget):
"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: <IF_STMT> tmp += '.' elif col == BARRIER: tmp += '%' elif col == FOOD: tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",if col == LAND:
"def reset(self): logger.debug('Arctic.reset()') with self._lock: if self.__conn is not None: self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): <IF_STMT> logger.debug('Library reset() %s' % l) l._reset()","if hasattr(l, '_reset') and callable(l._reset):"
"def add_cand_to_check(cands): for cand in cands: x = cand.creator <IF_STMT> continue if x not in fan_out: heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",if x is None:
"def on_task_modify(self, task, config): for entry in task.entries: <IF_STMT> size = entry['torrent'].size / 1024 / 1024 log.debug('%s size: %s MB' % (entry['title'], size)) entry['content_size'] = size",if 'torrent' in entry:
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] if self.do_manders: results += ['Manders'] if self.do_rwc: results += ['RWC'] <IF_STMT> results += ['Costes'] return results return []",if self.do_costes:
"def create_root(cls, site=None, title='Root', request=None, **kwargs): if not site: site = Site.objects.get_current() root_nodes = cls.objects.root_nodes().filter(site=site) if not root_nodes: article = Article() revision = ArticleRevision(title=title, **kwargs) <IF_STMT> revision.set_from_request(request) article.add_revision(revision, save=True) article.save() root = cls.objects.create(site=site, article=article) article.add_object_relation(root) else: root = root_nodes[0] return root",if request:
"def get(self, key): filename = self._get_filename(key) try: with open(filename, 'rb') as f: pickle_time = pickle.load(f) <IF_STMT> return pickle.load(f) else: os.remove(filename) return None except (IOError, OSError, pickle.PickleError): return None",if pickle_time == 0 or pickle_time >= time():
"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): <IF_STMT> self.add_body(message, value) elif name == 'EMAIL_ATTACHMENT': self.add_attachment(message, value) else: self.set_option(message, name, value, target) return message",if name == 'EMAIL_BODY':
"def updateVar(name, data, mode=None): if mode: <IF_STMT> core.config.globalVariables[name].append(data) elif mode == 'add': core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",if mode == 'append':
"def insert_errors(el, errors, form_id=None, form_index=None, error_class='error', error_creator=default_error_creator): el = _find_form(el, form_id=form_id, form_index=form_index) for name, error in errors.items(): <IF_STMT> continue for error_el, message in _find_elements_for_name(el, name, error): assert isinstance(message, (basestring, type(None), ElementBase)), 'Bad message: %r' % message _insert_error(error_el, message, error_class, error_creator)",if error is None:
"def read(self, item, recursive=False, sort=False): item = _normalize_path(item) if item in self._store: <IF_STMT> del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) else: return self._read_dir(item, recursive=recursive, sort=sort)",if item in self._expire_time and self._expire_time[item] < datetime.now():
"def _stash_splitter(states): keep, split = ([], []) if state_func is not None: for s in states: ns = state_func(s) if isinstance(ns, SimState): split.append(ns) <IF_STMT> split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return (keep, split)","elif isinstance(ns, (list, tuple, set)):"
"def run(self): while self.runflag: <IF_STMT> with self.lock: tasks = list(self.queue) self.queue.clear() while len(tasks) > 0: pathname, remotepath = tasks.pop(0) self.bcloud_app.upload_page.add_bg_task(pathname, remotepath) self.last = time() else: sleep(1)",if time() - self.last > 5 and self.qsize() > 0:
"def _append_patch(self, patch_dir, patch_files): for patch in patch_files: <IF_STMT> tmp = patch patch = {} for key in tmp.keys(): patch[os.path.join(patch_dir, key)] = tmp[key] self.patches.append(patch) else: self.patches.append(os.path.join(patch_dir, patch))",if type(patch) is dict:
"def __remote_port(self): port = 22 if self.git_has_remote: m = re.match('^(.*?)?@([^/:]*):?([0-9]+)?', self.git_remote.url) if m: <IF_STMT> port = m.group(3) return int(port)",if m.group(3):
"def _create_or_get_helper(self, infer_mode: Optional[bool]=None, **kwargs) -> Helper: prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) <IF_STMT> self._train_helper = helper elif not is_training and self._infer_helper is None: self._infer_helper = helper return helper",if is_training and self._train_helper is None:
"def flushChangeClassifications(self, schedulerid, less_than=None): if less_than is not None: classifications = self.classifications.setdefault(schedulerid, {}) for changeid in list(classifications): <IF_STMT> del classifications[changeid] else: self.classifications[schedulerid] = {} return defer.succeed(None)",if changeid < less_than:
"def pid_from_name(name): processes = [] for pid in os.listdir('/proc'): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) if name in pname: return pid <IF_STMT> return pid except: pass raise ProcessException('No process with such name: %s' % name)","if name in cmdline.split(' ', 1)[0]:"
"def spew(): seenUID = False start() for part in query: <IF_STMT> seenUID = True if part.type == 'body': yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, 'spew_' + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and (not seenUID): space() yield self.spew_uid(id, msg, write, flush) finish() flush()",if part.type == 'uid':
def rx(): while True: rx_i = rep.recv() <IF_STMT> rep.send(b'done') break rep.send(b'i'),if rx_i == b'1000':
"def test_search_incorrect_base_exception_1(self): self.connection_1c.bind() try: result = self.connection_1c.search('o=nonexistant', '(cn=*)', search_scope=SUBTREE, attributes=['cn', 'sn']) <IF_STMT> _, result = self.connection_1c.get_response(result) self.fail('exception not raised') except LDAPNoSuchObjectResult: pass",if not self.connection_1c.strategy.sync:
"def value_from_datadict(self, data, files, prefix): count = int(data['%s-count' % prefix]) values_with_indexes = [] for i in range(0, count): <IF_STMT> continue values_with_indexes.append((int(data['%s-%d-order' % (prefix, i)]), self.child_block.value_from_datadict(data, files, '%s-%d-value' % (prefix, i)))) values_with_indexes.sort() return [v for i, v in values_with_indexes]","if data['%s-%d-deleted' % (prefix, i)]:"
"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error('# channels not specified') if not self._sampwidth: raise Error('sample width not specified') <IF_STMT> raise Error('sampling rate not specified') self._write_header(datasize)",if not self._framerate:
def wait_til_ready(cls): while True: now = time.time() next_iteration = now // 1.0 + 1 <IF_STMT> break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0),if cls.connector.ready:
"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: <IF_STMT> if resp.match(key[:-1], val): break elif not resp.match(key, val): break else: actions[action] = opts return actions",if key[-1] == '!':
"def close(self, wait=True, abort=False): """"""Close the socket connection."""""" if not self.closed and (not self.closing): self.closing = True self.server._trigger_event('disconnect', self.sid, run_async=False) if not abort: self.send(packet.Packet(packet.CLOSE)) self.closed = True self.queue.put(None) <IF_STMT> self.queue.join()",if wait:
"def model_parse(self): for name, submodel in self.model.named_modules(): for op_type in SUPPORTED_OP_TYPE: <IF_STMT> self.target_layer[name] = submodel self.already_pruned[name] = 0","if isinstance(submodel, op_type):"
"def pack_identifier(self): """"""Return a combined identifier for the whole pack if this has more than one episode."""""" if self.id_type == 'ep': <IF_STMT> return 'S%02dE%02d-E%02d' % (self.season, self.episode, self.episode + self.episodes - 1) else: return self.identifier else: return self.identifier",if self.episodes > 1:
"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode('utf-8') <IF_STMT> v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",if type(v) == unicode:
"def _enable_contours_changed(self, value): """"""Turns on and off the contours."""""" if self.module_manager is None: return if value: self.actor.inputs = [self.contour] <IF_STMT> self.actor.mapper.scalar_mode = 'use_cell_data' else: self.actor.inputs = [self.grid_plane] self.actor.mapper.scalar_mode = 'default' self.render()",if self.contour.filled_contours:
"def _apply_abs_paths(data, script_dir): for flag_data in data.values(): <IF_STMT> continue default = flag_data.get('default') if not default or not isinstance(default, six.string_types) or os.path.sep not in default: continue abs_path = os.path.join(script_dir, default) if os.path.exists(abs_path): flag_data['default'] = abs_path","if not isinstance(flag_data, dict):"
"def button_release(self, mapper): self.pressed = False if self.waiting_task and self.active is None and (not self.action): mapper.cancel_task(self.waiting_task) self.waiting_task = None <IF_STMT> self.normalaction.button_press(mapper) mapper.schedule(0.02, self.normalaction.button_release) elif self.active: self.active.button_release(mapper) self.active = None",if self.normalaction:
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break <IF_STMT> p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",elif p:
"def status(self, name, error='No matching script logs found'): with self.script_lock: if self.script_running and self.script_running[1] == name: return self.script_running[1:] <IF_STMT> return self.script_last[1:] else: raise ValueError(error)",elif self.script_last and self.script_last[1] == name:
"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): <IF_STMT> curses.setupterm() if curses.tigetnum('colors') > 0: return True elif colorama: if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",if curses:
"def main(): configFilename = 'twitterbot.ini' if sys.argv[1:]: configFilename = sys.argv[1] try: <IF_STMT> raise Exception() load_config(configFilename) except Exception as e: print('Error while loading ini file %s' % configFilename, file=sys.stderr) print(e, file=sys.stderr) print(__doc__, file=sys.stderr) sys.exit(1) bot = TwitterBot(configFilename) return bot.run()",if not os.path.exists(configFilename):
def safe_to_kill(request): if os.path.exists(DRAIN_FILE): with open(DRAIN_FILE) as f: dt = datetime.datetime.fromtimestamp(float(f.read())) delta = datetime.datetime.now() - dt <IF_STMT> return Response(status_int=200) else: return Response(status_int=400) else: return Response(status_int=400),if delta.seconds > 2:
"def get_class_name(item): class_name, module_name = (None, None) for parent in reversed(item.listchain()): <IF_STMT> class_name = parent.name elif isinstance(parent, pytest.Module): module_name = parent.module.__name__ break if class_name and '.tasks.' not in module_name: return '{}.{}'.format(module_name, class_name) else: return module_name","if isinstance(parent, pytest.Class):"
"def getAllFitsLite(): fits = eos.db.getFitListLite() shipMap = {f.shipID: None for f in fits} for shipID in shipMap: ship = eos.db.getItem(shipID) <IF_STMT> shipMap[shipID] = (ship.name, ship.getShortName()) fitsToPurge = set() for fit in fits: try: fit.shipName, fit.shipNameShort = shipMap[fit.shipID] except (KeyError, TypeError): fitsToPurge.add(fit) for fit in fitsToPurge: fits.remove(fit) return fits",if ship is not None:
"def _process(self, event_data): self.machine.callbacks(self.machine.prepare_event, event_data) _LOGGER.debug('%sExecuted machine preparation callbacks before conditions.', self.machine.name) try: for trans in self.transitions[event_data.state.name]: event_data.transition = trans <IF_STMT> event_data.result = True break except Exception as err: event_data.error = err raise finally: self.machine.callbacks(self.machine.finalize_event, event_data) _LOGGER.debug('%sExecuted machine finalize callbacks', self.machine.name) return event_data.result",if trans.execute(event_data):
"def fetch_comments(self, force=False, limit=None): comments = [] if force is True or self.badges['comments'] > 0: query_params = {'filter': 'commentCard,copyCommentCard'} <IF_STMT> query_params['limit'] = limit comments = self.client.fetch_json('/cards/' + self.id + '/actions', query_params=query_params) return sorted(comments, key=lambda comment: comment['date']) return comments",if limit is not None:
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: <IF_STMT> continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if not self._is_expression():
"def _replace_home(x): if xp.ON_WINDOWS: home = builtins.__xonsh__.env['HOMEDRIVE'] + builtins.__xonsh__.env['HOMEPATH'][0] <IF_STMT> x = x.replace(home, '~', 1) if builtins.__xonsh__.env.get('FORCE_POSIX_PATHS'): x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env['HOME'] if x.startswith(home): x = x.replace(home, '~', 1) return x",if x.startswith(home):
"def project_review(plans): for plan in plans: print('Inspecting {} plan'.format(plan)) branches = get_branches_from_plan(plan) for branch in branches: build_results = get_results_from_branch(branch) for build in build_results: build_key = build.get('buildResultKey') or None print('Inspecting build - {}'.format(build_key)) <IF_STMT> for status in STATUS_CLEANED_RESULTS: remove_build_result(build_key=build_key, status=status)",if build_key:
"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x['description'] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in(('metadata', 'batch'), x) if batches: <IF_STMT> batches = [batches] for batch in batches: if batch in names: dups.add(batch) if len(dups) > 0: raise ValueError('Batch names must be unique from sample descriptions.\nClashing batch names: %s' % sorted(list(dups)))","if not isinstance(batches, (list, tuple)):"
"def _check_signal(self): """"""Checks if a signal was received and issues a message."""""" proc_signal = getattr(self.proc, 'signal', None) if proc_signal is None: return sig, core = proc_signal sig_str = SIGNAL_MESSAGES.get(sig) if sig_str: if core: sig_str += ' (core dumped)' print(sig_str, file=sys.stderr) <IF_STMT> self.errors += sig_str + '\n'",if self.errors is not None:
"def loadLabelFile(self, labelpath): labeldict = {} if not os.path.exists(labelpath): f = open(labelpath, 'w', encoding='utf-8') else: with open(labelpath, 'r', encoding='utf-8') as f: data = f.readlines() for each in data: file, label = each.split('\t') <IF_STMT> label = label.replace('false', 'False') label = label.replace('true', 'True') labeldict[file] = eval(label) else: labeldict[file] = [] return labeldict",if label:
"def exists_col_to_many(self, select_columns: List[str]) -> bool: for column in select_columns: <IF_STMT> root_relation = get_column_root_relation(column) if self.is_relation_many_to_many(root_relation) or self.is_relation_one_to_many(root_relation): return True return False",if is_column_dotted(column):
"def check_sequence_matches(seq, template): i = 0 for pattern in template: <IF_STMT> pattern = {pattern} got = set(seq[i:i + len(pattern)]) assert got == pattern i += len(got)","if not isinstance(pattern, set):"
"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue <IF_STMT> for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()","if hasattr(module, 'aliases'):"
"def result(): R, V = (rays, virtual_rays) if V is not None: <IF_STMT> V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() if len(V) != d - R.dim() or (R + V).dim() != d: raise ValueError('virtual rays must be linearly independent and with other rays span the ambient space.') return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if normalize:
"def communicate(self, _input=None, _timeout=None) -> Tuple[bytes, bytes]: if parse_args().print_commands: <IF_STMT> print_stderr(color_line('=> ', 14) + ' '.join((str(arg) for arg in self.args))) stdout, stderr = super().communicate(_input, _timeout) self.stdout_text = stdout.decode('utf-8') if stdout else None self.stderr_text = stderr.decode('utf-8') if stderr else None return (stdout, stderr)",if self.args != get_sudo_refresh_command():
"def convert(data): result = [] for d in data: <IF_STMT> result.append((d[0], None, d[1])) elif isinstance(d, basestring): result.append(d) return result","if isinstance(d, tuple) and len(d) == 2:"
"def validate(self, value): try: value = [datetime.datetime.strptime(range, '%Y-%m-%d %H:%M:%S') for range in value.split(' to ')] <IF_STMT> return True else: return False except ValueError: return False",if len(value) == 2 and value[0] <= value[1]:
"def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return for f in os.listdir(dirname): if f in ('.', '..'): continue path = dirname + os.sep + f <IF_STMT> rmdir(path) else: os.unlink(path) os.rmdir(dirname)",if os.path.isdir(path):
"def onCompletion(self, text): res = [] for l in text.split('\n'): if not l: continue l = l.split(':') <IF_STMT> continue res.append([l[0].strip(), l[1].strip()]) self.panel.setSlides(res)",if len(l) != 2:
def pytest_collection_modifyitems(items): for item in items: <IF_STMT> if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) if 'init' not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123)),if item.nodeid.startswith('tests/infer'):
"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): if name == 'EMAIL_BODY': self.add_body(message, value) <IF_STMT> self.add_attachment(message, value) else: self.set_option(message, name, value, target) return message",elif name == 'EMAIL_ATTACHMENT':
def extend_with_zeroes(b): try: for x in b: x = to_constant(x) <IF_STMT> yield x else: yield 0 for _ in range(32): yield 0 except Exception as e: return,"if isinstance(x, int):"
"def _start_cluster(*, cleanup_atexit=True): global _default_cluster if _default_cluster is None: cluster_addr = os.environ.get('EDGEDB_TEST_CLUSTER_ADDR') <IF_STMT> conn_spec = json.loads(cluster_addr) _default_cluster = edgedb_cluster.RunningCluster(**conn_spec) else: data_dir = os.environ.get('EDGEDB_TEST_DATA_DIR') _default_cluster = _init_cluster(data_dir=data_dir, cleanup_atexit=cleanup_atexit) return _default_cluster",if cluster_addr:
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() <IF_STMT> break line = list(enwik9_norm_transform([line]))[0] if line != ' ' and line != '': if line[0] == ' ': line = line[1:] f2.writelines(line + '\n')",if not line:
"def is_entirely_italic(line): style = subs.styles.get(line.style, SSAStyle.DEFAULT_STYLE) for fragment, sty in parse_tags(line.text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') <IF_STMT> return False return True",if not sty.italic and fragment and (not fragment.isspace()):
def __get_all_nodes(self): nodes = [] next_level = [self.__tree.get_root()] while len(next_level) != 0: cur_level = next_level nodes += next_level next_level = [] for cur_node in cur_level: children = cur_node.get_children() <IF_STMT> next_level += children return nodes,if children is not None:
"def _openvpn_stdout(self): while True: line = self.process.stdout.readline() if not line: <IF_STMT> return time.sleep(0.05) continue yield try: self.server.output.push_output(line) except: logger.exception('Failed to push vpn output', 'server', server_id=self.server.id) yield",if self.process.poll() is not None or self.is_interrupted():
"def payment_received_handler(event): if isinstance(event.message.action, types.MessageActionPaymentSentMe): payment: types.MessageActionPaymentSentMe = event.message.action if payment.payload.decode('UTF-8') == 'product A': await bot.send_message(event.message.from_id, 'Thank you for buying product A!') <IF_STMT> await bot.send_message(event.message.from_id, 'Thank you for buying product B!') raise events.StopPropagation",elif payment.payload.decode('UTF-8') == 'product B':
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer <IF_STMT> return LintProblem(token.start_mark.line + 1, next.start_mark.column, max_desc) elif min != -1 and spaces < min: return LintProblem(token.start_mark.line + 1, next.start_mark.column + 1, min_desc)",if max != -1 and spaces > max:
"def seek_to_block(self, pos): baseofs = 0 ofs = 0 for b in self.blocks: <IF_STMT> self.current_block = b break baseofs += b.compressed_size ofs += b.uncompressed_size else: self.current_block = None self.current_stream = BytesIO(b'') return self.current_block_start = ofs self.stream.seek(self.basepos + baseofs) buf = BytesIO(self.stream.read(self.current_block.compressed_size)) self.current_stream = self.current_block.decompress(buf)",if ofs + b.uncompressed_size > pos:
"def rewrite_hunks(hunks): deltas = (hunk.b_length - hunk.a_length for hunk in hunks) offsets = accumulate(deltas, initial=0) for hunk, offset in zip(hunks, offsets): new_b = hunk.a_start + offset if hunk_of_additions_only(hunk): new_b += 1 <IF_STMT> new_b -= 1 yield hunk._replace(b_start=new_b)",elif hunk_of_removals_only(hunk):
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): <IF_STMT> if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if len(q) == 1:
"def get_url(token, base_url): """"""Parse an <url> token."""""" if token.type == 'url': return _get_url_tuple(token.value, base_url) elif token.type == 'function': if token.name == 'attr': return check_attr_function(token, 'url') <IF_STMT> return _get_url_tuple(token.arguments[0].value, base_url)","elif token.name == 'url' and len(token.arguments) in (1, 2):"
"def read(self, count): if self.closed: return self.upstream.read(count) try: while len(self.upstream) < count: <IF_STMT> with self.buf_in: self.transport.downstream_recv(self.buf_in) else: break return self.upstream.read(count) except: logger.debug(traceback.format_exc())",if self.buf_in or self._poll_read(10):
"def get_timestamp_for_block(self, block_hash: HexBytes, max_tries: Optional[int]=10) -> int: counter = 0 block: AttributeDict = None if block_hash in self._block_cache.keys(): block = self._block_cache.get(block_hash) else: while block is None: <IF_STMT> raise ValueError(f'Block hash {block_hash.hex()} does not exist.') counter += 1 block = self._block_cache.get(block_hash) await asyncio.sleep(0.5) return block.get('timestamp')",if counter == max_tries:
"def reader(): batch_out = [] for video_name in self.video_list: video_idx = self.video_list.index(video_name) video_feat = self.load_file(video_name) batch_out.append((video_feat, video_idx)) <IF_STMT> yield batch_out batch_out = []",if len(batch_out) == self.batch_size:
"def cleanup(): gscript.message(_('Erasing temporary files...')) for temp_map, maptype in temp_maps: <IF_STMT> gscript.run_command('g.remove', flags='f', type=maptype, name=temp_map, quiet=True)","if gscript.find_file(temp_map, element=maptype)['name']:"
"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() <IF_STMT> self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) elif self._flow_type == REALTIME: try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",if self._has_stop_signal(raw_inputs):
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('mw', 'hi', 'corr', 'trunc'): sent += [self.handle_word(w) for w in child] <IF_STMT> sent.append(self.handle_word(child)) elif child.tag not in self.tags_to_ignore: raise ValueError('Unexpected element %s' % child.tag) return BNCSentence(elt.attrib['n'], sent)","elif child.tag in ('w', 'c'):"
"def bind_subscribers_to_graphql_type(self, graphql_type): for field, subscriber in self._subscribers.items(): <IF_STMT> raise ValueError('Field %s is not defined on type %s' % (field, self.name)) graphql_type.fields[field].subscribe = subscriber",if field not in graphql_type.fields:
"def _get_from_json(self, *, name, version): url = urljoin(self.url, posixpath.join(name, str(version), 'json')) async with aiohttp_session(auth=self.auth) as session: async with session.get(url) as response: <IF_STMT> raise PackageNotFoundError(package=name, url=url) response.raise_for_status() response = await response.json() dist = response['info']['requires_dist'] or [] if dist: return dist return await self._get_from_files(response['urls'])",if response.status == 404:
"def is_active(self): if not self.pk: log_level = get_setting('LOG_MISSING_SWITCHES') if log_level: logger.log(log_level, 'Switch %s not found', self.name) <IF_STMT> switch, _created = Switch.objects.get_or_create(name=self.name, defaults={'active': get_setting('SWITCH_DEFAULT')}) cache = get_cache() cache.set(self._cache_key(self.name), switch) return get_setting('SWITCH_DEFAULT') return self.active",if get_setting('CREATE_MISSING_SWITCHES'):
"def add_requirements(self, requirements): if self._legacy: self._legacy.add_requirements(requirements) else: run_requires = self._data.setdefault('run_requires', []) always = None for entry in run_requires: <IF_STMT> always = entry break if always is None: always = {'requires': requirements} run_requires.insert(0, always) else: rset = set(always['requires']) | set(requirements) always['requires'] = sorted(rset)",if 'environment' not in entry and 'extra' not in entry:
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] if check.message: message = f'{idx}. {check.message}' else: message = None example = cast(Case, check.example) display_example(example, check.name, message, result.seed) <IF_STMT> click.echo('\n')",if idx != len(checks):
"def __call__(self, frame: FrameType, event: str, arg: Any) -> 'CallTracer': code = frame.f_code if event not in SUPPORTED_EVENTS or code.co_name == 'trace_types' or (self.should_trace and (not self.should_trace(code))): return self try: if event == EVENT_CALL: self.handle_call(frame) <IF_STMT> self.handle_return(frame, arg) else: logger.error('Cannot handle event %s', event) except Exception: logger.exception('Failed collecting trace') return self",elif event == EVENT_RETURN:
"def get_maps(test): pages = set() for addr in test['pre']['memory'].keys(): pages.add(addr >> 12) for addr in test['pos']['memory'].keys(): pages.add(addr >> 12) maps = [] for p in sorted(pages): <IF_STMT> maps[-1] = (maps[-1][0], maps[-1][1] + 4096) else: maps.append((p << 12, 4096)) return maps",if len(maps) > 0 and maps[-1][0] + maps[-1][1] == p << 12:
"def process_rotate_aes_key(self): if hasattr(self.options, 'rotate_aes_key') and isinstance(self.options.rotate_aes_key, six.string_types): <IF_STMT> self.options.rotate_aes_key = True elif self.options.rotate_aes_key.lower() == 'false': self.options.rotate_aes_key = False",if self.options.rotate_aes_key.lower() == 'true':
"def apply_figure(self, figure): super(legend_text_legend, self).apply_figure(figure) properties = self.properties.copy() with suppress(KeyError): del properties['margin'] with suppress(KeyError): texts = figure._themeable['legend_text_legend'] for text in texts: <IF_STMT> text = text._text text.set(**properties)","if not hasattr(text, '_x'):"
"def tearDown(self): for i in range(len(self.tree) - 1, -1, -1): s = os.path.join(self.root, self.tree[i]) <IF_STMT> os.rmdir(s) else: os.remove(s) os.rmdir(self.root)",if not '.' in s:
"def _get_id(self, type, id): fields = id.split(':') if len(fields) >= 3: if type != fields[-2]: logger.warning('Expected id of type %s but found type %s %s', type, fields[-2], id) return fields[-1] fields = id.split('/') if len(fields) >= 3: itype = fields[-2] <IF_STMT> logger.warning('Expected id of type %s but found type %s %s', type, itype, id) return fields[-1].split('?')[0] return id",if type != itype:
"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: <IF_STMT> yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if matchSelf:
"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource('dynamodb').Table('Sphere11.Dev.ResourceLocks') results = table.scan() for r in results['Items']: if 'LockDate' in r: r['LockDate'] = datetime.fromtimestamp(r['LockDate']) <IF_STMT> r['RevisionDate'] = datetime.fromtimestamp(r['RevisionDate']) print(tabulate.tabulate(results['Items'], headers='keys', tablefmt='fancy_grid'))",if 'RevisionDate' in r:
"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True err_msg = 'T2T: skipped importing {num_missing} data_generators modules.' print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) if log_all: print('Did not import module: %s; Cause: %s' % (module, err_str)) <IF_STMT> print('From module %s' % module) raise err","if not _is_import_err_msg(err_str, module):"
"def find_needle(self, tree, focused=None): if isinstance(tree, list): for el in tree: res = self.find_needle(el, focused) <IF_STMT> return res elif isinstance(tree, dict): nodes = tree.get('nodes', []) + tree.get('floating_nodes', []) if focused: for node in nodes: if node['id'] == focused['id']: return tree elif tree['focused']: return tree return self.find_needle(nodes, focused) return {}",if res:
"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = ['pixel_longitude', 'pixel_latitude'] for var_name, val in self.file_content.items(): if isinstance(val, netCDF4.Variable): ds_info = {'file_type': self.filetype_info['file_type'], 'resolution': res} <IF_STMT> ds_info['coordinates'] = coordinates yield (DatasetID(name=var_name, resolution=res), ds_info)",if not self.is_geo:
"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() <IF_STMT> sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') if end_slash >= 0: sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",if test_path.lower().startswith(parent_path.lower()):
"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: if hasattr(o, 'profile'): del o.profile <IF_STMT> del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path if hasattr(o, 'services_config'): del o.services_config return vars(o) except Exception as e: return str(o)","if hasattr(o, 'credentials'):"
"def submit(self, fn, *args, **kwargs): with self._shutdown_lock: <IF_STMT> raise RuntimeError('cannot schedule new futures after shutdown') f = _base.Future() w = _WorkItem(f, fn, args, kwargs) self._work_queue.put(w) self._adjust_thread_count() return f",if self._shutdown:
"def __viewerKeyPress(viewer, event): view = viewer.view() if not isinstance(view, GafferSceneUI.SceneView): return False if event == __editSourceKeyPress: selectedPath = __sceneViewSelectedPath(view) <IF_STMT> __editSourceNode(view.getContext(), view['in'], selectedPath) return True elif event == __editTweaksKeyPress: selectedPath = __sceneViewSelectedPath(view) if selectedPath is not None: __editTweaksNode(view.getContext(), view['in'], selectedPath) return True",if selectedPath is not None:
"def _split_to_option_groups_and_paths(self, args): opt_groups = [] current = [] for arg in args: <IF_STMT> opts = self._arg_parser.parse_args(current)[0] opt_groups.append(opts) current = [] else: current.append(arg) if opt_groups: return (opt_groups, current) raise ValueError('Nothing to split')","if arg.replace('-', '') == '' and len(arg) >= 3:"
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): <IF_STMT> changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",if value:
"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) if target_pid == pid: return except OSError: e = sys.exc_info()[1] <IF_STMT> return time.sleep(0.05) assert False, 'wait_for_child() timed out'",if e.args[0] == errno.ECHILD:
"def _get_os_version_lsb_release(): try: output = subprocess.check_output('lsb_release -sri', shell=True) lines = output.strip().split() name, version = lines <IF_STMT> version = '' return (name, version) except: return _get_os_version_uname()",if version.lower() == 'rolling':
"def _check_snapshot_status_healthy(self, snapshot_uuid): status = '' try: while True: status, locked = self._get_snapshot_status(snapshot_uuid) <IF_STMT> break eventlet.sleep(2) except Exception: with excutils.save_and_reraise_exception(): LOG.exception('Failed to get snapshot status. [%s]', snapshot_uuid) LOG.debug('Lun [%(snapshot)s], status [%(status)s].', {'snapshot': snapshot_uuid, 'status': status}) return status == 'Healthy'",if not locked:
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 <IF_STMT> n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",if self.HasCloseButton():
"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: <IF_STMT> continue if not isinstance(key, bytes): key = text_type(key).encode(charset) if not isinstance(value, bytes): value = text_type(value).encode(charset) yield (_fast_url_quote_plus(key) + '=' + _fast_url_quote_plus(value))",if value is None:
"def get_response(self, exc_fmt=None): self.callback = None if __debug__: self.parent._log(3, '%s:%s.ready.wait' % (self.name, self.tag)) self.ready.wait() if self.aborted is not None: typ, val = self.aborted <IF_STMT> exc_fmt = '%s - %%s' % typ raise typ(exc_fmt % str(val)) return self.response",if exc_fmt is None:
"def extract_items(self): responses = self.fetch() items = [] for response in responses: page_key = response.meta.get('page_key') or response.url item = {'key': page_key, 'items': None, 'templates': None} extracted_items = [dict(i) for i in self.spider.parse(response) if not isinstance(i, Request)] <IF_STMT> item['items'] = extracted_items item['templates'] = [i['_template'] for i in extracted_items if i.get('_template')] items.append(item) return items",if extracted_items:
"def fit_one(self, x): for i, xi in x.items(): if self.with_centering: self.median[i].update(xi) <IF_STMT> self.iqr[i].update(xi) return self",if self.with_scaling:
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True <IF_STMT> left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",elif not self.word_boundary_char(text[left - 1]):
def _validate_duplicate_detection_history_time_window(namespace): if namespace.duplicate_detection_history_time_window: if iso8601pattern.match(namespace.duplicate_detection_history_time_window): pass <IF_STMT> pass else: raise CLIError('--duplicate-detection-history-time-window Value Error : {0} value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min'.format(namespace.duplicate_detection_history_time_window)),elif timedeltapattern.match(namespace.duplicate_detection_history_time_window):
"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') if end_slash >= 0: sub = sub[:end_slash] <IF_STMT> continue subkeys.append(sub) return subkeys",if not sub:
"def generator(self, data): <IF_STMT> silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: if var in silent_vars: continue yield (0, [int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val)])",if self._config.SILENT:
"def start_requests(self): if self.fail_before_yield: 1 / 0 for s in range(100): qargs = {'total': 10, 'seed': s} url = self.mockserver.url('/follow?%s') % urlencode(qargs, doseq=1) yield Request(url, meta={'seed': s}) <IF_STMT> 2 / 0 assert self.seedsseen, 'All start requests consumed before any download happened'",if self.fail_yielding:
"def populateGridlines(self): cTicks = self.getSystemCurve(self.ticksId) cGridlines = self.getSystemCurve(self.gridlinesId) cGridlines.clearPoints() nTicks = cTicks.getNPoints() for iTick in range(nTicks): <IF_STMT> p = cTicks.getPoint(iTick) cGridlines.addPoint(p.getX(), p.getY())",if self.hasGridlines and iTick % self.ticksPerGridline == 0:
"def handle_before_events(request, event_list): if not event_list: return '' if not hasattr(event_list, '__iter__'): project = event_list.project event_list = [event_list] else: projects = set((e.project for e in event_list)) <IF_STMT> project = projects.pop() else: project = None for plugin in plugins.for_project(project): safe_execute(plugin.before_events, request, event_list) return ''",if len(projects) == 1:
"def handle_parse_result(self, ctx, opts, args): if self.name in opts: <IF_STMT> self._raise_exclusive_error() if self.multiple and len(set(opts[self.name])) > 1: self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self.mutually_exclusive.intersection(opts):
"def current_word(cursor_offset, line): """"""the object.attribute.attribute just before or under the cursor"""""" pos = cursor_offset start = pos end = pos word = None for m in current_word_re.finditer(line): <IF_STMT> start = m.start(1) end = m.end(1) word = m.group(1) if word is None: return None return LinePart(start, end, word)",if m.start(1) < pos and m.end(1) >= pos:
"def query_to_script_path(path, query): if path != '*': script = os.path.join(path, query.split(' ')[0]) <IF_STMT> raise IOError(""Script '{}' not found in script directory"".format(query)) return os.path.join(path, query).split(' ') return query",if not os.path.exists(script):
"def expand(self, pbegin): identity_map = 'Identity' >> beam.Map(lambda x: x) if self._dataset_key.is_flattened_dataset_key(): <IF_STMT> return self._flat_pcollection | identity_map else: return list(self._pcollection_dict.values()) | 'FlattenAnalysisInputs' >> beam.Flatten(pipeline=pbegin.pipeline) else: return self._pcollection_dict[self._dataset_key] | identity_map",if self._flat_pcollection:
"def processCoords(coords): newcoords = deque() for x, y, z in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) if p not in box: continue nx, ny, nz = p <IF_STMT> level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords","if level.blockAt(nx, ny, nz) == 0:"
"def delete_byfilter(userId, remove=True, session=None, **dbfilter): if not session: session = db.Session ret = False results = session.query(ObjectStorageMetadata).filter_by(**dbfilter) if results: for result in results: <IF_STMT> session.delete(result) else: result.update({'record_state_key': 'to_delete', 'record_state_val': str(time.time())}) ret = True return ret",if remove:
"def fields(self, fields): fields_xml = '' for field in fields: field_dict = DEFAULT_FIELD.copy() field_dict.update(field) <IF_STMT> field_dict['required'] = 'true' fields_xml += FIELD_XML_TEMPLATE % field_dict + '\n' self.xml = force_unicode(force_unicode(self.xml).replace(u'<!-- REPLACE FIELDS -->', force_unicode(fields_xml)))",if self.unique_key_field == field['name']:
"def get_all_users(self, access_token, timeout=None): if timeout is None: timeout = DEFAULT_TIMEOUT headers = self.retrieve_header(access_token) try: response = await self.standard_request('get', '/walkoff/api/users', timeout=DEFAULT_TIMEOUT, headers=headers) <IF_STMT> resp = await response.json() return (resp, 'Success') else: return 'Invalid Credentials' except asyncio.CancelledError: return (False, 'TimedOut')",if response.status == 200:
"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] if value and row[0] == value: break <IF_STMT> idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",if idx == len(os_widget.get_model()) - 1:
"def translate_module_name(module: str, relative: int) -> Tuple[str, int]: for pkg in VENDOR_PACKAGES: for alt in ('six.moves', 'six'): substr = '{}.{}'.format(pkg, alt) if module.endswith('.' + substr) or (module == substr and relative): return (alt, 0) <IF_STMT> return (alt + '.' + module.partition('.' + substr + '.')[2], 0) return (module, relative)",if '.' + substr + '.' in module:
"def escape(m): all, tail = m.group(0, 1) assert all.startswith('\\') esc = simple_escapes.get(tail) if esc is not None: return esc if tail.startswith('x'): hexes = tail[1:] <IF_STMT> raise ValueError(""invalid hex string escape ('\\%s')"" % tail) try: i = int(hexes, 16) except ValueError: raise ValueError(""invalid hex string escape ('\\%s')"" % tail) else: try: i = int(tail, 8) except ValueError: raise ValueError(""invalid octal string escape ('\\%s')"" % tail) return chr(i)",if len(hexes) < 2:
"def __get_k8s_container_name(self, job_wrapper): raw_id = job_wrapper.job_destination.id if isinstance(raw_id, str): cleaned_id = re.sub('[^-a-z0-9]', '-', raw_id) <IF_STMT> cleaned_id = 'x%sx' % cleaned_id return cleaned_id return 'job-container'",if cleaned_id.startswith('-') or cleaned_id.endswith('-'):
"def _power_exact(y, xc, yc, xe): yc, ye = (y.int, y.exp) while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 if ye < 0: return None exponent = xe * 10 ** ye <IF_STMT> xc = exponent else: xc = 0 return 5",if y and xe:
"def lpush(key, *vals, **kwargs): ttl = kwargs.get('ttl') cap = kwargs.get('cap') if not ttl and (not cap): _client.lpush(key, *vals) else: pipe = _client.pipeline() pipe.lpush(key, *vals) <IF_STMT> pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",if cap:
"def render_headers(self) -> bytes: if not hasattr(self, '_headers'): parts = [b'Content-Disposition: form-data; ', format_form_param('name', self.name)] if self.filename: filename = format_form_param('filename', self.filename) parts.extend([b'; ', filename]) <IF_STMT> content_type = self.content_type.encode() parts.extend([b'\r\nContent-Type: ', content_type]) parts.append(b'\r\n\r\n') self._headers = b''.join(parts) return self._headers",if self.content_type is not None:
"def validate_custom_field_data(field_type: int, field_data: ProfileFieldData) -> None: try: <IF_STMT> if len(field_data) < 1: raise JsonableError(_('Field must have at least one choice.')) validate_choice_field_data(field_data) elif field_type == CustomProfileField.EXTERNAL_ACCOUNT: validate_external_account_field_data(field_data) except ValidationError as error: raise JsonableError(error.message)",if field_type == CustomProfileField.CHOICE:
"def get_data(self, path): """"""Gross hack to contort loader to deal w/ load_*()'s bad API."""""" if self.file and path == self.path: <IF_STMT> file = self.file else: self.file = file = open(self.path, 'r') with file: return file.read() else: return super().get_data(path)",if not self.file.closed:
"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) <IF_STMT> self.transfer_finished = True return if self._data_wrapper is not None: chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",if not chunk:
"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): <IF_STMT> line = line.split(' ')[1].strip('\'""\r\n') if not ('<' in line or line in dep): dep.append(line) return [i for i in dep if os.path.exists(i)]",if line.startswith('#include') or line.startswith('%include'):
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): """"""Add data to be displayed in the buffer."""""" self.values.extend(lines) if scroll_end: if not self.editing: self.start_display_at = len(self.values) - len(self._my_widgets) <IF_STMT> self.start_display_at = len(self.values) - len(self._my_widgets)",elif scroll_if_editing:
"def test_getline(self): with tokenize.open(self.file_name) as fp: for index, line in enumerate(fp): <IF_STMT> line += '\n' cached_line = linecache.getline(self.file_name, index + 1) self.assertEqual(line, cached_line)",if not line.endswith('\n'):
"def selectRow(self, rowNumber, highlight=None): if rowNumber == 'h': rowNumber = 0 else: rowNumber = int(rowNumber) + 1 if 1 > rowNumber >= len(self.cells) + 1: raise Exception('Invalid row number.') else: selected = self.cells[rowNumber][0].selected for cell in self.cells[rowNumber]: <IF_STMT> if selected: cell.deselect() else: cell.select() elif highlight: cell.mouseEnter() else: cell.mouseLeave()",if highlight is None:
"def put(self, session): with sess_lock: self.parent.put(session) for sp in self.skip_paths: if request.path.startswith(sp): return <IF_STMT> try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",if session.sid in self._cache:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue <IF_STMT> self.add_doc_id(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def extract(self, zip): max_nb = maxNbFile(self) for index, field in enumerate(zip.array('file')): <IF_STMT> self.warning('ZIP archive contains many files, but only first %s files are processed' % max_nb) break self.processFile(field)",if max_nb is not None and max_nb <= index:
"def get_norm(norm, out_channels): if isinstance(norm, str): <IF_STMT> return None norm = {'BN': BatchNorm2d, 'GN': lambda channels: nn.GroupNorm(32, channels), 'nnSyncBN': nn.SyncBatchNorm, '': lambda x: x}[norm] return norm(out_channels)",if len(norm) == 0:
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper <IF_STMT> ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._aggregate_rows:
"def emitIpToDomainsData(self, data, event): self.emitRawRirData(data, event) domains = data.get('domains') if isinstance(domains, list): for domain in domains: if self.checkForStop(): return None domain = domain.strip() <IF_STMT> self.emitHostname(domain, event)",if domain:
"def delete(self): from weblate.trans.models import Change, Suggestion, Vote fast_deletes = [] for item in self.fast_deletes: <IF_STMT> fast_deletes.append(Vote.objects.filter(suggestion__in=item)) fast_deletes.append(Change.objects.filter(suggestion__in=item)) fast_deletes.append(item) self.fast_deletes = fast_deletes return super().delete()",if item.model is Suggestion:
def token(self): if not self._token: try: cookie_token = self.state['request'].headers.cookie[CSRF_TOKEN].value except KeyError: cookie_token = '' <IF_STMT> self._token = cookie_token else: self._token = get_random_string(TOKEN_LENGTH) return self._token,if len(cookie_token) == TOKEN_LENGTH:
"def get_logs(last_file=None, last_time=None): try: response = client.get_logs(last_file=last_file, last_time=last_time) get_logs_streamer(show_timestamp=not hide_time, all_containers=all_containers, all_info=all_info)(response) return response except (ApiException, HTTPError) as e: <IF_STMT> handle_cli_error(e, message='Could not get logs for run `{}`.'.format(client.run_uuid)) sys.exit(1)",if not follow:
"def update(self, targets): Section.update(self, targets) outputNames = set() for target in targets: g = target.globals() outputNames.update([k for k in g.keys() if k.startswith('output:')]) rows = [] outputNames = sorted(outputNames) for outputName in outputNames: row = self.__rows.get(outputName) <IF_STMT> row = _OutputRow(outputName) self.__rows[outputName] = row row.update(targets) row.setAlternate(len(rows) % 2) rows.append(row) self._mainColumn()[:] = rows",if row is None:
"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess('branch').stdout: if git_branch_line.startswith('*'): git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() <IF_STMT> alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",if BRANCH_ALIAS_MARKER in git_branch_line:
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): if not line: return True <IF_STMT> return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",if linenum > 0 and line[0] not in '\t ':
"def resolve_references(self, note, reflist): assert len(note['ids']) == 1 id = note['ids'][0] for ref in reflist: <IF_STMT> continue ref.delattr('refname') ref['refid'] = id assert len(ref['ids']) == 1 note.add_backref(ref['ids'][0]) ref.resolved = 1 note.resolved = 1",if ref.resolved:
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide <IF_STMT> return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)","if dist(currentPos, self.ends[color]) == 1:"
"def __hierarchyViewKeyPress(hierarchyView, event): if event == __editSourceKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) <IF_STMT> __editSourceNode(hierarchyView.getContext(), hierarchyView.scene(), selectedPath) return True elif event == __editTweaksKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) if selectedPath is not None: __editTweaksNode(hierarchyView.getContext(), hierarchyView.scene(), selectedPath) return True",if selectedPath is not None:
"def getSubsegments(self): for num, localdata in self.lfh.LocalData: for bucket, seginfo in localdata.SegmentInfo: <IF_STMT> continue yield Win32Subsegment(self.trace, self.heap, seginfo.ActiveSubsegment)",if seginfo.ActiveSubsegment == 0:
"def test_full_hd_bluray(self): cur_test = 'full_hd_bluray' cur_qual = common.Quality.FULLHDBLURAY for name, tests in iteritems(self.test_cases): for test in tests: <IF_STMT> self.assertEqual(cur_qual, common.Quality.name_quality(test)) else: self.assertNotEqual(cur_qual, common.Quality.name_quality(test))",if name == cur_test:
"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num elif self.op == '-': self.current -= num elif self.op == '*': self.current *= num <IF_STMT> self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '/':
"def strip_export_type(path): matched = re.search('#([a-zA-Z0-9\\-]+\\\\+[a-zA-Z0-9\\-]+)?$', path.encode('utf-8')) mime_type = None if matched: fragment = matched.group(0) mime_type = matched.group(1) <IF_STMT> mime_type = mime_type.replace('+', '/') path = path[:-len(fragment)] return (path, mime_type)",if mime_type is not None:
"def _save_as_module(file, data, binary=False): if not data: return with open(file, 'w') as f: f.write('DATA=') <IF_STMT> f.write('""') f.write(base64.b64encode(data).decode('ascii')) f.write('""') else: f.write(str(data).replace('\\\\', '\\')) f.flush()",if binary:
"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text <IF_STMT> if len(self._scopeStrictModes) > 0: self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)","if text == '""use strict""' or text == ""'use strict'"":"
"def run(self, ttl=None): self.zeroconf = zeroconf.Zeroconf() zeroconf.ServiceBrowser(self.zeroconf, self.domain, MDNSHandler(self)) if ttl: gobject.timeout_add(ttl * 1000, self.shutdown) self.__running = True self.__mainloop = gobject.MainLoop() context = self.__mainloop.get_context() while self.__running: try: <IF_STMT> context.iteration(True) else: time.sleep(0.1) except KeyboardInterrupt: break self.zeroconf.close() logger.debug('MDNSListener.run() quit')",if context.pending():
"def topology_change_notify(self, port_state): notice = False if port_state is PORT_STATE_FORWARD: for port in self.ports.values(): if port.role is DESIGNATED_PORT: notice = True break else: notice = True if notice: self.send_event(EventTopologyChange(self.dp)) <IF_STMT> self._transmit_tc_bpdu() else: self._transmit_tcn_bpdu()",if self.is_root_bridge:
def close_open_fds(keep=None): keep = [maybe_fileno(f) for f in keep or [] if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): if fd not in keep: try: os.close(fd) except OSError as exc: <IF_STMT> raise,if exc.errno != errno.EBADF:
"def collect_attributes(options, node, master_list): """"""Collect all attributes"""""" for ii in node.instructions: if field_check(ii, 'attributes'): s = getattr(ii, 'attributes') if isinstance(s, list): for x in s: if x not in master_list: master_list.append(x) <IF_STMT> master_list.append(s) for nxt in node.next.values(): collect_attributes(options, nxt, master_list)",elif s != None and s not in master_list:
"def remove_test_run_directories(expiry_time: int=60 * 60) -> int: removed = 0 directories = glob.glob(os.path.join(UUID_VAR_DIR, 'test-backend', 'run_*')) for test_run in directories: <IF_STMT> try: shutil.rmtree(test_run) removed += 1 except FileNotFoundError: pass return removed",if round(time.time()) - os.path.getmtime(test_run) > expiry_time:
"def read_work_titles(fields): found = [] if '240' in fields: for line in fields['240']: title = join_subfield_values(line, ['a', 'm', 'n', 'p', 'r']) <IF_STMT> found.append(title) if '130' in fields: for line in fields['130']: title = ' '.join(get_lower_subfields(line)) if title not in found: found.append(title) return {'work_titles': found} if found else {}",if title not in found:
"def _process_v1_msg(prot, msg): header = None body = msg[1] if not isinstance(body, (binary_type, mmap, memoryview)): raise ValidationError(body, 'Body must be a bytestream.') if len(msg) > 2: header = msg[2] <IF_STMT> raise ValidationError(header, 'Header must be a dict.') for k, v in header.items(): header[k] = msgpack.unpackb(v) ctx = MessagePackMethodContext(prot, MessagePackMethodContext.SERVER) ctx.in_string = [body] ctx.transport.in_header = header return ctx","if not isinstance(header, dict):"
"def find(self, node): typename = type(node).__name__ method = getattr(self, 'find_{}'.format(typename), None) if method is None: fields = getattr(node, '_fields', None) <IF_STMT> return for field in fields: value = getattr(node, field) for result in self.find(value): yield result else: for result in method(node): yield result",if fields is None:
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] <IF_STMT> parts.append(param.name) if param.type: parts.append(param.type) out += [' : '.join(parts)] if param.desc and ''.join(param.desc).strip(): out += self._str_indent(param.desc) out += [''] return out",if param.name:
"def _get_image(self, image_list, source): if source.startswith('wx'): img = wx.ArtProvider_GetBitmap(source, wx.ART_OTHER, _SIZE) else: path = os.path.join(_BASE, source) <IF_STMT> img = wx.Image(path, wx.BITMAP_TYPE_GIF).ConvertToBitmap() else: img = wx.Image(path, wx.BITMAP_TYPE_PNG).ConvertToBitmap() return image_list.Add(img)",if source.endswith('gif'):
"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: <IF_STMT> sectors.append(submob) for r, submob in zip(np.arange(0, self.radius, dr), sectors): if type(submob) != AnnularSector: continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",if type(submob) == AnnularSector:
"def _sqlite_post_configure_engine(url, engine, follower_ident): from sqlalchemy import event  @event.listens_for(engine, 'connect') def connect(dbapi_connection, connection_record): <IF_STMT> dbapi_connection.execute('ATTACH DATABASE ""test_schema.db"" AS test_schema') else: dbapi_connection.execute('ATTACH DATABASE ""%s_test_schema.db"" AS test_schema' % follower_ident)",if not follower_ident:
"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) if conf_file: with open(conf_file) as conf_handle: value = fn(conf_handle) <IF_STMT> return value return None",if value:
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True <IF_STMT> return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",if len(a) != b.len:
"def _get_initialized_app(app): """"""Returns a reference to an initialized App instance."""""" if app is None: return firebase_admin.get_app() if isinstance(app, firebase_admin.App): initialized_app = firebase_admin.get_app(app.name) <IF_STMT> raise ValueError('Illegal app argument. App instance not initialized via the firebase module.') return app raise ValueError('Illegal app argument. Argument must be of type  firebase_admin.App, but given ""{0}"".'.format(type(app)))",if app is not initialized_app:
def compiled_query(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.compiled_query_ is None: self.compiled_query_ = CompiledQuery() finally: self.lazy_init_lock_.release() return self.compiled_query_,if self.compiled_query_ is None:
"def clean_subevent(event, subevent): if event.has_subevents: <IF_STMT> raise ValidationError(_('Subevent cannot be null for event series.')) if event != subevent.event: raise ValidationError(_('The subevent does not belong to this event.')) elif subevent: raise ValidationError(_('The subevent does not belong to this event.'))",if not subevent:
"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return 'TINYBLOB' if length <= self.LENGTH_LIMIT_BLOB: return 'BLOB' <IF_STMT> return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_MEDIUMBLOB:
"def decompress(self, data): if not data: return data if not self._first_try: return self._obj.decompress(data) self._data += data try: decompressed = self._obj.decompress(data) <IF_STMT> self._first_try = False self._data = None return decompressed except zlib.error: self._first_try = False self._obj = zlib.decompressobj(-zlib.MAX_WBITS) try: return self.decompress(self._data) finally: self._data = None",if decompressed:
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) if events | pyuv.fs.UV_RENAME: <IF_STMT> self.watches.pop(path).close()",if not os.path.exists(path):
"def __init__(self, duration, batch_shape, event_shape, validate_args=None): if duration is None: <IF_STMT> duration = event_shape[0] elif duration != event_shape[0]: if event_shape[0] != 1: raise ValueError('duration, event_shape mismatch: {} vs {}'.format(duration, event_shape)) event_shape = torch.Size((duration,) + event_shape[1:]) self._duration = duration super().__init__(batch_shape, event_shape, validate_args)",if event_shape[0] != 1:
"def _CheckPrerequisites(self): """"""Exits if any of the prerequisites is not met."""""" if not FLAGS.kubectl: raise Exception('Please provide path to kubectl tool using --kubectl flag. Exiting.') if not FLAGS.kubeconfig: raise Exception('Please provide path to kubeconfig using --kubeconfig flag. Exiting.') if self.disk_specs and self.disk_specs[0].disk_type == disk.STANDARD: <IF_STMT> raise Exception('Please provide a list of Ceph Monitors using --ceph_monitors flag.')",if not FLAGS.ceph_monitors:
"def invalidateDependentSlices(self, iFirstCurve): if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) <IF_STMT> c.invalidate() elif i == iFirstCurve: break","if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType):"
"def find_backwards(self, offset): try: for _, token_type, token_value in reversed(self.tokens[self.offset:offset]): if token_type in ('comment', 'linecomment'): try: prefix, comment = token_value.split(None, 1) except ValueError: continue <IF_STMT> return [comment.rstrip()] return [] finally: self.offset = offset",if prefix in self.comment_tags:
"def parse_column_definitions(self, elem): for column_elem in elem.findall('column'): name = column_elem.get('name', None) assert name is not None, ""Required 'name' attribute missing from column def"" index = column_elem.get('index', None) assert index is not None, ""Required 'index' attribute missing from column def"" index = int(index) self.columns[name] = index <IF_STMT> self.largest_index = index assert 'value' in self.columns, ""Required 'value' column missing from column def"" if 'name' not in self.columns: self.columns['name'] = self.columns['value']",if index > self.largest_index:
"def __find_smallest(self): """"""Find the smallest uncovered value in the matrix."""""" minval = sys.maxsize for i in range(self.n): for j in range(self.n): if not self.row_covered[i] and (not self.col_covered[j]): <IF_STMT> minval = self.C[i][j] return minval",if minval > self.C[i][j]:
def includes_tools_for_display_in_tool_panel(self): if self.includes_tools: tool_dicts = self.metadata['tools'] for tool_dict in tool_dicts: <IF_STMT> return True return False,"if tool_dict.get('add_to_tool_panel', True):"
"def commit(self, notify=False): if self.editing: text = self._text if text: try: value = self.type(text) except ValueError: return value = self.clamp_value(value) else: value = self.empty if value is NotImplemented: return self.value = value self.insertion_point = None <IF_STMT> self.change_text(unicode(value)) else: self._text = unicode(value) self.editing = False else: self.insertion_point = None",if notify:
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end if end < self.plugin_args.start: continue <IF_STMT> break for vaddr in utils.xrange(start, end, 4096): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if start > self.plugin_args.end:
"def _check_for_duplicate_host_entries(self, task_entries): non_host_statuses = (models.HostQueueEntry.Status.PARSING, models.HostQueueEntry.Status.ARCHIVING) for task_entry in task_entries: using_host = task_entry.host is not None and task_entry.status not in non_host_statuses <IF_STMT> self._assert_host_has_no_agent(task_entry)",if using_host:
def get_biggest_wall_time(jsons): lowest_wall = None for j in jsons: <IF_STMT> lowest_wall = j['wall_time'] if lowest_wall < j['wall_time']: lowest_wall = j['wall_time'] return lowest_wall,if lowest_wall is None:
"def log_change_report(self, old_value, new_value, include_details=False): from octoprint.util import map_boolean with self._check_mutex: self._logger.info('Connectivity changed from {} to {}'.format(map_boolean(old_value, 'online', 'offline'), map_boolean(new_value, 'online', 'offline'))) <IF_STMT> self.log_details()",if include_details:
"def _include_block(self, value, context=None): if hasattr(value, 'render_as_block'): <IF_STMT> new_context = context.get_all() else: new_context = {} return jinja2.Markup(value.render_as_block(context=new_context)) return jinja2.Markup(value)",if context:
"def __lt__(self, other): try: A, B = (self[0], other[0]) if A and B: <IF_STMT> return self[2] < other[2] return A < B return self[1] < other[1] except IndexError: return NotImplemented",if A == B:
"def _get_port(): while True: port = 20000 + random.randint(1, 9999) for i in range(5): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex(('127.0.0.1', port)) <IF_STMT> continue else: return port",if result == 0:
"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: if type(targets) != list and type(targets) != tuple: targets = tuple(targets) <IF_STMT> targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",elif type(targets) != tuple:
"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = {'common': 'dns_ip'} if 'node' not in facts: facts['node'] = {} for role in params.keys(): <IF_STMT> for param in params[role]: if param in facts[role]: facts['node'][param] = facts[role].pop(param) return facts",if role in facts:
"def build_dimension_param(self, dimension, params): prefix = 'Dimensions.member' i = 0 for dim_name in dimension: dim_value = dimension[dim_name] <IF_STMT> if isinstance(dim_value, six.string_types): dim_value = [dim_value] for value in dim_value: params['%s.%d.Name' % (prefix, i + 1)] = dim_name params['%s.%d.Value' % (prefix, i + 1)] = value i += 1 else: params['%s.%d.Name' % (prefix, i + 1)] = dim_name i += 1",if dim_value:
"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: <IF_STMT> flag = 0 break if flag: self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",if _typ == typ and key is _key:
"def run(self): while True: message = self.in_queue.get() <IF_STMT> self.reset() elif message == EXIT: return else: index, transaction = message self.results_queue.put((index, self.validate(transaction)))",if message == RESET:
"def __run(self): threads = self.parameters()['threads'].getTypedValue() with IECore.tbb_global_control(IECore.tbb_global_control.parameter.max_allowed_parallelism, IECore.hardwareConcurrency() if threads == 0 else threads): self._executeStartupFiles(self.root().getName()) defaultMessageHandler = IECore.MessageHandler.getDefaultHandler() <IF_STMT> IECore.MessageHandler.setDefaultHandler(Gaffer.ProcessMessageHandler(defaultMessageHandler)) return self._run(self.parameters().getValidatedValue())","if not isinstance(defaultMessageHandler, Gaffer.ProcessMessageHandler):"
"def adjust_uri(self, uri, relativeto): """"""Adjust the given ``uri`` based on the given relative URI."""""" key = (uri, relativeto) if key in self._uri_cache: return self._uri_cache[key] if uri[0] != '/': <IF_STMT> v = self._uri_cache[key] = posixpath.join(posixpath.dirname(relativeto), uri) else: v = self._uri_cache[key] = '/' + uri else: v = self._uri_cache[key] = uri return v",if relativeto is not None:
"def decoder(s): r = [] decode = [] for c in s: <IF_STMT> decode.append('&') elif c == '-' and decode: if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",if c == '&' and (not decode):
"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() if line.startswith('-'): args.extend(self._split_option(line)) <IF_STMT> args.append(line) return args",elif line and (not line.startswith('#')):
"def _method_events_callback(self, values): try: previous_echoed = values['child_result_list'][-1].decode().split('\n')[-2].strip() if previous_echoed.endswith('foo1'): return 'echo foo2\n' elif previous_echoed.endswith('foo2'): return 'echo foo3\n' <IF_STMT> return 'exit\n' else: raise Exception('Unexpected output {0!r}'.format(previous_echoed)) except IndexError: return 'echo foo1\n'",elif previous_echoed.endswith('foo3'):
"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): <IF_STMT> result.append(DELETE_SUCCESSFUL) elif status == MemcacheDeleteResponse.NOT_FOUND: result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",if status == MemcacheDeleteResponse.DELETED:
"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) <IF_STMT> plug.setInput(randomNode['outFloat']) elif isinstance(plug, Gaffer.Color3fPlug): plug.setInput(randomNode['outColor']) GafferUI.NodeEditor.acquire(randomNode)","if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)):"
"def escapeentities(self, line): """"""Escape all Unicode characters to HTML entities."""""" result = '' pos = TextPosition(line) while not pos.finished(): if ord(pos.current()) > 128: codepoint = hex(ord(pos.current())) <IF_STMT> codepoint = hex(ord(pos.next()) + 63488) result += '&#' + codepoint[1:] + ';' else: result += pos.current() pos.skipcurrent() return result",if codepoint == '0xd835':
def get_and_set_all_aliases(self): all_aliases = [] for page in self.pages: <IF_STMT> all_aliases.extend(page.relations.aliases_norm) if page.relations.aliases is not None: all_aliases.extend(page.relations.aliases) return set(all_aliases),if page.relations.aliases_norm is not None:
"def _list_cases(suite): for test in suite: <IF_STMT> _list_cases(test) elif isinstance(test, unittest.TestCase): if support.match_test(test): print(test.id())","if isinstance(test, unittest.TestSuite):"
"def get_next_requests(self, max_n_requests, **kwargs): next_pages = [] partitions = set(kwargs.pop('partitions', [])) for partition_id in range(0, self.queue_partitions): <IF_STMT> continue results = self.queue.get_next_requests(max_n_requests, partition_id) next_pages.extend(results) self.logger.debug('Got %d requests for partition id %d', len(results), partition_id) return next_pages",if partition_id not in partitions:
"def __iter__(self): if self.query is not None and sqlite.is_read_only_query(self.query): cur = self.connection.cursor() results = cur.execute(self.query) <IF_STMT> yield [col[0] for col in cur.description] for i, row in enumerate(results): if i >= self.limit: break yield [val for val in row] else: yield",if self.headers:
"def rollback(self): for operation, values in self.current_transaction_state[::-1]: if operation == 'insert': values.remove() <IF_STMT> old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",elif operation == 'update':
"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) <IF_STMT> return int(q) elif self._start >= value > self._stop: q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError('{} is not in numeric range'.format(value))",if r == self._zero:
"def validate_name_and_description(body, check_length=True): for attribute in ['name', 'description', 'display_name', 'display_description']: value = body.get(attribute) <IF_STMT> if isinstance(value, six.string_types): body[attribute] = value.strip() if check_length: try: utils.check_string_length(body[attribute], attribute, min_length=0, max_length=255) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",if value is not None:
"def printWiki(): firstHeading = False for m in protocol: if m[0] == '': <IF_STMT> output('|}') __printWikiHeader(m[1], m[2]) firstHeading = True else: output('|-') output('| <span style=""white-space:nowrap;""><tt>' + m[0] + '</tt></span> || || ' + m[1]) output('|}')",if firstHeading:
"def _get_platforms(data): platform_list = [] for item in data: if item.startswith('PlatformEdit.html?'): parameter_list = item.split('PlatformEdit.html?', 1)[1].split('&') for parameter in parameter_list: <IF_STMT> platform_list.append(parameter.split('=')[1]) return platform_list",if parameter.startswith('platformName'):
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v['Category'] != 'Deprecated': if v['FeatureType'] == 'val': <IF_STMT> states.append((name, v['Value'])) elif name.startswith('SCLEX_'): lexers.append((name, v['Value'])) return (lexers, states)",if name.startswith('SCE_'):
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): <IF_STMT> if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",if not operation_name:
"def _insertNewItemAtParent(self, targetIndex): if not self.isContainer(targetIndex): return elif not self.isContainerOpen(targetIndex): uri = self._rows[targetIndex].uri modelNode = self.getNodeForURI(uri) <IF_STMT> modelNode.markForRefreshing() return self.refreshView(targetIndex)",if modelNode:
"def _get_trace(self, model, guide, args, kwargs): model_trace, guide_trace = super()._get_trace(model, guide, args, kwargs) for node in model_trace.nodes.values(): <IF_STMT> log_prob = node['packed']['unscaled_log_prob'] require_backward(log_prob) self._saved_state = (model, model_trace, guide_trace, args, kwargs) return (model_trace, guide_trace)",if node['type'] == 'sample' and (not node['is_observed']):
"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: if value is None: continue if not isinstance(key, bytes): key = text_type(key).encode(charset) <IF_STMT> value = text_type(value).encode(charset) yield (_fast_url_quote_plus(key) + '=' + _fast_url_quote_plus(value))","if not isinstance(value, bytes):"
"def handle_parse_result(self, ctx, opts, args): with augment_usage_errors(ctx, param=self): value = self.consume_value(ctx, opts) try: value = self.full_process_value(ctx, value) except Exception: <IF_STMT> raise value = None if self.callback is not None: try: value = invoke_param_callback(self.callback, ctx, self, value) except Exception: if not ctx.resilient_parsing: raise if self.expose_value: ctx.params[self.name] = value return (value, args)",if not ctx.resilient_parsing:
"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: <IF_STMT> return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) elif dict[pattern[i]] != list_str[i]: return False return True",if list_str[i] in set_value:
"def create(self, path, wipe=False): _path = self.validatepath(path) with ftp_errors(self, path): <IF_STMT> empty_file = io.BytesIO() self.ftp.storbinary(str('STOR ') + _encode(_path, self.ftp.encoding), empty_file) return True return False",if wipe or not self.isfile(path):
"def build_output_for_item(self, item): output = [] for field in self.fields: values = self._get_item(item, field) <IF_STMT> values = [values] for value in values: if value: output.append(self.build_output_for_single_value(value)) return ''.join(output)","if not isinstance(values, list):"
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): if not name[0].isupper(): <IF_STMT> if is_resource_action(member): resource_methods[name] = member return resource_methods",if not name.startswith('wait_until'):
"def get_command(cls): ifconfig_cmd = 'ifconfig' for path in ['/sbin', '/usr/sbin', '/bin', '/usr/bin']: <IF_STMT> ifconfig_cmd = os.path.join(path, ifconfig_cmd) break ifconfig_cmd = ifconfig_cmd + ' -a' return ifconfig_cmd","if os.path.exists(os.path.join(path, ifconfig_cmd)):"
"def main(): base_dir = os.path.join(os.path.split(__file__)[0], '..', '..') for path in PATHS: path = os.path.join(base_dir, path) for root, _, files in os.walk(path): for file in files: extension = os.path.splitext(file)[1] <IF_STMT> path = os.path.join(root, file) validate_header(path)",if extension in EXTENSIONS:
"def auth_login(request): form = RegistrationForm(request.POST or None) if form.is_valid(): authed_user = authenticate(username=form.cleaned_data['username'], password=form.cleaned_data['password']) <IF_STMT> login(request, authed_user) return HttpResponse('Success') raise Http404",if authed_user:
"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) if user is None: self.users[current_user.id] = dict(session_count=1, key=_key) else: <IF_STMT> user['session_count'] += 1 user['key'] = _key",if _new_login:
"def fetch(self, fingerprints): to_fetch = [f for f in fingerprints if f not in self._cache] self._logger.debug('cache size %s' % len(self._cache)) self._logger.debug('to fetch %d from %d' % (len(to_fetch), len(fingerprints))) [self._redis_pipeline.hgetall(key) for key in to_fetch] responses = self._redis_pipeline.execute() for index, key in enumerate(to_fetch): response = responses[index] <IF_STMT> self._cache[key] = response[FIELD_STATE] else: self._cache[key] = self.NOT_CRAWLED",if len(response) > 0 and FIELD_STATE in response:
"def _append_to_io_queue(self, data, stream_name): parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: if part: for block in re.split('(.{%d,})' % (self._get_squeeze_threshold() + 1), part): <IF_STMT> self._queued_io_events.append((block, stream_name))",if block:
"def find_file_at_path_with_indexes(self, path, url): if url.endswith('/'): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith('/' + self.index_file): <IF_STMT> return self.redirect(url, url[:-len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: if os.path.isfile(os.path.join(path, self.index_file)): return self.redirect(url, url + '/') raise MissingFileError(path)",if os.path.isfile(path):
"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), 'ouroboros') for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) if ext == '.py' or (ext == '' and os.path.isdir(os.path.join(basedir, name))): if module_name not in IGNORE_MODULES and module_name not in native: <IF_STMT> modules.append(module_name) return set(modules)",if not (fast and module_name in KNOWN_PROBLEM_MODULES):
def housenumber(self): if self.address: expression = '\\d+' pattern = re.compile(expression) match = pattern.search(self.address) <IF_STMT> return int(match.group(0)),if match:
"def get_pip_version(import_path=BASE_IMPORT_PATH): try: pip = importlib.import_module(import_path) except ImportError: <IF_STMT> return get_pip_version(import_path='pip') else: import subprocess version = subprocess.check_output(['pip', '--version']) if version: version = version.decode('utf-8').split()[1] return version return '0.0.0' version = getattr(pip, '__version__', None) return version",if import_path != 'pip':
"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: if not self.__progress_status: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY <IF_STMT> self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY time.sleep(sleep_time)",elif self.__show_animation:
def range_key_names(self): keys = [self.range_key_attr] for index in self.global_indexes: range_key = None for key in index.schema: <IF_STMT> range_key = keys.append(key['AttributeName']) keys.append(range_key) return keys,if key['KeyType'] == 'RANGE':
"def run(self): dist = self.distribution commands = dist.command_options.keys() settings = {} for cmd in commands: if cmd == 'saveopts': continue for opt, (src, val) in dist.get_option_dict(cmd).items(): <IF_STMT> settings.setdefault(cmd, {})[opt] = val edit_config(self.filename, settings, self.dry_run)",if src == 'command line':
"def parse_move(self, node): old, new = ('', '') for child in node: tag, text = (child.tag, child.text) text = text.strip() if text else None if tag == 'Old' and text: old = text <IF_STMT> new = text return Move(old, new)",elif tag == 'New' and text:
"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = (self.pyflakes_enabled, self.pep8_enabled) for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() if (run_pyflakes or run_pep8) and current_finfo is not None: <IF_STMT> finfo.run_code_analysis(run_pyflakes, run_pep8)",if current_finfo is not finfo:
"def tchg(var, width): """"""Convert time string to given length"""""" ret = '%2dh%02d' % (var / 60, var % 60) <IF_STMT> ret = '%2dh' % (var / 60) if len(ret) > width: ret = '%2dd' % (var / 60 / 24) if len(ret) > width: ret = '%2dw' % (var / 60 / 24 / 7) return ret",if len(ret) > width:
"def spider_log_activity(self, messages): for i in range(0, messages): <IF_STMT> self.sp_sl_p.send(sha1(str(randint(1, 1000))), b'http://helloworld.com/way/to/the/sun/' + b'0') else: self.sp_sl_p.send(sha1(str(randint(1, 1000))), b'http://way.to.the.sun' + b'0') self.sp_sl_p.flush()",if i % 2 == 0:
"def decode_serial(self, offset): serialnum = (self.cache[offset + 3] << 24) + (self.cache[offset + 2] << 16) + (self.cache[offset + 1] << 8) + self.cache[offset] serialstr = '' is_alnum = True for i in range(4): <IF_STMT> is_alnum = False break serialstr += chr(self.cache[offset + 3 - i]) serial = serialstr if is_alnum else str(serialnum) self.ann_field(offset, offset + 3, 'Serial ' + serial)",if not chr(self.cache[offset + 3 - i]).isalnum():
def gettext(rv): for child in rv.childNodes: if child.nodeType == child.TEXT_NODE: yield child.nodeValue <IF_STMT> for item in gettext(child): yield item,if child.nodeType == child.ELEMENT_NODE:
"def determine_block_hints(self, text): hints = '' if text: if text[0] in ' \n\x85\u2028\u2029': hints += str(self.best_indent) if text[-1] not in '\n\x85\u2028\u2029': hints += '-' <IF_STMT> hints += '+' return hints",elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029':
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue <IF_STMT> if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type","if isinstance(arg, bytes):"
"def as_iconbitmap(cls, rkey): """"""Get image path for use in iconbitmap property"""""" img = None if rkey in cls._stock: data = cls._stock[rkey] <IF_STMT> fpath = data['filename'] fname = os.path.basename(fpath) name, file_ext = os.path.splitext(fname) file_ext = str(file_ext).lower() if file_ext in TK_BITMAP_FORMATS: img = BITMAP_TEMPLATE.format(fpath) return img","if data['type'] not in ('stock', 'data', 'image'):"
"def anonymize_ip(ip): if ip: match = RE_FIRST_THREE_OCTETS_OF_IP.findall(str(ip)) <IF_STMT> return '%s%s' % (match[0][0], '0') return ''",if match:
"def serialize_tail(self): msg = bytearray() for v in self.info: <IF_STMT> value = v['value'].encode('utf-8') elif v['type'] == BMP_TERM_TYPE_REASON: value = struct.pack('!H', v['value']) v['len'] = len(value) msg += struct.pack(self._TLV_PACK_STR, v['type'], v['len']) msg += value return msg",if v['type'] == BMP_TERM_TYPE_STRING:
"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): <IF_STMT> return text[i:end] if not unclosed_end and text[end] == '<': unclosed_end = end end += 1 raise TokenizationException('Unclosed comment', text[i:unclosed_end])",if text[end - 2:end] == '#}':
"def ComboBoxDroppedHeightTest(windows): """"""Check if each combobox height is the same as the reference"""""" bugs = [] for win in windows: if not win.ref: continue if win.Class() != 'ComboBox' or win.ref.Class() != 'ComboBox': continue <IF_STMT> bugs.append(([win], {}, testname, 0)) return bugs",if win.DroppedRect().height() != win.ref.DroppedRect().height():
"def testBadModeArgument(self): bad_mode = 'qwerty' try: f = self.open(TESTFN, bad_mode) except ValueError as msg: <IF_STMT> s = str(msg) if TESTFN in s or bad_mode not in s: self.fail('bad error message for invalid mode: %s' % s) else: f.close() self.fail('no error for invalid mode: %s' % bad_mode)",if msg.args[0] != 0:
"def command_group_expired(self, command_group_name): try: deprecate_info = self._command_loader.command_group_table[command_group_name].group_kwargs.get('deprecate_info', None) <IF_STMT> return deprecate_info.expired() except AttributeError: pass return False",if deprecate_info:
"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): if val == 0: assert 2500 - 500 < count < 2500 + 500 <IF_STMT> assert 7500 - 500 < count < 7500 + 500 else: assert False",elif val == 1:
"def get_labels(directory): cache = get_labels.__cache if directory not in cache: l = {} for t in get_visual_configs(directory)[0][LABEL_SECTION]: <IF_STMT> Messager.warning(""In configuration, labels for '%s' defined more than once. Only using the last set."" % t.storage_form(), -1) l[t.storage_form()] = t.terms[1:] cache[directory] = l return cache[directory]",if t.storage_form() in l:
"def try_split(self, split_text: List[str]): ret = [] for i in split_text: if len(i) == 0: continue val = int(i, 2) <IF_STMT> return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f'binary successful, returning {ret.__repr__()}') return ret",if val > 255 or val < 0:
"def setCellValue(self, row_idx, col, value): assert col.id == 'repls-marked' with self._lock: rgroup = self.events[row_idx] <IF_STMT> return rgroup._marked = value == 'true' and True or False if self._tree: self._tree.invalidateCell(row_idx, col)","if not isinstance(rgroup, findlib2.ReplaceHitGroup):"
"def create(cls, settlement_manager, resource_id): """"""Create a production chain that can produce the given resource."""""" resource_producer = {} for abstract_building in AbstractBuilding.buildings.values(): for resource, production_line in abstract_building.lines.items(): <IF_STMT> resource_producer[resource] = [] resource_producer[resource].append((production_line, abstract_building)) return ProductionChain(settlement_manager, resource_id, resource_producer)",if resource not in resource_producer:
def get_all_partition_sets(self): partition_sets = [] if self.partitions_handle: partition_sets.extend(self.partitions_handle.get_partition_sets()) if self.scheduler_handle: partition_sets.extend([schedule_def.get_partition_set() for schedule_def in self.scheduler_handle.all_schedule_defs() <IF_STMT>]) return partition_sets,"if isinstance(schedule_def, PartitionScheduleDefinition)"
"def _sendDatapointsNow(self, datapoints): metrics = {} payload_pb = Payload() for metric, datapoint in datapoints: <IF_STMT> metric_pb = payload_pb.metrics.add() metric_pb.metric = metric metrics[metric] = metric_pb else: metric_pb = metrics[metric] point_pb = metric_pb.points.add() point_pb.timestamp = int(datapoint[0]) point_pb.value = datapoint[1] self.sendString(payload_pb.SerializeToString())",if metric not in metrics:
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper <IF_STMT> ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._dicts:
"def get_metrics(): classifier, feature_labels = load_classifier() available_metrics = ImgageMetrics.get_metric_classes() effective_metrics = [] for metric in available_metrics: for label in feature_labels: for label_part in metric.get_labels(): <IF_STMT> effective_metrics.append(metric) return (classifier, feature_labels, available_metrics)",if label_part == label and metric not in effective_metrics:
"def test_nic_names(self): p = subprocess.Popen(['ipconfig', '/all'], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: if 'pseudo-interface' in nic.replace(' ', '-').lower(): continue <IF_STMT> self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)",if nic not in out:
"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) if value is not result: if replace: self[key] = result <IF_STMT> result.parent = self result.key = key return result","if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple):"
"def _EvaluateFile(self, test_list, file): name, ext = os.path.splitext(file) if ext == '.cc' or ext == '.cpp' or ext == '.c': <IF_STMT> logger.SilentLog('Found native test file %s' % file) test_list.append(name)","if re.search('_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$', name):"
"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): <IF_STMT> leading_ws.append(c) else: break if self.indchar is None: self.indchar = c elif c != self.indchar: self.strict_err_or_warn('found mixing of tabs and spaces', inputstring, i) return ''.join(leading_ws)",if c in legal_indent_chars:
"def ident_values(self): value = self._ident_values if value is False: value = None <IF_STMT> wrapped = self.wrapped idents = getattr(wrapped, 'ident_values', None) if idents: value = [self._wrap_hash(ident) for ident in idents] self._ident_values = value return value",if not self.orig_prefix:
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') <IF_STMT> if name not in found_names: found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",if name.startswith(expr):
"def pid_from_name(name): for pid in os.listdir('/proc'): try: int(pid) except: continue pname = '' with open('/proc/%s/cmdline' % pid, 'r') as f: pname = f.read() <IF_STMT> return int(pid) raise ProcessException('No process with such name: %s' % name)",if name in pname:
"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) <IF_STMT> raise AssertionError('Not a folder: %s' % self.path) if self.watcher: self.watcher.emit('created', self)",if not node.isdir:
"def setUp(self): BaseTestCase.setUp(self) self.rawData = [] self.dataByKey = {} for i in range(1, 11): stringCol = 'String %d' % i fixedCharCol = ('Fixed Char %d' % i).ljust(40) rawCol = 'Raw %d' % i <IF_STMT> nullableCol = 'Nullable %d' % i else: nullableCol = None dataTuple = (i, stringCol, rawCol, fixedCharCol, nullableCol) self.rawData.append(dataTuple) self.dataByKey[i] = dataTuple",if i % 2:
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] <IF_STMT> yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if level + 1 == len(hits):
"def __repr__(self): attrs = [] for k in self.keydata: <IF_STMT> attrs.append('p(%d)' % (self.size() + 1,)) elif hasattr(self.key, k): attrs.append(k) if self.has_private(): attrs.append('private') return '<%s @0x%x %s>' % (self.__class__.__name__, id(self), ','.join(attrs))",if k == 'p':
def autoload(self): if self._app.config.THEME == 'auto': <IF_STMT> if get_osx_theme() == 1: theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: theme = self._app.config.THEME self.load_theme(theme),if sys.platform == 'darwin':
"def _get_matching_bracket(self, s, pos): if s[pos] != '{': return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == '{': depth += 1 elif c == '}': depth -= 1 <IF_STMT> break pos += 1 if pos < end and s[pos] == '}': return pos return None",if depth == 0:
"def update_meter(self, output, target, meters={'accuracy'}): output = self.__to_tensor(output) target = self.__to_tensor(target) for meter in meters: <IF_STMT> self.__addmeter(meter) if meter in ['ap', 'map', 'confusion']: target_th = self._ver2tensor(target) self.meter[meter].add(output, target_th) else: self.meter[meter].add(output, target)",if meter not in self.meter.keys():
"def _reinit_optimizers_with_oss(self): optimizers = self.lightning_module.trainer.optimizers for x, optimizer in enumerate(optimizers): if is_lightning_optimizer(optimizer): optimizer = optimizer._optimizer <IF_STMT> optim_class = type(optimizer) zero_optimizer = OSS(params=optimizer.param_groups, optim=optim_class, **optimizer.defaults) optimizers[x] = zero_optimizer del optimizer trainer = self.lightning_module.trainer trainer.optimizers = optimizers trainer.convert_to_lightning_optimizers()","if not isinstance(optimizer, OSS):"
"def OnSelChanged(self, event): self.item = event.GetItem() if self.item: self.log.write('OnSelChanged: %s' % self.GetItemText(self.item)) <IF_STMT> self.log.write(', BoundingRect: %s\n' % self.GetBoundingRect(self.item, True)) else: self.log.write('\n') event.Skip()",if wx.Platform == '__WXMSW__':
"def parse_batch(args): errmsg = 'Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1).' if args.batch is not None: rule, batchdef = parse_key_value_arg(args.batch, errmsg=errmsg) try: batch, batches = batchdef.split('/') batch = int(batch) batches = int(batches) except ValueError: raise ValueError(errmsg) <IF_STMT> raise ValueError(errmsg) return Batch(rule, batch, batches) return None",if batch > batches or batch < 1:
"def get_foreign_key_columns(self, engine, table_name): foreign_keys = set() table = db_utils.get_table(engine, table_name) inspector = reflection.Inspector.from_engine(engine) for column_dict in inspector.get_columns(table_name): column_name = column_dict['name'] column = getattr(table.c, column_name) <IF_STMT> foreign_keys.add(column_name) return foreign_keys",if column.foreign_keys:
"def update(self, t): l = int(t * self.nr_of_tiles) for i in range(self.nr_of_tiles): t = self.tiles_order[i] <IF_STMT> self.turn_off_tile(t) else: self.turn_on_tile(t)",if i < l:
"def read(self, amt=None): if self._rbuf and (not amt is None): L = len(self._rbuf) <IF_STMT> amt -= L else: s = self._rbuf[:amt] self._rbuf = self._rbuf[amt:] return s s = self._rbuf + self._raw_read(amt) self._rbuf = b'' return s",if amt > L:
"def draw_menu_button(self, context, layout, node, text): if hasattr(node.id_data, 'sv_show_socket_menus') and node.id_data.sv_show_socket_menus: <IF_STMT> layout.menu('SV_MT_SocketOptionsMenu', text='', icon='TRIA_DOWN')",if self.is_output or self.is_linked or (not self.use_prop):
def __enter__(self): with DB.connection_context(): session_record = SessionRecord() session_record.f_session_id = self._session_id session_record.f_engine_name = self._engine_name session_record.f_engine_type = EngineType.STORAGE session_record.f_engine_address = {} session_record.f_create_time = current_timestamp() rows = session_record.save(force_insert=True) <IF_STMT> raise Exception(f'create session record {self._session_id} failed') LOGGER.debug(f'save session {self._session_id} record') self.create() return self,if rows != 1:
"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop(2.0) <IF_STMT> self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.sl_hdlr:
"def _dec_device(self, srcdev, dstdev): if srcdev: self.srcdevs[srcdev] -= 1 <IF_STMT> del self.srcdevs[srcdev] self._set_limits('read', self.srcdevs) if dstdev: self.dstdevs[dstdev] -= 1 if self.dstdevs[dstdev] == 0: del self.dstdevs[dstdev] self._set_limits('write', self.dstdevs)",if self.srcdevs[srcdev] == 0:
"def array_for(self, i): if 0 <= i < self._cnt: <IF_STMT> return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[i >> level & 31] level -= 5 return node._array affirm(False, u'Index out of Range')",if i >= self.tailoff():
"def convert_tensor(self, offsets, sizes): results = [] for b, batch in enumerate(offsets): utterances = [] for p, utt in enumerate(batch): size = sizes[b][p] <IF_STMT> utterances.append(utt[0:size]) else: utterances.append(torch.tensor([], dtype=torch.int)) results.append(utterances) return results",if sizes[b][p] > 0:
"def _predict_proba(self, X, preprocess=True): if preprocess: X = self.preprocess(X) if self.problem_type == REGRESSION: return self.model.predict(X) y_pred_proba = self.model.predict_proba(X) if self.problem_type == BINARY: if len(y_pred_proba.shape) == 1: return y_pred_proba <IF_STMT> return y_pred_proba[:, 1] else: return y_pred_proba elif y_pred_proba.shape[1] > 2: return y_pred_proba else: return y_pred_proba[:, 1]",elif y_pred_proba.shape[1] > 1:
def timeout(self): now = ptime.time() dt = now - self.lastPlayTime if dt < 0: return n = int(self.playRate * dt) if n != 0: self.lastPlayTime += float(n) / self.playRate <IF_STMT> self.play(0) self.jumpFrames(n),if self.currentIndex + n > self.image.shape[self.axes['t']]:
"def __init__(self, data, weights=None, ddof=0): self.data = np.asarray(data) if weights is None: self.weights = np.ones(self.data.shape[0]) else: self.weights = np.asarray(weights).astype(float) <IF_STMT> self.weights = self.weights.squeeze() self.ddof = ddof",if len(self.weights.shape) > 1 and len(self.weights) > 1:
"def writerow(self, row): unicode_row = [] for col in row: <IF_STMT> unicode_row.append(col.encode('utf-8').strip()) else: unicode_row.append(col) self.writer.writerow(unicode_row) data = self.queue.getvalue() data = data.decode('utf-8') data = self.encoder.encode(data) self.stream.write(data) self.queue.truncate(0)",if type(col) == str or type(col) == unicode:
"def __init__(self, choices, allow_blank=False, **kwargs): self.choiceset = choices self.allow_blank = allow_blank self._choices = dict() for k, v in choices: <IF_STMT> for k2, v2 in v: self._choices[k2] = v2 else: self._choices[k] = v super().__init__(**kwargs)","if type(v) in [list, tuple]:"
"def simp_ext(_, expr): if expr.op.startswith('zeroExt_'): arg = expr.args[0] <IF_STMT> return arg return ExprCompose(arg, ExprInt(0, expr.size - arg.size)) if expr.op.startswith('signExt_'): arg = expr.args[0] add_size = expr.size - arg.size new_expr = ExprCompose(arg, ExprCond(arg.msb(), ExprInt(size2mask(add_size), add_size), ExprInt(0, add_size))) return new_expr return expr",if expr.size == arg.size:
"def mark_differences(value: str, compare_against: str): result = [] for i, char in enumerate(value): try: <IF_STMT> result.append('<font color=""red"">{}</font>'.format(char)) else: result.append(char) except IndexError: result.append(char) return ''.join(result)",if char != compare_against[i]:
"def run_query(self, query, user): url = '%s%s' % (self.base_url, '&'.join(query.split('\n'))) error = None data = None try: response = requests.get(url, auth=self.auth, verify=self.verify) <IF_STMT> data = _transform_result(response) else: error = 'Failed getting results (%d)' % response.status_code except Exception as ex: data = None error = str(ex) return (data, error)",if response.status_code == 200:
"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, 'md_bg_color') and self.focus_behavior: <IF_STMT> self.md_bg_color = self.theme_cls.bg_normal elif not self.focus_color: self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color","if hasattr(self, 'theme_cls') and (not self.focus_color):"
"def tearDown(self): if not self.is_playback(): try: <IF_STMT> self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: if self.storage_account_name is not None: self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self.hosted_service_name is not None:
"def name2cp(k): if k == 'apos': return ord(""'"") if hasattr(htmlentitydefs, 'name2codepoint'): return htmlentitydefs.name2codepoint[k] else: k = htmlentitydefs.entitydefs[k] <IF_STMT> return int(k[2:-1]) return ord(codecs.latin_1_decode(k)[0])",if k.startswith('&#') and k.endswith(';'):
"def _para_set(self, params, part): if len(params) == 0: result = suggest([i.get_name() for i in self._options], part) return result elif len(params) == 1: paramName = params[0] if paramName not in self._options: return [] opt = self._options[paramName] paramType = opt.get_type() <IF_STMT> values = [opt.get_default_value() == 'True' and 'False' or 'True'] else: values = self._memory[paramName] return suggest(values, part) else: return []",if paramType == 'boolean':
"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) <IF_STMT> return -1 if a > b: return 1 return 0 except: return cmp(x, y)",if a < b:
"def execute(self, statement, arguments=None): while True: try: if arguments: self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: <IF_STMT> raise else: break if statement.lstrip().upper().startswith('SELECT'): return self.cursor.fetchall()",if 'locked' not in getSafeExString(ex):
"def _test_forever(self, tests): while True: for test_name in tests: yield test_name <IF_STMT> return if self.ns.fail_env_changed and self.environment_changed: return",if self.bad:
"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] if user.room: <IF_STMT> hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage('left-notification').format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",if self.isRoomSame(user.room):
"def AutoTest(): with open(sys.argv[1], 'rb') as f: for line in f.read().split(b'\n'): line = BYTES2SYSTEMSTR(line.strip()) if not line: continue elif line.startswith('#'): print(line) else: print('>>> ' + line) os.system(line) sys.stdout.write('\npress enter to continue...') <IF_STMT> input() else: raw_input() sys.stdout.write('\n')",if PY3:
"def get_first_field(layout, clz): for layout_object in layout.fields: <IF_STMT> return layout_object elif hasattr(layout_object, 'get_field_names'): gf = get_first_field(layout_object, clz) if gf: return gf","if issubclass(layout_object.__class__, clz):"
"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: if isinstance(kwargs.get('event_data', {}).get(key), str): <IF_STMT> kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)",if len(kwargs['event_data'][key]) > 1024:
"def visit_productionlist(self, node): self.new_state() names = [] for production in node: names.append(production['tokenname']) maxlen = max((len(name) for name in names)) for production in node: <IF_STMT> self.add_text(production['tokenname'].ljust(maxlen) + ' ::=') lastname = production['tokenname'] else: self.add_text('%s' % (' ' * len(lastname))) self.add_text(production.astext() + self.nl) self.end_state(wrap=False) raise nodes.SkipNode",if production['tokenname']:
"def uuid(self): if not getattr(self, '_uuid', None): <IF_STMT> self._uuid = self.repository._kp_uuid(self.path) else: self._uuid = str(uuid.uuid4()) return self._uuid",if self.repository is not None:
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) <IF_STMT> self._definition.pop(v, None) elif self._definition == 'ANY': if v == 'ANY': self._definition = [] elif v in self._definition: self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))","if isinstance(self._definition, dict):"
"def make(self): pygments_dir = join(self.dir, 'externals', 'pygments') if exists(pygments_dir): run_in_dir('hg pull', pygments_dir, self.log.info) run_in_dir('hg update', pygments_dir, self.log.info) else: <IF_STMT> os.makedirs(dirname(pygments_dir)) run_in_dir('hg clone http://dev.pocoo.org/hg/pygments-main %s' % basename(pygments_dir), dirname(pygments_dir), self.log.info)",if not exists(dirname(pygments_dir)):
def set_field(self): i = 0 for string in self.display_string: <IF_STMT> self.config[self.field + str(i)] = self.conversion_fn(self.str[i]) else: self.config[self.field + str(i)] = self.str[i] i = i + 1,if self.conversion_fn:
"def cleanup(self): with self.lock: for proc in self.processes: <IF_STMT> continue proc.join() self.processes.remove(proc) log.debug('Subprocess %s cleaned up', proc.name)",if proc.is_alive():
"def setup(self, gen): Node.setup(self, gen) for c in self.children: c.setup(gen) if not self.accepts_epsilon: for c in self.children: <IF_STMT> break else: self.accepts_epsilon = 1 gen.changed()",if not c.accepts_epsilon:
"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes['seq_num'])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 if self.completed_calls >= self._resolve_at_msg_count: <IF_STMT> self.done_future.set_result(None)",if not self.done_future.done():
"def build_canned_image_list(path): layers_path = get_bitbake_var('BBLAYERS') canned_wks_layer_dirs = [] if layers_path is not None: for layer_path in layers_path.split(): for wks_path in (WIC_DIR, SCRIPTS_CANNED_IMAGE_DIR): cpath = os.path.join(layer_path, wks_path) <IF_STMT> canned_wks_layer_dirs.append(cpath) cpath = os.path.join(path, CANNED_IMAGE_DIR) canned_wks_layer_dirs.append(cpath) return canned_wks_layer_dirs",if os.path.isdir(cpath):
"def _recv_loop(self) -> None: async with self._ws as connection: self._connected = True self.connection = connection while self._connected: try: resp = await self.connection.recv() <IF_STMT> await self._on_message(resp) except (websockets.ConnectionClosed, ConnectionResetError): logger.info('connection closed') break await asyncio.sleep(0) if self._connected: self._loop.create_task(self.dispose())",if resp:
"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): if start in line: should_yield = True continue <IF_STMT> return if should_yield and line: yield line.strip().split(' ')[0]",if end and end in line:
"def handle_parse_result(self, ctx, opts, args): if self.name in opts: if self.mutually_exclusive.intersection(opts): self._raise_exclusive_error() <IF_STMT> self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self.multiple and len(set(opts[self.name])) > 1:
"def write(self, s): if self.interactive: <IF_STMT> self.active_mode.write(s) else: component.get('CmdLine').add_line(s, False) self.events.append(s) else: print(colors.strip_colors(s))","if isinstance(self.active_mode, deluge.ui.console.modes.cmdline.CmdLine):"
"def findfiles(path): files = [] for name in os.listdir(path): if name.startswith('.') or name == 'lastsnap.jpg': continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode <IF_STMT> files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",if stat.S_ISDIR(mode):
"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, '_UNDOCUMENTED', False): continue if startswith is not None and (not key.startswith(startswith)): continue <IF_STMT> continue names.append(key) return names","if getattr(command, 'positional_arg', False):"
"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): <IF_STMT> lines[i] = line[:-2] + '\n' elif line.endswith('\r'): lines[i] = line[:-1] + '\n'",if line.endswith('\r\n'):
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end if end < self.plugin_args.start: continue if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 4096): <IF_STMT> yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if self.plugin_args.start <= vaddr <= self.plugin_args.end:
"def get_shape_at_node(self, node, assumptions): for k, v in assumptions.items(): <IF_STMT> return v if node.inputs: return node.container.shape(input_shapes=[self.get_shape_at_node(input_node, assumptions) for input_node in node.inputs]) else: return node.container.shape(None)",if k in node.names:
"def fix_doc(self, doc): type = doc.get('type', {}).get('key') if type == '/type/work': <IF_STMT> doc['authors'] = [a for a in doc['authors'] if 'author' in a and 'key' in a['author']] elif type == '/type/edition': if 'title_prefix' in doc: title = doc['title_prefix'].strip() + ' ' + doc.get('title', '') doc['title'] = title.strip() del doc['title_prefix'] return doc",if doc.get('authors'):
"def modify_column(self, column: List[Optional['Cell']]): for i in range(len(column)): gate = column[i] <IF_STMT> continue elif isinstance(gate, ParityControlCell): column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])",if gate is self:
"def onSync(self, auto=False, reload=True): if not auto or (self.pm.profile['syncKey'] and self.pm.profile['autoSync'] and (not self.safeMode)): from aqt.sync import SyncManager if not self.unloadCollection(): return self.state = 'sync' self.syncer = SyncManager(self, self.pm) self.syncer.sync() if reload: <IF_STMT> self.loadCollection()",if not self.col:
"def _has_url_match(self, match, request_url): url = match['url'] if _is_string(url): <IF_STMT> return self._has_strict_url_match(url, request_url) else: url_without_qs = request_url.split('?', 1)[0] return url == url_without_qs elif isinstance(url, re._pattern_type) and url.match(request_url): return True else: return False",if match['match_querystring']:
"def pool_image(self, image): if self.count < self.pool_size: self.pool.append(image) self.count += 1 return image else: p = random.random() <IF_STMT> random_id = random.randint(0, self.pool_size - 1) temp = self.pool[random_id] self.pool[random_id] = image return temp else: return image",if p > 0.5:
"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: <IF_STMT> width = operation['right'] - operation['left'] height = operation['bottom'] - operation['top'] if operation['type'] == 'resize': width = operation['width'] height = operation['height'] return (width, height)",if operation['type'] == 'crop':
"def validate_matrix(matrix): if not matrix: return None for key, value in matrix.items(): <IF_STMT> raise ValidationError('`{}` defines a non uniform distribution, and it cannot be used with bayesian optimization.'.format(key)) return matrix",if value.is_distribution and (not value.is_uniform):
"def scm_to_conandata(self): try: scm_to_conandata = get_env('CONAN_SCM_TO_CONANDATA') <IF_STMT> scm_to_conandata = self.get_item('general.scm_to_conandata') return scm_to_conandata.lower() in ('1', 'true') except ConanException: return False",if scm_to_conandata is None:
"def _link_vrf_table(self, vrf_table, rt_list): route_family = vrf_table.route_family for rt in rt_list: rt_rf_id = rt + ':' + str(route_family) table_set = self._tables_for_rt.get(rt_rf_id) <IF_STMT> table_set = set() self._tables_for_rt[rt_rf_id] = table_set table_set.add(vrf_table) LOG.debug('Added VrfTable %s to import RT table list: %s', vrf_table, rt)",if table_set is None:
"def add_tags(self, cve_results: Dict[str, Dict[str, Dict[str, str]]], file_object: FileObject): for component in cve_results: for cve_id in cve_results[component]: entry = cve_results[component][cve_id] <IF_STMT> self.add_analysis_tag(file_object, 'CVE', 'critical CVE', TagColor.RED, True) return",if self._entry_has_critical_rating(entry):
"def _validate(self): try: super(CustomClassifier, self)._validate() except UnsupportedDataType: if self.dtype in FACTOR_DTYPES: raise UnsupportedDataType(typename=type(self).__name__, dtype=self.dtype, hint='Did you mean to create a CustomFactor?') <IF_STMT> raise UnsupportedDataType(typename=type(self).__name__, dtype=self.dtype, hint='Did you mean to create a CustomFilter?') raise",elif self.dtype in FILTER_DTYPES:
"def formatMessage(self, record): recordcopy = copy(record) levelname = recordcopy.levelname seperator = ' ' * (8 - len(recordcopy.levelname)) if self.use_colors: levelname = self.color_level_name(levelname, recordcopy.levelno) <IF_STMT> recordcopy.msg = recordcopy.__dict__['color_message'] recordcopy.__dict__['message'] = recordcopy.getMessage() recordcopy.__dict__['levelprefix'] = levelname + ':' + seperator return super().formatMessage(recordcopy)",if 'color_message' in recordcopy.__dict__:
def dumpregs(self): for reg in list(self.regs.retaddr) + list(self.regs.misc) + list(self.regs.common) + list(self.regs.flags): enum = self.get_reg_enum(reg) <IF_STMT> debug('# Could not dump register %r' % reg) continue name = 'U.x86_const.UC_X86_REG_%s' % reg.upper() value = self.uc.reg_read(enum) debug('uc.reg_read(%(name)s) ==> %(value)x' % locals()),if not reg or enum is None:
"def filter(self, lexer, stream): current_type = None current_value = None for ttype, value in stream: if ttype is current_type: current_value += value else: <IF_STMT> yield (current_type, current_value) current_type = ttype current_value = value if current_type is not None: yield (current_type, current_value)",if current_type is not None:
"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): <IF_STMT> should_yield = True continue if end and end in line: return if should_yield and line: yield line.strip().split(' ')[0]",if start in line:
"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, 'config'), 'r') as f: for line in f: line = line.strip() if line.startswith('['): section = line[1:-1].strip() config[section] = dict() <IF_STMT> key, value = line.replace(' ', '').split('=') config[section][key] = value return config",elif section:
"def test_has_arg(fn, name, accept_all, expected): if isinstance(fn, str): context = dict() try: exec('def {}: pass'.format(fn), context) except SyntaxError: <IF_STMT> raise pytest.skip('Function is not compatible with Python 2') context.pop('__builtins__', None) fn, = context.values() assert has_arg(fn, name, accept_all) is expected","if sys.version_info >= (3,):"
"def ObjectExpression(self, properties, **kwargs): data = [] for prop in properties: self.emit(prop['value']) <IF_STMT> raise NotImplementedError('ECMA 5.1 does not support computed object properties!') data.append((to_key(prop['key']), prop['kind'][0])) self.emit('LOAD_OBJECT', tuple(data))",if prop['computed']:
"def run(self): for domain, locale, po in self.locales: <IF_STMT> path = os.path.join('locale', locale, 'LC_MESSAGES') else: path = os.path.join(self.build_dir, locale, 'LC_MESSAGES') mo = os.path.join(path, '%s.mo' % domain) self.mkpath(path) self.spawn(['msgfmt', '-o', mo, po])",if self.inplace:
"def _compute_map(self, first_byte, second_byte=None): if first_byte != 15: return 'XED_ILD_MAP0' else: if second_byte == None: return 'XED_ILD_MAP1' if second_byte == 56: return 'XED_ILD_MAP2' <IF_STMT> return 'XED_ILD_MAP3' if second_byte == 15 and self.amd_enabled: return 'XED_ILD_MAPAMD' die('Unhandled escape {} / map {} bytes'.format(first_byte, second_byte))",if second_byte == 58:
def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): <IF_STMT> buf.append(c) elif c == '\\': escaped = True elif c == '>': return ''.join(buf) else: buf.append(c) raise Exception('Unclosed tag ' + ''.join(buf)),if escaped:
"def print_pairs(attrs=None, offset_y=0): fmt = ' ({0}:{1}) ' fmt_len = len(fmt) for bg, fg in get_fg_bg(): try: color = curses.color_pair(pair_number(fg, bg)) <IF_STMT> for attr in attrs: color |= attr screen.addstr(offset_y + bg, fg * fmt_len, fmt.format(fg, bg), color) pass except curses.error: pass",if not attrs is None:
"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: <IF_STMT> axis = int(inputs[1]) elif _is_int_seq(inputs[1]): axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)","if isinstance(inputs[1], int):"
"def run(self, args, **kwargs): if args.trace_tag: kwargs['trace_tag'] = args.trace_tag if args.trigger_instance: kwargs['trigger_instance'] = args.trigger_instance if args.execution: kwargs['execution'] = args.execution if args.rule: kwargs['rule'] = args.rule if args.sort_order: <IF_STMT> kwargs['sort_asc'] = True elif args.sort_order in ['desc', 'descending']: kwargs['sort_desc'] = True return self.manager.query_with_count(limit=args.last, **kwargs)","if args.sort_order in ['asc', 'ascending']:"
def retaddr(): sp = pwndbg.regs.sp stack = pwndbg.vmmap.find(sp) frame = gdb.newest_frame() addresses = [] while frame: addresses.append(frame.pc()) frame = frame.older() start = stack.vaddr stop = start + stack.memsz while addresses and start < sp < stop: value = pwndbg.memory.u(sp) <IF_STMT> index = addresses.index(value) del addresses[:index] print(pwndbg.chain.format(sp)) sp += pwndbg.arch.ptrsize,if value in addresses:
"def update_from_dictio(self, dictio_item): for index, dictio_payload in enumerate(dictio_item, 1): fuzz_payload = None for fuzz_payload in self.payloads[index]: fuzz_payload.content = dictio_payload.content fuzz_payload.type = dictio_payload.type <IF_STMT> self.add({'full_marker': None, 'word': None, 'index': index, 'field': None}, dictio_item[index - 1])",if fuzz_payload is None:
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: <IF_STMT> result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False elif not rline.endswith(eline): return False return True","if isinstance(result, str):"
"def execute_sql(self, sql, params=None, commit=True): try: cursor = super(RetryOperationalError, self).execute_sql(sql, params, commit) except OperationalError: if not self.is_closed(): self.close() with __exception_wrapper__: cursor = self.cursor() cursor.execute(sql, params or ()) <IF_STMT> self.commit() return cursor",if commit and (not self.in_transaction()):
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: <IF_STMT> return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",if operation:
"def removeTrailingWs(self, aList): i = 0 while i < len(aList): if self.is_ws(aList[i]): j = i i = self.skip_ws(aList, i) assert j < i <IF_STMT> del aList[j:i] i = j else: i += 1",if i >= len(aList) or aList[i] == '\n':
"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: <IF_STMT> arg = self._process_filter(arg, host_state) elif isinstance(arg, basestring): arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result","if isinstance(arg, list):"
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('mw', 'hi', 'corr', 'trunc'): sent += [self.handle_word(w) for w in child] elif child.tag in ('w', 'c'): sent.append(self.handle_word(child)) <IF_STMT> raise ValueError('Unexpected element %s' % child.tag) return BNCSentence(elt.attrib['n'], sent)",elif child.tag not in self.tags_to_ignore:
"def get_display_price(base: Union[TaxedMoney, TaxedMoneyRange], display_gross: bool=False) -> Money: """"""Return the price amount that should be displayed based on settings."""""" if not display_gross: display_gross = display_gross_prices() if isinstance(base, TaxedMoneyRange): <IF_STMT> base = MoneyRange(start=base.start.gross, stop=base.stop.gross) else: base = MoneyRange(start=base.start.net, stop=base.stop.net) if isinstance(base, TaxedMoney): base = base.gross if display_gross else base.net return base",if display_gross:
"def check_classes(self, node): if isinstance(node, nodes.Element): for class_value in node['classes'][:]: if class_value in self.strip_classes: node['classes'].remove(class_value) <IF_STMT> return 1",if class_value in self.strip_elements:
"def validate(outfile=sys.stdout, silent_success=False): """"""Validates all installed models."""""" try: num_errors = get_validation_errors(outfile) <IF_STMT> return outfile.write('%s error%s found.\n' % (num_errors, num_errors != 1 and 's' or '')) except ImproperlyConfigured: outfile.write(""Skipping validation because things aren't configured properly."")",if silent_success and num_errors == 0:
"def check_basename_conflicts(self, targets): """"""Apps' basenames are used as bundle directory names. Ensure they are all unique."""""" basename_seen = {} for target in targets: <IF_STMT> raise self.BasenameConflictError(""Basename must be unique, found two targets use the same basename: {}'\n\t{} and \n\t{}"".format(target.basename, basename_seen[target.basename].address.spec, target.address.spec)) basename_seen[target.basename] = target",if target.basename in basename_seen:
"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None <IF_STMT> self.latest = True else: if 'preview' in api_version_str: self.preview = True parts = api_version_str.split('-') self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError('The API version {} is not in a supported format'.format(api_version_str))",if api_version_str == 'latest':
"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp(x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p) <IF_STMT> y = result[0] elif len(result) == 2: y1, y2 = result y = y1 if y1 & 1 == y else y2 else: return None return ec.Point(self._curve, x, y)",if len(result) == 1:
"def _visit_import_alike(self, node: Union[cst.Import, cst.ImportFrom]) -> bool: names = node.names if isinstance(names, cst.ImportStar): return False for name in names: self.provider.set_metadata(name, self.scope) asname = name.asname <IF_STMT> name_values = _gen_dotted_names(cst.ensure_type(asname.name, cst.Name)) else: name_values = _gen_dotted_names(name.name) for name_value, _ in name_values: self.scope.record_assignment(name_value, node) return False",if asname is not None:
def test_sanity_no_unmatched_parentheses(CorpusType: Type[ColumnCorpus]): corpus = CorpusType() unbalanced_entities = [] for sentence in corpus.get_all_sentences(): entities = sentence.get_spans('ner') for entity in entities: entity_text = ''.join((t.text for t in entity.tokens)) <IF_STMT> unbalanced_entities.append(entity_text) assert unbalanced_entities == [],if not has_balanced_parantheses(entity_text):
"def _learn_rate_adjust(self): if self.learn_rate_decays == 1.0: return learn_rate_decays = self._vp(self.learn_rate_decays) learn_rate_minimums = self._vp(self.learn_rate_minimums) for index, decay in enumerate(learn_rate_decays): new_learn_rate = self.net_.learnRates[index] * decay <IF_STMT> self.net_.learnRates[index] = new_learn_rate if self.verbose >= 2: print('Learn rates: {}'.format(self.net_.learnRates))",if new_learn_rate >= learn_rate_minimums[index]:
"def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None): v = self.get_xmp_tag(xmp_tags, tags) if v is not None: <IF_STMT> setattr(self, attr, v) else: if (cast == float or cast == int) and '/' in v: v = self.try_parse_fraction(v) setattr(self, attr, cast(v))",if cast is None:
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while 'e' in tokens[i + 1:]: i = tokens.index('e', i + 1) s = i - 1 e = i + 1 <IF_STMT> continue if re.match('[+-]', str(tokens[e])): e += 1 if re.match('[0-9]', str(tokens[e])): e += 1 tokens[s:e] = [''.join(tokens[s:e])] i -= 1 return tokens","if not re.match('[0-9]', str(tokens[s])):"
"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: <IF_STMT> executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) if executable.check(): return executable pytest.skip('no suitable %s found' % (name,)) return executable",if sys.platform == 'win32':
"def set_meta(self, dataset, overwrite=True, **kwd): super().set_meta(dataset, overwrite=overwrite, **kwd) try: <IF_STMT> with tarfile.open(dataset.file_name, 'r') as temptar: dataset.metadata.fast5_count = sum((1 for f in temptar if f.name.endswith('.fast5'))) except Exception as e: log.warning('%s, set_meta Exception: %s', self, e)",if dataset and tarfile.is_tarfile(dataset.file_name):
"def run(self): for k in list(iterkeys(self.objs)): <IF_STMT> continue v = self.objs[k] if v['_class'] == 'User': self.split_user(k, v) elif v['_class'] in ['Message', 'PrintJob', 'Question', 'Submission', 'UserTest']: v['participation'] = v['user'] del v['user'] return self.objs",if k.startswith('_'):
"def _findInTree(t, n): ret = [] if type(t) is dict: <IF_STMT> ret.append(t) for k, v in t.items(): ret += _findInTree(v, n) if type(t) is list: for v in t: ret += _findInTree(v, n) return ret",if '_name' in t and t['_name'] == n:
"def parseArrayPattern(self): node = Node() elements = [] self.expect('[') while not self.match(']'): <IF_STMT> self.lex() elements.append(null) else: if self.match('...'): restNode = Node() self.lex() rest = self.parseVariableIdentifier() elements.append(restNode.finishRestElement(rest)) break else: elements.append(self.parsePatternWithDefault()) if not self.match(']'): self.expect(',') self.expect(']') return node.finishArrayPattern(elements)","if self.match(','):"
"def _set_log_writer(self): if self.config['logging']: config = self.config['log_writer_config'] <IF_STMT> self.log_writer = LogWriter(**config) elif config['writer'] == 'tensorboard': self.log_writer = TensorBoardWriter(**config) else: raise ValueError(f""Unrecognized writer option: {config['writer']}"") else: self.log_writer = None",if config['writer'] == 'json':
"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): <IF_STMT> entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') if not len(head): entries.append(('all_comment', [line])) continue entries.append(('hostname', [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError('Multiple hostnames (%s) found!' % hostnames_found) return entries",if not len(line.strip()):
"def get_all_values(self, project): if isinstance(project, models.Model): project_id = project.id else: project_id = project if project_id not in self.__cache: cache_key = self._make_key(project_id) result = cache.get(cache_key) <IF_STMT> result = self.reload_cache(project_id) else: self.__cache[project_id] = result return self.__cache.get(project_id, {})",if result is None:
"def needed_libraries(self): for cmd in self.load_commands_of_type(12): tname = self._get_typename('dylib_command') dylib_command = cmd.cast(tname) name_addr = cmd.obj_offset + dylib_command.name dylib_name = self.obj_vm.read(name_addr, 256) <IF_STMT> idx = dylib_name.find('\x00') if idx != -1: dylib_name = dylib_name[:idx] yield dylib_name",if dylib_name:
"def compress(self, data_list): warn_untested() if data_list: <IF_STMT> error = self.error_messages['invalid_year'] raise forms.ValidationError(error) if data_list[0] in forms.fields.EMPTY_VALUES: error = self.error_messages['invalid_month'] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) day = monthrange(year, month)[1] return date(year, month, day) return None",if data_list[1] in forms.fields.EMPTY_VALUES:
"def put(self, obj, block=True, timeout=None): assert not self._closed if not self._sem.acquire(block, timeout): raise Full with self._notempty: with self._cond: <IF_STMT> self._start_thread() self._buffer.append(obj) self._unfinished_tasks.release() self._notempty.notify()",if self._thread is None:
"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) if not version: has_module = has_module_directory or exists(module_directory) else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile <IF_STMT> break return has_module",if has_module:
"def expanduser(path): if path[:1] == '~': c = path[1:2] <IF_STMT> return gethome() if c == os.sep: return asPyString(File(gethome(), path[2:]).getPath()) return path",if not c:
"def mock_touch(self, bearer, version=None, revision=None, **kwargs): if version: <IF_STMT> try: return self.versions[int(version) - 1] except (IndexError, ValueError): return None else: return None return file_models.FileVersion()",if self.versions:
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] <IF_STMT> wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",elif member[0] == 'wildcards':
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): <IF_STMT> if eline not in rline: return False elif not rline.endswith(eline): return False return True",if contains:
"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() <IF_STMT> self._update_value() elif event.GetKeyCode() == wx.WXK_DELETE: self.SetValue('') if event.GetKeyCode() != wx.WXK_RETURN: event.Skip()",elif event.GetKeyCode() == wx.WXK_RETURN:
"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) <IF_STMT> continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue if hasattr(module, 'aliases'): for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()","if module is None or not hasattr(module, attr):"
def eventIterator(): while True: yield eventmodule.wait() while True: event = eventmodule.poll() <IF_STMT> break else: yield event,if event.type == NOEVENT:
"def _get_state_without_padding(self, state_with_padding, padding): lean_state = {} for key, value in state_with_padding.items(): <IF_STMT> lean_length = value.numel() - padding lean_state[key] = value[:lean_length] else: lean_state[key] = value return lean_state",if torch.is_tensor(value):
"def _get_validate(data): """"""Retrieve items to validate, from single samples or from combined joint calls."""""" if data.get('vrn_file') and tz.get_in(['config', 'algorithm', 'validate'], data): return utils.deepish_copy(data) elif 'group_orig' in data: for sub in multi.get_orig_items(data): <IF_STMT> sub_val = utils.deepish_copy(sub) sub_val['vrn_file'] = data['vrn_file'] return sub_val return None",if 'validate' in sub['config']['algorithm']:
"def OnPopup(self, form, popup_handle): for num, action_name, menu_name, shortcut in self.actions: <IF_STMT> ida_kernwin.attach_action_to_popup(form, popup_handle, None) else: handler = command_handler_t(self, num, 2) desc = ida_kernwin.action_desc_t(action_name, menu_name, handler, shortcut) ida_kernwin.attach_dynamic_action_to_popup(form, popup_handle, desc)",if menu_name is None:
"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print('struct {}'.format(self.name)) for field in self.fields: <IF_STMT> offset = '0x??' else: offset = '0x{:02x}'.format(field.offset) print('{}+{} {} {}'.format(' ' * indent, offset, field.name, field.type)) if isinstance(field.type, Structure): field.type.show(indent + 1)",if field.offset is None:
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: <IF_STMT> if not operation_name: if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation","if isinstance(definition, ast.OperationDefinition):"
"def getSubMenu(self, callingWindow, context, mainItem, selection, rootMenu, i, pitem): msw = True if 'wxMSW' in wx.PlatformInfo else False self.context = context self.abilityIds = {} sub = wx.Menu() for ability in self.fighter.abilities: <IF_STMT> continue menuItem = self.addAbility(rootMenu if msw else sub, ability) sub.Append(menuItem) menuItem.Check(ability.active) return sub",if not ability.effect.isImplemented:
"def consume(self, event: Dict[str, Any]) -> None: with self.lock: logging.debug('Received missedmessage_emails event: %s', event) user_profile_id = event['user_profile_id'] <IF_STMT> self.batch_start_by_recipient[user_profile_id] = time.time() self.events_by_recipient[user_profile_id].append(event) self.ensure_timer()",if user_profile_id not in self.batch_start_by_recipient:
"def __init__(self, start_enabled=False, use_hardware=True): self._use_hardware = use_hardware if use_hardware: self._button = Button(BUTTON_GPIO_PIN) self._enabled = start_enabled <IF_STMT> self._button.when_pressed = self._enable",if not start_enabled:
"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) <IF_STMT> cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",if op.stage == OperandStage.map:
"def load_package(name, path): if os.path.isdir(path): extensions = machinery.SOURCE_SUFFIXES[:] + machinery.BYTECODE_SUFFIXES[:] for extension in extensions: init_path = os.path.join(path, '__init__' + extension) <IF_STMT> path = init_path break else: raise ValueError('{!r} is not a package'.format(path)) spec = util.spec_from_file_location(name, path, submodule_search_locations=[]) if name in sys.modules: return _exec(spec, sys.modules[name]) else: return _load(spec)",if os.path.exists(init_path):
def setup(level=None): from pipeline.logging import pipeline_logger as logger from pipeline.log.handlers import EngineLogHandler if level in set(logging._levelToName.values()): logger.setLevel(level) logging._acquireLock() try: for hdl in logger.handlers: <IF_STMT> break else: hdl = EngineLogHandler() hdl.setLevel(logger.level) logger.addHandler(hdl) finally: logging._releaseLock(),"if isinstance(hdl, EngineLogHandler):"
"def find_approximant(x): c = 0.0001 it = sympy.ntheory.continued_fraction_convergents(sympy.ntheory.continued_fraction_iterator(x)) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 if abs(i - x) <= tol: return i <IF_STMT> break return x",if tol < machine_epsilon:
"def resolve(self, debug: bool=False, silent: bool=False, level: Optional[int]=None) -> bool: if silent: spinner = nullcontext(type('Mock', (), {})) else: spinner = yaspin(text='resolving...') with spinner as spinner: while True: resolved = self._resolve(debug=debug, silent=silent, level=level, spinner=spinner) <IF_STMT> continue self.graph.clear() return resolved",if resolved is None:
"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() if name == 'MOV': <IF_STMT> return 'LSR' elif instr.mnemonic.startswith('lsl'): return 'LSL' elif instr.mnemonic.startswith('asr'): return 'ASR' return OP_NAME_MAP.get(name, name)",if instr.mnemonic.startswith('lsr'):
"def run_all(rule_list, defined_variables, defined_actions, stop_on_first_trigger=False): rule_was_triggered = False for rule in rule_list: result = run(rule, defined_variables, defined_actions) if result: rule_was_triggered = True <IF_STMT> return True return rule_was_triggered",if stop_on_first_trigger:
"def get_filters(self, request): filter_specs = [] if self.lookup_opts.admin.list_filter and (not self.opts.one_to_one_field): filter_fields = [self.lookup_opts.get_field(field_name) for field_name in self.lookup_opts.admin.list_filter] for f in filter_fields: spec = FilterSpec.create(f, request, self.params, self.model) <IF_STMT> filter_specs.append(spec) return (filter_specs, bool(filter_specs))",if spec and spec.has_output():
def get_type(type_ref): kind = type_ref.get('kind') if kind == TypeKind.LIST: item_ref = type_ref.get('ofType') <IF_STMT> raise Exception('Decorated type deeper than introspection query.') return GraphQLList(get_type(item_ref)) elif kind == TypeKind.NON_NULL: nullable_ref = type_ref.get('ofType') if not nullable_ref: raise Exception('Decorated type deeper than introspection query.') return GraphQLNonNull(get_type(nullable_ref)) return get_named_type(type_ref['name']),if not item_ref:
"def _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers): if method == 'POST': body = json.loads(body) <IF_STMT> return self.test_response(httplib.ACCEPTED, '') else: data = '{""error_name"":""bad destination"", ""errors"": [""Bad destination""]}' return self.test_response(httplib.BAD_REQUEST, data)",if 'destination' in body:
"def _get_prefixed_values(data, prefix): """"""Collect lines which start with prefix; with trimming"""""" matches = [] for line in data.splitlines(): line = line.strip() <IF_STMT> match = line[len(prefix):] match = match.strip() matches.append(match) return matches",if line.startswith(prefix):
"def _power_exact(y, xc, yc, xe): yc, ye = (y.int, y.exp) while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 <IF_STMT> return None exponent = xe * 10 ** ye if y and xe: xc = exponent else: xc = 0 return 5",if ye < 0:
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): <IF_STMT> values.append(value) self.stack = values",if len(values) + 1 < idx:
"def toggleFactorReload(self, value=None): self.serviceFittingOptions['useGlobalForceReload'] = value if value is not None else not self.serviceFittingOptions['useGlobalForceReload'] fitIDs = set() for fit in set(self._loadedFits): if fit is None: continue <IF_STMT> fit.factorReload = self.serviceFittingOptions['useGlobalForceReload'] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",if fit.calculated:
"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <IF_STMT> constant_init(m, 1) elif isinstance(m, nn.Linear): normal_init(m, std=0.01)","elif isinstance(m, nn.BatchNorm2d):"
"def _unzip_file(self, filepath, ext): try: <IF_STMT> zf = zipfile.ZipFile(filepath) zf.extractall(os.path.dirname(filepath)) zf.close() elif ext == '.tar': tf = tarfile.open(filepath) tf.extractall(os.path.dirname(filepath)) tf.close() except Exception as e: raise ValueError('Error reading file %r!\n%s' % (filepath, e))",if ext == '.zip':
"def add_multiple_tasks(data, parent): data = json.loads(data) new_doc = {'doctype': 'Task', 'parent_task': parent if parent != 'All Tasks' else ''} new_doc['project'] = frappe.db.get_value('Task', {'name': parent}, 'project') or '' for d in data: <IF_STMT> continue new_doc['subject'] = d.get('subject') new_task = frappe.get_doc(new_doc) new_task.insert()",if not d.get('subject'):
"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode('ascii', 'ignore')) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: <IF_STMT> continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue if kwdSndx == soundex(key.encode('ascii', 'ignore')): matchesappend(key) return _sortKeywords(keyword, matches)",if key in seenDict:
"def visit_If(self, node): self.newline() self.write('if ') self.visit(node.test) self.write(':') self.body(node.body) while True: else_ = node.orelse <IF_STMT> node = else_[0] self.newline() self.write('elif ') self.visit(node.test) self.write(':') self.body(node.body) else: self.newline() self.write('else:') self.body(else_) break","if len(else_) == 1 and isinstance(else_[0], If):"
"def _eyeLinkHardwareAndSoftwareVersion(self): try: tracker_software_ver = 0 eyelink_ver = self._eyelink.getTrackerVersion() <IF_STMT> tvstr = self._eyelink.getTrackerVersionString() vindex = tvstr.find('EYELINK CL') tracker_software_ver = int(float(tvstr[vindex + len('EYELINK CL'):].strip())) return (eyelink_ver, tracker_software_ver) except Exception: print2err('EYELINK Error during _eyeLinkHardwareAndSoftwareVersion:') printExceptionDetailsToStdErr() return EyeTrackerConstants.EYETRACKER_ERROR",if eyelink_ver == 3:
"def execute(self, context): for monad in context.blend_data.node_groups: if monad.bl_idname == 'SverchGroupTreeType': <IF_STMT> try: monad.update_cls() except Exception as err: print(err) print('{} group class could not be created'.format(monad.name)) return {'FINISHED'}","if not getattr(bpy.types, monad.cls_bl_idname, None):"
"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: if list_str[i] in set_value: return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) el<IF_STMT> return False return True",if dict[pattern[i]] != list_str[i]:
"def decorator_handle(tokens): """"""Process decorators."""""" defs = [] decorates = [] for i, tok in enumerate(tokens): if 'simple' in tok and len(tok) == 1: decorates.append('@' + tok[0]) <IF_STMT> varname = decorator_var + '_' + str(i) defs.append(varname + ' = ' + tok[0]) decorates.append('@' + varname) else: raise CoconutInternalException('invalid decorator tokens', tok) return '\n'.join(defs + decorates) + '\n'",elif 'test' in tok and len(tok) == 1:
"def wait_impl(self, cpid): for i in range(10): spid, status, rusage = os.wait3(os.WNOHANG) <IF_STMT> break time.sleep(1.0) self.assertEqual(spid, cpid) self.assertEqual(status, 0, 'cause = %d, exit = %d' % (status & 255, status >> 8)) self.assertTrue(rusage)",if spid == cpid:
"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): <IF_STMT> assert 2500 - 500 < count < 2500 + 500 elif val == 1: assert 7500 - 500 < count < 7500 + 500 else: assert False",if val == 0:
"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: if self.stopframe and frame.f_code.co_flags & CO_GENERATOR: return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None <IF_STMT> raise BdbQuit if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",if self.quitting:
"def mouse(self, button, mods, x, y): if button == 1: for i in range(4): <IF_STMT> self.hit = i elif button == -1: self.hit = None elif self.hit != None: self.coords[self.hit] = (x, y) self.view.dirty()","if hypot(x - self.coords[i][0], y - self.coords[i][1]) < 4:"
"def __init__(self, *commands): self.all_cmds = list(map(lambda cmd: cmd[0] if isinstance(cmd, list) else cmd, commands)) for command in commands: self.cmd = command if isinstance(command, list) else [command] self.cmd_path = pwndbg.which.which(self.cmd[0]) <IF_STMT> break",if self.cmd_path:
"def _recv_obj(self, suppress_error=False): """"""Receive a (picklable) object"""""" if self.conn.closed: raise OSError('handle is closed') try: buf = self.conn.recv_bytes() except (ConnectionError, EOFError) as e: <IF_STMT> return logger.debug('receive has failed', exc_info=e) try: self._set_remote_close_cause(e) raise PipeShutdownError() finally: self._close() obj = RemoteObjectUnpickler.loads(buf, self) logger.debug('received %r', obj) return obj",if suppress_error:
"def act(self, obs): with chainer.no_backprop_mode(): batch_obs = self.batch_states([obs], self.xp, self.phi) action_distrib = self.model(batch_obs) <IF_STMT> return chainer.cuda.to_cpu(action_distrib.most_probable.array)[0] else: return chainer.cuda.to_cpu(action_distrib.sample().array)[0]",if self.act_deterministically:
"def _classify(nodes_by_level): missing, invalid, downloads = ([], [], []) for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) elif node.binary == BINARY_INVALID: invalid.append(node) <IF_STMT> downloads.append(node) return (missing, invalid, downloads)","elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD):"
"def persist(self, *_): for key, obj in self._objects.items(): try: state = obj.get_state() <IF_STMT> continue md5 = hashlib.md5(state).hexdigest() if self._last_state.get(key) == md5: continue self._persist_provider.store(key, state) except Exception as e: system_log.exception('PersistHelper.persist fail') else: self._last_state[key] = md5",if not state:
"def enter(self, doc, **kwds): """"""Enters the mode, arranging for necessary grabs ASAP"""""" super(ColorPickMode, self).enter(doc, **kwds) if self._started_from_key_press: doc = self.doc tdw = self.doc.tdw t, x, y = doc.get_last_event_info(tdw) <IF_STMT> self._pick_color_mode(tdw, x, y, self._pickmode) self._start_drag_on_next_motion_event = True self._needs_drag_start = True","if None not in (x, y):"
"def on_profiles_loaded(self, profiles): cb = self.builder.get_object('cbProfile') model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() <IF_STMT> continue if name.endswith('.sccprofile'): name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",if name.endswith('.mod'):
"def subprocess_post_check(completed_process: subprocess.CompletedProcess, raise_error: bool=True) -> None: if completed_process.returncode: if completed_process.stdout is not None: print(completed_process.stdout, file=sys.stdout, end='') <IF_STMT> print(completed_process.stderr, file=sys.stderr, end='') if raise_error: raise PipxError(f""{' '.join([str(x) for x in completed_process.args])!r} failed"") else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process.stderr is not None:
"def test_connect(ipaddr, port, device, partition, method, path, headers=None, query_string=None): if path == '/a': for k, v in headers.iteritems(): <IF_STMT> break else: test_errors.append('%s: %s not in %s' % (test_header, test_value, headers))",if k.lower() == test_header.lower() and v == test_value:
"def test_stat_result_pickle(self): result = os.stat(self.fname) for proto in range(pickle.HIGHEST_PROTOCOL + 1): p = pickle.dumps(result, proto) self.assertIn(b'stat_result', p) <IF_STMT> self.assertIn(b'cos\nstat_result\n', p) unpickled = pickle.loads(p) self.assertEqual(result, unpickled)",if proto < 4:
def run_sql(sql): table = sql.split(' ')[5] logger.info('Updating table {}'.format(table)) with transaction.atomic(): with connection.cursor() as cursor: cursor.execute(sql) rows = cursor.fetchall() <IF_STMT> raise Exception('Sentry notification that {} is migrated'.format(table)),if not rows:
"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x <IF_STMT> self.box[2] = x if y < self.box[1]: self.box[1] = y if y > self.box[3]: self.box[3] = y",if x > self.box[2]:
"def _packageFocusOutViaKeyPress(self, row, column, txt): if txt: self._set_current_cell(row + 1, column) else: widget = self.cellWidget(row + 1, column) <IF_STMT> self._delete_cell(row, column) new_request = self.get_request() self.context_model.set_request(new_request) self._update_request_column(column, self.context_model)","if widget and isinstance(widget, PackageSelectWidget):"
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith('win'): output = output.replace('\\\n', '') environ = {} for line in output.splitlines(0): line = line.rstrip() <IF_STMT> continue item = _ParseBashEnvStr(line) if item: environ[item[0]] = item[1] return environ",if not line:
"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() <IF_STMT> record = None except KeyError: record = None if not record: record = {'r': 'unknown', 'dns': {}, 'g': 1, 'query_count': 0} return record",if time_now - record['update'] > self.ttl:
"def test_filehash(self): """"""tests the hashes of the files in data/"""""" fp = self.get_data_path() for fn in os.listdir(fp): <IF_STMT> continue expected_hash = fn fullp = os.path.join(fp, fn) output = self.run_command('sha1sum ' + fullp, exitcode=0) result = output.split(' ')[0] self.assertEqual(result, expected_hash)",if '.' in fn:
"def test_new_vs_reference_code_stream_read_during_iter(read_idx, read_len, bytecode): reference = SlowCodeStream(bytecode) latest = CodeStream(bytecode) for index, (actual, expected) in enumerate(zip(latest, reference)): assert actual == expected if index == read_idx: readout_actual = latest.read(read_len) readout_expected = reference.read(read_len) assert readout_expected == readout_actual <IF_STMT> assert latest.program_counter >= len(reference) else: assert latest.program_counter == reference.program_counter",if reference.program_counter >= len(reference):
"def setup_logging(): try: logconfig = config.get('logging_config_file') <IF_STMT> logging.config.fileConfig(logconfig, disable_existing_loggers=False) logger.info('logging initialized') logger.debug('debug') except Exception as e: print('Unable to set logging configuration:', str(e), file=sys.stderr) raise",if logconfig and os.path.exists(logconfig):
def all_words(filename): start_char = True for c in characters(filename): <IF_STMT> word = '' if c.isalnum(): word = c.lower() start_char = False else: pass elif c.isalnum(): word += c.lower() else: start_char = True yield word,if start_char == True:
"def _get_nonce(self, url, new_nonce_url): if not self._nonces: logger.debug('Requesting fresh nonce') <IF_STMT> response = self.head(url) else: response = self._check_response(self.head(new_nonce_url), content_type=None) self._add_nonce(response) return self._nonces.pop()",if new_nonce_url is None:
"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): if line.startswith(comment): <IF_STMT> continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",if line[len(comment):].lstrip().startswith(comment):
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: if isinstance(arg, bool): gvariant += ' {}'.format(str(arg).lower()) <IF_STMT> gvariant += f' {arg}' elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","elif isinstance(arg, (int, float)):"
"def _SkipGroup(buffer, pos, end): """"""Skip sub-group.  Returns the new position."""""" while 1: tag_bytes, pos = ReadTag(buffer, pos) new_pos = SkipField(buffer, pos, end, tag_bytes) <IF_STMT> return pos pos = new_pos",if new_pos == -1:
"def update_participants(self, refresh=True): for participant in list(self.participants_dict): if participant is None or participant == self.simulator_config.broadcast_part: continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: <IF_STMT> self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",if participant in self.participants_dict:
"def feature_reddit(layer_data, graph): feature = {} times = {} indxs = {} for _type in layer_data: if len(layer_data[_type]) == 0: continue idxs = np.array(list(layer_data[_type].keys())) tims = np.array(list(layer_data[_type].values()))[:, 1] feature[_type] = np.array(list(graph.node_feature[_type].loc[idxs, 'emb']), dtype=np.float) times[_type] = tims indxs[_type] = idxs <IF_STMT> attr = feature[_type] return (feature, times, indxs, attr)",if _type == 'def':
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: <IF_STMT> tts[name] = '%ssort' % name if tag.internal: tts['~%s' % name] = '~%ssort' % name return tts",if tag.user:
"def max_radius(iterator): radius_result = dict() for k, v in iterator: if v[0] not in radius_result: radius_result[v[0]] = v[1] <IF_STMT> radius_result[v[0]] = v[1] return radius_result",elif v[1] >= radius_result[v[0]]:
"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join(constant.profile['APPDATA'], u'..', u'Local', u'Microsoft', u'Vault') if os.path.exists(main_vault_directory): for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault(os.path.join(main_vault_directory, vault_directory)) <IF_STMT> pwd_found.append(cred) return pwd_found",if cred:
"def disconnect_sync(self, connection, close_connection=False): key = id(connection) ts = self.in_use.pop(key) if close_connection: self.connections_map.pop(key) self._connection_close_sync(connection) el<IF_STMT> self.connections_map.pop(key) self._connection_close_sync(connection) else: with self._lock_sync: heapq.heappush(self.connections_sync, (ts, key))",if self.stale_timeout and self.is_stale(ts):
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) <IF_STMT> self._populate_number(element, k, v)","elif type(v) in [int, float, long, complex]:"
"def readframes(self, nframes): if self._ssnd_seek_needed: self._ssnd_chunk.seek(0) dummy = self._ssnd_chunk.read(8) pos = self._soundpos * self._framesize <IF_STMT> self._ssnd_chunk.seek(pos + 8) self._ssnd_seek_needed = 0 if nframes == 0: return '' data = self._ssnd_chunk.read(nframes * self._framesize) if self._convert and data: data = self._convert(data) self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth) return data",if pos:
"def target_glob(tgt, hosts): ret = {} for host in hosts: <IF_STMT> ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {})) ret[host].update({'host': host}) if __opts__.get('ssh_user'): ret[host].update({'user': __opts__['ssh_user']}) return ret","if fnmatch.fnmatch(tgt, host):"
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] <IF_STMT> return attval.value_callback() return attval.value",if attval.value_callback:
"def remove_property(self, key): with self.secure() as config: keys = key.split('.') current_config = config for i, key in enumerate(keys): <IF_STMT> return if i == len(keys) - 1: del current_config[key] break current_config = current_config[key]",if key not in current_config:
"def _class_browser(parent): try: file = __file__ except NameError: file = sys.argv[0] <IF_STMT> file = sys.argv[1] else: file = sys.argv[0] dir, file = os.path.split(file) name = os.path.splitext(file)[0] flist = PyShell.PyShellFileList(parent) global file_open file_open = flist.open ClassBrowser(flist, name, [dir], _htest=True)",if sys.argv[1:]:
"def get_only_text_part(self, msg): count = 0 only_text_part = None for part in msg.walk(): if part.is_multipart(): continue count += 1 mimetype = part.get_content_type() or 'text/plain' <IF_STMT> return False else: only_text_part = part return only_text_part",if mimetype != 'text/plain' or count != 1:
"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or '' for line in commit_msg.splitlines(): parts = line.strip('# ').split(':', 1) key, val = parts if len(parts) > 1 else (parts[0], '') if key == 'CI_KEEP_ALIVE': ci_names = val.replace(',', ' ').lower().split() if val else [] <IF_STMT> result = True return result",if len(ci_names) == 0 or ci.lower() in ci_names:
"def _calc_block_io(self, blkio): """"""Calculate block IO stats."""""" for stats in blkio['io_service_bytes_recursive']: if stats['op'] == 'Read': self._blk_read += stats['value'] <IF_STMT> self._blk_write += stats['value']",elif stats['op'] == 'Write':
"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('Oracle backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value)",if settings.USE_TZ:
"def load_state_dict(self, state_dict): for module_name, module_state_dict in state_dict.items(): if module_name in self.module_pool: <IF_STMT> self.module_pool[module_name].module.load_state_dict(module_state_dict) else: self.module_pool[module_name].load_state_dict(module_state_dict) else: logging.info(f'Missing {module_name} in module_pool, skip it..')",if self.config['dataparallel']:
"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): if i >= min(len(scales), len(vidxs) // 2): break scale = scales[i] <IF_STMT> vidx1, vidx2 = (vidxs[i * 2], vidxs[i * 2 + 1]) scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData",if not math.isnan(scale):
"def __init__(self, factors, contrast_matrices, num_columns): self.factors = tuple(factors) factor_set = frozenset(factors) if not isinstance(contrast_matrices, dict): raise ValueError('contrast_matrices must be dict') for factor, contrast_matrix in six.iteritems(contrast_matrices): <IF_STMT> raise ValueError('Unexpected factor in contrast_matrices dict') if not isinstance(contrast_matrix, ContrastMatrix): raise ValueError('Expected a ContrastMatrix, not %r' % (contrast_matrix,)) self.contrast_matrices = contrast_matrices if not isinstance(num_columns, six.integer_types): raise ValueError('num_columns must be an integer') self.num_columns = num_columns",if factor not in factor_set:
"def app(scope, receive, send): while True: message = await receive() <IF_STMT> await send({'type': 'websocket.accept'}) elif message['type'] == 'websocket.receive': pass elif message['type'] == 'websocket.disconnect': break",if message['type'] == 'websocket.connect':
"def value__set(self, value): for i, (option, checked) in enumerate(self.options): <IF_STMT> self.selectedIndex = i break else: raise ValueError('Option %r not found (from %s)' % (value, ', '.join([repr(o) for o, c in self.options])))",if option == str(value):
"def init_links(self): links = LinkCallback.find_links(self) callbacks = [] for link, src_plot, tgt_plot in links: cb = Link._callbacks['bokeh'][type(link)] <IF_STMT> continue callbacks.append(cb(self.root, link, src_plot, tgt_plot)) return callbacks",if src_plot is None or (link._requires_target and tgt_plot is None):
"def _validate_scalar_extensions(self) -> List[str]: errors = [] for extension in [x for x in self.extensions if isinstance(x, GraphQLScalarTypeExtension)]: extended = self.type_definitions.get(extension.name) ext_errors = _validate_extension(extended, extension.name, GraphQLScalarType, 'SCALAR') errors.extend(ext_errors) <IF_STMT> errors.extend(_validate_extension_directives(extension, extended, 'SCALAR')) return errors",if not ext_errors:
"def copy_tcltk(src, dest, symlink): """"""copy tcl/tk libraries on Windows (issue #93)"""""" for libversion in ('8.5', '8.6'): for libname in ('tcl', 'tk'): srcdir = join(src, 'tcl', libname + libversion) destdir = join(dest, 'tcl', libname + libversion) <IF_STMT> copyfileordir(srcdir, destdir, symlink)",if os.path.exists(srcdir) and (not os.path.exists(destdir)):
"def parse(self, response): try: content = response.content.decode('utf-8', 'ignore') content = json.loads(content, strict=False) except: self.logger.error('Fail to parse the response in json format') return for item in content['data']: if 'objURL' in item: img_url = self._decode_url(item['objURL']) <IF_STMT> img_url = item['hoverURL'] else: continue yield dict(file_url=img_url)",elif 'hoverURL' in item:
"def check_and_reload(self): for table_name, table_version in self._table_versions.items(): table = self.app.tool_data_tables.get(table_name, None) <IF_STMT> return self.reload_genomes()",if table is not None and (not table.is_current_version(table_version)):
"def _get_query_defaults(self, query_defns): defaults = {} for k, v in query_defns.items(): try: <IF_STMT> defaults[k] = self._get_default_obj(v['schema']) else: defaults[k] = v['schema']['default'] except KeyError: pass return defaults",if v['schema']['type'] == 'object':
"def ftp_login(host, port, username=None, password=None, anonymous=False): ret = False try: ftp = ftplib.FTP() ftp.connect(host, port, timeout=6) <IF_STMT> ftp.login() else: ftp.login(username, password) ret = True ftp.quit() except Exception: pass return ret",if anonymous:
def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value if 's' in thisDynamic: thisDynamic = thisDynamic[1:] if thisDynamic[-1] == 'z': thisDynamic = thisDynamic[:-1] <IF_STMT> return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None],if thisDynamic in dynamicStrToScalar:
"def processCoords(coords): newcoords = deque() for x, y, z in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) <IF_STMT> continue nx, ny, nz = p if level.blockAt(nx, ny, nz) == 0: level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords",if p not in box:
"def _set_property(self, target_widget, pname, value): if pname == 'text': wstate = str(target_widget['state']) <IF_STMT> target_widget['state'] = 'normal' target_widget.delete('0', tk.END) target_widget.insert('0', value) target_widget['state'] = wstate else: super(EntryBaseBO, self)._set_property(target_widget, pname, value)",if wstate != 'normal':
"def teardown(): try: time.sleep(1) except KeyboardInterrupt: return while launchers: p = launchers.pop() <IF_STMT> try: p.stop() except Exception as e: print(e) pass if p.poll() is None: try: time.sleep(0.25) except KeyboardInterrupt: return if p.poll() is None: try: print('cleaning up test process...') p.signal(SIGKILL) except: print(""couldn't shutdown process: "", p)",if p.poll() is None:
"def checkAndRemoveDuplicate(self, node): for bucket in self.buckets: for n in bucket.getNodes(): <IF_STMT> self.removeContact(n)","if (n.ip, n.port) == (node.ip, node.port) and n.id != node.id:"
def toString(): flags = u'' try: if this.glob: flags += u'g' <IF_STMT> flags += u'i' if this.multiline: flags += u'm' except: pass v = this.value if this.value else '(?:)' return u'/%s/' % v + flags,if this.ignore_case:
"def import_submodules(package_name): package = sys.modules[package_name] results = {} for loader, name, is_pkg in pkgutil.iter_modules(package.__path__): full_name = package_name + '.' + name module = importlib.import_module(full_name) setattr(sys.modules[__name__], name, module) results[full_name] = module if is_pkg: valid_pkg = import_submodules(full_name) <IF_STMT> results.update(valid_pkg) return results",if valid_pkg:
"def _call(self, cmd): what = cmd['command'] if what == 'list': name = cmd['properties'].get('name') <IF_STMT> return {'watchers': ['one', 'two', 'three']} return {'pids': [123, 456]} elif what == 'dstats': return {'info': {'pid': 789}} elif what == 'listsockets': return {'status': 'ok', 'sockets': [{'path': self._unix, 'fd': 5, 'name': 'XXXX', 'backlog': 2048}], 'time': 1369647058.967524} raise NotImplementedError(cmd)",if name is None:
"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease): <IF_STMT> continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)","if xlib.XFilterEvent(e, e.xany.window):"
"def translate(self, line): parsed = self.RE_LINE_PARSER.match(line) if parsed: value = parsed.group(3) stage = parsed.group(1) <IF_STMT> return '\n# HTTP Request:\n' + self.stripslashes(value) elif stage == 'reply': return '\n\n# HTTP Response:\n' + self.stripslashes(value) elif stage == 'header': return value + '\n' else: return value return line",if stage == 'send':
def toString(): flags = u'' try: <IF_STMT> flags += u'g' if this.ignore_case: flags += u'i' if this.multiline: flags += u'm' except: pass v = this.value if this.value else '(?:)' return u'/%s/' % v + flags,if this.glob:
"def __exit__(self, *exc_info): super(WarningsChecker, self).__exit__(*exc_info) if all((a is None for a in exc_info)): if self.expected_warning is not None: <IF_STMT> __tracebackhide__ = True pytest.fail('DID NOT WARN')",if not any((r.category in self.expected_warning for r in self)):
"def run(self): for k, v in iteritems(self.objs): if k.startswith('_'): continue <IF_STMT> if v['email'] == '': v['email'] = None if v['ip'] == '0.0.0.0': v['ip'] = None return self.objs",if v['_class'] == 'User':
"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): if i <= start_after: continue <IF_STMT> self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",if i == 2 and self.count < 1:
def check(self): tcp_client = self.tcp_create() if tcp_client.connect(): tcp_client.send(b'ABCDE') response = tcp_client.recv(5) tcp_client.close() if response: <IF_STMT> self.endianness = '>' elif response.startswith(b'ScMM'): self.endianness = '<' return True return False,if response.startswith(b'MMcS'):
"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): if src_root != src_dir: rel_root = os.path.relpath(src_root, src_dir) else: rel_root = '' if skip_variables and rel_root.startswith('variables'): continue dst_root = os.path.join(dst_dir, rel_root) <IF_STMT> os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if not os.path.exists(dst_root):
"def _set_hostport(self, host, port): if port is None: i = host.rfind(':') j = host.rfind(']') <IF_STMT> try: port = int(host[i + 1:]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1:]) host = host[:i] else: port = self.default_port if host and host[0] == '[' and (host[-1] == ']'): host = host[1:-1] self.host = host self.port = port",if i > j:
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] <IF_STMT> field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",if key == 'nw_src':
"def _clear_storage(): """"""Clear old files from storage."""""" hacs = get_hacs() storagefiles = ['hacs'] for s_f in storagefiles: path = f'{hacs.core.config_path}/.storage/{s_f}' <IF_STMT> hacs.log.info(f'Cleaning up old storage file {path}') os.remove(path)",if os.path.isfile(path):
"def action_delete(self, ids): try: count = 0 for pk in ids: <IF_STMT> count += 1 flash(ngettext('Record was successfully deleted.', '%(count)s records were successfully deleted.', count, count=count), 'success') except Exception as ex: flash(gettext('Failed to delete records. %(error)s', error=str(ex)), 'error')",if self.delete_model(self.get_one(pk)):
"def test_inclusion(all_values): for values in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_XXX'}, {'guid_2'}]: test_predicate = in_set(values, 'volume_guid') included_values = set() for val in all_values: <IF_STMT> included_values.add(val) assert included_values == all_values.intersection(values)",if test_predicate.do_include({'volume_guid': val}):
"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = mod_attr_path.split('#') if '#' in mod_attr_path else (mod_attr_path, '') full_mod_path = '{}.{}'.format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) if attr_path: for part in attr_path.split('.'): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: <IF_STMT> return None raise ex",if checked:
"def __exit__(self, exc_type, exc_val, exc_tb): if self.fusefat is not None: self.fusefat.send_signal(signal.SIGINT) for count in range(10): time.sleep(0.1) <IF_STMT> break else: self.fusefat.terminate() time.sleep(self.delay) assert not os.path.exists(self.canary) self.dev_null.close() shutil.rmtree(self.tmpdir)",if self.fusefat.poll() is not None:
"def check_context_processors(output): with output.section('Context processors') as section: processors = list(chain(*[template['OPTIONS'].get('context_processors', []) for template in settings.TEMPLATES])) required_processors = ('cms.context_processors.cms_settings',) for processor in required_processors: <IF_STMT> section.error('%s context processor must be in TEMPLATES option context_processors' % processor)",if processor not in processors:
"def test_converters(self): response = self._get('datatypes/converters') self._assert_status_code_is(response, 200) converters_list = response.json() found_fasta_to_tabular = False for converter in converters_list: self._assert_has_key(converter, 'source', 'target', 'tool_id') <IF_STMT> found_fasta_to_tabular = True assert found_fasta_to_tabular",if converter['source'] == 'fasta' and converter['target'] == 'tabular':
"def remove_pid(self, watcher, pid): if pid in self._pids[watcher]: logger.debug('Removing %d from %s' % (pid, watcher)) self._pids[watcher].remove(pid) <IF_STMT> logger.debug('Stopping the periodic callback for {0}'.format(watcher)) self._callbacks[watcher].stop()",if len(self._pids[watcher]) == 0:
"def _fc_layer(self, sess, bottom, name, trainable=True, relu=True): with tf.variable_scope(name) as scope: shape = bottom.get_shape().as_list() dim = 1 for d in shape[1:]: dim *= d x = tf.reshape(bottom, [-1, dim]) weight = self._get_fc_weight(sess, name, trainable=trainable) bias = self._get_bias(sess, name, trainable=trainable) fc = tf.nn.bias_add(tf.matmul(x, weight), bias) <IF_STMT> fc = tf.nn.relu(fc) return fc",if relu:
"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: if root_path: config_root_path = drive.get('root_path') <IF_STMT> return drive elif volume_guid_path: config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",if config_root_path and root_path == config_root_path:
"def rewire_init(expr): new_args = [] if expr[0] == HySymbol('setv'): pairs = expr[1:] while len(pairs) > 0: k, v = (pairs.pop(0), pairs.pop(0)) <IF_STMT> v.append(HySymbol('None')) new_args.append(k) new_args.append(v) expr = HyExpression([HySymbol('setv')] + new_args).replace(expr) return expr",if k == HySymbol('__init__'):
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue <IF_STMT> doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))",if child.tagName == 'Directory':
"def _v2_common(self, cfg): LOG.debug('v2_common: handling config:\n%s', cfg) if 'nameservers' in cfg: search = cfg.get('nameservers').get('search', []) dns = cfg.get('nameservers').get('addresses', []) name_cmd = {'type': 'nameserver'} <IF_STMT> name_cmd.update({'search': search}) if len(dns) > 0: name_cmd.update({'addresses': dns}) LOG.debug('v2(nameserver) -> v1(nameserver):\n%s', name_cmd) self.handle_nameserver(name_cmd)",if len(search) > 0:
"def __start_element_handler(self, name, attrs): if name == 'mime-type': if self.type: for extension in self.extensions: self[extension] = self.type self.type = attrs['type'].lower() self.extensions = [] elif name == 'glob': pattern = attrs['pattern'] <IF_STMT> self.extensions.append(pattern[1:].lower())",if pattern.startswith('*.'):
"def get_attr_by_data_model(self, dmodel, exclude_record=False): if exclude_record: return list(filter(lambda x: x.data_model == dmodel and x.value == '' <IF_STMT> else False, self._inferred_intent)) else: return list(filter(lambda x: x.data_model == dmodel and x.value == '' if hasattr(x, 'data_model') else False, self._inferred_intent))","if x.attribute != 'Record' and hasattr(x, 'data_model')"
"def general(metadata, value): if metadata.get('commands') and value: if not metadata.get('nargs'): v = quote(value) else: v = value return u'{0} {1}'.format(metadata['commands'][0], v) elif not value: return None <IF_STMT> return quote(value) else: return value",elif not metadata.get('nargs'):
"def get_images(self): images = [] try: tag = MP4(self['~filename']) except Exception: return [] for cover in tag.get('covr', []): <IF_STMT> mime = 'image/jpeg' elif cover.imageformat == MP4Cover.FORMAT_PNG: mime = 'image/png' else: mime = 'image/' f = get_temp_cover_file(cover) images.append(EmbeddedImage(f, mime)) return images",if cover.imageformat == MP4Cover.FORMAT_JPEG:
"def run_cmd(self, util, value): state = util.state if not state.argument_supplied: state.argument_supplied = True if value == 'by_four': state.argument_value = 4 <IF_STMT> state.argument_negative = True else: state.argument_value = value elif value == 'by_four': state.argument_value *= 4 elif isinstance(value, int): state.argument_value *= 10 state.argument_value += value elif value == 'negative': state.argument_value = -state.argument_value",elif value == 'negative':
"def finish_character_data(self): if self.character_data: <IF_STMT> line, column = self.character_pos token = XmlToken(XML_CHARACTER_DATA, self.character_data, None, line, column) self.tokens.append(token) self.character_data = ''",if not self.skip_ws or not self.character_data.isspace():
"def check_syntax(filename, raise_error=False): """"""Return True if syntax is okay."""""" with autopep8.open_with_encoding(filename) as input_file: try: compile(input_file.read(), '<string>', 'exec', dont_inherit=True) return True except (SyntaxError, TypeError, UnicodeDecodeError): <IF_STMT> raise else: return False",if raise_error:
"def write(self, file): if not self._been_written: self._been_written = True for attribute, value in self.__dict__.items(): <IF_STMT> self.write_recursive(value, file) w = file.write w('\t%s = {\n' % self._id) w('\t\tisa = %s;\n' % self.__class__.__name__) for attribute, value in self.__dict__.items(): if attribute[0] != '_': w('\t\t%s = %s;\n' % (attribute, self.tostring(value))) w('\t};\n\n')",if attribute[0] != '_':
"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() if name is not None: key.name = name <IF_STMT> key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",if metadata is not None:
"def fill_buf(self, db, len_=None): with open('/dev/urandom', 'rb') as rfh: first = True for id_, in db.query('SELECT id FROM test'): if len_ is None and first: val = b'' first = False <IF_STMT> val = rfh.read(random.randint(0, 140)) else: val = rfh.read(len_) db.execute('UPDATE test SET buf=? WHERE id=?', (val, id_))",elif len_ is None:
"def load_category_from_parser(self, parser): for cate in parser.keys(): id = parser.get_id(cate) <IF_STMT> self._data['cates'][id] = 0 else: self._data['cates'][id] = self.count_unread(id) self._is_init = False self.save()",if self._is_init:
"def after_insert(self): if self.prescription: frappe.db.set_value('Lab Prescription', self.prescription, 'lab_test_created', 1) <IF_STMT> self.invoiced = True if not self.lab_test_name and self.template: self.load_test_from_template() self.reload()","if frappe.db.get_value('Lab Prescription', self.prescription, 'invoiced'):"
"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): <IF_STMT> continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True if not add: continue missing.append((source.context, source.source, '')) if missing: self.add_units(None, missing)",if 'terminology' not in source.all_flags:
def refresh(self): if self._obj: base = self._db.get_media_from_handle(self._obj.get_reference_handle()) <IF_STMT> self._title = base.get_description() self._value = base.get_path(),if base:
"def _set_parse_context(self, tag, tag_attrs): if not self._wb_parse_context: if tag == 'style': self._wb_parse_context = 'style' <IF_STMT> if self._allow_js_type(tag_attrs): self._wb_parse_context = 'script'",elif tag == 'script':
"def can_read(self): if hasattr(self.file, '__iter__'): iterator = iter(self.file) head = next(iterator, None) if head is None: self.repaired = [] return True <IF_STMT> self.repaired = itertools.chain([head], iterator) return True else: raise IOSourceError('Could not open source: %r (mode: %r)' % (self.file, self.options['mode'])) return False","if isinstance(head, str):"
"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get('headers') is not None: if kwargs['headers'].get('user-agent'): <IF_STMT> kwargs['headers']['user-agent'] = f""{user_agent} {kwargs['headers']['user-agent']}"" else: kwargs['headers']['user-agent'] = user_agent else: kwargs['headers'] = {'user-agent': user_agent} return request_method(*args, **kwargs)",if user_agent not in kwargs['headers']['user-agent']:
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper <IF_STMT> ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._naive or not self._joins or self.verify_naive():
"def populate_data(apps, schema_editor): Menu = apps.get_model('menu', 'Menu') for menu in Menu.objects.all(): <IF_STMT> json_str = menu.json_content while isinstance(json_str, str): json_str = json.loads(json_str) menu.json_content_new = json_str menu.save()","if isinstance(menu.json_content, str):"
"def virtualenv_exists(self): if os.path.exists(self.virtualenv_location): <IF_STMT> extra = ['Scripts', 'activate.bat'] else: extra = ['bin', 'activate'] return os.path.isfile(os.sep.join([self.virtualenv_location] + extra)) return False",if os.name == 'nt':
"def get_minkowski_function(name, variable): fn_name = name + get_postfix(variable) if hasattr(MEB, fn_name): return getattr(MEB, fn_name) el<IF_STMT> raise ValueError(f'Function {fn_name} not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`.') else: raise ValueError(f'Function {fn_name} not available.')",if variable.is_cuda:
"def build_temp_workspace(files): tempdir = tempfile.mkdtemp(prefix='yamllint-tests-') for path, content in files.items(): path = os.path.join(tempdir, path).encode('utf-8') <IF_STMT> os.makedirs(os.path.dirname(path)) if type(content) is list: os.mkdir(path) else: mode = 'wb' if isinstance(content, bytes) else 'w' with open(path, mode) as f: f.write(content) return tempdir",if not os.path.exists(os.path.dirname(path)):
"def clean_form(self, request, user, form, cleaned_data): for field in self.get_fields(): <IF_STMT> continue try: cleaned_data[field.fieldname] = field.clean(request, user, cleaned_data[field.fieldname]) except ValidationError as e: form.add_error(field.fieldname, e) return cleaned_data",if field.fieldname not in cleaned_data:
"def setUp(self): self.realm = service.InMemoryWordsRealm('realmname') self.checker = checkers.InMemoryUsernamePasswordDatabaseDontUse() self.portal = portal.Portal(self.realm, [self.checker]) self.factory = service.IRCFactory(self.realm, self.portal) c = [] for nick in self.STATIC_USERS: <IF_STMT> nick = nick.decode('utf-8') c.append(self.realm.createUser(nick)) self.checker.addUser(nick, nick + '_password') return DeferredList(c)","if isinstance(nick, bytes):"
"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes['seq_num'])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 <IF_STMT> if not self.done_future.done(): self.done_future.set_result(None)",if self.completed_calls >= self._resolve_at_msg_count:
"def fill_in_standard_formats(book): for x in std_format_code_types.keys(): <IF_STMT> ty = std_format_code_types[x] fmt_str = std_format_strings.get(x) fmtobj = Format(x, ty, fmt_str) book.format_map[x] = fmtobj",if x not in book.format_map:
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): if bigger_than_3_only and less_than_7_only and (i == 4): continue if bigger_than_3_only and i <= 3: continue <IF_STMT> continue if even_only and i % 2 != 0: continue result.append(i) return result",if less_than_7_only and i >= 7:
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): <IF_STMT> parser.read_line(line) continue parser.read_line(line) if not line.strip(): if i > 0 and (not lines[i - 1].strip()): return False continue if line.startswith('def ') or line.startswith('class '): return True if line.startswith(('#', '@', ' ', ')')): continue return False return False",if parser.is_quoted():
"def __getattr__(self, key): for tag in self.tag.children: if tag.name not in ('input',): continue <IF_STMT> from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError","if 'name' in tag.attrs and tag.attrs['name'] in (key,):"
"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x if x > self.box[2]: self.box[2] = x if y < self.box[1]: self.box[1] = y <IF_STMT> self.box[3] = y",if y > self.box[3]:
def find_shell(): global DEFAULT_SHELL if not DEFAULT_SHELL: for shell in propose_shell(): <IF_STMT> DEFAULT_SHELL = shell break if not DEFAULT_SHELL: DEFAULT_SHELL = '/bin/sh' return DEFAULT_SHELL,"if os.path.isfile(shell) and os.access(shell, os.X_OK):"
"def addAggregators(sheet, cols, aggrnames): """"""Add each aggregator in list of *aggrnames* to each of *cols*."""""" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: if not hasattr(c, 'aggregators'): c.aggregators = [] <IF_STMT> c.aggregators += [aggr]",if aggr and aggr not in c.aggregators:
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.pathAbsoluteFromProjectEncoded()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:
"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: <IF_STMT> msg = 'This account is already in use.' raise AuthAlreadyAssociated(backend, msg) elif not user: user = social.user return {'social': social, 'user': user, 'is_new': user is None, 'new_association': social is None}",if user and social.user != user:
"def _text(bitlist): out = '' for typ, text in bitlist: if not typ: out += text elif typ == 'em': out += '\\fI%s\\fR' % text <IF_STMT> out += '\\fB%s\\fR' % text else: raise ValueError('unexpected tag %r inside text' % (typ,)) out = out.strip() out = re.sub(re.compile('^\\s+', re.M), '', out) return out","elif typ in ['strong', 'code']:"
"def OnRadioSelect(self, event): fitID = self.mainFrame.getActiveFit() if fitID is not None: self.mainFrame.command.Submit(cmd.GuiChangeImplantLocationCommand(fitID=fitID, source=ImplantLocation.FIT <IF_STMT> else ImplantLocation.CHARACTER))",if self.rbFit.GetValue()
"def hexdump(data): """"""yield lines with hexdump of data"""""" values = [] ascii = [] offset = 0 for h, a in sixteen(data): <IF_STMT> yield (offset, ' '.join([''.join(values), ''.join(ascii)])) del values[:] del ascii[:] offset += 16 else: values.append(h) ascii.append(a)",if h is None:
"def submit(self): bot_token = self.config['bot_token'] chat_ids = self.config['chat_id'] chat_ids = [chat_ids] if isinstance(chat_ids, str) else chat_ids text = '\n'.join(super().submit()) if not text: logger.debug('Not calling telegram API (no changes)') return result = None for chunk in chunkstring(text, self.MAX_LENGTH, numbering=True): for chat_id in chat_ids: res = self.submitToTelegram(bot_token, chat_id, chunk) <IF_STMT> result = res return result",if res.status_code != requests.codes.ok or res is None:
"def onMessage(self, payload, isBinary): if not isBinary: self.result = 'Expected binary message with payload, but got binary.' el<IF_STMT> self.result = 'Expected binary message with payload of length %d, but got %d.' % (self.DATALEN, len(payload)) else: self.behavior = Case.OK self.result = 'Received binary message of length %d.' % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len(payload) != self.DATALEN:
"def verify_output(actual, expected): actual = _read_file(actual, 'Actual') expected = _read_file(join(CURDIR, expected), 'Expected') if len(expected) != len(actual): raise AssertionError('Lengths differ. Expected %d lines but got %d' % (len(expected), len(actual))) for exp, act in zip(expected, actual): tester = fnmatchcase if '*' in exp else eq <IF_STMT> raise AssertionError('Lines differ.\nExpected: %s\nActual:   %s' % (exp, act))","if not tester(act.rstrip(), exp.rstrip()):"
"def _in_out_vector_helper(self, name1, name2, ceil): vector = [] stats = self.record if ceil is None: ceil = self._get_max_rate(name1, name2) maxlen = self.config.get_stats_history_length() for n in [name1, name2]: for i in range(maxlen + 1): <IF_STMT> vector.append(float(stats[i][n]) / ceil) else: vector.append(0.0) return vector",if i < len(stats):
"def _init_param(param, mode): if isinstance(param, str): param = _resolve(param) elif isinstance(param, (list, tuple)): param = [_init_param(p, mode) for p in param] elif isinstance(param, dict): <IF_STMT> param = from_params(param, mode=mode) else: param = {k: _init_param(v, mode) for k, v in param.items()} return param","if {'ref', 'class_name', 'config_path'}.intersection(param.keys()):"
"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for page, soup in soups.items(): for a in soup.find_all('a'): if not a.has_attr('pantsref'): continue pantsref = a['pantsref'] <IF_STMT> raise TaskError(f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it') a['href'] = rel_href(page, precomputed.pantsref[pantsref])",if pantsref not in precomputed.pantsref:
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <IF_STMT> return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",elif '.' in svalue:
"def default(self, o): try: <IF_STMT> return str(o) else: if hasattr(o, 'profile'): del o.profile if hasattr(o, 'credentials'): del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path if hasattr(o, 'services_config'): del o.services_config return vars(o) except Exception as e: return str(o)",if type(o) == datetime.datetime:
"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: <IF_STMT> return ['-%s' % name] elif value not in (False, None): if split_single_char_options: return ['-%s' % name, '%s' % value] else: return ['-%s%s' % (name, value)] elif value is True: return ['--%s' % dashify(name)] elif value is not False and value is not None: return ['--%s=%s' % (dashify(name), value)] return []",if value is True:
"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: <IF_STMT> return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: if context.rounding == ROUND_FLOOR: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context.rounding == ROUND_CEILING:
"def OnLeftUp(self, event): if self.Drawing: self.Drawing = False <IF_STMT> world_rect = (self.Canvas.PixelToWorld(self.RBRect[0]), self.Canvas.ScalePixelToWorld(self.RBRect[1])) wx.CallAfter(self.CallBack, world_rect) self.RBRect = None",if self.RBRect:
"def _map_answers(answers): result = [] for a in answers.split('|'): user_answers = [] result.append(dict(sourcerAnswers=user_answers)) for r in a.split(','): <IF_STMT> user_answers.append(dict(noAnswer=True)) else: start_, end_ = map(int, r.split(':')) user_answers.append(dict(s=start_, e=end_)) return result",if r == 'None':
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: <IF_STMT> parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if d.GetLayer() == pcbnew.Edge_Cuts:
"def get_size(self): size = self.start_size for operation in self.ran_operations: <IF_STMT> size = operation[1][0] elif operation[0] == 'crop': crop = operation[1][0] size = (crop[2] - crop[0], crop[3] - crop[1]) return size",if operation[0] == 'resize':
"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) <IF_STMT> create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) if account.discriminator == 'gmailaccount': set_labels_for_imapuids(account, db_session) db_session.commit()",if account.discriminator == 'easaccount':
"def OnEndDrag(self, event): self.StopDragging() dropTarget = event.GetItem() if not dropTarget: dropTarget = self.GetRootItem() if self.IsValidDropTarget(dropTarget): self.UnselectAll() <IF_STMT> self.SelectItem(dropTarget) self.OnDrop(dropTarget, self._dragItem)",if dropTarget != self.GetRootItem():
"def validate(self, frame, value): if self.sep and isinstance(value, string_types): value = value.split(self.sep) if isinstance(value, list): <IF_STMT> return [self.specs[0].validate(frame, v) for v in value] else: return [[s.validate(frame, v) for v, s in izip(val, self.specs)] for val in value] raise ValueError('Invalid MultiSpec data: %r' % value)",if len(self.specs) == 1:
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None): QNetBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): <IF_STMT> action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32) self._action_space = action_space self._append_output_layer()",if action_space is None:
"def n_weights(self): """"""Return the number of weights (parameters) in this network."""""" n_weights = 0 for i, w in enumerate(self.all_weights): n = 1 for s in w.get_shape(): try: s = int(s) except: s = 1 <IF_STMT> n = n * s n_weights = n_weights + n return n_weights",if s:
"def _arg_desc(name, ctx): for param in ctx.command.params: if param.name == name: desc = param.opts[-1] <IF_STMT> desc = param.human_readable_name return desc raise AssertionError(name)",if desc[0] != '-':
"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any((fnmatch(name, pattern) for pattern in basename_ignore)): continue path = path_so_far + '/' + name if path_so_far else name if any((fnmatch(path, pattern) for pattern in path_ignore)): continue full_name = os.path.join(directory, name) if os.path.isdir(full_name): for file_path in walk(full_name, path): yield file_path <IF_STMT> yield path",elif os.path.isfile(full_name):
"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): if dst.is_id('IRDst'): <IF_STMT> raise ValueError('Multiple destinations!') final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",if final_dst is not None:
"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} <IF_STMT> filters['resource_ref'] = args.resource_ref if args.policy_type: filters['policy_type'] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",if args.resource_ref:
"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): if other_folder == folder: continue if other_path == path: duplicates.append(other_folder) <IF_STMT> self.duplicates[folder] = duplicates",if len(duplicates):
"def limit_clause(self, select, **kw): text = '' if select._limit_clause is not None: text += '\n LIMIT ' + self.process(select._limit_clause, **kw) if select._offset_clause is not None: <IF_STMT> text += '\n LIMIT ' + self.process(sql.literal(-1)) text += ' OFFSET ' + self.process(select._offset_clause, **kw) else: text += ' OFFSET ' + self.process(sql.literal(0), **kw) return text",if select._limit_clause is None:
"def _get_activation(self, act): """"""Get activation block based on the name."""""" if isinstance(act, str): if act.lower() == 'gelu': return GELU() <IF_STMT> return GELU(approximate=True) else: return gluon.nn.Activation(act) assert isinstance(act, gluon.Block) return act",elif act.lower() == 'approx_gelu':
"def __eq__(self, other): try: if self.type != other.type: return False if self.type == 'ASK': return self.askAnswer == other.askAnswer <IF_STMT> return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",elif self.type == 'SELECT':
"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): <IF_STMT> open_tags += 1 elif node.tag_type == CLOSE_TAG: open_tags -= 1 elif isinstance(node, HtmlDataFragment) and node.is_text_content and (open_tags == 0): text.append(html_body[node.start:node.end]) return text",if node.tag_type == OPEN_TAG:
"def test_do_change(self): """"""Test if VTK object changes when trait is changed."""""" p = Prop() p.edge_visibility = not p.edge_visibility p.representation = 'p' p.opacity = 0.5 p.color = (0, 1, 0) p.diffuse_color = (1, 1, 1) p.specular_color = (1, 1, 0) for t, g in p._updateable_traits_: val = getattr(p._vtk_obj, g)() <IF_STMT> self.assertEqual(val, getattr(p, t + '_')) else: self.assertEqual(val, getattr(p, t))",if t == 'representation':
"def update_item(source_doc, target_doc, source_parent): target_doc.t_warehouse = '' if source_doc.material_request_item and source_doc.material_request: add_to_transit = frappe.db.get_value('Stock Entry', source_name, 'add_to_transit') <IF_STMT> warehouse = frappe.get_value('Material Request Item', source_doc.material_request_item, 'warehouse') target_doc.t_warehouse = warehouse target_doc.s_warehouse = source_doc.t_warehouse target_doc.qty = source_doc.qty - source_doc.transferred_qty",if add_to_transit:
"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: <IF_STMT> config_root_path = drive.get('root_path') if config_root_path and root_path == config_root_path: return drive elif volume_guid_path: config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",if root_path:
"def f_freeze(_): repos = utils.get_repos() for name, path in repos.items(): url = '' cp = subprocess.run(['git', 'remote', '-v'], cwd=path, capture_output=True) <IF_STMT> url = cp.stdout.decode('utf-8').split('\n')[0].split()[1] print(f'{url},{name},{path}')",if cp.returncode == 0:
"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): <IF_STMT> raise RuntimeError('only contiguous arrays may be used as arguments to this operation') if self.flags.f_contiguous: order = 'F' else: order = 'C' result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call(self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size) return result else: return self",if not self.flags.forc:
"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: if function.is_implemented: <IF_STMT> continue self._explore(function.entry_point, []) function.context[self.KEY] = True",if self.KEY in function.context:
"def test_default_configuration_no_encoding(self): transformations = [] for i in range(2): transformation, original = _test_preprocessing(NoEncoding) self.assertEqual(transformation.shape, original.shape) self.assertTrue((transformation == original).all()) transformations.append(transformation) <IF_STMT> self.assertTrue((transformations[-1] == transformations[-2]).all())",if len(transformations) > 1:
"def main(): """"""main function"""""" parser = argparse.ArgumentParser(description='Let a cow speak for you') parser.add_argument('text', nargs='*', default=None, help='text to say') ns = parser.parse_args() if ns.text is None or len(ns.text) == 0: text = '' while True: inp = sys.stdin.read(4096) if inp.endswith('\n'): inp = inp[:-1] <IF_STMT> break text += inp else: text = ' '.join(ns.text) cow = get_cow(text) print(cow)",if not inp:
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == 'i386': reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == 'amd64': reg = emu.getRegister(envi.archs.amd64.REG_RDI) <IF_STMT> self.vw.makePointer(reg, follow=True)",if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:
"def get_boarding_status(project): status = 'Pending' if project: doc = frappe.get_doc('Project', project) if flt(doc.percent_complete) > 0.0 and flt(doc.percent_complete) < 100.0: status = 'In Process' <IF_STMT> status = 'Completed' return status",elif flt(doc.percent_complete) == 100.0:
"def set_weights(self, new_weights): weights = self.get_weights() if len(weights) != len(new_weights): raise ValueError('len of lists mismatch') tuples = [] for w, new_w in zip(weights, new_weights): <IF_STMT> new_w = new_w.reshape(w.shape) tuples.append((w, new_w)) nn.batch_set_value(tuples)",if len(w.shape) != new_w.shape:
"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs['setting'] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, '') value = kwargs['value'] if setting in DEFAULTS.keys(): <IF_STMT> setattr(json_api_settings, setting, value) elif hasattr(json_api_settings, setting): delattr(json_api_settings, setting)",if value is not None:
"def knamn(self, sup, cdict): cname = cdict[sup].class_name if not cname: namesp, tag = cdict[sup].name.split('.') <IF_STMT> ctag = self.root.modul[namesp].factory(tag).__class__.__name__ cname = '%s.%s' % (namesp, ctag) else: cname = tag + '_' return cname",if namesp:
"def setdefault(self, key, default=None): try: o = self.data[key]() except KeyError: o = None if o is None: <IF_STMT> self._commit_removals() self.data[key] = KeyedRef(default, self._remove, key) return default else: return o",if self._pending_removals:
"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) <IF_STMT> self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): if self.list_ctrl.IsSelected(index): self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",if event.EventObject is self.list_ctrl:
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): if i == j: continue edge = (nodes[i], nodes[j]) <IF_STMT> graph.del_edge(edge) graph.add_edge(edge, 1)",if graph.has_edge(edge):
"def _parse_param_value(name, datatype, default): if datatype == 'bool': <IF_STMT> return True elif default.lower() == 'false': return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': if type(default) == float: return default else: return float(default) else: return str(default)",if default.lower() == 'true':
"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: <IF_STMT> size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","if isinstance(elem, str):"
"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): if name not in subst: subst[name] = var <IF_STMT> subst[name].PasteVariable(var) return subst",elif subst[name] is not var:
"def _load_weights_if_possible(self, model, init_weight_path=None): """"""Loads model weights when it is provided."""""" if init_weight_path: logging.info('Load weights: {}'.format(init_weight_path)) <IF_STMT> checkpoint = tf.train.Checkpoint(model=model, optimizer=self._create_optimizer()) checkpoint.restore(init_weight_path) else: model.load_weights(init_weight_path) else: logging.info('Weights not loaded from path:{}'.format(init_weight_path))",if self.use_tpu:
"def _cleanup_inactive_receivexlogs(self, site): if site in self.receivexlogs: if not self.receivexlogs[site].running: <IF_STMT> self.receivexlogs[site].join() del self.receivexlogs[site]",if self.receivexlogs[site].is_alive():
"def get_asset(self, path): """"""Loads an asset by path."""""" clean_path = cleanup_path(path).strip('/') nodes = [self.asset_root] + self.theme_asset_roots for node in nodes: for piece in clean_path.split('/'): node = node.get_child(piece) <IF_STMT> break if node is not None: return node return None",if node is None:
"def palindromic_substrings(s): if not s: return [[]] results = [] for i in range(len(s), 0, -1): sub = s[:i] <IF_STMT> for rest in palindromic_substrings(s[i:]): results.append([sub] + rest) return results",if sub == sub[::-1]:
"def debug_tree(tree): l = [] for elt in tree: <IF_STMT> l.append(_names.get(elt, elt)) elif isinstance(elt, str): l.append(elt) else: l.append(debug_tree(elt)) return l","if isinstance(elt, (int, long)):"
"def shared_username(account): username = os.environ.get('SHARED_USERNAME', 'PKKid') for user in account.users(): <IF_STMT> return username elif user.username and user.email and user.id and (username.lower() in (user.username.lower(), user.email.lower(), str(user.id))): return username pytest.skip('Shared user %s wasn`t found in your MyPlex account' % username)",if user.title.lower() == username.lower():
"def process_schema_element(self, e): if e.name is None: return self.debug1('adding element: %s', e.name) t = self.get_type(e.type) if t: <IF_STMT> del self.pending_elements[e.name] self.retval[self.tns].elements[e.name] = e else: self.pending_elements[e.name] = e",if e.name in self.pending_elements:
"def __setitem__(self, key, value): with self._lock: try: link = self._get_link_and_move_to_front_of_ll(key) except KeyError: <IF_STMT> self._set_key_and_add_to_front_of_ll(key, value) else: evicted = self._set_key_and_evict_last_in_ll(key, value) super(LRI, self).__delitem__(evicted) super(LRI, self).__setitem__(key, value) else: link[VALUE] = value",if len(self) < self.max_size:
"def __delattr__(self, name): if name == '__dict__': raise AttributeError(""%r object attribute '__dict__' is read-only"" % self.__class__.__name__) if name in self._local_type_vars: <IF_STMT> type_attr = getattr(self._local_type, name, _marker) type(type_attr).__delete__(type_attr, self) return dct = _local_get_dict(self) try: del dct[name] except KeyError: raise AttributeError(name)",if name in self._local_type_del_descriptors:
"def update_participants(self, refresh=True): for participant in list(self.participants_dict): <IF_STMT> continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: if participant in self.participants_dict: self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",if participant is None or participant == self.simulator_config.broadcast_part:
"def insert_bigger_b_add(node): if node.op == theano.tensor.add: inputs = list(node.inputs) <IF_STMT> inputs[-1] = theano.tensor.concatenate((inputs[-1], inputs[-1])) return [node.op(*inputs)] return False",if inputs[-1].owner is None:
"def _activate_cancel_status(self, cancel_status): if self._cancel_status is not None: self._cancel_status._tasks.remove(self) self._cancel_status = cancel_status if self._cancel_status is not None: self._cancel_status._tasks.add(self) <IF_STMT> self._attempt_delivery_of_any_pending_cancel()",if self._cancel_status.effectively_cancelled:
"def writeLibraryGeometry(fp, meshes, config, shapes=None): progress = Progress(len(meshes), None) fp.write('\n  <library_geometries>\n') for mIdx, mesh in enumerate(meshes): <IF_STMT> shape = None else: shape = shapes[mIdx] writeGeometry(fp, mesh, config, shape) progress.step() fp.write('  </library_geometries>\n')",if shapes is None:
"def init_module_config(module_json, config, config_path=default_config_path): if 'config' in module_json['meta']: if module_json['meta']['config']: if module_json['name'] not in config: config.add_section(module_json['name']) for config_var in module_json['meta']['config']: <IF_STMT> config.set(module_json['name'], config_var, '') return config",if config_var not in config[module_json['name']]:
"def get_const_defines(flags, prefix=''): defs = [] for k, v in globals().items(): if isinstance(v, int): if v & flags: <IF_STMT> if k.startswith(prefix): defs.append(k) else: defs.append(k) return defs",if prefix:
"def __init__(self, source, encoding=DEFAULT_ENCODING): self.data = {} with open(source, encoding=encoding) as file_: for line in file_: line = line.strip() <IF_STMT> continue k, v = line.split('=', 1) k = k.strip() v = v.strip() if len(v) >= 2 and (v[0] == ""'"" and v[-1] == ""'"" or (v[0] == '""' and v[-1] == '""')): v = v.strip('\'""') self.data[k] = v",if not line or line.startswith('#') or '=' not in line:
"def __detect_console_logger(self): logger = self.log while logger: for handler in logger.handlers[:]: <IF_STMT> if handler.stream in (sys.stdout, sys.stderr): self.logger_handlers.append(handler) if logger.root == logger: break else: logger = logger.root","if isinstance(handler, StreamHandler):"
"def check_heuristic_in_sql(): heurs = set() excluded = ['Equal assembly or pseudo-code', 'All or most attributes'] for heur in HEURISTICS: name = heur['name'] <IF_STMT> continue sql = heur['sql'] if sql.lower().find(name.lower()) == -1: print('SQL command not correctly associated to %s' % repr(name)) print(sql) assert sql.find(name) != -1 heurs.add(name) print('Heuristics:') import pprint pprint.pprint(heurs)",if name in excluded:
"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: <IF_STMT> self.seek_to_block(self.cursor) part = self.current_stream.read(size) if size > 0: if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",if not self.in_current_block(self.cursor):
"def get_project_dir(env): project_file = workon_home / env / '.project' if project_file.exists(): with project_file.open() as f: project_dir = f.readline().strip() <IF_STMT> return project_dir else: err('Corrupted or outdated:', project_file, '\nDirectory', project_dir, ""doesn't exist."")",if os.path.exists(project_dir):
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <IF_STMT> curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] return tf.keras.backend.stop_gradient(new_mem)",if reuse_len is not None and reuse_len > 0:
"def cleanup_channel(self, to_cleanup): public_key, id_ = to_cleanup try: with db_session: channel = self.session.mds.ChannelMetadata.get_for_update(public_key=public_key, id_=id_) <IF_STMT> return channel.local_version = 0 channel.contents.delete(bulk=True) except Exception as e: self._logger.warning('Exception while cleaning unsubscribed channel: %', str(e))",if not channel:
"def best_image(width, height): image = images[0] for img in images: if img.width == width and img.height == height: return img <IF_STMT> image = img return image",elif img.width >= width and img.width * img.height > image.width * image.height:
"def add_peer_to_blob(self, contact: 'KademliaPeer', key: bytes) -> None: now = self.loop.time() if key in self._data_store: current = list(filter(lambda x: x[0] == contact, self._data_store[key])) <IF_STMT> self._data_store[key][self._data_store[key].index(current[0])] = (contact, now) else: self._data_store[key].append((contact, now)) else: self._data_store[key] = [(contact, now)]",if len(current) > 0:
"def dump(self): self.ql.log.info('[*] Dumping object: %s' % self.sf_name) for field in self._fields_: if isinstance(getattr(self, field[0]), POINTER64): self.ql.log.info('%s: 0x%x' % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info('%s: %d' % (field[0], getattr(self, field[0]))) <IF_STMT> self.ql.log.info('%s: %s' % (field[0], getattr(self, field[0]).decode()))","elif isinstance(getattr(self, field[0]), bytes):"
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end <IF_STMT> continue if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 4096): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if end < self.plugin_args.start:
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') if name.startswith(expr): if name not in found_names: found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) <IF_STMT> break return sorted(cplns, key=operator.itemgetter(1))",if not scoperef:
"def get_xenapi_host(self): """"""Return the xenapi host on which nova-compute runs on."""""" with self._get_session() as session: <IF_STMT> return session.xenapi.host.get_by_uuid(self.host_uuid) else: return session.xenapi.session.get_this_host(session.handle)",if self.host_uuid:
def stream_docker_log(log_stream): async for line in log_stream: <IF_STMT> logger.debug(line['stream'].strip()) elif 'status' in line: logger.debug(line['status'].strip()) elif 'error' in line: logger.error(line['error'].strip()) raise DockerBuildError,if 'stream' in line and line['stream'].strip():
"def test_wildcard_import(): bonobo = __import__('bonobo') assert bonobo.__version__ for name in dir(bonobo): <IF_STMT> continue attr = getattr(bonobo, name) if inspect.ismodule(attr): continue assert name in bonobo.__all__",if name.startswith('_'):
"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val <IF_STMT> const = not true_val elif not compare.compatible_with(v, False): const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var","elif not compare.compatible_with(v, True):"
"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, 'id') controls_ids = required_key(item, 'controls') if not isinstance(controls_ids, list): <IF_STMT> msg = 'Policy {id_} contains invalid controls list {controls}.'.format(id_=id_, controls=str(controls_ids)) raise ValueError(msg) self.policies[id_] = controls_ids",if controls_ids != 'all':
"def pong(self, payload: Union[str, bytes]='') -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <IF_STMT> payload = payload.decode('utf-8') self.logger.debug(f'Sending a pong data frame (session id: {self.session_id}, payload: {payload})') data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PONG) with self.sock_send_lock: self.sock.send(data)","if isinstance(payload, bytes):"
"def _extract_curve_feature_log(arg): """"""extract sampled curve feature for log items"""""" try: inp, res = arg config = inp.config with inp.target: sch, args = inp.task.instantiate(config) fea = feature.get_buffer_curve_sample_flatten(sch, args, sample_n=20) x = np.concatenate((fea, list(config.get_other_option().values()))) <IF_STMT> y = inp.task.flop / np.mean(res.costs) else: y = 0.0 return (x, y) except Exception: return None",if res.error_no == 0:
"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' if ss['branch']: source += '[branch %s] ' % ss['branch'] <IF_STMT> source += str(ss['revision']) else: source += 'HEAD' if ss['patch'] is not None: source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['revision']:
"def find_repository(): orig_path = path = os.path.realpath('.') drive, path = os.path.splitdrive(path) while path: current_path = os.path.join(drive, path) current_repo = LocalRepository(current_path) if current_repo.isValid(): return current_repo path, path_tail = os.path.split(current_path) <IF_STMT> raise CannotFindRepository('Cannot find repository for %s' % (orig_path,))",if not path_tail:
"def compute_indices(text: str, tokens): indices = [] for i, token in enumerate(tokens): <IF_STMT> current_index = indices[-1] + len(tokens[i - 1][0]) indices.append(current_index + text[current_index:].find(token[0])) else: indices.append(text.find(token[0])) return indices",if 1 <= i:
"def _add_defaults_data_files(self): if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): item = convert_path(item) <IF_STMT> self.filelist.append(item) else: dirname, filenames = item for f in filenames: f = convert_path(f) if os.path.isfile(f): self.filelist.append(f)",if os.path.isfile(item):
def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe('compiler.libcxx') if not compiler or not libcxx: return '' if str(compiler) in GCC_LIKE: <IF_STMT> return '_GLIBCXX_USE_CXX11_ABI=0' elif str(libcxx) == 'libstdc++11': return '_GLIBCXX_USE_CXX11_ABI=1' return '',if str(libcxx) == 'libstdc++':
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) <IF_STMT> self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","elif isinstance(v, basestring):"
"def test_seek(self): <IF_STMT> print('create large file via seek (may be sparse file) ...') with self.open(TESTFN, 'wb') as f: f.write(b'z') f.seek(0) f.seek(size) f.write(b'a') f.flush() if verbose: print('check file size with os.fstat') self.assertEqual(os.fstat(f.fileno())[stat.ST_SIZE], size + 1)",if verbose:
"def serialize_review_url_field(self, obj, **kwargs): if obj.review_ui: review_request = obj.get_review_request() <IF_STMT> local_site_name = review_request.local_site.name else: local_site_name = None return local_site_reverse('file-attachment', local_site_name=local_site_name, kwargs={'review_request_id': review_request.display_id, 'file_attachment_id': obj.pk}) return ''",if review_request.local_site_id:
"def on_item_down_clicked(self, button): model = self.treeview.get_model() for s in self._get_selected(): <IF_STMT> old = model.get_iter(s[0]) iter = model.insert(s[0] + 2) for i in range(3): model.set_value(iter, i, model.get_value(old, i)) model.remove(old) self.treeview.get_selection().select_iter(iter) self._update_filter_string()",if s[0] < len(model) - 1:
"def writer(self): """"""loop forever and copy socket->serial"""""" while self.alive: try: data = self.socket.recv(1024) <IF_STMT> break self.serial.write(b''.join(self.rfc2217.filter(data))) except socket.error as msg: self.log.error('{}'.format(msg)) break self.stop()",if not data:
"def __getitem__(self, key): if key == 1: return self.get_value() elif key == 0: return self.cell[0] elif isinstance(key, slice): s = list(self.cell.__getitem__(key)) <IF_STMT> s[s.index(self.cell[1])] = self.get_value() return s else: raise IndexError(key)",if self.cell[1] in s:
"def test_error_stream(environ, start_response): writer = start_response('200 OK', []) wsgi_errors = environ['wsgi.errors'] error_msg = None for method in ['flush', 'write', 'writelines']: <IF_STMT> error_msg = ""wsgi.errors has no '%s' attr"" % method if not error_msg and (not callable(getattr(wsgi_errors, method))): error_msg = 'wsgi.errors.%s attr is not callable' % method if error_msg: break return_msg = error_msg or 'success' writer(return_msg) return []","if not hasattr(wsgi_errors, method):"
"def job_rule_modules(app): rules_module_list = [] for rules_module_name in __job_rule_module_names(app): rules_module = sys.modules.get(rules_module_name, None) <IF_STMT> rules_module = importlib.import_module(rules_module_name) rules_module_list.append(rules_module) return rules_module_list",if not rules_module:
"def discover_hdfstore(f): d = dict() for key in f.keys(): d2 = d key2 = key.lstrip('/') while '/' in key2: group, key2 = key2.split('/', 1) <IF_STMT> d2[group] = dict() d2 = d2[group] d2[key2] = f.get_storer(key) return discover(d)",if group not in d2:
"def test_update_zone(self): zone = self.driver.list_zones()[0] updated_zone = self.driver.update_zone(zone=zone, domain='', extra={'paused': True}) self.assertEqual(zone.id, updated_zone.id) self.assertEqual(zone.domain, updated_zone.domain) self.assertEqual(zone.type, updated_zone.type) self.assertEqual(zone.ttl, updated_zone.ttl) for key in set(zone.extra) | set(updated_zone.extra): <IF_STMT> self.assertNotEqual(zone.extra[key], updated_zone.extra[key]) else: self.assertEqual(zone.extra[key], updated_zone.extra[key])","if key in ('paused', 'modified_on'):"
"def ESP(phrase): for num, name in enumerate(devname): <IF_STMT> dev = devid[num] if custom_action_keyword['Dict']['On'] in phrase: ctrl = '=ON' say('Turning On ' + name) elif custom_action_keyword['Dict']['Off'] in phrase: ctrl = '=OFF' say('Turning Off ' + name) rq = requests.head('https://' + ip + dev + ctrl, verify=False)",if name.lower() in phrase:
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue if nw_id_ == nw_id: ret.append(port.port_no) <IF_STMT> ret.append(port.port_no) return ret",elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:
"def tail(filename): if os.path.isfile(filename): file = open(filename, 'r') st_results = os.stat(filename) st_size = st_results[6] file.seek(st_size) while 1: where = file.tell() line = file.readline() <IF_STMT> time.sleep(1) file.seek(where) else: print(line) else: print_error('File not found, cannot tail.')",if not line:
"def proc_day_of_week(d): if expanded[4][0] != '*': diff_day_of_week = nearest_diff_method(d.isoweekday() % 7, expanded[4], 7) if diff_day_of_week is not None and diff_day_of_week != 0: <IF_STMT> d += relativedelta(days=diff_day_of_week, hour=23, minute=59, second=59) else: d += relativedelta(days=diff_day_of_week, hour=0, minute=0, second=0) return (True, d) return (False, d)",if is_prev:
"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): if name.startswith('check_'): method = getattr(self, name) <IF_STMT> method() finally: warnings.formatwarning = oldformatwarning",if method and callable(method):
"def get(self, request, *args, **kwargs): if self.revision: <IF_STMT> try: return send_file(request, self.revision.file.path, self.revision.created, self.attachment.original_filename) except OSError: pass else: return HttpResponseRedirect(self.revision.file.url) raise Http404",if settings.USE_LOCAL_PATH:
"def _close(self): super(Recording, self)._close() if self._log_n is not None: for i in range(self.n): <IF_STMT> self._log_n[i].close() self._log_n[i] = None",if self._log_n[i] is not None:
"def addTags(self, rpcObjects=None): hosts = self._getOnlyHostObjects(rpcObjects) if hosts: title = 'Add Tags' body = 'What tags should be added?\n\nUse a comma or space between each' tags, choice = self.getText(title, body, '') <IF_STMT> tags = str(tags).replace(' ', ',').split(',') for host in hosts: self.cuebotCall(host.addTags, 'Add Tags to %s Failed' % host.data.name, tags) self._update()",if choice:
"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = ['pixel_longitude', 'pixel_latitude'] for var_name, val in self.file_content.items(): <IF_STMT> ds_info = {'file_type': self.filetype_info['file_type'], 'resolution': res} if not self.is_geo: ds_info['coordinates'] = coordinates yield (DatasetID(name=var_name, resolution=res), ds_info)","if isinstance(val, netCDF4.Variable):"
"def extract_from_file(fname: PathIsh) -> Iterator[Extraction]: path = Path(fname) fallback_dt = file_mtime(path) p = Parser(path) for r in p.walk(): <IF_STMT> yield r else: yield Visit(url=r.url, dt=fallback_dt, locator=Loc.file(fname), context=r.context)","if isinstance(r, Exception):"
"def init_module_config(module_json, config, config_path=default_config_path): if 'config' in module_json['meta']: if module_json['meta']['config']: <IF_STMT> config.add_section(module_json['name']) for config_var in module_json['meta']['config']: if config_var not in config[module_json['name']]: config.set(module_json['name'], config_var, '') return config",if module_json['name'] not in config:
"def _create_entities(parsed_entities, sidx, eidx): entities = [] for k, vs in parsed_entities.items(): <IF_STMT> vs = [vs] for value in vs: entities.append({'entity': k, 'start': sidx, 'end': eidx, 'value': value}) return entities","if not isinstance(vs, list):"
"def _telegram_upload_stream(self, stream, **kwargs): """"""Perform upload defined in a stream."""""" msg = None try: stream.accept() msg = self._telegram_special_message(chat_id=stream.identifier.id, content=stream.raw, msg_type=stream.stream_type, **kwargs) except Exception: log.exception(f'Upload of {stream.name} to {stream.identifier} failed.') else: <IF_STMT> stream.error() else: stream.success()",if msg is None:
"def readlines(self, size=-1): if self._nbr == self._size: return [] out = [] nbr = 0 while True: line = self.readline() <IF_STMT> break out.append(line) if size > -1: nbr += len(line) if nbr > size: break return out",if not line:
"def clean_permissions(cls, requestor: 'User', group: auth_models.Group, errors: Dict[Optional[str], List[ValidationError]], cleaned_input: dict): field = 'add_permissions' permission_items = cleaned_input.get(field) if permission_items: cleaned_input[field] = get_permissions(permission_items) <IF_STMT> cls.ensure_can_manage_permissions(requestor, errors, field, permission_items)",if not requestor.is_superuser:
"def _bwd(subj=None, obj=None, seen=None): seen.add(obj) for s, o in evalPath(graph, (None, self.path, obj)): <IF_STMT> yield (s, o) if self.more: if s in seen: continue for s2, o2 in _bwd(None, s, seen): yield (s2, o)",if not subj or subj == s:
"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: <IF_STMT> uptime['value'] = stats['uptime'] / 60 / 60 / 24 uptime['unit'] = _('days') elif stats['uptime'] > 3600: uptime['value'] = stats['uptime'] / 60 / 60 uptime['unit'] = _('hours') else: uptime['value'] = stats['uptime'] / 60 uptime['unit'] = _('minutes') return {'cache_stats': cache_stats, 'uptime': uptime}",if stats['uptime'] > 86400:
def refresh(self): if self._handle: source = self._db.get_repository_from_handle(self._handle) <IF_STMT> self._title = str(source.get_type()) self._value = source.get_name(),if source:
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) <IF_STMT> return None elif '.' in svalue: return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",if not svalue:
"def parseGrants(self, tree): for grant in tree.findall('.//Grant'): grantee = Grantee() g = grant.find('.//Grantee') grantee.xsi_type = g.attrib['{http://www.w3.org/2001/XMLSchema-instance}type'] grantee.permission = grant.find('Permission').text for el in g: <IF_STMT> grantee.display_name = el.text else: grantee.tag = el.tag grantee.name = el.text self.grantees.append(grantee)",if el.tag == 'DisplayName':
"def __init__(self, name: Optional[str]=None, order: int=0): if name is None: if order == 0: name = 'std_dev' <IF_STMT> name = 'sample_std_dev' else: name = f'std_dev{order})' super().__init__(name=name, order=order) self.order = order",elif order == 1:
"def _shouldRollover(self): if self.maxBytes > 0: try: self.stream.seek(0, 2) except IOError: return True <IF_STMT> return True else: self._degrade(False, 'Rotation done or not needed at this time') return False",if self.stream.tell() >= self.maxBytes:
"def userfullname(): """"""Get the user's full name."""""" global _userfullname if not _userfullname: uid = os.getuid() entry = pwd_from_uid(uid) <IF_STMT> _userfullname = entry[4].split(',')[0] or entry[0] if not _userfullname: _userfullname = 'user%d' % uid return _userfullname",if entry:
"def drop(self): sql = ""if object_id('%s') is not null drop table %s"" % (self.tname, self.tname) try: self.execute(sql) except Exception as e: self.conn.rollback() <IF_STMT> raise sql = 'drop table if exists %s' % self.tname self.execute(sql)",if 'syntax error' not in str(e):
"def _find_delimiter(f, block_size=2 ** 16): delimiter = b'\n' if f.tell() == 0: return 0 while True: b = f.read(block_size) <IF_STMT> return f.tell() elif delimiter in b: return f.tell() - len(b) + b.index(delimiter) + 1",if not b:
"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) <IF_STMT> force_list = True values.extend((_convert(x[1]) for x in sorted(container.items()))) if not force_list and len(values) == 1: values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls(((k, _convert(v)) for k, v in iteritems(container)))","if container.pop(_list_marker, False):"
"def fitting(self, value): self._fitting = value if self._fitting is not None: <IF_STMT> try: os.makedirs(dirname(self.checkpoint_path())) except FileExistsError as ex: pass if not os.path.exists(dirname(self.tensorboard_path())): try: os.makedirs(dirname(self.tensorboard_path())) except FileExistsError as ex: pass",if not os.path.exists(dirname(self.checkpoint_path())):
"def _make_headers(self): libraries = self._df.columns.to_list() columns = [] for library in libraries: version = self._package_versions[library] library_description = self._libraries_description.get(library) <IF_STMT> library += ' {}'.format(library_description) columns.append('{library}<br><small>{version}</small>'.format(library=library, version=version)) return [''] + columns",if library_description:
"def plugin_on_song_ended(self, song, stopped): if song is not None: poll = self.rating_box.poll_vote() <IF_STMT> ups = int(song.get('~#wins') or 0) downs = int(song.get('~#losses') or 0) ups += poll[0] downs += poll[1] song['~#wins'] = ups song['~#losses'] = downs song['~#rating'] = ups / max(ups + downs, 2) song['~#score'] = ups - downs",if poll[0] >= 1 or poll[1] >= 1:
"def submit(self, pig_script, params): workflow = None try: workflow = self._create_workflow(pig_script, params) mapping = dict([(param['name'], param['value']) for param in workflow.get_parameters()]) oozie_wf = _submit_workflow(self.user, self.fs, self.jt, workflow, mapping) finally: <IF_STMT> workflow.delete(skip_trash=True) return oozie_wf",if workflow:
"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) <IF_STMT> correct += 1 else: self.fail('Invalid schema was parsed: ' + example.schema_string) except: if not example.valid: correct += 1 else: self.fail('Valid schema failed to parse: ' + example.schema_string) fail_msg = 'Parse behavior correct on %d out of %d schemas.' % (correct, len(EXAMPLES)) self.assertEqual(correct, len(EXAMPLES), fail_msg)",if example.valid:
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('wf', 'punc'): itm = self.handle_word(child) <IF_STMT> sent.extend(itm) else: sent.append(itm) else: raise ValueError('Unexpected element %s' % child.tag) return SemcorSentence(elt.attrib['snum'], sent)",if self._unit == 'word':
"def _set_property(self, target_widget, pname, value): if pname == 'text': state = target_widget.cget('state') <IF_STMT> target_widget.configure(state=tk.NORMAL) target_widget.insert('0.0', value) target_widget.configure(state=tk.DISABLED) else: target_widget.insert('0.0', value) else: super(TKText, self)._set_property(target_widget, pname, value)",if state == tk.DISABLED:
"def get_vrf_tables(self, vrf_rf=None): vrf_tables = {} for (scope_id, table_id), table in self._tables.items(): if scope_id is None: continue <IF_STMT> continue vrf_tables[scope_id, table_id] = table return vrf_tables",if vrf_rf is not None and table_id != vrf_rf:
"def new_f(self, *args, **kwargs): for obj in f(self, *args, **kwargs): if self.protected == False: if 'user' in obj and obj['user']['protected']: continue <IF_STMT> continue yield obj",elif 'protected' in obj and obj['protected']:
"def draw(self, context): col = self.layout.column() col.operator('node.sv_show_latest_commits') if context.scene.sv_new_version: col_alert = self.layout.column() col_alert.alert = True col_alert.operator('node.sverchok_update_addon', text='Upgrade Sverchok addon') else: col.operator('node.sverchok_check_for_upgrades_wsha', text='Check for updates') with sv_preferences() as prefs: <IF_STMT> col.operator('node.sv_run_pydoc')",if prefs.developer_mode:
"def generate_tag_1_data(ids): if len(ids) != SAMPLE_NUM: raise ValueError('len ids should equal to sample number') counter = 0 for sample_i in range(SAMPLE_NUM): one_data = [ids[sample_i]] valid_set = [x for x in range(TAG_INTERVAL[0], TAG_INTERVAL[1])] features = np.random.choice(valid_set, FEATURE_NUM, replace=False) one_data += [':'.join([x, '1.0']) for x in features] counter += 1 <IF_STMT> print('generate data {}'.format(counter)) yield one_data",if counter % 10000 == 0:
"def handle_api_languages(self, http_context): mgr = PluginManager.get(aj.context) languages = set() for id in mgr: locale_dir = mgr.get_content_path(id, 'locale') <IF_STMT> for lang in os.listdir(locale_dir): if lang != 'app.pot': languages.add(lang) return sorted(list(languages))",if os.path.isdir(locale_dir):
"def update(self, t): for i in range(self.grid.x): for j in range(self.grid.y): distance = self.test_func(i, j, t) <IF_STMT> self.turn_off_tile(i, j) elif distance < 1: self.transform_tile(i, j, distance) else: self.turn_on_tile(i, j)",if distance == 0:
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): if isinstance(text, CodeViewText): text.autocompleter = Completer(text) <IF_STMT> text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","elif isinstance(text, ShellText):"
"def test_create_repository(repo_name, expected_status, client): with client_with_identity('devtable', client) as cl: body = {'namespace': 'devtable', 'repository': repo_name, 'visibility': 'public', 'description': 'foo'} result = conduct_api_call(client, RepositoryList, 'post', None, body, expected_code=expected_status).json <IF_STMT> assert result['name'] == repo_name assert model.repository.get_repository('devtable', repo_name).name == repo_name",if expected_status == 201:
"def _apply_filter(filter_item, filter_list): for filter_method in filter_list: try: <IF_STMT> return False except Exception as e: raise MessageException(""Toolbox filter exception from '{}': {}."".format(filter_method.__name__, unicodify(e))) return True","if not filter_method(context, filter_item):"
"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) if not data: break <IF_STMT> data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write('%s: I/O error: %s\n' % (filename, msg)) return 1 out.write('%s %s\n' % (m.hexdigest(), filename)) return 0","if isinstance(data, str):"
"def get_block_loc_keys(block): """"""Extract loc_keys used by @block"""""" symbols = set() for instr in block.lines: <IF_STMT> if isinstance(instr.raw, list): for expr in instr.raw: symbols.update(get_expr_locs(expr)) else: for arg in instr.args: symbols.update(get_expr_locs(arg)) return symbols","if isinstance(instr, AsmRaw):"
"def get_operations(cls, info, operations: List[ProductAttributeAssignInput]): """"""Resolve all passed global ids into integer PKs of the Attribute type."""""" product_attrs_pks = [] variant_attrs_pks = [] for operation in operations: pk = from_global_id_strict_type(operation.id, only_type=Attribute, field='operations') <IF_STMT> product_attrs_pks.append(pk) else: variant_attrs_pks.append(pk) return (product_attrs_pks, variant_attrs_pks)",if operation.type == ProductAttributeType.PRODUCT:
def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree['activities'].values(): if act['type'] == 'SubProcess': _collect_manual_intervention_nodes(act['pipeline']) <IF_STMT> manual_intervention_nodes.add(act['id']),elif act['component']['code'] in MANUAL_INTERVENTION_COMP_CODES:
"def prompt_authorization(self, stacks: List[Stack]): auth_required_per_resource = auth_per_resource(stacks) for resource, authorization_required in auth_required_per_resource: <IF_STMT> auth_confirm = confirm(f'\t{self.start_bold}{resource} may not have authorization defined, Is this okay?{self.end_bold}', default=False) if not auth_confirm: raise GuidedDeployFailedError(msg='Security Constraints Not Satisfied!')",if not authorization_required:
"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): if self.source in CLOUD_PROVIDERS: <IF_STMT> credential = cred break elif cred.credential_type.kind != 'vault': credential = cred break return credential","if cred.kind == self.source.replace('ec2', 'aws'):"
"def validate_party_details(self): if self.party: if not frappe.db.exists(self.party_type, self.party): frappe.throw(_('Invalid {0}: {1}').format(self.party_type, self.party)) <IF_STMT> self.validate_account_type(self.party_account, [erpnext.get_party_account_type(self.party_type)])","if self.party_account and self.party_type in ('Customer', 'Supplier'):"
"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): <IF_STMT> if i in direct_upstreams: t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield (k, tuple(t))",if g:
"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): <IF_STMT> break scale = scales[i] if not math.isnan(scale): vidx1, vidx2 = (vidxs[i * 2], vidxs[i * 2 + 1]) scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData","if i >= min(len(scales), len(vidxs) // 2):"
"def _make_ext_obj(self, obj): ext = self._get_ext_class(obj.objname)() for name, val in obj.body: <IF_STMT> raise Exception('Error val should be a list, this is a python-opcua bug', name, type(val), val) else: for attname, v in val: self._set_attr(ext, attname, v) return ext","if not isinstance(val, list):"
"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: if row[refnum + 1] is None: row[refnum + 1] = line return <IF_STMT> break self.rows.insert(i, self.newRow(linenum, refnum, line))",elif row[0] > linenum:
def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) if match: continue if strip_delimiters: <IF_STMT> continue yield line,"if ',' in line or ';' in line:"
"def encodingChanged(self, idx): encoding = str(self.mode_combo.currentText()) validator = None if encoding == 'hex': txt = str(self.data_edit.text()) <IF_STMT> self.data_edit.setText('') regex = QtCore.QRegExp('^[0-9A-Fa-f]+$') validator = QtGui.QRegExpValidator(regex) self.data_edit.setValidator(validator) self.renderMemory()",if not all((c in string.hexdigits for c in txt)):
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): <IF_STMT> self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()",if redo:
"def _transform_bin(self, X: DataFrame): if self._bin_map: <IF_STMT> X = X.copy(deep=True) with pd.option_context('mode.chained_assignment', None): for column in self._bin_map: X[column] = binning.bin_column(series=X[column], mapping=self._bin_map[column], dtype=self._astype_map[column]) return X",if not self.inplace:
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') <IF_STMT> text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text","if ""'"" in text:"
"def read(self): """"""Reads the robots.txt URL and feeds it to the parser."""""" try: f = urllib.request.urlopen(self.url) except urllib.error.HTTPError as err: <IF_STMT> self.disallow_all = True elif err.code >= 400 and err.code < 500: self.allow_all = True else: raw = f.read() self.parse(raw.decode('utf-8').splitlines())","if err.code in (401, 403):"
"def post_create(self, user, billing=None): from weblate.trans.models import Change if billing: billing.projects.add(self) <IF_STMT> self.access_control = Project.ACCESS_PRIVATE else: self.access_control = Project.ACCESS_PUBLIC self.save() if not user.is_superuser: self.add_user(user, '@Administration') Change.objects.create(action=Change.ACTION_CREATE_PROJECT, project=self, user=user, author=user)",if billing.plan.change_access_control:
"def visitConst(self, node): if self.documentable: <IF_STMT> self.documentable.append(make_docstring(node.value, node.lineno)) else: self.documentable = None","if type(node.value) in (StringType, UnicodeType):"
"def requires(self): requires = copy.deepcopy(self._requires) parameters = self.parameters for value in parameters.values(): if isinstance(value, basestring) and '::' in value: stack_name, _ = value.split('::') else: continue <IF_STMT> requires.add(stack_name) return requires",if stack_name not in requires:
"def __load_protos(): g = globals() for k, v in g.items(): <IF_STMT> name = k[4:] modname = name.lower() try: mod = __import__(modname, g, level=1) PPP.set_p(v, getattr(mod, name)) except (ImportError, AttributeError): continue",if k.startswith('PPP_'):
"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) <IF_STMT> normal_init(m, std=0.01)","elif isinstance(m, nn.Linear):"
"def get_data(self): """"""get all data from sockets"""""" si = self.inputs parameters = [] for socket in si: <IF_STMT> parameters.append(socket.sv_get()) else: parameters.append(socket.sv_get(default=[[]])) return match_long_repeat(parameters)",if len(socket.prop_name) > 0:
"def test_parse_query_params_comparable_field(self): query_params = {'filter[int_field][gt]': 42, 'filter[int_field][lte]': 9000} fields = self.view.parse_query_params(query_params) for key, field_name in fields.items(): if field_name['int_field']['op'] == 'gt': assert_equal(field_name['int_field']['value'], 42) <IF_STMT> assert_equal(field_name['int_field']['value'], 9000) else: self.fail()",elif field_name['int_field']['op'] == 'lte':
"def _create_examples(self, lines, set_type): """"""Creates examples for the training and dev sets."""""" examples = [] for i, line in enumerate(lines): <IF_STMT> continue guid = '%s-%s' % (set_type, i) text = line[0] bbox = line[1] label = line[2] examples.append(DocExample(guid=guid, text_a=text, text_b=None, bbox=bbox, label=label)) return examples",if i == 0:
"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = mod_attr_path.split('#') if '#' in mod_attr_path else (mod_attr_path, '') full_mod_path = '{}.{}'.format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) <IF_STMT> for part in attr_path.split('.'): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: if checked: return None raise ex",if attr_path:
"def _load_ui_modules(self, modules: Any) -> None: if isinstance(modules, types.ModuleType): self._load_ui_modules(dict(((n, getattr(modules, n)) for n in dir(modules)))) elif isinstance(modules, list): for m in modules: self._load_ui_modules(m) else: assert isinstance(modules, dict) for name, cls in modules.items(): try: <IF_STMT> self.ui_modules[name] = cls except TypeError: pass","if issubclass(cls, UIModule):"
"def _remove_obsolete_leafs(input_dict): if not isinstance(input_dict, dict): return if input_dict[LEAF_MARKER]: bottom_leafs = input_dict[LEAF_MARKER] for leaf in bottom_leafs: <IF_STMT> input_dict[LEAF_MARKER].remove(leaf) for subtree in input_dict.keys(): _remove_obsolete_leafs(input_dict[subtree])",if leaf in input_dict:
"def decode(self, value, force=False): """"""Return a unicode string from the bytes-like representation"""""" if self.decode_responses or force: <IF_STMT> value = value.tobytes() if isinstance(value, bytes): value = value.decode(self.encoding, self.encoding_errors) return value","if isinstance(value, memoryview):"
"def audit(self, directive): value = _get_value(directive) if not value: return server_side = directive.name.startswith('proxy_') for var in compile_script(value): char = '' <IF_STMT> char = '\\n' elif not server_side and var.can_contain('\r'): char = '\\r' else: continue reason = 'At least variable ""${var}"" can contain ""{char}""'.format(var=var.name, char=char) self.add_issue(directive=[directive] + var.providers, reason=reason)",if var.can_contain('\n'):
"def checkFilename(filename): while True: if filename[0] == ""'"": filename = filename[1:] <IF_STMT> filename = filename[:-1] if os.path.exists(filename): return filename filename = input(""[!] Cannot find '%s'.\n[*] Enter a valid name of the file containing the paths to test -> "" % filename)","if filename[len(filename) - 1] == ""'"":"
"def findfiles(self, dir, base, rec): try: names = os.listdir(dir or os.curdir) except os.error as msg: print(msg) return [] list = [] subdirs = [] for name in names: fn = os.path.join(dir, name) <IF_STMT> subdirs.append(fn) elif fnmatch.fnmatch(name, base): list.append(fn) if rec: for subdir in subdirs: list.extend(self.findfiles(subdir, base, rec)) return list",if os.path.isdir(fn):
"def loop(handler, obj): handler.response.write('<table>') for k, v in obj.__dict__.items(): <IF_STMT> style = 'color: red' if not v else '' handler.response.write('<tr style=""{}""><td>{}:</td><td>{}</td></tr>'.format(style, k, v)) handler.response.write('</table>')","if not k in ('data', 'gae_user', 'credentials', 'content', 'config'):"
"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: if sys.platform == 'win32': executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) <IF_STMT> return executable pytest.skip('no suitable %s found' % (name,)) return executable",if executable.check():
"def __init__(self, socketpath=None): if socketpath is None: <IF_STMT> socketpath = '/var/run/usbmuxd' else: socketpath = '/var/run/usbmuxd' self.socketpath = socketpath self.listener = MuxConnection(socketpath, BinaryProtocol) try: self.listener.listen() self.version = 0 self.protoclass = BinaryProtocol except MuxVersionError: self.listener = MuxConnection(socketpath, PlistProtocol) self.listener.listen() self.protoclass = PlistProtocol self.version = 1 self.devices = self.listener.devices",if sys.platform == 'darwin':
"def _validate_distinct_on_different_types_and_field_orders(self, collection, query, expected_results, get_mock_result): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): <IF_STMT> self.assertDictEqual(results[i], expected_results[i]) elif isinstance(results[i], list): self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0","if isinstance(results[i], dict):"
"def getRootId(self, id): with self.connect() as cu: while True: stmt = 'select parent_path_id from hierarchy where path_id = ?' cu.execute(stmt, (id,)) parent_id = cu.fetchone()[0] <IF_STMT> return id id = parent_id",if parent_id is None or parent_id == id:
"def add(self, path): with self.get_lock(path): <IF_STMT> self.entries[path] = {} self.entries[path]['lock'] = self.new_locks[path] del self.new_locks[path] self.lru.append(path)",if not path in self.entries:
"def _get_coordinates_for_dataset_key(self, dsid): """"""Get the coordinate dataset keys for *dsid*."""""" ds_info = self.ids[dsid] cids = [] for cinfo in ds_info.get('coordinates', []): if not isinstance(cinfo, dict): cinfo = {'name': cinfo} cinfo['resolution'] = ds_info['resolution'] <IF_STMT> cinfo['polarization'] = ds_info['polarization'] cid = DatasetID(**cinfo) cids.append(self.get_dataset_key(cid)) return cids",if 'polarization' in ds_info:
"def build_from_gdobj(cls, gdobj, steal=False): ret = BuiltinInitPlaceholder() if steal: assert ffi.typeof(gdobj).kind == 'pointer' ret._gd_ptr = gdobj el<IF_STMT> ret._gd_ptr = cls._copy_gdobj(gdobj) else: ret._gd_ptr = cls._copy_gdobj(ffi.addressof(gdobj)) ret.__class__ = cls return ret",if ffi.typeof(gdobj).kind == 'pointer':
"def _listen_output(self): """"""NB! works in background thread"""""" try: while True: chars = self._proc.read(1) <IF_STMT> as_bytes = chars.encode(self.encoding) self._make_output_available(as_bytes) else: self._error = 'EOF' break except Exception as e: self._error = str(e)",if len(chars) > 0:
"def result(metrics: Dict[metric_types.MetricKey, Any]) -> Dict[metric_types.AttributionsKey, Dict[Text, Union[float, np.ndarray]]]: """"""Returns mean attributions."""""" total_attributions = metrics[total_attributions_key] weighted_count = metrics[weighted_example_count_key] attributions = {} for k, v in total_attributions.items(): <IF_STMT> attributions[k] = float('nan') else: attributions[k] = v / weighted_count return {key: attributions}","if np.isclose(weighted_count, 0.0):"
"def write_if_changed(path, data): if isinstance(data, str): data = data.encode() changed = False with open(os.open(path, os.O_CREAT | os.O_RDWR), 'wb+') as f: f.seek(0) current = f.read() <IF_STMT> changed = True f.seek(0) f.write(data) f.truncate() os.fsync(f) return changed",if current != data:
"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): <IF_STMT> if scan_argv(self.argv, other_option) is not None: raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option",if option != other_option:
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: <IF_STMT> continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if arg is None:
"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks except AttributeError: tasks = self.tasks <IF_STMT> app = tasks[0]._app if app is None and body is not None: app = body._app return app if app is not None else current_app",if len(tasks):
"def add_field(self, field): self.remove_field(field.name) self.fields[field.name] = field self.columns[field.db_column] = field self._sorted_field_list.insert(field) self._update_field_lists() if field.default is not None: self.defaults[field] = field.default <IF_STMT> self._default_callables[field] = field.default self._default_callable_list.append((field.name, field.default)) else: self._default_dict[field] = field.default self._default_by_name[field.name] = field.default",if callable(field.default):
"def _get_families(self): families = [] for name, ext in self._get_family_dirs(): <IF_STMT> family = self.get_resource(FileSystemPackageFamilyResource.key, location=self.location, name=name) else: family = self.get_resource(FileSystemCombinedPackageFamilyResource.key, location=self.location, name=name, ext=ext) families.append(family) return families",if ext is None:
"def test(model, data_loader, device=None): device = device or torch.device('cpu') model.eval() correct = 0 total = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(data_loader): <IF_STMT> break data, target = (data.to(device), target.to(device)) outputs = model(data) _, predicted = torch.max(outputs.data, 1) total += target.size(0) correct += (predicted == target).sum().item() return correct / total",if batch_idx * len(data) > TEST_SIZE:
"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: <IF_STMT> sleep_time = ThreadPool.PROGRESS_IDLE_DELAY elif self.__show_animation: self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY time.sleep(sleep_time)",if not self.__progress_status:
"def _parse_subtitles(self, video_data, url_key): subtitles = {} for translation in video_data.get('translations', []): vtt_path = translation.get(url_key) <IF_STMT> continue lang = translation.get('language_w3c') or ISO639Utils.long2short(translation['language_medium']) subtitles.setdefault(lang, []).append({'ext': 'vtt', 'url': vtt_path}) return subtitles",if not vtt_path:
"def postprocess_message(self, msg): if msg['type'] == 'sample' and msg['value'] is not None: fn, value = (msg['fn'], msg['value']) value_batch_ndims = jnp.ndim(value) - fn.event_dim fn_batch_ndim = len(fn.batch_shape) <IF_STMT> prepend_shapes = (1,) * (value_batch_ndims - fn_batch_ndim) msg['fn'] = tree_map(lambda x: jnp.reshape(x, prepend_shapes + jnp.shape(x)), fn)",if fn_batch_ndim < value_batch_ndims:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_filename(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def createError(self, line, pos, description): global ENABLE_PYIMPORT msg = 'Line ' + unicode(line) + ': ' + unicode(description) if ENABLE_JS2PY_ERRORS: <IF_STMT> import js2py.base return js2py.base.MakeError('SyntaxError', msg) else: return ENABLE_JS2PY_ERRORS(msg) else: return JsSyntaxError(msg)","if isinstance(ENABLE_JS2PY_ERRORS, bool):"
"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract(page, start_index, end_index, self.template.ignored_regions) for item in arg_to_iter(extracted): <IF_STMT> if isinstance(item, (ItemProcessor, dict)): item[u'_template'] = self.template.id items.append(item) return items",if item:
"def create_volume(self, volume): """"""Create a volume."""""" try: cmd = ['volume', 'create', volume['name'], '%sG' % volume['size']] <IF_STMT> cmd.append('pool') cmd.append(self.configuration.eqlx_pool) if self.configuration.san_thin_provision: cmd.append('thin-provision') out = self._eql_execute(*cmd) self.add_multihost_access(volume) return self._get_volume_data(out) except Exception: with excutils.save_and_reraise_exception(): LOG.error('Failed to create volume ""%s"".', volume['name'])",if self.configuration.eqlx_pool != 'default':
"def clean(self): if not self.code: self.code = u'static-%s' % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter(code=self.code, site__isnull=True) <IF_STMT> placeholders = placeholders.exclude(pk=self.pk) if placeholders.exists(): raise ValidationError(_('A static placeholder with the same site and code already exists'))",if self.pk:
"def spawnMenu(self, event): clickedPos = self.getRowByAbs(event.Position) self.ensureSelection(clickedPos) selection = self.getSelectedBoosters() mainBooster = None if clickedPos != -1: try: booster = self.boosters[clickedPos] except IndexError: pass else: <IF_STMT> mainBooster = booster itemContext = None if mainBooster is None else _t('Booster') menu = ContextMenu.getMenu(self, mainBooster, selection, ('boosterItem', itemContext), ('boosterItemMisc', itemContext)) if menu: self.PopupMenu(menu)",if booster in self.original:
"def init_errorhandler(): for ex in default_exceptions: <IF_STMT> app.register_error_handler(ex, error_http) elif ex == 500: app.register_error_handler(ex, internal_error) if services.ldap:  @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug('LDAP server not accessible while trying to login to opds feed') return error_http(FailedDependency())",if ex < 500:
"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): <IF_STMT> t = anytype elif 'M' in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif 'i' in fmt: t = int elif 'f' in fmt: t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",if shape:
def Proc2(IntParIO): IntLoc = IntParIO + 10 while True: if Char1Glob == 'A': IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 <IF_STMT> break return IntParIO,if EnumLoc == Ident1:
"def opengroup(self, name=None): gid = self.groups self.groupwidths.append(None) if self.groups > MAXGROUPS: raise error('too many groups') if name is not None: ogid = self.groupdict.get(name, None) <IF_STMT> raise error('redefinition of group name %r as group %d; was group %d' % (name, gid, ogid)) self.groupdict[name] = gid return gid",if ogid is not None:
"def __setattr__(self, name: str, val: Any): if name.startswith('COMPUTED_'): if name in self: old_val = self[name] <IF_STMT> return raise KeyError(""Computed attributed '{}' already exists with a different value! old={}, new={}."".format(name, old_val, val)) self[name] = val else: super().__setattr__(name, val)",if old_val == val:
"def get_all_function_symbols(self, module='kernel'): """"""Gets all the function tuples for the given module"""""" ret = [] symtable = self.type_map if module in symtable: mod = symtable[module] for addr, (name, _sym_types) in mod.items(): <IF_STMT> addr = addr + self.shift_address ret.append([name, addr]) else: debug.info('All symbols requested for non-existent module %s' % module) return ret",if self.shift_address and addr:
"def __call__(self, frame: FrameType, event: str, arg: Any) -> 'CallTracer': code = frame.f_code if event not in SUPPORTED_EVENTS or code.co_name == 'trace_types' or (self.should_trace and (not self.should_trace(code))): return self try: <IF_STMT> self.handle_call(frame) elif event == EVENT_RETURN: self.handle_return(frame, arg) else: logger.error('Cannot handle event %s', event) except Exception: logger.exception('Failed collecting trace') return self",if event == EVENT_CALL:
def test_update_topic(self): async with self.chat_client: await self._create_thread() topic = 'update topic' async with self.chat_thread_client: await self.chat_thread_client.update_topic(topic=topic) <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():
"def render_observation(self): x = self.read_head_position label = 'Observation Grid: ' x_str = '' for j in range(-1, self.rows + 1): if j != -1: x_str += ' ' * len(label) for i in range(-2, self.input_width + 2): <IF_STMT> x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += '\n' x_str = label + x_str return x_str",if i == x[0] and j == x[1]:
"def build(opt): dpath = os.path.join(opt['datapath'], 'QA-ZRE') version = None if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):
"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get('origin', '') <IF_STMT> origin = url url = repo.remotes.get(origin) if url: repo.pull(origin_uri=url) else: print('No pull URL.') else: print(command_help['git pull'])",if url in repo.remotes:
"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b'' last_sop_idx = sop_idx = 0 skip = True for opcode, data, sop_idx in script.raw_iter(): <IF_STMT> r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx if script[sop_idx:sop_idx + len(sig)] == sig: skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",if not skip:
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result['venture_id'] = ip.venture.id if ip.device is not None: result['device_id'] = ip.device.id <IF_STMT> result['venture_id'] = ip.device.venture.id return result",if ip.device.venture is not None:
"def restore(self, state): """"""Restore the state of a mesh previously saved using save()"""""" import pickle state = pickle.loads(state) for k in state: if isinstance(state[k], list): <IF_STMT> state[k] = [[v.x(), v.y(), v.z()] for v in state[k]] state[k] = np.array(state[k]) setattr(self, k, state[k])","if isinstance(state[k][0], QtGui.QVector3D):"
"def get_extra_lines(tup): ext_name, pyopencl_ver = tup if ext_name is not None: <IF_STMT> yield '' yield ('Available with OpenCL %s.' % ext_name[3:]) yield '' else: yield '' yield ('Available with the ``%s`` extension.' % ext_name) yield '' if pyopencl_ver is not None: yield '' yield ('.. versionadded:: %s' % pyopencl_ver) yield ''",if ext_name.startswith('CL_'):
"def _gen_remote_uri(fileobj: IO[bytes], remote_uri: Optional[ParseResult], remote_path_prefix: Optional[str], remote_path_suffix: Optional[str], sha256sum: Optional[str]) -> ParseResult: if remote_uri is None: assert remote_path_prefix is not None and remote_path_suffix is not None <IF_STMT> sha256sum = _hash_fileobj(fileobj) return urlparse(os.path.join(remote_path_prefix, f'{sha256sum}{remote_path_suffix}')) else: return remote_uri",if sha256sum is None:
"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) if not cmd.check(f'docker check for {self.path.k8s}'): <IF_STMT> log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) if log_cmd.check(f'docker logs for {self.path.k8s}'): print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if not cmd.stdout.strip():
def get_range(self): present = self.xml.find('{%s}range' % self.namespace) if present is not None: attributes = present.attrib return_value = dict() <IF_STMT> return_value['minimum'] = attributes['min'] if 'max' in attributes: return_value['maximum'] = attributes['max'] return return_value return False,if 'min' in attributes:
"def _configuredOn(self, workerid, builderid=None, masterid=None): cfg = [] for cs in itervalues(self.configured): <IF_STMT> continue bid, mid = self.db.builders.builder_masters[cs['buildermasterid']] if builderid is not None and bid != builderid: continue if masterid is not None and mid != masterid: continue cfg.append({'builderid': bid, 'masterid': mid}) return cfg",if cs['workerid'] != workerid:
"def __exit__(self, type, value, traceback): try: if type is not None: return self.exception_handler(type, value, traceback) finally: final_contexts = _state.contexts _state.contexts = self.old_contexts <IF_STMT> raise StackContextInconsistentError('stack_context inconsistency (may be caused by yield within a ""with StackContext"" block)') self.new_contexts = None",if final_contexts is not self.new_contexts:
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: <IF_STMT> return None elif self._keys[hash_] == key: self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: return None",if self._keys[hash_] is self._empty:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_logout_url(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def data_generator(): i = 0 max_batch_index = len(X_train) // batch_size tot = 0 while 1: <IF_STMT> yield (np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan) else: yield (X_train[i * batch_size:(i + 1) * batch_size], y_train[i * batch_size:(i + 1) * batch_size]) i += 1 tot += 1 i = i % max_batch_index",if tot > 3 * len(X_train):
"def title(self): ret = theme['title'] if isinstance(self.name, six.string_types): width = self.statwidth() return ret + self.name[0:width].center(width).replace(' ', '-') + theme['default'] for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace(' ', '-') <IF_STMT> if op.color: ret = ret + theme['frame'] + char['dash'] + theme['title'] else: ret = ret + char['space'] return ret",if i + 1 != len(self.vars):
"def get_container_from_dport(dport, docker_client): for container in docker_client.containers(): try: ports = container['Ports'] for port in ports: <IF_STMT> if port['PublicPort'] == int(dport): return container except KeyError: print(ports) pass",if 'PublicPort' in port:
"def _get_parents_data(self, data): parents = 0 if data[COLUMN_PARENT]: family = self.db.get_family_from_handle(data[COLUMN_PARENT][0]) if family.get_father_handle(): parents += 1 <IF_STMT> parents += 1 return parents",if family.get_mother_handle():
"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) if old_mtime == mtime: cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) <IF_STMT> cache.popitem(last=False) return result",if len(cache) > max_size:
"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) <IF_STMT> cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",elif op.stage == OperandStage.agg:
"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b'' last_sop_idx = sop_idx = 0 skip = True for opcode, data, sop_idx in script.raw_iter(): if not skip: r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx <IF_STMT> skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",if script[sop_idx:sop_idx + len(sig)] == sig:
"def extractall(zip: typing.Any, path: str) -> NoneType: for name in zip.namelist(): member = zip.getinfo(name) extracted_path = zip._extract_member(member, path, None) attr = member.external_attr >> 16 <IF_STMT> os.chmod(extracted_path, attr)",if attr != 0:
"def find_all_gyptest_files(directory): result = [] for root, dirs, files in os.walk(directory): <IF_STMT> dirs.remove('.svn') result.extend([os.path.join(root, f) for f in files if is_test_name(f)]) result.sort() return result",if '.svn' in dirs:
"def load(cls, storefile, template_store): if not hasattr(storefile, 'read'): storefile = open(storefile, 'rb') store = cls.convertfile(storefile, template_store) for unit in store.units: if unit.isheader(): continue <IF_STMT> unit.target = unit.source unit.rich_target = unit.rich_source return store",if cls.needs_target_sync:
"def postOptions(self): _BasicOptions.postOptions(self) if self['jobs']: conflicts = ['debug', 'profile', 'debug-stacktraces', 'exitfirst'] for option in conflicts: if self[option]: raise usage.UsageError(""You can't specify --%s when using --jobs"" % option) if self['nopm']: <IF_STMT> raise usage.UsageError('You must specify --debug when using --nopm ') failure.DO_POST_MORTEM = False",if not self['debug']:
"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: if not i < len(extra.tokens): break entry = extra.tokens[i] token = jsdict({'type': entry.type, 'value': entry.value}) if extra.range: token.range = entry.range <IF_STMT> token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",if extra.loc:
"def on_rebalance_end(self) -> None: """"""Call when rebalancing is done."""""" self.rebalancing = False if self._rebalancing_span: self._rebalancing_span.finish() self._rebalancing_span = None sensor_state = self._rebalancing_sensor_state try: <IF_STMT> self.log.warning('Missing sensor state for rebalance #%s', self.rebalancing_count) else: self.sensors.on_rebalance_end(self, sensor_state) finally: self._rebalancing_sensor_state = None",if not sensor_state:
"def decorator(request, *args, **kwargs): if CALENDAR_VIEW_PERM: user = request.user if not user: return HttpResponseRedirect(settings.LOGIN_URL) occurrence, event, calendar = get_objects(request, **kwargs) if calendar: allowed = CHECK_CALENDAR_PERM_FUNC(calendar, user) <IF_STMT> return HttpResponseRedirect(settings.LOGIN_URL) return function(request, *args, **kwargs) return HttpResponseNotFound('<h1>Page not found</h1>') return function(request, *args, **kwargs)",if not allowed:
"def reduce_arguments(self, args): assert isinstance(args, nodes.Arguments) if args.incorrect_order(): raise InvalidArguments('All keyword arguments must be after positional arguments.') reduced_pos = [self.reduce_single(arg) for arg in args.arguments] reduced_kw = {} for key in args.kwargs.keys(): <IF_STMT> raise InvalidArguments('Keyword argument name is not a string.') a = args.kwargs[key] reduced_kw[key] = self.reduce_single(a) return (reduced_pos, reduced_kw)","if not isinstance(key, str):"
"def _encode(n, nbytes, little_endian=False): retval = [] n = long(n) for i in range(nbytes): <IF_STMT> retval.append(chr(n & 255)) else: retval.insert(0, chr(n & 255)) n >>= 8 return ''.join(retval)",if little_endian:
"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() new_i.id = self.id cls.id = old_id for prop in cls.properties: if prop is not 'members': <IF_STMT> val = getattr(self, prop) setattr(new_i, prop, val) new_i.members = [] return new_i",if self.has(prop):
"def dataspec(config): master = (yield fakemaster.make_master()) data = connector.DataConnector() data.setServiceParent(master) if config['out'] != '--': dirs = os.path.dirname(config['out']) <IF_STMT> os.makedirs(dirs) f = open(config['out'], 'w') else: f = sys.stdout if config['global'] is not None: f.write('window.' + config['global'] + '=') f.write(json.dumps(data.allEndpoints(), indent=2)) f.close() defer.returnValue(0)",if dirs and (not os.path.exists(dirs)):
"def _parseSCDOCDC(self, src): """"""[S|CDO|CDC]*"""""" while 1: src = src.lstrip() <IF_STMT> src = src[4:] elif src.startswith('-->'): src = src[3:] else: break return src",if src.startswith('<!--'):
"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) if visitor.needs_fix(): print('%s: %s' % (filename, visitor.get_stats())) <IF_STMT> print('Fixing: %s' % filename) fix_file(filename)",if fix:
"def shutdown(self): """"""Shutdown host system."""""" self._check_dbus(MANAGER) use_logind = self.sys_dbus.logind.is_connected _LOGGER.info('Initialize host power off %s', 'logind' if use_logind else 'systemd') try: await self.sys_core.shutdown() finally: <IF_STMT> await self.sys_dbus.logind.power_off() else: await self.sys_dbus.systemd.power_off()",if use_logind:
"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] if _is_punctuation(char): output.append([char]) start_new_word = True else: <IF_STMT> output.append([]) start_new_word = False output[-1].append(char) i += 1 return [''.join(x) for x in output]",if start_new_word:
"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: if tp == 'write': out.write(msg) <IF_STMT> out.flush() elif tp == 'write_flush': out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",elif tp == 'flush':
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) <IF_STMT> results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results","elif isinstance(in_arrays, dict):"
"def check_billing_view(user, permission, obj): if hasattr(obj, 'all_projects'): <IF_STMT> return True return any((check_permission(user, permission, prj) for prj in obj.all_projects)) return check_permission(user, permission, obj)",if user.is_superuser or obj.owners.filter(pk=user.pk).exists():
"def ensure_output_spaces_contain_the_same_data(self, y, y_ensured): stride = y.shape[1] self.assertEqual(y.shape[0] * y.shape[1], y_ensured.shape[0]) self.assertEqual(len(y_ensured.shape), 1) for row in range(y.shape[0]): for column in range(y.shape[1]): <IF_STMT> self.assertEqual(y[row, column], y_ensured[row * stride + column]) else: self.assertEqual(y[row][column], y_ensured[row * stride + column])",if sp.issparse(y):
"def train(self, training_data: TrainingData, config: Optional[RasaNLUModelConfig]=None, **kwargs: Any) -> None: """"""Tokenize all training data."""""" for example in training_data.training_examples: for attribute in MESSAGE_ATTRIBUTES: if example.get(attribute) is not None and (not example.get(attribute) == ''): <IF_STMT> tokens = self._split_name(example, attribute) else: tokens = self.tokenize(example, attribute) example.set(TOKENS_NAMES[attribute], tokens)","if attribute in [INTENT, ACTION_NAME, INTENT_RESPONSE_KEY]:"
"def refresh_token(self, strategy, *args, **kwargs): token = self.extra_data.get('refresh_token') or self.extra_data.get('access_token') backend = self.get_backend(strategy) if token and backend and hasattr(backend, 'refresh_token'): backend = backend(strategy=strategy) response = backend.refresh_token(token, *args, **kwargs) extra_data = backend.extra_data(self, self.uid, response, self.extra_data) <IF_STMT> self.save()",if self.set_extra_data(extra_data):
"def _verify_environ(_collected_environ): try: yield finally: new_environ = dict(os.environ) current_test = new_environ.pop('PYTEST_CURRENT_TEST', None) old_environ = dict(_collected_environ) old_environ.pop('PYTEST_CURRENT_TEST', None) <IF_STMT> raise DirtyTest('Left over environment variables', current_test, _compare_eq_dict(new_environ, old_environ, verbose=2))",if new_environ != old_environ:
"def clean_len(self, line): """"""Calculate wisible length of string"""""" if isinstance(line, basestring): return len(self.screen.markup.clean_markup(line)) elif isinstance(line, tuple) or isinstance(line, list): markups = self.screen.markup.get_markup_vars() length = 0 for i in line: <IF_STMT> length += len(i) return length",if i not in markups:
"def _build_merged_dataset_args(datasets): merged_dataset_args = [] for dataset in datasets: dataset_code_column = _parse_dataset_code(dataset) arg = dataset_code_column['code'] column_index = dataset_code_column['column_index'] <IF_STMT> arg = (dataset_code_column['code'], {'column_index': [column_index]}) merged_dataset_args.append(arg) return merged_dataset_args",if column_index is not None:
"def update_watch_data_table_paths(self): if hasattr(self.tool_data_watcher, 'monitored_dirs'): for tool_data_table_path in self.tool_data_paths: <IF_STMT> self.tool_data_watcher.watch_directory(tool_data_table_path)",if tool_data_table_path not in self.tool_data_watcher.monitored_dirs:
"def getsource(obj): """"""Wrapper around inspect.getsource"""""" try: try: src = encoding.to_unicode(inspect.getsource(obj)) except TypeError: <IF_STMT> src = encoding.to_unicode(inspect.getsource(obj.__class__)) else: src = getdoc(obj) return src except (TypeError, IOError): return","if hasattr(obj, '__class__'):"
"def __iter__(self): for model in self.app_config.get_models(): admin_model = AdminModel(model, **self.options) for model_re in self.model_res: if model_re.search(admin_model.name): break else: <IF_STMT> continue yield admin_model",if self.model_res:
"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() if self._has_stop_signal(raw_inputs): self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) <IF_STMT> try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",elif self._flow_type == REALTIME:
"def dump(self): self.ql.log.info('[*] Dumping object: %s' % self.sf_name) for field in self._fields_: <IF_STMT> self.ql.log.info('%s: 0x%x' % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info('%s: %d' % (field[0], getattr(self, field[0]))) elif isinstance(getattr(self, field[0]), bytes): self.ql.log.info('%s: %s' % (field[0], getattr(self, field[0]).decode()))","if isinstance(getattr(self, field[0]), POINTER64):"
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): """"""Validating that user has inputted a value set and that configuration has been initialized"""""" super().validate_configuration(configuration) try: assert 'value_set' in configuration.kwargs, 'value_set is required' assert isinstance(configuration.kwargs['value_set'], (list, set, dict)), 'value_set must be a list or a set' <IF_STMT> assert '$PARAMETER' in configuration.kwargs['value_set'], 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True","if isinstance(configuration.kwargs['value_set'], dict):"
def test_one_dead_branch(): with deterministic_PRNG(): seen = set()  @run_to_buffer def x(data): i = data.draw_bytes(1)[0] if i > 0: data.mark_invalid() i = data.draw_bytes(1)[0] if len(seen) < 255: seen.add(i) <IF_STMT> data.mark_interesting(),elif i not in seen:
"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) if event.EventObject is self.list_ctrl: self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): <IF_STMT> self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",if self.list_ctrl.IsSelected(index):
"def prime(self, callback): <IF_STMT> self.cbhdl = simulator.register_rwsynch_callback(callback, self) if self.cbhdl is None: raise_error(self, 'Unable set up %s Trigger' % str(self)) Trigger.prime(self)",if self.cbhdl is None:
"def fstab_configuration(middleware): for command in [['systemctl', 'daemon-reload'], ['systemctl', 'restart', 'local-fs.target']] if osc.IS_LINUX else [['mount', '-uw', '/']]: ret = subprocess.run(command, capture_output=True) <IF_STMT> middleware.logger.debug(f'''Failed to execute ""{' '.join(command)}"": {ret.stderr.decode()}''')",if ret.returncode:
"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield (simple_colorize(fromdesc, 'description'), simple_colorize(todesc, 'description')) for i, line in enumerate(diffs): <IF_STMT> if i > 0: yield (simple_colorize('---', 'separator'), simple_colorize('---', 'separator')) else: yield line",if line is None:
"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = ', '.join(orig_text.replace(', ', ',').split(',')[:-1]) tags = [] for tag in self.tags_list: if ',' in orig_text: <IF_STMT> tags.append('%s,%s' % (text, tag)) tags.append('%s, %s' % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)","if orig_text[-1] not in (',', ' '):"
"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord('0') <IF_STMT> digit *= 2 if digit >= 10: digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",if even:
def __get_param_string__(params): params_string = [] for key in sorted(params.keys()): <IF_STMT> return value = params[key] params_string.append('' if value == 'null' else str(value)) return '|'.join(params_string),if 'REFUND' in params[key] or '|' in params[key]:
"def _map_handlers(self, session, event_class, mapfn): for event in DOC_EVENTS: event_handler_name = event.replace('-', '_') <IF_STMT> event_handler = getattr(self, event_handler_name) format_string = DOC_EVENTS[event] num_args = len(format_string.split('.')) - 2 format_args = (event_class,) + ('*',) * num_args event_string = event + format_string % format_args unique_id = event_class + event_handler_name mapfn(event_string, event_handler, unique_id)","if hasattr(self, event_handler_name):"
"def _create_param_lr(self, param_and_grad): param = param_and_grad[0] param_lr = param.optimize_attr['learning_rate'] if type(param_lr) == Variable: return param_lr el<IF_STMT> return self._global_learning_rate() else: with default_main_program()._lr_schedule_guard(is_with_opt=True), framework.name_scope('scale_with_param_lr'): return self._global_learning_rate() * param_lr",if param_lr == 1.0:
"def __getitem__(self, key): try: return self._clsmap[key] except KeyError as e: <IF_STMT> self._mutex.acquire() try: if not self.initialized: self._init() self.initialized = True return self._clsmap[key] finally: self._mutex.release() raise e",if not self.initialized:
"def save(self, force=False): if not force: <IF_STMT> return if time.time() - self.last_save_time < 10: return with self.lock: with open(self.file_path, 'w') as fd: for ip in self.cache: record = self.cache[ip] rule = record['r'] connect_time = record['c'] update_time = record['update'] fd.write('%s %s %d %d\n' % (ip, rule, connect_time, update_time)) self.last_save_time = time.time() self.need_save = False",if not self.need_save:
"def pick(items, sel): for x, s in zip(items, sel): <IF_STMT> yield x elif not x.is_atom() and (not s.is_atom()): yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",if match(s):
"def isValidFloat(config_param_name, value, constraints): if isinstance(value, float): constraints.setdefault('min', MIN_VALID_FLOAT_VALUE) constraints.setdefault('max', MAX_VALID_FLOAT_VALUE) minv = float(constraints.get('min')) maxv = float(constraints.get('max')) <IF_STMT> if value <= maxv: return value raise FloatValueError(config_param_name, value, constraints)",if value >= minv:
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if 'meta-environment' in root or 'cross-canadian' in root: continue <IF_STMT> continue if 'do_build' not in name and 'do_populate_sdk' not in name: f.append(os.path.join(root, name)) return f",if 'qemux86copy-' in root or 'qemux86-' in root:
"def __get_photo(self, person_or_marriage): """"""returns the first photo in the media list or None"""""" media_list = person_or_marriage.get_media_list() for media_ref in media_list: media_handle = media_ref.get_reference_handle() media = self.database.get_media_from_handle(media_handle) mime_type = media.get_mime_type() <IF_STMT> return media return None",if mime_type and mime_type.startswith('image'):
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError('TypeError', 'callbackfn must be a function') _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) <IF_STMT> res.append(kValue) k += 1 return args.space.ConstructArray(res)","if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):"
"def optimize(self, graph: Graph): for v in graph.inputs: if not v.has_attribute(SplitTarget): continue <IF_STMT> DumpGraph().optimize(graph) raise NotImplementedError(f'Input Variable {v} is too large to handle in WebGL backend') return (graph, False)",if flags.DEBUG:
"def detach_volume(self, volume): for node in self.list_nodes(): if type(node.image) is not list: continue for disk in node.image: <IF_STMT> disk_id = disk.extra['disk_id'] return self._do_detach_volume(node.id, disk_id) return False",if disk.id == volume.id:
"def Yield(value, level=1): g = greenlet.getcurrent() while level != 0: if not isinstance(g, genlet): raise RuntimeError('yield outside a genlet') <IF_STMT> g.parent.set_child(g) g = g.parent level -= 1 g.switch(value)",if level > 1:
"def get_all_pipeline_nodes(pipeline: pipeline_pb2.Pipeline) -> List[pipeline_pb2.PipelineNode]: """"""Returns all pipeline nodes in the given pipeline."""""" result = [] for pipeline_or_node in pipeline.nodes: which = pipeline_or_node.WhichOneof('node') <IF_STMT> result.append(pipeline_or_node.pipeline_node) else: raise NotImplementedError('Only pipeline nodes supported.') return result",if which == 'pipeline_node':
"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): if not hasattr(self, name): setattr(self, name, value) for name, value in settings.items(): <IF_STMT> raise ImproperlyConfigured(""Invalid setting '{}' for {}"".format(name, self.__class__.__name__)) setattr(self, name, value)",if name not in default_settings:
"def _check_choice(self): if self.type == 'choice': <IF_STMT> raise OptionError(""must supply a list of choices for type 'choice'"", self) elif type(self.choices) not in (types.TupleType, types.ListType): raise OptionError(""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self) elif self.choices is not None: raise OptionError('must not supply choices for type %r' % self.type, self)",if self.choices is None:
"def prepare(self, size=None): if _is_seekable(self.file): start_pos = self.file.tell() self.file.seek(0, 2) end_pos = self.file.tell() self.file.seek(start_pos) fsize = end_pos - start_pos <IF_STMT> self.remain = fsize else: self.remain = min(fsize, size) return self.remain",if size is None:
"def _setSitemapTargets(): if not conf.sitemapUrl: return infoMsg = ""parsing sitemap '%s'"" % conf.sitemapUrl logger.info(infoMsg) found = False for item in parseSitemap(conf.sitemapUrl): <IF_STMT> found = True kb.targets.add((item.strip(), None, None, None, None)) if not found and (not conf.forms) and (not conf.crawlDepth): warnMsg = 'no usable links found (with GET parameters)' logger.warn(warnMsg)","if re.match('[^ ]+\\?(.+)', item, re.I):"
"def test_CY_decomposition(self, tol): """"""Tests that the decomposition of the CY gate is correct"""""" op = qml.CY(wires=[0, 1]) res = op.decomposition(op.wires) mats = [] for i in reversed(res): <IF_STMT> mats.append(np.kron(i.matrix, np.eye(2))) else: mats.append(i.matrix) decomposed_matrix = np.linalg.multi_dot(mats) assert np.allclose(decomposed_matrix, op.matrix, atol=tol, rtol=0)",if len(i.wires) == 1:
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break <IF_STMT> lidx += 1 if not start: start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",if stmt == lines[lidx]:
"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode='fan_out') <IF_STMT> init.constant(module.bias, 0) elif isinstance(module, nn.BatchNorm2d): init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=0.001) if module.bias: init.constant(module.bias, 0)",if module.bias:
def _get_directory_size_in_bytes(directory): total = 0 try: for entry in os.scandir(directory): <IF_STMT> total += entry.stat().st_size elif entry.is_dir(): total += _get_directory_size_in_bytes(entry.path) except NotADirectoryError: return os.path.getsize(directory) except PermissionError: return 0 return total,if entry.is_file():
"def run_cmd(self, util, to, always_push_mark=False): if to == 'bof': util.push_mark_and_goto_position(0) elif to == 'eof': util.push_mark_and_goto_position(self.view.size()) elif to in ('eow', 'bow'): visible = self.view.visible_region() pos = visible.a if to == 'bow' else visible.b <IF_STMT> util.push_mark_and_goto_position(pos) else: util.set_cursors([sublime.Region(pos)])",if always_push_mark:
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if 'results.train_y_misclass' in dd: if dd['results.train_y_misclass'] < optimal_measure: optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): <IF_STMT> print(key + ': ' + str(value))",if 'hyper_parameters' in key:
"def clean_vc_position(self): vc_position = self.cleaned_data['vc_position'] if self.validate_vc_position: conflicting_members = Device.objects.filter(virtual_chassis=self.instance.virtual_chassis, vc_position=vc_position) <IF_STMT> raise forms.ValidationError('A virtual chassis member already exists in position {}.'.format(vc_position)) return vc_position",if conflicting_members.exists():
"def cal_pads(auto_pad, pad_shape): spatial_size = len(pad_shape) pads = [0] * spatial_size * 2 for i in range(spatial_size): if auto_pad == 'SAME_LOWER': pads[i + spatial_size] = pad_shape[i] // 2 pads[i] = pad_shape[i] - pads[i + spatial_size] <IF_STMT> pads[i] = pad_shape[i] // 2 pads[i + spatial_size] = pad_shape[i] - pads[i] return pads",elif auto_pad == 'SAME_UPPER':
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def test_cwl_rnaseq(self, install_test_files): with install_cwl_test_files() as work_dir: with utils.chdir(os.path.join(work_dir, 'rnaseq')): <IF_STMT> shutil.rmtree('cromwell_work') subprocess.check_call(['bcbio_vm.py', 'cwlrun', 'cromwell', 'rnaseq-workflow'])",if os.path.exists('cromwell_work'):
def files_per_version(self): xpath = './files/file' files = self.root.findall(xpath) versions = {} for file in files: vfile = file.findall('version') for version in vfile: nb = version.attrib['nb'] <IF_STMT> versions[nb] = [] versions[nb].append(file.attrib['url']) return versions,if not nb in versions:
"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('SQLite backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value)",if settings.USE_TZ:
"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: t.status = 'aborted by user' <IF_STMT> status('%s aborted' % t.name, priority=2) except Exception as e: t.exception = e t.status = 'exception' vd.exceptionCaught(e) if t.sheet: t.sheet.currentThreads.remove(t)",if status:
"def ESP(phrase): for num, name in enumerate(devname): if name.lower() in phrase: dev = devid[num] <IF_STMT> ctrl = '=ON' say('Turning On ' + name) elif custom_action_keyword['Dict']['Off'] in phrase: ctrl = '=OFF' say('Turning Off ' + name) rq = requests.head('https://' + ip + dev + ctrl, verify=False)",if custom_action_keyword['Dict']['On'] in phrase:
"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] <IF_STMT> parts.append('PRIMARY KEY') if not_null: parts.append('NOT NULL') result[name] = ' '.join(parts) return result",if primary_key:
"def _validate_forward_input(x, n_in): if n_in != 1: if not isinstance(x, (tuple, list)): raise TypeError(f'Expected input to be a tuple or list; instead got {type(x)}.') <IF_STMT> raise ValueError(f'Input tuple length ({len(x)}) does not equal required number of inputs ({n_in}).')",if len(x) != n_in:
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): if isinstance(val, compat.string_types): return '  %s' % val <IF_STMT> return '  %.1f KB' % (val / 1024.0 ** 1) elif val < 1024 ** 3: return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val",elif val < 1024 ** 2:
"def get_path_name(self): if self.is_root(): return '@' + self.name else: parent_name = self.parent.get_path_name() <IF_STMT> return '/'.join([parent_name, '@' + self.name]) else: return '@' + self.name",if parent_name:
"def parse(cls, api, json): lst = List(api) setattr(lst, '_json', json) for k, v in json.items(): <IF_STMT> setattr(lst, k, User.parse(api, v)) elif k == 'created_at': setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",if k == 'user':
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: if not py_file.endswith('.py'): continue <IF_STMT> bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if self.compile:
"def to_json_dict(self): d = super().to_json_dict() d['bullet_list'] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: <IF_STMT> d['header'] = self.header.to_json_dict() else: d['header'] = self.header if self.subheader is not None: if isinstance(self.subheader, RenderedContent): d['subheader'] = self.subheader.to_json_dict() else: d['subheader'] = self.subheader return d","if isinstance(self.header, RenderedContent):"
"def makeSomeFiles(pathobj, dirdict): pathdict = {} for key, value in dirdict.items(): child = pathobj.child(key) if isinstance(value, bytes): pathdict[key] = child child.setContent(value) <IF_STMT> child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError('only strings and dicts allowed as values') return pathdict","elif isinstance(value, dict):"
"def Restore(self): picker, obj = (self._window, self._pObject) value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: <IF_STMT> if type(value) == list: value = value[-1] picker.SetPath(value) return True return False","if issubclass(picker.__class__, wx.FileDialog):"
"def recv(self, buffer_size): try: return super(SSLConnection, self).recv(buffer_size) except ssl.SSLError as err: <IF_STMT> return b'' if err.args[0] in (ssl.SSL_ERROR_EOF, ssl.SSL_ERROR_ZERO_RETURN): self.handle_close() return b'' raise","if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):"
"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in ('toplevel', 'detailed'): <IF_STMT> category = category.split('/')[0] if category not in self.errors_by_category: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",if self.counting != 'detailed':
"def _get_y(self, data_inst): if self.stratified: y = [v for i, v in data_inst.mapValues(lambda v: v.label).collect()] <IF_STMT> y = self.transform_regression_label(data_inst) else: y = [0] * data_inst.count() return y",if self.need_transform:
"def test_all_project_files(self): if sys.platform.startswith('win'): return for filepath in support.all_project_files(): with open(filepath, 'rb') as fp: encoding = tokenize.detect_encoding(fp.readline)[0] self.assertIsNotNone(encoding, ""can't detect encoding for %s"" % filepath) with open(filepath, 'r') as fp: source = fp.read() source = source.decode(encoding) tree = driver.parse_string(source) new = unicode(tree) <IF_STMT> self.fail('Idempotency failed: %s' % filepath)","if diff(filepath, new, encoding):"
"def test_resource_arn_override_generator(self): overrides = set() for k, v in manager.resources.items(): arn_gen = bool(v.__dict__.get('get_arns') or v.__dict__.get('generate_arn')) <IF_STMT> overrides.add(k) overrides = overrides.difference({'account', 's3', 'hostedzone', 'log-group', 'rest-api', 'redshift-snapshot', 'rest-stage'}) if overrides: raise ValueError('unknown arn overrides in %s' % ', '.join(overrides))",if arn_gen:
"def _check_dsl_runner(self) -> None: """"""Checks if runner in dsl is Kubeflow V2 runner."""""" with open(self.flags_dict[labels.PIPELINE_DSL_PATH], 'r') as f: dsl_contents = f.read() <IF_STMT> raise RuntimeError('KubeflowV2DagRunner not found in dsl.')",if 'KubeflowV2DagRunner' not in dsl_contents:
"def create_warehouse(warehouse_name, properties=None, company=None): if not company: company = '_Test Company' warehouse_id = erpnext.encode_company_abbr(warehouse_name, company) if not frappe.db.exists('Warehouse', warehouse_id): warehouse = frappe.new_doc('Warehouse') warehouse.warehouse_name = warehouse_name warehouse.parent_warehouse = 'All Warehouses - _TCUV' warehouse.company = company warehouse.account = get_warehouse_account(warehouse_name, company) <IF_STMT> warehouse.update(properties) warehouse.save() return warehouse.name else: return warehouse_id",if properties:
"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): if not len(line.strip()): entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') <IF_STMT> entries.append(('all_comment', [line])) continue entries.append(('hostname', [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError('Multiple hostnames (%s) found!' % hostnames_found) return entries",if not len(head):
"def _get_omega(self): if self._omega is None: n = self.get_drift_dim() // 2 omg = sympl.calc_omega(n) if self.oper_dtype == Qobj: self._omega = Qobj(omg, dims=self.dyn_dims) self._omega_qobj = self._omega <IF_STMT> self._omega = sp.csr_matrix(omg) else: self._omega = omg return self._omega",elif self.oper_dtype == sp.csr_matrix:
"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v <IF_STMT> out = get_in_inputs(key, v) if out: return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]","elif isinstance(v, (list, tuple, dict)):"
def visit_binary(binary): if binary.operator == operators.eq: cols = util.column_set(chain(*[c.proxy_set for c in columns.difference(omit)])) <IF_STMT> for c in reversed(columns): if c.shares_lineage(binary.right) and (not only_synonyms or c.name == binary.left.name): omit.add(c) break,if binary.left in cols and binary.right in cols:
"def wait_tasks_or_abort(futures, timeout=60, kill_switch_ev=None): try: LazySingletonTasksCoordinator.wait_tasks(futures, return_when=FIRST_EXCEPTION, raise_exceptions=True) except Exception as e: <IF_STMT> kill_switch_ev.set() LazySingletonTasksCoordinator.wait_tasks(futures, return_when=ALL_COMPLETED, raise_exceptions=False, timeout=timeout) raise e",if kill_switch_ev is not None:
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: <IF_STMT> return False elif isinstance(s, np.ndarray) and s.size == 0: return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True",if s is None:
"def setVaName(self, va, parent=None): if parent is None: parent = self curname = self.vw.getName(va) if curname is None: curname = '' name, ok = QInputDialog.getText(parent, 'Enter...', 'Name', text=curname) if ok: name = str(name) <IF_STMT> raise Exception('Duplicate Name: %s' % name) self.vw.makeName(va, name)",if self.vw.vaByName(name):
"def generic_tag_compiler(params, defaults, name, node_class, parser, token): """"""Returns a template.Node subclass."""""" bits = token.split_contents()[1:] bmax = len(params) def_len = defaults and len(defaults) or 0 bmin = bmax - def_len if len(bits) < bmin or len(bits) > bmax: <IF_STMT> message = '%s takes %s arguments' % (name, bmin) else: message = '%s takes between %s and %s arguments' % (name, bmin, bmax) raise TemplateSyntaxError(message) return node_class(bits)",if bmin == bmax:
"def extract_segmentation_mask(annotation): poly_specs = annotation[DensePoseDataRelative.S_KEY] if isinstance(poly_specs, torch.Tensor): return poly_specs import pycocotools.mask as mask_utils segm = torch.zeros((DensePoseDataRelative.MASK_SIZE,) * 2, dtype=torch.float32) for i in range(DensePoseDataRelative.N_BODY_PARTS): poly_i = poly_specs[i] <IF_STMT> mask_i = mask_utils.decode(poly_i) segm[mask_i > 0] = i + 1 return segm",if poly_i:
"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), 'ouroboros') for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) <IF_STMT> if module_name not in IGNORE_MODULES and module_name not in native: if not (fast and module_name in KNOWN_PROBLEM_MODULES): modules.append(module_name) return set(modules)","if ext == '.py' or (ext == '' and os.path.isdir(os.path.join(basedir, name))):"
"def filelist_from_patterns(pats, rootdir=None): if rootdir is None: rootdir = '.' fileset = set([]) lines = [line.strip() for line in pats] for line in lines: pat = line[2:] newfiles = glob(osp.join(rootdir, pat)) if line.startswith('+'): fileset.update(newfiles) <IF_STMT> fileset.difference_update(newfiles) else: raise ValueError('line must start with + or -') filelist = list(fileset) return filelist",elif line.startswith('-'):
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue if entity_ref not in statuses_by_refs: continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) <IF_STMT> statuses_by_refs[entity_ref].append(status) return statuses_by_refs",if status:
"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value <IF_STMT> self._reverse_infos[info] = len(self._infos) self._infos.append(info) if reference not in self._reverse_references: self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = '%d,%d' % (self._reverse_infos[info], self._reverse_references[reference]) else: raise Exception(""unsupported type '%s'"" % type(value))",if info not in self._reverse_infos:
"def ChangeStyle(self, combos): style = 0 for combo in combos: <IF_STMT> if combo.GetLabel() == 'TR_VIRTUAL': style = style | HTL.TR_VIRTUAL else: try: style = style | eval('wx.' + combo.GetLabel()) except: style = style | eval('HTL.' + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",if combo.GetValue() == 1:
"def _parse_csrf(self, response): for d in response: if d.startswith('Set-Cookie:'): for c in d.split(':', 1)[1].split(';'): <IF_STMT> self._CSRFtoken = c.strip(' \r\n') log.verbose('Got new cookie: %s', self._CSRFtoken) break if self._CSRFtoken != None: break",if c.strip().startswith('CSRF-Token-'):
"def test_page_size_matching_max_returned_rows(app_client_returned_rows_matches_page_size): fetched = [] path = '/fixtures/no_primary_key.json' while path: response = app_client_returned_rows_matches_page_size.get(path) fetched.extend(response.json['rows']) assert len(response.json['rows']) in (1, 50) path = response.json['next_url'] <IF_STMT> path = path.replace('http://localhost', '') assert 201 == len(fetched)",if path:
"def get_mapping_exception_message(mappings: List[Tuple[Text, Text]]): """"""Return a message given a list of duplicates."""""" message = '' for name, action_name in mappings: <IF_STMT> message += '\n' message += ""Intent '{}' is set to trigger action '{}', which is not defined in the domain."".format(name, action_name) return message",if message:
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: <IF_STMT> yield x,if x:
"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = (expr.real, expr.imag) if -delta < real.get_float_value() < delta: real = Integer(0) <IF_STMT> imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",if -delta < imag.get_float_value() < delta:
"def make_row(self): res = [] for i in range(self.num_cols): t = sqlite3_column_type(self.stmnt, i) if t == SQLITE_INTEGER: res.append(sqlite3_column_int(self.stmnt, i)) elif t == SQLITE_FLOAT: res.append(sqlite3_column_double(self.stmnt, i)) <IF_STMT> res.append(sqlite3_column_text(self.stmnt, i)) else: raise NotImplementedError return tuple(res)",elif t == SQLITE_TEXT:
"def try_convert(self, string): string = string.strip() try: return int(string) except: try: return float(string) except: if string == 'True': return True <IF_STMT> return False return string",if string == 'False':
"def configure_create_table_epilogue(store): for val in ['', ' ENGINE=InnoDB']: store.config['create_table_epilogue'] = val store._set_sql_flavour() <IF_STMT> store.log.info(""create_table_epilogue='%s'"", val) return raise Exception('Can not create a transactional table.')",if store._test_transaction():
"def _check_rule(self, match, target_dict, cred_dict): """"""Recursively checks credentials based on the brains rules."""""" try: new_match_list = self.rules[match] except KeyError: <IF_STMT> new_match_list = ('rule:%s' % self.default_rule,) else: return False return self.check(new_match_list, target_dict, cred_dict)",if self.default_rule and match != self.default_rule:
"def get_civil_names(self): congresspeople_ids = self.get_all_congresspeople_ids() for i, congress_id in enumerate(congresspeople_ids): if not np.math.isnan(float(congress_id)): percentage = i / self.total * 100 msg = 'Processed {} out of {} ({:.2f}%)' print(msg.format(i, self.total, percentage), end='\r') data = self.fetch_data_repository(congress_id) <IF_STMT> yield dict(data)",if data is not None:
"def parse_network_whitelist(self, network_whitelist_location): networks = [] with open(network_whitelist_location, 'r') as text_file: for line in text_file: line = line.strip().strip(""'"").strip('""') <IF_STMT> networks.append(line) return networks",if isIPv4(line) or isIPv6(line):
"def _pick(self, cum): if self._isleaf(): return (self.bd[0], self.s) el<IF_STMT> return self.left._pick(cum) else: return self.right._pick(cum - self.left.s)",if cum < self.left.s:
"def serialize_content_range(value): if isinstance(value, (tuple, list)): if len(value) not in (2, 3): raise ValueError('When setting content_range to a list/tuple, it must be length 2 or 3 (not %r)' % value) <IF_STMT> begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value",if len(value) == 2:
"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): <IF_STMT> fields[k] = v continue if k == 'full_title': fields['title'] = [read_short_title(v)] return fields","if k in ('lccn', 'oclc', 'isbn'):"
"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) if action == 'deletion': del translation[ind] <IF_STMT> ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",elif action == 'replacement':
"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == 'tacotron': text = tacotron_cleaner.cleaners.custom_english_cleaners(text) elif t == 'jaconv': text = jaconv.normalize(text) <IF_STMT> if vietnamese_cleaners is None: raise RuntimeError('Please install underthesea') text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f'Not supported: type={t}') return text",elif t == 'vietnamese':
"def hook_GetVariable(ql, address, params): if params['VariableName'] in ql.env: var = ql.env[params['VariableName']] read_len = read_int64(ql, params['DataSize']) <IF_STMT> write_int64(ql, params['Attributes'], 0) write_int64(ql, params['DataSize'], len(var)) if read_len < len(var): return EFI_BUFFER_TOO_SMALL if params['Data'] != 0: ql.mem.write(params['Data'], var) return EFI_SUCCESS return EFI_NOT_FOUND",if params['Attributes'] != 0:
"def test_setupapp(self, overrideRootMenu): """"""Call setupApp with each possible graphics type."""""" root = self.root flist = FileList(root) for tktype in alltypes: with self.subTest(tktype=tktype): macosx._tk_type = tktype macosx.setupApp(root, flist) <IF_STMT> self.assertTrue(overrideRootMenu.called) overrideRootMenu.reset_mock()","if tktype in ('carbon', 'cocoa'):"
"def names(self, persistent=None): u = set() result = [] for s in [self.__storage(None), self.__storage(self.__category)]: for b in s: if persistent is not None and b.persistent != persistent: continue <IF_STMT> continue if b.name not in u: result.append(b.name) u.add(b.name) return result",if b.name.startswith('__'):
"def _check_extra_specs(key, value=None): extra_specs = diff.get('extra_specs') specific_type = extra_specs.get(key) if extra_specs else None old_type = None new_type = None if specific_type: old_type, new_type = specific_type <IF_STMT> old_type = True if old_type and old_type.upper() == value else False new_type = True if new_type and new_type.upper() == value else False return (old_type, new_type)",if value:
"def _write_lock_file(self, repo, force=True): if force or (self._update and self._write_lock): updated_lock = self._locker.set_lock_data(self._package, repo.packages) <IF_STMT> self._io.write_line('') self._io.write_line('<info>Writing lock file</>')",if updated_lock:
"def process_message(self, msg): if msg['type'] == 'sample': batch_shape = msg['fn'].batch_shape <IF_STMT> batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape) batch_shape[self.dim] = self.size msg['fn'] = msg['fn'].expand(torch.Size(batch_shape))",if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:
def _test_reducibility(self): graph = networkx.DiGraph(self._graph) self._make_supergraph(graph) while True: changed = False changed |= self._remove_self_loop(graph) changed |= self._merge_single_entry_node(graph) <IF_STMT> break,if not changed:
"def __init__(self, roberta, num_classes=2, dropout=0.0, prefix=None, params=None): super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params) self.roberta = roberta self._units = roberta._units with self.name_scope(): self.classifier = nn.HybridSequential(prefix=prefix) <IF_STMT> self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=self._units, activation='tanh')) if dropout: self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=num_classes))",if dropout:
"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip('\\') for a, o in self.objects.items(): if not o.name: continue <IF_STMT> return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] if m: name = m[0] return self.get_object_from_name(name, False)",if o.name.lower() == name.lower():
"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): <IF_STMT> method = getattr(self, name) if method and callable(method): method() finally: warnings.formatwarning = oldformatwarning",if name.startswith('check_'):
"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) if not defaults and value == default_values[k]: continue <IF_STMT> print_func = repr pieces.append('%s=%s' % (k, print_func(value))) if pieces or self.__base__: return '%s(%s)' % (self.__class__.__name__, ', '.join(pieces)) else: return ''","if isinstance(value, basestring):"
"def apply(self, **kwargs: Any) -> None: for node in self.document.traverse(nodes.target): <IF_STMT> continue if 'ismod' in node and node.parent.__class__ is nodes.section and (node.parent.index(node) == 1): node.parent['ids'][0:0] = node['ids'] node.parent.remove(node)",if not node['ids']:
"def add_special_token_2d(values: List[List[int]], special_token: int=0, use_first_value: bool=False) -> List[List[int]]: results = torch.jit.annotate(List[List[int]], []) for value in values: result = torch.jit.annotate(List[int], []) <IF_STMT> special_token = value[0] result.append(special_token) result.extend(value) result.append(special_token) results.append(result) return results",if use_first_value and len(value) > 0:
"def test_import(self): TIMEOUT = 5 command = [sys.executable, '-c', 'import tornado.test.resolve_test_helper'] start = time.time() popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT)) while time.time() - start < TIMEOUT: return_code = popen.poll() <IF_STMT> self.assertEqual(0, return_code) return time.sleep(0.05) self.fail('import timed out')",if return_code is not None:
"def find_item_for_key(self, e): for item in self._items: if item.keycode == e.key and item.shift == e.shift and (item.alt == e.alt): focus = get_focus() <IF_STMT> return self._items.index(item) else: return -1 return -1","if self.command_is_enabled(item, focus):"
"def check_app_config_brackets(self): for sn, app in cherrypy.tree.apps.items(): if not isinstance(app, cherrypy.Application): continue <IF_STMT> continue for key in app.config.keys(): if key.startswith('[') or key.endswith(']'): warnings.warn('The application mounted at %r has config section names with extraneous brackets: %r. Config *files* need brackets; config *dicts* (e.g. passed to tree.mount) do not.' % (sn, key))",if not app.config:
"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: for m in getattr(a, 'modules', []): m = m.strip() for mod in self.modules: <IF_STMT> if getattr(mod, 'module_name', '').strip() == m: return True return False","if getattr(mod, 'module_type', '').strip() == mod_type.strip():"
"def write_config_to_file(self, folder, filename, config): do_not_write = ['hyperparameter_search_space_updates'] with open(os.path.join(folder, filename), 'w') as f: f.write('\n'.join([key + '=' + str(value) for key, value in sorted(config.items(), key=lambda x: x[0]) <IF_STMT>]))",if not key in do_not_write
"def parsing(self, parsing): self._parsed = parsing for k, v in self._body: <IF_STMT> v.value.parsing(parsing) elif isinstance(v, AoT): for t in v.body: t.value.parsing(parsing)","if isinstance(v, Table):"
"def test_crashers_crash(self): for fname in glob.glob(CRASHER_FILES): <IF_STMT> continue if test.support.verbose: print('Checking crasher:', fname) assert_python_failure(fname)",if os.path.basename(fname) in infinite_loops:
"def __getitem__(self, k) -> 'SimMemView': if isinstance(k, slice): if k.step is not None: raise ValueError('Slices with strides are not supported') elif k.start is None: raise ValueError('Must specify start index') <IF_STMT> raise ValueError('Slices with stop index are not supported') else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",elif k.stop is not None:
def get_lowest_wall_time(jsons): lowest_wall = None for j in jsons: <IF_STMT> lowest_wall = j['wall_time'] if lowest_wall > j['wall_time']: lowest_wall = j['wall_time'] return lowest_wall,if lowest_wall is None:
"def extract_wav_headers(data): pos = 12 subchunks = [] while pos + 8 <= len(data) and len(subchunks) < 10: subchunk_id = data[pos:pos + 4] subchunk_size = struct.unpack_from('<I', data[pos + 4:pos + 8])[0] subchunks.append(WavSubChunk(subchunk_id, pos, subchunk_size)) <IF_STMT> break pos += subchunk_size + 8 return subchunks",if subchunk_id == b'data':
"def _any_targets_have_native_sources(self, targets): for tgt in targets: for type_constraint, target_predicate in self._native_target_matchers.items(): <IF_STMT> return True return False",if type_constraint.satisfied_by(tgt) and target_predicate(tgt):
"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: continue <IF_STMT> raise serializers.ValidationError('Process types can only contain [a-z]') if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value","if not re.match(PROCTYPE_MATCH, k):"
"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord('0') if even: digit *= 2 <IF_STMT> digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",if digit >= 10:
"def transform(a, cmds): buf = a.split('\n') for cmd in cmds: ctype, line, col, char = cmd if ctype == 'D': <IF_STMT> buf[line] = buf[line][:col] + buf[line][col + len(char):] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == 'I': buf[line] = buf[line][:col] + char + buf[line][col:] buf = '\n'.join(buf).split('\n') return '\n'.join(buf)",if char != '\n':
"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} for edge in self.edges: if edge.is_dangling(): raise ValueError('Cannot contract copy tensor with dangling edges') if self._is_my_trace(edge): continue partner_node, shared_axis = self._get_partner(edge) <IF_STMT> partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",if partner_node not in partners:
"def _bind_interactive_rez(self): if config.set_prompt and self.settings.prompt: stored_prompt = os.getenv('REZ_STORED_PROMPT_CMD') curr_prompt = stored_prompt or os.getenv('PROMPT', '') <IF_STMT> self.setenv('REZ_STORED_PROMPT_CMD', curr_prompt) new_prompt = '%%REZ_ENV_PROMPT%%' new_prompt = new_prompt + ' %s' if config.prefix_prompt else '%s ' + new_prompt new_prompt = new_prompt % curr_prompt self._addline('set PROMPT=%s' % new_prompt)",if not stored_prompt:
"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) if not definition: IECore.msg(IECore.Msg.Level.Error, 'GafferImageUI.CatalogueUI', ""No column registered with name '%s'"" % name) continue <IF_STMT> c = GafferUI.PathListingWidget.IconColumn(definition.title(), '', name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns","if isinstance(definition, IconColumn):"
"def _check_invalid_keys(self, section_name, section): for key in section: key_name = str(key) valid_key_names = [s[0] for s in self.keys] is_valid_key = key_name in valid_key_names <IF_STMT> err_msg = ""'{0}' is not a valid key name for '{1}'. Must be one of these: {2}"".format(key_name, section_name, ', '.join(valid_key_names)) raise InvalidConfig(err_msg)",if not is_valid_key:
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == '__pycache__': continue if name.endswith('.py'): names.add(name.split('.')[0]) <IF_STMT> names.add(name) if packages: packages = {package.lower().replace('-', '_') for package in packages} if len(names & packages) == len(packages): return packages return names",elif path.is_dir() and '.' not in name:
"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): <IF_STMT> keynames[i] = k[1:] negate.add(k[1:])  def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): if k in negate: composite[i] = -v return composite return getit",if k[:1] == '-':
"def iter_symbols(code): """"""Yield names and strings used by `code` and its nested code objects"""""" for name in code.co_names: yield name for const in code.co_consts: if isinstance(const, six.string_types): yield const <IF_STMT> for name in iter_symbols(const): yield name","elif isinstance(const, CodeType):"
"def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None: with self._lock: <IF_STMT> current_directions = self._studies[study_id].directions if directions == current_directions: return elif len(current_directions) == 1 and current_directions[0] == StudyDirection.NOT_SET: self._studies[study_id].directions = list(directions) self._backend.set_study_directions(study_id, directions) return self._backend.set_study_directions(study_id, directions)",if study_id in self._studies:
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: <IF_STMT> x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",if self.__Token:
"def _check_blocking(self, current_time): if self._switch_flag is False: active_greenlet = self._active_greenlet <IF_STMT> self._notify_greenlet_blocked(active_greenlet, current_time) self._switch_flag = False",if active_greenlet is not None and active_greenlet != self._hub:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('BlockDos\\.net', headers.get(HTTP_HEADER.SERVER, ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: <IF_STMT> in_section = True elif in_section: if line.startswith('>>END'): break out.append(line.rstrip('\r\n')) return out",if line.startswith('>>%s' % section_name):
"def shortcut(self, input, ch_out, stride, is_first, name): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <IF_STMT> return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) elif is_first: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return input",if is_first or stride == 1:
"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: if string_value is not None: string_value = str(string_value).strip() <IF_STMT> param_value = int(string_value) except ValueError: self.pcluster_config.warn(""Unable to convert the value '{0}' to an Integer. Using default value for parameter '{1}'"".format(string_value, self.key)) return param_value",if string_value != 'NONE':
"def get_running(workers): running = [] for worker in workers: current_test_name = worker.current_test_name <IF_STMT> continue dt = time.monotonic() - worker.start_time if dt >= PROGRESS_MIN_TIME: text = '%s (%s)' % (current_test_name, format_duration(dt)) running.append(text) return running",if not current_test_name:
"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: if stats['uptime'] > 86400: uptime['value'] = stats['uptime'] / 60 / 60 / 24 uptime['unit'] = _('days') <IF_STMT> uptime['value'] = stats['uptime'] / 60 / 60 uptime['unit'] = _('hours') else: uptime['value'] = stats['uptime'] / 60 uptime['unit'] = _('minutes') return {'cache_stats': cache_stats, 'uptime': uptime}",elif stats['uptime'] > 3600:
"def add_actors(self): """"""Adds `self.actors` to the scene."""""" if not self._actors_added: self.reader.render_window = self.scene.render_window self._update_reader() self._actors_added = True <IF_STMT> self._visible_changed(self.visible) self.scene.render()",if not self.visible:
"def _add_uniqu_suffix(self, titles): counters = dict() titles_with_suffix = [] for title in titles: counters[title] = counters[title] + 1 if title in counters else 1 <IF_STMT> title = f'{title} ({counters[title]})' titles_with_suffix.append(title) return titles_with_suffix",if counters[title] > 1:
"def _verify_udf_resources(self, job, config): udf_resources = config.get('userDefinedFunctionResources', ()) self.assertEqual(len(job.udf_resources), len(udf_resources)) for found, expected in zip(job.udf_resources, udf_resources): <IF_STMT> self.assertEqual(found.udf_type, 'resourceUri') self.assertEqual(found.value, expected['resourceUri']) else: self.assertEqual(found.udf_type, 'inlineCode') self.assertEqual(found.value, expected['inlineCode'])",if 'resourceUri' in expected:
"def __init__(self, layout, value=None, string=None, *, dtype: np.dtype=np.float64) -> None: """"""Constructor."""""" self.layout = layout if value is None: <IF_STMT> self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) if self.value.shape != (self.layout.gaDims,): raise ValueError('value must be a sequence of length %s' % self.layout.gaDims)",if string is None:
"def read_file(filename, print_error=True): """"""Returns the contents of a file."""""" try: for encoding in ['utf-8', 'latin1']: try: with io.open(filename, encoding=encoding) as fp: return fp.read() except UnicodeDecodeError: pass except IOError as exception: <IF_STMT> print(exception, file=sys.stderr) return None",if print_error:
"def get_albums_for_iter(self, iter_): obj = self.get_value(iter_) if isinstance(obj, AlbumNode): return {obj.album} albums = set() for child_iter, value in self.iterrows(iter_): <IF_STMT> albums.add(value.album) else: albums.update(self.get_albums_for_iter(child_iter)) return albums","if isinstance(value, AlbumNode):"
"def wait_til_ready(cls, connector=None): if connector is None: connector = cls.connector while True: now = time.time() next_iteration = now // 1.0 + 1 <IF_STMT> break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0)",if connector.ready:
"def remove_property(self, key): with self.secure() as config: keys = key.split('.') current_config = config for i, key in enumerate(keys): if key not in current_config: return <IF_STMT> del current_config[key] break current_config = current_config[key]",if i == len(keys) - 1:
"def get(self, hash160, default=None): v = self.p2s_for_hash(hash160) <IF_STMT> return v if hash160 not in self._secret_exponent_cache: v = self.path_for_hash160(hash160) if v: fingerprint, path = v for key in self._secrets.get(fingerprint, []): subkey = key.subkey_for_path(path) self._add_key_to_cache(subkey) return self._secret_exponent_cache.get(hash160, default)",if v:
"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: <IF_STMT> targets = tuple(targets) elif type(targets) != tuple: targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",if type(targets) != list and type(targets) != tuple:
"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: <IF_STMT> if isinstance(dt, dgl.DGLGraph): return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)",if dt is not None:
"def capture_server(evt, buf, serv): try: serv.listen(5) conn, addr = serv.accept() except socket.timeout: pass else: n = 200 while n > 0: r, w, e = select.select([conn], [], []) <IF_STMT> data = conn.recv(10) buf.write(data.replace('\n', '')) if '\n' in data: break n -= 1 time.sleep(0.01) conn.close() finally: serv.close() evt.set()",if r:
"def elem(): if ints_only: return random.randint(0, 10000000000) else: t = random.randint(0, 2) if t == 0: return random.randint(0, 10000000000) elif t == 1: return float(random.randint(0, 10000000000)) <IF_STMT> return strings[random.randint(0, len(strings) - 1)] return random_string(random.randint(100, 1000))",elif strings is not None:
"def has_changed(self, initial, data): if self.disabled: return False if initial is None: initial = ['' for x in range(0, len(data))] el<IF_STMT> initial = self.widget.decompress(initial) for field, initial, data in zip(self.fields, initial, data): try: initial = field.to_python(initial) except ValidationError: return True if field.has_changed(initial, data): return True return False","if not isinstance(initial, list):"
"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) <IF_STMT> if hasattr(package.__loader__, 'get_data'): file_contents = package.__loader__.get_data(filename) return (file_contents.replace(os.linesep, '\n'), filename) return (open(filename).read(), filename)","if hasattr(package, '__loader__'):"
def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None <IF_STMT> self.waiters -= 1 self.wakeup.release(),if self.waiters:
"def stage(self, x, num_modules, num_blocks, channels, multi_scale_output=True, name=None): out = x for i in range(num_modules): <IF_STMT> out = self.high_resolution_module(out, num_blocks, channels, multi_scale_output=False, name=name + '_' + str(i + 1)) else: out = self.high_resolution_module(out, num_blocks, channels, name=name + '_' + str(i + 1)) return out",if i == num_modules - 1 and multi_scale_output == False:
"def changeFrontAlteration(intV, alter): fati = self.frontAlterationTransposeInterval if fati: newFati = interval.add([fati, intV]) self.frontAlterationTransposeInterval = newFati self.frontAlterationAccidental.alter = self.frontAlterationAccidental.alter + alter <IF_STMT> self.frontAlterationTransposeInterval = None self.frontAlterationAccidental = None else: self.frontAlterationTransposeInterval = intV self.frontAlterationAccidental = pitch.Accidental(alter)",if self.frontAlterationAccidental.alter == 0:
"def set_to_train(self): for T in self.trainable_attributes(): for k, v in T.items(): <IF_STMT> c_f.set_requires_grad(v, requires_grad=False) v.eval() else: v.train() self.maybe_freeze_trunk_batchnorm()",if k in self.freeze_these:
"def _migrate(self, sig=None, compact=True): with self.lock: sig = sig or self.sig <IF_STMT> return if sig in self.WORDS and len(self.WORDS[sig]) > 0: PostingList.Append(self.session, sig, self.WORDS[sig], sig=sig, compact=compact) del self.WORDS[sig]",if sig in GPL_NEVER_MIGRATE:
"def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs): if self.prediction_bar is None: <IF_STMT> self.prediction_bar = self.training_tracker.add_child(len(eval_dataloader)) else: self.prediction_bar = NotebookProgressBar(len(eval_dataloader)) self.prediction_bar.update(1) else: self.prediction_bar.update(self.prediction_bar.value + 1)",if self.training_tracker is not None:
"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print('struct {}'.format(self.name)) for field in self.fields: if field.offset is None: offset = '0x??' else: offset = '0x{:02x}'.format(field.offset) print('{}+{} {} {}'.format(' ' * indent, offset, field.name, field.type)) <IF_STMT> field.type.show(indent + 1)","if isinstance(field.type, Structure):"
"def __exit__(self, exc, value, tb): for key in self.overrides.keys(): old_value = self.old[key] <IF_STMT> delattr(self.instance, key) else: setattr(self.instance, key, old_value) self.instance.save()",if old_value is NULL:
"def complete(self, block): with self._condition: <IF_STMT> return False if self._complete(): self._calculate_state_root_if_not_already_done() return True if block: self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",if not self._final:
def parseArguments(self): args = [] self.expect('(') if not self.match(')'): while self.startIndex < self.length: args.append(self.isolateCoverGrammar(self.parseAssignmentExpression)) <IF_STMT> break self.expectCommaSeparator() self.expect(')') return args,if self.match(')'):
"def isValidDateString(config_param_name, value, valid_value): try: if value == 'DD-MM-YYYY': return value day, month, year = value.split('-') <IF_STMT> raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if int(day) < 1 or int(day) > 31:
"def build_tree(path): tree = Tree() for basename, entry in trees[path].items(): <IF_STMT> mode = stat.S_IFDIR sha = build_tree(pathjoin(path, basename)) else: mode, sha = entry tree.add(basename, mode, sha) object_store.add_object(tree) return tree.id","if isinstance(entry, dict):"
"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {'objects': 0, 'containers': 0, 'accounts': 0} qdir = 'quarantined' for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) <IF_STMT> linkcount = os.lstat(qtgt).st_nlink if linkcount > 2: qcounts[qtype] += linkcount - 2 return qcounts",if os.path.exists(qtgt):
"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: <IF_STMT> return False if dim_value < 0: raise Exception('Negative dimension is illegal: %d' % dim_value) return True","if not isinstance(dim_value, int):"
"def BraceDetectAll(words): """"""Return a new list of words, possibly with BracedTree instances."""""" out = [] for w in words: if len(w.parts) >= 3: brace_tree = _BraceDetect(w) <IF_STMT> out.append(brace_tree) continue out.append(w) return out",if brace_tree:
"def __init__(original, self, *args, **kwargs): data = args[0] if len(args) > 0 else kwargs.get('data') if data is not None: try: <IF_STMT> raise Exception('cannot gather example input when dataset is loaded from a file.') input_example_info = _InputExampleInfo(input_example=deepcopy(data[:INPUT_EXAMPLE_SAMPLE_ROWS])) except Exception as e: input_example_info = _InputExampleInfo(error_msg=str(e)) setattr(self, 'input_example_info', input_example_info) original(self, *args, **kwargs)","if isinstance(data, str):"
"def setRow(self, row, vals): if row > self.rowCount() - 1: self.setRowCount(row + 1) for col in range(len(vals)): val = vals[col] item = self.itemClass(val, row) item.setEditable(self.editable) sortMode = self.sortModes.get(col, None) <IF_STMT> item.setSortMode(sortMode) format = self._formats.get(col, self._formats[None]) item.setFormat(format) self.items.append(item) self.setItem(row, col, item) item.setValue(val)",if sortMode is not None:
"def wakeUp(self): """"""Write one byte to the pipe, and flush it."""""" if self.o is not None: try: util.untilConcludes(os.write, self.o, b'x') except OSError as e: <IF_STMT> raise",if e.errno != errno.EAGAIN:
"def _setup(self, field_name, owner_model): resolved_classes = [] for m in self.model_classes: <IF_STMT> if m == owner_model.__name__: resolved_classes.append(owner_model) else: raise Exception(""PolyModelType: Unable to resolve model '{}'."".format(m)) else: resolved_classes.append(m) self.model_classes = tuple(resolved_classes) super(PolyModelType, self)._setup(field_name, owner_model)","if isinstance(m, string_type):"
"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: value_ = self._late_binding_returnvalues.get(key, KeyError) if value_ is KeyError: value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) <IF_STMT> value_ = schema.validate(value_) self._late_binding_returnvalues[key] = value_ return value_ else: return value",if schema is not None:
"def convert(self, ctx, argument): arg = argument.replace('0x', '').lower() if arg[0] == '#': arg = arg[1:] try: value = int(arg, base=16) <IF_STMT> raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace(' ', '_') method = getattr(discord.Colour, arg, None) if arg.startswith('from_') or method is None or (not inspect.ismethod(method)): raise BadColourArgument(arg) return method()",if not 0 <= value <= 16777215:
"def get_versions(*, all=False, quiet=None): import bonobo from bonobo.util.pkgs import bonobo_packages yield _format_version(bonobo, quiet=quiet) if all: for name in sorted(bonobo_packages): <IF_STMT> try: mod = __import__(name.replace('-', '_')) try: yield _format_version(mod, name=name, quiet=quiet) except Exception as exc: yield '{} ({})'.format(name, exc) except ImportError as exc: yield '{} is not importable ({}).'.format(name, exc)",if name != 'bonobo':
"def assertOperationsInjected(self, plan, **kwargs): for migration, _backward in plan: operations = iter(migration.operations) for operation in operations: <IF_STMT> next_operation = next(operations) self.assertIsInstance(next_operation, contenttypes_management.RenameContentType) self.assertEqual(next_operation.app_label, migration.app_label) self.assertEqual(next_operation.old_model, operation.old_name_lower) self.assertEqual(next_operation.new_model, operation.new_name_lower)","if isinstance(operation, migrations.RenameModel):"
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) if match: continue <IF_STMT> if ',' in line or ';' in line: continue yield line",if strip_delimiters:
"def read_lccn(line, is_marc8=False): found = [] for k, v in get_raw_subfields(line, ['a']): lccn = v.strip() if re_question.match(lccn): continue m = re_lccn.search(lccn) if not m: continue lccn = re_letters_and_bad.sub('', m.group(1)).strip() <IF_STMT> found.append(lccn) return found",if lccn:
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == 'likelihood.noise_covar.raw_noise': self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == 'mean_module.constant': self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) <IF_STMT> self.assertIsInstance(constraint, gpytorch.constraints.Positive)",elif name == 'covar_module.base_kernel.raw_lengthscale':
"def _cleanupSocket(self): """"""Close the Connection's socket."""""" try: self._sock.shutdown(socket.SHUT_WR) except: return try: while True: r, w, e = select.select([self._sock], [], []) <IF_STMT> break except: pass self._sock.close()",if not r or not self._sock.recv(1024):
"def fadeIn(self, acts=None, t=None, duration=None): """"""Gradually switch on the input list of meshes by increasing opacity."""""" if self.bookingMode: acts, t, duration, rng = self._parse(acts, t, duration) for tt in rng: alpha = linInterpolate(tt, [t, t + duration], [0, 1]) self.events.append((tt, self.fadeIn, acts, alpha)) else: for a in self._performers: <IF_STMT> continue a.alpha(self._inputvalues) return self",if a.alpha() >= self._inputvalues:
"def get_config_updates_recursive(self): config_updates = self.config_updates.copy() for sr_path, subrunner in self.subrunners.items(): <IF_STMT> continue update = subrunner.get_config_updates_recursive() if update: config_updates[rel_path(self.path, sr_path)] = update return config_updates","if not is_prefix(self.path, sr_path):"
"def setArgs(self, **kwargs): """"""See GridSearchCostGamma"""""" for key, value in list(kwargs.items()): if key in ('folds', 'nfolds'): self._n_folds = int(value) <IF_STMT> self._validator_kwargs['max_epochs'] = value else: GridSearchDOE.setArgs(self, **{key: value})",elif key in 'max_epochs':
def _parse_composite_axis(composite_axis_name: str): axes_names = [axis for axis in composite_axis_name.split(' ') if len(axis) > 0] for axis in axes_names: <IF_STMT> continue assert 'a' <= axis[0] <= 'z' for letter in axis: assert str.isdigit(letter) or 'a' <= letter <= 'z' return axes_names,if axis == '_':
"def visit_For(self, node, for_branch='body', **kwargs): if for_branch == 'body': self.sym_visitor.visit(node.target, store_as_param=True) branch = node.body elif for_branch == 'else': branch = node.else_ elif for_branch == 'test': self.sym_visitor.visit(node.target, store_as_param=True) <IF_STMT> self.sym_visitor.visit(node.test) return else: raise RuntimeError('Unknown for branch') for item in branch or (): self.sym_visitor.visit(item)",if node.test is not None:
def contains_only_whitespace(node): if is_tag(node): <IF_STMT> if not any([unicode(s).strip() for s in node.contents]): return True return False,if not any([not is_text(s) for s in node.contents]):
"def dir_tag_click(event): mouse_index = self.path_bar.index('@%d,%d' % (event.x, event.y)) lineno = int(float(mouse_index)) if lineno == 1: self.request_focus_into('') else: assert lineno == 2 dir_range = get_dir_range(event) if dir_range: _, end_index = dir_range path = self.path_bar.get('2.0', end_index) <IF_STMT> path += '\\' self.request_focus_into(path)",if path.endswith(':'):
"def validate_employee_id(self): if self.employee: sales_person = frappe.db.get_value('Sales Person', {'employee': self.employee}) <IF_STMT> frappe.throw(_('Another Sales Person {0} exists with the same Employee id').format(sales_person))",if sales_person and sales_person != self.name:
def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/infer'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:
"def poll(self, timeout): if timeout < 0: timeout = None events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout) results = defaultdict(lambda: POLL_NULL) for e in events: fd = e.ident if e.filter == select.KQ_FILTER_READ: results[fd] |= POLL_IN <IF_STMT> results[fd] |= POLL_OUT return results.items()",elif e.filter == select.KQ_FILTER_WRITE:
"def _read_dimensions(self, *dimnames, **kwargs): path = kwargs.get('path', '/') try: <IF_STMT> return [self.rootgrp.dimensions[dname] for dname in dimnames] group = self.path2group[path] return [group.dimensions[dname] for dname in dimnames] except KeyError: raise self.Error('In file %s:\nError while reading dimensions: `%s` with kwargs: `%s`' % (self.path, dimnames, kwargs))",if path == '/':
def spam_to_me(address): sock = eventlet.connect(address) while True: try: sock.sendall(b'hello world') time.sleep(0.001) except socket.error as e: <IF_STMT> return raise,if get_errno(e) == errno.EPIPE:
"def has_hash_of(self, destpath, code, package_level): """"""Determine if a file has the hash of the code."""""" if destpath is not None and os.path.isfile(destpath): with univ_open(destpath, 'r') as opened: compiled = readfile(opened) hashash = gethash(compiled) <IF_STMT> return True return False","if hashash is not None and hashash == self.comp.genhash(code, package_level):"
"def insert(self, index, item): if len(self.lists) == 1: self.lists[0].insert(index, item) self._balance_list(0) else: list_idx, rel_idx = self._translate_index(index) <IF_STMT> raise IndexError() self.lists[list_idx].insert(rel_idx, item) self._balance_list(list_idx) return",if list_idx is None:
"def _parse_class_simplified(symbol): results = {} name = symbol.name + '(' name += ', '.join([analyzer.expand_attribute(base) for base in symbol.bases]) name += ')' for sym in symbol.body: if isinstance(sym, ast.FunctionDef): result = _parse_function_simplified(sym, symbol.name) results.update(result) <IF_STMT> result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, 'c') return results","elif isinstance(sym, ast.ClassDef):"
"def append_vars(pairs, result): for name, value in sorted(pairs.items()): if isinstance(value, list): value = '[%s]' % ','.join(value) <IF_STMT> result.append('%s:%s=%s' % (package, name, value)) else: result.append('%s=%s' % (name, value))",if package:
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: <IF_STMT> self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if len(self._editableChildren):
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False <IF_STMT> return False return True","elif isinstance(v, _bytes):"
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False <IF_STMT> return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True","elif isinstance(s, np.ndarray) and s.size == 0:"
"def scan_resource_conf(self, conf): if 'properties' in conf: if 'attributes' in conf['properties']: if 'exp' in conf['properties']['attributes']: <IF_STMT> return CheckResult.PASSED return CheckResult.FAILED",if conf['properties']['attributes']['exp']:
"def encode(self): if self.expr in gpregs.expr: self.value = gpregs.expr.index(self.expr) self.parent.rot2.value = 0 elif isinstance(self.expr, ExprOp) and self.expr.op == allshifts[3]: reg, value = self.expr.args <IF_STMT> return False self.value = gpregs.expr.index(reg) if not isinstance(value, ExprInt): return False value = int(value) if not value in [8, 16, 24]: return False self.parent.rot2.value = value // 8 return True",if reg not in gpregs.expr:
"def validate_transaction_reference(self): bank_account = self.paid_to if self.payment_type == 'Receive' else self.paid_from bank_account_type = frappe.db.get_value('Account', bank_account, 'account_type') if bank_account_type == 'Bank': <IF_STMT> frappe.throw(_('Reference No and Reference Date is mandatory for Bank transaction'))",if not self.reference_no or not self.reference_date:
"def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: if hasattr(monad, 'cls_bl_idname'): <IF_STMT> return monad return None",if monad.cls_bl_idname == self.cls_bl_idname:
"def _create_mask(self, plen): mask = [] for i in range(16): if plen >= 8: mask.append(255) <IF_STMT> mask.append(255 >> 8 - plen << 8 - plen) else: mask.append(0) plen -= 8 return mask",elif plen > 0:
"def dataset_to_stream(dataset, input_name): """"""Takes a tf.Dataset and creates a numpy stream of ready batches."""""" for example in fastmath.dataset_as_numpy(dataset): features = example[0] inp, out = (features[input_name], example[1]) mask = features['mask'] if 'mask' in features else None <IF_STMT> inp = inp.astype(np.int32) if isinstance(out, np.uint8): out = out.astype(np.int32) yield ((inp, out) if mask is None else (inp, out, mask))","if isinstance(inp, np.uint8):"
"def _idle_redraw_cb(self): assert self._idle_redraw_src_id is not None queue = self._idle_redraw_queue if len(queue) > 0: bbox = queue.pop(0) <IF_STMT> super(CanvasRenderer, self).queue_draw() else: super(CanvasRenderer, self).queue_draw_area(*bbox) if len(queue) == 0: self._idle_redraw_src_id = None return False return True",if bbox is None:
"def mutated(self, indiv): """"""mutate some genes of the given individual"""""" res = indiv.copy() for i in range(self.numParameters): <IF_STMT> if self.xBound is None: res[i] = indiv[i] + gauss(0, self.mutationStdDev) else: res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev), self.maxs[i]), self.mins[i]) return res",if random() < self.mutationProb:
"def _justifyDrawParaLine(tx, offset, extraspace, words, last=0): setXPos(tx, offset) text = b' '.join(words) if last: tx._textOut(text, 1) else: nSpaces = len(words) - 1 <IF_STMT> tx.setWordSpace(extraspace / float(nSpaces)) tx._textOut(text, 1) tx.setWordSpace(0) else: tx._textOut(text, 1) setXPos(tx, -offset) return offset",if nSpaces:
"def _read_0(self, stream): r = b'' while True: c = stream.read(2) <IF_STMT> raise EOFError() if c == b'\x00\x00': break r += c return r.decode(self.encoding)",if len(c) != 2:
"def run(self, app, editor, args): line_nums = [] for cursor in editor.cursors: <IF_STMT> line_nums.append(cursor.y) data = editor.lines[cursor.y].get_data().upper() editor.lines[cursor.y].set_data(data)",if cursor.y not in line_nums:
"def create_default_energy_point_rules(): for rule in get_default_energy_point_rules(): rule_exists = frappe.db.exists('Energy Point Rule', {'reference_doctype': rule.get('reference_doctype')}) <IF_STMT> continue doc = frappe.get_doc(rule) doc.insert(ignore_permissions=True)",if rule_exists:
"def __new__(cls, *nodes): if not nodes: raise TypeError('DisjunctionNode() requires at least one node') elif len(nodes) == 1: return nodes[0] self = super(DisjunctionNode, cls).__new__(cls) self.__nodes = [] for node in nodes: if not isinstance(node, Node): raise TypeError('DisjunctionNode() expects Node instances as arguments; received a non-Node instance %r' % node) <IF_STMT> self.__nodes.extend(node.__nodes) else: self.__nodes.append(node) return self","if isinstance(node, DisjunctionNode):"
def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: if w not in index: yield from dfs(w) <IF_STMT> while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v]:]) del stack[index[v]:] identified.update(scc) yield scc,elif w not in identified:
"def unpack_item_obj(map_uuid_global_id, misp_obj): obj_meta = get_object_metadata(misp_obj) obj_id = None io_content = None for attribute in misp_obj.attributes: <IF_STMT> obj_id = attribute.value io_content = attribute.data if obj_id and io_content: res = Item.create_item(obj_id, obj_meta, io_content) map_uuid_global_id[misp_obj.uuid] = get_global_id('item', obj_id)",if attribute.object_relation == 'raw-data':
"def parse(self, response): soup = BeautifulSoup(response.content.decode('utf-8', 'ignore'), 'lxml') image_divs = soup.find_all('div', class_='imgpt') pattern = re.compile('murl\\"":\\""(.*?)\\.jpg') for div in image_divs: href_str = html_parser.HTMLParser().unescape(div.a['m']) match = pattern.search(href_str) <IF_STMT> name = match.group(1) if six.PY3 else match.group(1).encode('utf-8') img_url = '{}.jpg'.format(name) yield dict(file_url=img_url)",if match:
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) <IF_STMT> _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",if fn is not None:
"def decompileFormat1(self, reader, otFont): self.classDefs = classDefs = [] startGlyphID = reader.readUShort() glyphCount = reader.readUShort() for i in range(glyphCount): glyphName = otFont.getglyphName(startGlyphID + i) classValue = reader.readUShort() <IF_STMT> classDefs.append((glyphName, classValue))",if classValue:
"def compress(self, data_list): if len(data_list) == 2: value, lookup_expr = data_list <IF_STMT> if lookup_expr not in EMPTY_VALUES: return Lookup(value=value, lookup_expr=lookup_expr) else: raise forms.ValidationError(self.error_messages['lookup_required'], code='lookup_required') return None",if value not in EMPTY_VALUES:
"def open_compat(path, mode='r'): if mode in ['r', 'rb'] and (not os.path.exists(path)): raise FileNotFoundError(u'The file ""%s"" could not be found' % path) if sys.version_info >= (3,): encoding = 'utf-8' errors = 'replace' <IF_STMT> encoding = None errors = None return open(path, mode, encoding=encoding, errors=errors) else: return open(path, mode)","if mode in ['rb', 'wb', 'ab']:"
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) <IF_STMT> continue real_errors.append(line) return real_errors",if _path[-1] != current_path[-1]:
"def filter_by_level(record, level_per_module): name = record['name'] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = '' if '' in level_per_module: level = level_per_module[''] for n in name.split('.'): lookup += n <IF_STMT> level = level_per_module[lookup] lookup += '.' if level is False: return False return record['level'].no >= level",if lookup in level_per_module:
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 <IF_STMT> n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",if self.HasMaximizeButton():
"def search(a, b, desired): if a == b: return a if abs(b - a) < 0.005: ca = count(a) cb = count(b) dista = abs(desired - ca) distb = abs(desired - cb) <IF_STMT> return a else: return b m = (a + b) / 2.0 cm = count(m) if desired < cm: return search(m, b, desired) else: return search(a, m, desired)",if dista < distb:
"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): <IF_STMT> newline = re.sub('\\slocalhost\\s', ' ', line) if line != newline: logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), 'w') for line in lines: f.write(line) f.close()",if '::1' in line:
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' <IF_STMT> continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield (cog_name, cog_id)",if not fpath.exists():
def _get_dbutils(): try: import IPython ip_shell = IPython.get_ipython() <IF_STMT> raise _NoDbutilsError return ip_shell.ns_table['user_global']['dbutils'] except ImportError: raise _NoDbutilsError except KeyError: raise _NoDbutilsError,if ip_shell is None:
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: ext = os.path.splitext(os.path.normcase(py_file))[1] <IF_STMT> continue if self.compile: bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if ext != PYTHON_SOURCE_EXTENSION:
"def compute_distances_mu(line, pts, result, gates, tolerance): """"""calculate all distances with mathuutils"""""" line_origin = V(line[0]) line_end = V(line[-1]) local_result = [[], [], [], [], []] for point in pts: data = compute_distance(V(point), line_origin, line_end, tolerance) for i, res in enumerate(local_result): res.append(data[i]) for i, res in enumerate(result): <IF_STMT> res.append(local_result[i])",if gates[i]:
"def _get_next_segment(self, segment_path, page_size, segment_cursor=None): if segment_path: <IF_STMT> return None return Segment(self.client, segment_path, page_size, segment_cursor) return None",if self.end_time and self._is_later_than_end_time(segment_path):
def _check_number_of_sessions(): nb_desktop_sessions = sessions.get_number_of_desktop_sessions(ignore_gdm=True) if nb_desktop_sessions > 1: print('WARNING : There are %d other desktop sessions open. The GPU switch will not become effective until you have manually logged out from ALL desktop sessions.\nContinue ? (y/N)' % (nb_desktop_sessions - 1)) confirmation = ask_confirmation() <IF_STMT> sys.exit(0),if not confirmation:
"def delete_compute_environment(self, compute_environment_name): if compute_environment_name is None: raise InvalidParameterValueException('Missing computeEnvironment parameter') compute_env = self.get_compute_environment(compute_environment_name) if compute_env is not None: self._compute_environments.pop(compute_env.arn) self.ecs_backend.delete_cluster(compute_env.ecs_name) <IF_STMT> instance_ids = [instance.id for instance in compute_env.instances] self.ec2_backend.terminate_instances(instance_ids)",if compute_env.env_type == 'MANAGED':
"def run(self): results = {} for func_name in [fn for fn in self.check_functions if not self.args.get('check') or self.args.get('check') == fn]: function = getattr(self, func_name) log.warn(function.__doc__) result = function() <IF_STMT> log.info('\n'.join(result)) results.update({func_name: result}) return results",if result:
"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [br for br in self.blockRenderers <IF_STMT>] if len(blockRenderers) < len(self.blockRenderers): self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",if br.layer is Layer.Blocks or br.layer not in layers
"def get_library_dirs(platform, arch=None): if platform == 'win32': jre_home = get_jre_home(platform) jdk_home = JAVA_HOME <IF_STMT> jre_home = jre_home.decode('utf-8') return [join(jdk_home, 'lib'), join(jdk_home, 'bin', 'server')] elif platform == 'android': return ['libs/{}'.format(arch)] return []","if isinstance(jre_home, bytes):"
"def save_plugin_options(self): for name, option_widgets in self._plugin_option_widgets.items(): <IF_STMT> self.config['plugins'][name] = {} plugin_config = self.config['plugins'][name] for option_name, option_widget in option_widgets.items(): plugin_config[option_name] = option_widget.option.get_widget_value(option_widget.widget)",if name not in self.config['plugins']:
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 elif str_in[pos] == end_tag: depth -= 1 <IF_STMT> break sel = str_in[start_pos + 1:pos] return sel",if depth == 0:
"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val elif not compare.compatible_with(v, True): const = not true_val <IF_STMT> const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var","elif not compare.compatible_with(v, False):"
def multiline_indentation(self): if self._multiline_indentation is None: offset = 0 <IF_STMT> offset = 2 indentation = make_indentation(3 * self.indent_size + offset) self._multiline_indentation = indentation if self.current_rule: indent_extra = make_indentation(self.indent_size) return self._multiline_indentation + indent_extra return self._multiline_indentation,if self.show_aligned_keywords:
"def __call__(self, event, data=None): datatype, delta = event self.midi_ctrl.delta += delta if TIMING_CLOCK in datatype and (not self.played): self.midi_ctrl.pulse += 1 <IF_STMT> t_master = 60.0 self.midi_ctrl.bpm = round(60.0 / self.midi_ctrl.delta, 0) self.midi_ctrl.pulse = 0 self.midi_ctrl.delta = 0.0",if self.midi_ctrl.pulse == self.midi_ctrl.ppqn:
"def handle_sent(self, elt): sent = [] for child in elt: <IF_STMT> itm = self.handle_word(child) if self._unit == 'word': sent.extend(itm) else: sent.append(itm) else: raise ValueError('Unexpected element %s' % child.tag) return SemcorSentence(elt.attrib['snum'], sent)","if child.tag in ('wf', 'punc'):"
"def _handle_def_errors(testdef): if testdef.error: <IF_STMT> if isinstance(testdef.exception, Exception): raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception('Test parse failure')",if testdef.exception:
"def _authorized_sid(self, jid, sid, ifrom, iq): with self._preauthed_sids_lock: <IF_STMT> del self._preauthed_sids[jid, sid, ifrom] return True return False","if (jid, sid, ifrom) in self._preauthed_sids:"
"def wait(self, timeout=None): if self.returncode is None: <IF_STMT> msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",if timeout is None:
"def _gen_legal_y_s_t(self): while True: y = self._gen_random_scalar() s = self.tec_arithmetic.mul(scalar=y, a=self.tec_arithmetic.get_generator()) t = self._hash_tec_element(s) <IF_STMT> LOGGER.info('randomly generated y, S, T') return (y, s, t)",if self.tec_arithmetic.is_in_group(s) and type(t) != int:
def write_out(): while True: <IF_STMT> time.sleep(0.1) continue data_str = self.instrument_queue.get() data_str = data_str.splitlines() tb.write('') for line in data_str: tb.write(line) tb.write('\n'),if self.instrument_queue.empty():
"def _parse_preamble(self): """"""Parse metadata about query (PRIVATE)."""""" meta = {} while self.line: regx = re.search(_RE_QUERY, self.line) if regx: self.query_id = regx.group(1) <IF_STMT> self.seq_len = int(self.line.strip().split()[1]) self.line = self.handle.readline().strip() return meta",if self.line.startswith('Match_columns'):
"def init_sequence(self, coll_name, seq_config): if not isinstance(seq_config, list): raise Exception('""sequence"" config must be a list') handlers = [] for entry in seq_config: <IF_STMT> raise Exception('""sequence"" entry must be a dict') name = entry.get('name', '') handler = self.load_coll(name, entry) handlers.append(handler) return HandlerSeq(handlers)","if not isinstance(entry, dict):"
def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith(' '): ind += 1 else: <IF_STMT> ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if ':' in line and len(line) > 0: lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if start < ind:
def wait(self): while True: return_code = self._process.poll() if return_code is not None: line = self._process.stdout.readline().decode('utf-8') <IF_STMT> break log.debug(line.strip('\n')) return True,if line == '':
"def __getattr__(self, key): for tag in self.tag.children: <IF_STMT> continue if 'name' in tag.attrs and tag.attrs['name'] in (key,): from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError","if tag.name not in ('input',):"
"def compare_hash(hash_of_gold, path_to_file): with open(path_to_file, 'rb') as f: hash_of_file = hashlib.sha256(f.read()).hexdigest() <IF_STMT> print('########## Hash sum of', path_to_file, 'differs from the target, the topology will be deleted !!! ##########') shutil.rmtree(os.path.dirname(path_to_file))",if hash_of_file != hash_of_gold:
def on_completed2(): doner[0] = True if not qr: <IF_STMT> observer.on_next(False) observer.on_completed() elif donel[0]: observer.on_next(True) observer.on_completed(),if len(ql) > 0:
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): <IF_STMT> m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] if is_tuple: return tuple(data) else: return data else: return None",if item < 0:
"def _open_url(cls, url): if config.browser: cmd = [config.browser, url] <IF_STMT> print('running command: %s' % ' '.join(cmd)) p = Popen(cmd) p.communicate() else: if not config.quiet: print('opening URL in browser: %s' % url) webbrowser.open_new(url)",if not config.quiet:
"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = (self.c, self, self.w) if w: if hasattr(g.app.gui, 'set_minibuffer_label'): g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) <IF_STMT> k.mb_prefix = s",if protect:
"def __init__(self, path): self.symcaches = [] for path in path.split(';'): if os.path.isdir(path): self.symcaches.append(SymbolCache(dirname=path)) continue <IF_STMT> import cobra self.symcaches.append(cobra.CobraProxy(path)) continue",if path.startswith('cobra://') or path.startswith('cobrassl://'):
"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode='fan_out') if module.bias: init.constant(module.bias, 0) <IF_STMT> init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=0.001) if module.bias: init.constant(module.bias, 0)","elif isinstance(module, nn.BatchNorm2d):"
"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): <IF_STMT> removed.append(key) elif old[key] != new[key]: removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff['removed'] = sorted(removed) if added: diff['added'] = sorted(added) return diff",if key not in new:
"def __init__(self, *args, **kwargs): _kwargs = {'max_length': 20, 'widget': forms.TextInput(attrs={'autocomplete': 'off'}), 'label': _('Card number')} if 'types' in kwargs: self.accepted_cards = set(kwargs.pop('types')) difference = self.accepted_cards - VALID_CARDS <IF_STMT> raise ImproperlyConfigured('The following accepted_cards are unknown: %s' % difference) _kwargs.update(kwargs) super().__init__(*args, **_kwargs)",if difference:
"def dumps(self): sections = [] for name, env_info in self._dependencies_.items(): sections.append('[ENV_%s]' % name) for var, values in sorted(env_info.vars.items()): tmp = '%s=' % var <IF_STMT> tmp += '[%s]' % ','.join(['""%s""' % val for val in values]) else: tmp += '%s' % values sections.append(tmp) return '\n'.join(sections)","if isinstance(values, list):"
"def air_quality(self): aqi_data = self._get_aqi_data() if aqi_data: if aqi_data.get('status') == 'ok': aqi_data = self._organize(aqi_data) aqi_data = self._manipulate(aqi_data) <IF_STMT> self.py3.error(aqi_data.get('data')) return {'cached_until': self.py3.time_in(self.cache_timeout), 'full_text': self.py3.safe_format(self.format, aqi_data)}",elif aqi_data.get('status') == 'error':
"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): <IF_STMT> return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y","if not isinstance(y, (list, tuple)):"
"def _rate(cls, sample1, sample2): """"""Simple rate"""""" try: interval = sample2[0] - sample1[0] <IF_STMT> raise Infinity() delta = sample2[1] - sample1[1] if delta < 0: raise UnknownValue() return (sample2[0], delta / interval, sample2[2], sample2[3]) except Infinity: raise except UnknownValue: raise except Exception as e: raise NaN(e)",if interval == 0:
"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get('headers') is not None: <IF_STMT> if user_agent not in kwargs['headers']['user-agent']: kwargs['headers']['user-agent'] = f""{user_agent} {kwargs['headers']['user-agent']}"" else: kwargs['headers']['user-agent'] = user_agent else: kwargs['headers'] = {'user-agent': user_agent} return request_method(*args, **kwargs)",if kwargs['headers'].get('user-agent'):
"def remove_addons(auth, resource_object_list): for config in AbstractNode.ADDONS_AVAILABLE: try: settings_model = config.node_settings except LookupError: settings_model = None <IF_STMT> addon_list = settings_model.objects.filter(owner__in=resource_object_list, is_deleted=False) for addon in addon_list: addon.after_delete(auth.user)",if settings_model:
"def Decorator(*args, **kwargs): delay = 0.2 num_attempts = 15 cur_attempt = 0 while True: try: return f(*args, **kwargs) except exceptions.WebDriverException as e: logging.warning('Selenium raised %s', utils.SmartUnicode(e)) cur_attempt += 1 <IF_STMT> raise time.sleep(delay)",if cur_attempt == num_attempts:
"def _cleanup_parts_dir(parts_dir, local_plugins_dir, parts): if os.path.exists(parts_dir): logger.info('Cleaning up parts directory') for subdirectory in os.listdir(parts_dir): path = os.path.join(parts_dir, subdirectory) <IF_STMT> try: shutil.rmtree(path) except NotADirectoryError: os.remove(path) for part in parts: part.mark_cleaned(steps.BUILD) part.mark_cleaned(steps.PULL)",if path != local_plugins_dir:
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos['reach_leaf_node'].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos['node_pos'][t_idx] <IF_STMT> continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(tree, sample, cur_node_idx) if reach_leaf: node_pos['reach_leaf_node'][t_idx] = True node_pos['node_pos'][t_idx] = rs return node_pos",if cur_node_idx == -1:
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] <IF_STMT> results += ['Manders'] if self.do_rwc: results += ['RWC'] if self.do_costes: results += ['Costes'] return results return []",if self.do_manders:
"def create_connection(self, infos, f2, laddr_infos, protocol): for family in infos: try: <IF_STMT> for laddr in laddr_infos: try: break except OSError: protocol = 'foo' else: continue except OSError: protocol = 'bar' else: break else: raise return protocol",if f2:
"def app_middleware(next, root, info, **kwargs): app_auth_header = 'HTTP_AUTHORIZATION' prefix = 'bearer' request = info.context if request.path == API_PATH: if not hasattr(request, 'app'): request.app = None auth = request.META.get(app_auth_header, '').split() if len(auth) == 2: auth_prefix, auth_token = auth <IF_STMT> request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",if auth_prefix.lower() == prefix:
"def when(self, matches, context): ret = [] for episode in matches.named('episode', lambda match: len(match.initiator) == 1): group = matches.markers.at_match(episode, lambda marker: marker.name == 'group', index=0) <IF_STMT> if not matches.range(*group.span, predicate=lambda match: match.name == 'title'): ret.append(episode) return ret",if group:
def locate_via_pep514(spec): with _PY_LOCK: if not _PY_AVAILABLE: from . import pep514 _PY_AVAILABLE.extend(pep514.discover_pythons()) _PY_AVAILABLE.append(CURRENT) for cur_spec in _PY_AVAILABLE: <IF_STMT> return cur_spec.path,if cur_spec.satisfies(spec):
def setCorkImageDefault(self): if settings.corkBackground['image'] != '': i = self.cmbCorkImage.findData(settings.corkBackground['image']) <IF_STMT> self.cmbCorkImage.setCurrentIndex(i),if i != -1:
"def _split_key(key): if isinstance(key, util.string_types): if key == _WILDCARD_TOKEN: return (_DEFAULT_TOKEN,) <IF_STMT> key = key[1:] return key.split('.') else: return (key,)",elif key.startswith('.' + _WILDCARD_TOKEN):
"def detach_volume(self, volume): for node in self.list_nodes(): <IF_STMT> continue for disk in node.image: if disk.id == volume.id: disk_id = disk.extra['disk_id'] return self._do_detach_volume(node.id, disk_id) return False",if type(node.image) is not list:
"def create(self, private=False): try: <IF_STMT> log.info('Creating private channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name, 'is_private': True}) else: log.info('Creating channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name}) except SlackAPIResponseError as e: if e.error == 'user_is_bot': raise RoomError(f'Unable to create channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e)",if private:
"def test_dataset_has_valid_etag(self, dataset_name): py_script_path = list(filter(lambda x: x, dataset_name.split('/')))[-1] + '.py' dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True) etag = None try: response = requests.head(dataset_url, allow_redirects=True, proxies=None, timeout=10) <IF_STMT> etag = response.headers.get('Etag') except (EnvironmentError, requests.exceptions.Timeout): pass self.assertIsNotNone(etag)",if response.status_code == 200:
"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): if os.path.islink(dirpath): continue log.info('changing mode of %s to %o', dirpath, mode) <IF_STMT> os.chmod(dirpath, mode)",if not self.dry_run:
def _clean(self): logger.info('Cleaning up...') if self._process is not None: <IF_STMT> for _ in range(3): self._process.terminate() time.sleep(0.5) if self._process.poll() is not None: break else: self._process.kill() self._process.wait() logger.error('KILLED') if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info('Cleanup complete'),if self._process.poll() is None:
"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char['text'].isspace(): <IF_STMT> yield current_word current_word = [] elif current_word and self.char_begins_new_word(current_word, char): yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word",if current_word:
"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) if comps: r = _lookup(comps, specs, provided, name, i + 1, l) <IF_STMT> return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",if r is not None:
"def run(cmd, task=None): process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True) output_lines = [] while True: line = process.stdout.readline() <IF_STMT> break line = line.decode('utf-8') output_lines += [line] logger.info(line.rstrip('\n')) process.stdout.close() exit_code = process.wait() if exit_code: output = ''.join(output_lines) raise subprocess.CalledProcessError(exit_code, cmd, output=output)",if not line:
"def process_response(self, request, response): if response.status_code == 404 and request.path_info.endswith('/') and (not is_valid_path(request.path_info)) and is_valid_path(request.path_info[:-1]): newurl = request.path[:-1] <IF_STMT> with safe_query_string(request): newurl += '?' + request.META['QUERY_STRING'] return HttpResponsePermanentRedirect(newurl) return response",if request.GET:
"def dependencies(self): deps = [] midx = None if self.ref is not None: query = TypeQuery(self.ref) super = query.execute(self.schema) if super is None: log.debug(self.schema) raise TypeNotFound(self.ref) <IF_STMT> deps.append(super) midx = 0 return (midx, deps)",if not super.builtin():
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'): <IF_STMT> with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, 'read'): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs",if isfile(self.object):
"def _save(self): fd, tempname = tempfile.mkstemp() fd = os.fdopen(fd, 'w') json.dump(self._cache, fd, indent=2, separators=(',', ': ')) fd.close() try: <IF_STMT> os.makedirs(os.path.dirname(self.filename)) shutil.move(tempname, self.filename) except (IOError, OSError): os.remove(tempname)",if not os.path.exists(os.path.dirname(self.filename)):
"def refiner_configs(self): rv = {} for refiner in refiner_manager: <IF_STMT> rv[refiner.name] = {k: v for k, v in self.config.items(refiner.name)} return rv",if self.config.has_section(refiner.name):
"def com_slice(self, primary, node, assigning): lower = upper = None if len(node.children) == 2: <IF_STMT> upper = self.com_node(node.children[1]) else: lower = self.com_node(node.children[0]) elif len(node.children) == 3: lower = self.com_node(node.children[0]) upper = self.com_node(node.children[2]) return Slice(primary, assigning, lower, upper, lineno=extractLineNo(node))",if node.children[0].type == token.COLON:
"def close(self, *args, **kwargs): super(mytqdm, self).close(*args, **kwargs) if hasattr(self, 'sp'): if self.total and self.n < self.total: self.sp(bar_style='danger') el<IF_STMT> self.sp(bar_style='success') else: self.sp(close=True)",if self.leave:
def test_alloc(self): b = bytearray() alloc = b.__alloc__() self.assertTrue(alloc >= 0) seq = [alloc] for i in range(100): b += b'x' alloc = b.__alloc__() self.assertTrue(alloc >= len(b)) <IF_STMT> seq.append(alloc),if alloc not in seq:
"def flush_file(self, key, f): f.flush() <IF_STMT> f.compress = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum((1 for f in self.files.values() if f.fileobj is not None)) if open_files > self.MAX_OPEN_FILES: f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",if self.compress:
"def _run(self): self.running = True while self.running: try: self.sched.run() except Exception as x: logging.error('Error during scheduler execution: %s' % str(x), exc_info=True) <IF_STMT> time.sleep(5)",if self.running:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <IF_STMT> self.set_max_rows(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
"def check(dbdef): """"""drop script must clear the database"""""" for version in dbdef: connector = MemConnector().bound(None) create(dbdef, version, connector) drop(dbdef, version, connector) remaining = connector.execute(""SELECT * FROM sqlite_master WHERE name NOT LIKE 'sqlite_%'"").fetchall() <IF_STMT> yield ('{0}:drop.sql'.format(version), remaining)",if remaining:
"def test_open_overwrite_offset_size(self, sftp): """"""Test writing data at a specific offset"""""" f = None try: self._create_file('file', 'xxxxyyyy') f = (yield from sftp.open('file', 'r+')) yield from f.write('zz', 3) yield from f.close() with open('file') as localf: self.assertEqual(localf.read(), 'xxxzzyyy') finally: <IF_STMT> yield from f.close() remove('file')",if f:
def pump(): import sys as _sys while self.countdown_active(): if not (self.connected('send') and other.connected('recv')): break try: data = other.recv(timeout=0.05) except EOFError: break <IF_STMT> return if not data: continue try: self.send(data) except EOFError: break if not _sys: return self.shutdown('send') other.shutdown('recv'),if not _sys:
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if 'results.train_y_misclass' in dd: <IF_STMT> optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): if 'hyper_parameters' in key: print(key + ': ' + str(value))",if dd['results.train_y_misclass'] < optimal_measure:
"def valid(self): valid = True <IF_STMT> return valid else: try: with io.open(self.pathfile, 'w', encoding='utf-8') as f: f.close() except OSError: valid = False if os.path.exists(self.pathfile): os.remove(self.pathfile) return valid",if os.path.exists(self.pathfile):
"def __getitem__(self, key): try: value = self.cache[key] except KeyError: f = BytesIO(self.dict[key.encode(self.keyencoding)]) value = Unpickler(f).load() <IF_STMT> self.cache[key] = value return value",if self.writeback:
"def hasMenu(cls, callingWindow, mainItem, selection, *fullContexts): for i, fullContext in enumerate(fullContexts): srcContext = fullContext[0] for menuHandler in cls.menus: m = menuHandler() <IF_STMT> return True return False","if m._baseDisplay(callingWindow, srcContext, mainItem, selection):"
"def lr_read_tables(module=tab_module, optimize=0): global _lr_action, _lr_goto, _lr_productions, _lr_method try: exec('import %s as parsetab' % module) global parsetab <IF_STMT> _lr_action = parsetab._lr_action _lr_goto = parsetab._lr_goto _lr_productions = parsetab._lr_productions _lr_method = parsetab._lr_method return 1 else: return 0 except (ImportError, AttributeError): return 0",if optimize or Signature.digest() == parsetab._lr_signature:
"def _Determine_Do(self): if sys.platform.startswith('win'): self.applicable = 1 for opt, optarg in self.chosenOptions: if opt == '--moz-tools': self.value = os.path.abspath(os.path.normpath(optarg)) break else: <IF_STMT> self.value = os.environ[self.name] else: self.value = None else: self.applicable = 0 self.determined = 1",if os.environ.has_key(self.name):
"def parse_chunked(self, unreader): size, rest = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() if not rest: raise NoMoreData() yield rest[:size] rest = rest[size:] while len(rest) < 2: rest += unreader.read() <IF_STMT> raise ChunkMissingTerminator(rest[:2]) size, rest = self.parse_chunk_size(unreader, data=rest[2:])",if rest[:2] != b'\r\n':
"def _scroll_down(self, cli): """"""Scroll window down."""""" info = self.render_info if self.vertical_scroll < info.content_height - info.window_height: <IF_STMT> self.content.move_cursor_down(cli) self.vertical_scroll += 1",if info.cursor_position.y <= info.configured_scroll_offsets.top:
"def _add_defaults_data_files(self): if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): item = convert_path(item) if os.path.isfile(item): self.filelist.append(item) else: dirname, filenames = item for f in filenames: f = convert_path(f) <IF_STMT> self.filelist.append(f)",if os.path.isfile(f):
"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): <IF_STMT> continue if i == 2 and self.count < 1: self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",if i <= start_after:
"def is_open(self): if self.signup_code: return True el<IF_STMT> if self.messages.get('invalid_signup_code'): messages.add_message(self.request, self.messages['invalid_signup_code']['level'], self.messages['invalid_signup_code']['text'].format(**{'code': self.get_code()})) return settings.ACCOUNT_OPEN_SIGNUP",if self.signup_code_present:
"def on_delete_from_disk(self, widget, data=None): model, iter = self.get_selection().get_selected() if iter: path = model.get_value(iter, COLUMN_PATH) <IF_STMT> ErrorDialog(_(""Can't delete system item from disk."")).launch() else: os.remove(path) self.update_items()",if self.is_defaultitem(path):
"def get_detections_for_batch(self, images): images = images[..., ::-1] detected_faces = self.face_detector.detect_from_batch(images.copy()) results = [] for i, d in enumerate(detected_faces): <IF_STMT> results.append(None) continue d = d[0] d = np.clip(d, 0, None) x1, y1, x2, y2 = map(int, d[:-1]) results.append((x1, y1, x2, y2)) return results",if len(d) == 0:
def on_update(self): self.max_per_well = 0 for pd in list(self.plate_well_site.values()): for wd in list(pd.values()): nplanes = sum([len(x) for x in list(wd.values())]) <IF_STMT> self.max_per_well = nplanes for registrant in self.registrants: registrant(),if nplanes > self.max_per_well:
"def is_writable(self, path): result = False while not result: if os.path.exists(path): result = os.access(path, os.W_OK) break parent = os.path.dirname(path) <IF_STMT> break path = parent return result",if parent == path:
"def _check_seed(self, seed): if seed is not None: <IF_STMT> self._raise_error('The random number generator seed value, seed, should be integer type or None.') if seed < 0: self._raise_error('The random number generator seed value, seed, should be non-negative integer or None.')",if type(seed) != int:
"def write(self, x): self._errors = 'backslashescape' if self.encoding != 'mbcs' else 'surrogateescape' try: return io.TextIOWrapper.write(self, to_text(x, errors=self._errors)) except UnicodeDecodeError: <IF_STMT> self._errors = 'surrogateescape' else: self._errors = 'replace' return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))",if self._errors != 'surrogateescape':
"def post(self, request, *args, **kwargs): validated_session = [] for session_id in request.data: session = get_object_or_none(Session, id=session_id) <IF_STMT> validated_session.append(session_id) self.model.objects.create(name='kill_session', args=session.id, terminal=session.terminal) return Response({'ok': validated_session})",if session and (not session.is_finished):
"def _has_list_or_dict_var_value_before(self, arg_index): for idx, value in enumerate(self.args): <IF_STMT> return False if variablematcher.is_list_variable(value) and (not variablematcher.is_list_variable_subitem(value)): return True if robotapi.is_dict_var(value) and (not variablematcher.is_dict_var_access(value)): return True return False",if idx > arg_index:
"def test_return_correct_type(self): for proto in protocols: <IF_STMT> self._check_return_correct_type('abc', 0) else: for obj in [b'abc\n', 'abc\n', -1, -1.1 * 0.1, str]: self._check_return_correct_type(obj, proto)",if proto == 0:
"def backward_impl(self, inputs, outputs, prop_down, accum): axis = self.forward_func.info.args['axis'] if prop_down[-1]: g_dy = inputs[-1].grad g_dy_ = F.stack(*[o.grad for o in outputs], axis=axis) <IF_STMT> g_dy += g_dy_ else: g_dy.copy_from(g_dy_)",if accum[-1]:
"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) elif was_selected: <IF_STMT> i = len(self.items) - 1 self.list.select_set(i)",if i >= len(self.items):
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = '' if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text <IF_STMT> root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if i > 0:
"def _get_tracks_compositors_list(): tracks_list = [] tracks = current_sequence().tracks compositors = current_sequence().compositors for track_index in range(1, len(tracks) - 1): track_compositors = [] for j in range(0, len(compositors)): comp = compositors[j] <IF_STMT> track_compositors.append(comp) tracks_list.append(track_compositors) return tracks_list",if comp.transition.b_track == track_index:
"def __getattr__(self, name): if name in self._sections: return '\n'.join(self._sections[name]) el<IF_STMT> return '' else: raise ConanException(""ConfigParser: Unrecognized field '%s'"" % name)",if self._allowed_fields and name in self._allowed_fields:
"def get_first_param_index(self, group_id, param_group, partition_id): for index, param in enumerate(param_group): param_id = self.get_param_id(param) <IF_STMT> return index return None",if partition_id in self.param_to_partition_ids[group_id][param_id]:
"def handle_uv_sockets(self, context): u_socket = self.inputs['U'] v_socket = self.inputs['V'] if self.cast_mode == 'Sphere': u_socket.hide_safe = True v_socket.hide_safe = True elif self.cast_mode in ['Cylinder', 'Prism']: v_socket.hide_safe = True <IF_STMT> u_socket.hide_safe = False else: if u_socket.hide_safe: u_socket.hide_safe = False if v_socket.hide_safe: v_socket.hide_safe = False",if u_socket.hide_safe:
"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, 'r') as f: lines = f.readlines() <IF_STMT> return with open(source, 'w') as f: if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]): f.write(lines[0]) for line in lines[1:]: f.write(line)",if len(lines) < 1:
"def inner(request, *args, **kwargs): page = request.current_page if page: if page.login_required and (not request.user.is_authenticated): return redirect_to_login(urlquote(request.get_full_path()), settings.LOGIN_URL) site = get_current_site() <IF_STMT> return _handle_no_page(request) return func(request, *args, **kwargs)","if not user_can_view_page(request.user, page, site):"
"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: <IF_STMT> self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: if '_create_temporary' in traceback.format_exc(): self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()","if kwargs.get('in_place', False):"
"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: <IF_STMT> if len(kwargs['event_data'][key]) > 1024: kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)","if isinstance(kwargs.get('event_data', {}).get(key), str):"
"def parse_auth(val): if val is not None: authtype, params = val.split(' ', 1) <IF_STMT> if authtype == 'Basic' and '""' not in params: pass else: params = parse_auth_params(params) return (authtype, params) return val",if authtype in known_auth_schemes:
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) <IF_STMT> self.cache[args] = (value, now) return value except TypeError: return func(*args)",if value:
def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color if t.theme_style == 'Dark': if self.md_bg_color == t.primary_color: c = t.primary_dark <IF_STMT> c = t.accent_dark return c,elif self.md_bg_color == t.accent_color:
def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 <IF_STMT> part_l |= 1 << 31 part_h >>= 1 if rflag: part_h ^= 3623878656 _table_h.append(part_h) return _table_h,if part_h & 1:
"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from['Stats']): if not old_obj.summary: self.entries_count['Stats'] -= 1 continue new_obj = self.model_to['Stats']() for key in new_obj.__table__.columns._data.keys(): <IF_STMT> continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",if key not in old_obj.__table__.columns:
"def get_in_turn_repetition(pred, is_cn=False): """"""Get in-turn repetition."""""" if len(pred) == 0: return 1.0 if isinstance(pred[0], str): pred = [tok.lower() for tok in pred] if is_cn: pred = ''.join(pred) tri_grams = set() for i in range(len(pred) - 2): tri_gram = tuple(pred[i:i + 3]) <IF_STMT> return 1.0 tri_grams.add(tri_gram) return 0.0",if tri_gram in tri_grams:
"def translate(): assert Lex.next() is AttributeList reader.read() attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: <IF_STMT> v = subs_attrs(v) if v: parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",if k == 'attrlist':
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) <IF_STMT> raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')","if not isinstance(self.axis, int):"
"def __getattr__(self, attrname): if attrname in ('visamp', 'visamperr', 'visphi', 'visphierr'): return ma.masked_array(self.__dict__['_' + attrname], mask=self.flag) elif attrname in ('cflux', 'cfluxerr'): <IF_STMT> return ma.masked_array(self.__dict__['_' + attrname], mask=self.flag) else: return None else: raise AttributeError(attrname)",if self.__dict__['_' + attrname] != None:
"def draw(self, context): layout = self.layout presets.draw_presets_ops(layout, context=context) for category in presets.get_category_names(): <IF_STMT> if category in preset_category_menus: class_name = preset_category_menus[category].__name__ layout.menu(class_name)",if category in preset_category_menus:
"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value if info not in self._reverse_infos: self._reverse_infos[info] = len(self._infos) self._infos.append(info) <IF_STMT> self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = '%d,%d' % (self._reverse_infos[info], self._reverse_references[reference]) else: raise Exception(""unsupported type '%s'"" % type(value))",if reference not in self._reverse_references:
"def format_bpe_text(symbols, delimiter=b'@@'): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = b'' if isinstance(symbols, str): symbols = symbols.encode() delimiter_len = len(delimiter) for symbol in symbols: <IF_STMT> word += symbol[:-delimiter_len] else: word += symbol words.append(word) word = b'' return b' '.join(words)",if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:
"def output_type(data, request, response): accept = request.accept if accept in ('', '*', '/'): handler = default or (handlers and next(iter(handlers.values()))) else: handler = default accepted = [accept_quality(accept_type) for accept_type in accept.split(',')] accepted.sort(key=itemgetter(0)) for _quality, accepted_content_type in reversed(accepted): <IF_STMT> handler = handlers[accepted_content_type] break if not handler: raise falcon.HTTPNotAcceptable(error) response.content_type = handler.content_type return handler(data, request=request, response=response)",if accepted_content_type in handlers:
"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: if item is None: flatten_items.append(b'') elif isinstance(item, bytes): flatten_items.append(item) <IF_STMT> flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b'\n'.join(flatten_items)","elif isinstance(item, int):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_mime_type(d.getVarInt32()) continue if tt == 16: self.set_quality(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 8:
"def delete(self, waiters): msgs = self.ofctl.get_all_flow(waiters) for msg in msgs: for stats in msg.body: vlan_id = VlanRouter._cookie_to_id(REST_VLANID, stats.cookie) <IF_STMT> self.ofctl.delete_flow(stats) assert len(self.packet_buffer) == 0",if vlan_id == self.vlan_id:
def missing_push_allowance(push_allowances: List[PushAllowance]) -> bool: for push_allowance in push_allowances: if push_allowance.actor.databaseId is None: continue <IF_STMT> return False return True,if str(push_allowance.actor.databaseId) == str(app_config.GITHUB_APP_ID):
"def _cluster_page(self, htmlpage): template_cluster, preferred = (_CLUSTER_NA, None) if self.clustering: self.clustering.add_page(htmlpage) <IF_STMT> clt = self.clustering.classify(htmlpage) if clt != -1: template_cluster = preferred = self.template_names[clt] else: template_cluster = _CLUSTER_OUTLIER return (template_cluster, preferred)",if self.clustering.is_fit:
"def readlines(self, size=-1): if self._nbr == self._size: return [] out = [] nbr = 0 while True: line = self.readline() if not line: break out.append(line) if size > -1: nbr += len(line) <IF_STMT> break return out",if nbr > size:
"def post_mortem(t=None): <IF_STMT> t = sys.exc_info()[2] if t is None: raise ValueError('A valid traceback must be passed if no exception is being handled.') p = BPdb() p.reset() p.interaction(None, t)",if t is None:
"def fixup(m): txt = m.group(0) if txt[:2] == '&#': try: <IF_STMT> return unichr(int(txt[3:-1], 16)) else: return unichr(int(txt[2:-1])) except ValueError: pass else: try: txt = unichr(htmlentitydefs.name2codepoint[txt[1:-1]]) except KeyError: pass return txt",if txt[:3] == '&#x':
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') <IF_STMT> value = item.group('value') value = _pythonize(value) if not item.group('name'): args.append(value) else: name = item.group('name') kwargs[name] = value return (tuple(args), kwargs)",if value is None:
def IT(cpu): cc = cpu.instruction.cc true_case = cpu._evaluate_conditional(cc) for c in cpu.instruction.mnemonic[1:]: <IF_STMT> cpu._it_conditional.append(true_case) elif c == 'e': cpu._it_conditional.append(not true_case),if c == 't':
"def flatten(self): result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) <IF_STMT> self.limit = 0 if self.reverse: data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",if len(data) < 100:
"def _get_beta_accumulators(self): with tf.init_scope(): <IF_STMT> graph = None else: graph = tf.get_default_graph() return (self._get_non_slot_variable('beta1_power', graph=graph), self._get_non_slot_variable('beta2_power', graph=graph))",if tf.executing_eagerly():
"def prefixed(self, prefix: _StrType) -> typing.Iterator['Env']: """"""Context manager for parsing envvars with a common prefix."""""" try: old_prefix = self._prefix <IF_STMT> self._prefix = prefix else: self._prefix = f'{old_prefix}{prefix}' yield self finally: self._prefix = None self._prefix = old_prefix",if old_prefix is None:
"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get('content-type') if ct: ct, options = parse_options_header(ct) charset = options.get('charset') if ct in JSON_CONTENT_TYPES: return self.json(charset) <IF_STMT> return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",elif ct.startswith('text/'):
"def test_incrementaldecoder(self): UTF8Writer = codecs.getwriter('utf-8') for sizehint in [None, -1] + list(range(1, 33)) + [64, 128, 256, 512, 1024]: istream = BytesIO(self.tstring[0]) ostream = UTF8Writer(BytesIO()) decoder = self.incrementaldecoder() while 1: data = istream.read(sizehint) <IF_STMT> break else: u = decoder.decode(data) ostream.write(u) self.assertEqual(ostream.getvalue(), self.tstring[1])",if not data:
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) <IF_STMT> delete_all(fn_full) elif fn.endswith('.png'): os.remove(fn_full) elif fn.endswith('.md'): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",if os.path.isdir(fn):
"def _delete_reason(self): for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)): ext = _lib.X509_REVOKED_get_ext(self._revoked, i) obj = _lib.X509_EXTENSION_get_object(ext) <IF_STMT> _lib.X509_EXTENSION_free(ext) _lib.X509_REVOKED_delete_ext(self._revoked, i) break",if _lib.OBJ_obj2nid(obj) == _lib.NID_crl_reason:
"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) if a < b: return -1 <IF_STMT> return 1 return 0 except: return cmp(x, y)",if a > b:
"def get_indentation_count(view, start): indent_count = 0 i = start - 1 while i > 0: ch = view.substr(i) scope = view.scope_name(i) if 'string.quoted' in scope or 'comment' in scope or 'preprocessor' in scope: extent = view.extract_scope(i) i = extent.a - 1 continue else: i -= 1 <IF_STMT> indent_count -= 1 elif ch == '{': indent_count += 1 return indent_count",if ch == '}':
"def set(self, name, value, ex=None, px=None, nx=False, xx=False): if not nx and (not xx) or (nx and self._db.get(name, None) is None) or (xx and (not self._db.get(name, None) is None)): if ex > 0: self._db.expire(name, datetime.now() + timedelta(seconds=ex)) <IF_STMT> self._db.expire(name, datetime.now() + timedelta(milliseconds=px)) self._db[name] = str(value) return True else: return None",elif px > 0:
"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): if start in line: should_yield = True continue if end and end in line: return <IF_STMT> yield line.strip().split(' ')[0]",if should_yield and line:
"def iter_event_handlers(self, resource: resources_.Resource, event: bodies.RawEvent) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn('SimpleRegistry.iter_event_handlers() is deprecated; use ResourceWatchingRegistry.iter_handlers().', DeprecationWarning) cause = _create_watching_cause(resource, event) for handler in self._handlers: <IF_STMT> pass elif registries.match(handler=handler, cause=cause, ignore_fields=True): yield handler","if not isinstance(handler, handlers.ResourceWatchingHandler):"
def __enter__(self): if log_timer: <IF_STMT> self.logger.debug('%s starting' % self.name) else: print('[%s starting]...' % self.name) self.tstart = time.time(),if self.logger:
"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True err_msg = 'T2T: skipped importing {num_missing} data_generators modules.' print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) <IF_STMT> print('Did not import module: %s; Cause: %s' % (module, err_str)) if not _is_import_err_msg(err_str, module): print('From module %s' % module) raise err",if log_all:
"def _ungroup(sequence, groups=None): for v in sequence: if isinstance(v, (list, tuple)): <IF_STMT> groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v",if groups is not None:
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time <IF_STMT> self.stop() if self.counter == self.count: self.stop(),if dt > self.timeout:
def dont_let_stderr_buffer(): while True: line = context.daemon.stderr.readline() <IF_STMT> return if DEAD_DEPLOYD_WORKER_MESSAGE.encode('utf-8') in line: context.num_workers_crashed += 1 print(f'deployd stderr: {line}'),if not line:
"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: if self.firsttime is None or x_stats.firsttime < self.firsttime: self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: <IF_STMT> self.lasttime = x_stats.lasttime self.last = x_stats.last",if self.lasttime is None or x_stats.lasttime >= self.lasttime:
"def test_rlimit_get(self): import resource p = psutil.Process(os.getpid()) names = [x for x in dir(psutil) if x.startswith('RLIMIT')] assert names for name in names: value = getattr(psutil, name) self.assertGreaterEqual(value, 0) <IF_STMT> self.assertEqual(value, getattr(resource, name)) self.assertEqual(p.rlimit(value), resource.getrlimit(value)) else: ret = p.rlimit(value) self.assertEqual(len(ret), 2) self.assertGreaterEqual(ret[0], -1) self.assertGreaterEqual(ret[1], -1)",if name in dir(resource):
"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): if not prop_name in entity.unindexed_properties(): prop_vals = entity[prop_name] <IF_STMT> num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes","if isinstance(prop_vals, list):"
"def check_value_check(self, x_data, t_data, use_cudnn): x = chainer.Variable(x_data) t = chainer.Variable(t_data) with chainer.using_config('use_cudnn', use_cudnn): <IF_STMT> functions.softmax_cross_entropy(x, t, enable_double_backprop=self.enable_double_backprop) else: with self.assertRaises(ValueError): functions.softmax_cross_entropy(x, t, enable_double_backprop=self.enable_double_backprop)",if self.valid:
"def get_note_title_file(note): mo = note_title_re.match(note.get('content', '')) if mo: fn = mo.groups()[0] fn = fn.replace(' ', '_') fn = fn.replace('/', '_') <IF_STMT> return '' if isinstance(fn, str): fn = unicode(fn, 'utf-8') else: fn = unicode(fn) if note_markdown(note): fn += '.mkdn' else: fn += '.txt' return fn else: return ''",if not fn:
"def _parseparam(s): plist = [] while s[:1] == ';': s = s[1:] end = s.find(';') while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find(';', end + 1) if end < 0: end = len(s) f = s[:end] <IF_STMT> i = f.index('=') f = f[:i].strip().lower() + '=' + f[i + 1:].strip() plist.append(f.strip()) s = s[end:] return plist",if '=' in f:
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue <IF_STMT> continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))",if grandchild.tagName != 'File':
"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: <IF_STMT> ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.datetime): ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.time): ret = value.strftime('%H:%M:%S') else: ret = value return ret","if isinstance(value, datetime.date):"
"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) <IF_STMT> IECore.msg(IECore.Msg.Level.Error, 'GafferImageUI.CatalogueUI', ""No column registered with name '%s'"" % name) continue if isinstance(definition, IconColumn): c = GafferUI.PathListingWidget.IconColumn(definition.title(), '', name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns",if not definition:
"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): <IF_STMT> v = v.item() if isinstance(v, dict): v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics","if isinstance(v, torch.Tensor):"
"def start(self, connection): try: if self.client_name: creds = gssapi.Credentials(name=gssapi.Name(self.client_name)) else: creds = None hostname = self.get_hostname(connection) name = gssapi.Name(b'@'.join([self.service, hostname]), gssapi.NameType.hostbased_service) context = gssapi.SecurityContext(name=name, creds=creds) return context.step(None) except gssapi.raw.misc.GSSError: <IF_STMT> return NotImplemented else: raise",if self.fail_soft:
"def nanmax(self, axis=None, dtype=None, keepdims=None): ret = self._reduction('nanmax', axis=axis, dtype=dtype, keepdims=keepdims, todense=True) if not issparse(ret): <IF_STMT> return ret xps = get_sparse_module(self.spmatrix) ret = SparseNDArray(xps.csr_matrix(ret)) return ret return ret",if get_array_module(ret).isscalar(ret):
"def utterance_to_sample(query_data, tagging_scheme, language): tokens, tags = ([], []) current_length = 0 for chunk in query_data: chunk_tokens = tokenize(chunk[TEXT], language) tokens += [Token(t.value, current_length + t.start, current_length + t.end) for t in chunk_tokens] current_length += len(chunk[TEXT]) <IF_STMT> tags += negative_tagging(len(chunk_tokens)) else: tags += positive_tagging(tagging_scheme, chunk[SLOT_NAME], len(chunk_tokens)) return {TOKENS: tokens, TAGS: tags}",if SLOT_NAME not in chunk:
"def use_index(self, term: Union[str, Index], *terms: Union[str, Index]) -> 'QueryBuilder': for t in (term, *terms): if isinstance(t, Index): self._use_indexes.append(t) <IF_STMT> self._use_indexes.append(Index(t))","elif isinstance(t, str):"
"def reconfigServiceWithBuildbotConfig(self, new_config): if new_config.manhole != self.manhole: if self.manhole: yield self.manhole.disownServiceParent() self.manhole = None <IF_STMT> self.manhole = new_config.manhole yield self.manhole.setServiceParent(self) yield service.ReconfigurableServiceMixin.reconfigServiceWithBuildbotConfig(self, new_config)",if new_config.manhole:
"def cleanup_folder(target_folder): for file in os.listdir(target_folder): file_path = os.path.join(target_folder, file) try: <IF_STMT> os.remove(file_path) except Exception as e: logging.error(e)",if os.path.isfile(file_path):
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] <IF_STMT> return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)","if isinstance(k, float):"
"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) <IF_STMT> throw(DecompileError('Unsupported operation: %s' % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) if x is not None: decompiler.stack.append(x)",if method is None:
"def shutdown(self, timeout, callback=None): logger.debug('background worker got shutdown request') with self._lock: if self.is_alive: self._queue.put_nowait(_TERMINATOR) <IF_STMT> self._wait_shutdown(timeout, callback) self._thread = None self._thread_for_pid = None logger.debug('background worker shut down')",if timeout > 0.0:
"def getDOMImplementation(features=None): if features: <IF_STMT> features = domreg._parse_feature_string(features) for f, v in features: if not Document.implementation.hasFeature(f, v): return None return Document.implementation","if isinstance(features, str):"
"def validate_subevent(self, subevent): if self.context['event'].has_subevents: <IF_STMT> raise ValidationError('You need to set a subevent.') if subevent.event != self.context['event']: raise ValidationError('The specified subevent does not belong to this event.') elif subevent: raise ValidationError('You cannot set a subevent for this event.') return subevent",if not subevent:
"def einsum(job_id, idx, einsum_expr, data_list): _, all_parties = session_init(job_id, idx) with SPDZ(): <IF_STMT> x = FixedPointTensor.from_source('x', data_list[0]) y = FixedPointTensor.from_source('y', all_parties[1]) else: x = FixedPointTensor.from_source('x', all_parties[0]) y = FixedPointTensor.from_source('y', data_list[1]) return x.einsum(y, einsum_expr).get()",if idx == 0:
"def slowSorted(qq): """"""Reference sort peformed by insertion using only <"""""" rr = list() for q in qq: i = 0 for i in range(len(rr)): <IF_STMT> rr.insert(i, q) break else: rr.append(q) return rr",if q < rr[i]:
"def _format_entry(entry, src): if entry: result = [] for x in entry.split(','): x = x.strip() if os.path.exists(os.path.join(src, x)): result.append(relpath(os.path.join(src, x), src)) <IF_STMT> result.append(relpath(os.path.abspath(x), src)) else: raise RuntimeError('No entry script %s found' % x) return ','.join(result)",elif os.path.exists(x):
"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): if shape: t = anytype elif 'M' in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif 'i' in fmt: t = int <IF_STMT> t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",elif 'f' in fmt:
"def tool_lineages(self, trans): rval = [] for id, tool in self.app.toolbox.tools(): <IF_STMT> lineage_dict = tool.lineage.to_dict() else: lineage_dict = None entry = dict(id=id, lineage=lineage_dict) rval.append(entry) return rval","if hasattr(tool, 'lineage'):"
"def item(self, tensor): numel = 0 if len(tensor.shape) > 0: numel = fct.reduce(op.mul, tensor.shape) <IF_STMT> raise ValueError(f'expected tensor with one element, got {tensor.shape}') if numel == 1: return tensor[0] return tensor",if numel != 1:
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1).json() if 'Version' in resp: match = AGENT_VERSION_EXP.search(resp.get('Version')) <IF_STMT> meta['ecs_version'] = match.group(1) except Exception as e: self.log.debug('Error getting ECS version: %s' % str(e)) return meta",if match is not None and len(match.groups()) == 1:
"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): <IF_STMT> yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",if leaf == SymbolTrue:
"def _test_set_metadata(self, metadata, mask=None): header = ofproto.OXM_OF_METADATA match = OFPMatch() if mask is None: match.set_metadata(metadata) else: <IF_STMT> header = ofproto.OXM_OF_METADATA_W match.set_metadata_masked(metadata, mask) metadata &= mask self._test_serialize_and_parser(match, header, metadata, mask)",if mask + 1 >> 64 != 1:
"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == 'class': icon = 'class' elif tok.visibility == 'private': icon = 'method_priv' <IF_STMT> icon = 'method_prot' else: icon = 'method' crp.set_property('pixbuf', imagelibrary.pixbufs[icon])",elif tok.visibility == 'protected':
"def path_sum2(root, s): if root is None: return [] res = [] stack = [(root, [root.val])] while stack: node, ls = stack.pop() if node.left is None and node.right is None and (sum(ls) == s): res.append(ls) <IF_STMT> stack.append((node.left, ls + [node.left.val])) if node.right is not None: stack.append((node.right, ls + [node.right.val])) return res",if node.left is not None:
"def clear_slot(self, slot_id, trigger_changed): if self.slots[slot_id] is not None: old_resource_id = self.slots[slot_id].resource_id <IF_STMT> del self.sell_list[old_resource_id] else: del self.buy_list[old_resource_id] self.slots[slot_id] = None if trigger_changed: self._changed()",if self.slots[slot_id].selling:
"def OnRightUp(self, event): self.HandleMouseEvent(event) self.Unbind(wx.EVT_RIGHT_UP, handler=self.OnRightUp) self.Unbind(wx.EVT_MOUSE_CAPTURE_LOST, handler=self.OnRightUp) self._right = False if not self._left: self.Unbind(wx.EVT_MOTION, handler=self.OnMotion) self.SendChangeEvent() self.SetToolTip(wx.ToolTip(self._tooltip)) <IF_STMT> self.ReleaseMouse()",if self.HasCapture():
"def __init__(self, *args, **kwargs): for arg in args: for k, v in arg.items(): <IF_STMT> arg[k] = AttrDict(v) else: arg[k] = v super(AttrDict, self).__init__(*args, **kwargs)","if isinstance(v, dict):"
"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: t.status = 'aborted by user' if status: status('%s aborted' % t.name, priority=2) except Exception as e: t.exception = e t.status = 'exception' vd.exceptionCaught(e) <IF_STMT> t.sheet.currentThreads.remove(t)",if t.sheet:
"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): if text == '': self.labelList.item(i).setCheckState(2) <IF_STMT> self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",elif text != self.labelList.item(i).text():
"def __attempt_add_to_linked_match(self, input_name, hdca, collection_type_description, subcollection_type): structure = get_structure(hdca, collection_type_description, leaf_subcollection_type=subcollection_type) if not self.linked_structure: self.linked_structure = structure self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type else: <IF_STMT> raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE) self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type",if not self.linked_structure.can_match(structure):
"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) <IF_STMT> break if not online and (not self._is_testbot_online()): break else: raise AssertionError('test bot is still {}'.format('offline' if online else 'online'))",if online and self._is_testbot_online():
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 <IF_STMT> self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",if self.match_function(path):
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) <IF_STMT> continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:
def brightness_func(args): device = _get_device_from_filter(args) if args.set is None: if args.raw: print(str(device.brightness)) else: print('Brightness: {0}%'.format(device.brightness)) else: brightness_value = float(_clamp_u8(args.set)) <IF_STMT> print('Setting brightness to {0}%'.format(brightness_value)) device.brightness = brightness_value,if not args.raw:
"def _setup(self, field_name, owner_model): if not self.model_class: <IF_STMT> self.model_class = owner_model else: raise Exception(""ModelType: Unable to resolve model '{}'."".format(self.model_name)) super(ModelType, self)._setup(field_name, owner_model)",if self.model_name == owner_model.__name__:
"def build_json_schema_object(cls, parent_builder=None): builder = builders.ObjectBuilder(cls, parent_builder) if builder.count_type(builder.type) > 1: return builder for _, name, field in cls.iterate_with_name(): if isinstance(field, fields.EmbeddedField): builder.add_field(name, field, _parse_embedded(field, builder)) <IF_STMT> builder.add_field(name, field, _parse_list(field, builder)) else: builder.add_field(name, field, _create_primitive_field_schema(field)) return builder","elif isinstance(field, fields.ListField):"
"def filter_module(mod, type_req=None, subclass_req=None): for name in dir(mod): val = getattr(mod, name) if type_req is not None and (not isinstance(val, type_req)): continue <IF_STMT> continue yield (name, val)","if subclass_req is not None and (not issubclass(val, subclass_req)):"
"def get_icon(self): if self.icon is not None: if os.path.exists(self.icon): try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass icon_name, extension = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() <IF_STMT> return theme.load_icon(icon_name, 24, 0)",if theme.has_icon(icon_name):
"def sysctlTestAndSet(name, limit): """"""Helper function to set sysctl limits"""""" if '/' not in name: name = '/proc/sys/' + name.replace('.', '/') with open(name, 'r') as readFile: oldLimit = readFile.readline() if isinstance(limit, int): <IF_STMT> with open(name, 'w') as writeFile: writeFile.write('%d' % limit) else: with open(name, 'w') as writeFile: writeFile.write(limit)",if int(oldLimit) < limit:
"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) if online and self._is_testbot_online(): break <IF_STMT> break else: raise AssertionError('test bot is still {}'.format('offline' if online else 'online'))",if not online and (not self._is_testbot_online()):
"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: if context.rounding == ROUND_CEILING: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: <IF_STMT> return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context.rounding == ROUND_FLOOR:
"def _get_item_columns_panel(items, rows): hbox = Gtk.HBox(False, 4) n_item = 0 col_items = 0 vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) while n_item < len(items): item = items[n_item] vbox.pack_start(item, False, False, 0) n_item += 1 col_items += 1 <IF_STMT> vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) col_items = 0 return hbox",if col_items > rows:
"def _changed(self): if self.gtk_range.get_sensitive(): <IF_STMT> self.timer.cancel() self.timer = _Timer(0.5, lambda: GLib.idle_add(self._write)) self.timer.start()",if self.timer:
"def unlock_graph(result, callback, interval=1, propagate=False, max_retries=None): if result.ready(): second_level_res = result.get() <IF_STMT> with allow_join_result(): signature(callback).delay(list(joinall(second_level_res, propagate=propagate))) else: unlock_graph.retry(countdown=interval, max_retries=max_retries)",if second_level_res.ready():
"def update(self, other=None, /, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if other is not None: <IF_STMT> other = dict(other) for key, o in other.items(): d[key] = KeyedRef(o, self._remove, key) for key, o in kwargs.items(): d[key] = KeyedRef(o, self._remove, key)","if not hasattr(other, 'items'):"
"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: if hasattr(o, 'profile'): del o.profile if hasattr(o, 'credentials'): del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path <IF_STMT> del o.services_config return vars(o) except Exception as e: return str(o)","if hasattr(o, 'services_config'):"
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: <IF_STMT> log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) if ignore_timeouts and is_timeout(e): return [] if ignore_non_errors and is_noerr(e): return [] raise",if DEBUG_COMM:
def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: proc = PipelineProcess.objects.get(id=proc_id) <IF_STMT> continue for dr in self.doctors: if dr.confirm(proc): dr.cure(proc) break,if not proc.is_alive or proc.is_frozen:
"def to_value(self, value): ret = {} for key, val in value.items(): <IF_STMT> ret[key] = val elif key == 'points': ret[key] = {k: {'from': v[0], 'to': v[1]} for k, v in val.items()} else: ret[key] = {'from': val[0], 'to': val[1]} return ret","if key in ['attachments', 'custom_attributes', 'description_diff']:"
"def default_generator(self, dataset, epochs=1, mode='fit', deterministic=True, pad_batches=True): for epoch in range(epochs): for X_b, y_b, w_b, ids_b in dataset.iterbatches(batch_size=self.batch_size, deterministic=deterministic, pad_batches=pad_batches): <IF_STMT> dropout = np.array(0.0) else: dropout = np.array(1.0) yield ([X_b, dropout], [y_b], [w_b])",if mode == 'predict':
"def _cygwin_hack_find_addresses(target): addresses = [] for h in [target, 'localhost', '127.0.0.1']: try: addr = get_local_ip_for(h) <IF_STMT> addresses.append(addr) except socket.gaierror: pass return defer.succeed(addresses)",if addr not in addresses:
"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: if action_node.notify: task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify <IF_STMT> return self._chain_notify return None",elif self._chain_notify:
"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: <IF_STMT> break entry = extra.tokens[i] token = jsdict({'type': entry.type, 'value': entry.value}) if extra.range: token.range = entry.range if extra.loc: token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",if not i < len(extra.tokens):
"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: if not self.in_current_block(self.cursor): self.seek_to_block(self.cursor) part = self.current_stream.read(size) <IF_STMT> if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",if size > 0:
"def get_properties_from_model(model_class): """"""Show properties from a model"""""" properties = [] attr_names = [name for name, value in inspect.getmembers(model_class, isprop)] for attr_name in attr_names: <IF_STMT> attr_names.remove(attr_name) else: properties.append(dict(label=attr_name, name=attr_name.strip('_').replace('_', ' '))) return sorted(properties, key=lambda k: k['label'])",if attr_name.endswith('pk'):
"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] if value is not None: return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent <IF_STMT> return",if mydict is None:
"def multicolumn(self, list, format, cols=4): """"""Format a list of items into a multi-column list."""""" result = '' rows = (len(list) + cols - 1) // cols for col in range(cols): result = result + '<td width=""%d%%"" valign=top>' % (100 // cols) for i in range(rows * col, rows * col + rows): <IF_STMT> result = result + format(list[i]) + '<br>\n' result = result + '</td>' return '<table width=""100%%"" summary=""list""><tr>%s</tr></table>' % result",if i < len(list):
"def format_exc(exc=None): """"""Return exc (or sys.exc_info if None), formatted."""""" try: <IF_STMT> exc = _exc_info() if exc == (None, None, None): return '' import traceback return ''.join(traceback.format_exception(*exc)) finally: del exc",if exc is None:
"def assert_counts(res, lang, files, blank, comment, code): for line in res: fields = line.split() if len(fields) >= 5: <IF_STMT> self.assertEqual(files, int(fields[1])) self.assertEqual(blank, int(fields[2])) self.assertEqual(comment, int(fields[3])) self.assertEqual(code, int(fields[4])) return self.fail('Found no output line for {}'.format(lang))",if fields[0] == lang:
"def __iter__(self): for name, value in self.__class__.__dict__.items(): <IF_STMT> continue if isinstance(value, flag_value): yield (name, self._has_flag(value.flag))","if isinstance(value, alias_flag_value):"
"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_(beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment) if args.fp16: model.half() <IF_STMT> model.cuda()",if use_cuda:
"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote('extracted from web2py', 'developer') for source_str in mydict.keys(): target_str = mydict[source_str] if target_str == source_str: target_str = u'' <IF_STMT> target_str = u'' pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",elif target_str.startswith(u'*** '):
"def __sparse_values_set(instances, static_col_indexes: list): tmp_result = {idx: set() for idx in static_col_indexes} for _, instance in instances: data_generator = instance.features.get_all_data() for idx, value in data_generator: <IF_STMT> continue tmp_result[idx].add(value) result = [tmp_result[x] for x in static_col_indexes] return result",if idx not in tmp_result:
def puts(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.puts_ is None: self.puts_ = PutRequest() finally: self.lazy_init_lock_.release() return self.puts_,if self.puts_ is None:
"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} if args.resource_ref: filters['resource_ref'] = args.resource_ref <IF_STMT> filters['policy_type'] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",if args.policy_type:
"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = '' for line in string.split(entry, '\n'): if line[0:5] == 'GN   ': GN = string.strip(line[5:]) <IF_STMT> GN = GN[0:-1] return GN if line[0:2] == '//': break return GN",if GN[-1] == '.':
"def processMovie(self, atom): for field in atom: <IF_STMT> self.processTrack(field['track']) if 'movie_hdr' in field: self.processMovieHeader(field['movie_hdr'])",if 'track' in field:
"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: video_packet = self._get_video_packet() <IF_STMT> self._decode_video_packet(video_packet) if video_packet.image is not None or not skip_empty_frame: break if _debug: print('Returning', video_packet) return video_packet.image",if video_packet.image == 0:
"def get_devices(display=None): base = '/dev/input' for filename in os.listdir(base): if filename.startswith('event'): path = os.path.join(base, filename) <IF_STMT> continue try: _devices[path] = EvdevDevice(display, path) except OSError: pass return list(_devices.values())",if path in _devices:
"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error('# channels not specified') <IF_STMT> raise Error('sample width not specified') if not self._framerate: raise Error('sampling rate not specified') self._write_header(datasize)",if not self._sampwidth:
"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, '..') for line in fuzzresult.history.content.splitlines(): record = line.split('/') if len(record) == 6 and record[1]: self.queue_url(urljoin(base_url, record[1])) <IF_STMT> self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, '%s/CVS/Entries' % record[1]))",if record[0] == 'D':
"def tearDown(self): """"""Shutdown the UDP server."""""" try: <IF_STMT> self.server.stop(2.0) if self.sock_hdlr: self.root_logger.removeHandler(self.sock_hdlr) self.sock_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:
"def get_backend(find_library=None): try: global _lib, _ctx <IF_STMT> _lib = _load_library(find_library) _setup_prototypes(_lib) _ctx = _Context() _logger.warning('OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284)') return _OpenUSB() except usb.libloader.LibraryException: _logger.error('Error loading OpenUSB backend', exc_info=False) return None except Exception: _logger.error('Error loading OpenUSB backend', exc_info=True) return None",if _lib is None:
"def __init__(self, event, event_info, fields=[]): _wmi_object.__init__(self, event, fields=fields) _set(self, 'event_type', None) _set(self, 'timestamp', None) _set(self, 'previous', None) if event_info: event_type = self.event_type_re.match(event_info.Path_.Class).group(1).lower() _set(self, 'event_type', event_type) if hasattr(event_info, 'TIME_CREATED'): _set(self, 'timestamp', from_1601(event_info.TIME_CREATED)) <IF_STMT> _set(self, 'previous', event_info.PreviousInstance)","if hasattr(event_info, 'PreviousInstance'):"
"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: <IF_STMT> continue if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg): Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug('Adding ' + pkg + ' to the schedule list')",if pkg in Scheduler.listOfPackagesCurrentlyBuilding:
"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: <IF_STMT> self.write('') self.process(line) if not is_command(line): gap = True return 0",if gap:
"def diff(old, new, display=True): """"""Nice colored diff implementation"""""" if not isinstance(old, list): old = decolorize(str(old)).splitlines() if not isinstance(new, list): new = decolorize(str(new)).splitlines() line_types = {' ': '%Reset', '-': '%Red', '+': '%Green', '?': '%Pink'} if display: for line in difflib.Differ().compare(old, new): <IF_STMT> continue print(colorize(line_types[line[0]], line)) return old != new",if line.startswith('?'):
"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() if settings.MAX_PAGE_SIZE: <IF_STMT> return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",if limit == 0:
"def slice_fill(self, slice_): """"""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"""""" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): if self.squeeze_dims[i]: new_slice_.append(0) <IF_STMT> new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",elif offset < len(slice_):
"def wrapper(*args, **kw): instance = args[0] try: <IF_STMT> ret_dict = instance._create_ret_object(instance.FAILURE, None, True, instance.MUST_JSON) instance.logger.error(instance.MUST_JSON) return (jsonify(ret_dict), 400) except BadRequest: ret_dict = instance._create_ret_object(instance.FAILURE, None, True, instance.MUST_JSON) instance.logger.error(instance.MUST_JSON) return (jsonify(ret_dict), 400) instance.logger.debug('JSON is valid') return f(*args, **kw)",if request.get_json() is None:
"def add_css(self, data): if data: for medium, paths in data.items(): for path in paths: <IF_STMT> self._css.setdefault(medium, []).append(path)",if not self._css.get(medium) or path not in self._css[medium]:
"def mangle_template(template: str, template_vars: Set[str]) -> str: if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template: raise Exception('Cannot parse a template containing reserved strings') for var in template_vars: original = f'{{{var}}}' <IF_STMT> raise Exception(f'Template string is missing a reference to ""{var}"" referred to in kwargs') template = template.replace(original, mangled_name(var)) return template",if original not in template:
"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode('ascii', 'ignore')) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: if key in seenDict: continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue <IF_STMT> matchesappend(key) return _sortKeywords(keyword, matches)","if kwdSndx == soundex(key.encode('ascii', 'ignore')):"
"def GetInfo(self): for k, v in sorted(self.memory_parameters.items()): <IF_STMT> continue if not v: continue print('%s: \t%#08x (%s)' % (k, v, v)) print('Memory ranges:') print('Start\t\tEnd\t\tLength') for start, length in self.runs: print('0x%X\t\t0x%X\t\t0x%X' % (start, start + length, length))",if k.startswith('Pad'):
"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): is_list, property_type, is_strong = attributes[0:3] if is_strong and property in self._properties: <IF_STMT> children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",if not is_list:
"def normalize_res_identifier(self, emu, cw, val): mask = 16 ** (emu.get_ptr_size() // 2) - 1 << 16 if val & mask: name = emu.read_mem_string(val, cw) <IF_STMT> try: name = int(name[1:]) except Exception: return 0 else: name = val return name",if name[0] == '#':
"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): if best_silhouette is None: pass <IF_STMT> break best_silhouette = silhouette best_a = a best_k = k return (best_a, best_silhouette, best_k)",elif silhouette <= best_silhouette:
"def find_commit_type(sha): try: o = obj_store[sha] except KeyError: <IF_STMT> raise else: if isinstance(o, Commit): commits.add(sha) elif isinstance(o, Tag): tags.add(sha) commits.add(o.object[1]) else: raise KeyError('Not a commit or a tag: %s' % sha)",if not ignore_unknown:
"def on_search_entry_keypress(self, widget, event): key = Gdk.keyval_name(event.keyval) if key == 'Escape': self.hide_search_box() elif key == 'Return': <IF_STMT> self.search_prev = False self.do_search(None) else: self.search_prev = True",if event.state & Gdk.ModifierType.SHIFT_MASK:
"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: <IF_STMT> if '=' in each: key, value = each.split('=', 1) else: key, value = (each, '') result[key] = value namespace.webhook_properties = result",if each:
"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() <IF_STMT> continue self.lock.acquire() self.modules[path_id] = module self.lock.release() if resolve: resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",if path_id is None:
"def _get_download_link(self, url, download_type='torrent'): links = {'torrent': '', 'magnet': ''} try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find('div', {'class': 'download'}) if downloads: for download in downloads.findAll('a'): link = download['href'] <IF_STMT> links['magnet'] = link else: links['torrent'] = urljoin(self.urls['base_url'], link) except Exception: pass return links[download_type]",if link.startswith('magnet'):
"def _parse_fields(cls, read): read = unicode_to_str(read) if type(read) is not str: _wrong_type_for_arg(read, 'str', 'read') fields = {} while read and read[0] != ';': <IF_STMT> DeserializeError(read, 'does not separate fields with commas') read = read[1:] key, _type, value, read = cls._parse_field(read) fields[key] = (_type, value) if read: read = read[1:] return (fields, read)","if read and read[0] != ',':"
"def _convertDict(self, d): r = {} for k, v in d.items(): <IF_STMT> v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) if isinstance(k, bytes): k = str(k, 'utf-8') r[k] = v return r","if isinstance(v, bytes):"
"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) <IF_STMT> cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) if len(cache) > max_size: cache.popitem(last=False) return result",if old_mtime == mtime:
def isFinished(self): if self.count > self.epiLen: self.res() return True else: <IF_STMT> self.pertGlasPos(0) if self.count == self.epiLen / 2 + 1: self.env.reset() self.pertGlasPos(1) self.count += 1 return False,if self.count == 1:
"def _check_vulnerabilities(self, processed_analysis): matched_vulnerabilities = list() for vulnerability in self._rule_base_vulnerabilities: <IF_STMT> vulnerability_data = vulnerability.get_dict() name = vulnerability_data.pop('short_name') matched_vulnerabilities.append((name, vulnerability_data)) return matched_vulnerabilities","if evaluate(processed_analysis, vulnerability.rule):"
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): <IF_STMT> return '  %s' % val elif val < 1024 ** 2: return '  %.1f KB' % (val / 1024.0 ** 1) elif val < 1024 ** 3: return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val","if isinstance(val, compat.string_types):"
"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) if rd: self.handle_request() <IF_STMT> break",if self.event is not None and self.event.is_set():
"def resize(self, *e): bold = ('helvetica', -self._size.get(), 'bold') helv = ('helvetica', -self._size.get()) xspace = self._size.get() yspace = self._size.get() for widget in self._widgets: widget['node_font'] = bold widget['leaf_font'] = helv widget['xspace'] = xspace widget['yspace'] = yspace if self._size.get() < 20: widget['line_width'] = 1 <IF_STMT> widget['line_width'] = 2 else: widget['line_width'] = 3 self._layout()",elif self._size.get() < 30:
"def __assertTilesChangedInRegion(self, t1, t2, region): for tileOriginTuple in t1.keys(): tileOrigin = imath.V2i(*tileOriginTuple) tileRegion = imath.Box2i(tileOrigin, tileOrigin + imath.V2i(GafferImage.ImagePlug.tileSize())) <IF_STMT> self.assertNotEqual(t1[tileOriginTuple], t2[tileOriginTuple]) else: self.assertEqual(t1[tileOriginTuple], t2[tileOriginTuple])","if GafferImage.BufferAlgo.intersects(tileRegion, region):"
"def grouped_by_prefix(args, prefixes): """"""Group behave args by (directory) scope into multiple test-runs."""""" group_args = [] current_scope = None for arg in args.strip().split(): assert not arg.startswith('-'), 'REQUIRE: arg, not options' scope = select_prefix_for(arg, prefixes) if scope != current_scope: <IF_STMT> yield ' '.join(group_args) group_args = [] current_scope = scope group_args.append(arg) if group_args: yield ' '.join(group_args)",if group_args:
"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) <IF_STMT> continue if isinstance(value, basestring): print_func = repr pieces.append('%s=%s' % (k, print_func(value))) if pieces or self.__base__: return '%s(%s)' % (self.__class__.__name__, ', '.join(pieces)) else: return ''",if not defaults and value == default_values[k]:
"def setInnerHTML(self, html): log.HTMLClassifier.classify(log.ThugLogging.url if log.ThugOpts.local else log.last_url, html) self.tag.clear() for node in bs4.BeautifulSoup(html, 'html.parser').contents: self.tag.append(node) name = getattr(node, 'name', None) <IF_STMT> continue handler = getattr(log.DFT, 'handle_%s' % (name,), None) if handler: handler(node)",if name is None:
"def createFields(self): yield Enum(Bits(self, 'class', 2), self.CLASS_DESC) yield Enum(Bit(self, 'form'), self.FORM_DESC) if self['class'].value == 0: yield Enum(Bits(self, 'type', 5), self.TYPE_DESC) else: yield Bits(self, 'type', 5) yield ASNInteger(self, 'size', 'Size in bytes') size = self['size'].value if size: <IF_STMT> for field in self._handler(self, size): yield field else: yield RawBytes(self, 'raw', size)",if self._handler:
"def _process_service_request(self, pkttype, pktid, packet): """"""Process a service request"""""" service = packet.get_string() packet.check_end() if service == self._next_service: self.logger.debug2('Accepting request for service %s', service) self._next_service = None self.send_packet(MSG_SERVICE_ACCEPT, String(service)) <IF_STMT> self._auth_in_progress = True self._send_deferred_packets() else: raise DisconnectError(DISC_SERVICE_NOT_AVAILABLE, 'Unexpected service request received')",if self.is_server() and service == _USERAUTH_SERVICE:
"def _read_fixed_body(self, content_length: int, delegate: httputil.HTTPMessageDelegate) -> None: while content_length > 0: body = await self.stream.read_bytes(min(self.params.chunk_size, content_length), partial=True) content_length -= len(body) <IF_STMT> with _ExceptionLoggingContext(app_log): ret = delegate.data_received(body) if ret is not None: await ret",if not self._write_finished or self.is_client:
"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) <IF_STMT> return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.ECHILD: return time.sleep(0.05) assert False, 'wait_for_child() timed out'",if target_pid == pid:
"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) <IF_STMT> cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",elif op.stage == OperandStage.combine:
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): <IF_STMT> yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,if word not in Force_Split_Words:
"def _iter_tags(self, type=None): """"""Yield all raw tags (limit to |type| if specified)"""""" for n in itertools.count(): tag = self._get_tag(n) <IF_STMT> yield tag if tag['d_tag'] == 'DT_NULL': break",if type is None or tag['d_tag'] == type:
"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [(idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) if line.lstrip().startswith(searchfor.lstrip())] else: res = [(idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) <IF_STMT>] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return ''",if line.startswith(searchfor)
"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('Oracle backend does not support timezone-aware datetimes when USE_TZ is False.') return unicode(value)",if settings.USE_TZ:
"def _sniff(filename, oxlitype): try: with open(filename, 'rb') as fileobj: header = fileobj.read(4) if header == b'OXLI': fileobj.read(1) ftype = fileobj.read(1) <IF_STMT> return True return False except OSError: return False",if binascii.hexlify(ftype) == oxlitype:
"def unget(self, char): if char is not EOF: <IF_STMT> self.chunk = char + self.chunk self.chunkSize += 1 else: self.chunkOffset -= 1 assert self.chunk[self.chunkOffset] == char",if self.chunkOffset == 0:
"def scan(rule, extensions, paths, ignore_paths=None): """"""The libsast scan."""""" try: options = {'match_rules': rule, 'match_extensions': extensions, 'ignore_paths': ignore_paths, 'show_progress': False} scanner = Scanner(options, paths) res = scanner.scan() <IF_STMT> return format_findings(res['pattern_matcher'], paths[0]) except Exception: logger.exception('libsast scan') return {}",if res:
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern <IF_STMT> key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'): template = DateEpoch(lineBeginOnly=key != 'EPOCH') elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'): template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template",if '%' not in pattern:
"def _forward_response(self, src, dst): """"""Forward an SCP response between two remote SCP servers"""""" try: exc = (yield from src.await_response()) <IF_STMT> dst.send_error(exc) return exc else: dst.send_ok() return None except OSError as exc: return exc",if exc:
"def _maybe_signal_recovery_end() -> None: if self.in_recovery and (not self.active_remaining_total()): self.flush_buffers() self._set_recovery_ended() <IF_STMT> self._actives_span.set_tag('Actives-Ready', True) self.signal_recovery_end.set()",if self._actives_span is not None:
"def main(): tmpdir = None try: tmpdir = tempfile.mkdtemp() pip_zip = os.path.join(tmpdir, 'pip.zip') with open(pip_zip, 'wb') as fp: fp.write(b85decode(DATA.replace(b'\n', b''))) sys.path.insert(0, pip_zip) bootstrap(tmpdir=tmpdir) finally: <IF_STMT> shutil.rmtree(tmpdir, ignore_errors=True)",if tmpdir:
"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None if api_version_str == 'latest': self.latest = True else: <IF_STMT> self.preview = True parts = api_version_str.split('-') self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError('The API version {} is not in a supported format'.format(api_version_str))",if 'preview' in api_version_str:
"def _merge(self, items, map_id, dep_id, use_disk, meminfo, mem_limit): combined = self.combined merge_combiner = self.aggregator.mergeCombiners for k, v in items: o = combined.get(k) combined[k] = merge_combiner(o, v) if o is not None else v <IF_STMT> mem_limit = self._rotate()",if use_disk and meminfo.rss > mem_limit:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_value(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 8:
"def nice(deltat): times = _('second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years').split(':') d = abs(int(deltat)) for div, time in zip((60, 60, 24, 7, 4, 12, 100), times): <IF_STMT> return '%s%i %s' % (deltat < 0 and '-' or '', d, time.split(',')[d != 1]) d /= div",if d < div * 5:
"def after_get_object(self, event, view_kwargs): if event and event.state == 'draft': <IF_STMT> raise ObjectNotFound({'parameter': '{id}'}, 'Event: not found')","if not is_logged_in() or not has_access('is_coorganizer', event_id=event.id):"
def daemonize_if_required(self): if self.options.daemon: <IF_STMT> log.shutdown_multiprocessing_logging_listener(daemonizing=True) salt.utils.process.daemonize() self._setup_mp_logging_listener(),if self._setup_mp_logging_listener_ is True:
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: <IF_STMT> yield module break else: yield module",if module.is_compatible_with(client):
"def _incremental_avg_dp(self, avg, new_el, idx): for attr in ['coarse_segm', 'fine_segm', 'u', 'v']: setattr(avg, attr, (getattr(avg, attr) * idx + getattr(new_el, attr)) / (idx + 1)) <IF_STMT> setattr(new_el, attr, None) return avg",if idx:
"def run(self, paths=[]): collapsed = False for item in SideBarSelection(paths).getSelectedDirectories(): for view in item.views(): <IF_STMT> Window().focus_view(view) self.collapse_sidebar_folder() collapsed = True view.close()",if not collapsed:
"def test_reductions(expr, rdd): result = compute(expr, rdd) expected = compute(expr, data) if not result == expected: print(result) print(expected) <IF_STMT> assert abs(result - expected) < 0.001 else: assert result == expected","if isinstance(result, float):"
"def deltask(task, d): if task[:3] != 'do_': task = 'do_' + task bbtasks = d.getVar('__BBTASKS', False) or [] if task in bbtasks: bbtasks.remove(task) d.delVarFlag(task, 'task') d.setVar('__BBTASKS', bbtasks) d.delVarFlag(task, 'deps') for bbtask in d.getVar('__BBTASKS', False) or []: deps = d.getVarFlag(bbtask, 'deps', False) or [] <IF_STMT> deps.remove(task) d.setVarFlag(bbtask, 'deps', deps)",if task in deps:
"def _apply_weightnorm(self, list_layers): """"""Try apply weightnorm for all layer in list_layers."""""" for i in range(len(list_layers)): try: layer_name = list_layers[i].name.lower() <IF_STMT> list_layers[i] = WeightNormalization(list_layers[i]) except Exception: pass",if 'conv1d' in layer_name or 'dense' in layer_name:
"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == 'Average': self._local_aggregators.append(_AverageAggregator()) elif operator == 'Count': self._local_aggregators.append(_CountAggregator()) <IF_STMT> self._local_aggregators.append(_MaxAggregator()) elif operator == 'Min': self._local_aggregators.append(_MinAggregator()) elif operator == 'Sum': self._local_aggregators.append(_SumAggregator())",elif operator == 'Max':
"def _conv_layer(self, sess, bottom, name, trainable=True, padding='SAME', relu=True): with tf.variable_scope(name) as scope: filt = self._get_conv_filter(sess, name, trainable=trainable) conv_biases = self._get_bias(sess, name, trainable=trainable) conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding) bias = tf.nn.bias_add(conv, conv_biases) <IF_STMT> bias = tf.nn.relu(bias) return bias",if relu:
"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} for edge in self.edges: if edge.is_dangling(): raise ValueError('Cannot contract copy tensor with dangling edges') <IF_STMT> continue partner_node, shared_axis = self._get_partner(edge) if partner_node not in partners: partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",if self._is_my_trace(edge):
def close(self): with self._lock: 'Close this _MultiFileWatcher object forever.' <IF_STMT> self._folder_handlers = {} LOGGER.debug('Stopping observer thread even though there is a non-zero number of event observers!') else: LOGGER.debug('Stopping observer thread') self._observer.stop() self._observer.join(timeout=5),if len(self._folder_handlers) != 0:
"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): <IF_STMT> self.labelList.item(i).setCheckState(2) elif text != self.labelList.item(i).text(): self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",if text == '':
"def _get_messages(self): r = [] try: self._connect() self._login() for message in self._fetch(): <IF_STMT> r.append(message) self._connection.expunge() self._connection.close() self._connection.logout() except MailFetcherError as e: self.log('error', str(e)) return r",if message:
def get_current_user(self): try: <IF_STMT> return config.get('json_authentication_override') tkn_header = self.request.headers['authorization'] except KeyError: raise WebAuthNError(reason='Missing Authorization Header') else: tkn_str = tkn_header.split(' ')[-1] try: tkn = self.jwt_validator(tkn_str) except AuthenticationError as e: raise WebAuthNError(reason=e.message) else: return tkn,if config.get('development') and config.get('json_authentication_override'):
def _get_data(self): formdata = self._formdata if formdata: data = [] for item in formdata: model = self.loader.get_one(item) if item else None <IF_STMT> data.append(model) else: self._invalid_formdata = True self._set_data(data) return self._data,if model:
"def _getSubstrings(self, va, size, ltyp): subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) if loc and loc[L_LTYPE] == LOC_STRING and (loc[L_VA] > va): subs.add((loc[L_VA], loc[L_SIZE])) <IF_STMT> subs = subs.union(set(loc[L_TINFO])) return list(subs)",if loc[L_TINFO]:
def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: <IF_STMT> if monad.cls_bl_idname == self.cls_bl_idname: return monad return None,"if hasattr(monad, 'cls_bl_idname'):"
"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: if peer.bad: peer.status = PEER_BAD <IF_STMT> peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",elif peer.last_good > cutoff:
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name <IF_STMT> rval = '{} ({})'.format(rval, composite_file.description) if composite_file.optional: rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if composite_file.description:
"def testUiViewServerDump_windowIntM1(self): device = None try: device = MockDevice(version=15, startviewserver=True) vc = ViewClient(device, device.serialno, adb=TRUE, autodump=False) vc.dump(window=-1) vc.findViewByIdOrRaise('id/home') finally: <IF_STMT> device.shutdownMockViewServer()",if device:
"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) <IF_STMT> v = self._convertDict(v) if isinstance(k, bytes): k = str(k, 'utf-8') r[k] = v return r","elif isinstance(v, dict):"
def _testSendmsgTimeout(self): try: self.cli_sock.settimeout(0.03) try: while True: self.sendmsgToServer([b'a' * 512]) except socket.timeout: pass except OSError as exc: <IF_STMT> raise else: self.fail('socket.timeout not raised') finally: self.misc_event.set(),if exc.errno != errno.ENOMEM:
"def addError(self, test, err): if err[0] is SkipTest: if self.showAll: self.stream.writeln(str(err[1])) <IF_STMT> self.stream.write('s') self.stream.flush() return _org_AddError(self, test, err)",elif self.dots:
"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return <IF_STMT> self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",elif self.scroll_down_rect().collidepoint(p):
def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: idx = l.lower().find('copyright') if idx < 0: continue copyright = l[idx + 9:].strip() <IF_STMT> continue copyright = sanitise(copyright) if not copyright.find('200') >= 0 and (not copyright.find('199') >= 0): continue ret.add(copyright) return ret,if not copyright:
"def get_selectable_values(self, request): shop = lfs.core.utils.get_default_shop(request) countries = [] for country in shop.shipping_countries.all(): <IF_STMT> selected = True else: selected = False countries.append({'id': country.id, 'name': country.name, 'selected': selected}) return countries",if country in self.value.all():
"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 if col == nCol: for col in range(0, nCol, 2): if not self.layout.itemAt(row, col): break <IF_STMT> col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)",if col + 2 == nCol:
def contains_only_whitespace(node): if is_tag(node): if not any([not is_text(s) for s in node.contents]): <IF_STMT> return True return False,if not any([unicode(s).strip() for s in node.contents]):
"def tokenize_generator(cw): ret = [] done = {} for op in ops: ch = op.symbol[0] <IF_STMT> continue sops = start_symbols[ch] cw.write(""case '%s':"" % ch) for t in gen_tests(sops, 1): cw.write(t) done[ch] = True return ret",if ch in done:
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: <IF_STMT> nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)","if isinstance(nbChars, int):"
"def init(self, *args, **kwargs): if '_state' not in kwargs: state = {} for arg in ('children', 'windowState', 'detachedPanels'): if arg in kwargs: state[arg] = kwargs[arg] del kwargs[arg] <IF_STMT> kwargs['_state'] = state originalInit(self, *args, **kwargs)",if state:
def spm_decode(tokens: List[str]) -> List[str]: words = [] pieces: List[str] = [] for t in tokens: <IF_STMT> if len(pieces) > 0: words.append(''.join(pieces)) pieces = [t[1:]] else: pieces.append(t) if len(pieces) > 0: words.append(''.join(pieces)) return words,if t[0] == DecodeMixin.spm_bos_token:
"def _compare_dirs(self, dir1: str, dir2: str) -> List[str]: diff = [] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <IF_STMT> diff.append(file_) return diff",if not os.path.exists(target_path):
"def credentials(self): """"""The session credentials as a dict"""""" creds = {} if self._creds: <IF_STMT> creds['aws_access_key_id'] = self._creds.access_key if self._creds.secret_key: creds['aws_secret_access_key'] = self._creds.secret_key if self._creds.token: creds['aws_session_token'] = self._creds.token if self._session.region_name: creds['aws_region'] = self._session.region_name if self.requester_pays: creds['aws_request_payer'] = 'requester' return creds",if self._creds.access_key:
"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: for m in getattr(a, 'modules', []): m = m.strip() for mod in self.modules: if getattr(mod, 'module_type', '').strip() == mod_type.strip(): <IF_STMT> return True return False","if getattr(mod, 'module_name', '').strip() == m:"
"def find_file_at_path_with_indexes(self, path, url): if url.endswith('/'): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith('/' + self.index_file): if os.path.isfile(path): return self.redirect(url, url[:-len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: <IF_STMT> return self.redirect(url, url + '/') raise MissingFileError(path)","if os.path.isfile(os.path.join(path, self.index_file)):"
def _use_full_params(self) -> None: for p in self.params: if not p._is_sharded: <IF_STMT> assert p._fp16_shard.storage().size() != 0 p.data = p._fp16_shard else: assert p._full_param_padded.storage().size() != 0 p.data = p._full_param_padded[:p._orig_size.numel()].view(p._orig_size),if self.mixed_precision:
"def _attrdata(self, cont, name, *val): if not name: return (None, False) if isinstance(name, Mapping): if val: raise TypeError('Cannot set a value to %s' % name) return (name, True) elif val: <IF_STMT> return ({name: val[0]}, True) else: raise TypeError('Too may arguments') else: cont = self._extra.get(cont) return (cont.get(name) if cont else None, False)",if len(val) == 1:
"def evaluate(env, net, device='cpu'): obs = env.reset() reward = 0.0 steps = 0 while True: obs_v = ptan.agent.default_states_preprocessor([obs]).to(device) action_v = net(obs_v) action = action_v.data.cpu().numpy()[0] obs, r, done, _ = env.step(action) reward += r steps += 1 <IF_STMT> break return (reward, steps)",if done:
"def convert_html_js_files(app: Sphinx, config: Config) -> None: """"""This converts string styled html_js_files to tuple styled one."""""" html_js_files = [] for entry in config.html_js_files: <IF_STMT> html_js_files.append((entry, {})) else: try: filename, attrs = entry html_js_files.append((filename, attrs)) except Exception: logger.warning(__('invalid js_file: %r, ignored'), entry) continue config.html_js_files = html_js_files","if isinstance(entry, str):"
"def _check_duplications(self, regs): """"""n^2 loop which verifies that each reg exists only once."""""" for reg in regs: count = 0 for r in regs: <IF_STMT> count += 1 if count > 1: genutil.die('reg %s defined more than once' % reg)",if reg == r:
"def PyJsHoisted_vault_(key, forget, this, arguments, var=var): var = Scope({u'this': this, u'forget': forget, u'key': key, u'arguments': arguments}, var) var.registers([u'forget', u'key']) if PyJsStrictEq(var.get(u'key'), var.get(u'passkey')): return var.put(u'secret', var.get(u'null')) <IF_STMT> else var.get(u'secret') or var.put(u'secret', var.get(u'secretCreatorFn')(var.get(u'object')))",if var.get(u'forget')
"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) <IF_STMT> d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v, dict):"
"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): if value & 128: <IF_STMT> out.append('(unknown)') else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate('Transceiver compliance', ', '.join(out))",if len(TRANSCEIVER[t]) < b + 1:
"def process_string(self, remove_repetitions, sequence): string = '' for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: <IF_STMT> pass elif char == self.labels[self.space_index]: string += ' ' else: string = string + char return string",if remove_repetitions and i != 0 and (char == sequence[i - 1]):
"def clean(self): username = self.cleaned_data.get('username') password = self.cleaned_data.get('password') if username and password: self.user_cache = authenticate(username=username, password=password) <IF_STMT> raise forms.ValidationError(self.error_messages['invalid_login']) elif not self.user_cache.is_active: raise forms.ValidationError(self.error_messages['inactive']) self.check_for_test_cookie() return self.cleaned_data",if self.user_cache is None:
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get('events', []))): if e.get('event') == UserUttered.type_name: return False <IF_STMT> return e.get('name') == ACTION_LISTEN_NAME return False",elif e.get('event') == ActionExecuted.type_name:
"def getReferences(view, name=''): """"""Find all reference definitions."""""" refs = [] name = re.escape(name) if name == '': refs.extend(view.find_all('(?<=^\\[)([^\\]]+)(?=\\]:)', 0)) else: refs.extend(view.find_all('(?<=^\\[)(%s)(?=\\]:)' % name, 0)) regions = refs ids = {} for reg in regions: name = view.substr(reg).strip() key = name.lower() <IF_STMT> ids[key].regions.append(reg) else: ids[key] = Obj(regions=[reg], label=name) return ids",if key in ids:
"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: <IF_STMT> if self.revs_enabled: self.assertTrue(url.endswith('/latest'), msg=url) else: self.assertTrue(url.endswith('/download_urls'), msg=url) return headers.get(header_name)",if header_name in headers:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_shuffle_name(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def make_release_tree(self, base_dir, files): """"""Make the release tree."""""" self.mkpath(base_dir) create_tree(base_dir, files, dry_run=self.dry_run) if not files: self.log.warning('no files to distribute -- empty manifest?') else: self.log.info('copying files to %s...', base_dir) for filename in files: <IF_STMT> self.log.warning(""'%s' not a regular file -- skipping"", filename) else: dest = os.path.join(base_dir, filename) self.copy_file(filename, dest) self.distribution.metadata.write_pkg_info(base_dir)",if not os.path.isfile(filename):
"def _parse_names_set(feature_names): """"""Helping function of `_parse_feature_names` that parses a set of feature names."""""" feature_collection = OrderedDict() for feature_name in feature_names: <IF_STMT> feature_collection[feature_name] = ... else: raise ValueError('Failed to parse {}, expected string'.format(feature_name)) return feature_collection","if isinstance(feature_name, str):"
"def get_connection(self, url, proxies=None): with self.pools.lock: pool = self.pools.get(url) <IF_STMT> return pool pool = NpipeHTTPConnectionPool(self.npipe_path, self.timeout, maxsize=self.max_pool_size) self.pools[url] = pool return pool",if pool:
"def _parse_dimensions(dimensions): arrays = [] names = [] for key in dimensions: values = [v['name'] for v in key['values']] role = key.get('role', None) <IF_STMT> values = [_fix_quarter_values(v) for v in values] values = pd.DatetimeIndex(values) arrays.append(values) names.append(key['name']) midx = pd.MultiIndex.from_product(arrays, names=names) if len(arrays) == 1 and isinstance(midx, pd.MultiIndex): midx = midx.levels[0] return midx","if role in ('time', 'TIME_PERIOD'):"
"def _add_trials(self, name, spec): """"""Add trial by invoking TrialRunner."""""" resource = {} resource['trials'] = [] trial_generator = BasicVariantGenerator() trial_generator.add_configurations({name: spec}) while not trial_generator.is_finished(): trial = trial_generator.next_trial() <IF_STMT> break runner.add_trial(trial) resource['trials'].append(self._trial_info(trial)) return resource",if not trial:
"def _retrieve_key(self): url = 'http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf' text = '' try: r = requests.get(url, timeout=self.timeout, proxies=self.proxies) text = r.text except: self.error = 'ERROR - URL Connection' if text: expression = ""'(....-....-....-....)';"" pattern = re.compile(expression) match = pattern.search(text) <IF_STMT> self.key = match.group(1) return self.key else: self.error = 'ERROR - No API Key'",if match:
"def test_net(net, env, count=10, device='cpu'): rewards = 0.0 steps = 0 for _ in range(count): obs = env.reset() while True: obs_v = ptan.agent.float32_preprocessor([obs]).to(device) mu_v = net(obs_v)[0] action = mu_v.squeeze(dim=0).data.cpu().numpy() action = np.clip(action, -1, 1) obs, reward, done, _ = env.step(action) rewards += reward steps += 1 <IF_STMT> break return (rewards / count, steps / count)",if done:
"def compile(self, filename, obfuscate=False, raw=False, magic='\x00' * 8): body = marshal.dumps(compile(self.visit(self._source_ast), filename, 'exec')) if obfuscate: body_len = len(body) offset = 0 if raw else 8 output = bytearray(body_len + 8) for i, x in enumerate(body): output[i + offset] = ord(x) ^ 2 ** ((65535 - i) % 65535) % 251 <IF_STMT> for i in xrange(8): output[i] = 0 return output elif raw: return body else: return magic + body",if raw:
"def _map_saslprep(s): """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space"""""" r = [] for c in s: if stringprep.in_table_c12(c): r.append(' ') <IF_STMT> r.append(c) return ''.join(r)",elif not stringprep.in_table_b1(c):
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[w, pos] <IF_STMT> lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas",elif w in self.word_dict:
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: <IF_STMT> value = [extract_pyreal(item) for item in value.leaves] if any((item is None for item in value)): return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value","if value.has_form('List', None):"
"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) <IF_STMT> logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() if src.exists(): shutil.copy(src, dest)",if dest.exists():
"def assert_readback(vehicle, values): i = 10 while i > 0: time.sleep(0.1) i -= 0.1 for k, v in values.items(): <IF_STMT> continue break if i <= 0: raise Exception('Did not match in channels readback %s' % values)",if vehicle.channels[k] != v:
"def _get_linode_client(self): api_key = self.credentials.conf('key') api_version = self.credentials.conf('version') if api_version == '': api_version = None if not api_version: api_version = 3 regex_v4 = re.compile('^[0-9a-f]{64}$') regex_match = regex_v4.match(api_key) <IF_STMT> api_version = 4 else: api_version = int(api_version) return _LinodeLexiconClient(api_key, api_version)",if regex_match:
"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: <IF_STMT> self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: if self.lasttime is None or x_stats.lasttime >= self.lasttime: self.lasttime = x_stats.lasttime self.last = x_stats.last",if self.firsttime is None or x_stats.firsttime < self.firsttime:
"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] <IF_STMT> raise ValueError('Not all values of X are of equal length.') x_len = lengths[0] else: x_len = len(X) if y is not None: if len(y) != x_len: raise ValueError('X and y are not of equal length.') if self.regression and y is not None and (y.ndim == 1): y = y.reshape(-1, 1) return (X, y)",if len(set(lengths)) > 1:
"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" 'the ignore method' ignore = getattr(self, 'ignore') for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) if k in self.combinations: for k1 in self.combinations[k]: <IF_STMT> setattr(self, k1, ignore)","if not hasattr(self, k1):"
"def _parse_list(self, tokens): assert tokens[0] in ('[', '(') delim = ']' if tokens.pop(0) == '[' else ')' expr = ExpressionList() while tokens and tokens[0] != delim: item = self._parse(tokens) <IF_STMT> if tokens.pop(0) != ',': raise ExpressionSyntaxError('Expected: "",""') expr.append(item) if not tokens or tokens[0] != delim: raise ExpressionSyntaxError('Missing: ""%s""' % delim) else: tokens.pop(0) return expr",if tokens and tokens[0] != delim:
def param_value(self): for token in self: if token.token_type == 'value': return token.stripped_value <IF_STMT> for token in token: if token.token_type == 'bare-quoted-string': for token in token: if token.token_type == 'value': return token.stripped_value return '',if token.token_type == 'quoted-string':
"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): <IF_STMT> if line[len(comment):].lstrip().startswith(comment): continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",if line.startswith(comment):
"def lots_connected_to_existing_roads(model): set = [] for h in model.HarvestCells: for i, j in model.ExistingRoads: <IF_STMT> if h not in set: set.append(h) return set",if i in model.COriginNodeForCell[h] or j in model.COriginNodeForCell[h]:
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('\\Abarra_counter_session=', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None retval |= re.search('(\\A|\\b)barracuda_', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def test_files(self): dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): <IF_STMT> names.append(os.path.join(test_dir, n)) for filename in names: if test_support.verbose: print('Testing %s' % filename) source = read_pyfile(filename) self.check_roundtrip(source)",if n.endswith('.py') and (not n.startswith('bad')):
"def test_calibrate_target(create_target): mod, params = testing.synthetic.get_workload() dataset = get_calibration_dataset(mod, 'data') with relay.quantize.qconfig(calibrate_mode='kl_divergence'): <IF_STMT> with tvm.target.Target('llvm'): relay.quantize.quantize(mod, params, dataset) else: relay.quantize.quantize(mod, params, dataset)",if create_target:
"def _cleanSubmodule(self, _=None): rc = RC_SUCCESS if self.submodules: command = ['submodule', 'foreach', '--recursive', 'git', 'clean', '-f', '-f', '-d'] <IF_STMT> command.append('-x') rc = (yield self._dovccmd(command)) defer.returnValue(rc)",if self.mode == 'full' and self.method == 'fresh':
"def screen_length_to_bytes_count(string, screen_length_limit, encoding): bytes_count = 0 screen_length = 0 for unicode_char in string: screen_length += screen_len(unicode_char) char_bytes_count = len(unicode_char.encode(encoding)) bytes_count += char_bytes_count <IF_STMT> bytes_count -= char_bytes_count break return bytes_count",if screen_length > screen_length_limit:
"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) if example.valid: correct += 1 else: self.fail('Invalid schema was parsed: ' + example.schema_string) except: <IF_STMT> correct += 1 else: self.fail('Valid schema failed to parse: ' + example.schema_string) fail_msg = 'Parse behavior correct on %d out of %d schemas.' % (correct, len(EXAMPLES)) self.assertEqual(correct, len(EXAMPLES), fail_msg)",if not example.valid:
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): <IF_STMT> if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed","if isinstance(value, bool):"
"def normalize(d: Dict[Any, Any]) -> Dict[str, Any]: first_exception = None for normalizer in normalizers: try: normalized = normalizer(d) except KeyError as e: <IF_STMT> first_exception = e else: return normalized assert first_exception is not None raise first_exception",if not first_exception:
"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: if not isinstance(root_objs, Iterable): root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: <IF_STMT> args = self.get_callback_args(root_obj, callback) if args: yield args",if root_obj:
"def test_opdm_to_oqdm(self): for file in filter(lambda x: x.endswith('.hdf5'), os.listdir(DATA_DIRECTORY)): molecule = MolecularData(filename=os.path.join(DATA_DIRECTORY, file)) <IF_STMT> test_oqdm = map_one_pdm_to_one_hole_dm(molecule.fci_one_rdm) true_oqdm = numpy.eye(molecule.n_qubits) - molecule.fci_one_rdm assert numpy.allclose(test_oqdm, true_oqdm)",if molecule.fci_one_rdm is not None:
"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: if self.checkForStop(): return None subDomain = subDomainElem.get('subdomain', '').strip() <IF_STMT> self.emitHostname(subDomain, event)",if subDomain:
"def download_cve(download_path: str, years: Optional[List[int]]=None, update: bool=False): if update: process_url(CVE_URL.format('modified'), download_path) else: all_cve_urls = get_cve_links(CVE_URL, years) <IF_STMT> raise CveLookupException('Error: No CVE links found') for url in all_cve_urls: process_url(url, download_path)",if not all_cve_urls:
"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" assert directive and directive[0] == '@' skip_flag = directive in ('@others', '@all') while i < len(s): if match_word(s, i, directive): return (True, i) else: i = skip_line(s, i) <IF_STMT> i = skip_ws(s, i) return (False, -1)",if skip_flag:
"def run_async(self, nuke_cursors): interface_type = self.view.settings().get('git_savvy.interface') for cls in subclasses: if cls.interface_type == interface_type: vid = self.view.id() interface = interfaces.get(vid, None) <IF_STMT> interface = interfaces[vid] = cls(view=self.view) interface.render(nuke_cursors=nuke_cursors) break",if not interface:
"def scan_resource_conf(self, conf): if 'properties' in conf: <IF_STMT> if str(conf['properties']['sslEnforcement']).lower() == 'enabled': return CheckResult.PASSED return CheckResult.FAILED",if 'sslEnforcement' in conf['properties']:
"def do_shorts(opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != '': opt, optstring = (optstring[0], optstring[1:]) if short_has_arg(opt, shortopts): if optstring == '': <IF_STMT> raise GetoptError('option -%s requires argument' % opt, opt) optstring, args = (args[0], args[1:]) optarg, optstring = (optstring, '') else: optarg = '' opts.append(('-' + opt, optarg)) return (opts, args)",if not args:
def release(self): tid = _thread.get_ident() with self.lock: <IF_STMT> raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release(),if self.owner != tid:
"def _summarize_kraken(fn): """"""get the value at species level"""""" kraken = {} list_sp, list_value = ([], []) with open(fn) as handle: for line in handle: cols = line.strip().split('\t') sp = cols[5].strip() <IF_STMT> list_sp.append(sp) list_value.append(cols[0]) kraken = {'kraken_sp': list_sp, 'kraken_value': list_value} return kraken",if len(sp.split(' ')) > 1 and (not sp.startswith('cellular')):
"def _sync_remote_run(remote_run): assert remote_run.remote remote_name = remote_run.remote.name pull_args = click_util.Args(remote=remote_name, delete=False) try: remote_impl_support.pull_runs([remote_run], pull_args) except Exception as e: <IF_STMT> log.exception('pull %s from %s', remote_run.id, remote_name) else: log.error('error pulling %s from %s: %s', remote_run.id, remote_name, e)",if log.getEffectiveLevel() <= logging.DEBUG:
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) <IF_STMT> subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if sign is None:
"def import_til(self): log('Importing type libraries...') cur = self.db_cursor() sql = ""select name from diff.program_data where type = 'til'"" cur.execute(sql) for row in cur.fetchall(): til = row['name'] <IF_STMT> til = til.decode('utf-8') try: add_default_til(til) except: log('Error loading til %s: %s' % (row['name'], str(sys.exc_info()[1]))) cur.close() auto_wait()",if type(til) is bytes:
"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess('branch').stdout: <IF_STMT> git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() if BRANCH_ALIAS_MARKER in git_branch_line: alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",if git_branch_line.startswith('*'):
"def add_include_dirs(self, args): ids = [] for a in args: <IF_STMT> a = a.includedirs if not isinstance(a, IncludeDirs): raise InvalidArguments('Include directory to be added is not an include directory object.') ids.append(a) self.include_dirs += ids","if hasattr(a, 'includedirs'):"
"def _serialize_feature(self, feature): name = feature.unique_name() <IF_STMT> self._features_dict[feature.unique_name()] = feature.to_dictionary() for dependency in feature.get_dependencies(deep=True): name = dependency.unique_name() if name not in self._features_dict: self._features_dict[name] = dependency.to_dictionary()",if name not in self._features_dict:
"def generate_io(chart_type, race_configs, environment): structures = [] for race_config in race_configs: <IF_STMT> title = chart_type.format_title(environment, race_config.track, es_license=race_config.es_license, suffix='%s-io' % race_config.label) structures.append(chart_type.io(title, environment, race_config)) return structures",if 'io' in race_config.charts:
"def format_partition(partition, partition_schema): tokens = [] if isinstance(partition, dict): for name in partition_schema: <IF_STMT> tok = _format_partition_kv(name, partition[name], partition_schema[name]) else: tok = name tokens.append(tok) else: for name, value in zip(partition_schema, partition): tok = _format_partition_kv(name, value, partition_schema[name]) tokens.append(tok) return 'PARTITION ({})'.format(', '.join(tokens))",if name in partition:
"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, 'condition', Undefined) copy = self if condition is not Undefined: if isinstance(condition, core.SchemaBase): pass <IF_STMT> kwds = parse_shorthand(condition['field'], context.get('data', None)) copy = self.copy(deep=['condition']) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict(validate=validate, ignore=ignore, context=context)",elif 'field' in condition and 'type' not in condition:
"def _checkForCommand(self): prompt = b'cftp> ' if self._expectingCommand and self._lineBuffer == prompt: buf = b'\n'.join(self._linesReceived) <IF_STMT> buf = buf[len(prompt):] self.clearBuffer() d, self._expectingCommand = (self._expectingCommand, None) d.callback(buf)",if buf.startswith(prompt):
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' <IF_STMT> return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if key in LoggerFactory.schedule_logger_dict:
"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, 'i', 'ii', 'iii', 'iv', 'v', 'a', 'b', 'c', 'd', 'e']: for wrong in ['cd', 'part', 'dis', 'disc', 'dvd']: if '%s%s' % (wrong, nr) in nzb_name.lower(): wrong_found += 1 <IF_STMT> return -30 return 0 except: log.error('Failed doing halfMultipartScore: %s', traceback.format_exc()) return 0",if wrong_found == 1:
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') if value is None: value = item.group('value') value = _pythonize(value) <IF_STMT> args.append(value) else: name = item.group('name') kwargs[name] = value return (tuple(args), kwargs)",if not item.group('name'):
"def leaves(self, unique=True): """"""Get the leaves of the tree starting at this root."""""" if not self.children: return [self] else: res = list() for child in self.children: for sub_child in child.leaves(unique=unique): <IF_STMT> res.append(sub_child) return res",if not unique or sub_child not in res:
"def to_tree(self, tagname=None, idx=None, namespace=None): axIds = set((ax.axId for ax in self._axes)) for chart in self._charts: for id, axis in chart._axes.items(): <IF_STMT> setattr(self, axis.tagname, axis) axIds.add(id) return super(PlotArea, self).to_tree(tagname)",if id not in axIds:
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) <IF_STMT> rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.CONNECT_MODE:
"def close_all_connections(): global _managers, _lock, _in_use, _timer _lock.acquire() try: <IF_STMT> _timer.cancel() _timer = None for domain, managers in _managers.items(): for manager in managers: manager.close() _managers = {} finally: _lock.release()",if _timer:
"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): <IF_STMT> value[i] = self._instrument(item) return model","if isinstance(item, tf.keras.layers.Layer):"
"def target_glob(tgt, hosts): ret = {} for host in hosts: if fnmatch.fnmatch(tgt, host): ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {})) ret[host].update({'host': host}) <IF_STMT> ret[host].update({'user': __opts__['ssh_user']}) return ret",if __opts__.get('ssh_user'):
"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + ': ') <IF_STMT> sys.stderr.write(data.decode('utf8')) else: sys.stderr.write(data) if data[-1] == '\n': self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",if bytes:
"def task_thread(): while not task_queue.empty(): host, port, username, password = task_queue.get() logger.info('try burst {}:{} use username:{} password:{}'.format(host, port, username, password)) <IF_STMT> with task_queue.mutex: task_queue.queue.clear() result_queue.put((username, password))","if telnet_login(host, port, username, password):"
"def _format_results(name, ppl, scores, metrics): """"""Format results."""""" result_str = '' if ppl: result_str = '%s ppl %.2f' % (name, ppl) if scores: for metric in metrics: <IF_STMT> result_str += ', %s %s %.1f' % (name, metric, scores[metric]) else: result_str = '%s %s %.1f' % (name, metric, scores[metric]) return result_str",if result_str:
"def info_query(self, query): """"""Send a query which only returns 1 row"""""" self._cmysql.query(query) first_row = () if self._cmysql.have_result_set: first_row = self._cmysql.fetch_row() <IF_STMT> self._cmysql.free_result() raise errors.InterfaceError('Query should not return more than 1 row') self._cmysql.free_result() return first_row",if self._cmysql.fetch_row():
"def reset_class(self): for f in self.fields_order: <IF_STMT> f.value = int(f.strbits, 2) elif 'default_val' in f.kargs: f.value = int(f.kargs['default_val'], 2) else: f.value = None if f.fname: setattr(self, f.fname, f)",if f.strbits and isbin(f.strbits):
"def _walk_map_list(self, access_func): seen = [] cur = self while cur: <IF_STMT> break yield cur seen.append(cur.obj_offset) if len(seen) > 1024: break cur = access_func(cur)",if cur.obj_offset in seen:
def bgdel(): q = bgdelq while True: name = q.get() while os.path.exists(name): try: <IF_STMT> os.remove(name) else: shutil.rmtree(name) except: pass if os.path.exists(name): time.sleep(0.1),if os.path.isfile(name):
"def _find_all_variables(transfer_variable): d = {} for _k, _v in transfer_variable.__dict__.items(): if isinstance(_v, Variable): d[_v._name] = _v <IF_STMT> d.update(_find_all_variables(_v)) return d","elif isinstance(_v, BaseTransferVariables):"
"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] <IF_STMT> break if idx == len(os_widget.get_model()) - 1: idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",if value and row[0] == value:
"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = '%d/%ds' % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: if methods == ALL: methods = '' <IF_STMT> methods = ''.join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, 'RATELIMIT_CACHE_PREFIX', 'rl:') return prefix + hashlib.md5(u''.join(parts).encode('utf-8')).hexdigest()","elif isinstance(methods, (list, tuple)):"
"def findfiles(path): files = [] for name in os.listdir(path): <IF_STMT> continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode if stat.S_ISDIR(mode): files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",if name.startswith('.') or name == 'lastsnap.jpg':
"def __getitem__(self, key): if isinstance(key, str_types): keys = self.get_keys() <IF_STMT> raise KeyError(' ""{0}"" is an invalid key'.format(key)) else: return self[keys.index(key)] else: return list.__getitem__(self, key)",if key not in keys:
"def test_assert_set_equal(estimate: tp.Iterable[int], message: str) -> None: reference = {1, 2, 3} try: testing.assert_set_equal(estimate, reference) except AssertionError as error: if not message: raise AssertionError('An error has been raised while it should not.') from error np.testing.assert_equal(error.args[0].split('\n')[1:], message) else: <IF_STMT> raise AssertionError('An error should have been raised.')",if message:
"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: <IF_STMT> subp = os.path.join(pth, p) p = os.path.join(prefix, p) if recursive and os.path.isdir(subp): res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",if p[0] != '.':
"def check(self, runner, script, info): if isinstance(info, ast.FunctionDef): for arg in info.args.args: <IF_STMT> if arg.id in script.modelVars: self.problem('Function {0} may shadow model variable {1}'.format(info.name, arg.id), lineno=info.lineno)","if isinstance(arg, ast.Name):"
"def db_lookup(field, key, publish_year=None): sql = 'select sum(ebook_count) as num from subjects where field=$field and key=$key' if publish_year: <IF_STMT> sql += ' and publish_year between $y1 and $y2' y1, y2 = publish_year else: sql += ' and publish_year=$publish_year' return list(ebook_count_db.query(sql, vars=locals()))[0].num","if isinstance(publish_year, (tuple, list)):"
"def put(self, session): with sess_lock: self.parent.put(session) for sp in self.skip_paths: <IF_STMT> return if session.sid in self._cache: try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",if request.path.startswith(sp):
"def summarize(self): if self.bad_commit and self.good_commit: for subresult in self.subresults.values(): sub = subresult.summarize() <IF_STMT> return sub return 'Detected bad commit in {} repository:\n{} {}'.format(self.repo_name, self.bad_commit, get_message(self.suite, self.bad_commit)) return ''",if sub:
def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: if p.len == 0: nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 <IF_STMT> break num_nullable = len(nullable) return nullable,if len(nullable) == num_nullable:
"def _cast_float64_to_float32(self, feeds): for input_name, input_type in self.inputs: <IF_STMT> feed = feeds.get(input_name) if feed is not None and feed.dtype == np.float64: feeds[input_name] = feed.astype(np.float32) return feeds",if input_type == 'tensor(float)':
"def proc_minute(d): if expanded[0][0] != '*': diff_min = nearest_diff_method(d.minute, expanded[0], 60) <IF_STMT> if is_prev: d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return (True, d) return (False, d)",if diff_min is not None and diff_min != 0:
"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) if detyper is None: continue deval = detyper(val) <IF_STMT> continue ctx[key] = deval self._detyped = ctx return ctx",if deval is None:
"def get_or_create_user(request, user_data): try: user = User.objects.get(sso_id=user_data['id']) <IF_STMT> update_user(user, user_data) return user except User.DoesNotExist: user = User.objects.create_user(user_data['username'], user_data['email'], is_active=user_data.get('is_active', True), sso_id=user_data['id']) user.update_acl_key() setup_new_user(request.settings, user) return user","if user_needs_updating(user, user_data):"
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) <IF_STMT> self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","elif isinstance(v, bool):"
def load(cls): if not cls._loaded: cls.log.debug('Loading action_sets...') <IF_STMT> cls._find_action_sets(PATHS.ACTION_SETS_DIRECTORY) else: cls.action_sets = JsonDecoder.load(PATHS.ACTION_SETS_JSON_FILE) cls.log.debug('Done!') cls._loaded = True,if not horizons.globals.fife.use_atlases:
"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) <IF_STMT> value = setting.Value.split('|') value.remove(edge) setting.Value = '|'.join(value) if updater: updater.UpdateSetting(setting) if answer == Gtk.ResponseType.NO: return False return True",if answer == Gtk.ResponseType.YES:
"def read_tsv(input_file, quotechar=None): """"""Reads a tab separated value file."""""" with open(input_file, 'r', encoding='utf-8-sig') as f: reader = csv.reader(f, delimiter='\t', quotechar=quotechar) lines = [] for line in reader: <IF_STMT> line = list((str(cell, 'utf-8') for cell in line)) lines.append(line) return lines",if sys.version_info[0] == 2:
"def devd_devfs_hook(middleware, data): if data.get('subsystem') != 'CDEV': return if data['type'] == 'CREATE': disks = await middleware.run_in_thread(lambda: sysctl.filter('kern.disks')[0].value.split()) if data['cdev'] not in disks: return await added_disk(middleware, data['cdev']) elif data['type'] == 'DESTROY': <IF_STMT> return await remove_disk(middleware, data['cdev'])",if not RE_ISDISK.match(data['cdev']):
"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: <IF_STMT> if self.watchdirs[watchdir_id]['enabled']: client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",if col and col.get_title() == _('Active'):
"def _execute(self, options, args): if len(args) < 1: raise CommandError(_('Not enough arguments')) paths = args songs = [self.load_song(p) for p in paths] for song in songs: <IF_STMT> raise CommandError(_('Image editing not supported for %(file_name)s (%(file_format)s)') % {'file_name': song('~filename'), 'file_format': song('~format')}) for song in songs: try: song.clear_images() except AudioFileError as e: raise CommandError(e)",if not song.can_change_images:
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: <IF_STMT> try: if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()): filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules",if pricing_rule.condition:
"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text if text == '""use strict""' or text == ""'use strict'"": <IF_STMT> self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)",if len(self._scopeStrictModes) > 0:
"def _find_remote_inputs(metadata): out = [] for fr_key in metadata.keys(): if isinstance(fr_key, (list, tuple)): frs = fr_key else: frs = [fr_key] for fr in frs: <IF_STMT> out.append(fr) return out",if objectstore.is_remote(fr):
"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] if first.tag == 'p' and first.text is not None: m = RE_CHECKBOX.match(first.text) <IF_STMT> first.text = self.markdown.htmlStash.store(get_checkbox(m.group('state')), safe=True) + m.group('line') found = True return found",if m is not None:
"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [''] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) <IF_STMT> directories.append(filename) elif os.path.exists(os.path.join(basedir, filename)): yield filename","if os.path.isdir(os.path.join(basedir, filename)):"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_version(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _dump(self, fd): with self.no_unpicklable_properties(): <IF_STMT> d = pickle.dumps(self) module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0] d = d.replace(b'c__main__', b'c' + module_name.encode('ascii')) fd.write(d) else: pickle.dump(self, fd)",if self.__module__ == '__main__':
"def assert_session_stack(classes): assert len(_SklearnTrainingSession._session_stack) == len(classes) for idx, (sess, (parent_clazz, clazz)) in enumerate(zip(_SklearnTrainingSession._session_stack, classes)): assert sess.clazz == clazz <IF_STMT> assert sess._parent is None else: assert sess._parent.clazz == parent_clazz",if idx == 0:
"def native_color(c): try: color = CACHE[c] except KeyError: <IF_STMT> c = NAMED_COLOR[c] color = Color.FromArgb(int(c.rgba.a * 255), int(c.rgba.r), int(c.rgba.g), int(c.rgba.b)) CACHE[c] = color return color","if isinstance(c, str):"
"def callback(name): for neighbor_name in reactor.configuration.neighbors.keys(): neighbor = reactor.configuration.neighbors.get(neighbor_name, None) <IF_STMT> continue neighbor.rib.outgoing.announce_watchdog(name) yield False reactor.processes.answer_done(service)",if not neighbor:
"def token_producer(source): token = source.read_uint8() while token is not None: <IF_STMT> yield DataToken(read_data(token, source)) elif is_small_integer(token): yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",if is_push_data_token(token):
"def setattr(self, req, ino, attr, to_set, fi): print('setattr:', ino, to_set) a = self.attr[ino] for key in to_set: <IF_STMT> a['st_mode'] = S_IFMT(a['st_mode']) | S_IMODE(attr['st_mode']) else: a[key] = attr[key] self.attr[ino] = a self.reply_attr(req, a, 1.0)",if key == 'st_mode':
"def check_enum_exports(module, eq_callback, only=None): """"""Make sure module exports all mnemonics from enums"""""" for attr in enumerate_module(module, enum.Enum): <IF_STMT> print('SKIP', attr) continue for flag, value in attr.__members__.items(): print(module, flag, value) eq_callback(getattr(module, flag), value)",if only is not None and attr not in only:
"def remove_edit_vars_to(self, n): try: removals = [] for v, cei in self.edit_var_map.items(): <IF_STMT> removals.append(v) for v in removals: self.remove_edit_var(v) assert len(self.edit_var_map) == n except ConstraintNotFound: raise InternalError('Constraint not found during internal removal')",if cei.index >= n:
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): <IF_STMT> e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",if e.value is None:
"def add_I_prefix(current_line: List[str], ner: int, tag: str): for i in range(0, len(current_line)): if i == 0: f.write(line_list[i]) <IF_STMT> f.write(' I-' + tag) else: f.write(' ' + current_line[i]) f.write('\n')",elif i == ner:
def select_word_at_cursor(self): word_region = None selection = self.view.sel() for region in selection: word_region = self.view.word(region) <IF_STMT> selection.clear() selection.add(word_region) return word_region return word_region,if not word_region.empty():
"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num <IF_STMT> self.current -= num elif self.op == '*': self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '-':
"def strip_pod(lines): in_pod = False stripped_lines = [] for line in lines: if re.match('^=(?:end|cut)', line): in_pod = False elif re.match('^=\\w+', line): in_pod = True <IF_STMT> stripped_lines.append(line) return stripped_lines",elif not in_pod:
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) <IF_STMT> files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",if data:
"def loadPerfsFromModule(self, module): """"""Return a suite of all perfs cases contained in the given module"""""" perfs = [] for name in dir(module): obj = getattr(module, name) <IF_STMT> perfs.append(self.loadPerfsFromPerfCase(obj)) return self.suiteClass(perfs)","if type(obj) == types.ClassType and issubclass(obj, PerfCase):"
"def download_subtitle(self, subtitle): if isinstance(subtitle, XSubsSubtitle): logger.info('Downloading subtitle %r', subtitle) r = self.session.get(subtitle.download_link, headers={'Referer': subtitle.page_link}, timeout=10) r.raise_for_status() <IF_STMT> logger.debug('Unable to download subtitle. No data returned from provider') return subtitle.content = fix_line_ending(r.content)",if not r.content:
"def get_inlaws(self, person): inlaws = [] family_handles = person.get_family_handle_list() for handle in family_handles: fam = self.database.get_family_from_handle(handle) if fam.father_handle and (not fam.father_handle == person.handle): inlaws.append(self.database.get_person_from_handle(fam.father_handle)) <IF_STMT> inlaws.append(self.database.get_person_from_handle(fam.mother_handle)) return inlaws",elif fam.mother_handle and (not fam.mother_handle == person.handle):
"def _check_xorg_conf(): if is_there_a_default_xorg_conf_file(): print('WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not create it yourself, it was likely generated by your distribution or by an Nvidia utility.\nThis file may contain hard-coded GPU configuration that could interfere with optimus-manager, so it is recommended that you delete it before proceeding.\nIgnore this warning and proceed with GPU switching ? (y/N)') confirmation = ask_confirmation() <IF_STMT> sys.exit(0)",if not confirmation:
"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = '%d/%ds' % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: <IF_STMT> methods = '' elif isinstance(methods, (list, tuple)): methods = ''.join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, 'RATELIMIT_CACHE_PREFIX', 'rl:') return prefix + hashlib.md5(u''.join(parts).encode('utf-8')).hexdigest()",if methods == ALL:
"def num_of_mapped_volumes(self, initiator): cnt = 0 for lm_link in self.req('lun-maps')['lun-maps']: idx = lm_link['href'].split('/')[-1] try: lm = self.req('lun-maps', idx=int(idx))['content'] except exception.NotFound: continue <IF_STMT> cnt += 1 return cnt",if lm['ig-name'] == initiator:
"def _setAbsoluteY(self, value): if value is None: self._absoluteY = None else: <IF_STMT> value = 10 elif value == 'below': value = -70 try: value = common.numToIntOrFloat(value) except ValueError as ve: raise TextFormatException(f'Not a supported absoluteY position: {value!r}') from ve self._absoluteY = value",if value == 'above':
"def render_markdown(text): users = {u.username.lower(): u for u in get_mention_users(text)} parts = MENTION_RE.split(text) for pos, part in enumerate(parts): if not part.startswith('@'): continue username = part[1:].lower() <IF_STMT> user = users[username] parts[pos] = '**[{}]({} ""{}"")**'.format(part, user.get_absolute_url(), user.get_visible_name()) text = ''.join(parts) return mark_safe(MARKDOWN(text))",if username in users:
def start_process(self): with self.thread_lock: <IF_STMT> self.allow_process_request = False t = threading.Thread(target=self.__start) t.daemon = True t.start(),if self.allow_process_request:
"def close(self): if self._fh.closed: return self._fh.close() if os.path.isfile(self._filename): <IF_STMT> salt.utils.win_dacl.copy_security(source=self._filename, target=self._tmp_filename) else: shutil.copymode(self._filename, self._tmp_filename) st = os.stat(self._filename) os.chown(self._tmp_filename, st.st_uid, st.st_gid) atomic_rename(self._tmp_filename, self._filename)",if salt.utils.win_dacl.HAS_WIN32:
"def _splitSchemaNameDotFieldName(sn_fn, fnRequired=True): if sn_fn.find('.') != -1: schemaName, fieldName = sn_fn.split('.', 1) schemaName = schemaName.strip() fieldName = fieldName.strip() if schemaName and fieldName: return (schemaName, fieldName) elif not fnRequired: schemaName = sn_fn.strip() <IF_STMT> return (schemaName, None) controlflow.system_error_exit(2, f'{sn_fn} is not a valid custom schema.field name.')",if schemaName:
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: <IF_STMT> op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list","if not isinstance(op_list, list):"
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) <IF_STMT> for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True if not event.get_date_object(): return True return False",if family:
"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): <IF_STMT> if value is not None: value = value == 'true' self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)","if key in ('disabled', 'is_up'):"
"def get_billable_and_total_duration(activity, start_time, end_time): precision = frappe.get_precision('Timesheet Detail', 'hours') activity_duration = time_diff_in_hours(end_time, start_time) billing_duration = 0.0 if activity.billable: billing_duration = activity.billing_hours <IF_STMT> billing_duration = activity_duration * activity.billing_hours / activity.hours return (flt(activity_duration, precision), flt(billing_duration, precision))",if activity_duration != activity.billing_hours:
def cpus(self): try: cpus = self.inspect['Spec']['Resources']['Reservations']['NanoCPUs'] / 1000000000.0 <IF_STMT> cpus = int(cpus) return cpus except TypeError: return None except KeyError: return 0,if cpus == int(cpus):
"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): <IF_STMT> func_name = list(prop_value.keys())[0] if func_name.startswith('_'): func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props","if isinstance(prop_value, dict) and prop_value:"
"def _yield_unescaped(self, string): while '\\' in string: finder = EscapeFinder(string) yield (finder.before + finder.backslashes) <IF_STMT> yield self._unescape(finder.text) else: yield finder.text string = finder.after yield string",if finder.escaped and finder.text:
"def _check_matches(rule, matches): errors = 0 for match in matches: filematch = _match_to_test_file(match) <IF_STMT> utils.error(""The match '{}' for rule '{}' points to a non existing test module path: {}"", match, rule, filematch) errors += 1 return errors",if not filematch.exists():
"def focused_windows(): tree = i3.get_tree() workspaces = tree.workspaces() for workspace in workspaces: container = workspace while container: if not hasattr(container, 'focus') or not container.focus: break container_id = container.focus[0] container = container.find_by_id(container_id) <IF_STMT> coname = container.name wsname = workspace.name print('WS', wsname + ':', coname)",if container:
"def normals(self, value): if value is not None: value = np.asanyarray(value, dtype=np.float32) value = np.ascontiguousarray(value) <IF_STMT> raise ValueError('Incorrect normals shape') self._normals = value",if value.shape != self.positions.shape:
"def test_hexdigest(self): for cons in self.hash_constructors: h = cons() <IF_STMT> self.assertIsInstance(h.digest(16), bytes) self.assertEqual(hexstr(h.digest(16)), h.hexdigest(16)) else: self.assertIsInstance(h.digest(), bytes) self.assertEqual(hexstr(h.digest()), h.hexdigest())",if h.name in self.shakes:
"def _get_cluster_status(self): try: return self.dataproc_client.projects().regions().clusters().get(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name, fields='status').execute() except HttpError as e: <IF_STMT> return None else: raise e",if e.resp.status == 404:
"def _items_from(self, context): self._context = context if self._is_local_variable(self._keyword_name, context): for item in self._items_from_controller(context): yield item else: for df in context.datafiles: self._yield_for_other_threads() <IF_STMT> for item in self._items_from_datafile(df): yield item",if self._items_from_datafile_should_be_checked(df):
"def Command(argv, funcs, path_val): arg, i = COMMAND_SPEC.Parse(argv) status = 0 if arg.v: for kind, arg in _ResolveNames(argv[i:], funcs, path_val): <IF_STMT> status = 1 else: print(arg) else: util.warn('*** command without -v not not implemented ***') status = 1 return status",if kind is None:
"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: if isinstance(node, Preprint): category = 'preprint' <IF_STMT> category = 'registration' else: category = node.project_or_component client().delete(index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404])",elif node.is_registration:
def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): <IF_STMT> content = getDictFromTree(child) else: content = child.text if ret_dict.has_key(child.tag): if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or '') else: ret_dict[child.tag] = content or '' return ret_dict,if child.getchildren():
"def get(self, block=True, timeout=None, ack=False): if not block: return self.get_nowait() start_time = time.time() while True: try: return self.get_nowait(ack) except BaseQueue.Empty: <IF_STMT> lasted = time.time() - start_time if timeout > lasted: time.sleep(min(self.max_timeout, timeout - lasted)) else: raise else: time.sleep(self.max_timeout)",if timeout:
"def rewrite(self, string): string = super(JSReplaceFuzzy, self).rewrite(string) cdx = self.url_rewriter.rewrite_opts['cdx'] if cdx.get('is_fuzzy'): expected = unquote(cdx['url']) actual = unquote(self.url_rewriter.wburl.url) exp_m = self.rx_obj.search(expected) act_m = self.rx_obj.search(actual) <IF_STMT> result = string.replace(exp_m.group(1), act_m.group(1)) if result != string: string = result return string",if exp_m and act_m:
"def locate_exe_dir(d, check=True): exe_dir = os.path.join(d, 'Scripts') if ON_WINDOWS else os.path.join(d, 'bin') if not os.path.isdir(exe_dir): <IF_STMT> bin_dir = os.path.join(d, 'bin') if os.path.isdir(bin_dir): return bin_dir if check: raise InvalidVirtualEnv('Unable to locate executables directory.') return exe_dir",if ON_WINDOWS:
"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == 'append': if s not in sys.path: sys.path.append(s) el<IF_STMT> sys.path.insert(0, s)",if s != sys.path[0]:
"def create_season_banners(self, show_obj): if self.season_banners and show_obj: result = [] for season, episodes in show_obj.episodes.iteritems(): <IF_STMT> logger.log(u'Metadata provider ' + self.name + ' creating season banners for ' + show_obj.name, logger.DEBUG) result = result + [self.save_season_banners(show_obj, season)] return all(result) return False","if not self._has_season_banner(show_obj, season):"
"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: if 'nbgrader' not in cell.metadata: continue grade = cell.metadata['nbgrader']['grade'] solution = cell.metadata['nbgrader']['solution'] locked = cell.metadata['nbgrader']['locked'] <IF_STMT> continue grade_id = cell.metadata['nbgrader']['grade_id'] if grade_id in ids: raise ValidationError('Duplicate grade id: {}'.format(grade_id)) ids.add(grade_id)",if not grade and (not solution) and (not locked):
"def read_version(): regexp = re.compile(""^__version__\\W*=\\W*'([\\d.abrc]+)'"") init_py = os.path.join(os.path.dirname(__file__), 'aiopg', '__init__.py') with open(init_py) as f: for line in f: match = regexp.match(line) <IF_STMT> return match.group(1) else: raise RuntimeError('Cannot find version in aiopg/__init__.py')",if match is not None:
"def _column_keys(self): """"""Get a dictionary of all columns and their case mapping."""""" if not self.exists: return {} with self.db.lock: if self._columns is None: table = self.table self._columns = {} for column in table.columns: name = normalize_column_name(column.name) key = normalize_column_key(name) <IF_STMT> log.warning('Duplicate column: %s', name) self._columns[key] = name return self._columns",if key in self._columns:
"def find_controller_by_names(self, names, testname): namestring = '.'.join(names) if not namestring.startswith(self.name): return None if namestring == self.name: return self for suite in self.suites: res = suite.find_controller_by_names(namestring[len(self.name) + 1:].split('.'), testname) <IF_STMT> return res",if res:
"def _volume_x_metadata_get_item(context, volume_id, key, model, notfound_exec, session=None): result = _volume_x_metadata_get_query(context, volume_id, model, session=session).filter_by(key=key).first() if not result: <IF_STMT> raise notfound_exec(id=volume_id) else: raise notfound_exec(metadata_key=key, volume_id=volume_id) return result",if model is models.VolumeGlanceMetadata:
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] <IF_STMT> if dd['results.train_y_misclass'] < optimal_measure: optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): if 'hyper_parameters' in key: print(key + ': ' + str(value))",if 'results.train_y_misclass' in dd:
"def _stop_by_max_time_mins(self): """"""Stop optimization process once maximum minutes have elapsed."""""" if self.max_time_mins: total_mins_elapsed = (datetime.now() - self._start_datetime).total_seconds() / 60.0 <IF_STMT> raise KeyboardInterrupt('{:.2f} minutes have elapsed. TPOT will close down.'.format(total_mins_elapsed))",if total_mins_elapsed >= self.max_time_mins:
"def __new__(meta, cls_name, bases, cls_dict): func = cls_dict.get('func') monad_cls = super(FuncMonadMeta, meta).__new__(meta, cls_name, bases, cls_dict) if func: <IF_STMT> functions = func else: functions = (func,) for func in functions: registered_functions[func] = monad_cls return monad_cls",if type(func) is tuple:
"def get_tokens_unprocessed(self, text): buffered = '' insertions = [] lng_buffer = [] for i, t, v in self.language_lexer.get_tokens_unprocessed(text): <IF_STMT> if lng_buffer: insertions.append((len(buffered), lng_buffer)) lng_buffer = [] buffered += v else: lng_buffer.append((i, t, v)) if lng_buffer: insertions.append((len(buffered), lng_buffer)) return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))",if t is self.needle:
"def get_conditions(filters): conditions = {'docstatus': ('=', 1)} if filters.get('from_date') and filters.get('to_date'): conditions['result_date'] = ('between', (filters.get('from_date'), filters.get('to_date'))) filters.pop('from_date') filters.pop('to_date') for key, value in filters.items(): <IF_STMT> conditions[key] = value return conditions",if filters.get(key):
"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): <IF_STMT> if datetime.now() - limit > value: value = datetime.now() - limit elif datetime.now() + limit < value: value = datetime.now() + limit elif value > limit: value = limit return value",if config[key]['inverse'] is True:
"def GetCurrentKeySet(self): """"""Return CurrentKeys with 'darwin' modifications."""""" result = self.GetKeySet(self.CurrentKeys()) if sys.platform == 'darwin': for k, v in result.items(): v2 = [x.replace('<Alt-', '<Option-') for x in v] <IF_STMT> result[k] = v2 return result",if v != v2:
"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) if hasattr(package, '__loader__'): <IF_STMT> file_contents = package.__loader__.get_data(filename) return (file_contents.replace(os.linesep, '\n'), filename) return (open(filename).read(), filename)","if hasattr(package.__loader__, 'get_data'):"
"def iter_from_X_lengths(X, lengths): if lengths is None: yield (0, len(X)) else: n_samples = X.shape[0] end = np.cumsum(lengths).astype(np.int32) start = end - lengths <IF_STMT> raise ValueError('more than {:d} samples in lengths array {!s}'.format(n_samples, lengths)) for i in range(len(lengths)): yield (start[i], end[i])",if end[-1] > n_samples:
"def change_sel(self): """"""Change the view's selections."""""" if self.alter_select and len(self.sels) > 0: <IF_STMT> self.view.show(self.sels[0]) self.view.sel().clear() self.view.sel().add_all(self.sels)",if self.multi_select is False:
"def cb_syncthing_device_data_changed(self, daemon, nid, address, client_version, inbps, outbps, inbytes, outbytes): if nid in self.devices: device = self.devices[nid] device['address'] = address <IF_STMT> device['version'] = client_version device['inbps'] = '%s/s (%s)' % (sizeof_fmt(inbps), sizeof_fmt(inbytes)) device['outbps'] = '%s/s (%s)' % (sizeof_fmt(outbps), sizeof_fmt(outbytes))","if client_version not in ('?', None):"
"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: if match not in matches: <IF_STMT> match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",if self.match_name:
"def __update_parents(self, fileobj, path, delta): """"""Update all parent atoms with the new size."""""" if delta == 0: return for atom in path: fileobj.seek(atom.offset) size = cdata.uint_be(fileobj.read(4)) <IF_STMT> size = cdata.ulonglong_be(fileobj.read(12)[4:]) fileobj.seek(atom.offset + 8) fileobj.write(cdata.to_ulonglong_be(size + delta)) else: fileobj.seek(atom.offset) fileobj.write(cdata.to_uint_be(size + delta))",if size == 1:
"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: if field.primary_key: continue requires_index = any((field.index, field.unique, isinstance(field, ForeignKeyField))) <IF_STMT> fields.append(field) return fields",if requires_index:
"def __init__(self, value): """"""Initialize the integer to the given value."""""" self._mpz_p = new_mpz() self._initialized = False if isinstance(value, float): raise ValueError('A floating point type is not a natural number') self._initialized = True if isinstance(value, (int, long)): _gmp.mpz_init(self._mpz_p) result = _gmp.gmp_sscanf(tobytes(str(value)), b('%Zd'), self._mpz_p) <IF_STMT> raise ValueError(""Error converting '%d'"" % value) else: _gmp.mpz_init_set(self._mpz_p, value._mpz_p)",if result != 1:
"def decode(cls, data): while data: length, format_type, control_flags, sequence, pid = unpack(cls.Header.PACK, data[:cls.Header.LEN]) <IF_STMT> raise NetLinkError('Buffer underrun') yield cls.format(format_type, control_flags, sequence, pid, data[cls.Header.LEN:length]) data = data[length:]",if len(data) < length:
"def __post_init__(self): if self._node_id is not None: <IF_STMT> raise ValueError('invalid node_id: {}'.format(hexlify(self._node_id).decode())) if self.udp_port is not None and (not 1 <= self.udp_port <= 65535): raise ValueError('invalid udp port') if self.tcp_port is not None and (not 1 <= self.tcp_port <= 65535): raise ValueError('invalid tcp port') if not is_valid_public_ipv4(self.address, self.allow_localhost): raise ValueError(f""invalid ip address: '{self.address}'"")",if not len(self._node_id) == constants.HASH_LENGTH:
"def orderUp(self, items): sel = [] undoinfo = [] for bid, lid in items: if isinstance(lid, int): undoinfo.append(self.orderUpLineUndo(bid, lid)) sel.append((bid, lid - 1)) <IF_STMT> undoinfo.append(self.orderUpBlockUndo(bid)) if bid == 0: return items else: sel.append((bid - 1, None)) self.addUndo(undoinfo, 'Move Up') return sel",elif lid is None:
"def filter_data(self, min_len, max_len): logging.info(f'filtering data, min len: {min_len}, max len: {max_len}') initial_len = len(self.src) filtered_src = [] filtered_tgt = [] for src, tgt in zip(self.src, self.tgt): <IF_STMT> filtered_src.append(src) filtered_tgt.append(tgt) self.src = filtered_src self.tgt = filtered_tgt filtered_len = len(self.src) logging.info(f'pairs before: {initial_len}, after: {filtered_len}')",if min_len <= len(src) <= max_len and min_len <= len(tgt) <= max_len:
"def layer_pretrained(self, net, args, options): model = getattr(torchvision.models, args[0])(pretrained=True) model.train(True) if options.layer: layers = list(model.children())[:options.layer] <IF_STMT> layers[-1] = nn.Sequential(*layers[-1][:options.sublayer]) else: layers = [model] print('List of pretrained layers:', layers) raise ValidationException('layer=-1 required for pretrained, sublayer=-1 optional.  Layers outputted above.') return nn.Sequential(*layers)",if options.sublayer:
"def deleteCalendar(users): calendarId = normalizeCalendarId(sys.argv[5]) for user in users: user, cal = buildCalendarGAPIObject(user) <IF_STMT> continue gapi.call(cal.calendarList(), 'delete', soft_errors=True, calendarId=calendarId)",if not cal:
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) <IF_STMT> return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: if module.is_compatible_with(client): yield module break else: yield module",if not clients:
"def update_me(self): try: while 1: line = self.queue.get_nowait() <IF_STMT> self.delete(1.0, tk.END) else: self.insert(tk.END, str(line)) self.see(tk.END) self.update_idletasks() except queue.Empty: pass self.after(100, self.update_me)",if line is None:
"def request_power_state(self, state, force=False): if self.current_state != state or force: <IF_STMT> self.request_in_progress = True logging.info('Requesting %s' % state) cb = PowerManager.Callback(self, state) rets = self.parent.Plugins.run('on_power_state_change_requested', self, state, cb) cb.num_cb = len(rets) cb.check() else: logging.info('Another request in progress')",if not self.request_in_progress:
"def __getitem__(self, idx): super(BatchDataset, self).__getitem__(idx) maxidx = len(self.dataset) samples = [] for i in range(0, self.batchsize): j = idx * self.batchsize + i if j >= maxidx: break j = self.perm(j, maxidx) sample = self.dataset[j] <IF_STMT> samples.append(sample) samples = self.makebatch(samples) return samples",if self.filter(sample):
"def __call__(self, request, *args, **kwargs): template_vars = {} for form_name, form_class in self.forms.iteritems(): <IF_STMT> template_vars[form_name] = form_class(request) else: template_vars[form_name] = None if request.method == 'POST': action = self.find_post_handler_action(request) form = self.handlers[action](request, data=request.POST, files=request.FILES) template_vars.update(form.dispatch(action, request, *args, **kwargs)) return self.GET(template_vars, request, *args, **kwargs)","if form_class.must_display(request, *args, **kwargs):"
"def on_show_all(self, widget, another): if widget.get_active(): <IF_STMT> self.treeview.update_items(all=True, comment=True) else: self.treeview.update_items(all=True) elif another.get_active(): self.treeview.update_items(comment=True) else: self.treeview.update_items()",if another.get_active():
"def close(self): if self._closed: return self._closed = True for proto in self._pipes.values(): if proto is None: continue proto.pipe.close() if self._proc is not None and self._returncode is None and (self._proc.poll() is None): <IF_STMT> logger.warning('Close running child process: kill %r', self) try: self._proc.kill() except ProcessLookupError: pass",if self._loop.get_debug():
"def runTest(self): self.poco(text='wait UI').click() bomb_count = 0 while True: blue_fish = self.poco('fish_emitter').child('blue') yellow_fish = self.poco('fish_emitter').child('yellow') bomb = self.poco('fish_emitter').child('bomb') fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) if fish is bomb: bomb_count += 1 <IF_STMT> return else: fish.click() time.sleep(2.5)",if bomb_count > 3:
"def load_managers(*, loop, only): managers = {} for key in DB_CLASSES: <IF_STMT> continue params = DB_DEFAULTS.get(key) or {} params.update(DB_OVERRIDES.get(key) or {}) database = DB_CLASSES[key](**params) managers[key] = peewee_async.Manager(database, loop=loop) return managers",if only and key not in only:
"def links_extracted(self, request, links): for link in links: <IF_STMT> r = self._create_request(link.url) r.meta[b'depth'] = request.meta[b'depth'] + 1 self.schedule(r, self._get_score(r.meta[b'depth'])) link.meta[b'state'] = States.QUEUED",if link.meta[b'state'] == States.NOT_CRAWLED:
"def find_worktree_git_dir(dotgit): """"""Search for a gitdir for this worktree."""""" try: statbuf = os.stat(dotgit) except OSError: return None if not stat.S_ISREG(statbuf.st_mode): return None try: lines = open(dotgit, 'r').readlines() for key, value in [line.strip().split(': ') for line in lines]: <IF_STMT> return value except ValueError: pass return None",if key == 'gitdir':
"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: if not isinstance(dim_value, int): return False <IF_STMT> raise Exception('Negative dimension is illegal: %d' % dim_value) return True",if dim_value < 0:
"def init_logger(): configured_loggers = [log_config.get('root', {})] + [logger for logger in log_config.get('loggers', {}).values()] used_handlers = {handler for log in configured_loggers for handler in log.get('handlers', [])} for handler_id, handler in list(log_config['handlers'].items()): if handler_id not in used_handlers: del log_config['handlers'][handler_id] <IF_STMT> filename = handler['filename'] logfile_path = Path(filename).expanduser().resolve() handler['filename'] = str(logfile_path) logging.config.dictConfig(log_config)",elif 'filename' in handler.keys():
"def __call__(self): dmin, dmax = self.viewlim_to_dt() ymin = self.base.le(dmin.year) ymax = self.base.ge(dmax.year) ticks = [dmin.replace(year=ymin, **self.replaced)] while 1: dt = ticks[-1] <IF_STMT> return date2num(ticks) year = dt.year + self.base.get_base() ticks.append(dt.replace(year=year, **self.replaced))",if dt.year >= ymax:
"def taiga(request, trigger_id, key): signature = request.META.get('HTTP_X_TAIGA_WEBHOOK_SIGNATURE') if verify_signature(request._request.body, key, signature): data = data_filter(trigger_id, **request.data) status = save_data(trigger_id, data) return Response({'message': 'Success'}) <IF_STMT> else Response({'message': 'Failed!'}) Response({'message': 'Bad request'})",if status
"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: <IF_STMT> raise TypeError(f'Unexpected response type: `{type(response)}`') if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)","if not isinstance(response, rdf_client_fs.StatEntry):"
"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get('next', {}).get('url', None) if not src: break n += 1 <IF_STMT> vd.warning(f'stopping at max {max_next} pages') break vd.status(f'fetching next page from {src}') response = requests.get(src, stream=True)",if n > max_next:
"def __enter__(self): """"""Open a file and read it."""""" if self.code is None: LOGGER.info('File is reading: %s', self.path) <IF_STMT> self._file = open(self.path, encoding='utf-8') else: self._file = open(self.path, 'rU') self.code = self._file.read() return self","if sys.version_info >= (3,):"
"def facts_for_oauthclients(self, namespace): """"""Gathers facts for oauthclients used with logging"""""" self.default_keys_for('oauthclients') a_list = self.oc_command('get', 'oauthclients', namespace=namespace, add_options=['-l', LOGGING_SELECTOR]) if len(a_list['items']) == 0: return for item in a_list['items']: name = item['metadata']['name'] comp = self.comp(name) <IF_STMT> result = dict(redirectURIs=item['redirectURIs']) self.add_facts_for(comp, 'oauthclients', name, result)",if comp is not None:
"def get(self, k): with self._lock: <IF_STMT> self._data1[k] = self._data2[k] del self._data2[k] return self._data1.get(k)",if k not in self._data1 and k in self._data2:
"def _parseparam(s): plist = [] while s[:1] == ';': s = s[1:] end = s.find(';') while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find(';', end + 1) <IF_STMT> end = len(s) f = s[:end] if '=' in f: i = f.index('=') f = f[:i].strip().lower() + '=' + f[i + 1:].strip() plist.append(f.strip()) s = s[end:] return plist",if end < 0:
"def __init__(self, **params): if 'length' in params: <IF_STMT> raise ValueError('Supply either length or start and end to Player not both') params['start'] = 0 params['end'] = params.pop('length') - 1 elif params.get('start', 0) > 0 and (not 'value' in params): params['value'] = params['start'] super(Player, self).__init__(**params)",if 'start' in params or 'end' in params:
def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe('compiler.libcxx') if not compiler or not libcxx: return '' if str(compiler) in GCC_LIKE: if str(libcxx) == 'libstdc++': return '_GLIBCXX_USE_CXX11_ABI=0' <IF_STMT> return '_GLIBCXX_USE_CXX11_ABI=1' return '',elif str(libcxx) == 'libstdc++11':
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: if tag.user: tts[name] = '%ssort' % name <IF_STMT> tts['~%s' % name] = '~%ssort' % name return tts",if tag.internal:
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form('List', None): value = [extract_pyreal(item) for item in value.leaves] if any((item is None for item in value)): return None return value else: return None else: value = extract_pyreal(value) <IF_STMT> return None return value",if value is None or isinf(value) or isnan(value):
"def on_action_chosen(self, id, action, mark_changed=True): before = self.set_action(self.current, id, action) if mark_changed: <IF_STMT> self.undo.append(UndoRedo(id, before, action)) self.builder.get_object('btUndo').set_sensitive(True) self.on_profile_modified() else: self.on_profile_modified(update_ui=False) return before",if before.to_string() != action.to_string():
"def setUp(self): super(OperaterTest, self).setUp() if is_cli: import clr self.load_iron_python_test() <IF_STMT> clr.AddReference('System.Drawing.Primitives') else: clr.AddReference('System.Drawing')",if is_netcoreapp:
"def field_to_field_type(field): field_type = field['type'] if isinstance(field_type, dict): field_type = field_type['type'] if isinstance(field_type, list): field_type_length = len(field_type) if field_type_length == 0: raise Exception('Zero-length type list encountered, invalid CWL?') <IF_STMT> field_type = field_type[0] return field_type",elif len(field_type) == 1:
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: <IF_STMT> ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: if type(ah) is not ActionHandle: raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs",if type(item) is ActionHandle:
"def _Determine_Do(self): self.applicable = 1 configTokens = black.configure.items['configTokens'].Get() buildFlavour = black.configure.items['buildFlavour'].Get() if buildFlavour == 'full': self.value = False else: self.value = True for opt, optarg in self.chosenOptions: <IF_STMT> if not self.value: configTokens.append('tests') self.value = True elif opt == '--without-tests': if self.value: configTokens.append('notests') self.value = False self.determined = 1",if opt == '--with-tests':
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): <IF_STMT> rval = composite_name if composite_file.description: rval = '{} ({})'.format(rval, composite_file.description) if composite_file.optional: rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if i == index:
"def func(x, y): try: <IF_STMT> z = x + 2 * math.sin(y) return z ** 2 elif x == y: return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print('finished')",if x > y:
def test_suite(): suite = unittest.TestSuite() for fn in os.listdir(here): <IF_STMT> modname = 'distutils.tests.' + fn[:-3] __import__(modname) module = sys.modules[modname] suite.addTest(module.test_suite()) return suite,if fn.startswith('test') and fn.endswith('.py'):
"def check_stack_names(self, frame, expected): names = [] while frame: name = frame.f_code.co_name <IF_STMT> break names.append(name) frame = frame.f_back self.assertEqual(names, expected)",if name.startswith('check_') or name.startswith('call_'):
"def leave(self, reason=None): try: if self.id.startswith('C'): log.info('Leaving channel %s (%s)', self, self.id) self._bot.api_call('conversations.leave', data={'channel': self.id}) else: log.info('Leaving group %s (%s)', self, self.id) self._bot.api_call('conversations.leave', data={'channel': self.id}) except SlackAPIResponseError as e: <IF_STMT> raise RoomError(f'Unable to leave channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e) self._id = None",if e.error == 'user_is_bot':
"def ident(self): value = self._ident if value is False: value = None <IF_STMT> wrapped = self.wrapped ident = getattr(wrapped, 'ident', None) if ident is not None: value = self._wrap_hash(ident) self._ident = value return value",if not self.orig_prefix:
"def is_ac_power_connected(): for power_source_path in Path('/sys/class/power_supply/').iterdir(): try: with open(power_source_path / 'type', 'r') as f: <IF_STMT> continue with open(power_source_path / 'online', 'r') as f: if f.read(1) == '1': return True except IOError: continue return False",if f.read().strip() != 'Mains':
"def _get_pending_by_app_token(self, app_token): result = [] with self._pending_lock: self._remove_stale_pending() for data in self._pending_decisions: <IF_STMT> result.append(data) return result",if data.app_token == app_token:
"def do_create(specific_tables=None, base=Base): engine = get_engine() try: <IF_STMT> logger.info('Initializing only a subset of tables as requested: {}'.format(specific_tables)) base.metadata.create_all(engine, tables=specific_tables) else: base.metadata.create_all(engine) except Exception as err: raise Exception('could not create/re-create DB tables - exception: ' + str(err))",if specific_tables:
"def __setitem__(self, ndx, val): exprdata = None if ndx in self._data: exprdata = self._data[ndx] else: _ndx = normalize_index(ndx) <IF_STMT> exprdata = self._data[_ndx] if exprdata is None: raise KeyError(""Cannot set the value of Expression '%s' with invalid index '%s'"" % (self.cname(True), str(ndx))) exprdata.set_value(val)",if _ndx in self._data:
"def write(self, *bits): for bit in bits: <IF_STMT> self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if not self.bytestream:
"def terminate_subprocess(proc, timeout=0.1, log=None): if proc.poll() is None: <IF_STMT> log.info('Sending SIGTERM to %r', proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info('Sending SIGKILL to %r', proc) proc.kill() return proc.returncode",if log:
"def mkpanel(color, rows, cols, tly, tlx): win = curses.newwin(rows, cols, tly, tlx) pan = panel.new_panel(win) if curses.has_colors(): <IF_STMT> fg = curses.COLOR_WHITE else: fg = curses.COLOR_BLACK bg = color curses.init_pair(color, fg, bg) win.bkgdset(ord(' '), curses.color_pair(color)) else: win.bkgdset(ord(' '), curses.A_BOLD) return pan",if color == curses.COLOR_BLUE:
def all_words(filename): start_char = True for c in characters(filename): if start_char == True: word = '' <IF_STMT> word = c.lower() start_char = False else: pass elif c.isalnum(): word += c.lower() else: start_char = True yield word,if c.isalnum():
"def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict: init_vars = tf.train.list_variables(path) tf_weights = {} ignore_name = ['Adafactor', 'global_step'] for name, shape in tqdm(init_vars, desc='converting tf checkpoint to dict'): skip_key = any([pat in name for pat in ignore_name]) <IF_STMT> continue array = tf.train.load_variable(path, name) tf_weights[name] = array return tf_weights",if skip_key:
"def app(scope, receive, send): while True: message = await receive() if message['type'] == 'websocket.connect': await send({'type': 'websocket.accept'}) <IF_STMT> pass elif message['type'] == 'websocket.disconnect': break",elif message['type'] == 'websocket.receive':
def autoload(self): if self._app.config.THEME == 'auto': if sys.platform == 'darwin': <IF_STMT> theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: theme = self._app.config.THEME self.load_theme(theme),if get_osx_theme() == 1:
"def example_reading_spec(self): data_fields = {'targets': tf.VarLenFeature(tf.int64)} <IF_STMT> data_fields['inputs'] = tf.VarLenFeature(tf.int64) if self.packed_length: if self.has_inputs: data_fields['inputs_segmentation'] = tf.VarLenFeature(tf.int64) data_fields['inputs_position'] = tf.VarLenFeature(tf.int64) data_fields['targets_segmentation'] = tf.VarLenFeature(tf.int64) data_fields['targets_position'] = tf.VarLenFeature(tf.int64) data_items_to_decoders = None return (data_fields, data_items_to_decoders)",if self.has_inputs:
"def _prepare_travel_graph(self): for op in self.op_dict.values(): op.const = False if op.node.op in ['Const', 'Placeholder']: op.resolved = True <IF_STMT> op.const = True else: op.resolved = False",if op.node.op == 'Const':
"def get_filestream_file_items(self): data = {} fs_file_updates = self.get_filestream_file_updates() for k, v in six.iteritems(fs_file_updates): l = [] for d in v: offset = d.get('offset') content = d.get('content') assert offset is not None assert content is not None assert offset == 0 or offset == len(l), (k, v, l, d) <IF_STMT> l = [] l.extend(map(json.loads, content)) data[k] = l return data",if not offset:
"def _rewrite_exprs(self, table, what): from ibis.expr.analysis import substitute_parents what = util.promote_list(what) all_exprs = [] for expr in what: <IF_STMT> all_exprs.extend(expr.exprs()) else: bound_expr = ir.bind_expr(table, expr) all_exprs.append(bound_expr) return [substitute_parents(x, past_projection=False) for x in all_exprs]","if isinstance(expr, ir.ExprList):"
"def _group_by_commit_and_time(self, hits): result = {} for hit in hits: source_hit = hit['_source'] key = '%s_%s' % (source_hit['commit_info']['id'], source_hit['datetime']) benchmark = self._benchmark_from_es_record(source_hit) <IF_STMT> result[key]['benchmarks'].append(benchmark) else: run_info = self._run_info_from_es_record(source_hit) run_info['benchmarks'] = [benchmark] result[key] = run_info return result",if key in result:
"def _build_index(self): self._index = {} for start_char, sorted_offsets in self._offsets.items(): self._index[start_char] = {} for i, offset in enumerate(sorted_offsets.get_offsets()): identifier = sorted_offsets.get_identifier_by_offset(offset) <IF_STMT> self._index[start_char][identifier[0:self.index_depth]] = i",if identifier[0:self.index_depth] not in self._index[start_char]:
"def scan_resource_conf(self, conf): if 'properties' in conf: <IF_STMT> if 'exp' in conf['properties']['attributes']: if conf['properties']['attributes']['exp']: return CheckResult.PASSED return CheckResult.FAILED",if 'attributes' in conf['properties']:
"def _PatchArtifact(self, artifact: rdf_artifacts.Artifact) -> rdf_artifacts.Artifact: """"""Patches artifact to not contain byte-string source attributes."""""" patched = False for source in artifact.sources: attributes = source.attributes.ToDict() unicode_attributes = compatibility.UnicodeJson(attributes) <IF_STMT> source.attributes = unicode_attributes patched = True if patched: self.DeleteArtifact(str(artifact.name)) self.WriteArtifact(artifact) return artifact",if attributes != unicode_attributes:
"def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True) exit_code = c.wait() <IF_STMT> raise ClickException('%s: Editing failed!' % editor) except OSError as e: raise ClickException('%s: Editing failed: %s' % (editor, e))",if exit_code != 0:
"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): <IF_STMT> if glyph.contours[i].segments[j].type == 'line': controlPointIndices[index] = 1 index += s[1] return controlPointIndices",if j < segmentCount:
"def to_representation(self, value): old_social_string_fields = ['twitter', 'github', 'linkedIn'] request = self.context.get('request') show_old_format = request and is_deprecated(request.version, self.min_version) and (request.method == 'GET') if show_old_format: social = value.copy() for key in old_social_string_fields: <IF_STMT> social[key] = value[key][0] elif social.get(key) == []: social[key] = '' value = social return super(SocialField, self).to_representation(value)",if social.get(key):
"def iter_raw_frames(path, packet_sizes, ctx): with open(path, 'rb') as f: for i, size in enumerate(packet_sizes): packet = Packet(size) read_size = f.readinto(packet) assert size assert read_size == size if not read_size: break for frame in ctx.decode(packet): yield frame while True: try: frames = ctx.decode(None) except EOFError: break for frame in frames: yield frame <IF_STMT> break",if not frames:
"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) if fname or (pname and fname): continue if pname not in already_test and '/' not in pname: already_test.append(pname) <IF_STMT> shadow_pkgs.add(pname) return shadow_pkgs",if is_shadowing(pname):
"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): if isinstance(v, torch.Tensor): v = v.item() <IF_STMT> v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics","if isinstance(v, dict):"
"def insert_resets(f): newsync = dict() for k, v in f.sync.items(): <IF_STMT> newsync[k] = insert_reset(ResetSignal(k), v) else: newsync[k] = v f.sync = newsync",if f.clock_domains[k].rst is not None:
"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): if node is None: continue node_settings = node.get_addon(self.oauth_provider.short_name) <IF_STMT> continue if node_settings.external_account == external_account: yield node",if node_settings is None:
"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): if type(test.value) in self._const_types: <IF_STMT> continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",if not test.value:
"def flatten(self): result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) if len(data) < 100: self.limit = 0 <IF_STMT> data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",if self.reverse:
"def compute(self, x, y=None, targets=None): if targets is None: targets = self.out_params in_params = list(self.in_x) if len(in_params) == 1: args = [x] else: args = list(zip(*x)) if y is None: pipe = self.pipe else: pipe = self.train_pipe <IF_STMT> args.append(y) else: args += list(zip(*y)) in_params += self.in_y return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)",if len(self.in_y) == 1:
"def _import_top_module(self, name): for item in sys.path: <IF_STMT> module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) if module: return module return None","if isinstance(item, _StringType):"
"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: <IF_STMT> return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)",if k.lower() == ikey:
"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop('name')) action = 0 for key, val in props.get('options', {}).items(): if key == 'hooks': new_action = 0 for name, _val in val.items(): action = watcher.set_opt('hooks.%s' % name, _val) <IF_STMT> new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 return watcher.do_action(action)",if action == 1:
"def OnBodyClick(self, event=None): try: c = self.c p = c.currentPosition() <IF_STMT> self.OnActivateBody(event=event) g.doHook('bodyclick2', c=c, p=p, v=p, event=event) except: g.es_event_exception('bodyclick')","if not g.doHook('bodyclick1', c=c, p=p, v=p, event=event):"
"def _class_weights(spec: config.MetricsSpec) -> Optional[Dict[int, float]]: """"""Returns class weights associated with AggregationOptions at offset."""""" if spec.aggregate.HasField('top_k_list'): <IF_STMT> raise ValueError('class_weights are not supported when top_k_list used: spec={}'.format(spec)) return None return dict(spec.aggregate.class_weights) or None",if spec.aggregate.class_weights:
def _is_perf_file(file_path): f = get_file(file_path) for line in f: if line[0] == '#': continue r = event_regexp.search(line) <IF_STMT> f.close() return True f.close() return False,if r:
def _get_before_insertion_node(self): if self._nodes_stack.is_empty(): return None line = self._nodes_stack.parsed_until_line + 1 node = self._new_module.get_last_leaf() while True: parent = node.parent <IF_STMT> assert node.end_pos[0] <= line assert node.end_pos[1] == 0 or '\n' in self._prefix return node node = parent,"if parent.type in ('suite', 'file_input'):"
"def PyJsHoisted_parseClassRanges_(this, arguments, var=var): var = Scope({u'this': this, u'arguments': arguments}, var) var.registers([u'res']) pass if var.get(u'current')(Js(u']')): return Js([]) else: var.put(u'res', var.get(u'parseNonemptyClassRanges')()) <IF_STMT> var.get(u'bail')(Js(u'nonEmptyClassRanges')) return var.get(u'res')",if var.get(u'res').neg():
"def _recurse_children(self, offset): """"""Recurses thorugh the available children"""""" while offset < self.obj_offset + self.Length: item = obj.Object('VerStruct', offset=offset, vm=self.obj_vm, parent=self) <IF_STMT> raise StopIteration('Could not recover a key for a child at offset {0}'.format(item.obj_offset)) yield (item.get_key(), item.get_children()) offset = self.offset_pad(offset + item.Length) raise StopIteration('No children')",if item.Length < 1 or item.get_key() == None:
"def _adapt_types(self, descr): names = [] adapted_types = [] for col in descr: names.append(col[0]) impala_typename = col[1] typename = udf._impala_to_ibis_type[impala_typename.lower()] <IF_STMT> precision, scale = col[4:6] adapted_types.append(dt.Decimal(precision, scale)) else: adapted_types.append(typename) return (names, adapted_types)",if typename == 'decimal':
"def sniff(self, filename): try: <IF_STMT> with tarfile.open(filename, 'r') as temptar: for f in temptar: if not f.isfile(): continue if f.name.endswith('.fast5'): return True else: return False except Exception as e: log.warning('%s, sniff Exception: %s', self, e) return False",if filename and tarfile.is_tarfile(filename):
"def getValue(self): if getattr(self.object, 'type', '') != 'CURVE': return BezierSpline() evaluatedObject = getEvaluatedID(self.object) bSplines = evaluatedObject.data.splines if len(bSplines) > 0: spline = createSplineFromBlenderSpline(bSplines[0]) if spline is not None: <IF_STMT> spline.transform(evaluatedObject.matrix_world) return spline return BezierSpline()",if self.useWorldSpace:
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') <IF_STMT> text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text",if '>' in text:
"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: if hasattr(e, 'code'): <IF_STMT> return 3 if e.code == 501: return 1 raise return 2",if e.code == 405:
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) <IF_STMT> ctx.close_path()",elif code == Path.CLOSEPOLY:
"def called_by_shrinker(): frame = sys._getframe(0) while frame: fname = frame.f_globals.get('__file__', '') <IF_STMT> return True frame = frame.f_back return False",if os.path.basename(fname) == 'shrinker.py':
"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == 'append': <IF_STMT> sys.path.append(s) elif s != sys.path[0]: sys.path.insert(0, s)",if s not in sys.path:
"def get_instances(self, region: str, vpc: str): try: await self._cache_instances(region) return [instance for instance in self._instances_cache[region] <IF_STMT>] except Exception as e: print_exception(f'Failed to get RDS instances: {e}') return []",if instance['VpcId'] == vpc
def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: if page.relations.disambiguation_links_norm is not None: all_disambiguations.extend(page.relations.disambiguation_links_norm) <IF_STMT> all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),if page.relations.disambiguation_links is not None:
"def __str__(self, prefix='', printElemNumber=0): res = '' cnt = 0 for e in self.options_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'options%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 return res",if printElemNumber:
"def pre_save_task(self, task, credentials, verrors): if task['attributes']['encryption'] not in (None, '', 'AES256'): verrors.add('encryption', 'Encryption should be null or ""AES256""') if not credentials['attributes'].get('skip_region', False): <IF_STMT> response = await self.middleware.run_in_thread(self._get_client(credentials).get_bucket_location, Bucket=task['attributes']['bucket']) task['attributes']['region'] = response['LocationConstraint'] or 'us-east-1'","if not credentials['attributes'].get('region', '').strip():"
"def get_best_config_reward(self): """"""Returns the best configuration found so far, as well as the reward associated with this best config."""""" with self.LOCK: <IF_STMT> config_pkl = max(self._results, key=self._results.get) return (pickle.loads(config_pkl), self._results[config_pkl]) else: return (dict(), self._reward_while_pending())",if self._results:
"def parse_setup_cfg(self): if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: <IF_STMT> contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) if not parsed: return {} return parsed return {}",if six.PY2:
"def readall(read_fn, sz): buff = b'' have = 0 while have < sz: chunk = (yield from read_fn(sz - have)) have += len(chunk) buff += chunk <IF_STMT> raise TTransportException(TTransportException.END_OF_FILE, 'End of file reading from transport') return buff",if len(chunk) == 0:
"def _get_use_previous(f): if isinstance(f, AggregationFeature) and f.use_previous is not None: <IF_STMT> return ('', -1) else: unit = list(f.use_previous.times.keys())[0] value = f.use_previous.times[unit] return (unit, value) else: return ('', -1)",if len(f.use_previous.times.keys()) > 1:
"def istrue(self): try: return self._istrue() except Exception: self.exc = sys.exc_info() <IF_STMT> msg = [' ' * (self.exc[1].offset + 4) + '^'] msg.append('SyntaxError: invalid syntax') else: msg = traceback.format_exception_only(*self.exc[:2]) pytest.fail('Error evaluating %r expression\n%s\n%s' % (self.name, self.expr, '\n'.join(msg)), pytrace=False)","if isinstance(self.exc[1], SyntaxError):"
"def wait_for_crm_operation(operation, crm): """"""Poll for cloud resource manager operation until finished."""""" logger.info('wait_for_crm_operation: Waiting for operation {} to finish...'.format(operation)) for _ in range(MAX_POLLS): result = crm.operations().get(name=operation['name']).execute() <IF_STMT> raise Exception(result['error']) if 'done' in result and result['done']: logger.info('wait_for_crm_operation: Operation done.') break time.sleep(POLL_INTERVAL) return result",if 'error' in result:
"def cb_blob_detail_from_elem_and_buf(self, elem, buf): if elem.get('lang') != buf.lang: return '%s Code in %s' % (elem.get('lang'), buf.path) else: dir, base = os.path.split(buf.path) <IF_STMT> return '%s (%s)' % (base, dir) else: return base",if dir:
"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) <IF_STMT> if self.isfile(path): raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise",if code == '550':
"def p_clause(self, node, position): if isinstance(node, Graph): self.subjectDone(node) <IF_STMT> self.write(' ') self.write('{') self.depth += 1 serializer = N3Serializer(node, parent=self) serializer.serialize(self.stream) self.depth -= 1 self.write(self.indent() + '}') return True else: return False",if position is OBJECT:
"def get_default_shell_info(shell_name=None, settings=None): if not shell_name: settings = settings or load_settings(lazy=True) shell_name = settings.get('shell') if shell_name: return (shell_name, None) shell_path = os.environ.get('SHELL') <IF_STMT> shell_name = basepath(shell_path) else: shell_name = DEFAULT_SHELL return (shell_name, shell_path) return (shell_name, None)",if shell_path:
"def GetCategory(self, pidls): ret = [] for pidl in pidls: val = self.sf.GetDetailsEx(pidl, PKEY_Sample_AreaSize) val = int(val) if val < 255 // 3: cid = IDS_SMALL <IF_STMT> cid = IDS_MEDIUM else: cid = IDS_LARGE ret.append(cid) return ret",elif val < 2 * 255 // 3:
"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] <IF_STMT> typ = item[3] val = item[3] yield Token(typ, val)",elif item[3]:
"def add_package_declarations(generated_root_path): file_names = os.listdir(generated_root_path) for file_name in file_names: <IF_STMT> continue full_name = os.path.join(generated_root_path, file_name) add_package(full_name)",if not file_name.endswith('.java'):
"def _call_with_retry(out, retry, retry_wait, method, *args, **kwargs): for counter in range(retry + 1): try: return method(*args, **kwargs) except (NotFoundException, ForbiddenException, AuthenticationException, RequestErrorException): raise except ConanException as exc: <IF_STMT> raise else: if out: out.error(exc) out.info('Waiting %d seconds to retry...' % retry_wait) time.sleep(retry_wait)",if counter == retry:
"def to_wburl_str(url, type=BaseWbUrl.LATEST_REPLAY, mod='', timestamp='', end_timestamp=''): if WbUrl.is_query_type(type): tsmod = '' <IF_STMT> tsmod += mod + '/' tsmod += timestamp tsmod += '*' tsmod += end_timestamp tsmod += '/' + url if type == BaseWbUrl.URL_QUERY: tsmod += '*' return tsmod else: tsmod = timestamp + mod if len(tsmod) > 0: return tsmod + '/' + url else: return url",if mod:
"def _configured_ploidy(items): ploidies = collections.defaultdict(set) for data in items: ploidy = dd.get_ploidy(data) <IF_STMT> for k, v in ploidy.items(): ploidies[k].add(v) else: ploidies['default'].add(ploidy) out = {} for k, vs in ploidies.items(): assert len(vs) == 1, 'Multiple ploidies set for group calling: %s %s' % (k, list(vs)) out[k] = vs.pop() return out","if isinstance(ploidy, dict):"
"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] <IF_STMT> if self.isRoomSame(user.room): hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage('left-notification').format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",if user.room:
"def _thd_cleanup_instance(self): container_name = self.getContainerName() instances = self.client.containers(all=1, filters=dict(name=container_name)) for instance in instances: <IF_STMT> continue try: self.client.remove_container(instance['Id'], v=True, force=True) except NotFound: pass except docker.errors.APIError as e: if 'Conflict operation on container' not in str(e): raise",if ''.join(instance['Names']).strip('/') != container_name:
"def handle_ctcp(self, conn, evt): args = evt.arguments() source = evt.source().split('!')[0] if args: if args[0] == 'VERSION': conn.ctcp_reply(source, 'VERSION ' + BOT_VERSION) <IF_STMT> conn.ctcp_reply(source, 'PING') elif args[0] == 'CLIENTINFO': conn.ctcp_reply(source, 'CLIENTINFO PING VERSION CLIENTINFO')",elif args[0] == 'PING':
"def new_func(self, *args, **kwargs): obj = self.obj_ref() attr = self.attr if obj is not None: args = tuple((TrackedValue.make(obj, attr, arg) for arg in args)) <IF_STMT> kwargs = {key: TrackedValue.make(obj, attr, value) for key, value in iteritems(kwargs)} result = func(self, *args, **kwargs) self._changed_() return result",if kwargs:
"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): name = target.id <IF_STMT> doc = find_doc_for(target, body_lines) if doc is not None: variables[name] = doc elif isinstance(target, ast.Tuple): for e in target.elts: add_doc(e, variables, body_lines)",if name not in variables:
"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: if tp == 'write': out.write(msg) elif tp == 'flush': out.flush() <IF_STMT> out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",elif tp == 'write_flush':
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) <IF_STMT> continue if os.path.islink(p): continue if os.path.isdir(p): res += get_dir(p) else: res.append(p) return res",if skip_file(fname):
"def _list_outputs(self): outputs = super(VolSymm, self)._list_outputs() if os.path.exists(outputs['trans_file']): <IF_STMT> outputs['output_grid'] = re.sub('.(nlxfm|xfm)$', '_grid_0.mnc', outputs['trans_file']) return outputs","if 'grid' in open(outputs['trans_file'], 'r').read():"
"def _set_texture(self, texture): if texture.id is not self._texture.id: self._group = SpriteGroup(texture, self._group.blend_src, self._group.blend_dest, self._group.parent) <IF_STMT> self._vertex_list.tex_coords[:] = texture.tex_coords else: self._vertex_list.delete() self._texture = texture self._create_vertex_list() else: self._vertex_list.tex_coords[:] = texture.tex_coords self._texture = texture",if self._batch is None:
"def got_result(result): deployment = self.persistence_service.get() for node in deployment.nodes: <IF_STMT> dataset_ids = [(m.dataset.deleted, m.dataset.dataset_id) for m in node.manifestations.values()] self.assertIn((True, expected_dataset_id), dataset_ids) break else: self.fail('Node not found. {}'.format(node.uuid))","if same_node(node, origin):"
"def check_result(result, func, arguments): if check_warning(result) and result.value != ReturnCode.WARN_NODATA: log.warning(UcanWarning(result, func, arguments)) elif check_error(result): <IF_STMT> raise UcanCmdError(result, func, arguments) else: raise UcanError(result, func, arguments) return result",if check_error_cmd(result):
"def _compress_and_sort_bdg_files(out_dir, data): for fn in glob.glob(os.path.join(out_dir, '*bdg')): out_file = fn + '.gz' <IF_STMT> continue bedtools = config_utils.get_program('bedtools', data) with file_transaction(out_file) as tx_out_file: cmd = f'sort -k1,1 -k2,2n {fn} | bgzip -c > {tx_out_file}' message = f'Compressing and sorting {fn}.' do.run(cmd, message)",if utils.file_exists(out_file):
"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: if ha_tools_debug: print('killing %s' % member) proc = hosts[member]['proc'] <IF_STMT> os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print('%s already dead?' % member)","if sys.platform in ('win32', 'cygwin'):"
"def get_top_level_stats(self): for func, (cc, nc, tt, ct, callers) in self.stats.items(): self.total_calls += nc self.prim_calls += cc self.total_tt += tt <IF_STMT> self.top_level[func] = None if len(func_std_string(func)) > self.max_name_len: self.max_name_len = len(func_std_string(func))","if ('jprofile', 0, 'profiler') in callers:"
"def __str__(self): """"""Only keeps the True values."""""" result = ['SlicingSpec('] if self.entire_dataset: result.append(' Entire dataset,') if self.by_class: if isinstance(self.by_class, Iterable): result.append(' Into classes %s,' % self.by_class) <IF_STMT> result.append(' Up to class %d,' % self.by_class) else: result.append(' By classes,') if self.by_percentiles: result.append(' By percentiles,') if self.by_classification_correctness: result.append(' By classification correctness,') result.append(')') return '\n'.join(result)","elif isinstance(self.by_class, int):"
"def save_params(self): if self._save_controller: if not os.path.exists(self._save_controller): os.makedirs(self._save_controller) output_dir = self._save_controller else: <IF_STMT> os.makedirs('./.rlnas_controller') output_dir = './.rlnas_controller' with open(os.path.join(output_dir, 'rlnas.params'), 'wb') as f: pickle.dump(self._params_dict, f) _logger.debug('Save params done')",if not os.path.exists('./.rlnas_controller'):
"def unexport(self, pin): with self._lock: self._pin_refs[pin] -= 1 <IF_STMT> with io.open(self.path('unexport'), 'wb') as f: f.write(str(pin).encode('ascii'))",if self._pin_refs[pin] == 0:
"def emit(self, type, info=None): ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: if type in self.__proxy_properties__: self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev]) <IF_STMT> self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev])",elif type in self.__event_types_at_proxy:
"def __call__(self, params): all_errs = {} for handler in self.handlers: out_headers, res, errs = handler(params) all_errs.update(errs) <IF_STMT> return (out_headers, res, all_errs) return (None, None, all_errs)",if res is not None:
def await_test_end(self): iterations = 0 while True: <IF_STMT> self.log.debug('Await: iteration limit reached') return status = self.master.get_status() if status.get('status') == 'ENDED': return iterations += 1 time.sleep(1.0),if iterations > 100:
"def _load(self, path: str): ds = DataSet() with open(path, 'r', encoding='utf-8') as f: for line in f: line = line.strip() <IF_STMT> parts = line.split('\t') raw_words1 = parts[1] raw_words2 = parts[2] target = parts[0] if raw_words1 and raw_words2 and target: ds.append(Instance(raw_words1=raw_words1, raw_words2=raw_words2, target=target)) return ds",if line:
"def avatar_delete(event_id, speaker_id): if request.method == 'DELETE': speaker = DataGetter.get_speakers(event_id).filter_by(user_id=login.current_user.id, id=speaker_id).first() <IF_STMT> speaker.photo = '' speaker.small = '' speaker.thumbnail = '' speaker.icon = '' save_to_db(speaker) return jsonify({'status': 'ok'}) else: abort(403)",if speaker:
"def getline(filename, lineno, *args, **kwargs): line = py2exe_getline(filename, lineno, *args, **kwargs) if not line: try: with open(filename, 'rb') as f: for i, line in enumerate(f): line = line.decode('utf-8') <IF_STMT> break else: line = '' except (IOError, OSError): line = '' return line",if lineno == i + 1:
"def write(self, data): if not isinstance(data, (bytes, bytearray, memoryview)): raise TypeError('data argument must be byte-ish (%r)', type(data)) if not data: return if self._conn_lost: <IF_STMT> logger.warning('socket.send() raised exception.') self._conn_lost += 1 return if not self._buffer: self._loop.add_writer(self._sock_fd, self._write_ready) self._buffer.extend(data) self._maybe_pause_protocol()",if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:
"def _get_x_for_y(self, xValue, x, y): if not self.xmlMap: return 0 x_value = str(xValue) for anime in self.xmlMap.findall('anime'): try: <IF_STMT> return int(anime.get(y, 0)) except ValueError as e: continue return 0","if anime.get(x, False) == x_value:"
"def _RewriteModinfo(self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None): new_modinfo = '' for line in modinfo.split('\x00'): if not line: continue if to_remove and line.split('=')[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split('=')[0]) <IF_STMT> line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + '\x00' return new_modinfo",if line.startswith('vermagic'):
"def _score(self, X, y): for col in self.cols: X[col] = X[col].map(self.mapping[col]) <IF_STMT> random_state_generator = check_random_state(self.random_state) X[col] = X[col] * random_state_generator.normal(1.0, self.sigma, X[col].shape[0]) return X",if self.randomized and y is not None:
"def onMouseWheel(self, event): if self.selectedHuman.isVisible(): zoomOut = event.wheelDelta > 0 <IF_STMT> zoomOut = not zoomOut if event.x is not None: self.modelCamera.mousePickHumanCenter(event.x, event.y) if zoomOut: self.zoomOut() else: self.zoomIn()",if self.getSetting('invertMouseWheel'):
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == 'i386': reg = emu.getRegister(envi.archs.i386.REG_EDI) <IF_STMT> reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",elif self.arch == 'amd64':
"def callback(actions, form, tablename=None): if actions: if tablename and isinstance(actions, dict): actions = actions.get(tablename, []) <IF_STMT> actions = [actions] [action(form) for action in actions]","if not isinstance(actions, (list, tuple)):"
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): if bigger_than_3_only and less_than_7_only and (i == 4): continue if bigger_than_3_only and i <= 3: continue if less_than_7_only and i >= 7: continue <IF_STMT> continue result.append(i) return result",if even_only and i % 2 != 0:
"def set_trial_values(self, trial_id: int, values: Sequence[float]) -> None: with self._lock: cached_trial = self._get_cached_trial(trial_id) <IF_STMT> self._check_trial_is_updatable(cached_trial) updates = self._get_updates(trial_id) cached_trial.values = values updates.values = values return self._backend._update_trial(trial_id, values=values)",if cached_trial is not None:
"def _get_label_format(self, workunit): for label, label_format in self.LABEL_FORMATTING.items(): if workunit.has_label(label): return label_format if workunit.parent: label_format = self._get_label_format(workunit.parent) if label_format == LabelFormat.CHILD_DOT: return LabelFormat.DOT <IF_STMT> return LabelFormat.SUPPRESS return LabelFormat.FULL",if label_format == LabelFormat.CHILD_SUPPRESS:
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) <IF_STMT> return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):
"def _manage_torrent_cache(self): """"""Carry tracker/peer/file lists over to new torrent list"""""" for torrent in self._torrent_cache: new_torrent = rtorrentlib.common.find_torrent(torrent.info_hash, self.torrents) <IF_STMT> new_torrent.files = torrent.files new_torrent.peers = torrent.peers new_torrent.trackers = torrent.trackers self._torrent_cache = self.torrents",if new_torrent is not None:
"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) if target: <IF_STMT> target = _load_regions(target) else: target = [target] return target","if isinstance(target, six.string_types) and os.path.isfile(target):"
def _get_stdout(self): while True: BUFFER_SIZE = 1000 stdout_buffer = self.kernel.process.GetSTDOUT(BUFFER_SIZE) <IF_STMT> break yield stdout_buffer,if len(stdout_buffer) == 0:
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: <IF_STMT> ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if key == qkey:
"def test_expect_setecho_off(self): """"""This tests that echo may be toggled off."""""" p = pexpect.spawn('cat', echo=True, timeout=5) try: self._expect_echo_toggle(p) except IOError: <IF_STMT> if hasattr(unittest, 'SkipTest'): raise unittest.SkipTest('Not supported on this platform.') return 'skip' raise",if sys.platform.lower().startswith('sunos'):
"def _resolve_relative_config(dir, config): icon = config.get('icon') if icon: <IF_STMT> icon = File(icon) else: icon = dir.resolve_file(icon) document_root = config.get('document_root') if document_root: if zim.fs.isabs(document_root) or not dir: document_root = Dir(document_root) else: document_root = dir.resolve_dir(document_root) return (icon, document_root)",if zim.fs.isabs(icon) or not dir:
"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): <IF_STMT> if ent_id in res: pass else: res.append(ent_id) return res",if descriptor in ent_desc:
"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) if timeout >= 0: deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) <IF_STMT> n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",if timeout < 0:
"def banned(): if request.endpoint == 'views.themes': return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() if user and user.banned: return (render_template('errors/403.html', error='You have been banned from this CTF'), 403) <IF_STMT> return (render_template('errors/403.html', error='Your team has been banned from this CTF'), 403)",if team and team.banned:
"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) if msg: self.on_message(msg) return True self.close() except socket.error as err: <IF_STMT> pass else: self.on_error(err) return False","if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK):"
"def update_topic_attr_as_not(modeladmin, request, queryset, attr): for topic in queryset: if attr == 'sticky': topic.sticky = not topic.sticky elif attr == 'closed': topic.closed = not topic.closed <IF_STMT> topic.hidden = not topic.hidden topic.save()",elif attr == 'hidden':
"def Startprobe(self, q): while not self.finished: try: sniff(iface=self.interface, count=10, prn=lambda x: q.put(x)) except: pass <IF_STMT> break",if self.finished:
"def _maybe_female(self, path_elements, female, strict): if female: <IF_STMT> elements = path_elements + ['female'] try: return self._get_file(elements, '.png', strict=strict) except ValueError: if strict: raise elif strict: raise ValueError('Pokemon %s has no gender differences' % self.species_id) return self._get_file(path_elements, '.png', strict=strict)",if self.has_gender_differences:
def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): <IF_STMT> ind += 1 else: if start < ind: ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if ':' in line and len(line) > 0: lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if ind < len(strings) and strings[ind].startswith(' '):
"def _send_with_auth(self, req_kwargs, desired_auth, rsession): if desired_auth.oauth: <IF_STMT> self._oauth_creds.refresh(httplib2.Http()) req_kwargs['headers'] = req_kwargs.get('headers', {}) req_kwargs['headers']['Authorization'] = 'Bearer ' + self._oauth_creds.access_token return rsession.request(**req_kwargs)",if self._oauth_creds.access_token_expired:
"def parse_search_response(json_data): """"""Construct response for any input"""""" if json_data is None: return {'error': 'Error parsing empty search engine response'} try: return json.loads(json_data) except json.JSONDecodeError: logger.exception('Error parsing search engine response') m = re_pre.search(json_data) <IF_STMT> return {'error': 'Error parsing search engine response'} error = web.htmlunquote(m.group(1)) solr_error = 'org.apache.lucene.queryParser.ParseException: ' if error.startswith(solr_error): error = error[len(solr_error):] return {'error': error}",if m is None:
"def wrapper(*args, **kws): missing = [] saved = getattr(warnings, '__warningregistry__', missing).copy() try: return func(*args, **kws) finally: <IF_STMT> try: del warnings.__warningregistry__ except AttributeError: pass else: warnings.__warningregistry__ = saved",if saved is missing:
"def parse_expression(self): """"""Return string containing command to run."""""" expression_el = self.root.find('expression') if expression_el is not None: expression_type = expression_el.get('type') <IF_STMT> raise Exception('Unknown expression type [%s] encountered' % expression_type) return expression_el.text return None",if expression_type != 'ecma5.1':
"def test_geocode(): count = 0 found = False for tweet in T.search(None, geocode='40.7484,-73.9857,1mi'): <IF_STMT> found = True break if count > 500: break count += 1 assert found",if (tweet['place'] or {}).get('name') == 'Manhattan':
"def __init__(self, name: Optional[str]=None, order: int=0): if name is None: <IF_STMT> name = 'std_dev' elif order == 1: name = 'sample_std_dev' else: name = f'std_dev{order})' super().__init__(name=name, order=order) self.order = order",if order == 0:
"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() <IF_STMT> return -1 elif a == b: return 0 else: raise TypeError('expected date or datetime object') return 1",if a < b:
def run(self): tid = self.ident try: with self._lock: _GUIS[tid] = self self._state(True) self.new_mail_notifications(summarize=True) loop_count = 0 while self._sock: loop_count += 1 self._select_sleep(1) self.change_state() <IF_STMT> self.new_mail_notifications() finally: del _GUIS[tid],if loop_count % 5 == 0:
"def __cache_dimension_masks(self, *args): if len(self.masks) == 0: for m1 in args: batch_size, emb_dim, h, w = m1.size() <IF_STMT> mask = self.feat_size_w_mask(h, m1) self.masks[h] = mask",if h not in self.masks:
"def __call__(self, *flattened_representation): unflattened_representation = [] for index, subtree in self.children: <IF_STMT> unflattened_representation.append(flattened_representation[index]) else: sub_representation = flattened_representation[index] unflattened_representation.append(subtree(*sub_representation)) return self._cls(*unflattened_representation, **self._kwargs)",if subtree is None:
"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] if y == 0: y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = (x, y, z) <IF_STMT> d.dismiss('Goto')",if event.num_clicks == 2:
"def get_mapped_input_keysequences(self, mode='global', prefix=u''): globalmaps, modemaps = self.get_keybindings(mode) candidates = list(globalmaps.keys()) + list(modemaps.keys()) if prefix is not None: prefixes = prefix + ' ' cand = [c for c in candidates if c.startswith(prefixes)] <IF_STMT> candidates = cand + [prefix] else: candidates = cand return candidates",if prefix in candidates:
"def _set_length(self, length): with self._cond: self._length = length <IF_STMT> self._ready = True self._cond.notify() del self._cache[self._job]",if self._index == self._length:
"def _pct_encoded_replace_unreserved(mo): try: i = int(mo.group(1), 16) <IF_STMT> return chr(i) else: return mo.group().upper() except ValueError: return mo.group()",if _unreserved[i]:
"def is_open(self): if self.signup_code: return True elif self.signup_code_present: <IF_STMT> messages.add_message(self.request, self.messages['invalid_signup_code']['level'], self.messages['invalid_signup_code']['text'].format(**{'code': self.get_code()})) return settings.ACCOUNT_OPEN_SIGNUP",if self.messages.get('invalid_signup_code'):
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: <IF_STMT> field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",if member[0] == key:
"def move_sender_strings_to_sender_model(apps, schema_editor): sender_model = apps.get_model('documents', 'Sender') document_model = apps.get_model('documents', 'Document') for document in document_model.objects.all(): <IF_STMT> DOCUMENT_SENDER_MAP[document.pk], created = sender_model.objects.get_or_create(name=document.sender, defaults={'slug': slugify(document.sender)})",if document.sender:
"def compute_output_shape(self, input_shape): if None not in input_shape[1:]: <IF_STMT> total = np.prod(input_shape[2:4]) * self.num_anchors else: total = np.prod(input_shape[1:3]) * self.num_anchors return (input_shape[0], total, 4) else: return (input_shape[0], None, 4)",if keras.backend.image_data_format() == 'channels_first':
"def decompress(self, value): if value: if type(value) == PhoneNumber: <IF_STMT> return ['+%d' % value.country_code, national_significant_number(value)] else: return value.split('.') return [None, '']",if value.country_code and value.national_number:
"def ignore(self, other): if isinstance(other, Suppress): <IF_STMT> super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if other not in self.ignoreExprs:
"def mkdir(self, mode=511, parents=False, exist_ok=False): if self._closed: self._raise_closed() if not parents: try: self._accessor.mkdir(self, mode) except FileExistsError: <IF_STMT> raise else: try: self._accessor.mkdir(self, mode) except FileExistsError: if not exist_ok or not self.is_dir(): raise except OSError as e: if e.errno != ENOENT: raise self.parent.mkdir(parents=True) self._accessor.mkdir(self, mode)",if not exist_ok or not self.is_dir():
"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == '|': m -= 1 n -= 1 mask[m] = 1 elif dirs[m, n] == '^': m -= 1 <IF_STMT> n -= 1 else: raise UnboundLocalError('Illegal move') return mask","elif dirs[m, n] == '<':"
"def clean(self, *args, **kwargs): data = super().clean(*args, **kwargs) if isinstance(data, File): filename = data.name ext = os.path.splitext(filename)[1] ext = ext.lower() <IF_STMT> raise forms.ValidationError(_('Filetype not allowed!')) return data",if ext not in self.ext_whitelist:
"def get_doc_object(obj, what=None): if what is None: if inspect.isclass(obj): what = 'class' <IF_STMT> what = 'module' elif callable(obj): what = 'function' else: what = 'object' if what == 'class': return SphinxClassDoc(obj, '', func_doc=SphinxFunctionDoc) elif what in ('function', 'method'): return SphinxFunctionDoc(obj, '') else: return SphinxDocString(pydoc.getdoc(obj))",elif inspect.ismodule(obj):
"def apply_pssm(val): if val is not None: val_c = PSSM_VALUES.get(val, None) <IF_STMT> assert isinstance(val, tuple(PSSM_VALUES.values())), ""'store_as' should be one of: %r or an instance of %r not %r"" % (tuple(PSSM_VALUES.keys()), tuple(PSSM_VALUES.values()), val) return val return val_c()",if val_c is None:
"def read_postmaster_opts(self): """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file"""""" result = {} try: with open(os.path.join(self._postgresql.data_dir, 'postmaster.opts')) as f: data = f.read() for opt in data.split('"" ""'): <IF_STMT> name, val = opt.split('=', 1) result[name.strip('-')] = val.rstrip('""\n') except IOError: logger.exception('Error when reading postmaster.opts') return result",if '=' in opt and opt.startswith('--'):
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('F5-TrafficShield', headers.get(HTTP_HEADER.SERVER, ''), re.I) is not None retval |= re.search('\\AASINFO=', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None <IF_STMT> break return retval",if retval:
"def on_task_start(self, task, config): for item in config: for plugin_name, plugin_config in item.items(): try: thelist = plugin.get(plugin_name, self).get_list(plugin_config) except AttributeError: raise PluginError('Plugin %s does not support list interface' % plugin_name) <IF_STMT> raise plugin.PluginError(thelist.immutable)",if thelist.immutable:
"def nq(t): p = t[0] if t and t[0] in '-+' else '' t = t[len(p):] if t.startswith('tag:') or t.startswith('in:'): try: raw_tag = session.config.get_tag(t.split(':')[1]) <IF_STMT> t = 'in:%s' % raw_tag.slug except (IndexError, KeyError, TypeError): pass return p + t",if raw_tag and raw_tag.hasattr(slug):
"def _recur_strip(s): if is_str(s): <IF_STMT> return ' '.join(s.strip().split()) else: return ' '.join(s.strip().split()).replace(bos_token + ' ', '') else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s)",if bos_token == '':
"def __delitem__(self, key): """"""Deleting tag[key] deletes all 'key' attributes for the tag."""""" for item in self.attrs: <IF_STMT> self.attrs.remove(item) self._getAttrMap() if self.attrMap.has_key(key): del self.attrMap[key]",if item[0] == key:
"def comment_import_help(init_file, out_file): f_out = open(out_file, 'w') output = '' updated = False with open(init_file, 'r') as f_in: for line in f_in: <IF_STMT> updated = True line = '# ' + line output += line f_out.write(output) f_out.close() return updated",if 'import' in line and '_help' in line and (not updated):
def prepare_text(lines): out = [] for s in lines.split('|'): s = s.strip() <IF_STMT> s = '{\\i1}%s{\\i0}' % s[1:].strip() out.append(s) return '\\N'.join(out),if s.startswith('/'):
def sqlctx(sc): pytest.importorskip('pyspark') from odo.backends.sparksql import HiveContext try: yield HiveContext(sc) finally: dbpath = 'metastore_db' logpath = 'derby.log' <IF_STMT> assert os.path.isdir(dbpath) shutil.rmtree(dbpath) if os.path.exists(logpath): assert os.path.isfile(logpath) os.remove(logpath),if os.path.exists(dbpath):
"def _user2dict(self, uid): usdict = None if uid in self.users: usdict = self.users[uid] <IF_STMT> infos = self.users_info[uid] for attr in infos: usdict[attr['attr_type']] = attr['attr_data'] usdict['uid'] = uid return usdict",if uid in self.users_info:
"def _validate_options(self): for option in self.options: <IF_STMT> if self.options.required[option] is True and (not self.options[option]): if option == Constants.PASSWORD_CLEAR: option = 'password'.upper() raise FrameworkException(""Value required for the '%s' option."" % option.upper()) return","if not type(self.options[option]) in [bool, int]:"
"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) <IF_STMT> mkdir(dest.parent) if dest.exists(): logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() if src.exists(): shutil.copy(src, dest)",if not dest.parent.is_dir():
"def truncate_seq_pair(tokens_a, tokens_b, max_length): """"""Truncates a sequence pair in place to the maximum length."""""" while True: total_length = len(tokens_a) + len(tokens_b) <IF_STMT> break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop()",if total_length <= max_length:
"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel['channel'] defaults = {'currency': channel.currency_code} if 'discount_value' in add_channel.keys(): defaults['discount_value'] = add_channel.get('discount_value') <IF_STMT> defaults['min_spent_amount'] = add_channel.get('min_amount_spent', None) models.VoucherChannelListing.objects.update_or_create(voucher=voucher, channel=channel, defaults=defaults)",if 'min_amount_spent' in add_channel.keys():
"def services(self, id=None, name=None): for service_dict in self.service_ls(id=id, name=name): service_id = service_dict['ID'] service_name = service_dict['NAME'] <IF_STMT> continue task_list = self.service_ps(service_id) yield DockerService.from_cli(self, service_dict, task_list)",if not service_name.startswith(self._name_prefix):
"def lll(dirname): for name in os.listdir(dirname): <IF_STMT> full = os.path.join(dirname, name) if os.path.islink(full): print(name, '->', os.readlink(full))","if name not in (os.curdir, os.pardir):"
"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote('extracted from web2py', 'developer') for source_str in mydict.keys(): target_str = mydict[source_str] <IF_STMT> target_str = u'' elif target_str.startswith(u'*** '): target_str = u'' pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",if target_str == source_str:
"def __init__(self, **kwargs): for k, v in kwargs.items(): setattr(self, k, v) self.attempted_charsets = set() request = cherrypy.serving.request if request.handler is not None: <IF_STMT> cherrypy.log('Replacing request.handler', 'TOOLS.ENCODE') self.oldhandler = request.handler request.handler = self",if self.debug:
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith('>>%s' % section_name): in_section = True <IF_STMT> if line.startswith('>>END'): break out.append(line.rstrip('\r\n')) return out",elif in_section:
"def bit_length(n): try: return n.bit_length() except AttributeError: norm = deflate_long(n, False) hbyte = byte_ord(norm[0]) <IF_STMT> return 1 bitlen = len(norm) * 8 while not hbyte & 128: hbyte <<= 1 bitlen -= 1 return bitlen",if hbyte == 0:
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward <IF_STMT> break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if done:
"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) <IF_STMT> del translation[ind] elif action == 'replacement': ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",if action == 'deletion':
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) <IF_STMT> sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if ki != 0:
def get_dirlist(_rootdir): dirlist = [] with os.scandir(_rootdir) as rit: for entry in rit: <IF_STMT> dirlist.append(entry.path) dirlist += get_dirlist(entry.path) return dirlist,if not entry.name.startswith('.') and entry.is_dir():
"def __init__(self, fixed: MQTTFixedHeader=None, variable_header: PublishVariableHeader=None, payload=None): if fixed is None: header = MQTTFixedHeader(PUBLISH, 0) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PublishPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = payload",if fixed.packet_type is not PUBLISH:
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) <IF_STMT> continue if fname == 'PureMVC_Python_1_0': continue if fname[-4:] == '.pyc': continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",if fname == 'output':
"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = (0, 0) for ts in self.time_steps: <IF_STMT> raw_rewards += ts.raw_reward if ts.processed_reward is not None: processed_rewards += ts.processed_reward return (raw_rewards, processed_rewards)",if ts.raw_reward is not None:
"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() <IF_STMT> args.extend(self._split_option(line)) elif line and (not line.startswith('#')): args.append(line) return args",if line.startswith('-'):
"def __on_change_button_clicked(self, widget=None): """"""compute all primary objects and toggle the 'Change' attribute"""""" self.change_status = not self.change_status for prim_obj, tmp in self.xobjects: obj_change = self.top.get_object('%s_change' % prim_obj) <IF_STMT> continue self.change_entries[prim_obj].set_val(self.change_status) obj_change.set_active(self.change_status)",if not obj_change.get_sensitive():
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue <IF_STMT> continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield (cog_name, cog_id)","if not isinstance(data, dict):"
"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): <IF_STMT> raise BlockError(_error.InstanceError % (self.name, inst.callername))",if not inst.modctxt:
def _is_xml(accepts): if accepts.startswith(b'application/'): has_xml = accepts.find(b'xml') if has_xml > 0: semicolon = accepts.find(b';') <IF_STMT> return True return False,if semicolon < 0 or has_xml < semicolon:
"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): <IF_STMT> return target else: mapper = _mapper_or_none(target) if mapper is not None: return mapper else: return _MapperEventsHold(target) else: return target","if issubclass(target, mapperlib.Mapper):"
"def _get_font_afm(self, prop): key = hash(prop) font = self.afmfontd.get(key) <IF_STMT> fname = findfont(prop, fontext='afm') font = self.afmfontd.get(fname) if font is None: font = AFM(file(findfont(prop, fontext='afm'))) self.afmfontd[fname] = font self.afmfontd[key] = font return font",if font is None:
"def __call__(self, groupby): normalize_reduction_funcs(self, ndim=groupby.ndim) df = groupby while df.op.output_types[0] not in (OutputType.dataframe, OutputType.series): df = df.inputs[0] if self.raw_func == 'size': self.output_types = [OutputType.series] else: self.output_types = [OutputType.dataframe] <IF_STMT> else [OutputType.series] if self.output_types[0] == OutputType.dataframe: return self._call_dataframe(groupby, df) else: return self._call_series(groupby, df)",if groupby.op.output_types[0] == OutputType.dataframe_groupby
"def save(self): if self.preferences.get(ENCRYPT_ON_DISK, False): <IF_STMT> return self.storage.write(self.to_dict(encrypt_password=self.encryption_password)) elif not self.is_locked: log.warning('Disk encryption requested but no password available for encryption. Resetting encryption preferences and saving wallet in an unencrypted state.') self.preferences[ENCRYPT_ON_DISK] = False return self.storage.write(self.to_dict())",if self.encryption_password is not None:
"def isValidDateString(config_param_name, value, valid_value): try: if value == 'DD-MM-YYYY': return value day, month, year = value.split('-') if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) <IF_STMT> raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if int(month) < 1 or int(month) > 12:
"def _capture(self, call_name, data=None, **kwargs): if data is None: data = self.get_default_context() else: default_context = self.get_default_context() <IF_STMT> default_context.update(data) else: default_context['extra']['extra_data'] = data data = default_context client = self.get_sentry_client() return getattr(client, call_name)(data=data, **kwargs)","if isinstance(data, dict):"
"def check(input, expected_output=None, expected_ffi_error=False): import _cffi_backend ffi = _cffi_backend.FFI() if not expected_ffi_error: ct = ffi.typeof(input) assert isinstance(ct, ffi.CType) assert ct.cname == (expected_output or input) else: e = py.test.raises(ffi.error, ffi.typeof, input) <IF_STMT> assert str(e.value) == expected_ffi_error","if isinstance(expected_ffi_error, str):"
"def run(self): """"""Process queries from task queue, stop if processor is None."""""" while True: try: processor, iprot, oprot, otrans, callback = self.queue.get() <IF_STMT> break processor.process(iprot, oprot) callback(True, otrans.getvalue()) except Exception: logging.exception('Exception while processing request') callback(False, '')",if processor is None:
"def search(self, query): query = query.strip().lower() results = [] for provider in SidebarItemProvider.all(self.context): for item in provider.provide(): if 'url' in item: search_source = '$'.join([item.get('id', ''), item.get('name', '')]).lower() <IF_STMT> results.append({'title': item['name'], 'icon': item['icon'], 'url': item['url']}) return results",if query in search_source:
"def handle(self) -> None: """"""Handles a request ignoring dropped connections."""""" try: BaseHTTPRequestHandler.handle(self) except (ConnectionError, socket.timeout) as e: self.connection_dropped(e) except Exception as e: <IF_STMT> self.log_error('SSL error occurred: %s', e) else: raise if self.server.shutdown_signal: self.initiate_shutdown()",if self.server.ssl_context is not None and is_ssl_error(e):
"def cdn_url_handler(error, endpoint, kwargs): if endpoint == 'cdn': path = kwargs.pop('path') cdn = app.config.get('cdn', '//cdnjscn.b0.upaiyun.com/libs/') return urljoin(cdn, path) else: exc_type, exc_value, tb = sys.exc_info() <IF_STMT> reraise(exc_type, exc_value, tb) else: raise error",if exc_value is error:
"def pairs(self): for path in os.listdir('src'): if path == '.svn': continue dep = join('src', path) <IF_STMT> continue yield (dep, join(build_dir, path))",if isdir(dep):
"def get_condition(self): """"""Return the condition element's name."""""" for child in self.xml: <IF_STMT> cond = child.tag.split('}', 1)[-1] if cond in self.conditions: return cond return 'not-authorized'",if '{%s}' % self.namespace in child.tag:
"def end(self, tag): try: f = self.dispatch[tag] except KeyError: <IF_STMT> return try: f = self.dispatch[tag.split(':')[-1]] except KeyError: return return f(self, ''.join(self._data))",if ':' not in tag:
"def checkIfSessionCodeExists(self, sessionCode): if self.emrtFile: sessionsForExperiment = self.emrtFile.root.data_collection.session_meta_data.where('experiment_id == %d' % (self.active_experiment_id,)) sessionCodeMatch = [sess for sess in sessionsForExperiment if sess['code'] == sessionCode] <IF_STMT> return True return False",if len(sessionCodeMatch) > 0:
"def save_bytearray(self, obj): if self.proto < 5: <IF_STMT> self.save_reduce(bytearray, (), obj=obj) else: self.save_reduce(bytearray, (bytes(obj),), obj=obj) return n = len(obj) if n >= self.framer._FRAME_SIZE_TARGET: self._write_large_bytes(BYTEARRAY8 + pack('<Q', n), obj) else: self.write(BYTEARRAY8 + pack('<Q', n) + obj)",if not obj:
"def _restore_freeze(self, new): size_change = [] for k, v in six.iteritems(self._freeze_backup): newv = new.get(k, []) <IF_STMT> size_change.append((self._key_name(k), len(v), len(newv))) if size_change: logger.info('These collections were modified but restored in {}: {}'.format(self._name, ', '.join(map(lambda t: '({}: {}->{})'.format(*t), size_change)))) restore_collection(self._freeze_backup)",if len(v) != len(newv):
"def check_options(self, expr, evaluation, options): for key in options: if key != 'System`SameTest': <IF_STMT> evaluation.message('ContainsOnly', 'optx', Symbol(key)) else: return evaluation.message('ContainsOnly', 'optx', Symbol(key), expr) return None",if expr is None:
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith('.'): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): <IF_STMT> self.bundle_package(itempath) elif nm.endswith('.py'): self.bundle_module(itempath)","if os.path.exists(os.path.join(itempath, '__init__.py')):"
"def _read_block(self, size): if self._file_end is not None: max_size = self._file_end - self._file.tell() <IF_STMT> size = max_size size = max(min(size, max_size), 0) return self._file.read(size)",if size == -1:
"def question_mark(self): """"""Shows help for this command and it's sub-commands."""""" ret = [] if self.param_help_msg or len(self.subcommands) == 0: ret.append(self._quick_help()) if len(self.subcommands) > 0: for k, _ in sorted(self.subcommands.items()): command_path, param_help, cmd_help = self._instantiate_subcommand(k)._quick_help(nested=True) <IF_STMT> ret.append((command_path, param_help, cmd_help)) return (CommandsResponse(STATUS_OK, self.help_formatter(ret)), self.__class__)",if command_path or param_help or cmd_help:
"def list_domains(self, r53, **kwargs): marker = None domains = [] while True: if marker: response = self.wrap_aws_rate_limited_call(r53.list_domains(Marker=marker)) else: response = self.wrap_aws_rate_limited_call(r53.list_domains) for domain in response.get('Domains'): domains.append(domain) <IF_STMT> marker = response.get('NextPageMarker') else: break return domains",if response.get('NextPageMarker'):
"def writer(stream, items): sep = '' for item in items: stream.write(sep) sep = ' ' if not isinstance(item, str): item = str(item) if not PY3K: <IF_STMT> item = str(item) stream.write(item) stream.write('\n')","if not isinstance(item, unicode):"
"def f(view, s): if mode == modes.INTERNAL_NORMAL: view.run_command('toggle_comment') <IF_STMT> pt = utils.next_non_white_space_char(view, s.a, white_space=' \t') else: pt = utils.next_non_white_space_char(view, self.view.line(s.a).a, white_space=' \t') return R(pt, pt) return s","if utils.row_at(self.view, s.a) != utils.row_at(self.view, self.view.size()):"
"def _parse_timestamp(value): if value: match = _TIMESTAMP_PATTERN.match(value) <IF_STMT> if match.group(2): format = '%Y-%m-%d %H:%M:%S.%f' value = match.group() else: format = '%Y-%m-%d %H:%M:%S' value = datetime.datetime.strptime(value, format) else: raise Exception('Cannot convert ""{}"" into a datetime'.format(value)) else: value = None return value",if match:
"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): if model_site['type'] == 'sample': log_r_term = model_site['log_prob'] <IF_STMT> log_r_term = log_r_term - guide_trace.nodes[name]['log_prob'] log_r.add((stacks[name], log_r_term.detach())) return log_r",if not model_site['is_observed']:
"def get_translationproject(self): """"""returns the translation project belonging to this directory."""""" if self.is_language() or self.is_project(): return None el<IF_STMT> return self.translationproject else: aux_dir = self while not aux_dir.is_translationproject() and aux_dir.parent is not None: aux_dir = aux_dir.parent return aux_dir.translationproject",if self.is_translationproject():
"def get_hosted_content(): try: scheme, rest = target.split('://', 1) prefix, host_and_port = rest.split('.interactivetool.') faked_host = rest <IF_STMT> faked_host = rest.split('/', 1)[0] url = '%s://%s' % (scheme, host_and_port) response = requests.get(url, timeout=1, headers={'Host': faked_host}) return response.text except Exception as e: print(e) return None",if '/' in rest:
def install(self): log.info(self.openssl_cli) if not self.has_openssl or self.args.force: <IF_STMT> self._download_src() else: log.debug('Already has src {}'.format(self.src_file)) self._unpack_src() self._build_src() self._make_install() else: log.info('Already has installation {}'.format(self.install_dir)) version = self.openssl_version if self.version not in version: raise ValueError(version),if not self.has_src:
"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): if i % 2: pieces.append(force_text(getattr(self, piece)())) <IF_STMT> pieces.append(re_escaped.sub('\\1', piece)) return ''.join(pieces)",elif piece:
"def get_current_events_users(calendar): now = timezone.make_aware(datetime.now(), timezone.get_current_timezone()) result = [] day = Day(calendar.events.all(), now) for o in day.get_occurrences(): <IF_STMT> usernames = o.event.title.split(',') for username in usernames: result.append(User.objects.get(username=username.strip())) return result",if o.start <= now <= o.end:
"def from_cfn_params(self, cfn_params): """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch."""""" cfn_converter = self.definition.get('cfn_param_mapping', None) if cfn_converter and cfn_params: <IF_STMT> self.value = int(float(get_cfn_param(cfn_params, cfn_converter))) return self","if get_cfn_param(cfn_params, 'Scheduler') == 'awsbatch':"
"def onCompletion(self, text): res = [] for l in text.split('\n'): <IF_STMT> continue l = l.split(':') if len(l) != 2: continue res.append([l[0].strip(), l[1].strip()]) self.panel.setChapters(res)",if not l:
"def update_ranges(l, i): for _range in l: if i == _range[0] - 1: _range[0] = i merge_ranges(l) return <IF_STMT> _range[1] = i merge_ranges(l) return l.append([i, i]) l.sort(key=lambda x: x[0])",elif i == _range[1] + 1:
"def process_dollar(token, state, command_line): if not state.is_range_start_line_parsed: <IF_STMT> raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.end.append(token) return (parse_line_ref, command_line)",if command_line.line_range.start:
"def _parse_description(self, text: str): result = dict(links=[], versions=[]) for line in text.splitlines(): clean = REX_TAG.sub('', line.strip()) <IF_STMT> result['severity'] = clean.split()[1] continue if clean.startswith('Affects:'): result['name'] = clean.split()[1] continue if ' or higher' in clean: result['versions'] = self._get_versions(clean) result['links'].extend(REX_LINK.findall(line)) return result",if clean.startswith('Severity:'):
"def apply(self, chart, grammar): for prod in grammar.productions(empty=True): for index in compat.xrange(chart.num_leaves() + 1): new_edge = TreeEdge.from_production(prod, index) <IF_STMT> yield new_edge","if chart.insert(new_edge, ()):"
"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: <IF_STMT> self.current += num elif self.op == '-': self.current -= num elif self.op == '*': self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",if self.op == '+':
"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = (50, 50, 50) for frame in g.app.windowList: w = frame and frame.top if w: r = w.geometry() w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) x += 30 y += 30 <IF_STMT> x = 10 + delta y = 40 + delta delta += 10",if x > 200:
def redirect(self): c = self.c if c.config.getBool('eval-redirect'): self.old_stderr = g.stdErrIsRedirected() self.old_stdout = g.stdOutIsRedirected() <IF_STMT> g.redirectStderr() if not self.old_stdout: g.redirectStdout(),if not self.old_stderr:
"def on_event(self, c, button, data): if self.rvGestureGrab.get_reveal_child(): <IF_STMT> self.use() elif button == 'Y' and data[0] == 0: self.start_over()",if button == 'A' and data[0] == 0:
"def __init__(self, in_feats, out_feats, norm='both', bias=True, activation=None): super(DenseGraphConv, self).__init__() self._in_feats = in_feats self._out_feats = out_feats self._norm = norm with self.name_scope(): self.weight = self.params.get('weight', shape=(in_feats, out_feats), init=mx.init.Xavier(magnitude=math.sqrt(2.0))) <IF_STMT> self.bias = self.params.get('bias', shape=(out_feats,), init=mx.init.Zero()) else: self.bias = None self._activation = activation",if bias:
"def _import_top_module(self, name): for item in sys.path: if isinstance(item, _StringType): module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) <IF_STMT> return module return None",if module:
"def resolver(schemas, f): if not callable(f): return if not hasattr(f, 'accepts'): return new_params = [] for p in f.accepts: <IF_STMT> new_params.append(p.resolve(schemas)) else: raise ResolverError('Invalid parameter definition {0}'.format(p)) f.accepts.clear() f.accepts.extend(new_params)","if isinstance(p, (Patch, Ref, Attribute)):"
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) if fname == 'output': continue <IF_STMT> continue if fname[-4:] == '.pyc': continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",if fname == 'PureMVC_Python_1_0':
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: if rightname[0] == '@': for rightname2 in groups[rightname]: rightnames.add(rightname2) <IF_STMT> break else: rightnames.add(rightname)",if not includeAll:
"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from['Stats']): <IF_STMT> self.entries_count['Stats'] -= 1 continue new_obj = self.model_to['Stats']() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns: continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",if not old_obj.summary:
"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] <IF_STMT> value = os.environ[envvar] if six.PY2: value = value.decode('utf8') return value else: raise InvalidConfigException(""{} - environment variable '{}' not set"".format(msg, var)) else: raise InvalidConfigException(""{} - environment variable name '{}' does not match pattern '{}'"".format(msg, var, _ENV_VAR_PAT_STR))",if envvar in os.environ:
"def __next__(self): self._parse_reset() while True: try: line = next(self.input_iter) except StopIteration: <IF_STMT> raise Error('newline inside string') raise self.line_num += 1 if '\x00' in line: raise Error('line contains NULL byte') pos = 0 while pos < len(line): pos = self._parse_process_char(line, pos) self._parse_eol() if self.state == self.START_RECORD: break fields = self.fields self.fields = [] return fields",if len(self.field) > 0:
"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes('\x00\x00\x01', self.current_size, self.current_size + 1024 * 1024 * 8) <IF_STMT> padsize = pos - self.current_size if padsize: yield PaddingBytes(self, 'pad[]', padsize // 8) chunk = Chunk(self, 'chunk[]') try: chunk['content/data'] except: pass yield chunk",if pos is not None:
"def spew(): seenUID = False start() for part in query: if part.type == 'uid': seenUID = True <IF_STMT> yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, 'spew_' + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and (not seenUID): space() yield self.spew_uid(id, msg, write, flush) finish() flush()",if part.type == 'body':
"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key]['inverse'] is True: if datetime.now() - limit > value: value = datetime.now() - limit el<IF_STMT> value = datetime.now() + limit elif value > limit: value = limit return value",if datetime.now() + limit < value:
"def _fix_var_naming(operators, names, mod='input'): new_names = [] map = {} for op in operators: if mod == 'input': iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name <IF_STMT> break for name in names: new_names.append(map[name]) return new_names",if len(map) == len(names):
"def traverse(tree): """"""Generator dropping comment nodes"""""" for entry in tree: spaceless = [e for e in entry if not nginxparser.spacey(e)] if spaceless: key = spaceless[0] values = spaceless[1] if len(spaceless) > 1 else None else: key = values = '' if isinstance(key, list): new = copy.deepcopy(entry) new[1] = filter_comments(values) yield new el<IF_STMT> yield spaceless",if key != '#' and spaceless:
"def mergeCombiners(self, x, y): for item in y: <IF_STMT> self.heap.push(x, item) else: self.heap.push_pop(x, item) return x",if len(x) < self.heap_limit:
"def test_scatter(self, harness: primitive_harness.Harness): f_name = harness.params['f_lax'].__name__ dtype = harness.params['dtype'] if jtu.device_under_test() == 'tpu': <IF_STMT> raise unittest.SkipTest(f'TODO: complex {f_name} on TPU fails in JAX') self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))","if dtype is np.complex64 and f_name in ['scatter_min', 'scatter_max']:"
"def TryMerge(self, decoder): while decoder.avail() > 0: tag = decoder.getVarInt32() if tag == TAG_BEGIN_ITEM_GROUP: type_id, message = Item.Decode(decoder) <IF_STMT> self.items[type_id].MergeFrom(Item(message)) else: self.items[type_id] = Item(message) continue if tag == 0: raise ProtocolBuffer.ProtocolBufferDecodeError decoder.skipData(tag)",if type_id in self.items:
"def process_continuations(lines): global continuation_pattern olines = [] while len(lines) != 0: line = no_comments(lines[0]) line = line.strip() lines.pop(0) if line == '': continue <IF_STMT> line = continuation_pattern.sub('', line) if len(lines) >= 1: combined_lines = [line + lines[0]] lines.pop(0) lines = combined_lines + lines continue olines.append(line) del lines return olines",if continuation_pattern.search(line):
"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: if pkg in Scheduler.listOfPackagesCurrentlyBuilding: continue <IF_STMT> Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug('Adding ' + pkg + ' to the schedule list')",if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg):
"def find_distribution_modules(name=__name__, file=__file__): current_dist_depth = len(name.split('.')) - 1 current_dist = os.path.join(os.path.dirname(file), *[os.pardir] * current_dist_depth) abs = os.path.abspath(current_dist) dist_name = os.path.basename(abs) for dirpath, dirnames, filenames in os.walk(abs): package = (dist_name + dirpath[len(abs):]).replace('/', '.') <IF_STMT> yield package for filename in filenames: if filename.endswith('.py') and filename != '__init__.py': yield '.'.join([package, filename])[:-3]",if '__init__.py' in filenames:
"def transform_value(i, v, *args): if i not in converter_functions: return v else: try: return converter_functions[i](v, *args) except Exception as e: if failonerror == 'inline': return e <IF_STMT> raise e else: return errorvalue",elif failonerror:
"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile(max_size=self._storage.max_memory_size, suffix='.S3Boto3StorageFile', dir=setting('FILE_UPLOAD_TEMP_DIR')) <IF_STMT> self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) if self._storage.gzip and self.obj.content_encoding == 'gzip': self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",if 'r' in self._mode:
"def connect(self, host, port, timeout): fp = Telnet() for i in range(50): try: fp.sock = socket.create_connection((host, int(port)), timeout=int(timeout), source_address=('', 1023 - i)) break except socket.error as e: <IF_STMT> raise e self.need_handshake = True return TCP_Connection(fp)","if (e.errno, e.strerror) != (98, 'Address already in use'):"
"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: if re.search('^\\s*\\/\\*', source): comment = source[0, source.index('*/') + 2] <IF_STMT> comment = re.search('^\\s*\\/\\/', source).group(0) else: comment = None if comment: source = re.sub('^\\s+', '', source[len(comment):]) trailing_comments.append(comment) return '\n'.join(trailing_comments) + source","elif re.search('^\\s*\\/\\/', source):"
"def yview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.vsb.get() elif mode == 'moveto': frameHeight = self.innerframe.winfo_reqheight() self._startY = value * float(frameHeight) else: clipperHeight = self._clipper.winfo_height() <IF_STMT> jump = int(clipperHeight * self._jfraction) else: jump = clipperHeight self._startY = self._startY + value * jump self.reposition()",if units == 'units':
"def visit(stmt): """"""Collect information about VTCM buffers and their alignments."""""" if isinstance(stmt, tvm.tir.AttrStmt): if stmt.attr_key == 'storage_scope' and stmt.value == 'local.vtcm': vtcm_buffers.append(stmt.node) <IF_STMT> if not stmt.node in alignments: alignments[stmt.node] = [] alignments[stmt.node].append(stmt.value)",elif stmt.attr_key == 'storage_alignment':
"def cost(P): loss_b = 0 loss_w = 0 for i, xi in enumerate(xc): xi = np.dot(xi, P) for j, xj in enumerate(xc[i:]): xj = np.dot(xj, P) M = dist(xi, xj) G = sinkhorn(wc[i], wc[j + i], M, reg, k) <IF_STMT> loss_w += np.sum(G * M) else: loss_b += np.sum(G * M) return loss_w / loss_b",if j == 0:
"def __init__(self, comm, in_channels, out_channels, ksize, pad=1): super(Block, self).__init__() with self.init_scope(): <IF_STMT> self.conv = ParallelConvolution2D(comm, in_channels, out_channels, ksize, pad=pad, nobias=True) else: self.conv = chainer.links.Convolution2D(in_channels, out_channels, ksize, pad=pad, nobias=True) self.bn = L.BatchNormalization(out_channels)",if comm.size <= in_channels:
"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, 'i', 'ii', 'iii', 'iv', 'v', 'a', 'b', 'c', 'd', 'e']: for wrong in ['cd', 'part', 'dis', 'disc', 'dvd']: <IF_STMT> wrong_found += 1 if wrong_found == 1: return -30 return 0 except: log.error('Failed doing halfMultipartScore: %s', traceback.format_exc()) return 0","if '%s%s' % (wrong, nr) in nzb_name.lower():"
"def should_include(service): for f in filt: <IF_STMT> state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == 'source': source = filt[f] if source == 'image' or source == 'build': if source not in service.options: return False else: raise UserError('Invalid value for source filter: %s' % source) else: raise UserError('Invalid filter: %s' % f) return True",if f == 'status':
"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return 'TINYBLOB' <IF_STMT> return 'BLOB' if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_BLOB:
"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] <IF_STMT> y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = (x, y, z) if event.num_clicks == 2: d.dismiss('Goto')",if y == 0:
"def check_related_active_jobs(self, obj): active_jobs = obj.get_active_jobs() if len(active_jobs) > 0: raise ActiveJobConflict(active_jobs) time_cutoff = now() - dateutil.relativedelta.relativedelta(minutes=1) recent_jobs = obj._get_related_jobs().filter(finished__gte=time_cutoff) for unified_job in recent_jobs.get_real_instances(): <IF_STMT> raise PermissionDenied(_('Related job {} is still processing events.').format(unified_job.log_format))",if not unified_job.event_processing_finished:
"def run(self): self.alive = True if _log.isEnabledFor(_DEBUG): _log.debug('started') while self.alive: task = self.queue.get() <IF_STMT> function, args, kwargs = task assert function try: function(*args, **kwargs) except: _log.exception('calling %s', function) if _log.isEnabledFor(_DEBUG): _log.debug('stopped')",if task:
"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return exists, contents = read_sysconfig_file(fn) updated_am = 0 for k, v in adjustments.items(): if v is None: continue v = str(v) <IF_STMT> continue contents[k] = v updated_am += 1 if updated_am: lines = [str(contents)] if not exists: lines.insert(0, util.make_header()) util.write_file(fn, '\n'.join(lines) + '\n', 420)",if len(v) == 0 and (not allow_empty):
"def wrapper(self: RequestHandler, *args, **kwargs) -> Optional[Awaitable[None]]: if self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path.rstrip('/') <IF_STMT> if self.request.query: uri += '?' + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",if uri:
def output_handles_from_execution_plan(execution_plan): output_handles_for_current_run = set() for step_level in execution_plan.execution_step_levels(): for step in step_level: for step_input in step.step_inputs: <IF_STMT> output_handles_for_current_run.update(step_input.source_handles) return output_handles_for_current_run,if step_input.source_handles:
"def _read_value(self, item): item = _normalize_path(item) if item in self._store: <IF_STMT> del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) elif item in self._children: return PathResult(item, dir=True) else: raise KeyError(item)",if item in self._expire_time and self._expire_time[item] < datetime.now():
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 <IF_STMT> start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",if not start:
"def _update_help_obj_params(help_obj, data_params, params_equal, attr_key_tups): loaded_params = [] for param_obj in help_obj.parameters: loaded_param = next((n for n in data_params if params_equal(param_obj, n)), None) <IF_STMT> BaseHelpLoader._update_obj_from_data_dict(param_obj, loaded_param, attr_key_tups) loaded_params.append(param_obj) help_obj.parameters = loaded_params",if loaded_param:
"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() <IF_STMT> aList = w.sizes() if len(aList) == 2: n1, n2 = aList ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",if w:
def _check_required_env_variables(vars): for var in vars: <IF_STMT> self.tc.logger.error('%s is not set. Did you forget to source your build environment setup script?' % var) raise OEQAPreRun,if not os.environ.get(var):
def clean_indexes(): for coll_name in mongo.collection_types.keys(): coll = mongo.get_collection(coll_name) indexes = coll_indexes[coll_name] try: for index in coll.list_indexes(): name = index['name'] <IF_STMT> continue coll.drop_index(name) except pymongo.errors.OperationFailure: pass,if name == '_id' or name == '_id_' or name in indexes:
"def _compare_dirs(self, dir1, dir2): diff = [] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <IF_STMT> diff.append(file_) return diff",if not os.path.exists(target_path):
"def load_state_dict(self, state_dict, strict=True): """"""Customized load."""""" self.language_model.load_state_dict(state_dict[self._language_model_key], strict=strict) if mpu.is_pipeline_last_stage(): <IF_STMT> self.multichoice_head.load_state_dict(state_dict[self._multichoice_head_key], strict=strict) else: print_rank_last('***WARNING*** could not find {} in the checkpoint, initializing to random'.format(self._multichoice_head_key))",if self._multichoice_head_key in state_dict:
"def _parse_timedelta(self, value): try: sum = datetime.timedelta() start = 0 while start < len(value): m = self._TIMEDELTA_PATTERN.match(value, start) <IF_STMT> raise Exception() num = float(m.group(1)) units = m.group(2) or 'seconds' units = self._TIMEDELTA_ABBREV_DICT.get(units, units) sum += datetime.timedelta(**{units: num}) start = m.end() return sum except: raise",if not m:
"def SetChildMenuBar(self, pChild): if not pChild: if self._pMyMenuBar: self.SetMenuBar(self._pMyMenuBar) else: self.SetMenuBar(self.GetMenuBar()) self._pMyMenuBar = None else: if pChild.GetMenuBar() is None: return <IF_STMT> self._pMyMenuBar = self.GetMenuBar() self.SetMenuBar(pChild.GetMenuBar())",if self._pMyMenuBar is None:
"def init_weights(self): """"""Initialize weights of the head."""""" bias_cls = bias_init_with_prob(0.01) normal_init(self.conv_reg, std=0.01) normal_init(self.conv_centerness, std=0.01) normal_init(self.conv_cls, std=0.01, bias=bias_cls) for branch in [self.cls_convs, self.reg_convs]: for module in branch.modules(): <IF_STMT> caffe2_xavier_init(module.conv)","if isinstance(module, ConvModule) and isinstance(module.conv, nn.Conv2d):"
"def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == 'success': continue _, exc_name, exc_cls, _ = result.thrift_spec[k] <IF_STMT> setattr(result, exc_name, e) break else: raise","if isinstance(e, exc_cls):"
"def scripts(self): application_root = current_app.config.get('APPLICATION_ROOT') subdir = application_root != '/' scripts = [] for script in get_registered_scripts(): <IF_STMT> scripts.append(f'<script defer src=""{script}""></script>') elif subdir: scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup('\n'.join(scripts))",if script.startswith('http'):
"def test_related_objects_local(self): result_key = 'get_all_related_objects_with_model_local' for model, expected in TEST_RESULTS[result_key].items(): objects = [(field, self._model(model, field)) for field in model._meta.get_fields(include_parents=False) <IF_STMT>] self.assertEqual(sorted(self._map_related_query_names(objects), key=self.key_name), sorted(expected, key=self.key_name))",if field.auto_created and (not field.concrete)
"def setTestOutcome(self, event): """"""Update outcome, exc_info and reason based on configured mappings"""""" if event.exc_info: ec, ev, tb = event.exc_info classname = ec.__name__ if classname in self.treatAsFail: short, long_ = self.labels(classname) self._setOutcome(event, 'failed', short, long_) <IF_STMT> short, long_ = self.labels(classname, upper=False) self._setOutcome(event, 'skipped', short, ""%s: '%s'"" % (long_, ev), str(ev))",elif classname in self.treatAsSkip:
"def small_count(v): if not v: return 0 z = [(1000000000, _('b')), (1000000, _('m')), (1000, _('k'))] v = int(v) for x, y in z: o, p = divmod(v, x) if o: <IF_STMT> return '%d%s' % (o, y) return '%.1f%s' % (v / float(x), y) return v",if len(str(o)) > 2 or not p:
"def __read(self, n): if self._read_watcher is None: raise UnsupportedOperation('read') while 1: try: return _read(self._fileno, n) except (IOError, OSError) as ex: <IF_STMT> raise wait_on_watcher(self._read_watcher, None, None, self.hub)",if ex.args[0] not in ignored_errors:
def locked(self): inputfiles = set(self.all_inputfiles()) outputfiles = set(self.all_outputfiles()) if os.path.exists(self._lockdir): for lockfile in self._locks('input'): with open(lockfile) as lock: for f in lock: f = f.strip() if f in outputfiles: return True for lockfile in self._locks('output'): with open(lockfile) as lock: for f in lock: f = f.strip() <IF_STMT> return True return False,if f in outputfiles or f in inputfiles:
"def _flags_to_int(flags): if not flags: return 0 if isinstance(flags, integer_types): return flags result = 0 try: <IF_STMT> flags = flags.split(',') for value in flags: value = value.strip().lower() if value: result |= _flags_str2int[value] except KeyError as ex: raise ValueError('Invalid backend or flag: %s\nPossible values: %s' % (ex, ', '.join(sorted(_flags_str2int.keys())))) return result","if isinstance(flags, basestring):"
"def setFg(self, colour, override=False): if not self.ttkFlag: self.containerStack[-1]['fg'] = colour gui.SET_WIDGET_FG(self._getContainerProperty('container'), colour, override) for child in self._getContainerProperty('container').winfo_children(): <IF_STMT> gui.SET_WIDGET_FG(child, colour, override) else: gui.trace('In ttk mode - trying to set FG to %s', colour) self.ttkStyle.configure('TLabel', foreground=colour) self.ttkStyle.configure('TFrame', foreground=colour)",if not self._isWidgetContainer(child):
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v['Category'] != 'Deprecated': if v['FeatureType'] == 'val': if name.startswith('SCE_'): states.append((name, v['Value'])) <IF_STMT> lexers.append((name, v['Value'])) return (lexers, states)",elif name.startswith('SCLEX_'):
def extract_error_message(response: requests.Response): if response.content: try: content = json.loads(response.content) <IF_STMT> return content['message'] except: logging.debug(f'Failed to parse the response content: {response.content}') return response.reason,if 'message' in content:
"def canvas_size(self): """"""Return the width and height for this sprite canvas"""""" width = height = 0 for image in self.images: x = image.x + image.absolute_width y = image.y + image.absolute_height <IF_STMT> width = x if height < y: height = y return (round_up(width), round_up(height))",if width < x:
"def _load_widgets(self): logger.info('Loading plugins preferences widgets') for plugin in self.plugin_manager.get_active_plugins(): plugin_name = plugin.metadata.get('name') try: preferences_widget = plugin.get_preferences_widget() <IF_STMT> self._tabs.addTab(preferences_widget, plugin_name) except Exception as reason: logger.error('Unable to add the preferences widget (%s): %s', plugin_name, reason) continue",if preferences_widget:
"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: if att not in common_attributes: prefix_words_are_adj = False <IF_STMT> return (words[-1:], words[:-1]) else: return ([string], []) else: return ([string], [])",if prefix_words_are_adj:
"def _reader(): if shuffle: random.shuffle(file_list) while True: for fn in file_list: for line in open(fn, 'r'): yield self._process_line(line) <IF_STMT> break",if not cycle:
"def load(weights, model, K, fsz, dil): index = 0 layers = model.layers for layer in layers._layers: <IF_STMT> if layer.W.shape == weights[index].shape: layer.W[:] = weights[index] else: layer.W[:] = dilate(weights[index], K, fsz, dil) index += 1","if hasattr(layer, 'W'):"
def upgrade(migrate_engine): print(__doc__) metadata.bind = migrate_engine liftoverjobs = dict() jobs = context.query(DeferredJob).filter_by(plugin='LiftOverTransferPlugin').all() for job in jobs: <IF_STMT> liftoverjobs[job.params['parentjob']] = [] liftoverjobs[job.params['parentjob']].append(job.id) for parent in liftoverjobs: lifts = liftoverjobs[parent] deferred = context.query(DeferredJob).filter_by(id=parent).first() deferred.params['liftover'] = lifts context.flush(),if job.params['parentjob'] not in liftoverjobs:
"def get_refs(self, recursive=False): """""":see: AbstractExpression.get_refs()"""""" if recursive: conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), []) <IF_STMT> conds_refs.extend(self.consequent.get_refs(True)) return conds_refs else: return self.refs",if self.consequent:
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): <IF_STMT> self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",if 'axis' in self.args:
"def CountMatches(pat, predicate): num_matches = 0 for i in xrange(256): b = chr(i) m = pat.match(b) left = bool(m) right = predicate(i) if left != right: self.fail('i = %d, b = %r, match: %s, predicate: %s' % (i, b, left, right)) <IF_STMT> num_matches += 1 return num_matches",if m:
"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError('You can either use {} with one positional argument or with keyword arguments, not both.'.format(cls.__name__)) if not args[0]: return super().__new__(cls) <IF_STMT> return cls return super().__new__(cls, *args, **kwargs)","if isinstance(args[0], cls):"
"def concatenateCharacterTokens(tokens): pendingCharacters = [] for token in tokens: type = token['type'] if type in ('Characters', 'SpaceCharacters'): pendingCharacters.append(token['data']) else: <IF_STMT> yield {'type': 'Characters', 'data': ''.join(pendingCharacters)} pendingCharacters = [] yield token if pendingCharacters: yield {'type': 'Characters', 'data': ''.join(pendingCharacters)}",if pendingCharacters:
"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): if func.type in support_set: pos_end = pos else: <IF_STMT> ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",if pos_end >= pos_start:
"def _visit(self, func): fname = func[0] if fname in self._flags: <IF_STMT> logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if self._flags[fname] == 1:
"def graph_merge_softmax_with_crossentropy_softmax(node): if node.op == softmax_with_bias: x, b = node.inputs for x_client in x.clients: <IF_STMT> big_client = x_client[0] if big_client in [b_client[0] for b_client in b.clients]: xx, bb, ll = big_client.inputs mergeable_client = big_client.op(x, b, ll) copy_stack_trace(node.outputs[0], mergeable_client[1]) return [mergeable_client[1]]",if x_client[0].op == crossentropy_softmax_argmax_1hot_with_bias:
"def confidence(self): if self.bbox: distance = Distance(self.northeast, self.southwest, units='km') for score, maximum in [(10, 0.25), (9, 0.5), (8, 1), (7, 5), (6, 7.5), (5, 10), (4, 15), (3, 20), (2, 25)]: if distance < maximum: return score <IF_STMT> return 1 return 0",if distance >= 25:
"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: if id(bp) == item_id: <IF_STMT> text = None bp.cond = text break self.RespondDebuggerData()",if text.strip().lower() == 'none':
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): <IF_STMT> if isinstance(text, CodeViewText): text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():"
"def visit_Macro(self, node, frame): macro_frame, macro_ref = self.macro_body(node, frame) self.newline() if frame.toplevel: <IF_STMT> self.write('context.exported_vars.add(%r)' % node.name) ref = frame.symbols.ref(node.name) self.writeline('context.vars[%r] = ' % node.name) self.write('%s = ' % frame.symbols.ref(node.name)) self.macro_def(macro_ref, macro_frame)",if not node.name.startswith('_'):
"def execute(cls, ctx, op): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) <IF_STMT> return cls._execute_map(ctx, op) else: return cls._execute_combine(ctx, op) finally: pd.reset_option('mode.use_inf_as_na')",if op.stage == OperandStage.map:
"def ranges(self, start, end): try: iterators = [i.ranges(start, end) for i in self.range_iterators] starts, ends, values = zip(*[next(i) for i in iterators]) starts = list(starts) ends = list(ends) values = list(values) while start < end: min_end = min(ends) yield (start, min_end, values) start = min_end for i, iterator in enumerate(iterators): <IF_STMT> starts[i], ends[i], values[i] = next(iterator) except StopIteration: return",if ends[i] == min_end:
"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) <IF_STMT> return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: if self.strict > 1: raise pass elif self.strict > 1: raise PermError('Empty domain-spec on exp=') return None",if len(a) == 1:
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue <IF_STMT> if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if exclude_unset:
"def __setattr__(self, name, value): try: field = self._meta.get_field(name) <IF_STMT> value = value[:field.max_length] except models.fields.FieldDoesNotExist: pass super.__setattr__(self, name, value)","if type(field) in [models.CharField, models.TextField] and type(value) == str:"
"def create_child(self, value=None, _id=None): with atomic(savepoint=False): child_key = self.get_next_child_key() <IF_STMT> value = child_key child = self.__class__.objects.create(id=_id, key=child_key, value=value) return child",if value is None:
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {'HasMoreTags': False, 'Tags': tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): <IF_STMT> result['HasMoreTags'] = True break if exclusive_start_tag_key and key < exclusive_start_tag_key: continue tags.append({'Key': key, 'Value': val}) return result",if limit and len(tags) >= limit:
"def emit(self, record): try: app = get_app() <IF_STMT> msg = self.format(record) debug_buffer = app.layout.get_buffer_by_name('debug_buffer') current_document = debug_buffer.document.text if current_document: msg = '\n'.join([current_document, msg]) debug_buffer.set_document(Document(text=msg), bypass_readonly=True) else: super().emit(record) except: self.handleError(record)","if app.is_running and getattr(app, 'debug', False):"
"def worker(): global error while True: num, q = pq.get() <IF_STMT> pq.task_done() break try: process_one(q) except Exception as e: error = e finally: pq.task_done()",if q is None or error is not None:
"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): <IF_STMT> if len(TRANSCEIVER[t]) < b + 1: out.append('(unknown)') else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate('Transceiver compliance', ', '.join(out))",if value & 128:
def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] <IF_STMT> return elif self.classifier.is_index_op(tok): tval = tok['text'] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break,if ttype == SCE_PL_UNUSED:
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: <IF_STMT> continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if item < vector[-1]:
"def __setattr__(self, name, value): if name == 'path': <IF_STMT> if value[0] != '/': raise ValueError('The page path should always start with a slash (""/"").') elif name == 'load_time': if value and (not isinstance(value, int)): raise ValueError('Page load time must be specified in integer milliseconds.') object.__setattr__(self, name, value)",if value and value != '':
"def awaitTermination(self, timeout=None): if self.scheduler is None: raise RuntimeError('StreamimgContext not started') try: deadline = time.time() + timeout if timeout is not None else None while True: is_terminated = self._runOnce() <IF_STMT> break if self.batchCallback: self.batchCallback() except KeyboardInterrupt: pass finally: self.sc.stop() logger.info('StreamingContext stopped successfully')",if is_terminated or (deadline is not None and time.time() > deadline):
def stopbutton(self): if GPIOcontrol: while mediastopbutton: time.sleep(0.25) <IF_STMT> print('Stopped') stop(),if not GPIO.input(stoppushbutton):
"def test_create_connection_timeout(self): with self.mocked_socket_module(): try: socket.create_connection((HOST, 1234)) except socket.timeout: pass except OSError as exc: <IF_STMT> raise else: self.fail('socket.timeout not raised')",if support.IPV6_ENABLED or exc.errno != errno.EAFNOSUPPORT:
"def handle_exception_and_die(e): if hasattr(e, 'kind'): <IF_STMT> sys.stderr.write('ABORT: ' + e.msg + '\n') sys.exit(e.value) elif e.kind == 'exit': sys.stderr.write('EXITING\n') sys.exit(e.value) else: print(str(e)) sys.exit(1)",if e.kind == 'die':
"def gets(self, key): with self.client_pool.get_and_release(destroy_on_fail=True) as client: try: return client.gets(key) except Exception: <IF_STMT> return (None, None) else: raise",if self.ignore_exc:
"def _execute(self, options, args): if len(args) < 3: raise CommandError(_('Not enough arguments')) tag = fsn2text(args[0]) value = fsn2text(args[1]) paths = args[2:] songs = [] for path in paths: song = self.load_song(path) <IF_STMT> raise CommandError(_('Can not set %r') % tag) self.log('Add %r to %r' % (value, tag)) song.add(tag, value) songs.append(song) self.save_songs(songs)",if not song.can_change(tag):
"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = '' if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) if place: place_title = place_displayer.display(self.dbstate.db, place) <IF_STMT> if len(place_title) > 25: text = place_title[:24] + '...' else: text = place_title return text",if place_title != '':
"def _Determine_Do(self): self.applicable = 1 self.value = os.environ.get(self.name, None) if self.value is None and black.configure.items.has_key('buildType'): buildType = black.configure.items['buildType'].Get() <IF_STMT> self.value = 'warn' else: self.value = None self.determined = 1",if buildType == 'debug':
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) <IF_STMT> continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, '__init__.py')): self.bundle_package(itempath) elif nm.endswith('.py'): self.bundle_module(itempath)",if nm.startswith('.'):
"def header_fields(self, fields): headers = dict(self.conn.response.getheaders()) ret = {} for field in fields: <IF_STMT> raise ValueError('%s was not found in response header' % field[1]) try: ret[field[0]] = int(headers[field[1]]) except ValueError: ret[field[0]] = headers[field[1]] return ret",if not headers.has_key(field[1]):
"def caesar_cipher(s, k): result = '' for char in s: n = ord(char) <IF_STMT> n = (n - 65 + k) % 26 + 65 if 96 < n < 123: n = (n - 97 + k) % 26 + 97 result = result + chr(n) return result",if 64 < n < 91:
"def qtTypeIdent(conn, *args): res = None value = None for val in args: if not hasattr(val, '__len__'): val = str(val) <IF_STMT> continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res",if len(val) == 0:
"def _parse_timezone(value: Optional[str], error: Type[Exception]) -> Union[None, int, timezone]: if value == 'Z': return timezone.utc elif value is not None: offset_mins = int(value[-2:]) if len(value) > 3 else 0 offset = 60 * int(value[1:3]) + offset_mins <IF_STMT> offset = -offset try: return timezone(timedelta(minutes=offset)) except ValueError: raise error() else: return None",if value[0] == '-':
"def indent(elem, level=0): i = '\n' + level * '  ' if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + '  ' <IF_STMT> elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i elif level and (not elem.tail or not elem.tail.strip()): elem.tail = i",if not elem.tail or not elem.tail.strip():
"def _make_slices(shape: tp.Tuple[int, ...], axes: tp.Tuple[int, ...], size: int, rng: np.random.RandomState) -> tp.List[slice]: slices = [] for a, s in enumerate(shape): if a in axes: <IF_STMT> raise ValueError('Cannot crossover on axis with size 1') start = rng.randint(s - size) slices.append(slice(start, start + size)) else: slices.append(slice(None)) return slices",if s <= 1:
"def _loadTestsFromTestCase(self, event, testCaseClass): evt = events.LoadFromTestCaseEvent(event.loader, testCaseClass) result = self.session.hooks.loadTestsFromTestCase(evt) if evt.handled: loaded_suite = result or event.loader.suiteClass() else: names = self._getTestCaseNames(event, testCaseClass) <IF_STMT> names = ['runTest'] loaded_suite = event.loader.suiteClass(map(testCaseClass, names)) if evt.extraTests: loaded_suite.addTests(evt.extraTests) return loaded_suite","if not names and hasattr(testCaseClass, 'runTest'):"
"def check_settings(self): if self.settings_dict['TIME_ZONE'] is not None: if not settings.USE_TZ: raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because USE_TZ is False."" % self.alias) <IF_STMT> raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because its engine handles time zones conversions natively."" % self.alias)",elif self.features.supports_timezones:
"def collect_conflicting_diffs(path, decisions): local_conflict_diffs = [] remote_conflict_diffs = [] for d in decisions: <IF_STMT> ld = adjust_patch_level(path, d.common_path, d.local_diff) rd = adjust_patch_level(path, d.common_path, d.remote_diff) local_conflict_diffs.extend(ld) remote_conflict_diffs.extend(rd) return (local_conflict_diffs, remote_conflict_diffs)",if d.conflict:
"def short_repr(obj): if isinstance(obj, (type, types.ModuleType, types.BuiltinMethodType, types.BuiltinFunctionType)): return obj.__name__ if isinstance(obj, types.MethodType): <IF_STMT> return obj.im_func.__name__ + ' (bound)' else: return obj.im_func.__name__ if isinstance(obj, (tuple, list, dict, set)): return '%d items' % len(obj) if isinstance(obj, weakref.ref): return 'all_weakrefs_are_one' return repr(obj)[:40]",if obj.im_self is not None:
"def _massage_uri(uri): if uri: <IF_STMT> uri = uri.replace('hdfs://', get_defaultfs()) elif uri.startswith('/'): uri = get_defaultfs() + uri return uri",if uri.startswith('hdfs:///'):
"def chsub(self, msg, chatid): cmd, evt, params = self.tokenize(msg, 3) if cmd == '/sub': sql = 'replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?)' el<IF_STMT> sql = 'delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1)' else: sql = 'delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ?' with self.bot.database as conn: conn.execute(sql, [chatid, evt, params]) conn.commit() return",if evt == 'everything':
"def undefined_symbols(self): result = [] for p in self.Productions: <IF_STMT> continue for s in p.prod: if not s in self.Prodnames and (not s in self.Terminals) and (s != 'error'): result.append((s, p)) return result",if not p:
"def modify_column(self, column: List[Optional['Cell']]): for i in range(len(column)): gate = column[i] if gate is self: continue <IF_STMT> column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])","elif isinstance(gate, ParityControlCell):"
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): <IF_STMT> rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.MULTI_EXIT_DISC:
"def writexml(self, stream, indent='', addindent='', newl='', strip=0, nsprefixes={}, namespace=''): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue <IF_STMT> v = str(v) if strip: v = ' '.join(v.split()) val = escape(v) w(val)","if not isinstance(v, str):"
"def _condition(ct): for qobj in args: <IF_STMT> for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError('Unsupported Q object') for attr, val in kwargs.items(): if getattr(ct, attr) != val: return False return True",if qobj.connector == 'AND' and (not qobj.negated):
"def results_iter(self): <IF_STMT> from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] elif needs_string_cast: date = typecast_timestamp(str(date)) yield date",if self.connection.ops.oracle:
"def get_job_type(self): if int(self.job_runtime_conf.get('dsl_version', 1)) == 2: job_type = self.job_runtime_conf['job_parameters'].get('common', {}).get('job_type') <IF_STMT> job_type = self.job_runtime_conf['job_parameters'].get('job_type', 'train') else: job_type = self.job_runtime_conf['job_parameters'].get('job_type', 'train') return job_type",if not job_type:
def validate_assessment_criteria(self): if self.assessment_criteria: total_weightage = 0 for criteria in self.assessment_criteria: total_weightage += criteria.weightage or 0 <IF_STMT> frappe.throw(_('Total Weightage of all Assessment Criteria must be 100%')),if total_weightage != 100:
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split(':') <IF_STMT> _id = split_provider_id[1] cursor = self.get_by_id(_id) if cursor: result.append(cursor) return result",if len(split_provider_id) == 2:
"def dump_predictions_to_database(relation, predictions): judge = 'iepy-run on {}'.format(datetime.now().strftime('%Y-%m-%d %H:%M')) for evidence, relation_is_present in predictions.items(): label = EvidenceLabel.YESRELATION <IF_STMT> else EvidenceLabel.NORELATION evidence.set_label(relation, label, judge, labeled_by_machine=True)",if relation_is_present
"def __init__(self, **kwargs): dfl = get_model_label(self.default_model_class) if 'to' in kwargs.keys(): old_to = get_model_label(kwargs.pop('to')) <IF_STMT> msg = '%s can only be a ForeignKey to %s; %s passed' % (self.__class__.__name__, dfl, old_to) warnings.warn(msg, SyntaxWarning) kwargs['to'] = dfl super().__init__(**kwargs)",if old_to.lower() != dfl.lower():
"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = (rb.data[ri], lb.data[li]) li += 1 if li >= BLOCKLEN: lb = lb.rightlink li = 0 ri -= 1 <IF_STMT> rb = rb.leftlink ri = BLOCKLEN - 1",if ri < 0:
"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: if API_CACHE is None: API_CACHE = {} <IF_STMT> API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",if API_CACHE.get(url) is None:
"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = (index.start, index.stop) new_start, new_stop = (None, None) if start is not None: new_stop = length - start <IF_STMT> new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",if stop is not None:
"def infer_returned_object(pyfunction, args): """"""Infer the `PyObject` this `PyFunction` returns after calling"""""" object_info = pyfunction.pycore.object_info result = object_info.get_exact_returned(pyfunction, args) if result is not None: return result result = _infer_returned(pyfunction, args) if result is not None: <IF_STMT> params = args.get_arguments(pyfunction.get_param_names(special_args=False)) object_info.function_called(pyfunction, params, result) return result return object_info.get_returned(pyfunction, args)",if args and pyfunction.get_module().get_resource() is not None:
"def _check_imports(lib): libs = ['PyQt4', 'PyQt5', 'PySide'] libs.remove(lib) for lib2 in libs: lib2 += '.QtCore' <IF_STMT> raise RuntimeError('Refusing to import %s because %s is already imported.' % (lib, lib2))",if lib2 in sys.modules:
"def _poll(fds, timeout): if timeout is not None: timeout = int(timeout * 1000) fd_map = {} pollster = select.poll() for fd in fds: pollster.register(fd, select.POLLIN) <IF_STMT> fd_map[fd.fileno()] = fd else: fd_map[fd] = fd ls = [] for fd, event in pollster.poll(timeout): if event & select.POLLNVAL: raise ValueError('invalid file descriptor %i' % fd) ls.append(fd_map[fd]) return ls","if hasattr(fd, 'fileno'):"
"def default(cls, connection=None): """"""show the default connection, or make CONNECTION the default"""""" if connection is not None: target = cls._get_config_filename(connection) <IF_STMT> if os.path.exists(cls._default_symlink): os.remove(cls._default_symlink) os.symlink(target, cls._default_symlink) else: cls._no_config_file_error(target) if os.path.exists(cls._default_symlink): print('Default connection is ' + cls._default_connection()) else: print('There is no default connection set')",if os.path.exists(target):
"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, '..') for line in fuzzresult.history.content.splitlines(): record = line.split('/') <IF_STMT> self.queue_url(urljoin(base_url, record[1])) if record[0] == 'D': self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, '%s/CVS/Entries' % record[1]))",if len(record) == 6 and record[1]:
"def _GetCSVRow(self, value): row = [] for type_info in value.__class__.type_infos: <IF_STMT> row.extend(self._GetCSVRow(value.Get(type_info.name))) elif isinstance(type_info, rdf_structs.ProtoBinary): row.append(text.Asciify(value.Get(type_info.name))) else: row.append(str(value.Get(type_info.name))) return row","if isinstance(type_info, rdf_structs.ProtoEmbedded):"
"def get_history(self, state, dict_, passive=PASSIVE_OFF): if self.key in dict_: return History.from_scalar_attribute(self, state, dict_[self.key]) else: <IF_STMT> passive ^= INIT_OK current = self.get(state, dict_, passive=passive) if current is PASSIVE_NO_RESULT: return HISTORY_BLANK else: return History.from_scalar_attribute(self, state, current)",if passive & INIT_OK:
"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) <IF_STMT> break elif current._parent is None: raise sa_exc.InvalidRequestError('Transaction %s is not on the active transaction list' % upto) else: current = current._parent return result",if current._parent is upto:
"def get_by_uri(self, uri: str) -> bytes: userId, bucket, key = self._parse_uri(uri) try: with db.session_scope() as dbsession: result = db_archivedocument.get(userId, bucket, key, session=dbsession) <IF_STMT> return utils.ensure_bytes(self._decode(result)) else: raise ObjectKeyNotFoundError(userId, bucket, key, caused_by=None) except Exception as err: logger.debug('cannot get data: exception - ' + str(err)) raise err",if result:
"def app(scope, receive, send): while True: message = await receive() if message['type'] == 'websocket.connect': await send({'type': 'websocket.accept'}) elif message['type'] == 'websocket.receive': pass <IF_STMT> break",elif message['type'] == 'websocket.disconnect':
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <IF_STMT> y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return ''.join(y)",elif r:
"def mouse_down(self, event): if event.button == 1: <IF_STMT> p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",if self.scrolling:
"def copy_from(self, other): if self is other: return self.strictness = other.strictness for name in self.all_behaviors: self.set_behavior(name, other.get_behavior(name)) for name in self._plain_attrs: val = getattr(other, name) if isinstance(val, set): val = val.copy() <IF_STMT> val = val.copy() setattr(self, name, val)","elif decimal and isinstance(val, decimal.Decimal):"
"def __array_wrap__(self, out_arr, context=None): if self.dim is None: return out_arr else: this = self[:] <IF_STMT> return Quantity.__array_wrap__(self[:], out_arr, context=context) else: return out_arr","if isinstance(this, Quantity):"
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: <IF_STMT> length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",if token.value == '{':
"def save_all_changed_extensions(self): """"""Save configuration changes to the user config file."""""" has_changes = False for ext_name in self.extensions: options = self.extensions[ext_name] for opt in options: <IF_STMT> has_changes = True if has_changes: self.ext_userCfg.Save()","if self.set_extension_value(ext_name, opt):"
"def to_dict(self): out = {} for key in ACTIVITY_KEYS: attr = getattr(self, key) <IF_STMT> out[key] = str(attr) else: out[key] = attr if self.streak: out['streak'] = self.streak return out","if isinstance(attr, (datetime.timedelta, datetime.datetime)):"
"def clean_publication_date(cls, cleaned_input): for add_channel in cleaned_input.get('add_channels', []): is_published = add_channel.get('is_published') publication_date = add_channel.get('publication_date') <IF_STMT> add_channel['publication_date'] = datetime.date.today()",if is_published and (not publication_date):
"def _random_blur(self, batch, sigma_max): for i in range(len(batch)): <IF_STMT> sigma = random.uniform(0.0, sigma_max) batch[i] = scipy.ndimage.filters.gaussian_filter(batch[i], sigma) return batch",if bool(random.getrandbits(1)):
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() <IF_STMT> return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if i >= length:
"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): if is_forwarded_environment_variable(name): <IF_STMT> value = file_host.rebase_to_worker_root(value) env[name] = value",if os.getenv('TRUSTED_HOST') and should_rebase_environment_value(name):
"def toterminal(self, tw): last_style = None for i, entry in enumerate(self.reprentries): if entry.style == 'long': tw.line('') entry.toterminal(tw) <IF_STMT> next_entry = self.reprentries[i + 1] if entry.style == 'long' or (entry.style == 'short' and next_entry.style == 'long'): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",if i < len(self.reprentries) - 1:
"def __init__(self, loc, tabs=None): if os.path.isdir(loc): for item in os.listdir(loc): <IF_STMT> continue path = os.path.join(loc, item) self.append(CronTab(user=False, tabfile=path)) elif os.path.isfile(loc): self.append(CronTab(user=False, tabfile=loc))",if item[0] == '.':
"def import_data(self, fname): """"""Import data in current namespace"""""" if self.count(): nsb = self.currentWidget() nsb.refresh_table() nsb.import_data(fname) <IF_STMT> self.dockwidget.setVisible(True) self.dockwidget.raise_()",if self.dockwidget and (not self.ismaximized):
"def get_menu_items(node): aList = [] for child in node.children: for tag in ('@menu', '@item'): <IF_STMT> name = child.h[len(tag) + 1:].strip() if tag == '@menu': aList.append(('%s %s' % (tag, name), get_menu_items(child), None)) else: b = g.splitLines(''.join(child.b)) aList.append((tag, name, b[0] if b else '')) break return aList",if child.h.startswith(tag):
"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError('MultiDict can only be called with one positional argument') if args: if hasattr(args[0], 'iteritems'): items = list(args[0].iteritems()) <IF_STMT> items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())","elif hasattr(args[0], 'items'):"
"def open(self) -> 'KeyValueDb': """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): if not os.path.isfile(self._name): raise IOError('%s exists and is not a file' % self._name) <IF_STMT> return self with open(self._name, 'rb') as _in: self.set_records(pickle.load(_in)) else: mkpath(os.path.dirname(self._name)) self.commit() return self",if os.path.getsize(self._name) == 0:
"def sortModules(self): super(NeuronDecomposableNetwork, self).sortModules() self._constructParameterInfo() self.decompositionIndices = {} for neuron in self._neuronIterator(): self.decompositionIndices[neuron] = [] for w in range(self.paramdim): inneuron, outneuron = self.paramInfo[w] <IF_STMT> self.decompositionIndices[inneuron].append(w) else: self.decompositionIndices[outneuron].append(w)",if self.espStyleDecomposition and outneuron[0] in self.outmodules:
"def visit_Options(self, node: qlast.Options) -> None: for i, opt in enumerate(node.options.values()): <IF_STMT> self.write(' ') self.write(opt.name) if not isinstance(opt, qlast.Flag): self.write(f' {opt.val}')",if i > 0:
"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: if possible_child_hash == item_hash: return True <IF_STMT> return False possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash not in self.items:
"def __call__(self, text, **kargs): words = jieba.tokenize(text, mode='search') token = Token() for w, start_pos, stop_pos in words: <IF_STMT> continue token.original = token.text = w token.pos = start_pos token.startchar = start_pos token.endchar = stop_pos yield token",if not accepted_chars.match(w) and len(w) <= 1:
"def test_analysis_jobs_cypher_syntax(neo4j_session): parameters = {'AWS_ID': None, 'UPDATE_TAG': None, 'OKTA_ORG_ID': None} for job_name in contents('cartography.data.jobs.analysis'): <IF_STMT> continue try: cartography.util.run_analysis_job(job_name, neo4j_session, parameters) except Exception as e: pytest.fail(f""run_analysis_job failed for analysis job '{job_name}' with exception: {e}"")",if not job_name.endswith('.json'):
"def _interleave_dataset_results_and_tensors(dataset_results, flat_run_tensors): flattened_results = [] for idx in range(len(dataset_results) + len(flat_run_tensors)): <IF_STMT> flattened_results.append(dataset_results[idx]) else: flattened_results.append(flat_run_tensors.pop(0)) return flattened_results",if dataset_results.get(idx):
"def test_k_is_stochastic_parameter(self): aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) <IF_STMT> seen[0] += True elif np.array_equal(observed, self.blur5x5): seen[1] += True else: raise Exception('Unexpected result in MedianBlur@2') if all(seen): break assert np.all(seen)","if np.array_equal(observed, self.blur3x3):"
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return <IF_STMT> return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",if minGuide == None:
"def UpdateRepository(self): if hasattr(self, 'commit_update'): <IF_STMT> if not path.isdir('.git/'): self.gitZipRepo() call(['git', 'reset', '--hard', 'origin/{}'.format(self.getBranch)]) self.ProcessCall_(['git', 'pull', 'origin', self.getBranch]) self.ProcessCall_(['pip', 'install', '-r', 'requirements.txt'])",if self.commit_update['Updates'] != []:
"def callback(result=Cr.NS_OK, message=None, success=None): if success is None: <IF_STMT> success = Ci.koIAsyncCallback.RESULT_SUCCESSFUL else: success = Ci.koIAsyncCallback.RESULT_ERROR data = Namespace(result=result, message=message, _com_interfaces_=[Ci.koIErrorInfo]) self._invoke_activate_callbacks(success, data)",if Cr.NS_SUCCEEDED(result):
def get_location(device): location = [] node = device while node: position = node.get_position() or '' <IF_STMT> position = ' [%s]' % position location.append(node.name + position) node = node.parent return ' / '.join(reversed(location)),if position:
"def load_checkpoint(path, model, optimizer, reset_optimizer): global global_step global global_epoch print('Load checkpoint from: {}'.format(path)) checkpoint = _load(path) model.load_state_dict(checkpoint['state_dict']) if not reset_optimizer: optimizer_state = checkpoint['optimizer'] <IF_STMT> print('Load optimizer state from {}'.format(path)) optimizer.load_state_dict(checkpoint['optimizer']) global_step = checkpoint['global_step'] global_epoch = checkpoint['global_epoch'] return model",if optimizer_state is not None:
"def run_command(self, command: str, data: Dict[str, object]) -> Dict[str, object]: """"""Run a specific command from the registry."""""" key = 'cmd_' + command method = getattr(self.__class__, key, None) if method is None: return {'error': ""Unrecognized command '%s'"" % command} else: <IF_STMT> del data['is_tty'] del data['terminal_width'] return method(self, **data)","if command not in {'check', 'recheck', 'run'}:"
"def call_init(self, node, instance): for b in instance.bindings: <IF_STMT> continue self._initialized_instances.add(b.data) node = self._call_init_on_binding(node, b) return node",if b.data in self._initialized_instances:
"def get_request_headers() -> Dict: url = urlparse(uri) candidates = ['%s://%s' % (url.scheme, url.netloc), '%s://%s/' % (url.scheme, url.netloc), uri, '*'] for u in candidates: <IF_STMT> headers = dict(DEFAULT_REQUEST_HEADERS) headers.update(self.config.linkcheck_request_headers[u]) return headers return {}",if u in self.config.linkcheck_request_headers:
"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: video_packet = self._get_video_packet() if video_packet.image == 0: self._decode_video_packet(video_packet) <IF_STMT> break if _debug: print('Returning', video_packet) return video_packet.image",if video_packet.image is not None or not skip_empty_frame:
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) <IF_STMT> ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",elif code == Path.CURVE3:
"def __init__(self, layout, value=None, string=None, *, dtype: np.dtype=np.float64) -> None: """"""Constructor."""""" self.layout = layout if value is None: if string is None: self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) <IF_STMT> raise ValueError('value must be a sequence of length %s' % self.layout.gaDims)","if self.value.shape != (self.layout.gaDims,):"
"def to_dict(self): contexts_ = {} for k, data in self.contexts.items(): data_ = data.copy() if 'context' in data_: del data_['context'] <IF_STMT> del data_['loaded'] contexts_[k] = data_ return dict(contexts=contexts_)",if 'loaded' in data_:
def include_module(module): if not include_these: return True result = False for check in include_these: if '/*' in check: <IF_STMT> result = True elif os.getcwd() + '/' + check + '.py' == module: result = True if result: print_status('Including module: ' + module) return result,if check[:-1] in module:
"def extract_from(msg_body, content_type='text/plain'): try: if content_type == 'text/plain': return extract_from_plain(msg_body) <IF_STMT> return extract_from_html(msg_body) except Exception: log.exception('ERROR extracting message') return msg_body",elif content_type == 'text/html':
"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data['features']), 2) for feature in response.data['features']: self.assertIn('bbox', feature) fid = feature['id'] <IF_STMT> self.assertEqual(feature['bbox'], self.bl1.bbox_geometry.extent) elif fid == 2: self.assertEqual(feature['bbox'], self.bl2.bbox_geometry.extent) else: self.fail('Unexpected id: {0}'.format(fid)) BoxedLocation.objects.all().delete()",if fid == 1:
"def overrideCommand(self, commandName, func): k = self d = k.masterBindingsDict for key in d: d2 = d.get(key) for key2 in d2: bi = d2.get(key2) <IF_STMT> bi.func = func d2[key2] = bi",if bi.commandName == commandName:
"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) <IF_STMT> r = _lookup(comps, specs, provided, name, i + 1, l) if r is not None: return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",if comps:
"def to_representation(self, value): old_social_string_fields = ['twitter', 'github', 'linkedIn'] request = self.context.get('request') show_old_format = request and is_deprecated(request.version, self.min_version) and (request.method == 'GET') if show_old_format: social = value.copy() for key in old_social_string_fields: if social.get(key): social[key] = value[key][0] <IF_STMT> social[key] = '' value = social return super(SocialField, self).to_representation(value)",elif social.get(key) == []:
"def process_ref_attribute(self, node, array_type=None): ref = qname_attr(node, 'ref') if ref: ref = self._create_qname(ref) <IF_STMT> return return xsd_elements.RefAttribute(node.tag, ref, self.schema, array_type=array_type)",if ref.namespace == 'http://www.w3.org/2001/XMLSchema':
"def unescape(text): """"""Removes '\\' escaping from 'text'."""""" rv = '' i = 0 while i < len(text): <IF_STMT> rv += text[i + 1] i += 1 else: rv += text[i] i += 1 return rv",if i + 1 < len(text) and text[i] == '\\':
"def wait_child_process(signum, frame): try: while True: child_pid, status = os.waitpid(-1, os.WNOHANG) if child_pid == 0: stat_logger.info('no child process was immediately available') break exitcode = status >> 8 stat_logger.info('child process %s exit with exitcode %s', child_pid, exitcode) except OSError as e: <IF_STMT> stat_logger.warning('current process has no existing unwaited-for child processes.') else: raise",if e.errno == errno.ECHILD:
"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) if ctg[0] == 'L' and unicodedata.name(c).find('LATIN') == -1: for separator in (' & ', '; ', ' and ', ' vs. ', ' with ', ' y '): <IF_STMT> parts = sortname.split(separator) break else: parts = [sortname] separator = '' return separator.join(map(_reverse_sortname, parts)) return name",if separator in sortname:
"def python_value(self, value): if value: <IF_STMT> pp = lambda x: x.time() return format_date_time(value, self.formats, pp) elif isinstance(value, datetime.datetime): return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value","if isinstance(value, basestring):"
"def __init__(self, fileobj, info): pages = [] complete = False while not complete: page = OggPage(fileobj) <IF_STMT> pages.append(page) complete = page.complete or len(page.packets) > 1 data = OggPage.to_packets(pages)[0][7:] super(OggTheoraCommentDict, self).__init__(data, framing=False) self._padding = len(data) - self._size",if page.serial == info.serial:
"def configure(self): if 'from_' in self.wmeta.properties: from_ = float(self.wmeta.properties['from_']) to = float(self.wmeta.properties.get('to', 0)) <IF_STMT> to = from_ + 1 self.wmeta.properties['to'] = str(to) super(TKSpinbox, self).configure()",if from_ > to:
def get_error_diagnostics(self): diagnostics = [] if self.stdout is not None: with open(self.stdout.name) as fds: contents = fds.read().strip() <IF_STMT> diagnostics.append('ab STDOUT:\n' + contents) if self.stderr is not None: with open(self.stderr.name) as fds: contents = fds.read().strip() if contents.strip(): diagnostics.append('ab STDERR:\n' + contents) return diagnostics,if contents.strip():
"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): <IF_STMT> if os.getenv('TRUSTED_HOST') and should_rebase_environment_value(name): value = file_host.rebase_to_worker_root(value) env[name] = value",if is_forwarded_environment_variable(name):
"def update_content(self, more_content: StringList) -> None: if isinstance(self.object, TypeVar): attrs = [repr(self.object.__name__)] for constraint in self.object.__constraints__: attrs.append(stringify_typehint(constraint)) if self.object.__covariant__: attrs.append('covariant=True') <IF_STMT> attrs.append('contravariant=True') more_content.append(_('alias of TypeVar(%s)') % ', '.join(attrs), '') more_content.append('', '') super().update_content(more_content)",if self.object.__contravariant__:
"def after(self, event, state): group = event.group for plugin in self.get_plugins(): <IF_STMT> continue metrics.incr('notifications.sent', instance=plugin.slug) yield self.future(plugin.rule_notify)","if not safe_execute(plugin.should_notify, group=group, event=event):"
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): <IF_STMT> n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) if not isinstance(n, _strtypes): raise TypeError('on must be a name or field, not: {0}'.format(n)) elif n not in fields: raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))",if n._child.isidentical(expr):
"def build_filter(arg): filt = {} if arg is not None: <IF_STMT> raise UserError('Arguments to --filter should be in form KEY=VAL') key, val = arg.split('=', 1) filt[key] = val return filt",if '=' not in arg:
"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break <IF_STMT> text = line[len(key) + 1:] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",if prog.match(line):
"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: <IF_STMT> category = 'preprint' elif node.is_registration: category = 'registration' else: category = node.project_or_component client().delete(index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404])","if isinstance(node, Preprint):"
"def update(self, preds, labels): if not _is_numpy_(labels): raise ValueError(""The 'labels' must be a numpy ndarray."") if not _is_numpy_(preds): raise ValueError(""The 'predictions' must be a numpy ndarray."") for i, lbl in enumerate(labels): value = preds[i, 1] bin_idx = int(value * self._num_thresholds) assert bin_idx <= self._num_thresholds <IF_STMT> self._stat_pos[bin_idx] += 1.0 else: self._stat_neg[bin_idx] += 1.0",if lbl:
def checkStatusClient(self): if str(self.comboxBoxIPAddress.currentText()) != '': <IF_STMT> self.btnEnable.setEnabled(False) self.btncancel.setEnabled(True) return None self.btnEnable.setEnabled(True) self.btncancel.setEnabled(False),if self.ClientsLogged[str(self.comboxBoxIPAddress.currentText())]['Status']:
"def colorizeDiffs(sheet, col, row, cellval): if not row or not col: return None vcolidx = sheet.visibleCols.index(col) rowidx = sheet.rows.index(row) if vcolidx < len(othersheet.visibleCols) and rowidx < len(othersheet.rows): otherval = othersheet.visibleCols[vcolidx].getDisplayValue(othersheet.rows[rowidx]) <IF_STMT> return 'color_diff' else: return 'color_diff_add'",if cellval.display != otherval:
"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info('Checking for %s' % wafvendor) <IF_STMT> detected.append(wafvendor) if not findall: break self.knowledge['wafname'] = detected return detected",if self.wafdetections[wafvendor](self):
"def get_repository_metadata_by_repository_id_changeset_revision(app, id, changeset_revision, metadata_only=False): """"""Get a specified metadata record for a specified repository in the tool shed."""""" if metadata_only: repository_metadata = get_repository_metadata_by_changeset_revision(app, id, changeset_revision) <IF_STMT> return repository_metadata.metadata return None return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)",if repository_metadata and repository_metadata.metadata:
def getmultiline(self): line = self.getline() if line[3:4] == '-': code = line[:3] while 1: nextline = self.getline() line = line + ('\n' + nextline) <IF_STMT> break return line,if nextline[:3] == code and nextline[3:4] != '-':
"def _validate_reports(value, *args, **kwargs): from osf.models import OSFUser for key, val in value.items(): if not OSFUser.load(key): raise ValidationValueError('Keys must be user IDs') <IF_STMT> raise ValidationTypeError('Values must be dictionaries') if 'category' not in val or 'text' not in val or 'date' not in val or ('retracted' not in val): raise ValidationValueError(('Values must include `date`, `category`, ', '`text`, `retracted` keys'))","if not isinstance(val, dict):"
"def deselectItem(self, item): if self.isSelected(item): <IF_STMT> listItem = self._getListItem(item) selections = self.getSelectedItems() selections.remove(self.loadHandler.getSelection(listItem)) self.setSelections(selections) else: self.deselectAll()",if self.multiSelect:
"def __init__(self, **kwargs): if self.name is None: raise RuntimeError('RenderPrimitive cannot be used directly') self.option_values = {} for key, val in kwargs.items(): if not key in self.options: raise ValueError(""primitive `{0}' has no option `{1}'"".format(self.name, key)) self.option_values[key] = val for name, (description, default) in self.options.items(): <IF_STMT> self.option_values[name] = default",if not name in self.option_values:
"def setup_smart_indent(self, view, lang): if type(view) == gedit.View: <IF_STMT> setattr(view, 'smart_indent_instance', SmartIndent()) handler_id = view.connect('key-press-event', view.smart_indent_instance.key_press_handler) self.handler_ids.append((handler_id, view)) view.smart_indent_instance.set_language(lang, view)","if getattr(view, 'smart_indent_instance', False) == False:"
"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = '' strings = [] for char in word: <IF_STMT> letters += char count += 1 else: if count > threshold: strings.append(letters) letters = '' count = 0 if count > threshold: strings.append(letters) return strings",if char in char_set:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_logout_url(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
def __create_table(self): for i in range(256): crcreg = i for j in range(8): <IF_STMT> crcreg = self.__CRCPOLYNOMIAL ^ crcreg >> 1 else: crcreg >>= 1 self.__crctable[i] = crcreg,if crcreg & 1 != 0:
"def destroy(self): """"""Flush all entries and empty cache"""""" for i in range(len(self.cached_rows)): id_ = self.cached_rows[i] self.cached_rows[i] = None <IF_STMT> try: inode = self.attrs[id_] except KeyError: pass else: del self.attrs[id_] self.setattr(inode) assert len(self.attrs) == 0",if id_ is not None:
"def set_config(self): """"""Set configuration options for QTextEdit."""""" c = self.c w = self.widget w.setWordWrapMode(QtGui.QTextOption.NoWrap) if 0: n = c.config.getInt('qt-rich-text-zoom-in') <IF_STMT> w.zoomIn(n) w.updateMicroFocus() w.setTabStopWidth(24)","if n not in (None, 0):"
"def mouseDragEvent(self, ev): if self.movable and ev.button() == QtCore.Qt.LeftButton: if ev.isStart(): self.moving = True self.cursorOffset = self.pos() - self.mapToParent(ev.buttonDownPos()) self.startPosition = self.pos() ev.accept() if not self.moving: return self.setPos(self.cursorOffset + self.mapToParent(ev.pos())) self.sigDragged.emit(self) <IF_STMT> self.moving = False self.sigPositionChangeFinished.emit(self)",if ev.isFinish():
"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: if not newParent._element.text: newParent._element.text = '' <IF_STMT> newParent._element.text += self._element.text self._element.text = '' base.Node.reparentChildren(self, newParent)",if self._element.text is not None:
"def _no_sp_or_bp(self, bl): for s in bl.vex.statements: for e in chain([s], s.expressions): <IF_STMT> reg = self.get_reg_name(self.project.arch, e.offset) if reg == 'ebp' or reg == 'esp': return False elif e.tag == 'Ist_Put': reg = self.get_reg_name(self.project.arch, e.offset) if reg == 'ebp' or reg == 'esp': return False return True",if e.tag == 'Iex_Get':
"def _get_import_chain(self, *, until=None): stack = inspect.stack()[2:] try: for frameinfo in stack: try: <IF_STMT> continue data = dedent(''.join(frameinfo.code_context)) if data.strip() == until: raise StopIteration yield (frameinfo.filename, frameinfo.lineno, data.strip()) del data finally: del frameinfo finally: del stack",if not frameinfo.code_context:
def stream_docker_log(log_stream): async for line in log_stream: if 'stream' in line and line['stream'].strip(): logger.debug(line['stream'].strip()) elif 'status' in line: logger.debug(line['status'].strip()) <IF_STMT> logger.error(line['error'].strip()) raise DockerBuildError,elif 'error' in line:
"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node['deps']: if dep == goal_node_index: return [curr_node['address']] for dep in curr_node['deps']: path = self.get_cycle_path(self.get_by_address(dep), goal_node_index) <IF_STMT> path.insert(0, curr_node['address']) return path return []",if len(path) > 0:
"def prompt(default=None): editor = 'nano' with tempfile.NamedTemporaryFile(mode='r+') as tmpfile: <IF_STMT> tmpfile.write(default) tmpfile.flush() child_pid = os.fork() is_child = child_pid == 0 if is_child: os.execvp(editor, [editor, tmpfile.name]) else: os.waitpid(child_pid, 0) tmpfile.seek(0) return tmpfile.read().strip()",if default:
"def _get_annotated_template(self, template): changed = False if template.get('version', '0.12.0') >= '0.13.0': using_js = self.spider._filter_js_urls(template['url']) body = 'rendered_body' if using_js else 'original_body' <IF_STMT> template['body'] = body changed = True if changed or not template.get('annotated'): _build_sample(template) return template",if template.get('body') != body:
"def collect(self, paths): for path in paths or (): relpath = os.path.relpath(path, self._artifact_root) dst = os.path.join(self._directory, relpath) safe_mkdir(os.path.dirname(dst)) <IF_STMT> shutil.copytree(path, dst) else: shutil.copy(path, dst) self._relpaths.add(relpath)",if os.path.isdir(path):
"def dependencies(context=None): """"""Return all dependencies detected by knowit."""""" deps = OrderedDict([]) try: initialize(context) for name, provider_cls in _provider_map.items(): <IF_STMT> deps[name] = available_providers[name].version else: deps[name] = {} except Exception: pass return deps",if name in available_providers:
"def _getaddrinfo(self, host_bytes, port, family, socktype, proto, flags): while True: ares = self.cares try: return self.__getaddrinfo(host_bytes, port, family, socktype, proto, flags) except gaierror: <IF_STMT> raise",if ares is self.cares:
"def write_entries(cmd, basename, filename): ep = cmd.distribution.entry_points if isinstance(ep, basestring) or ep is None: data = ep elif ep is not None: data = [] for section, contents in ep.items(): <IF_STMT> contents = EntryPoint.parse_group(section, contents) contents = '\n'.join(map(str, contents.values())) data.append('[%s]\n%s\n\n' % (section, contents)) data = ''.join(data) cmd.write_or_delete_file('entry points', filename, data, True)","if not isinstance(contents, basestring):"
"def _highlight_do(self): new_hl_text = self.highlight_text.text() if new_hl_text != self.hl_text: self.hl_text = new_hl_text if self.hl is not None: self.hl.setDocument(None) self.hl = None <IF_STMT> self.hl = Highlighter(self.hl_text, parent=self.doc) self.clear_highlight_button.setEnabled(bool(self.hl))",if self.hl_text:
"def traverse(node, functions=[]): if hasattr(node, 'grad_fn'): node = node.grad_fn if hasattr(node, 'variable'): node = graph.nodes_by_id.get(id(node.variable)) <IF_STMT> node.functions = list(functions) del functions[:] if hasattr(node, 'next_functions'): functions.append(type(node).__name__) for f in node.next_functions: if f[0]: functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, 'saved_tensors'): for t in node.saved_tensors: traverse(t)",if node:
"def compress(self, data_list): if data_list: page_id = data_list[1] if page_id in EMPTY_VALUES: <IF_STMT> return None raise forms.ValidationError(self.error_messages['invalid_page']) return Page.objects.get(pk=page_id) return None",if not self.required:
"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): if name == 'Index': continue x = item() <IF_STMT> self.assertEqual(type(x._fields), tuple)","if isinstance(x, ast.AST):"
"def handle_starttag(self, tag, attrs): if tag == 'base': self.base_url = dict(attrs).get('href') if self.scan_tag(tag): for attr, value in attrs: if self.scan_attr(attr): <IF_STMT> value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",if self.strip:
"def _initialize_asset_map(cls): cls._asset_name_to_path = {} assets = os.listdir(ASSETS_PATH) for asset in assets: path = os.path.join(ASSETS_PATH, asset) <IF_STMT> cls._asset_name_to_path[os.path.basename(path)] = path",if os.path.isfile(path):
"def dataReceived(self, data): self.buf += data if self._paused: log.startLogging(sys.stderr) log.msg('dataReceived while transport paused!') self.transport.loseConnection() else: self.transport.write(data) <IF_STMT> self.transport.loseConnection() else: self.pause()",if self.buf.endswith(b'\n0\n'):
def test_case_sensitive(self): with support.EnvironmentVarGuard() as env: env.unset('PYTHONCASEOK') <IF_STMT> self.skipTest('os.environ changes not reflected in _os.environ') loader = self.find_module() self.assertIsNone(loader),if b'PYTHONCASEOK' in _bootstrap_external._os.environ:
"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: if not self.exists(self.manifest_path): return {} mtime = self.getmtime(self.manifest_path) <IF_STMT> self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",if self._mtime is None or mtime > self._mtime:
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): <IF_STMT> self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == 'mean_module.constant': self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == 'covar_module.base_kernel.raw_lengthscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive)",if name == 'likelihood.noise_covar.raw_noise':
"def process_plugin_result(name, result): if result: try: jsonify(test=result) except Exception: logger.exception('Error while jsonifying settings from plugin {}, please contact the plugin author about this'.format(name)) raise else: <IF_STMT> del result['__enabled'] data[name] = result",if '__enabled' in result:
"def benchmarking(net, ctx, num_iteration, datashape=300, batch_size=64): input_shape = (batch_size, 3) + (datashape, datashape) data = mx.random.uniform(-1.0, 1.0, shape=input_shape, ctx=ctx, dtype='float32') dryrun = 5 for i in range(dryrun + num_iteration): <IF_STMT> tic = time.time() ids, scores, bboxes = net(data) ids.asnumpy() scores.asnumpy() bboxes.asnumpy() toc = time.time() - tic return toc",if i == dryrun:
"def merge_weekdays(base_wd, icu_wd): result = [] for left, right in zip(base_wd, icu_wd): <IF_STMT> result.append(left) continue left = set(left.split('|')) right = set(right.split('|')) result.append('|'.join(left | right)) return result",if left == right:
"def create_key(self, request): if self._ignored_parameters: url, body = self._remove_ignored_parameters(request) else: url, body = (request.url, request.body) key = hashlib.sha256() key.update(_to_bytes(request.method.upper())) key.update(_to_bytes(url)) if request.body: key.update(_to_bytes(body)) el<IF_STMT> for name, value in sorted(request.headers.items()): key.update(_to_bytes(name)) key.update(_to_bytes(value)) return key.hexdigest()",if self._include_get_headers and request.headers != _DEFAULT_HEADERS:
"def test_invalid_mountinfo(self): line = '20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-rootrw,errors=remount-ro,data=ordered' elements = line.split() for i in range(len(elements) + 1): lines = [' '.join(elements[0:i])] <IF_STMT> expected = None else: expected = ('/dev/mapper/vg0-root', 'ext4', '/') self.assertEqual(expected, util.parse_mount_info('/', lines))",if i < 10:
"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) if k in keep_current: if keep_nested is not None: <IF_STMT> yield (k, dict(self.nested_filter(v.items(), keep_nested))) else: yield (k, v)","if isinstance(v, dict):"
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos['reach_leaf_node'].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos['node_pos'][t_idx] if cur_node_idx == -1: continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(tree, sample, cur_node_idx) <IF_STMT> node_pos['reach_leaf_node'][t_idx] = True node_pos['node_pos'][t_idx] = rs return node_pos",if reach_leaf:
"def _pop_waiting_trial_id(self) -> Optional[int]: for trial in self._storage.get_all_trials(self._study_id, deepcopy=False): <IF_STMT> continue if not self._storage.set_trial_state(trial._trial_id, TrialState.RUNNING): continue _logger.debug('Trial {} popped from the trial queue.'.format(trial.number)) return trial._trial_id return None",if trial.state != TrialState.WAITING:
"def get_step_best(self, step_models): best_score = None best_model = '' for model in step_models: model_info = self.models_trained[model] score = model_info.get_score() <IF_STMT> continue if best_score is None or score < best_score: best_score = score best_model = model LOGGER.info(f'step {self.n_step}, best model {best_model}') return best_model",if score is None:
"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() if f is not None and f.type in ('or', 'and', 'not'): <IF_STMT> queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f",if block_end:
"def _buffer_decode(self, input, errors, final): if self.decoder is None: output, consumed, byteorder = codecs.utf_16_ex_decode(input, errors, 0, final) if byteorder == -1: self.decoder = codecs.utf_16_le_decode <IF_STMT> self.decoder = codecs.utf_16_be_decode elif consumed >= 2: raise UnicodeError('UTF-16 stream does not start with BOM') return (output, consumed) return self.decoder(input, self.errors, final)",elif byteorder == 1:
"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) <IF_STMT> data = StringIO() if self.encryptor: self.encryptor.decrypt(db, data) else: raise EncryptionError('Encrpyted credential storage: {}'.format(self.db)) return json.loads(data.getvalue()) else: return json.load(db) except: return {'creds': []}",if content == 'Salted__':
"def _getbytes(self, start, l=1): out = [] for ad in range(l): offset = ad + start + self.base_address <IF_STMT> raise IOError('not enough bytes') out.append(int_to_byte(Byte(offset))) return b''.join(out)",if not is_mapped(offset):
def cache_sqs_queues_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get('environment') == 'prod': cache_sqs_queues_for_account.delay(account_id) el<IF_STMT> cache_sqs_queues_for_account.delay(account_id) stats.count(f'{function}.success') return True,"if account_id in config.get('celery.test_account_ids', []):"
"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: <IF_STMT> row[refnum + 1] = line return elif row[0] > linenum: break self.rows.insert(i, self.newRow(linenum, refnum, line))",if row[refnum + 1] is None:
"def __setattr__(self, name, val): if self.__dict__.get(name, 'hamster_graphics_no_value_really') == val: return Sprite.__setattr__(self, name, val) if name == 'image_data': self._surface = None <IF_STMT> self.__dict__['width'] = self.image_data.get_width() self.__dict__['height'] = self.image_data.get_height()",if self.image_data:
"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz * (1 - sqr(tanh(x))),)",if x.type in discrete_types:
"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + ':[y/n]') <IF_STMT> return True if output.lower() == 'n': return False",if output.lower() == 'y':
"def replace_documentation_for_matching_shape(self, event_name, section, **kwargs): if self._shape_name == section.context.get('shape'): self._replace_documentation(event_name, section) for section_name in section.available_sections: sub_section = section.get_section(section_name) <IF_STMT> self._replace_documentation(event_name, sub_section) else: self.replace_documentation_for_matching_shape(event_name, sub_section)",if self._shape_name == sub_section.context.get('shape'):
"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + ':[y/n]') if output.lower() == 'y': return True <IF_STMT> return False",if output.lower() == 'n':
"def __getitem__(self, index): if self._check(): if isinstance(index, int): <IF_STMT> raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) if feature: feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",if index < 0 or index >= len(self.features):
"def _parse_locator(self, locator): prefix = None criteria = locator if not locator.startswith('//'): locator_parts = locator.partition('=') <IF_STMT> prefix = locator_parts[0] criteria = locator_parts[2].strip() return (prefix, criteria)",if len(locator_parts[1]) > 0:
"def trakt_episode_data_generate(self, data): uniqueSeasons = [] for season, episode in data: <IF_STMT> uniqueSeasons.append(season) seasonsList = [] for searchedSeason in uniqueSeasons: episodesList = [] for season, episode in data: if season == searchedSeason: episodesList.append({'number': episode}) seasonsList.append({'number': searchedSeason, 'episodes': episodesList}) post_data = {'seasons': seasonsList} return post_data",if season not in uniqueSeasons:
"def __init__(self, data, n_bins): bin_width = span / n_bins bins = [0] * n_bins for x in data: b = int(mpfloor((x - minimum) / bin_width)) <IF_STMT> b = 0 elif b >= n_bins: b = n_bins - 1 bins[b] += 1 self.bins = bins self.bin_width = bin_width",if b < 0:
"def infer_context(typ, context='http://schema.org'): parsed_context = urlparse(typ) if parsed_context.netloc: base = ''.join([parsed_context.scheme, '://', parsed_context.netloc]) <IF_STMT> context = urljoin(base, parsed_context.path) typ = parsed_context.fragment.strip('/') elif parsed_context.path: context = base typ = parsed_context.path.strip('/') return (context, typ)",if parsed_context.path and parsed_context.fragment:
"def parse(self, items): for index, item in enumerate(items): keys = self.build_key(item) if keys is None: continue self.items[tuple(keys)] = (index, item) <IF_STMT> log.info('Unable to update table (keys: %r)', keys)","if not self.path_set(self.table, keys, (index, item)):"
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == 'layers': child = dict_to_XML('layer', val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: <IF_STMT> child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",if tag == 'config':
"def _get_config_value(self, section, key): if section: <IF_STMT> self.log.error(""Error: Config section '%s' not found"", section) return None return self.config[section].get(key, self.config[key]) else: return self.config[key]",if section not in self.config:
"def h_line_down(self, input): end_this_line = self.value.find('\n', self.cursor_position) if end_this_line == -1: if self.scroll_exit: self.h_exit_down(None) else: self.cursor_position = len(self.value) else: self.cursor_position = end_this_line + 1 for x in range(self.cursorx): <IF_STMT> break elif self.value[self.cursor_position] == '\n': break else: self.cursor_position += 1",if self.cursor_position > len(self.value) - 1:
"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) <IF_STMT> break if isinstance(data, str): data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write('%s: I/O error: %s\n' % (filename, msg)) return 1 out.write('%s %s\n' % (m.hexdigest(), filename)) return 0",if not data:
"def main(input): logging.info('Running Azure Cloud Custodian Policy %s', input) context = {'config_file': join(function_directory, 'config.json'), 'auth_file': join(function_directory, 'auth.json')} event = None subscription_id = None if isinstance(input, QueueMessage): <IF_STMT> return event = input.get_json() subscription_id = ResourceIdParser.get_subscription_id(event['subject']) handler.run(event, context, subscription_id)",if input.dequeue_count > max_dequeue_count:
"def maybeExtractTarball(self): if self.tarball: tar = self.computeTarballOptions() + ['-xvf', self.tarball] res = (yield self._Cmd(tar, abandonOnFailure=False)) <IF_STMT> yield self._Cmd(['rm', '-f', self.tarball], abandonOnFailure=False) yield self.runRmdir(self.repoDir(), abandonOnFailure=False)",if res:
"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop('name')) action = 0 for key, val in props.get('options', {}).items(): <IF_STMT> new_action = 0 for name, _val in val.items(): action = watcher.set_opt('hooks.%s' % name, _val) if action == 1: new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 return watcher.do_action(action)",if key == 'hooks':
"def _import_playlists(self, fns, library): added = 0 for filename in fns: name = _name_for(filename) with open(filename, 'rb') as f: <IF_STMT> playlist = parse_m3u(f, name, library=library) elif filename.endswith('.pls'): playlist = parse_pls(f, name, library=library) else: print_w(""Unsupported playlist type for '%s'"" % filename) continue self.changed(playlist) library.add(playlist) added += 1 return added",if filename.endswith('.m3u') or filename.endswith('.m3u8'):
"def unwrap_term_buckets(self, timestamp, term_buckets): for term_data in term_buckets: <IF_STMT> self.unwrap_interval_buckets(timestamp, term_data['key'], term_data['interval_aggs']['buckets']) else: self.check_matches(timestamp, term_data['key'], term_data)",if 'interval_aggs' in term_data:
"def _get_exception(flags, timeout_ms, payload_size): if flags & FLAG_ERROR: if flags & FLAG_TIMEOUT: return SpicommTimeoutError(timeout_ms / 1000.0) <IF_STMT> return SpicommOverflowError(payload_size) return SpicommError() return None",if flags & FLAG_OVERFLOW:
"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): if key in self.tagged_blocks: data = self.tagged_blocks.get_data(key) for pattern in data: <IF_STMT> return pattern return None",if pattern.pattern_id == pattern_id:
"def print_quiet(self, context, *args, **kwargs): for index, (key, value) in enumerate(itertools.chain(enumerate(args), kwargs.items())): <IF_STMT> print(self.format_quiet(index, key, value, fields=context.get_input_fields()))","if self.filter(index, key, value):"
"def complete(self, block): with self._condition: if not self._final: return False if self._complete(): self._calculate_state_root_if_not_already_done() return True <IF_STMT> self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",if block:
"def compression_rotator(source, dest): with open(source, 'rb') as sf: with gzip.open(dest, 'wb') as wf: while True: data = sf.read(CHUNK_SIZE) <IF_STMT> break wf.write(data) os.remove(source)",if not data:
"def mockup(self, records): provider = TransipProvider('', '', '') _dns_entries = [] for record in records: <IF_STMT> entries_for = getattr(provider, '_entries_for_{}'.format(record._type)) name = record.name if name == '': name = provider.ROOT_RECORD _dns_entries.extend(entries_for(name, record)) _dns_entries.append(DnsEntry('@', '3600', 'NS', 'ns01.transip.nl.')) self.mockupEntries = _dns_entries",if record._type in provider.SUPPORTS:
"def parse_known_args(self, args=None, namespace=None): entrypoint = self.prog.split(' ')[0] try: defs = get_defaults_for_argparse(entrypoint) ignore = defs.pop('Ignore', None) self.set_defaults(**defs) <IF_STMT> set_notebook_diff_ignores(ignore) except ValueError: pass return super(ConfigBackedParser, self).parse_known_args(args=args, namespace=namespace)",if ignore:
"def _maybeRebuildAtlas(self, threshold=4, minlen=1000): n = len(self.fragmentAtlas) if n > minlen and n > threshold * len(self.data): self.fragmentAtlas.rebuild(list(zip(*self._style(['symbol', 'size', 'pen', 'brush'])))) self.data['sourceRect'] = 0 <IF_STMT> self._sourceQRect.clear() self.updateSpots()",if _USE_QRECT:
"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: <IF_STMT> return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None if self.quitting: raise BdbQuit if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",if self.stopframe and frame.f_code.co_flags & CO_GENERATOR:
"def tearDown(self): if not self.is_playback(): try: if self.hosted_service_name is not None: self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: <IF_STMT> self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self.storage_account_name is not None:
"def make_log_msg(self, msg, *other_messages): MAX_MESSAGE_LENGTH = 1000 if not other_messages: return msg[-MAX_MESSAGE_LENGTH:] else: if len(msg): msg += '\n...\n' NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len(msg) else: NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH <IF_STMT> msg += other_messages[0][-NEXT_MESSAGE_OFFSET:] return self.make_log_msg(msg, *other_messages[1:]) else: return self.make_log_msg(msg)",if NEXT_MESSAGE_OFFSET > 0:
"def wrapper(self: RequestHandler, *args, **kwargs) -> Optional[Awaitable[None]]: if self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path.rstrip('/') if uri: <IF_STMT> uri += '?' + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",if self.request.query:
"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == 'QTCORE': continue value = env['LIBPATH_' + var] <IF_STMT> core = env[coreval] accu = [] for lib in value: if lib in core: continue accu.append(lib) env['LIBPATH_' + var] = accu",if value:
"def _attach_children(self, other, exclude_worldbody, dry_run=False): for other_child in other.all_children(): <IF_STMT> self_child = self.get_children(other_child.spec.name) self_child._attach(other_child, exclude_worldbody, dry_run)",if not other_child.spec.repeated:
def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): if child.getchildren(): content = getDictFromTree(child) else: content = child.text <IF_STMT> if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or '') else: ret_dict[child.tag] = content or '' return ret_dict,if ret_dict.has_key(child.tag):
"def nsUriMatch(self, value, wanted, strict=0, tt=type(())): """"""Return a true value if two namespace uri values match."""""" if value == wanted or (type(wanted) is tt and value in wanted): return 1 if not strict and value is not None: wanted = type(wanted) is tt and wanted or (wanted,) value = value[-1:] != '/' and value or value[:-1] for item in wanted: <IF_STMT> return 1 return 0",if item == value or item[:-1] == value:
"def update_repository(self, ignore_issues=False, force=False): """"""Update."""""" if not await self.common_update(ignore_issues, force): return if self.repository_manifest: <IF_STMT> self.content.path.remote = '' if self.content.path.remote == 'apps': self.data.domain = get_first_directory_in_directory(self.tree, self.content.path.remote) self.content.path.remote = f'apps/{self.data.name}' self.content.path.local = self.localpath",if self.data.content_in_root:
"def addOutput(self, data, isAsync=None, **kwargs): isAsync = _get_async_param(isAsync, **kwargs) if isAsync: self.terminal.eraseLine() self.terminal.cursorBackward(len(self.lineBuffer) + len(self.ps[self.pn])) self.terminal.write(data) if isAsync: <IF_STMT> self.terminal.nextLine() self.terminal.write(self.ps[self.pn]) if self.lineBuffer: oldBuffer = self.lineBuffer self.lineBuffer = [] self.lineBufferIndex = 0 self._deliverBuffer(oldBuffer)",if self._needsNewline():
"def is_installed(self, dlc_title='') -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info('version', dlc_title) installed = True if dlc_version else False <IF_STMT> status = self.legacy_get_dlc_status(dlc_title) installed = True if status in ['installed', 'updatable'] else False elif self.install_dir and os.path.exists(self.install_dir): installed = True return installed",if not installed:
"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() <IF_STMT> if os.path.exists(sockname): os.remove(sockname) self.sock = None",if type(sockname) is str:
"def post_file(self, file_path, graph_type='edges', file_type='csv'): dataset_id = self.dataset_id tok = self.token base_path = self.server_base_path with open(file_path, 'rb') as file: out = requests.post(f'{base_path}/api/v2/upload/datasets/{dataset_id}/{graph_type}/{file_type}', verify=self.certificate_validation, headers={'Authorization': f'Bearer {tok}'}, data=file.read()).json() <IF_STMT> raise Exception(out) return out",if not out['success']:
"def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls): """"""Extract the VQA V2 image data set to directory unless it's there."""""" for url in image_urls: filename = os.path.basename(url) download_url = os.path.join(image_root_url, url) path = generator_utils.maybe_download(directory, filename, download_url) unzip_dir = os.path.join(directory, filename.strip('.zip')) <IF_STMT> zipfile.ZipFile(path, 'r').extractall(directory)",if not tf.gfile.Exists(unzip_dir):
"def __call__(self, environ, start_response): for key in ('REQUEST_URL', 'REQUEST_URI', 'UNENCODED_URL'): <IF_STMT> continue request_uri = unquote(environ[key]) script_name = unquote(environ.get('SCRIPT_NAME', '')) if request_uri.startswith(script_name): environ['PATH_INFO'] = request_uri[len(script_name):].split('?', 1)[0] break return self.app(environ, start_response)",if key not in environ:
"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) <IF_STMT> for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model","elif isinstance(value, list):"
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == '.' or i == '..': continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() <IF_STMT> filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if pattern.match(value) is not None:
"def get_text(self, nodelist): """"""Return a string representation of the motif's properties listed on nodelist ."""""" retlist = [] for node in nodelist: if node.nodeType == Node.TEXT_NODE: retlist.append(node.wholeText) <IF_STMT> retlist.append(self.get_text(node.childNodes)) return re.sub('\\s+', ' ', ''.join(retlist))",elif node.hasChildNodes:
"def _persist_metadata(self, dirname, filename): metadata_path = '{0}/{1}.json'.format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: if self.latest: self.merge_json({'GraphImages': self.posts}, metadata_path) else: self.save_json({'GraphImages': self.posts}, metadata_path) <IF_STMT> if self.latest: self.merge_json({'GraphStories': self.stories}, metadata_path) else: self.save_json({'GraphStories': self.stories}, metadata_path)",if self.stories:
"def _get_python_wrapper_content(self, job_class, args): job = job_class(['-r', 'hadoop'] + list(args)) job.sandbox() with job.make_runner() as runner: runner._create_setup_wrapper_scripts() <IF_STMT> with open(runner._spark_python_wrapper_path) as f: return f.read() else: return None",if runner._spark_python_wrapper_path:
"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: <IF_STMT> w += 1 elif ch == '\t': w += abs(tab_width) - w % abs(tab_width) else: break return w",if ch == ' ':
def run(self): for _ in range(10): <IF_STMT> break self.spawn_i3status() if not self.ready: break self.lock.wait(5),if not self.py3_wrapper.running:
"def translate_len(builder: IRBuilder, expr: CallExpr, callee: RefExpr) -> Optional[Value]: if len(expr.args) == 1 and expr.arg_kinds == [ARG_POS]: expr_rtype = builder.node_type(expr.args[0]) <IF_STMT> builder.accept(expr.args[0]) return Integer(len(expr_rtype.types)) else: obj = builder.accept(expr.args[0]) return builder.builtin_len(obj, -1) return None","if isinstance(expr_rtype, RTuple):"
"def parse_auth(val): if val is not None: authtype, params = val.split(' ', 1) if authtype in known_auth_schemes: <IF_STMT> pass else: params = parse_auth_params(params) return (authtype, params) return val","if authtype == 'Basic' and '""' not in params:"
"def toxml(self): text = self.value self.parent.setBidi(getBidiType(text)) if not text.startswith(HTML_PLACEHOLDER_PREFIX): if self.parent.nodeName == 'p': text = text.replace('\n', '\n   ') <IF_STMT> text = '\n ' + text.replace('\n', '\n ') text = self.doc.normalizeEntities(text) return text",elif self.parent.nodeName == 'li' and self.parent.childNodes[0] == self:
"def get_all_related_many_to_many_objects(self): try: return self._all_related_many_to_many_objects except AttributeError: rel_objs = [] for klass in get_models(): for f in klass._meta.many_to_many: <IF_STMT> rel_objs.append(RelatedObject(f.rel.to, klass, f)) self._all_related_many_to_many_objects = rel_objs return rel_objs",if f.rel and self == f.rel.to._meta:
"def state_highstate(self, state, dirpath): opts = copy.copy(self.config) opts['file_roots'] = dict(base=[dirpath]) HIGHSTATE = HighState(opts) HIGHSTATE.push_active() try: high, errors = HIGHSTATE.render_highstate(state) <IF_STMT> import pprint pprint.pprint('\n'.join(errors)) pprint.pprint(high) out = HIGHSTATE.state.call_high(high) finally: HIGHSTATE.pop_active()",if errors:
"def _update_target_host(self, target, target_host): """"""Update target host."""""" target_host = None if target_host == '' else target_host if not target_host: for device_type, tgt in target.items(): <IF_STMT> target_host = tgt break if not target_host: target_host = 'llvm' if tvm.runtime.enabled('llvm') else 'stackvm' if isinstance(target_host, str): target_host = tvm.target.Target(target_host) return target_host",if device_type.value == tvm.nd.cpu(0).device_type:
def __console_writer(self): while True: self.__writer_event.wait() self.__writer_event.clear() if self.__console_view: <IF_STMT> self.log.debug('Writing console view to STDOUT') sys.stdout.write(self.console_markup.clear) sys.stdout.write(self.__console_view) sys.stdout.write(self.console_markup.TOTAL_RESET),if not self.short_only:
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break elif p: p.moveToThreadBack() <IF_STMT> break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",elif wrapped:
"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): if vm_details == 'Absent': query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) <IF_STMT> query_map.pop(alias) return query_map",if not query_map[alias]:
"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) <IF_STMT> continue if pname not in already_test and '/' not in pname: already_test.append(pname) if is_shadowing(pname): shadow_pkgs.add(pname) return shadow_pkgs",if fname or (pname and fname):
"def make_chains(chains_info): chains = [[] for _ in chains_info[0][1]] for i, num_ids in enumerate(chains_info[:-1]): num, ids = num_ids for j, ident in enumerate(ids): <IF_STMT> next_chain_info = chains_info[i + 1] previous = next_chain_info[1][j] block = SimpleBlock(num, ident, previous) chains[j].append(block) chains = {i: make_generator(chain) for i, chain in enumerate(chains)} return chains",if ident != '':
"def filter_input(mindate, maxdate, files): mindate = parse(mindate) if mindate is not None else datetime.datetime.min maxdate = parse(maxdate) if maxdate is not None else datetime.datetime.max for line in fileinput.input(files): tweet = json.loads(line) created_at = parse(tweet['created_at']) created_at = created_at.replace(tzinfo=None) <IF_STMT> print(json.dumps(tweet))",if mindate < created_at and maxdate > created_at:
"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: if self._exception is None: return self.value getcurrent().throw(*self._exception) else: <IF_STMT> raise ConcurrentObjectUseError('This Waiter is already used by %r' % (self.greenlet,)) self.greenlet = getcurrent() try: return self.hub.switch() finally: self.greenlet = None",if self.greenlet is not None:
"def default_loader(href, parse, encoding=None): with open(href) as file: <IF_STMT> data = ElementTree.parse(file).getroot() else: data = file.read() if encoding: data = data.decode(encoding) return data",if parse == 'xml':
def is_all_qud(world): m = True for obj in world: <IF_STMT> if obj.nice: m = m and True else: m = m and False else: m = m and True return m,if obj.blond:
"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed) return for region in self.view.sel(): <IF_STMT> continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",if region.empty():
"def add_fields(self, params): for key, val in params.iteritems(): <IF_STMT> new_params = {} for k in val: new_params['%s__%s' % (key, k)] = val[k] self.add_fields(new_params) else: self.add_field(key, val)","if isinstance(val, dict):"
"def find_magic(self, f, pos, magic): f.seek(pos) block = f.read(32 * 1024) if len(block) < len(magic): return -1 p = block.find(magic) while p < 0: pos += len(block) - len(magic) + 1 block = block[1 - len(magic):] + f.read(32 << 10) <IF_STMT> return -1 p = block.find(magic) return pos + p",if len(block) == len(magic) - 1:
"def check_strings(self): """"""Check that all strings have been consumed."""""" for i, aList in enumerate(self.string_tokens): <IF_STMT> g.trace('warning: line %s. unused strings' % i) for z in aList: print(self.dump_token(z))",if aList:
"def get_tokens_unprocessed(self, text): from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, COCOA_PROTOCOLS, COCOA_PRIMITIVES for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): <IF_STMT> if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS or value in COCOA_PRIMITIVES: token = Name.Builtin.Pseudo yield (index, token, value)",if token is Name or token is Name.Class:
"def key_from_key_value_dict(key_info): res = [] if not 'key_value' in key_info: return res for value in key_info['key_value']: <IF_STMT> e = base64_to_long(value['rsa_key_value']['exponent']) m = base64_to_long(value['rsa_key_value']['modulus']) key = RSA.construct((m, e)) res.append(key) return res",if 'rsa_key_value' in value:
"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) <IF_STMT> self.view.replace(edit, region, prefixed) return for region in self.view.sel(): if region.empty(): continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",if prefixed:
def finalize(self): if self.ct < 1: return elif self.ct == 1: return 0 total = ct = 0 dtp = None while self.heap: <IF_STMT> if dtp is None: dtp = heapq.heappop(self.heap) continue dt = heapq.heappop(self.heap) diff = dt - dtp ct += 1 total += total_seconds(diff) dtp = dt return float(total) / ct,if total == 0:
"def _test_configuration(self): config_path = self._write_config() try: self._log.debug('testing configuration') verboseflag = '-Q' <IF_STMT> verboseflag = '-v' p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, '-f', config_path]) if p.wait() != 0: raise RuntimeError('configuration test failed') self._log.debug('configuration seems ok') finally: os.remove(config_path)",if self._log.isEnabledFor(logging.DEBUG):
"def exe(self, ret): if not ret: self.assertEqual(ret, '') else: assert os.path.isabs(ret), ret <IF_STMT> assert os.path.isfile(ret), ret if hasattr(os, 'access') and hasattr(os, 'X_OK'): self.assertTrue(os.access(ret, os.X_OK))",if POSIX:
"def _do_cleanup(sg_name, device_id): masking_view_list = self.rest.get_masking_views_from_storage_group(array, sg_name) for masking_view in masking_view_list: <IF_STMT> self.rest.delete_masking_view(array, masking_view) self.rest.remove_vol_from_sg(array, sg_name, device_id, extra_specs) self.rest.delete_volume(array, device_id) self.rest.delete_storage_group(array, sg_name)",if 'STG-' in masking_view:
"def hide_tooltip_if_necessary(self, key): """"""Hide calltip when necessary"""""" try: calltip_char = self.get_character(self.calltip_position) before = self.is_cursor_before(self.calltip_position, char_offset=1) other = key in (Qt.Key_ParenRight, Qt.Key_Period, Qt.Key_Tab) <IF_STMT> QToolTip.hideText() except (IndexError, TypeError): QToolTip.hideText()","if calltip_char not in ('?', '(') or before or other:"
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {'HasMoreTags': False, 'Tags': tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): if limit and len(tags) >= limit: result['HasMoreTags'] = True break <IF_STMT> continue tags.append({'Key': key, 'Value': val}) return result",if exclusive_start_tag_key and key < exclusive_start_tag_key:
"def parametrize_function_name(request, function_name): suffixes = [] if 'parametrize' in request.keywords: argnames = request.keywords['parametrize'].args[::2] argnames = [x.strip() for names in argnames for x in names.split(',')] for name in argnames: value = request.getfuncargvalue(name) <IF_STMT> value = value.__name__ suffixes.append('{}={}'.format(name, value)) return '+'.join([function_name] + suffixes)",if inspect.isclass(value):
"def add_entities(self, positions): e1 = EntityFactory() for p in positions: <IF_STMT> start, length = p else: start, length = (p, 1) EntityOccurrenceFactory(document=self.doc, entity=e1, offset=start, offset_end=start + length, alias='AB')","if isinstance(p, tuple):"
"def transform_value(value): if isinstance(value, collections.MutableMapping): <IF_STMT> return DBRef(value['_ns'], transform_value(value['_id'])) else: return transform_dict(SON(value)) elif isinstance(value, list): return [transform_value(v) for v in value] return value",if '_id' in value and '_ns' in value:
"def remove(self, items): """"""Remove messages from lease management."""""" with self._add_remove_lock: for item in items: if self._leased_messages.pop(item.ack_id, None) is not None: self._bytes -= item.byte_size else: _LOGGER.debug('Item %s was not managed.', item.ack_id) <IF_STMT> _LOGGER.debug('Bytes was unexpectedly negative: %d', self._bytes) self._bytes = 0",if self._bytes < 0:
"def parse_hgsub(lines): """"""Fills OrderedDict with hgsub file content passed as list of lines"""""" rv = OrderedDict() for l in lines: ls = l.strip() <IF_STMT> continue name, value = l.split('=', 1) rv[name.strip()] = value.strip() return rv",if not ls or ls[0] == '#':
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: return None elif self._keys[hash_] == key: self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) <IF_STMT> return None",if initial_hash == hash_:
"def atom(token, no_symbol=False): try: return int(token) except ValueError: try: return float(token) except ValueError: <IF_STMT> return token[1:-1] elif no_symbol: return token else: return Symbol(token)","if token.startswith(""'"") or token.startswith('""'):"
"def __Suffix_Noun_Step1b(self, token): for suffix in self.__suffix_noun_step1b: <IF_STMT> token = token[:-1] self.suffixe_noun_step1b_success = True break return token",if token.endswith(suffix) and len(token) > 5:
"def _guardAgainstUnicode(self, data): if _pythonMajorVersion < 3: <IF_STMT> data = data.encode('utf8') elif isinstance(data, str): try: return data.encode('ascii') except UnicodeEncodeError: pass raise ValueError('pyDes can only work with encoded strings, not Unicode.') return data","if isinstance(data, unicode):"
"def populate_resource_parameters(self, tool_source): root = getattr(tool_source, 'root', None) if root is not None and hasattr(self.app, 'job_config') and hasattr(self.app.job_config, 'get_tool_resource_xml'): resource_xml = self.app.job_config.get_tool_resource_xml(root.get('id'), self.tool_type) if resource_xml is not None: inputs = root.find('inputs') <IF_STMT> inputs = parse_xml_string('<inputs/>') root.append(inputs) inputs.append(resource_xml)",if inputs is None:
"def test_arguments_regex(self): argument_matches = (('pip=1.1', ('pip', '1.1')), ('pip==1.1', None), ('pip=1.2=1', ('pip', '1.2=1'))) for argument, match in argument_matches: <IF_STMT> self.assertIsNone(salt.utils.args.KWARG_REGEX.match(argument)) else: self.assertEqual(salt.utils.args.KWARG_REGEX.match(argument).groups(), match)",if match is None:
def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = 'bl_%s' % self.businessline_id if self.service_id: sidebar_selected += '_s_%s' % self.service_id <IF_STMT> sidebar_selected += '_env_%s' % self.environment_id return sidebar_selected,if self.environment_id:
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result['venture_id'] = ip.venture.id <IF_STMT> result['device_id'] = ip.device.id if ip.device.venture is not None: result['venture_id'] = ip.device.venture.id return result",if ip.device is not None:
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True <IF_STMT> return True return False",if not event.get_date_object():
"def killIfDead(): if not self._isalive: self.log.debug(""WampLongPoll: killing inactive WAMP session with transport '{0}'"".format(self._transport_id)) self.onClose(False, 5000, 'session inactive') self._receive._kill() <IF_STMT> del self._parent._transports[self._transport_id] else: self.log.debug(""WampLongPoll: transport '{0}' is still alive"".format(self._transport_id)) self._isalive = False self.reactor.callLater(killAfter, killIfDead)",if self._transport_id in self._parent._transports:
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): <IF_STMT> l.warning('Found a bottom field in struct %s. Ignore and increment the offset using the default element size.', self.name) continue if not self._pack: align = ty.alignment if offset_so_far % align != 0: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets","if isinstance(ty, SimTypeBottom):"
"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get('allow_css_overrides'): filename = self.view.file_name() filetypes = self.settings.get('markdown_filetypes') <IF_STMT> for filetype in filetypes: if filename.endswith(filetype): css_filename = filename.rpartition(filetype)[0] + '.css' if os.path.isfile(css_filename): return u'<style>%s</style>' % load_utf8(css_filename) return ''",if filename and filetypes:
"def setFullCSSSource(self, fullsrc, inline=False): self.fullsrc = fullsrc if type(self.fullsrc) == six.binary_type: self.fullsrc = six.text_type(self.fullsrc, 'utf-8') if inline: self.inline = inline if self.fullsrc: self.srcFullIdx = self.fullsrc.find(self.src) if self.srcFullIdx < 0: del self.srcFullIdx self.ctxsrcFullIdx = self.fullsrc.find(self.ctxsrc) <IF_STMT> del self.ctxsrcFullIdx",if self.ctxsrcFullIdx < 0:
"def title(self): ret = theme['title'] if isinstance(self.name, six.string_types): width = self.statwidth() return ret + self.name[0:width].center(width).replace(' ', '-') + theme['default'] for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace(' ', '-') if i + 1 != len(self.vars): <IF_STMT> ret = ret + theme['frame'] + char['dash'] + theme['title'] else: ret = ret + char['space'] return ret",if op.color:
"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if self._requested_namespaces is not None and self._requested_namespaces != []: for requested_namespace in self._requested_namespaces: <IF_STMT> return [] elif requested_namespace[0] not in IGNORE_DBS: requested_databases.append(requested_namespace[0]) return requested_databases",if requested_namespace[0] is '*':
"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel['channel'] defaults = {'currency': channel.currency_code} <IF_STMT> defaults['discount_value'] = add_channel.get('discount_value') if 'min_amount_spent' in add_channel.keys(): defaults['min_spent_amount'] = add_channel.get('min_amount_spent', None) models.VoucherChannelListing.objects.update_or_create(voucher=voucher, channel=channel, defaults=defaults)",if 'discount_value' in add_channel.keys():
"def read_xml(path): with tf.gfile.GFile(path) as f: root = etree.fromstring(f.read()) annotations = {} for node in root.getchildren(): key, val = node2dict(node) <IF_STMT> annotations.setdefault(key, []).append(val) else: annotations[key] = val return annotations",if key == 'object':
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: <IF_STMT> result['venture_id'] = ip.venture.id if ip.device is not None: result['device_id'] = ip.device.id if ip.device.venture is not None: result['venture_id'] = ip.device.venture.id return result",if ip.venture is not None:
"def test_large_headers(self): with ExpectLog(gen_log, 'Unsatisfiable read', required=False): try: self.fetch('/', headers={'X-Filler': 'a' * 1000}, raise_error=True) self.fail('did not raise expected exception') except HTTPError as e: <IF_STMT> self.assertIn(e.response.code, (431, 599))",if e.response is not None:
"def validate_reserved_serial_no_consumption(self): for item in self.items: if item.s_warehouse and (not item.t_warehouse) and item.serial_no: for sr in get_serial_nos(item.serial_no): sales_order = frappe.db.get_value('Serial No', sr, 'sales_order') <IF_STMT> msg = _(""(Serial No: {0}) cannot be consumed as it's reserverd to fullfill Sales Order {1}."").format(sr, sales_order) frappe.throw(_('Item {0} {1}').format(item.item_code, msg))",if sales_order:
"def force_decode(string, encoding): if isinstance(string, str): <IF_STMT> string = string.decode(encoding) else: try: string = string.decode('utf-8') except UnicodeError: string = string.decode('latin1') return string",if encoding:
"def _add_cs(master_cs, sub_cs, prefix, delimiter='.', parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) if new_parameter.name == '': new_parameter.name = prefix <IF_STMT> new_parameter.name = '{}{}{}'.format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",elif not prefix == '':
"def __call__(self, *args, **kwargs): if self.log_file is not None: kwargs['file'] = self.log_file print(*args, **kwargs) <IF_STMT> self.log_file.flush() elif self.log_func is not None: self.log_func(*args, **kwargs)","if hasattr(self.log_file, 'flush'):"
"def df_index_expr(self, length_expr=None, as_range=False): """"""Generate expression to get or create index of DF"""""" if isinstance(self.index, types.NoneType): <IF_STMT> length_expr = df_length_expr(self) if as_range: return f'range({length_expr})' else: return f'numpy.arange({length_expr})' return 'self._index'",if length_expr is None:
"def _setWeight(self, value): if value is None: self._fontWeight = None else: <IF_STMT> raise TextFormatException(f'Not a supported fontWeight: {value}') self._fontWeight = value.lower()","if value.lower() not in ('normal', 'bold'):"
"def _test_configuration(self): config_path = self._write_config() try: self._log.debug('testing configuration') verboseflag = '-Q' if self._log.isEnabledFor(logging.DEBUG): verboseflag = '-v' p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, '-f', config_path]) <IF_STMT> raise RuntimeError('configuration test failed') self._log.debug('configuration seems ok') finally: os.remove(config_path)",if p.wait() != 0:
"def filter_queryset(self, request, queryset, view): kwargs = {} for field in view.filterset_fields: value = request.GET.get(field) if not value: continue if field == 'node_id': value = get_object_or_none(Node, pk=value) kwargs['node'] = value continue <IF_STMT> field = 'asset' kwargs[field] = value if kwargs: queryset = queryset.filter(**kwargs) logger.debug('Filter {}'.format(kwargs)) return queryset",elif field == 'asset_id':
"def _find_closing_brace(string, start_pos): """"""Finds the corresponding closing brace after start_pos."""""" bracks_open = 1 for idx, char in enumerate(string[start_pos:]): if char == '(': <IF_STMT> bracks_open += 1 elif char == ')': if string[idx + start_pos - 1] != '\\': bracks_open -= 1 if not bracks_open: return start_pos + idx + 1",if string[idx + start_pos - 1] != '\\':
"def _set_hostport(self, host, port): if port is None: i = host.rfind(':') j = host.rfind(']') if i > j: try: port = int(host[i + 1:]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1:]) host = host[:i] else: port = self.default_port <IF_STMT> host = host[1:-1] self.host = host self.port = port",if host and host[0] == '[' and (host[-1] == ']'):
"def __getstate__(self): state = {} for cls in type(self).mro(): cls_slots = getattr(cls, '__slots__', ()) for slot in cls_slots: if slot != '__weakref__': <IF_STMT> state[slot] = getattr(self, slot) state['_cookiejar_cookies'] = list(self.cookiejar) del state['cookiejar'] return state","if hasattr(self, slot):"
"def _evp_pkey_from_der_traditional_key(self, bio_data, password): key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL) if key != self._ffi.NULL: key = self._ffi.gc(key, self._lib.EVP_PKEY_free) <IF_STMT> raise TypeError('Password was given but private key is not encrypted.') return key else: self._consume_errors() return None",if password is not None:
"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" assert directive and directive[0] == '@' skip_flag = directive in ('@others', '@all') while i < len(s): <IF_STMT> return (True, i) else: i = skip_line(s, i) if skip_flag: i = skip_ws(s, i) return (False, -1)","if match_word(s, i, directive):"
"def _decorator(coro_func): fut = asyncio.ensure_future(coro_func()) self._tests.append((coro_func.__name__, fut)) if timeout_sec is not None: timeout_at = self._loop.time() + timeout_sec handle = self.MASTER_LOOP.call_at(timeout_at, self._set_exception_if_not_done, fut, asyncio.TimeoutError()) fut.add_done_callback(lambda *args: handle.cancel()) <IF_STMT> self._global_timeout_at = timeout_at return coro_func",if timeout_at > self._global_timeout_at:
"def _load(self, db, owner): self.__init(owner) db_result = db('SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ?', self.owner.worldid) for ship_id, state_id in db_result: ship = WorldObject.get_object_by_id(ship_id) state = self.shipStates[state_id] <IF_STMT> ship.add_move_callback(Callback(BehaviorMoveCallback._arrived, ship)) self.add_new_unit(ship, state)",if state == self.shipStates.moving:
"def addError(self, test, err): if err[0] is SkipTest: <IF_STMT> self.stream.writeln(str(err[1])) elif self.dots: self.stream.write('s') self.stream.flush() return _org_AddError(self, test, err)",if self.showAll:
"def _construct(self, node): self.flatten_mapping(node) ret = self.construct_pairs(node) keys = [d[0] for d in ret] keys_sorted = sorted(keys, key=_natsort_key) for key in keys: expected = keys_sorted.pop(0) <IF_STMT> raise ConstructorError(None, None, 'keys out of order: expected {} got {} at {}'.format(expected, key, node.start_mark)) return dict(ret)",if key != expected:
"def sample_pos_items_for_u(u, num): pos_items = self.train_items[u] n_pos_items = len(pos_items) pos_batch = [] while True: if len(pos_batch) == num: break pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0] pos_i_id = pos_items[pos_id] <IF_STMT> pos_batch.append(pos_i_id) return pos_batch",if pos_i_id not in pos_batch:
"def _get_id(self, type, id): fields = id.split(':') if len(fields) >= 3: <IF_STMT> logger.warning('Expected id of type %s but found type %s %s', type, fields[-2], id) return fields[-1] fields = id.split('/') if len(fields) >= 3: itype = fields[-2] if type != itype: logger.warning('Expected id of type %s but found type %s %s', type, itype, id) return fields[-1].split('?')[0] return id",if type != fields[-2]:
"def uninstall_environments(self, environments): environments = [env if not env.startswith(self.conda_context.envs_path) else os.path.basename(env) for env in environments] return_codes = [self.conda_context.exec_remove([env]) for env in environments] final_return_code = 0 for env, return_code in zip(environments, return_codes): <IF_STMT> log.debug(""Conda environment '%s' successfully removed."" % env) else: log.debug(""Conda environment '%s' could not be removed."" % env) final_return_code = return_code return final_return_code",if return_code == 0:
"def _add_hit_offset(self, context_list, string_name, original_offset, value): for context in context_list: hits_by_context_dict = self.hits_by_context.setdefault(context, {}) <IF_STMT> hits_by_context_dict[string_name] = (original_offset, value.encode('base64'))",if string_name not in hits_by_context_dict:
"def detab(self, text, length=None): """"""Remove a tab from the front of each line of the given text."""""" if length is None: length = self.tab_length newtext = [] lines = text.split('\n') for line in lines: if line.startswith(' ' * length): newtext.append(line[length:]) <IF_STMT> newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",elif not line.strip():
"def dump(self): print(self.package_name) for package, value in self.entries: print(str(package.version)) <IF_STMT> print('[FILTERED]') elif isinstance(value, list): variants = value for variant in variants: print('%s' % str(variant)) else: print('%s' % str(package))",if value is None:
"def __lexical_scope(*args, **kwargs): try: scope = Scope(quasi) <IF_STMT> binding_name_set_stack[-1].add_child(scope) binding_name_set_stack.append(scope) return func(*args, **kwargs) finally: if binding_name_set_stack[-1] is scope: binding_name_set_stack.pop()",if quasi:
"def getnotes(self, origin=None): if origin is None: result = self.translator_comments <IF_STMT> if result: result += '\n' + self.developer_comments else: result = self.developer_comments return result elif origin == 'translator': return self.translator_comments elif origin in ('programmer', 'developer', 'source code'): return self.developer_comments else: raise ValueError('Comment type not valid')",if self.developer_comments:
"def fix_datetime_fields(data: TableData, table: TableName) -> None: for item in data[table]: for field_name in DATE_FIELDS[table]: <IF_STMT> item[field_name] = datetime.datetime.fromtimestamp(item[field_name], tz=datetime.timezone.utc)",if item[field_name] is not None:
"def _check_for_cart_error(cart): if cart._safe_get_element('Cart.Request.Errors') is not None: error = cart._safe_get_element('Cart.Request.Errors.Error.Code').text <IF_STMT> raise CartInfoMismatchException('CartGet failed: AWS.ECommerceService.CartInfoMismatch make sure AssociateTag, CartId and HMAC are correct (dont use URLEncodedHMAC!!!)') raise CartException('CartGet failed: ' + error)",if error == 'AWS.ECommerceService.CartInfoMismatch':
"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: <IF_STMT> raise ValueError('Longitude is out of bounds, check your JSON format or data') if geometry[1] > 90 or geometry[1] < -90: raise ValueError('Latitude is out of bounds, check your JSON format or data')",if geometry[0] > 180 or geometry[0] < -180:
"def _mapper_output_protocol(self, step_num, step_map): map_key = self._step_key(step_num, 'mapper') if map_key in step_map: <IF_STMT> return self.output_protocol() else: return self.internal_protocol() else: return RawValueProtocol()",if step_map[map_key] >= len(step_map) - 1:
"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: <IF_STMT> if not os.path.exists(fspath + '.spt'): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if path == paths[-1]:
"def ping(self, payload: Union[str, bytes]='') -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <IF_STMT> payload = payload.decode('utf-8') self.logger.debug(f'Sending a ping data frame (session id: {self.session_id}, payload: {payload})') data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PING) with self.sock_send_lock: self.sock.send(data)","if isinstance(payload, bytes):"
"def is_ac_power_connected(): for power_source_path in Path('/sys/class/power_supply/').iterdir(): try: with open(power_source_path / 'type', 'r') as f: if f.read().strip() != 'Mains': continue with open(power_source_path / 'online', 'r') as f: <IF_STMT> return True except IOError: continue return False",if f.read(1) == '1':
"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): <IF_STMT> self.validate_base_model(model) if hasattr(model, '_feincms_content_models'): self.validate_content_type(model)","if hasattr(model, '_create_content_base'):"
"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <IF_STMT> module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_()",if module.bias is not None:
"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): if part.get_content_maintype() == 'multipart': continue ctype = part.get_content_type() <IF_STMT> ctype = OCTET_TYPE filename = part.get_filename() if not filename: filename = PART_FN_TPL % partnum headers = dict(part) LOG.debug(headers) headers['Content-Type'] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",if ctype is None:
"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == '|': m -= 1 n -= 1 mask[m] = 1 <IF_STMT> m -= 1 elif dirs[m, n] == '<': n -= 1 else: raise UnboundLocalError('Illegal move') return mask","elif dirs[m, n] == '^':"
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() <IF_STMT> continue match = COMMENT.match(line) if match: continue if strip_delimiters: if ',' in line or ';' in line: continue yield line",if line == '':
"def fetch(self, *tileables, **kw): ret_list = False if len(tileables) == 1 and isinstance(tileables[0], (tuple, list)): ret_list = True tileables = tileables[0] elif len(tileables) > 1: ret_list = True result = self._sess.fetch(*tileables, **kw) ret = [] for r, t in zip(result, tileables): <IF_STMT> ret.append(r.item()) else: ret.append(r) if ret_list: return ret return ret[0]","if hasattr(t, 'isscalar') and t.isscalar() and (getattr(r, 'size', None) == 1):"
"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) if container.pop(_list_marker, False): force_list = True values.extend((_convert(x[1]) for x in sorted(container.items()))) <IF_STMT> values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls(((k, _convert(v)) for k, v in iteritems(container)))",if not force_list and len(values) == 1:
"def _transform_init_kwargs(cls, kwargs): transformed = [] for field in list(kwargs.keys()): prop = getattr(cls, field, None) <IF_STMT> value = kwargs.pop(field) _transform_single_init_kwarg(prop, field, value, kwargs) transformed.append((field, value)) return transformed","if isinstance(prop, MoneyProperty):"
"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: if isinstance(fvalue, Packet): ret = fvalue.haslayer(cls) <IF_STMT> return ret return self.payload.haslayer(cls)",if ret:
def insert_broken_add_sometimes(node): if node.op == theano.tensor.add: last_time_replaced[0] = not last_time_replaced[0] <IF_STMT> return [off_by_half(*node.inputs)] return False,if last_time_replaced[0]:
"def testReadChunk10(self): self.createTempFile() with BZ2File(self.filename) as bz2f: text = '' while 1: str = bz2f.read(10) <IF_STMT> break text += str self.assertEqual(text, self.TEXT)",if not str:
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): sv_faces = [] for i, face in enumerate(dcel_mesh.faces): if face.inners and face.outer: 'Face ({}) has inner components! Sverchok cant show polygons with holes.'.format(i) if not face.outer or del_flag in face.flags: continue <IF_STMT> continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",if only_select and (not face.select):
"def __check_dict_contains(dct, dict_name, keys, comment='', result=True): for key in keys: <IF_STMT> result = False comment = __append_comment('Missing {0} in {1}'.format(key, dict_name), comment) return (result, comment)",if key not in six.iterkeys(dct):
"def _dump_arg_defaults(kwargs): """"""Inject default arguments for dump functions."""""" if current_app: kwargs.setdefault('cls', current_app.json_encoder) <IF_STMT> kwargs.setdefault('ensure_ascii', False) kwargs.setdefault('sort_keys', current_app.config['JSON_SORT_KEYS']) else: kwargs.setdefault('sort_keys', True) kwargs.setdefault('cls', JSONEncoder)",if not current_app.config['JSON_AS_ASCII']:
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue <IF_STMT> changed = True break self._reset_button.disabled = not changed",elif len(value) != 0:
"def parse_win_proxy(val): proxies = [] for p in val.split(';'): <IF_STMT> tab = p.split('=', 1) if tab[0] == 'socks': tab[0] = 'SOCKS4' proxies.append((tab[0].upper(), tab[1], None, None)) else: proxies.append(('HTTP', p, None, None)) return proxies",if '=' in p:
"def predict(collect_dir, keys): run_all = len(keys) == 0 validate_keys(keys) for exp_cfg in cfg: <IF_STMT> key = exp_cfg['key'] _predict(key, exp_cfg['sample_img'], collect_dir)",if run_all or exp_cfg['key'] in keys:
"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) <IF_STMT> key += '/tcp' if isinstance(v, list): result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result",if '/' not in key:
"def assert_conll_writer_output(dataset: InternalBioNerDataset, expected_output: List[str], sentence_splitter: SentenceSplitter=None): outfile_path = tempfile.mkstemp()[1] try: sentence_splitter = sentence_splitter <IF_STMT> else NoSentenceSplitter(tokenizer=SpaceTokenizer()) writer = CoNLLWriter(sentence_splitter=sentence_splitter) writer.write_to_conll(dataset, Path(outfile_path)) contents = [l.strip() for l in open(outfile_path).readlines() if l.strip()] finally: os.remove(outfile_path) assert contents == expected_output",if sentence_splitter
"def post(self, request, *args, **kwargs): self.comment_obj = get_object_or_404(Comment, id=request.POST.get('commentid')) if request.user == self.comment_obj.commented_by: form = LeadCommentForm(request.POST, instance=self.comment_obj) <IF_STMT> return self.form_valid(form) return self.form_invalid(form) data = {'error': ""You don't have permission to edit this comment.""} return JsonResponse(data)",if form.is_valid():
"def trivia_list(self, ctx: commands.Context): """"""List available trivia categories."""""" lists = set((p.stem for p in self._all_lists())) if await ctx.embed_requested(): await ctx.send(embed=discord.Embed(title=_('Available trivia lists'), colour=await ctx.embed_colour(), description=', '.join(sorted(lists)))) else: msg = box(bold(_('Available trivia lists')) + '\n\n' + ', '.join(sorted(lists))) <IF_STMT> await ctx.author.send(msg) else: await ctx.send(msg)",if len(msg) > 1000:
"def validate(self): result = validators.SUCCESS msgs = [] for validator in self._validators: res, err = validator.validate() if res == validators.ERROR: result = res elif res == validators.WARNING and result != validators.ERROR: result = res <IF_STMT> msgs.append(err) return (result, '\n'.join(msgs))",if len(err) > 0:
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') <IF_STMT> self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",elif mod_type == imp.PY_COMPILED:
"def flush_file(self, key, f): f.flush() if self.compress: f.compress = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum((1 for f in self.files.values() if f.fileobj is not None)) <IF_STMT> f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",if open_files > self.MAX_OPEN_FILES:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.add_version(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def init_author_file(self): self.author_map = {} if self.ui.config('git', 'authors'): f = open(self.repo.wjoin(self.ui.config('git', 'authors'))) try: for line in f: line = line.strip() <IF_STMT> continue from_, to = RE_AUTHOR_FILE.split(line, 2) self.author_map[from_] = to finally: f.close()",if not line or line.startswith('#'):
"def decode_imsi(self, imsi): new_imsi = '' for a in imsi: c = hex(a) <IF_STMT> new_imsi += str(c[3]) + str(c[2]) else: new_imsi += str(c[2]) + '0' mcc = new_imsi[1:4] mnc = new_imsi[4:6] return (new_imsi, mcc, mnc)",if len(c) == 4:
"def _get_infoset(self, prefname): """"""Return methods with the name starting with prefname."""""" infoset = [] excludes = ('%sinfoset' % prefname,) preflen = len(prefname) for name in dir(self.__class__): if name.startswith(prefname) and name not in excludes: member = getattr(self.__class__, name) <IF_STMT> infoset.append(name[preflen:].replace('_', ' ')) return infoset","if isinstance(member, MethodType):"
def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok['text'] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 <IF_STMT> break,if nestedCount <= 0:
"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = (c.rootPosition(), [], []) while p: <IF_STMT> p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, '@ignore'): p.moveToNodeAfterTree() elif p.h.startswith('@mark-for-unit-tests'): result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",if p.v in seen:
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier['part'] earlier_step = earlier['step'] found = False for current in current_parts: if earlier_part == current['part'] and earlier_step == current['step']: found = True break <IF_STMT> cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",if not found:
"def unmark_first_parents(event=None): """"""Mark the node and all its parents."""""" c = event.get('c') if not c: return changed = [] for parent in c.p.self_and_parents(): <IF_STMT> parent.v.clearMarked() parent.setAllAncestorAtFileNodesDirty() changed.append(parent.copy()) if changed: c.setChanged() c.redraw() return changed",if parent.isMarked():
"def stop(self): self._log('Monitor stop') self._stop_requested = True try: <IF_STMT> fd = os.open(self.fifo_path, os.O_WRONLY) os.write(fd, b'X') os.close(fd) except Exception as e: self._log('err while closing: {0}'.format(str(e))) if self._thread: self._thread.join() self._thread = None",if os.path.exists(self.fifo_path):
"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): <IF_STMT> f = False if f: cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]","if self.FindItemAtPosition((r, c)) is not None:"
"def _load_objects(self, obj_id_zset, limit, chunk_size=1000): ct = i = 0 while True: id_chunk = obj_id_zset[i:i + chunk_size] <IF_STMT> return i += chunk_size for raw_data in self._data[id_chunk]: if not raw_data: continue if self._use_json: yield json.loads(decode(raw_data)) else: yield raw_data ct += 1 if limit and ct == limit: return",if not id_chunk:
"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] if tf.keras.backend.is_sparse(val): val = tf.sparse.to_dense(val) <IF_STMT> val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",if val.dtype == tf.int64:
"def print_callees(self, *amount): width, list = self.get_print_list(amount) if list: self.calc_callees() self.print_call_heading(width, 'called...') for func in list: <IF_STMT> self.print_call_line(width, func, self.all_callees[func]) else: self.print_call_line(width, func, {}) print >> self.stream print >> self.stream return self",if func in self.all_callees:
"def on_task_input(self, task, config): if config is False: return for entry in task.entries: <IF_STMT> log_once('Corrected `%s` url (replaced &amp; with &)' % entry['title'], logger=log) entry['url'] = entry['url'].replace('&amp;', '&')",if '&amp;' in entry['url']:
"def function(self, inputs, outputs, ignore_empty=False): f = function(inputs, outputs, mode=self.mode) if self.mode is not None or theano.config.mode != 'FAST_COMPILE': topo = f.maker.fgraph.toposort() topo_ = [node for node in topo if not isinstance(node.op, self.ignore_topo)] if ignore_empty: assert len(topo_) <= 1, topo_ else: assert len(topo_) == 1, topo_ <IF_STMT> assert type(topo_[0].op) is self.op return f",if len(topo_) > 0:
"def _get_env_command(self) -> Sequence[str]: """"""Get command sequence for `env` with configured flags."""""" env_list = ['env'] for key in ['http_proxy', 'https_proxy']: value = self.build_provider_flags.get(key) <IF_STMT> continue value = str(value) env_list.append(f'{key}={value}') return env_list",if not value:
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) <IF_STMT> self.callback()",if self.callback:
"def clean(self): if not self.code: self.code = u'static-%s' % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter(code=self.code, site__isnull=True) if self.pk: placeholders = placeholders.exclude(pk=self.pk) <IF_STMT> raise ValidationError(_('A static placeholder with the same site and code already exists'))",if placeholders.exists():
"def load_parser(self): result = OrderedDict() for name, flags in self.filenames: filename = self.get_filename(name) for match in sorted(glob(filename), key=self.file_key): <IF_STMT> continue result[match] = TextParser(match, os.path.relpath(match, self.base), flags) return result",if match in result:
"def __init__(self, selectable, name=None): baseselectable = selectable while isinstance(baseselectable, Alias): baseselectable = baseselectable.element self.original = baseselectable self.supports_execution = baseselectable.supports_execution if self.supports_execution: self._execution_options = baseselectable._execution_options self.element = selectable if name is None: <IF_STMT> name = getattr(self.original, 'name', None) name = _anonymous_label('%%(%d %s)s' % (id(self), name or 'anon')) self.name = name",if self.original.named_with_column:
"def load_tour(self, tour_id): for tour_dir in self.tour_directories: tour_path = os.path.join(tour_dir, tour_id + '.yaml') if not os.path.exists(tour_path): tour_path = os.path.join(tour_dir, tour_id + '.yml') <IF_STMT> return self._load_tour_from_path(tour_path)",if os.path.exists(tour_path):
def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color if t.theme_style == 'Dark': <IF_STMT> c = t.primary_dark elif self.md_bg_color == t.accent_color: c = t.accent_dark return c,if self.md_bg_color == t.primary_color:
"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): if shapes[i] is not None: <IF_STMT> data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)","if isinstance(request, numbers.Integral):"
"def onClicked(event): if not self.path: <IF_STMT> os.makedirs(mh.getPath('render')) self.path = mh.getPath('render') filename, ftype = mh.getSaveFileName(os.path.splitext(self.path)[0], 'PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*)') if filename: if 'Thumbnail' in ftype: self.image.save(filename, iformat='PNG') else: self.image.save(filename) self.path = os.path.dirname(filename)",if not os.path.exists(mh.getPath('render')):
"def _build_dom(cls, content, mode): assert mode in ('html', 'xml') if mode == 'html': <IF_STMT> THREAD_STORAGE.html_parser = HTMLParser() dom = defusedxml.lxml.parse(StringIO(content), parser=THREAD_STORAGE.html_parser) return dom.getroot() else: if not hasattr(THREAD_STORAGE, 'xml_parser'): THREAD_STORAGE.xml_parser = XMLParser() dom = defusedxml.lxml.parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser) return dom.getroot()","if not hasattr(THREAD_STORAGE, 'html_parser'):"
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): <IF_STMT> ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",if code == Path.MOVETO:
"def _targets(self, sigmaparser): targets = set() for condfield in self.conditions: if condfield in sigmaparser.values: rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: <IF_STMT> targets.update(self.conditions[condfield][condvalue]) return targets",if condvalue in rulefieldvalues:
"def create_image_upload(): if request.method == 'POST': image = request.form['image'] <IF_STMT> image_file = uploaded_file(file_content=image) image_url = upload_local(image_file, UPLOAD_PATHS['temp']['image'].format(uuid=uuid4())) return jsonify({'status': 'ok', 'image_url': image_url}) else: return jsonify({'status': 'no_image'})",if image:
"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: if key[-1] == '!': <IF_STMT> break elif not resp.match(key, val): break else: actions[action] = opts return actions","if resp.match(key[:-1], val):"
"def accept_quality(accept, default=1): """"""Separates out the quality score from the accepted content_type"""""" quality = default if accept and ';' in accept: accept, rest = accept.split(';', 1) accept_quality = RE_ACCEPT_QUALITY.search(rest) <IF_STMT> quality = float(accept_quality.groupdict().get('quality', quality).strip()) return (quality, accept.strip())",if accept_quality:
"def save(self, session=None, to=None, pickler=None): if to and pickler: self._save_to = (pickler, to) if self._save_to and len(self) > 0: with self._lock: pickler, fn = self._save_to <IF_STMT> session.ui.mark(_('Saving %s state to %s') % (self, fn)) pickler(self, fn)",if session:
"def get_safe_settings(): """"""Returns a dictionary of the settings module, with sensitive settings blurred out."""""" settings_dict = {} for k in dir(settings): if k.isupper(): <IF_STMT> settings_dict[k] = '********************' else: settings_dict[k] = getattr(settings, k) return settings_dict",if HIDDEN_SETTINGS.search(k):
def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 if part_h & 1: part_l |= 1 << 31 part_h >>= 1 <IF_STMT> part_h ^= 3623878656 _table_h.append(part_h) return _table_h,if rflag:
"def dns_query(server, timeout, protocol, qname, qtype, qclass): request = dns.message.make_query(qname, qtype, qclass) if protocol == 'tcp': response = dns.query.tcp(request, server, timeout=timeout, one_rr_per_rrset=True) else: response = dns.query.udp(request, server, timeout=timeout, one_rr_per_rrset=True) <IF_STMT> response = dns.query.tcp(request, server, timeout=timeout, one_rr_per_rrset=True) return response",if response.flags & dns.flags.TC:
"def sum_and_divide(self, losses): if self.total_divisor != 0: output = torch.sum(losses) / self.total_divisor <IF_STMT> self.total_divisor = self.total_divisor.item() return output return torch.sum(losses * 0)",if torch.is_tensor(self.total_divisor):
def __iter__(self): for chunk in self.source: if chunk is not None: self.wait_counter = 0 yield chunk <IF_STMT> self.wait_counter += 1 else: logger.warning('Data poller has been receiving no data for {} seconds.\nClosing data poller'.format(self.wait_cntr_max * self.poll_period)) break time.sleep(self.poll_period),elif self.wait_counter < self.wait_cntr_max:
"def test_find_directive_from_block(self): blocks = self.config.parser_root.find_blocks('virtualhost') found = False for vh in blocks: <IF_STMT> servername = vh.find_directives('servername') self.assertEqual(servername[0].parameters[0], 'certbot.demo') found = True self.assertTrue(found)",if vh.filepath.endswith('sites-enabled/certbot.conf'):
"def assign_products(request, discount_id): """"""Assign products to given property group with given property_group_id."""""" discount = lfs_get_object_or_404(Discount, pk=discount_id) for temp_id in request.POST.keys(): <IF_STMT> temp_id = temp_id.split('-')[1] product = Product.objects.get(pk=temp_id) discount.products.add(product) html = [['#products-inline', products_inline(request, discount_id, as_string=True)]] result = json.dumps({'html': html, 'message': _(u'Products have been assigned.')}, cls=LazyEncoder) return HttpResponse(result, content_type='application/json')",if temp_id.startswith('product'):
"def ChangeStyle(self, combos): style = 0 for combo in combos: if combo.GetValue() == 1: <IF_STMT> style = style | HTL.TR_VIRTUAL else: try: style = style | eval('wx.' + combo.GetLabel()) except: style = style | eval('HTL.' + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",if combo.GetLabel() == 'TR_VIRTUAL':
"def _set_autocomplete(self, notebook): if notebook: try: <IF_STMT> notebook = NotebookInfo(notebook) obj, x = build_notebook(notebook) self.form.widgets['namespace'].notebook = obj self.form.widgets['page'].notebook = obj logger.debug('Notebook for autocomplete: %s (%s)', obj, notebook) except: logger.exception('Could not set notebook: %s', notebook) else: self.form.widgets['namespace'].notebook = None self.form.widgets['page'].notebook = None logger.debug('Notebook for autocomplete unset')","if isinstance(notebook, str):"
"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: <IF_STMT> return None subDomain = subDomainElem.get('subdomain', '').strip() if subDomain: self.emitHostname(subDomain, event)",if self.checkForStop():
"def get_all_subnets(self, subnet_ids=None, filters=None): matches = itertools.chain(*[x.values() for x in self.subnets.values()]) if subnet_ids: matches = [sn for sn in matches if sn.id in subnet_ids] <IF_STMT> unknown_ids = set(subnet_ids) - set(matches) raise InvalidSubnetIdError(unknown_ids) if filters: matches = generic_filter(filters, matches) return matches",if len(subnet_ids) > len(matches):
"def _compat_map(self, avs): apps = {} for av in avs: av.version = self app_id = av.application <IF_STMT> apps[amo.APP_IDS[app_id]] = av return apps",if app_id in amo.APP_IDS:
"def generator(self, data): if self._config.SILENT: silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: <IF_STMT> continue yield (0, [int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val)])",if var in silent_vars:
def warn_if_repeatable_read(self): if 'mysql' in self.current_engine().lower(): cursor = self.connection_for_read().cursor() if cursor.execute('SELECT @@tx_isolation'): isolation = cursor.fetchone()[0] <IF_STMT> warnings.warn(TxIsolationWarning('Polling results with transaction isolation level repeatable-read within the same transaction may give outdated results. Be sure to commit the transaction for each poll iteration.')),if isolation == 'REPEATABLE-READ':
"def filter_by_level(record, level_per_module): name = record['name'] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = '' <IF_STMT> level = level_per_module[''] for n in name.split('.'): lookup += n if lookup in level_per_module: level = level_per_module[lookup] lookup += '.' if level is False: return False return record['level'].no >= level",if '' in level_per_module:
"def _readStream(self, handle: str, path: str) -> None: eof = False file = Path(path) with file.open('w') as f: while not eof: response = await self._client.send('IO.read', {'handle': handle}) eof = response.get('eof', False) <IF_STMT> f.write(response.get('data', '')) await self._client.send('IO.close', {'handle': handle})",if path:
"def sendall(self, data, flags=0): if self._sslobj: <IF_STMT> raise ValueError('non-zero flags not allowed in calls to sendall() on %s' % self.__class__) amount = len(data) count = 0 while count < amount: v = self.send(data[count:]) count += v return amount else: return socket.sendall(self, data, flags)",if flags != 0:
def run(self): utils.assert_main_thread() if not channel: <IF_STMT> SwiDebugStartChromeCommand.run(self) else: self.window.run_command('swi_debug_start') elif paused: logger.info('Resuming...') channel.send(webkit.Debugger.resume()) else: logger.info('Pausing...') channel.send(webkit.Debugger.setSkipAllPauses(False)) channel.send(webkit.Debugger.pause()),if not chrome_launched():
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def _replace_home(x): if xp.ON_WINDOWS: home = builtins.__xonsh__.env['HOMEDRIVE'] + builtins.__xonsh__.env['HOMEPATH'][0] if x.startswith(home): x = x.replace(home, '~', 1) <IF_STMT> x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env['HOME'] if x.startswith(home): x = x.replace(home, '~', 1) return x",if builtins.__xonsh__.env.get('FORCE_POSIX_PATHS'):
"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() for key, value in list(semanticTags.items()): <IF_STMT> raise TypeError('At least one key is not a valid int position') if not isinstance(value, list): raise TypeError('At least one value of the provided dict is not a list of string') for x in value: if not isinstance(x, str): raise TypeError('At least one value of the provided dict is not a list of string') self.__semanticTags = semanticTags","if not isinstance(key, int):"
"def _recv(): try: return sock.recv(bufsize) except SSLWantReadError: pass except socket.error as exc: error_code = extract_error_code(exc) if error_code is None: raise <IF_STMT> raise r, w, e = select.select((sock,), (), (), sock.gettimeout()) if r: return sock.recv(bufsize)",if error_code != errno.EAGAIN or error_code != errno.EWOULDBLOCK:
"def _authenticate(self): oauth_token = self.options.get('oauth_token') if oauth_token and (not self.api.oauth_token): self.logger.info('Attempting to authenticate using OAuth token') self.api.oauth_token = oauth_token user = self.api.user(schema=_user_schema) <IF_STMT> self.logger.info('Successfully logged in as {0}', user) else: self.logger.error('Failed to authenticate, the access token is not valid') else: return JustinTVPluginBase._authenticate(self)",if user:
"def reverse(self, *args): assert self._path is not None, 'Cannot reverse url regex ' + self.regex.pattern assert len(args) == self._group_count, 'required number of arguments not found' if not len(args): return self._path converted_args = [] for a in args: <IF_STMT> a = str(a) converted_args.append(escape.url_escape(utf8(a), plus=False)) return self._path % tuple(converted_args)","if not isinstance(a, (unicode_type, bytes)):"
"def determine_block_hints(self, text): hints = '' if text: <IF_STMT> hints += str(self.best_indent) if text[-1] not in '\n\x85\u2028\u2029': hints += '-' elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029': hints += '+' return hints",if text[0] in ' \n\x85\u2028\u2029':
"def find_package_modules(package, mask): import fnmatch if hasattr(package, '__loader__') and hasattr(package.__loader__, '_files'): path = package.__name__.replace('.', os.path.sep) mask = os.path.join(path, mask) for fnm in package.__loader__._files.iterkeys(): <IF_STMT> yield os.path.splitext(fnm)[0].replace(os.path.sep, '.') else: path = package.__path__[0] for fnm in os.listdir(path): if fnmatch.fnmatchcase(fnm, mask): yield ('%s.%s' % (package.__name__, os.path.splitext(fnm)[0]))","if fnmatch.fnmatchcase(fnm, mask):"
"def _condition(ct): for qobj in args: if qobj.connector == 'AND' and (not qobj.negated): for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError('Unsupported Q object') for attr, val in kwargs.items(): <IF_STMT> return False return True","if getattr(ct, attr) != val:"
"def process(self, resources): session = local_session(self.manager.session_factory) client = session.client('logs') state = self.data.get('state', True) key = self.resolve_key(self.data.get('kms-key')) for r in resources: try: <IF_STMT> client.associate_kms_key(logGroupName=r['logGroupName'], kmsKeyId=key) else: client.disassociate_kms_key(logGroupName=r['logGroupName']) except client.exceptions.ResourceNotFoundException: continue",if state:
"def get_xmm(env, ii): if is_gather(ii): <IF_STMT> return gen_reg_simd_unified(env, 'xmm_evex', True) return gen_reg_simd_unified(env, 'xmm', False) if ii.space == 'evex': return gen_reg(env, 'xmm_evex') return gen_reg(env, 'xmm')",if ii.space == 'evex':
"def parent(self): """"""Return the parent device."""""" if self._has_parent is None: _parent = self._ctx.backend.get_parent(self._ctx.dev) self._has_parent = _parent is not None <IF_STMT> self._parent = Device(_parent, self._ctx.backend) else: self._parent = None return self._parent",if self._has_parent:
"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = (50, 50, 50) for frame in g.app.windowList: w = frame and frame.top <IF_STMT> r = w.geometry() w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) x += 30 y += 30 if x > 200: x = 10 + delta y = 40 + delta delta += 10",if w:
"def _GetGoodDispatchAndUserName(IDispatch, userName, clsctx): if userName is None: if isinstance(IDispatch, str): userName = IDispatch <IF_STMT> userName = IDispatch.encode('ascii', 'replace') elif type(userName) == unicode: userName = userName.encode('ascii', 'replace') else: userName = str(userName) return (_GetGoodDispatch(IDispatch, clsctx), userName)","elif isinstance(IDispatch, unicode):"
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: <IF_STMT> raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if return_type is bytes:
"def test_ESPnetDataset_h5file_1(h5file_1): dataset = IterableESPnetDataset(path_name_type_list=[(h5file_1, 'data4', 'hdf5')], preprocess=preprocess) for key, data in dataset: if key == 'a': assert data['data4'].shape == (100, 80) <IF_STMT> assert data['data4'].shape == (150, 80)",if key == 'b':
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) <IF_STMT> continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if field_val is _marker:
"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: <IF_STMT> if self.match_name: match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",if match not in matches:
"def _set_chat_ids(self, chat_id: SLT[int]) -> None: with self.__lock: <IF_STMT> raise RuntimeError(f""Can't set {self.chat_id_name} in conjunction with (already set) {self.username_name}s."") self._chat_ids = self._parse_chat_id(chat_id)",if chat_id and self._usernames:
"def discover(self, *objlist): ret = [] for l in self.splitlines(): <IF_STMT> continue if l[0] == 'Filename': continue try: int(l[2]) int(l[3]) except: continue ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",if len(l) < 5:
"def get_changed_module(self): source = self.resource.read() change_collector = codeanalyze.ChangeCollector(source) if self.replacement is not None: change_collector.add_change(self.skip_start, self.skip_end, self.replacement) for occurrence in self.occurrence_finder.find_occurrences(self.resource): start, end = occurrence.get_primary_range() <IF_STMT> self.handle.occurred_inside_skip(change_collector, occurrence) else: self.handle.occurred_outside_skip(change_collector, occurrence) result = change_collector.get_changed() if result is not None and result != source: return result",if self.skip_start <= start < self.skip_end:
"def hpat_pandas_series_var_impl(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): if skipna is None: skipna = True if skipna: valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data)) <IF_STMT> return numpy.nan return numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof) if len(self._data) <= ddof: return numpy.nan return self._data.var() * len(self._data) / (len(self._data) - ddof)",if valuable_length <= ddof:
"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, 'condition', Undefined) copy = self if condition is not Undefined: <IF_STMT> pass elif 'field' in condition and 'type' not in condition: kwds = parse_shorthand(condition['field'], context.get('data', None)) copy = self.copy(deep=['condition']) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict(validate=validate, ignore=ignore, context=context)","if isinstance(condition, core.SchemaBase):"
"def get_field_result(self, result, field_name): if isinstance(result.field, models.ImageField): <IF_STMT> img = getattr(result.obj, field_name) result.text = mark_safe('<a href=""%s"" target=""_blank"" title=""%s"" data-gallery=""gallery""><img src=""%s"" class=""field_img""/></a>' % (img.url, result.label, img.url)) self.include_image = True return result",if result.value:
"def run(self): try: while True: dp = self.queue_get_stoppable(self.inq) <IF_STMT> return obj = self.func(dp) self.queue_put_stoppable(self.outq, obj) except Exception: if self.stopped(): pass else: raise finally: self.stop()",if self.stopped():
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) <IF_STMT> self._progress_hook(batch) yield results",if self._progress_hook:
"def merge(self, other): d = self._name2ft for name, (f, t) in other._name2ft.items(): <IF_STMT> f2, t2 = d[name] f = f + f2 t = t + t2 d[name] = (f, t)",if name in d:
"def _addSettingsToPanels(self, category, left, right): count = len(profile.getSubCategoriesFor(category)) + len(profile.getSettingsForCategory(category)) p = left n = 0 for title in profile.getSubCategoriesFor(category): n += 1 + len(profile.getSettingsForCategory(category, title)) <IF_STMT> p = right configBase.TitleRow(p, _(title)) for s in profile.getSettingsForCategory(category, title): configBase.SettingRow(p, s.getName())",if n > count / 2:
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): <IF_STMT> continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if i == '.' or i == '..':
def check_network_private(test_network): test_net = ipaddress.IPNetwork(test_network) test_start = test_net.network test_end = test_net.broadcast for network in settings.vpn.safe_priv_subnets: network = ipaddress.IPNetwork(network) net_start = network.network net_end = network.broadcast <IF_STMT> return True return False,if test_start >= net_start and test_end <= net_end:
def _end_description(self): if self._summaryKey == 'content': self._end_content() else: value = self.popContent('description') context = self._getContext() <IF_STMT> context['textinput']['description'] = value elif self.inimage: context['image']['description'] = value self._summaryKey = None,if self.intextinput:
def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: <IF_STMT> nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 if len(nullable) == num_nullable: break num_nullable = len(nullable) return nullable,if p.len == 0:
"def process_bind_param(self, value, dialect): if value is not None: if MAX_METADATA_VALUE_SIZE is not None: for k, v in list(value.items()): sz = total_size(v) <IF_STMT> del value[k] log.warning('Refusing to bind metadata key {} due to size ({})'.format(k, sz)) value = json_encoder.encode(value).encode() return value",if sz > MAX_METADATA_VALUE_SIZE:
"def process_input_line(self, line, store_history=True): """"""process the input, capturing stdout"""""" stdout = sys.stdout splitter = self.IP.input_splitter try: sys.stdout = self.cout splitter.push(line) more = splitter.push_accepts_more() <IF_STMT> try: source_raw = splitter.source_raw_reset()[1] except: source_raw = splitter.raw_reset() self.IP.run_cell(source_raw, store_history=store_history) finally: sys.stdout = stdout",if not more:
"def _dump_section(self, name, values, f): doc = '__doc__' <IF_STMT> print('# %s' % values[doc], file=f) print('%s(' % name, file=f) for k, v in values.items(): if k.endswith('__doc__'): continue doc = k + '__doc__' if doc in values: print('# %s' % values[doc], file=f) print('%s = %s,' % (k, pprint.pformat(v, indent=8)), file=f) print(')\n', file=f)",if doc in values:
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration <IF_STMT> expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if not expiration.tzinfo:
"def table_entry(mode1, bind_type1, mode2, bind_type2): with sock(mode1) as sock1: bind(sock1, bind_type1) try: with sock(mode2) as sock2: bind(sock2, bind_type2) except OSError as exc: <IF_STMT> return 'INUSE' elif exc.winerror == errno.WSAEACCES: return 'ACCESS' raise else: return 'Success'",if exc.winerror == errno.WSAEADDRINUSE:
"def __init__(self, ruleset): self.ruleset = ruleset self.rules = {} for filename in self.ruleset.rules: for rule in self.ruleset.rules[filename]: <IF_STMT> continue manage_dictionary(self.rules, rule.path, []) self.rules[rule.path].append(rule)",if not rule.enabled:
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) <IF_STMT> attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) if reply == '!done': return r",if j == -1:
"def _check_decorator_overload(name: str, old: str, new: str) -> int: """"""Conditions for a decorator to overload an existing one."""""" properties = _property_decorators(name) if old == new: return _MERGE elif old in properties and new in properties: p_old, p_new = (properties[old].precedence, properties[new].precedence) <IF_STMT> return _DISCARD elif p_old == p_new: return _MERGE else: return _REPLACE raise OverloadedDecoratorError(name, '')",if p_old > p_new:
"def validate_pk(self): try: self._key = serialization.load_pem_private_key(self.key, password=None, backend=default_backend()) if self._key.key_size > 2048: AWSValidationException('The private key length is not supported. Only 1024-bit and 2048-bit are allowed.') except Exception as err: <IF_STMT> raise raise AWSValidationException('The private key is not PEM-encoded or is not valid.')","if isinstance(err, AWSValidationException):"
"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy['Version'] = '2012-10-17' if self.resource_policy.get('Statement') is None: self.resource_policy['Statement'] = custom_statements else: <IF_STMT> custom_statements = [custom_statements] statement = self.resource_policy['Statement'] if not isinstance(statement, list): statement = [statement] for s in custom_statements: if s not in statement: statement.append(s) self.resource_policy['Statement'] = statement","if not isinstance(custom_statements, list):"
"def load(self, repn): for key in repn: tmp = self._convert(key) <IF_STMT> self.declare(tmp) item = dict.__getitem__(self, tmp) item._active = True item.load(repn[key])",if tmp not in self:
"def on_press_release(x): """"""Keyboard callback function."""""" global is_recording, enable_trigger_record press = keyboard.KeyboardEvent('down', 28, 'space') release = keyboard.KeyboardEvent('up', 28, 'space') if x.event_type == 'down' and x.name == press.name: if not is_recording and enable_trigger_record: sys.stdout.write('Start Recording ... ') sys.stdout.flush() is_recording = True if x.event_type == 'up' and x.name == release.name: <IF_STMT> is_recording = False",if is_recording == True:
"def apply_mask(self, mask, data_t, data_f): ind_t, ind_f = (0, 0) out = [] for m in cycle(mask): if m: if ind_t == len(data_t): return out out.append(data_t[ind_t]) ind_t += 1 else: <IF_STMT> return out out.append(data_f[ind_f]) ind_f += 1 return out",if ind_f == len(data_f):
"def oo_contains_rule(source, apiGroups, resources, verbs): """"""Return true if the specified rule is contained within the provided source"""""" rules = source['rules'] if rules: for rule in rules: if set(rule['apiGroups']) == set(apiGroups): if set(rule['resources']) == set(resources): <IF_STMT> return True return False",if set(rule['verbs']) == set(verbs):
"def _maybe_commit_artifact(self, artifact_id): artifact_status = self._artifacts[artifact_id] if artifact_status['pending_count'] == 0 and artifact_status['commit_requested']: for callback in artifact_status['pre_commit_callbacks']: callback() <IF_STMT> self._api.commit_artifact(artifact_id) for callback in artifact_status['post_commit_callbacks']: callback()",if artifact_status['finalize']:
"def shuffler(iterator, pool_size=10 ** 5, refill_threshold=0.9): yields_between_refills = round(pool_size * (1 - refill_threshold)) pool = take_n(pool_size, iterator) while True: random.shuffle(pool) for i in range(yields_between_refills): yield pool.pop() next_batch = take_n(yields_between_refills, iterator) <IF_STMT> break pool.extend(next_batch) yield from pool",if not next_batch:
"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] <IF_STMT> return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)","elif isinstance(key, slice):"
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 <IF_STMT> self.files.append(file) else: self.find(file)",if self.match_function(file):
"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: <IF_STMT> continue grade = cell.metadata['nbgrader']['grade'] solution = cell.metadata['nbgrader']['solution'] locked = cell.metadata['nbgrader']['locked'] if not grade and (not solution) and (not locked): continue grade_id = cell.metadata['nbgrader']['grade_id'] if grade_id in ids: raise ValidationError('Duplicate grade id: {}'.format(grade_id)) ids.add(grade_id)",if 'nbgrader' not in cell.metadata:
"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue elif self._pos == start: return b'' else: chunk = chunk[start - self._pos:] <IF_STMT> chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",if stop is not None and self._pos > stop:
"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings['room']['name'] if 'room' in settings else None file_ = settings['file'] if 'file' in settings else None if 'event' in settings: if 'joined' in settings['event']: self._client.userlist.addUser(username, room, file_) <IF_STMT> self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",elif 'left' in settings['event']:
"def run_tests(): x = 5 with switch(x) as case: if case(0): print('zero') print('zero') <IF_STMT> print('one or two') elif case(3, 4): print('three or four') else: print('default') print('another')","elif case(1, 2):"
"def _populate(): for fname in glob.glob(os.path.join(os.path.dirname(__file__), 'data', '*.json')): with open(fname) as inf: data = json.load(inf) data = data[list(data.keys())[0]] data = data[list(data.keys())[0]] for item in data: <IF_STMT> LOGGER.warning('Repeated emoji {}'.format(item['key'])) else: TABLE[item['key']] = item['value']",if item['key'] in TABLE:
"def slot_to_material(bobject: bpy.types.Object, slot: bpy.types.MaterialSlot): mat = slot.material if mat is not None: baked_mat = mat.name + '_' + bobject.name + '_baked' <IF_STMT> mat = bpy.data.materials[baked_mat] return mat",if baked_mat in bpy.data.materials:
"def __keyPress(self, widget, event): if event.key == 'G' and event.modifiers & event.Modifiers.Control: if not all((hasattr(p, 'isGanged') for p in self.getPlugs())): return False <IF_STMT> self.__ungang() else: self.__gang() return True return False",if all((p.isGanged() for p in self.getPlugs())):
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') <IF_STMT> expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False elif not rline.endswith(eline): return False return True","if isinstance(expected, str):"
"def hosts_to_domains(self, hosts, exclusions=[]): domains = [] for host in hosts: elements = host.split('.') while len(elements) >= 2: if len(elements) == 2: domain = '.'.join(elements) else: domain = '.'.join(elements[1:]) <IF_STMT> domains.append(domain) del elements[0] return domains",if domain not in domains + exclusions:
"def hsconn_sender(self): while not self.stop_event.is_set(): try: request = self.send_queue.get(True, 6.0) <IF_STMT> self.socket.sendall(request) if self.send_queue is not None: self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",if self.socket is not None:
"def get_url_args(self, item): if self.url_args: <IF_STMT> url_args = self.url_args(item) else: url_args = dict(self.url_args) url_args['id'] = item.id return url_args else: return dict(operation=self.label, id=item.id)","if hasattr(self.url_args, '__call__'):"
"def list_projects(self): projects = [] page = 1 while True: repos = self._client.get('/user/repos', {'sort': 'full_name', 'page': page, 'per_page': 100}) page += 1 for repo in repos: projects.append({'id': repo['full_name'], 'name': repo['full_name'], 'description': repo['description'], 'is_private': repo['private']}) <IF_STMT> break return projects",if len(repos) < 100:
"def scripts(self): application_root = current_app.config.get('APPLICATION_ROOT') subdir = application_root != '/' scripts = [] for script in get_registered_scripts(): if script.startswith('http'): scripts.append(f'<script defer src=""{script}""></script>') <IF_STMT> scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup('\n'.join(scripts))",elif subdir:
"def print_map(node, l): if node.title not in l: l[node.title] = [] for n in node.children: <IF_STMT> w = {n.title: []} l[node.title].append(w) print_map(n, w) else: l[node.title].append(n.title)",if len(n.children) > 0:
"def _validate_distinct_on_different_types_and_field_orders(self, collection, query, expected_results, get_mock_result): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): if isinstance(results[i], dict): self.assertDictEqual(results[i], expected_results[i]) <IF_STMT> self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0","elif isinstance(results[i], list):"
"def run(self): for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'Question' or v['_class'] == 'Message' or v['_class'] == 'Announcement': v['admin'] = None return self.objs",if k.startswith('_'):
def qvec(self): qvec = np.array([]) if self.polrep == 'stokes': qvec = self._imdict['Q'] elif self.polrep == 'circ': <IF_STMT> qvec = np.real(0.5 * (self.lrvec + self.rlvec)) return qvec,if len(self.rlvec) != 0 and len(self.lrvec) != 0:
"def display_value(self, key, w): if key == 'vdevices': nids = [n['deviceID'] for n in self.get_value('devices')] for device in self.app.devices.values(): <IF_STMT> b = Gtk.CheckButton(device.get_title(), False) b.set_tooltip_text(device['id']) self['vdevices'].pack_start(b, False, False, 0) b.set_active(device['id'] in nids) self['vdevices'].show_all() else: EditorDialog.display_value(self, key, w)",if device['id'] != self.app.daemon.get_my_id():
"def _set_xflux_setting(self, **kwargs): for key, value in kwargs.items(): if key in self._settings_map: <IF_STMT> self._set_xflux_screen_color(value) self._current_color = str(value) if self.state == self.states['PAUSED']: self.state = self.states['RUNNING'] else: self._xflux.sendline(self._settings_map[key] + str(value)) self._c()",if key == 'color':
"def apply_acceleration(self, veh_ids, acc): """"""See parent class."""""" if type(veh_ids) == str: veh_ids = [veh_ids] acc = [acc] for i, vid in enumerate(veh_ids): <IF_STMT> this_vel = self.get_speed(vid) next_vel = max([this_vel + acc[i] * self.sim_step, 0]) self.kernel_api.vehicle.slowDown(vid, next_vel, 0.001)",if acc[i] is not None and vid in self.get_ids():
"def largest_factor_relatively_prime(a, b): """"""Return the largest factor of a relatively prime to b."""""" while 1: d = gcd(a, b) <IF_STMT> break b = d while 1: q, r = divmod(a, d) if r > 0: break a = q return a",if d <= 1:
"def check_status(self): try: du = psutil.disk_usage('/') <IF_STMT> raise ServiceWarning('{host} {percent}% disk usage exceeds {disk_usage}%'.format(host=host, percent=du.percent, disk_usage=DISK_USAGE_MAX)) except ValueError as e: self.add_error(ServiceReturnedUnexpectedResult('ValueError'), e)",if DISK_USAGE_MAX and du.percent >= DISK_USAGE_MAX:
"def build_reply(self, msg, text=None, private=False, threaded=False): response = self.build_message(text) if msg.is_group: <IF_STMT> response.frm = self.bot_identifier response.to = IRCPerson(str(msg.frm)) else: response.frm = IRCRoomOccupant(str(self.bot_identifier), msg.frm.room) response.to = msg.frm.room else: response.frm = self.bot_identifier response.to = msg.frm return response",if private:
"def _dict_refs(obj, named): """"""Return key and value objects of a dict/proxy."""""" try: <IF_STMT> for k, v in _items(obj): s = str(k) yield _NamedRef('[K] ' + s, k) yield _NamedRef('[V] ' + s + ': ' + _repr(v), v) else: for k, v in _items(obj): yield k yield v except (KeyError, ReferenceError, TypeError) as x: warnings.warn(""Iterating '%s': %r"" % (_classof(obj), x))",if named:
"def fetch_images(): images = [] marker = None while True: batch = image_service.detail(context, filters=filters, marker=marker, sort_key='created_at', sort_dir='desc') <IF_STMT> break images += batch marker = batch[-1]['id'] return images",if not batch:
"def compress(self, data_list): warn_untested() if data_list: if data_list[1] in forms.fields.EMPTY_VALUES: error = self.error_messages['invalid_year'] raise forms.ValidationError(error) <IF_STMT> error = self.error_messages['invalid_month'] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) day = monthrange(year, month)[1] return date(year, month, day) return None",if data_list[0] in forms.fields.EMPTY_VALUES:
"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): if key not in new: removed.append(key) <IF_STMT> removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff['removed'] = sorted(removed) if added: diff['added'] = sorted(added) return diff",elif old[key] != new[key]:
"def add_filters(self, function): try: subscription = self.exists(function) <IF_STMT> response = self._sns.call('set_subscription_attributes', SubscriptionArn=subscription['SubscriptionArn'], AttributeName='FilterPolicy', AttributeValue=json.dumps(self.filters)) kappa.event_source.sns.LOG.debug(response) except Exception: kappa.event_source.sns.LOG.exception('Unable to add filters for SNS topic %s', self.arn)",if subscription:
"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <IF_STMT> constant_init(m, 1) else: raise TypeError('pretrained must be a str or None')","elif isinstance(m, (_BatchNorm, nn.GroupNorm)):"
def test_is_native_login(self): for campaign in self.campaign_lists: native = campaigns.is_native_login(campaign) <IF_STMT> assert_true(native) else: assert_false(native) native = campaigns.is_proxy_login(self.invalid_campaign) assert_true(native is None),if campaign == 'prereg' or campaign == 'erpc':
"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: if isinstance(arg, list): arg = self._process_filter(arg, host_state) <IF_STMT> arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result","elif isinstance(arg, basestring):"
"def find_go_files_mtime(app_files): files, mtime = ([], 0) for f, mt in app_files.items(): <IF_STMT> continue if APP_CONFIG.nobuild_files.match(f): continue files.append(f) mtime = max(mtime, mt) return (files, mtime)",if not f.endswith('.go'):
"def ExcludePath(self, path): """"""Check to see if this is a service url and matches inbound_services."""""" skip = False for reserved_path in self.reserved_paths.keys(): <IF_STMT> if not self.inbound_services or self.reserved_paths[reserved_path] not in self.inbound_services: return (True, self.reserved_paths[reserved_path]) return (False, None)",if path.startswith(reserved_path):
"def param_cov(self) -> DataFrame: """"""Parameter covariance"""""" if self._param_cov is not None: param_cov = self._param_cov else: params = np.asarray(self.params) <IF_STMT> param_cov = self.model.compute_param_cov(params) else: param_cov = self.model.compute_param_cov(params, robust=False) return DataFrame(param_cov, columns=self._names, index=self._names)",if self.cov_type == 'robust':
"def test_calculate_all_attentions(module, atype): m = importlib.import_module(module) args = make_arg(atype=atype) <IF_STMT> batch = prepare_inputs('pytorch') else: raise NotImplementedError model = m.E2E(6, 5, args) with chainer.no_backprop_mode(): if 'pytorch' in module: att_ws = model.calculate_all_attentions(*batch)[0] else: raise NotImplementedError print(att_ws.shape)",if 'pytorch' in module:
"def __eq__(self, other): try: if self.type != other.type: return False <IF_STMT> return self.askAnswer == other.askAnswer elif self.type == 'SELECT': return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",if self.type == 'ASK':
"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError('Process types can only contain [a-z]') <IF_STMT> raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value","if not re.match(MEMLIMIT_MATCH, str(v)):"
"def get_connections(data_about): data = data_about.find('h3', text='Connections').findNext() connections = {} for row in data.find_all('tr'): key = row.find_all('td')[0].text value = row.find_all('td')[1] <IF_STMT> connections[key] = get_all_links(value) else: connections[key] = value.text return connections",if 'Teams' in key:
"def _compute_map(self, first_byte, second_byte=None): if first_byte != 15: return 'XED_ILD_MAP0' else: if second_byte == None: return 'XED_ILD_MAP1' if second_byte == 56: return 'XED_ILD_MAP2' if second_byte == 58: return 'XED_ILD_MAP3' <IF_STMT> return 'XED_ILD_MAPAMD' die('Unhandled escape {} / map {} bytes'.format(first_byte, second_byte))",if second_byte == 15 and self.amd_enabled:
"def compress(self, data_list): if data_list: page_id = data_list[1] <IF_STMT> if not self.required: return None raise forms.ValidationError(self.error_messages['invalid_page']) return Page.objects.get(pk=page_id) return None",if page_id in EMPTY_VALUES:
"def find_module(self, fullname, path=None): path = path or self.path_entry for _ext in ['js', 'pyj', 'py']: _filepath = os.path.join(self.path_entry, '%s.%s' % (fullname, _ext)) <IF_STMT> print('module found at %s:%s' % (_filepath, fullname)) return VFSModuleLoader(_filepath, fullname) print('module %s not found' % fullname) raise ImportError() return None",if _filepath in VFS:
"def __decToBin(self, myDec): n = 0 binOfDec = '' while myDec > 2 ** n: n = n + 1 if (myDec < 2 ** n) & (myDec != 0): n = n - 1 while n >= 0: <IF_STMT> myDec = myDec - 2 ** n binOfDec = binOfDec + '1' else: binOfDec = binOfDec + '0' n = n - 1 return binOfDec",if myDec >= 2 ** n:
def __str__(self): try: <IF_STMT> NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value)) return NVMLError._errcode_to_string[self.value] except NVMLError_Uninitialized: return 'NVML Error with code %d' % self.value,if self.value not in NVMLError._errcode_to_string:
"def abspath(pathdir: str) -> str: if Path is not None and isinstance(pathdir, Path): return pathdir.abspath() else: pathdir = path.abspath(pathdir) <IF_STMT> try: pathdir = pathdir.decode(fs_encoding) except UnicodeDecodeError as exc: raise UnicodeDecodeError('multibyte filename not supported on this filesystem encoding (%r)' % fs_encoding) from exc return pathdir","if isinstance(pathdir, bytes):"
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: <IF_STMT> if isfile(self.object): with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, 'read'): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs","if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'):"
"def _set_uid(self, val): if val is not None: if pwd is None: self.bus.log('pwd module not available; ignoring uid.', level=30) val = None <IF_STMT> val = pwd.getpwnam(val)[2] self._uid = val","elif isinstance(val, text_or_bytes):"
"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): <IF_STMT> continue node_settings = node.get_addon(self.oauth_provider.short_name) if node_settings is None: continue if node_settings.external_account == external_account: yield node",if node is None:
"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError('py_obj must be a wandb.Image') else: if hasattr(py_obj, '_boxes') and py_obj._boxes: box_keys = list(py_obj._boxes.keys()) else: box_keys = [] <IF_STMT> mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)","if hasattr(py_obj, 'masks') and py_obj.masks:"
"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: <IF_STMT> byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if self.bytenum == len(self.bytestream) - 1:
"def destroy(self, wipe=False): if self.state == self.UP: image = self.image() <IF_STMT> return self.confirm_destroy(image, self.full_name, abort=False) else: self.warn(""tried to destroy {0} which didn't exist"".format(self.full_name)) return True",if image:
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1).json() <IF_STMT> match = AGENT_VERSION_EXP.search(resp.get('Version')) if match is not None and len(match.groups()) == 1: meta['ecs_version'] = match.group(1) except Exception as e: self.log.debug('Error getting ECS version: %s' % str(e)) return meta",if 'Version' in resp:
"def _path_type(st, lst): parts = [] if st: <IF_STMT> parts.append('file') elif stat.S_ISDIR(st.st_mode): parts.append('dir') else: parts.append('other') if lst: if stat.S_ISLNK(lst.st_mode): parts.append('link') return ' '.join(parts)",if stat.S_ISREG(st.st_mode):
"def changed(self, action): if len(action.key) >= 1 and action.key[0].lower() == 'files': <IF_STMT> self.update_model(clear=False) else: self.update_model(clear=True)",if action.type == 'insert':
"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client('es') for r in resources: <IF_STMT> result = self.manager.retry(client.describe_elasticsearch_domain_config, DomainName=r['DomainName'], ignore_err_codes=('ResourceNotFoundException',)) if result: r[self.policy_attribute] = json.loads(result.get('DomainConfig').get('AccessPolicies').get('Options')) return super().process(resources)",if self.policy_attribute not in r:
"def line_items(self): line_items = [] for line in self.lines_str: line = line.split('|') line = line[1:-1] items = [] for item in line: i = re.search('(\\S+([ \\t]+\\S+)*)+', item) <IF_STMT> items.append(i.group()) else: items.append(' ') line_items.append(items) return line_items",if i:
"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode('utf-8') if type(v) == unicode: v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) <IF_STMT> self.success(res) else: self.success(res)",elif not args.content_only:
"def get_servers(self, detail=True, search_opts=None): rel_url = '/servers/detail' if detail else '/servers' if search_opts is not None: qparams = {} for opt, val in search_opts.iteritems(): qparams[opt] = val <IF_STMT> query_string = '?%s' % urllib.urlencode(qparams) rel_url += query_string return self.api_get(rel_url)['servers']",if qparams:
"def run(self): while not self.__exit__: <IF_STMT> sleep(10) continue o = self.playlist[0] self.playlist.remove(o) obj = json.loads(o) if not 'args' in obj: obj['args'] = {'ua': '', 'header': '', 'title': '', 'referer': ''} obj['play'] = False self.handle = launch_player(obj['urls'], obj['ext'], **obj['args']) self.handle.wait()",if len(self.playlist) == 0:
"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) <IF_STMT> run_logs = r.json()['data']['records'] result.extend([i['logs'][0]['stats']['id'] for i in run_logs]) last_date = r.json()['data']['lastTimestamp'] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f'pares keep ids data since {since_time}') time.sleep(1) if not last_date: break return result",if r.ok:
"def __saveWork(self, work, results): """"""Stores the resulting last log line to the cache with the proxy key"""""" del work try: <IF_STMT> __cached = self.__cache[results[0]] __cached[self.__TIME] = time.time() __cached[self.__LINE] = results[1] __cached[self.__LLU] = results[2] except KeyError as e: pass except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if results:
"def read_notes(rec): found = [] for tag in range(500, 595): if tag in (505, 520): continue fields = rec.get_fields(str(tag)) <IF_STMT> continue for f in fields: x = f.get_lower_subfields() if x: found.append(' '.join(x).strip(' ')) if found: return '\n\n'.join(found)",if not fields:
"def serialize_to(self, stream, alternate_script=None): stream.write(self.txo_ref.tx_ref.hash) stream.write_uint32(self.txo_ref.position) if alternate_script is not None: stream.write_string(alternate_script) el<IF_STMT> stream.write_string(self.coinbase) else: stream.write_string(self.script.source) stream.write_uint32(self.sequence)",if self.is_coinbase:
"def func_named(self, arg): result = None target = 'do_' + arg if target in dir(self): result = target el<IF_STMT> funcs = [fname for fname in self.keywords if fname.startswith(arg)] if len(funcs) == 1: result = 'do_' + funcs[0] return result",if self.abbrev:
"def static_login(self, token, *, bot): self.__session = aiohttp.ClientSession(connector=self.connector, ws_response_class=DiscordClientWebSocketResponse) old_token, old_bot = (self.token, self.bot_token) self._token(token, bot=bot) try: data = await self.request(Route('GET', '/users/@me')) except HTTPException as exc: self._token(old_token, bot=old_bot) <IF_STMT> raise LoginFailure('Improper token has been passed.') from exc raise return data",if exc.response.status == 401:
"def render_buttons(self): for x, button in enumerate(self.button_list): gcolor = Gdk.color_parse(self.color_list[x]) <IF_STMT> fgcolor = Gdk.color_parse('#FFFFFF') else: fgcolor = Gdk.color_parse('#000000') button.set_label(self.color_list[x]) button.set_sensitive(True) button.modify_bg(Gtk.StateType.NORMAL, gcolor) button.modify_fg(Gtk.StateType.NORMAL, fgcolor)","if util.get_hls_val(self.color_list[x], 'light') < 99:"
"def _set_text(self, data): lines = [] for key, value in data.items(): lines.append('') txt = yaml.dump({key: value}, default_flow_style=False) title = self.titles.get(key) <IF_STMT> lines.append('# %s' % title) lines.append(txt.rstrip()) txt = '\n'.join(lines) + '\n' txt = txt.lstrip() self.edit.setPlainText(txt)",if title:
"def build_path(self): for variable in re_path_template.findall(self.path): name = variable.strip('{}') <IF_STMT> value = self.api.auth.get_username() else: try: value = quote(self.session.params[name]) except KeyError: raise TweepError('No parameter value found for path variable: %s' % name) del self.session.params[name] self.path = self.path.replace(variable, value)",if name == 'user' and 'user' not in self.session.params and self.api.auth:
"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): <IF_STMT> prop_vals = entity[prop_name] if isinstance(prop_vals, list): num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes",if not prop_name in entity.unindexed_properties():
"def create_connection(self, address, protocol_factory=None, **kw): """"""Helper method for creating a connection to an ``address``."""""" protocol_factory = protocol_factory or self.create_protocol if isinstance(address, tuple): host, port = address <IF_STMT> self.logger.debug('Create connection %s:%s', host, port) _, protocol = await self._loop.create_connection(protocol_factory, host, port, **kw) await protocol.event('connection_made') else: raise NotImplementedError('Could not connect to %s' % str(address)) return protocol",if self.debug:
def _increment_bracket_num(self): self._current_bracket -= 1 if self._current_bracket < 0: self._current_bracket = self._get_num_brackets() - 1 self._current_iteration += 1 <IF_STMT> self._current_bracket = 0,if self._current_iteration > self.hyperband_iterations:
"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node['deps']: <IF_STMT> return [curr_node['address']] for dep in curr_node['deps']: path = self.get_cycle_path(self.get_by_address(dep), goal_node_index) if len(path) > 0: path.insert(0, curr_node['address']) return path return []",if dep == goal_node_index:
"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith('/'): records = as_dict(path + item, version, section) <IF_STMT> result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",if records:
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] if line != ' ' and line != '': <IF_STMT> line = line[1:] f2.writelines(line + '\n')",if line[0] == ' ':
"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): <IF_STMT> index = i break if index is not None: del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",if subscriber_web_sock == web_sock:
"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] if encoding is not None: s = s.decode(encoding) <IF_STMT> s = '%s %s' % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if withyear:
"def generate_sitemaps(filename): rows = (line.strip().split('\t') for line in open(filename)) for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]): things = [] _chunk = list(chunk) for segment in _chunk: sortkey = segment.pop(0) last_modified = segment.pop(-1) path = ''.join(segment) things.append(web.storage(path=path, last_modified=last_modified)) <IF_STMT> write('sitemaps/sitemap_%s.xml.gz' % sortkey, sitemap(things))",if things:
"def use_index(self, term: Union[str, Index], *terms: Union[str, Index]) -> 'QueryBuilder': for t in (term, *terms): <IF_STMT> self._use_indexes.append(t) elif isinstance(t, str): self._use_indexes.append(Index(t))","if isinstance(t, Index):"
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) <IF_STMT> return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if result == self.source:
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form('List', None): value = [extract_pyreal(item) for item in value.leaves] <IF_STMT> return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value",if any((item is None for item in value)):
"def _reemit_nested_event(self, event: Event): source_index = self.index(event.source) for attr in ('index', 'new_index'): <IF_STMT> src_index = ensure_tuple_index(event.index) setattr(event, attr, (source_index,) + src_index) if not hasattr(event, 'index'): setattr(event, 'index', source_index) getattr(self.events, event.type, self.events)(event)","if hasattr(event, attr):"
"def check(self): """"""Perform required checks to conclude if it's safe to operate"""""" if self.interpreter.manual is None: <IF_STMT> self.error = self.process.error self.tip = self.process.tip return False start = time.time() while not self._status(): if time.time() - start >= 2: self.error = ""can't connect to the minserver on {}:{}"".format(self.interpreter.host, self.interpreter.port) self.tip = 'check your vagrant machine is running' return False time.sleep(0.1) return True",if not self.process.healthy:
"def apply(self): new_block = self.block.copy() new_block.clear() for inst in self.block.body: <IF_STMT> const_assign = self._assign_const(inst) new_block.append(const_assign) inst = self._assign_getitem(inst, index=const_assign.target) new_block.append(inst) return new_block","if isinstance(inst, Assign) and inst.value in self.getattrs:"
"def _get_orientation(self): if self.state: rotation = [0] * 9 inclination = [0] * 9 gravity = [] geomagnetic = [] gravity = self.listener_a.values geomagnetic = self.listener_m.values <IF_STMT> ff_state = SensorManager.getRotationMatrix(rotation, inclination, gravity, geomagnetic) if ff_state: values = [0, 0, 0] values = SensorManager.getOrientation(rotation, values) return values",if gravity[0] is not None and geomagnetic[0] is not None:
def getFirstSubGraph(graph): if len(graph) == 0: return None subg = {} todo = [graph.keys()[0]] while len(todo) > 0: <IF_STMT> subg[todo[0]] = graph[todo[0]] todo.extend(graph[todo[0]]) del graph[todo[0]] del todo[0] return subg,if todo[0] in graph.keys():
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if 'Last-Modified' not in rv.headers: try: result = date if callable(result): result = result(rv) <IF_STMT> from werkzeug.http import http_date result = http_date(result) if result: rv.headers['Last-Modified'] = result except Exception: logging.getLogger(__name__).exception('Error while calculating the lastmodified value for response {!r}'.format(rv)) return rv","if not isinstance(result, basestring):"
"def set_invoice_details(self, row): invoice_details = self.invoice_details.get(row.voucher_no, {}) if row.due_date: invoice_details.pop('due_date', None) row.update(invoice_details) if row.voucher_type == 'Sales Invoice': <IF_STMT> self.set_delivery_notes(row) if self.filters.show_sales_person and row.sales_team: row.sales_person = ', '.join(row.sales_team) del row['sales_team']",if self.filters.show_delivery_notes:
"def process(output): modules = {} for line in output: name, size, instances, depends, state, _ = line.split(' ', 5) instances = int(instances) module = {'size': size, 'instances': instances, 'state': state} <IF_STMT> module['depends'] = [value for value in depends.split(',') if value] modules[name] = module return modules",if depends != '-':
"def _get_host_from_zc_service_info(service_info: zeroconf.ServiceInfo): """"""Get hostname or IP + port from zeroconf service_info."""""" host = None port = None if service_info and service_info.port and (service_info.server or len(service_info.addresses) > 0): <IF_STMT> host = socket.inet_ntoa(service_info.addresses[0]) else: host = service_info.server.lower() port = service_info.port return (host, port)",if len(service_info.addresses) > 0:
"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <IF_STMT> module.weight.data[module.padding_idx].zero_()",if module.padding_idx is not None:
"def visitFromImport(self, import_stmt, import_info): new_pairs = [] if not import_info.is_star_import(): for name, alias in import_info.names_and_aliases: try: pyname = self.pymodule[alias or name] <IF_STMT> continue except exceptions.AttributeNotFoundError: pass new_pairs.append((name, alias)) return importinfo.FromImport(import_info.module_name, import_info.level, new_pairs)","if occurrences.same_pyname(self.pyname, pyname):"
"def _apply_patches(self): try: s = Subprocess(log=self.logfile, cwd=self.build_dir, verbose=self.options.verbose) for patch in self.patches: <IF_STMT> for ed, source in patch.items(): s.shell('ed - %s < %s' % (source, ed)) else: s.shell('patch -p0 < %s' % patch) except: logger.error('Failed to patch `%s`.\n%s' % (self.build_dir, sys.exc_info()[1])) sys.exit(1)",if type(patch) is dict:
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == '.' or i == '..': continue path = os.path.join(dir, i) <IF_STMT> dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if os.path.isdir(path):
"def remove_invalid_dirs(paths, bp_dir, module_name): ret = [] for path in paths: <IF_STMT> ret.append(path) else: logging.warning('Dir ""%s"" of module ""%s"" does not exist', path, module_name) return ret","if os.path.isdir(os.path.join(bp_dir, path)):"
def update_sockets(self): inputs = self.inputs inputs_n = 'ABabcd' penta_sockets = pentagon_dict[self.grid_type].input_sockets for socket in inputs_n: if socket in penta_sockets: <IF_STMT> inputs[socket].hide_safe = False else: inputs[socket].hide_safe = True,if inputs[socket].hide_safe:
"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P) begin, nexti = (0, 0) for i, char in enumerate(sentence): pos = pos_list[i] if pos == 'B': begin = i elif pos == 'E': yield sentence[begin:i + 1] nexti = i + 1 <IF_STMT> yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",elif pos == 'S':
"def validate(self): if self.data.get('encrypted', True): key = self.data.get('target_key') <IF_STMT> raise PolicyValidationError('Encrypted snapshot copy requires kms key on %s' % (self.manager.data,)) return self",if not key:
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None <IF_STMT> filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",if not filename.startswith(os.sep):
"def validate_name_and_description(body, check_length=True): for attribute in ['name', 'description', 'display_name', 'display_description']: value = body.get(attribute) if value is not None: if isinstance(value, six.string_types): body[attribute] = value.strip() <IF_STMT> try: utils.check_string_length(body[attribute], attribute, min_length=0, max_length=255) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",if check_length:
"def pick(items, sel): for x, s in zip(items, sel): if match(s): yield x <IF_STMT> yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",elif not x.is_atom() and (not s.is_atom()):
"def wait_or_kill(self): """"""Wait for the program to terminate, or kill it after 5s."""""" if self.instance.poll() is None: logger.info('Interrupting %s and waiting...', self.coord) self.instance.send_signal(signal.SIGINT) t = monotonic_time() while monotonic_time() - t < 5: <IF_STMT> logger.info('Terminated %s.', self.coord) break time.sleep(0.1) else: self.kill()",if self.instance.poll() is not None:
"def sort_collection(self, models, many): ordering = self.ordering if not many or not ordering: return models for key in reversed(ordering): reverse = key[0] == '-' <IF_STMT> key = key[1:] models = sorted(models, key=partial(deep_getattr, key=key), reverse=reverse) return models",if reverse:
"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): <IF_STMT> palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: if self.PALETTE is None: palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",if new_id != -1:
"def _find_tcl_dir(): lib_dirs = [os.path.dirname(_x) for _x in sys.path if _x.lower().endswith('lib')] for lib_dir in lib_dirs: base_dir = os.path.join(lib_dir, TclLibrary.FOLDER) <IF_STMT> for root, _, files in os.walk(base_dir): if TclLibrary.INIT_TCL in files: return root",if os.path.exists(base_dir):
"def __next__(self): """"""Special paging functionality"""""" if self.iter is None: self.iter = iter(self.objs) try: return next(self.iter) except StopIteration: self.iter = None self.objs = [] <IF_STMT> self.page += 1 self._connection.get_response(self.action, self.params, self.page, self) return next(self) else: raise",if int(self.page) < int(self.total_pages):
"def parse(cls, api, json): lst = List(api) setattr(lst, '_json', json) for k, v in json.items(): if k == 'user': setattr(lst, k, User.parse(api, v)) <IF_STMT> setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",elif k == 'created_at':
"def real_type(self): real_type = self.type if self.flag_indicator: real_type = '#' if self.is_vector: <IF_STMT> real_type = 'Vector<{}>'.format(real_type) else: real_type = 'vector<{}>'.format(real_type) if self.is_generic: real_type = '!{}'.format(real_type) if self.is_flag: real_type = 'flags.{}?{}'.format(self.flag_index, real_type) return real_type",if self.use_vector_id:
"def check_fs(path): with open(path, 'rb') as f: code = python_bytes_to_unicode(f.read(), errors='replace') <IF_STMT> module = _load_module(evaluator, path, code) module_name = sys_path.dotted_path_in_sys_path(evaluator.project.sys_path, path) if module_name is not None: add_module(evaluator, module_name, module) return module",if name in code:
"def infoCalendar(users): calendarId = normalizeCalendarId(sys.argv[5], checkPrimary=True) i = 0 count = len(users) for user in users: i += 1 user, cal = buildCalendarGAPIObject(user) if not cal: continue result = gapi.call(cal.calendarList(), 'get', soft_errors=True, calendarId=calendarId) <IF_STMT> print(f'User: {user}, Calendar:{display.current_count(i, count)}') _showCalendar(result, 1, 1)",if result:
"def set_hidestate_input_sockets_to_cope_with_switchnum(self): tndict = get_indices_that_should_be_visible(self.node_state) for key, value in tndict.items(): socket = self.inputs[key] desired_hide_state = not value <IF_STMT> socket.hide_safe = desired_hide_state",if not socket.hide == desired_hide_state:
"def get_class_name(item): class_name, module_name = (None, None) for parent in reversed(item.listchain()): if isinstance(parent, pytest.Class): class_name = parent.name <IF_STMT> module_name = parent.module.__name__ break if class_name and '.tasks.' not in module_name: return '{}.{}'.format(module_name, class_name) else: return module_name","elif isinstance(parent, pytest.Module):"
"def run(self): versions = versioneer.get_versions() tempdir = tempfile.mkdtemp() generated = os.path.join(tempdir, 'rundemo') with open(generated, 'wb') as f: for line in open('src/rundemo-template', 'rb'): <IF_STMT> f.write(('versions = %r\n' % (versions,)).encode('ascii')) else: f.write(line) self.scripts = [generated] rc = build_scripts.run(self) os.unlink(generated) os.rmdir(tempdir) return rc",if line.strip().decode('ascii') == '#versions':
"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, 'user', None) result = {'ip_address': request.META['REMOTE_ADDR']} <IF_STMT> result.update({'email': user.email, 'id': user.id}) if user.name: result['name'] = user.name else: result = {} return mark_safe(json.dumps(result))",if user and user.is_authenticated():
"def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]: """"""Convert tokens to spans."""""" tokens = iter(line_tokenize()) line_no = 0 _line_start = line_start - 1 while line_no < _line_start: _token_type, token = next(tokens) yield (token, None) <IF_STMT> line_no += 1 for token_type, token in tokens: yield (token, _get_theme_style(token_type)) if token.endswith('\n'): line_no += 1 if line_no >= line_end: break",if token.endswith('\n'):
"def encode(self, encodeFun, value, defMode, maxChunkSize): substrate, isConstructed = self.encodeValue(encodeFun, value, defMode, maxChunkSize) tagSet = value.getTagSet() if tagSet: <IF_STMT> defMode = 1 return self.encodeTag(tagSet[-1], isConstructed) + self.encodeLength(len(substrate), defMode) + substrate + self._encodeEndOfOctets(encodeFun, defMode) else: return substrate",if not isConstructed:
def _run(self): while True: request = self._requests.get() <IF_STMT> self.shutdown() break self.process(request) self._requests.task_done(),if request is None:
"def _decode_payload(self, payload): if payload['enc'] == 'aes': try: payload['load'] = self.crypticle.loads(payload['load']) except salt.crypt.AuthenticationError: <IF_STMT> raise payload['load'] = self.crypticle.loads(payload['load']) return payload",if not self._update_aes():
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) if self.any_match: <IF_STMT> return not self.inverse elif not result: return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if result:
"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print('V', value) if isinstance(value, int): param_node.selected_mode = 'int' param_node.int_ = value <IF_STMT> param_node.selected_mode = 'float' param_node.float_ = value","elif isinstance(value, float):"
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue <IF_STMT> for client in clients: if module.is_compatible_with(client): yield module break else: yield module",if clients is not None:
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: if pricing_rule.condition: try: <IF_STMT> filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules","if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):"
"def build_query_string(kv_data, ignore_none=True): query_string = '' for k, v in kv_data.iteritems(): <IF_STMT> continue if query_string != '': query_string += '&' else: query_string = '?' query_string += k + '=' + str(v) return query_string",if ignore_none is True and kv_data[k] is None:
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): if k in striped_keys: <IF_STMT> sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret","if isinstance(v, NestedSpace):"
"def task_failed(self, task_id, hostname, reason): logger.debug('task %d failed with message %s', task_id, str(reason)) if hostname in self.host_dict: host_status = self.host_dict[hostname] host_status.task_failed(task_id) <IF_STMT> self.task_host_failed_dict[task_id] = set() self.task_host_failed_dict[task_id].add(hostname)",if task_id not in self.task_host_failed_dict:
"def match(path): for pat, _type, _property, default_title in patterns: m = web.re_compile('^' + pat).match(path) <IF_STMT> prefix = m.group() extra = web.lstrips(path, prefix) tokens = extra.split('/', 2) middle = web.listget(tokens, 1, '') suffix = web.listget(tokens, 2, '') if suffix: suffix = '/' + suffix return (_type, _property, default_title, prefix, middle, suffix) return (None, None, None, None, None, None)",if m:
"def _get_cached_resources(self, ids): key = self.get_cache_key(None) if self._cache.load(): resources = self._cache.get(key) <IF_STMT> self.log.debug('Using cached results for get_resources') m = self.get_model() id_set = set(ids) return [r for r in resources if r[m.id] in id_set] return None",if resources is not None:
"def has_api_behaviour(self, protocol): config = get_config() try: r = self.session.get(f'{protocol}://{self.event.host}:{self.event.port}', timeout=config.network_timeout) <IF_STMT> return True except requests.exceptions.SSLError: logger.debug(f'{[protocol]} protocol not accepted on {self.event.host}:{self.event.port}') except Exception: logger.debug(f'Failed probing {self.event.host}:{self.event.port}', exc_info=True)","if 'k8s' in r.text or ('""code""' in r.text and r.status_code != 200):"
"def get_file_type(self, context, parent_context=None): file_type = context.get(self.file_type_name, None) if file_type == '': <IF_STMT> file_type = parent_context.get(self.file_type_name, self.default_file_type) else: file_type = self.default_file_type return file_type",if parent_context:
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) <IF_STMT> remove = True if remove and (not add): selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if boxedChunks.issubset(selectedChunks):
"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] <IF_STMT> output.append([char]) start_new_word = True else: if start_new_word: output.append([]) start_new_word = False output[-1].append(char) i += 1 return [''.join(x) for x in output]",if _is_punctuation(char):
"def _save_images(notebook): if os.getenv('NB_NO_IMAGES') == '1': return logged = False for filename, img_bytes in _iter_notebook_images(notebook): <IF_STMT> log.info('Saving images') logged = True with open(filename, 'wb') as f: f.write(img_bytes)",if not logged:
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) <IF_STMT> minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",if minDist == None or guideDist < minDist:
"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: <IF_STMT> out.write(msg) elif tp == 'flush': out.flush() elif tp == 'write_flush': out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",if tp == 'write':
"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == 'SaltLoggingClass': for base in bases: <IF_STMT> include_trace = False if hasattr(base, 'garbage'): include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr(base, 'trace'):"
"def generatePidEncryptionTable(): table = [] for counter1 in range(0, 256): value = counter1 for counter2 in range(0, 8): <IF_STMT> value = value >> 1 else: value = value >> 1 value = value ^ 3988292384 table.append(value) return table",if value & 1 == 0:
def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/params'): <IF_STMT> item.add_marker(pytest.mark.stage('unit')) if 'init' not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123)),if 'stage' not in item.keywords:
"def python_value(self, value): if value: if isinstance(value, basestring): pp = lambda x: x.time() return format_date_time(value, self.formats, pp) <IF_STMT> return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value","elif isinstance(value, datetime.datetime):"
"def list_interesting_hosts(self): hosts = [] targets = self.target['other'] for target in targets: <IF_STMT> hosts.append({'ip': target.ip, 'description': target.domain + ' / ' + target.name}) return hosts",if self.is_interesting(target) and target.status and (target.status != 400):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue <IF_STMT> self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 24:
"def _wait_for_finish(self) -> PollExitResponse: while True: if self._backend: poll_exit_resp = self._backend.interface.communicate_poll_exit() logger.info('got exit ret: %s', poll_exit_resp) if poll_exit_resp: done = poll_exit_resp.done pusher_stats = poll_exit_resp.pusher_stats <IF_STMT> self._on_finish_progress(pusher_stats, done) if done: return poll_exit_resp time.sleep(2)",if pusher_stats:
def listing_items(method): marker = None once = True items = [] while once or items: for i in items: yield i if once or marker: <IF_STMT> items = method(parms={'marker': marker}) else: items = method() if len(items) == 10000: marker = items[-1] else: marker = None once = False else: items = [],if marker:
"def call(monad, *args): for arg, name in izip(args, ('hour', 'minute', 'second', 'microsecond')): if not isinstance(arg, NumericMixin) or arg.type is not int: throw(TypeError, ""'%s' argument of time(...) function must be of 'int' type. Got: %r"" % (name, type2str(arg.type))) <IF_STMT> throw(NotImplementedError) return ConstMonad.new(time(*tuple((arg.value for arg in args))))","if not isinstance(arg, ConstMonad):"
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) <IF_STMT> sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if sign * ki < -slop:
"def walk_links(self): link_info_list = [] for item in self.content: <IF_STMT> link_info = LinkInfo(link=item, name=item.name, sections=()) link_info_list.append(link_info) else: link_info_list.extend(item.walk_links()) return link_info_list","if isinstance(item, Link):"
"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') <IF_STMT> sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",if end_slash >= 0:
"def load_dict(dict_path, reverse=False): word_dict = {} with open(dict_path, 'rb') as fdict: for idx, line in enumerate(fdict): line = cpt.to_text(line) <IF_STMT> word_dict[idx] = line.strip('\n') else: word_dict[line.strip('\n')] = idx return word_dict",if reverse:
"def test_network(coords, feats, model, batch_sizes, forward_only=True): for batch_size in batch_sizes: bcoords = batched_coordinates([coords for i in range(batch_size)]) bfeats = torch.cat([feats for i in range(batch_size)], 0) <IF_STMT> with torch.no_grad(): time, length = forward(bcoords, bfeats, model) else: time, length = train(bcoords, bfeats, model) print(f'{net.__name__}\t{voxel_size}\t{batch_size}\t{length}\t{time}') torch.cuda.empty_cache()",if forward_only:
"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: <IF_STMT> self.utexc = np.zeros(ntexco, dtype=bool) if self.utexc is not True: self.utexc[indices] = True",if self.utexc is False:
"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) <IF_STMT> has_module = has_module_directory or exists(module_directory) else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile if has_module: break return has_module",if not version:
"def get_editops(self): if not self._editops: <IF_STMT> self._editops = editops(self._opcodes, self._str1, self._str2) else: self._editops = editops(self._str1, self._str2) return self._editops",if self._opcodes:
"def to_representation(self, data): value = super(CredentialTypeSerializer, self).to_representation(data) if value.get('managed_by_tower'): value['name'] = _(value['name']) for field in value.get('inputs', {}).get('fields', []): field['label'] = _(field['label']) <IF_STMT> field['help_text'] = _(field['help_text']) return value",if 'help_text' in field:
"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): <IF_STMT> v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v[i], dict):"
"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' if ss['branch']: source += '[branch %s] ' % ss['branch'] if ss['revision']: source += str(ss['revision']) else: source += 'HEAD' <IF_STMT> source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['patch'] is not None:
"def fit_one(self, x): for i, xi in x.items(): <IF_STMT> self.median[i].update(xi) if self.with_scaling: self.iqr[i].update(xi) return self",if self.with_centering:
"def start_response(self, status, headers, exc_info=None): if exc_info: try: if self.started: six.reraise(exc_info[0], exc_info[1], exc_info[2]) finally: exc_info = None self.request.status = int(status[:3]) for key, val in headers: <IF_STMT> self.request.set_content_length(int(val)) elif key.lower() == 'content-type': self.request.content_type = val else: self.request.headers_out.add(key, val) return self.write",if key.lower() == 'content-length':
"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp(x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p) if len(result) == 1: y = result[0] <IF_STMT> y1, y2 = result y = y1 if y1 & 1 == y else y2 else: return None return ec.Point(self._curve, x, y)",elif len(result) == 2:
"def trace(self, ee, rname): print(type(self)) self.traceIndent() guess = '' if self.inputState.guessing > 0: guess = ' [guessing]' print(ee + rname + guess) for i in xrange(1, self.k + 1): if i != 1: print(', ') <IF_STMT> v = self.LT(i).getText() else: v = 'null' print('LA(%s) == %s' % (i, v)) print('\n')",if self.LT(i):
"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] if primary_key: parts.append('PRIMARY KEY') <IF_STMT> parts.append('NOT NULL') result[name] = ' '.join(parts) return result",if not_null:
"def _parse_csrf(self, response): for d in response: if d.startswith('Set-Cookie:'): for c in d.split(':', 1)[1].split(';'): if c.strip().startswith('CSRF-Token-'): self._CSRFtoken = c.strip(' \r\n') log.verbose('Got new cookie: %s', self._CSRFtoken) break <IF_STMT> break",if self._CSRFtoken != None:
"def _update_from_item(self, row, download_item): progress_stats = download_item.progress_stats for key in self.columns: column = self.columns[key][0] <IF_STMT> status = '{0} {1}/{2}'.format(progress_stats['status'], progress_stats['playlist_index'], progress_stats['playlist_size']) self.SetStringItem(row, column, status) else: self.SetStringItem(row, column, progress_stats[key])",if key == 'status' and progress_stats['playlist_index']:
"def unmarshal_package_repositories(cls, data: Any) -> List['PackageRepository']: repositories = list() if data is not None: <IF_STMT> raise RuntimeError(f'invalid package-repositories: {data!r}') for repository in data: package_repo = cls.unmarshal(repository) repositories.append(package_repo) return repositories","if not isinstance(data, list):"
"def remove_message(e=None): itop = scanbox.nearest(0) sel = scanbox.curselection() if not sel: dialog(root, 'No Message To Remove', 'Please select a message to remove', '', 0, 'OK') return todo = [] for i in sel: line = scanbox.get(i) <IF_STMT> todo.append(string.atoi(scanparser.group(1))) mhf.removemessages(todo) rescan() fixfocus(min(todo), itop)",if scanparser.match(line) >= 0:
"def test_patches(): print('Botocore version: {} aiohttp version: {}'.format(botocore.__version__, aiohttp.__version__)) success = True for obj, digests in chain(_AIOHTTP_DIGESTS.items(), _API_DIGESTS.items()): digest = hashlib.sha1(getsource(obj).encode('utf-8')).hexdigest() <IF_STMT> print('Digest of {}:{} not found in: {}'.format(obj.__qualname__, digest, digests)) success = False assert success",if digest not in digests:
"def sample_admin_user(): """"""List of iris messages"""""" with iris_ctl.db_from_config(sample_db_config) as (conn, cursor): cursor.execute('SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1') result = cursor.fetchone() <IF_STMT> return result[0]",if result:
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: <IF_STMT> for rightname2 in groups[rightname]: rightnames.add(rightname2) if not includeAll: break else: rightnames.add(rightname)",if rightname[0] == '@':
"def build(self, input_shape): if isinstance(input_shape, list) and len(input_shape) == 2: self.data_mode = 'disjoint' self.F = input_shape[0][-1] else: <IF_STMT> self.data_mode = 'single' else: self.data_mode = 'batch' self.F = input_shape[-1]",if len(input_shape) == 2:
"def update_ranges(l, i): for _range in l: <IF_STMT> _range[0] = i merge_ranges(l) return elif i == _range[1] + 1: _range[1] = i merge_ranges(l) return l.append([i, i]) l.sort(key=lambda x: x[0])",if i == _range[0] - 1:
"def transform(a, cmds): buf = a.split('\n') for cmd in cmds: ctype, line, col, char = cmd <IF_STMT> if char != '\n': buf[line] = buf[line][:col] + buf[line][col + len(char):] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == 'I': buf[line] = buf[line][:col] + char + buf[line][col:] buf = '\n'.join(buf).split('\n') return '\n'.join(buf)",if ctype == 'D':
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple if os.path.exists(uri) == True: <IF_STMT> files.append(uri) if len(files) == 0: return open_dropped_files(files)",if utils.is_media_file(uri) == True:
"def __walk_proceed_remote_dir_act(self, r, args): dirjs, filejs = args j = r.json() if 'list' not in j: self.pd(""Key 'list' not found in the response of directory listing request:\n{}"".format(j)) return const.ERequestFailed paths = j['list'] for path in paths: <IF_STMT> dirjs.append(path) else: filejs.append(path) return const.ENoError",if path['isdir']:
"def TaskUpdatesVerbose(task, progress): if isinstance(task.info.progress, int): info = task.info <IF_STMT> progress = '%d%% (%s)' % (info.progress, info.state) print('Task %s (key:%s, desc:%s) - %s' % (info.name.info.name, info.key, info.description, progress))","if not isinstance(progress, str):"
"def dump_constants(header): output = StringIO.StringIO() output.write(header) for attribute in dir(FSEvents): value = getattr(FSEvents, attribute) <IF_STMT> output.write('%s = %s\n' % (attribute, hex(value))) content = output.getvalue() output.close() return content","if attribute.startswith('k') and isinstance(value, int):"
"def _ensure_data_is_loaded(self, sql_object, input_params, stdin_file, stdin_filename='-', stop_after_analysis=False): data_loads = [] for filename in sql_object.qtable_names: data_load = self._load_data(filename, input_params, stdin_file=stdin_file, stdin_filename=stdin_filename, stop_after_analysis=stop_after_analysis) <IF_STMT> data_loads.append(data_load) return data_loads",if data_load is not None:
"def _get_instantiation(self): if self._data is None: f, l, c, o = (c_object_p(), c_uint(), c_uint(), c_uint()) SourceLocation_loc(self, byref(f), byref(l), byref(c), byref(o)) <IF_STMT> f = File(f) else: f = None self._data = (f, int(l.value), int(c.value), int(c.value)) return self._data",if f:
def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() <IF_STMT> if splitrow[0] == 'INFO:': infos.append(' '.join(splitrow[1:])) return infos,if len(splitrow) > 0:
"def _brush_modified_cb(self, settings): """"""Updates the brush's base setting adjustments on brush changes"""""" for cname in settings: adj = self.brush_adjustment.get(cname, None) <IF_STMT> continue value = self.brush.get_base_value(cname) adj.set_value(value)",if adj is None:
"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = {'common': 'dns_ip'} if 'node' not in facts: facts['node'] = {} for role in params.keys(): if role in facts: for param in params[role]: <IF_STMT> facts['node'][param] = facts[role].pop(param) return facts",if param in facts[role]:
"def serialize_content_range(value): if isinstance(value, (tuple, list)): <IF_STMT> raise ValueError('When setting content_range to a list/tuple, it must be length 2 or 3 (not %r)' % value) if len(value) == 2: begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value","if len(value) not in (2, 3):"
"def clean(self): data = super().clean() if data.get('expires'): <IF_STMT> data['expires'] = make_aware(datetime.combine(data['expires'], time(hour=23, minute=59, second=59)), self.instance.event.timezone) else: data['expires'] = data['expires'].replace(hour=23, minute=59, second=59) if data['expires'] < now(): raise ValidationError(_('The new expiry date needs to be in the future.')) return data","if isinstance(data['expires'], date):"
"def _build(self, obj, stream, context): if self.include_name: name, obj = obj for sc in self.subcons: <IF_STMT> sc._build(obj, stream, context) return else: for sc in self.subcons: stream2 = BytesIO() context2 = context.__copy__() try: sc._build(obj, stream2, context2) except Exception: pass else: context.__update__(context2) stream.write(stream2.getvalue()) return raise SelectError('no subconstruct matched', obj)",if sc.name == name:
"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource('dynamodb').Table('Sphere11.Dev.ResourceLocks') results = table.scan() for r in results['Items']: <IF_STMT> r['LockDate'] = datetime.fromtimestamp(r['LockDate']) if 'RevisionDate' in r: r['RevisionDate'] = datetime.fromtimestamp(r['RevisionDate']) print(tabulate.tabulate(results['Items'], headers='keys', tablefmt='fancy_grid'))",if 'LockDate' in r:
"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): <IF_STMT> if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",if type(test.value) in self._const_types:
"def validate_max_discount(self): if self.rate_or_discount == 'Discount Percentage' and self.get('items'): for d in self.items: max_discount = frappe.get_cached_value('Item', d.item_code, 'max_discount') <IF_STMT> throw(_('Max discount allowed for item: {0} is {1}%').format(self.item_code, max_discount))",if max_discount and flt(self.discount_percentage) > flt(max_discount):
"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if 'identifiers' in rule and rule['identifiers'] is not None: for i_type, i_value in rule['identifiers'].items(): if i_type[0:3] == 'cce': <IF_STMT> return True return False",if not checks.is_cce_value_valid('CCE-' + str(i_value)):
"def parse_calendar_eras(data, calendar): eras = data.setdefault('eras', {}) for width in calendar.findall('eras/*'): width_type = NAME_MAP[width.tag] widths = eras.setdefault(width_type, {}) for elem in width.getiterator(): if elem.tag == 'era': _import_type_text(widths, elem, type=int(elem.attrib.get('type'))) <IF_STMT> eras[width_type] = Alias(_translate_alias(['eras', width_type], elem.attrib['path']))",elif elem.tag == 'alias':
"def validate_grammar() -> None: for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE: fn_productions = get_productions(fn) if all((p.name == fn_productions[0].name for p in fn_productions)): production_name = fn_productions[0].name expected_name = f'convert_{production_name}' <IF_STMT> raise Exception(f""The conversion function for '{production_name}' "" + f""must be called '{expected_name}', not '{fn.__name__}'."")",if fn.__name__ != expected_name:
"def split_ratio(row): if float(row['Numerator']) > 0: <IF_STMT> n, m = row['Splitratio'].split(':') return float(m) / float(n) else: return eval(row['Splitratio']) else: return 1",if ':' in row['Splitratio']:
def _handle_def_errors(testdef): if testdef.error: if testdef.exception: <IF_STMT> raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception('Test parse failure'),"if isinstance(testdef.exception, Exception):"
"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): if count <= 0: quotas_ok[quota] = 0 break avail = qa.results[quota] <IF_STMT> quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",if avail[1] is not None and avail[1] < count:
"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = (rb.data[ri], lb.data[li]) li += 1 <IF_STMT> lb = lb.rightlink li = 0 ri -= 1 if ri < 0: rb = rb.leftlink ri = BLOCKLEN - 1",if li >= BLOCKLEN:
"def __manipulate_item(self, item): if self._Cursor__manipulate: db = self._Cursor__collection.database son = db._fix_outgoing(item, self._Cursor__collection) else: son = item if self.__wrap is not None: <IF_STMT> return getattr(self._Cursor__collection, son[self.__wrap.type_field])(son) return self.__wrap(son, collection=self._Cursor__collection) else: return son",if self.__wrap.type_field in son:
"def apply_transforms(self): """"""Apply all of the stored transforms, in priority order."""""" self.document.reporter.attach_observer(self.document.note_transform_message) while self.transforms: <IF_STMT> self.transforms.sort() self.transforms.reverse() self.sorted = 1 priority, transform_class, pending, kwargs = self.transforms.pop() transform = transform_class(self.document, startnode=pending) transform.apply(**kwargs) self.applied.append((priority, transform_class, pending, kwargs))",if not self.sorted:
"def format_sql(sql, params): rv = [] if isinstance(params, dict): conv = _FormatConverter(params) <IF_STMT> sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): if param is None: rv.append('NULL') param = safe_repr(param) rv.append(param) return (sql, rv)",if params:
"def on_execution_item(self, cpath, execution): if not isinstance(execution, dict): return if 'executor' in execution and execution.get('executor') != 'jmeter': return scenario = execution.get('scenario', None) <IF_STMT> return if isinstance(scenario, str): scenario_name = scenario scenario = self.get_named_scenario(scenario_name) if not scenario: scenario = None scenario_path = Path('scenarios', scenario_name) else: scenario_path = cpath.copy() scenario_path.add_component('scenario') if scenario is not None: self.check_jmeter_scenario(scenario_path, scenario)",if not scenario:
"def _poll_ipc_requests(self) -> None: try: if self._ipc_requests.empty(): return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: <IF_STMT> self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception('Problem processing ipc request', exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",if os.path.isfile(filename):
"def get_scroll_distance_to_element(driver, element): try: scroll_position = driver.execute_script('return window.scrollY;') element_location = None element_location = element.location['y'] element_location = element_location - 130 <IF_STMT> element_location = 0 distance = element_location - scroll_position return distance except Exception: return 0",if element_location < 0:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_access_token(d.getPrefixedString()) continue <IF_STMT> self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:
"def _validate_and_define(params, key, value): key, force_generic = _validate_key(_unescape(key)) if key in params: raise SyntaxError(f'duplicate key ""{key}""') cls = _class_for_key.get(key, GenericParam) emptiness = cls.emptiness() if value is None: if emptiness == Emptiness.NEVER: raise SyntaxError('value cannot be empty') value = cls.from_value(value) el<IF_STMT> value = cls.from_wire_parser(dns.wire.Parser(_unescape(value))) else: value = cls.from_value(value) params[key] = value",if force_generic:
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): <IF_STMT> continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if exclude_meta and field.meta:
"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop() <IF_STMT> self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.sl_hdlr:
"def _wait_for_async_copy(self, share_name, file_path): count = 0 share_client = self.fsc.get_share_client(share_name) file_client = share_client.get_file_client(file_path) properties = file_client.get_file_properties() while properties.copy.status != 'success': count = count + 1 <IF_STMT> self.fail('Timed out waiting for async copy to complete.') self.sleep(6) properties = file_client.get_file_properties() self.assertEqual(properties.copy.status, 'success')",if count > 10:
"def __new__(cls, message_type: OrderBookMessageType, content: Dict[str, any], timestamp: Optional[float]=None, *args, **kwargs): if timestamp is None: <IF_STMT> raise ValueError('timestamp must not be None when initializing snapshot messages.') timestamp = int(time.time()) return super(KucoinOrderBookMessage, cls).__new__(cls, message_type, content, *args, timestamp=timestamp, **kwargs)",if message_type is OrderBookMessageType.SNAPSHOT:
"def _drop_unique_features(X: DataFrame, feature_metadata: FeatureMetadata, max_unique_ratio) -> list: features_to_drop = [] X_len = len(X) max_unique_value_count = X_len * max_unique_ratio for column in X: unique_value_count = len(X[column].unique()) <IF_STMT> features_to_drop.append(column) elif feature_metadata.get_feature_type_raw(column) in [R_CATEGORY, R_OBJECT] and unique_value_count > max_unique_value_count: features_to_drop.append(column) return features_to_drop",if unique_value_count == 1:
"def get_src_findex_by_pad(s, S, padding_mode, align_corners): if padding_mode == 'zero': return get_src_findex_with_zero_pad(s, S) elif padding_mode == 'reflect': <IF_STMT> return get_src_findex_with_reflect_pad(s, S, True) else: sf = get_src_findex_with_reflect_pad(s, S, False) return get_src_findex_with_repeat_pad(sf, S) elif padding_mode == 'repeat': return get_src_findex_with_repeat_pad(s, S)",if align_corners:
"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) if current._parent is upto: break <IF_STMT> raise sa_exc.InvalidRequestError('Transaction %s is not on the active transaction list' % upto) else: current = current._parent return result",elif current._parent is None:
"def __setattr__(self, name: str, val: Any): if name.startswith('COMPUTED_'): <IF_STMT> old_val = self[name] if old_val == val: return raise KeyError(""Computed attributed '{}' already exists with a different value! old={}, new={}."".format(name, old_val, val)) self[name] = val else: super().__setattr__(name, val)",if name in self:
"def get_fnlist(bbhandler, pkg_pn, preferred): """"""Get all recipe file names"""""" <IF_STMT> latest_versions, preferred_versions = bb.providers.findProviders(bbhandler.config_data, bbhandler.cooker.recipecaches[''], pkg_pn) fn_list = [] for pn in sorted(pkg_pn): if preferred: fn_list.append(preferred_versions[pn][1]) else: fn_list.extend(pkg_pn[pn]) return fn_list",if preferred:
"def links_extracted(self, _, links): links_deduped = {} for link in links: link_fingerprint = link.meta[FIELD_FINGERPRINT] <IF_STMT> continue links_deduped[link_fingerprint] = link [self._redis_pipeline.hmset(fingerprint, self._create_link_extracted(link)) for fingerprint, link in links_deduped.items()] self._redis_pipeline.execute()",if link_fingerprint in links_deduped:
"def __call__(self, name, rawtext, text, lineno, inliner, options=None, content=None): options = options or {} content = content or [] issue_nos = [each.strip() for each in utils.unescape(text).split(',')] config = inliner.document.settings.env.app.config ret = [] for i, issue_no in enumerate(issue_nos): node = self.make_node(name, issue_no, config, options=options) ret.append(node) <IF_STMT> sep = nodes.raw(text=', ', format='html') ret.append(sep) return (ret, [])",if i != len(issue_nos) - 1:
"def init_messengers(messengers): for messenger in messengers: <IF_STMT> module_path = messenger['type'] messenger['type'] = messenger['type'].split('.')[-1] else: module_path = 'oncall.messengers.' + messenger['type'] instance = getattr(importlib.import_module(module_path), messenger['type'])(messenger) for transport in instance.supports: _active_messengers[transport].append(instance)",if '.' in messenger['type']:
"def _process_enum_definition(self, tok): fields = [] for field in tok.fields: <IF_STMT> expression = self.expression_parser.parse(field.expression) else: expression = None fields.append(c_ast.CEnumField(name=field.name.first, value=expression)) name = tok.enum_name if name: name = 'enum %s' % tok.enum_name.first else: name = self._make_anonymous_type('enum') return c_ast.CTypeDefinition(name=name, type_definition=c_ast.CEnum(attributes=tok.attributes, fields=fields, name=name))",if field.expression:
def result_iterator(): try: fs.reverse() while fs: <IF_STMT> yield fs.pop().result() else: yield fs.pop().result(end_time - time.time()) finally: for future in fs: future.cancel(),if timeout is None:
def has_encrypted_ssh_key_data(self): try: ssh_key_data = self.get_input('ssh_key_data') except AttributeError: return False try: pem_objects = validate_ssh_private_key(ssh_key_data) for pem_object in pem_objects: <IF_STMT> return True except ValidationError: pass return False,"if pem_object.get('key_enc', False):"
"def test_seq_object_transcription_method(self): for nucleotide_seq in test_seqs: <IF_STMT> self.assertEqual(repr(Seq.transcribe(nucleotide_seq)), repr(nucleotide_seq.transcribe()))","if isinstance(nucleotide_seq, Seq.Seq):"
def max_elevation(self): max_el = None for y in xrange(self.height): for x in xrange(self.width): el = self.elevation['data'][y][x] <IF_STMT> max_el = el return max_el,if max_el is None or el > max_el:
"def stress(mapping, index): for count in range(OPERATIONS): function = random.choice(functions) function(mapping, index) <IF_STMT> print('\r', len(mapping), ' ' * 7, end='') print()",if count % 1000 == 0:
"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): if 'terminology' not in source.all_flags: continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True <IF_STMT> continue missing.append((source.context, source.source, '')) if missing: self.add_units(None, missing)",if not add:
"def get_generators(self): """"""Get a dict with all registered generators, indexed by name"""""" generators = {} for core in self.db.find(): <IF_STMT> _generators = core.get_generators({}) if _generators: generators[str(core.name)] = _generators return generators","if hasattr(core, 'get_generators'):"
"def act(self, state): if self.body.env.clock.frame < self.training_start_step: return policy_util.random(state, self, self.body).cpu().squeeze().numpy() else: action = self.action_policy(state, self, self.body) <IF_STMT> action = self.scale_action(torch.tanh(action)) return action.cpu().squeeze().numpy()",if not self.body.is_discrete:
"def try_open_completions_event(self, event=None): """"""(./) Open completion list after pause with no movement."""""" lastchar = self.text.get('insert-1c') if lastchar in TRIGGERS: args = TRY_A if lastchar == '.' else TRY_F self._delayed_completion_index = self.text.index('insert') <IF_STMT> self.text.after_cancel(self._delayed_completion_id) self._delayed_completion_id = self.text.after(self.popupwait, self._delayed_open_completions, args)",if self._delayed_completion_id is not None:
def token_is_available(self): if self.token: try: resp = requests.get('https://api.shodan.io/account/profile?key={0}'.format(self.token)) <IF_STMT> return True except Exception as ex: logger.error(str(ex)) return False,if resp and resp.status_code == 200 and ('member' in resp.json()):
"def next_bar_(self, event): bars = event.bar_dict self._current_minute = self._minutes_since_midnight(self.ucontext.now.hour, self.ucontext.now.minute) for day_rule, time_rule, func in self._registry: <IF_STMT> with ExecutionContext(EXECUTION_PHASE.SCHEDULED): with ModifyExceptionFromType(EXC_TYPE.USER_EXC): func(self.ucontext, bars) self._last_minute = self._current_minute",if day_rule() and time_rule():
"def decoder(s): r = [] decode = [] for c in s: if c == '&' and (not decode): decode.append('&') <IF_STMT> if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",elif c == '-' and decode:
"def admin_audit_get(admin_id): if settings.app.demo_mode: resp = utils.demo_get_cache() <IF_STMT> return utils.jsonify(resp) if not flask.g.administrator.super_user: return utils.jsonify({'error': REQUIRES_SUPER_USER, 'error_msg': REQUIRES_SUPER_USER_MSG}, 400) admin = auth.get_by_id(admin_id) resp = admin.get_audit_events() if settings.app.demo_mode: utils.demo_set_cache(resp) return utils.jsonify(resp)",if resp:
"def vjp(self, argnum, outgrad, ans, vs, gvs, args, kwargs): try: return self.vjps[argnum](outgrad, ans, vs, gvs, *args, **kwargs) except KeyError: <IF_STMT> errstr = 'Gradient of {0} not yet implemented.' else: errstr = 'Gradient of {0} w.r.t. arg number {1} not yet implemented.' raise NotImplementedError(errstr.format(self.fun.__name__, argnum))",if self.vjps == {}:
"def update(self, *args, **kwargs): assert not self.readonly longest_key = 0 _dict = self._dict reverse = self.reverse casereverse = self.casereverse for iterable in args + (kwargs,): <IF_STMT> iterable = iterable.items() for key, value in iterable: longest_key = max(longest_key, len(key)) _dict[key] = value reverse[value].append(key) casereverse[value.lower()][value] += 1 self._longest_key = max(self._longest_key, longest_key)","if isinstance(iterable, (dict, StenoDictionary)):"
"def update_ui(self, window): view = window.get_active_view() self.set_status(view) lang = 'plain_text' if view: buf = view.get_buffer() language = buf.get_language() <IF_STMT> lang = language.get_id() self.setup_smart_indent(view, lang)",if language:
"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): if name not in skip: name = '__%s__' % name <IF_STMT> res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)","if hasattr(a, name):"
"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: <IF_STMT> return 0.03 + ctrl.labelHeight * 3 elif item['layout'] == 'vert': return ctrl.labelHeight * len(item['options'])",if item['layout'] == 'horiz':
"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): if key in ('disabled', 'is_up'): <IF_STMT> value = value == 'true' self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)",if value is not None:
"def _read_json_content(self, body_is_optional=False): if 'content-length' not in self.headers: return self.send_error(411) if not body_is_optional else {} try: content_length = int(self.headers.get('content-length')) if content_length == 0 and body_is_optional: return {} request = json.loads(self.rfile.read(content_length).decode('utf-8')) <IF_STMT> return request except Exception: logger.exception('Bad request') self.send_error(400)","if isinstance(request, dict) and (request or body_is_optional):"
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, '_viewcode_modules', {}) for modname, entry in list(modules.items()): <IF_STMT> continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) if len(used) == 0: modules.pop(modname)",if entry is False:
"def frames(self): """"""an array of all the frames (including iframes) in the current window"""""" from thug.DOM.W3C.HTML.HTMLCollection import HTMLCollection frames = set() for frame in self._findAll(['frame', 'iframe']): <IF_STMT> from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation DOMImplementation.createHTMLElement(self.window.doc, frame) frames.add(frame._node) return HTMLCollection(self.doc, list(frames))","if not getattr(frame, '_node', None):"
"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: <IF_STMT> return not kw['dir'] ^ isdir(self.strpath) if 'file' in kw: return not kw['file'] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",if 'dir' in kw:
"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): <IF_STMT> continue if other_path == path: duplicates.append(other_folder) if len(duplicates): self.duplicates[folder] = duplicates",if other_folder == folder:
"def next(self, buf, pos): if pos >= len(buf): return (EOF, '', pos) mo = self.tokens_re.match(buf, pos) if mo: text = mo.group() type, regexp, test_lit = self.tokens[mo.lastindex - 1] pos = mo.end() <IF_STMT> type = self.literals.get(text, type) return (type, text, pos) else: c = buf[pos] return (self.symbols.get(c, None), c, pos + 1)",if test_lit:
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) <IF_STMT> self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if i == self._skip - 2:
"def convert(self, ctx, argument): arg = argument.replace('0x', '').lower() if arg[0] == '#': arg = arg[1:] try: value = int(arg, base=16) if not 0 <= value <= 16777215: raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace(' ', '_') method = getattr(discord.Colour, arg, None) <IF_STMT> raise BadColourArgument(arg) return method()",if arg.startswith('from_') or method is None or (not inspect.ismethod(method)):
"def run(self, **inputs): if self.inputs.copy_inputs: self.inputs.subjects_dir = os.getcwd() <IF_STMT> inputs['subjects_dir'] = self.inputs.subjects_dir for originalfile in [self.inputs.in_file, self.inputs.in_norm]: copy2subjdir(self, originalfile, folder='mri') return super(SegmentCC, self).run(**inputs)",if 'subjects_dir' in inputs:
"def get_queryset(self): if not hasattr(self, '_queryset'): <IF_STMT> qs = self.queryset else: qs = self.model._default_manager.get_queryset() if not qs.ordered: qs = qs.order_by(self.model._meta.pk.name) self._queryset = qs return self._queryset",if self.queryset is not None:
"def visit_simple_stmt(self, node: Node) -> Iterator[Line]: """"""Visit a statement without nested statements."""""" is_suite_like = node.parent and node.parent.type in STATEMENT if is_suite_like: <IF_STMT> yield from self.visit_default(node) else: yield from self.line(+1) yield from self.visit_default(node) yield from self.line(-1) else: if not self.is_pyi or not node.parent or (not is_stub_suite(node.parent)): yield from self.line() yield from self.visit_default(node)",if self.is_pyi and is_stub_body(node):
"def rawDataReceived(self, data): if self.timeout > 0: self.resetTimeout() self._pendingSize -= len(data) if self._pendingSize > 0: self._pendingBuffer.write(data) else: passon = b'' <IF_STMT> data, passon = (data[:self._pendingSize], data[self._pendingSize:]) self._pendingBuffer.write(data) rest = self._pendingBuffer self._pendingBuffer = None self._pendingSize = None rest.seek(0, 0) self._parts.append(rest.read()) self.setLineMode(passon.lstrip(b'\r\n'))",if self._pendingSize < 0:
"def handle(self, *args, **options): app_name = options.get('app_name') job_name = options.get('job_name') if app_name and (not job_name): job_name = app_name app_name = None if options.get('list_jobs'): print_jobs(only_scheduled=False, show_when=True, show_appname=True) else: <IF_STMT> print('Run a single maintenance job. Please specify the name of the job.') return self.runjob(app_name, job_name, options)",if not job_name:
"def _exportReceived(self, content, error=False, server=None, context={}, **kwargs): if error: <IF_STMT> self.error.emit(content['message'], True) else: self.error.emit(""Can't export the project from the server"", True) self.finished.emit() return self.finished.emit()",if content:
"def __iter__(self): n = self.n k = self.k j = int(np.ceil(n / k)) for i in range(k): test_index = np.zeros(n, dtype=bool) <IF_STMT> test_index[i * j:(i + 1) * j] = True else: test_index[i * j:] = True train_index = np.logical_not(test_index) yield (train_index, test_index)",if i < k - 1:
"def addType(self, graphene_type): meta = get_meta(graphene_type) if meta: <IF_STMT> self._typeMap[meta.name] = graphene_type else: raise Exception('Type {typeName} already exists in the registry.'.format(typeName=meta.name)) else: raise Exception('Cannot add unnamed type or a non-type to registry.')",if not graphene_type in self._typeMap:
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 <IF_STMT> bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 6:
"def _asStringList(self, sep=''): out = [] for item in self._toklist: <IF_STMT> out.append(sep) if isinstance(item, ParseResults): out += item._asStringList() else: out.append(str(item)) return out",if out and sep:
"def open_file_input(cli_parsed): files = glob.glob(os.path.join(cli_parsed.d, '*report.html')) if len(files) > 0: print('\n[*] Done! Report written in the ' + cli_parsed.d + ' folder!') print('Would you like to open the report now? [Y/n]') while True: try: response = input().lower() <IF_STMT> return True else: return strtobool(response) except ValueError: print('Please respond with y or n') else: print('[*] No report files found to open, perhaps no hosts were successful') return False",if response == '':
"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): <IF_STMT> realvalname, key = valname.split('.', 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: if name in self.values: self.__dict__[name] = config[name] del self._raw_config",if '.' in valname:
"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: <IF_STMT> exc_info = sys.exc_info() elif not isinstance(e, self.quiet_exceptions): app_log.error('Multiple exceptions in yield list', exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)",if exc_info is None:
"def test01e_json(self): """"""Testing GeoJSON input/output."""""" if not GEOJSON: return for g in self.geometries.json_geoms: geom = OGRGeometry(g.wkt) <IF_STMT> self.assertEqual(g.json, geom.json) self.assertEqual(g.json, geom.geojson) self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))","if not hasattr(g, 'not_equal'):"
"def __init__(self, hub=None): if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() <IF_STMT> _resolver.network_resolver.nameservers[:] = config.resolver_nameservers if config.resolver_timeout: _resolver.network_resolver.lifetime = config.resolver_timeout assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",if config.resolver_nameservers:
"def __iadd__(self, term): if isinstance(term, (int, long)): <IF_STMT> _gmp.mpz_add_ui(self._mpz_p, self._mpz_p, c_ulong(term)) return self if -65535 < term < 0: _gmp.mpz_sub_ui(self._mpz_p, self._mpz_p, c_ulong(-term)) return self term = Integer(term) _gmp.mpz_add(self._mpz_p, self._mpz_p, term._mpz_p) return self",if 0 <= term < 65536:
"def copy(dst, src): for k, v in src.iteritems(): <IF_STMT> d = {} dst[k] = d copy(d, v) else: dst[k] = v","if isinstance(v, dict):"
"def generator(self, data): self.procs = OrderedDict() for task in data: self.recurse_task(task, 0, 0, self.procs) for offset, name, level, pid, ppid, uid, euid, gid in self.procs.values(): <IF_STMT> yield (0, [Address(offset), str(name), str(level), int(pid), int(ppid), int(uid), int(gid), int(euid)])",if offset:
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) <IF_STMT> father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True if not mother_handle: return True return False",if family:
"def _arctic_task_exec(request): request.start_time = time.time() logging.debug('Executing asynchronous request for {}/{}'.format(request.library, request.symbol)) result = None try: request.is_running = True <IF_STMT> result = mongo_retry(request.fun)(*request.args, **request.kwargs) else: result = request.fun(*request.args, **request.kwargs) except Exception as e: request.exception = e finally: request.data = result request.end_time = time.time() request.is_running = False return result",if request.mongo_retry:
"def _setup_styles(self): for ttype, ndef in self.style: escape = EscapeSequence() <IF_STMT> escape.fg = self._color_index(ndef['color']) if ndef['bgcolor']: escape.bg = self._color_index(ndef['bgcolor']) if self.usebold and ndef['bold']: escape.bold = True if self.useunderline and ndef['underline']: escape.underline = True self.style_string[str(ttype)] = (escape.color_string(), escape.reset_string())",if ndef['color']:
"def process_string(self, remove_repetitions, sequence): string = '' for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: if remove_repetitions and i != 0 and (char == sequence[i - 1]): pass <IF_STMT> string += ' ' else: string = string + char return string",elif char == self.labels[self.space_index]:
"def arith_expr(self, nodelist): node = self.com_node(nodelist[0]) for i in range(2, len(nodelist), 2): right = self.com_node(nodelist[i]) <IF_STMT> node = Add(node, right, lineno=nodelist[1].context) elif nodelist[i - 1].type == token.MINUS: node = Sub(node, right, lineno=nodelist[1].context) else: raise ValueError('unexpected token: %s' % nodelist[i - 1][0]) return node",if nodelist[i - 1].type == token.PLUS:
"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = (index.start, index.stop) new_start, new_stop = (None, None) <IF_STMT> new_stop = length - start if stop is not None: new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",if start is not None:
"def getRoots(job): if job not in visited: visited.add(job) <IF_STMT> list(map(lambda p: getRoots(p), job._directPredecessors)) else: roots.add(job) list(map(lambda c: getRoots(c), job._children + job._followOns))",if len(job._directPredecessors) > 0:
"def visit_filter_projection(self, node, value): base = self.visit(node['children'][0], value) if not isinstance(base, list): return None comparator_node = node['children'][2] collected = [] for element in base: <IF_STMT> current = self.visit(node['children'][1], element) if current is not None: collected.append(current) return collected","if self._is_true(self.visit(comparator_node, element)):"
"def func(x, y): try: if x > y: z = x + 2 * math.sin(y) return z ** 2 <IF_STMT> return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print('finished')",elif x == y:
"def set_filter(self, dataset_opt): """"""This function create and set the pre_filter to the obj as attributes"""""" self.pre_filter = None for key_name in dataset_opt.keys(): <IF_STMT> new_name = key_name.replace('filters', 'filter') try: filt = instantiate_filters(getattr(dataset_opt, key_name)) except Exception: log.exception('Error trying to create {}, {}'.format(new_name, getattr(dataset_opt, key_name))) continue setattr(self, new_name, filt)",if 'filter' in key_name:
"def _add_states_to_lookup(self, trackers_as_states, trackers_as_actions, domain, online=False): """"""Add states to lookup dict"""""" for states in trackers_as_states: active_form = self._get_active_form_name(states[-1]) <IF_STMT> states = self._modified_states(states) feature_key = self._create_feature_key(states) self.lookup[feature_key] = active_form",if active_form and self._prev_action_listen_in_state(states[-1]):
"def list_loaded_payloads(self): print(helpers.color('\n [*] Available Payloads:\n')) lastBase = None x = 1 for name in sorted(self.active_payloads.keys()): parts = name.split('/') <IF_STMT> print() lastBase = parts[0] print('\t%s)\t%s' % (x, '{0: <24}'.format(name))) x += 1 print('\n') return",if lastBase and parts[0] != lastBase:
"def reprSmart(vw, item): ptype = type(item) if ptype is int: if -1024 < item < 1024: return str(item) <IF_STMT> return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) elif ptype is dict: return '{%s}' % ','.join(['%s:%s' % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]) else: return repr(item)",elif vw.isValidPointer(item):
"def ConfigSectionMap(section): config = ConfigParser.RawConfigParser() configurations = config_manager() configf = configurations.configf config.read(configf) dict1 = {} options = config.options(section) for option in options: try: dict1[option] = config.get(section, option) <IF_STMT> DebugPrint('skip: %s' % option) except: print('Exception on %s!' % option) dict1[option] = None return dict1",if dict1[option] == -1:
def on_success(result): subtasks = {} if result: subtasks = {self.nodes_keys.inverse[s['node_id']]: s.get('subtask_id') for s in result <IF_STMT>} if subtasks: print('subtask finished') self.next() else: print('waiting for a subtask to finish') time.sleep(10),if s.get('status') == 'Failure'
"def redirect_aware_commmunicate(p, sys=_sys): """"""Variant of process.communicate that works with in process I/O redirection."""""" assert sys is not None out, err = p.communicate() if redirecting_io(sys=sys): if out: out = unicodify(out) sys.stdout.write(out) out = None <IF_STMT> err = unicodify(err) sys.stderr.write(err) err = None return (out, err)",if err:
"def __exit__(self, *args, **kwargs): self._samples_cache = {} if is_validation_enabled() and isinstance(self.prior, dict): extra = set(self.prior) - self._param_hits <IF_STMT> warnings.warn(""pyro.module prior did not find params ['{}']. Did you instead mean one of ['{}']?"".format(""', '"".join(extra), ""', '"".join(self._param_misses))) return super().__exit__(*args, **kwargs)",if extra:
def __download_thread(self): while True: <IF_STMT> self.__current_download = self.__queue.get() self.__download_file(self.__current_download) time.sleep(0.1),if not self.__queue.empty():
"def plot_timer_command(args): import nnabla.monitor as M format_unit = dict(s='seconds', m='minutes', h='hours', d='days') if not args.ylabel: <IF_STMT> args.ylabel = 'Total elapsed time [{}]'.format(format_unit[args.time_unit]) else: args.ylabel = 'Elapsed time [{}/iter]'.format(format_unit[args.time_unit]) plot_any_command(args, M.plot_time_elapsed, dict(elapsed=args.elapsed, unit=args.time_unit)) return True",if args.elapsed:
"def resolve_page(root: ChannelContext[models.MenuItem], info, **kwargs): if root.node.page_id: requestor = get_user_or_app_from_context(info.context) requestor_has_access_to_all = requestor.is_active and requestor.has_perm(PagePermissions.MANAGE_PAGES) return PageByIdLoader(info.context).load(root.node.page_id).then(lambda page: page <IF_STMT> else None) return None",if requestor_has_access_to_all or page.is_visible
"def _certonly_new_request_common(self, mock_client, args=None): with mock.patch('certbot._internal.main._find_lineage_for_domains_and_certname') as mock_renewal: mock_renewal.return_value = ('newcert', None) with mock.patch('certbot._internal.main._init_le_client') as mock_init: mock_init.return_value = mock_client <IF_STMT> args = [] args += '-d foo.bar -a standalone certonly'.split() self._call(args)",if args is None:
"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError('MultiDict can only be called with one positional argument') if args: <IF_STMT> items = list(args[0].iteritems()) elif hasattr(args[0], 'items'): items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())","if hasattr(args[0], 'iteritems'):"
"def test08_ExceptionTypes(self): self.assertTrue(issubclass(db.DBError, Exception)) for i, j in db.__dict__.items(): <IF_STMT> self.assertTrue(issubclass(j, db.DBError), msg=i) if i not in ('DBKeyEmptyError', 'DBNotFoundError'): self.assertFalse(issubclass(j, KeyError), msg=i) self.assertTrue(issubclass(db.DBKeyEmptyError, KeyError)) self.assertTrue(issubclass(db.DBNotFoundError, KeyError))",if i.startswith('DB') and i.endswith('Error'):
"def _delegate_to_sinks(self, value: Any) -> None: for sink in self._sinks: if isinstance(sink, AgentT): await sink.send(value=value) <IF_STMT> await cast(TopicT, sink).send(value=value) else: await maybe_async(cast(Callable, sink)(value))","elif isinstance(sink, ChannelT):"
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 <IF_STMT> depth -= 1 if depth == 0: break sel = str_in[start_pos + 1:pos] return sel",elif str_in[pos] == end_tag:
"def confirm(request): details = request.session.get('reauthenticate') if not details: return redirect('home') request.user = User.objects.get(pk=details['user_pk']) if request.method == 'POST': confirm_form = PasswordConfirmForm(request, request.POST) <IF_STMT> request.session.pop('reauthenticate') request.session['reauthenticate_done'] = True return redirect('social:complete', backend=details['backend']) else: confirm_form = PasswordConfirmForm(request) context = {'confirm_form': confirm_form} context.update(details) return render(request, 'accounts/confirm.html', context)",if confirm_form.is_valid():
"def verify_credentials(self): if self.enabled: response = requests.get('https://api.exotel.com/v1/Accounts/{sid}'.format(sid=self.account_sid), auth=(self.api_key, self.api_token)) <IF_STMT> frappe.throw(_('Invalid credentials'))",if response.status_code != 200:
"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == 'class': icon = 'class' el<IF_STMT> icon = 'method_priv' elif tok.visibility == 'protected': icon = 'method_prot' else: icon = 'method' crp.set_property('pixbuf', imagelibrary.pixbufs[icon])",if tok.visibility == 'private':
"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: omit = elem.tag == 'kw' and elem.get('type') != 'teardown' start = event == 'start' if omit and start: omitted_kws += 1 if not omitted_kws: yield (event, elem) <IF_STMT> elem.clear() if omit and (not start): omitted_kws -= 1",elif not start:
"def on_double_click(self, event): path = self.get_selected_path() kind = self.get_selected_kind() name = self.get_selected_name() if kind == 'file': <IF_STMT> self.open_file(path) else: self.open_path_with_system_app(path) elif kind == 'dir': self.request_focus_into(path) return 'break'",if self.should_open_name_in_thonny(name):
"def search_cve(db: DatabaseInterface, product: Product) -> dict: result = {} for query_result in db.fetch_multiple(QUERIES['cve_lookup']): cve_entry = CveDbEntry(*query_result) <IF_STMT> result[cve_entry.cve_id] = {'score2': cve_entry.cvss_v2_score, 'score3': cve_entry.cvss_v3_score, 'cpe_version': build_version_string(cve_entry)} return result","if _product_matches_cve(product, cve_entry):"
"def find_go_files_mtime(app_files): files, mtime = ([], 0) for f, mt in app_files.items(): if not f.endswith('.go'): continue <IF_STMT> continue files.append(f) mtime = max(mtime, mt) return (files, mtime)",if APP_CONFIG.nobuild_files.match(f):
"def wrapper(filename): mtime = getmtime(filename) with lock: <IF_STMT> old_mtime, result = cache.pop(filename) if old_mtime == mtime: cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) if len(cache) > max_size: cache.popitem(last=False) return result",if filename in cache:
"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) <IF_STMT> typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",if item[0]:
"def _show_encoders(self, *args, **kwargs): if issubclass(self.current_module.__class__, BasePayload): encoders = self.current_module.get_encoders() <IF_STMT> headers = ('Encoder', 'Name', 'Description') print_table(headers, *encoders, max_column_length=100) return print_error('No encoders available')",if encoders:
"def __init__(self): Builder.__init__(self, commandName='VCExpress.exe', formatName='msvcProject') for key in ['VS90COMNTOOLS', 'VC80COMNTOOLS', 'VC71COMNTOOLS']: <IF_STMT> self.programDir = os.path.join(os.environ[key], '..', 'IDE') if self.programDir is None: for version in ['9.0', '8', '.NET 2003']: msvcDir = 'C:\\Program Files\\Microsoft Visual Studio %s\\Common7\\IDE' % version if os.path.exists(msvcDir): self.programDir = msvcDir",if os.environ.has_key(key):
"def _inner(*args, **kwargs): component_manager = args[0].component_manager for condition_name in condition_names: condition_result, err_msg = component_manager.evaluate_condition(condition_name) <IF_STMT> raise ComponentStartConditionNotMetError(err_msg) if not component_manager.all_components_running(*components): raise ComponentsNotStartedError(f'the following required components have not yet started: {json.dumps(components)}') return method(*args, **kwargs)",if not condition_result:
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <IF_STMT> return self.tk.getdouble(svalue) else: return self.tk.getint(svalue) except (ValueError, TclError): pass return value",elif '.' in svalue:
"def check_songs(): desc = numeric_phrase('%d song', '%d songs', len(songs)) with Task(_('Rescan songs'), desc) as task: task.copool(check_songs) for i, song in enumerate(songs): song = song._song <IF_STMT> app.library.reload(song) task.update((float(i) + 1) / len(songs)) yield",if song in app.library:
"def initialize(self): nn.init.xavier_uniform_(self.linear.weight.data) if self.linear.bias is not None: self.linear.bias.data.uniform_(-1.0, 1.0) if self.self_layer: nn.init.xavier_uniform_(self.linear_self.weight.data) <IF_STMT> self.linear_self.bias.data.uniform_(-1.0, 1.0)",if self.linear_self.bias is not None:
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) <IF_STMT> if result: return not self.inverse elif not result: return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if self.any_match:
"def toterminal(self, tw): for element in self.chain: element[0].toterminal(tw) <IF_STMT> tw.line('') tw.line(element[2], yellow=True) super(ExceptionChainRepr, self).toterminal(tw)",if element[2] is not None:
"def runMainLoop(self): """"""The curses gui main loop."""""" self.curses_app = LeoApp() stdscr = curses.initscr() if 1: self.dump_keys() try: self.curses_app.run() finally: curses.nocbreak() stdscr.keypad(0) curses.echo() curses.endwin() <IF_STMT> g.pr('Exiting Leo...')",if 'shutdown' in g.app.debug:
"def test_chunkcoding(self): for native, utf8 in zip(*[StringIO(f).readlines() for f in self.tstring]): u = self.decode(native)[0] self.assertEqual(u, utf8.decode('utf-8')) <IF_STMT> self.assertEqual(native, self.encode(u)[0])",if self.roundtriptest:
"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): <IF_STMT> self.sanitize_allowlist.append(line.strip()) except OSError: if explicit: log.warning(""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file)",if not line.startswith('#'):
"def get_all_extensions(subtree=None): if subtree is None: subtree = full_extension_tree() result = [] if isinstance(subtree, dict): for value in subtree.values(): if isinstance(value, dict): result += get_all_extensions(value) <IF_STMT> result += value.extensions elif isinstance(value, (list, tuple)): result += value elif isinstance(subtree, (ContentTypeMapping, ContentTypeDetector)): result = subtree.extensions elif isinstance(subtree, (list, tuple)): result = subtree return result","elif isinstance(value, (ContentTypeMapping, ContentTypeDetector)):"
"def _configuration_dict_to_commandlist(name, config_dict): command_list = ['config:%s' % name] for key, value in config_dict.items(): <IF_STMT> if value: b = 'true' else: b = 'false' command_list.append('%s:%s' % (key, b)) else: command_list.append('%s:%s' % (key, value)) return command_list",if type(value) is bool:
"def _RewriteModinfo(self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None): new_modinfo = '' for line in modinfo.split('\x00'): <IF_STMT> continue if to_remove and line.split('=')[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split('=')[0]) if line.startswith('vermagic'): line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + '\x00' return new_modinfo",if not line:
"def zip_random_open_test(self, f, compression): self.make_test_archive(f, compression) with zipfile.ZipFile(f, 'r', compression) as zipfp: zipdata1 = [] with zipfp.open(TESTFN) as zipopen1: while True: read_data = zipopen1.read(randint(1, 1024)) <IF_STMT> break zipdata1.append(read_data) testdata = ''.join(zipdata1) self.assertEqual(len(testdata), len(self.data)) self.assertEqual(testdata, self.data)",if not read_data:
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update <IF_STMT> self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",if self._call_count > self.ctl or age > self.ttl:
"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res <IF_STMT> f = f.encode('utf-8') if type(v) == unicode: v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",if type(f) == unicode:
"def _finalize_setup_keywords(self): for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'): value = getattr(self, ep.name, None) <IF_STMT> ep.require(installer=self.fetch_build_egg) ep.load()(self, ep.name, value)",if value is not None:
"def test_attributes_types(self): if not self.connection.strategy.pooled: <IF_STMT> self.connection.refresh_server_info() self.assertEqual(type(self.connection.server.schema.attribute_types['cn']), AttributeTypeInfo)",if not self.connection.server.info:
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) <IF_STMT> return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)","elif isinstance(k, bool):"
"def list2rec(x, test=False): if test: vid = '{}_{:06d}_{:06d}'.format(x[0], int(x[1]), int(x[2])) label = -1 return (vid, label) else: vid = '{}_{:06d}_{:06d}'.format(x[1], int(x[2]), int(x[3])) <IF_STMT> vid = '{}/{}'.format(convert_label(x[0]), vid) else: assert level == 1 label = class_mapping[convert_label(x[0])] return (vid, label)",if level == 2:
"def _expand_env(self, snapcraft_yaml): environment_keys = ['name', 'version'] for key in snapcraft_yaml: <IF_STMT> continue replacements = environment_to_replacements(get_snapcraft_global_environment(self.project)) snapcraft_yaml[key] = replace_attr(snapcraft_yaml[key], replacements) return snapcraft_yaml",if any((key == env_key for env_key in environment_keys)):
"def enableCtrls(self): for data in self.storySettingsData: name = data['name'] <IF_STMT> if 'requires' in data: set = self.getSetting(data['requires']) for i in self.ctrls[name]: i.Enable(set not in ['off', 'false', '0'])",if name in self.ctrls:
"def __init__(self, *args, **kwargs): super(ChallengePhaseCreateSerializer, self).__init__(*args, **kwargs) context = kwargs.get('context') if context: challenge = context.get('challenge') <IF_STMT> kwargs['data']['challenge'] = challenge.pk test_annotation = context.get('test_annotation') if test_annotation: kwargs['data']['test_annotation'] = test_annotation",if challenge:
def set_inactive(self): for title in self.gramplet_map: if self.gramplet_map[title].pui: <IF_STMT> self.gramplet_map[title].pui.active = False,if self.gramplet_map[title].gstate != 'detached':
"def authenticate(username, password): try: u = User.objects.get(username=username) <IF_STMT> userLogger.info('User logged in : %s', username) return u else: userLogger.warn('Attempt to log in to : %s', username) return False except DoesNotExist: return False","if check_password_hash(u.password, password):"
def _check_date(self): if not self.value: return None if not self.allow_date_in_past: if self.value < self.date_or_datetime().today(): <IF_STMT> self.value = self.date_or_datetime().today() else: self.value = self.date_or_datetime().today() + datetime.timedelta(1),if self.allow_todays_date:
"def update(self, E=None, **F): if E: <IF_STMT> for k in E: self[k] = E[k] else: for k, v in E: self[k] = v for k in F: self[k] = F[k]","if hasattr(E, 'keys'):"
"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): <IF_STMT> quotas_ok[quota] = 0 break avail = qa.results[quota] if avail[1] is not None and avail[1] < count: quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",if count <= 0:
"def gen_env_vars(): for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)): is_atty = fd.isatty() yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty))) <IF_STMT> yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b'')",if is_atty:
"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) <IF_STMT> k = str(k, 'utf-8') r[k] = v return r","if isinstance(k, bytes):"
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] <IF_STMT> dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",if attr not in node.attributes:
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: <IF_STMT> i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if dsn[i].isspace():
"def connect(self, buttons): for button in buttons: assert button is not None handled = False for handler_idx in range(0, len(self.__signal_handlers)): obj_class, signal, handler, handler_id = self.__signal_handlers[handler_idx] <IF_STMT> handler_id = button.connect(signal, handler) handled = True self.__signal_handlers[handler_idx] = (obj_class, signal, handler, handler_id) assert handled","if isinstance(button, obj_class):"
"def _parse_display(display): """"""Parse an X11 display value"""""" try: host, dpynum = display.rsplit(':', 1) if host.startswith('[') and host.endswith(']'): host = host[1:-1] idx = dpynum.find('.') <IF_STMT> screen = int(dpynum[idx + 1:]) dpynum = dpynum[:idx] else: screen = 0 except (ValueError, UnicodeEncodeError): raise ValueError('Invalid X11 display') from None return (host, dpynum, screen)",if idx >= 0:
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) <IF_STMT> os.remove(fn_full) elif fn.endswith('.md'): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",elif fn.endswith('.png'):
"def _sync_get(self, identifier, *args, **kw): self._mutex.acquire() try: try: <IF_STMT> return self._values[identifier] else: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value except KeyError: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value finally: self._mutex.release()",if identifier in self._values:
"def _query_fd(self): if self.stream is None: self._last_stat = (None, None) else: try: st = os.stat(self._filename) except OSError: e = sys.exc_info()[1] <IF_STMT> raise self._last_stat = (None, None) else: self._last_stat = (st[stat.ST_DEV], st[stat.ST_INO])",if e.errno != errno.ENOENT:
"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = '' if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) <IF_STMT> place_title = place_displayer.display(self.dbstate.db, place) if place_title != '': if len(place_title) > 25: text = place_title[:24] + '...' else: text = place_title return text",if place:
"def test_decoder_state(self): u = 'abc123' for encoding in all_unicode_encodings: <IF_STMT> self.check_state_handling_decode(encoding, u, u.encode(encoding)) self.check_state_handling_encode(encoding, u, u.encode(encoding))",if encoding not in broken_unicode_with_stateful:
"def cleanup(self): if os.path.exists(self.meta_gui_dir): for f in os.listdir(self.meta_gui_dir): <IF_STMT> os.remove(os.path.join(self.meta_gui_dir, f))",if os.path.splitext(f)[1] == '.desktop':
"def _have_applied_incense(self): for applied_item in inventory.applied_items().all(): self.logger.info(applied_item) <IF_STMT> mins = format_time(applied_item.expire_ms * 1000) self.logger.info('Not applying incense, currently active: %s, %s minutes remaining', applied_item.item.name, mins) return True else: self.logger.info('') return False return False",if applied_item.expire_ms > 0:
"def get_closest_point(self, point): point = to_point(point) cp, cd = (None, None) for p0, p1 in iter_pairs(self.pts, self.connected): diff = p1 - p0 l = diff.length d = diff / l pp = p0 + d * max(0, min(l, (point - p0).dot(d))) dist = (point - pp).length <IF_STMT> cp, cd = (pp, dist) return cp",if not cp or dist < cd:
"def process_return(lines): for line in lines: m = re.fullmatch('(?P<param>\\w+)\\s+:\\s+(?P<type>[\\w.]+)', line) <IF_STMT> yield f""**{m['param']}** : :class:`~{m['type']}`"" else: yield line",if m:
"def _classify(nodes_by_level): missing, invalid, downloads = ([], [], []) for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) <IF_STMT> invalid.append(node) elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD): downloads.append(node) return (missing, invalid, downloads)",elif node.binary == BINARY_INVALID:
"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: <IF_STMT> date_hdr = date_hdr.split(';')[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) if msg_ts > time.time() + 24 * 3600 or msg_ts < 1: return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",if ';' in date_hdr:
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): <IF_STMT> changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",if value != 1:
"def _rewrite_prepend_append(self, string, prepend, append=None): if append is None: append = prepend if not isinstance(string, StringElem): string = StringElem(string) string.sub.insert(0, prepend) if unicode(string).endswith(u'\n'): try: lastnode = string.flatten()[-1] <IF_STMT> lastnode.sub[-1] = lastnode.sub[-1].rstrip(u'\n') except IndexError: pass string.sub.append(append + u'\n') else: string.sub.append(append) return string","if isinstance(lastnode.sub[-1], unicode):"
"def parse_indentless_sequence_entry(self): if self.check_token(BlockEntryToken): token = self.get_token() <IF_STMT> self.states.append(self.parse_indentless_sequence_entry) return self.parse_block_node() else: self.state = self.parse_indentless_sequence_entry return self.process_empty_scalar(token.end_mark) token = self.peek_token() event = SequenceEndEvent(token.start_mark, token.start_mark) self.state = self.states.pop() return event","if not self.check_token(BlockEntryToken, KeyToken, ValueToken, BlockEndToken):"
"def walk_directory(directory, verbose=False): """"""Iterates a directory's text files and their contents."""""" for dir_path, _, filenames in os.walk(directory): for filename in filenames: file_path = os.path.join(dir_path, filename) if os.path.isfile(file_path) and (not filename.startswith('.')): with io.open(file_path, 'r', encoding='utf-8') as file: <IF_STMT> print('Reading {}'.format(filename)) doc_text = file.read() yield (filename, doc_text)",if verbose:
"def set_bounds(self, x, y, width, height): if self.native: <IF_STMT> vertical_shift = self.frame.vertical_shift else: vertical_shift = 0 self.native.Size = Size(width, height) self.native.Location = Point(x, y + vertical_shift)",if self.interface.parent is None:
"def _check_x11(self, command=None, *, exc=None, exit_status=None, **kwargs): """"""Check requesting X11 forwarding"""""" with (yield from self.connect()) as conn: <IF_STMT> with self.assertRaises(exc): yield from _create_x11_process(conn, command, **kwargs) else: proc = (yield from _create_x11_process(conn, command, **kwargs)) yield from proc.wait() self.assertEqual(proc.exit_status, exit_status) yield from conn.wait_closed()",if exc:
"def repr(self): try: <IF_STMT> from infogami.infobase.utils import prepr return prepr(self.obj) else: return repr(self.obj) except: return 'failed' return render_template('admin/memory/object', self.obj)","if isinstance(self.obj, (dict, web.threadeddict)):"
"def add(self, tag, values): if tag not in self.different: if tag not in self: self[tag] = values <IF_STMT> self.different.add(tag) self[tag] = [''] self.counts[tag] += 1",elif self[tag] != values:
"def _on_geturl(self, event): selected = self._status_list.get_selected() if selected != -1: object_id = self._status_list.GetItemData(selected) download_item = self._download_list.get_item(object_id) url = download_item.url <IF_STMT> clipdata = wx.TextDataObject() clipdata.SetText(url) wx.TheClipboard.Open() wx.TheClipboard.SetData(clipdata) wx.TheClipboard.Close()",if not wx.TheClipboard.IsOpened():
"def escape2null(text): """"""Return a string with escape-backslashes converted to nulls."""""" parts = [] start = 0 while True: found = text.find('\\', start) <IF_STMT> parts.append(text[start:]) return ''.join(parts) parts.append(text[start:found]) parts.append('\x00' + text[found + 1:found + 2]) start = found + 2",if found == -1:
def _process_inner_views(self): for view in self.baseviews: for inner_class in view.get_uninit_inner_views(): for v in self.baseviews: <IF_STMT> view.get_init_inner_views().append(v),"if isinstance(v, inner_class) and v not in view.get_init_inner_views():"
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_url(d.getPrefixedString()) continue <IF_STMT> self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:
"def test_sample_output(): comment = 'SAMPLE OUTPUT' skip_files = ['__init__.py'] errors = [] for _file in sorted(MODULE_PATH.iterdir()): if _file.suffix == '.py' and _file.name not in skip_files: with _file.open() as f: <IF_STMT> errors.append((comment, _file)) if errors: line = 'Missing sample error(s) detected!\n\n' for error in errors: line += '`{}` is not in module `{}`\n'.format(*error) print(line[:-1]) assert False",if comment not in f.read():
"def _get_planner(name, path, source): for klass in _planners: <IF_STMT> LOG.debug('%r accepted %r (filename %r)', klass, name, path) return klass LOG.debug('%r rejected %r', klass, name) raise ansible.errors.AnsibleError(NO_METHOD_MSG + repr(invocation))","if klass.detect(path, source):"
"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(' , ') else: hasConst = not (self._const.__class__ in native_numeric_types and self._const == 0) <IF_STMT> idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0): ostream.write(' - ') else: ostream.write(' + ')",if hasConst:
"def cluster_info_query(self): if self._major_version >= 90600: extra = ', CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END, slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver()' <IF_STMT> extra = 'timeline_id' + extra + ', pg_catalog.pg_control_checkpoint()' else: extra = '0' + extra else: extra = '0, NULL, NULL, NULL' return ('SELECT ' + self.TL_LSN + ', {2}').format(self.wal_name, self.lsn_name, extra)",if self.role == 'standby_leader':
"def __init__(self, *args, **kwargs): self.country = kwargs.pop('country') self.fields_needed = kwargs.pop('fields_needed', []) super(DynamicManagedAccountForm, self).__init__(*args, **kwargs) for f in self.fields_needed: <IF_STMT> field_name, field = FIELDS_BY_COUNTRY[self.country][f] self.fields[field_name] = field","if f in FIELDS_BY_COUNTRY.get(self.country, {}):"
"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): <IF_STMT> query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) if not query_map[alias]: query_map.pop(alias) return query_map",if vm_details == 'Absent':
"def on_strokes_edited(self): strokes = self._strokes() if strokes: translation = self._engine.raw_lookup(strokes) <IF_STMT> fmt = _('{strokes} maps to {translation}') else: fmt = _('{strokes} is not in the dictionary') info = self._format_label(fmt, (strokes,), translation) else: info = '' self.strokes_info.setText(info)",if translation is not None:
def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 <IF_STMT> self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release(),if self.count == 0:
"def _cat_blob(self, gcs_uri): """""":py:meth:`cat_file`, minus decompression."""""" blob = self._get_blob(gcs_uri) if not blob: return start = 0 while True: end = start + _CAT_CHUNK_SIZE try: chunk = blob.download_as_string(start=start, end=end) except google.api_core.exceptions.RequestRangeNotSatisfiable: return yield chunk <IF_STMT> return start = end",if len(chunk) < _CAT_CHUNK_SIZE:
"def device_iter(**kwargs): for dev in backend.enumerate_devices(): d = Device(dev, backend) tests = (val == _try_getattr(d, key) for key, val in kwargs.items()) <IF_STMT> yield d",if _interop._all(tests) and (custom_match is None or custom_match(d)):
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'): if isfile(self.object): with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() <IF_STMT> vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs","elif hasattr(self.object, 'read'):"
"def _execute_with_error(command, error, message): try: cli.invocation = cli.invocation_cls(cli_ctx=cli, parser_cls=cli.parser_cls, commands_loader_cls=cli.commands_loader_cls, help_cls=cli.help_cls) cli.invocation.execute(command.split()) except CLIError as ex: <IF_STMT> raise AssertionError('{}\nExpected: {}\nActual: {}'.format(message, error, ex)) return except Exception as ex: raise ex raise AssertionError(""exception not raised for '{0}'"".format(message))",if error not in str(ex):
"def ray_intersection(self, p, line): p = Vector(center(line.sites)) min_r = BIG_FLOAT nearest = None for v_i, v_j in self.edges: bound = LineEquation2D.from_two_points(v_i, v_j) intersection = bound.intersect_with_line(line) if intersection is not None: r = (p - intersection).length <IF_STMT> nearest = intersection min_r = r return nearest",if r < min_r:
"def CalculateChecksum(data): if isinstance(data, bytearray): total = sum(data) elif isinstance(data, bytes): <IF_STMT> total = sum(map(ord, data)) else: total = sum(data) else: total = sum(map(ord, data)) return total & 4294967295","if data and isinstance(data[0], bytes):"
"def __mul__(self, other: Union['Tensor', float]) -> 'Tensor': if isinstance(other, Tensor): <IF_STMT> errstr = f""Given backens are inconsistent. Found '{self.backend.name}'and '{other.backend.name}'"" raise ValueError(errstr) other = other.array array = self.backend.multiply(self.array, other) return Tensor(array, backend=self.backend)",if self.backend.name != other.backend.name:
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: <IF_STMT> self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",if i == start:
"def resolve_none(self, data): for tok_idx in range(len(data)): for feat_idx in range(len(data[tok_idx])): <IF_STMT> data[tok_idx][feat_idx] = '_' return data",if data[tok_idx][feat_idx] is None:
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) if not isinstance(n, _strtypes): raise TypeError('on must be a name or field, not: {0}'.format(n)) <IF_STMT> raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))",elif n not in fields:
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:
"def func_std_string(func_name): if func_name[:2] == ('~', 0): name = func_name[2] <IF_STMT> return '{%s}' % name[1:-1] else: return name else: return '%s:%d(%s)' % func_name",if name.startswith('<') and name.endswith('>'):
"def f(): try: for n in cycle([1, 19]): s = bufio.read(n) <IF_STMT> break results.append(s) except Exception as e: errors.append(e) raise",if not s:
"def stop(self): try: self.rpcserver.stop() if self.backend_rpcserver: self.backend_rpcserver.stop() <IF_STMT> self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",if self.cluster_rpcserver:
"def download(cls, architecture, path='./'): if cls.sanity_check(architecture): architecture_file = download_file(cls.architecture_map[architecture], directory=path) <IF_STMT> return None print('Coreml model {} is saved in [{}]'.format(architecture, path)) return architecture_file else: return None",if not architecture_file:
"def opps_output_converter(kpt_list): kpts = [] mpii_keys = to_opps_converter.keys() for mpii_idx in range(0, 16): <IF_STMT> model_idx = to_opps_converter[mpii_idx] x, y = kpt_list[model_idx] if x < 0 or y < 0: kpts += [0.0, 0.0, -1.0] else: kpts += [x, y, 1.0] else: kpts += [0.0, 0.0, -1.0] return kpts",if mpii_idx in mpii_keys:
"def _get_headers(self, headers=None): request_headers = headers or {} if self._client.client.config: config = self._client.client.config if 'Authorization' not in request_headers and config.token: request_headers.update({'Authorization': '{} {}'.format(config.authentication_type, config.token)}) <IF_STMT> request_headers.update({config.header: config.header_service}) return request_headers",if config.header and config.header_service:
"def get_last_traded_prices(cls, trading_pairs: List[str]) -> Dict[str, float]: results = dict() async with aiohttp.ClientSession() as client: resp = await client.get(f'{constants.REST_URL}/tickers') resp_json = await resp.json() for trading_pair in trading_pairs: resp_record = [o for o in resp_json <IF_STMT>][0] results[trading_pair] = float(resp_record['price']) return results",if o['symbol'] == convert_to_exchange_trading_pair(trading_pair)
"def reset_two_factor_hotp(): uid = request.form['uid'] otp_secret = request.form.get('otp_secret', None) if otp_secret: user = Journalist.query.get(uid) <IF_STMT> return render_template('admin_edit_hotp_secret.html', uid=uid) db.session.commit() return redirect(url_for('admin.new_user_two_factor', uid=uid)) else: return render_template('admin_edit_hotp_secret.html', uid=uid)","if not validate_hotp_secret(user, otp_secret):"
"def ctx_for_video(self, vurl): """"""Get a context dict for a given video URL"""""" ctx = self.get_context_dict() for portal, match, context_fn in self.PORTALS: <IF_STMT> try: ctx.update(context_fn(vurl)) ctx['portal'] = portal break except AttributeError: continue return ctx",if match.search(vurl):
"def get(self): name = request.args.get('filename') if name is not None: opts = dict() opts['type'] = 'episode' result = guessit(name, options=opts) res = dict() if 'episode' in result: res['episode'] = result['episode'] else: res['episode'] = 0 if 'season' in result: res['season'] = result['season'] else: res['season'] = 0 <IF_STMT> res['subtitle_language'] = str(result['subtitle_language']) return jsonify(data=res) else: return ('', 400)",if 'subtitle_language' in result:
"def package_files(package_path, directory_name): paths = [] directory_path = os.path.join(package_path, directory_name) for path, directories, filenames in os.walk(directory_path): relative_path = os.path.relpath(path, package_path) for filename in filenames: <IF_STMT> continue paths.append(os.path.join(relative_path, filename)) return paths",if filename[0] == '.':
"def parse_simple(d, data): units = {} for v in data[d]: key = v['name'] if not key: continue key_to_insert = make_key(key) <IF_STMT> index = 2 tmp = f'{key_to_insert}_{index}' while tmp in units: index += 1 tmp = f'{key_to_insert}_{index}' key_to_insert = tmp units[key_to_insert] = v['id'] return units",if key_to_insert in units:
"def parse_clademodelc(branch_type_no, line_floats, site_classes): """"""Parse results specific to the clade model C."""""" if not site_classes or len(line_floats) == 0: return for n in range(len(line_floats)): <IF_STMT> site_classes[n]['branch types'] = {} site_classes[n]['branch types'][branch_type_no] = line_floats[n] return site_classes",if site_classes[n].get('branch types') is None:
"def track_modules(self, *modules): """"""Add module names to the tracked list."""""" already_tracked = self.session.GetParameter('autodetect_build_local_tracked') or [] needed = set(modules) if not needed.issubset(already_tracked): needed.update(already_tracked) with self.session as session: session.SetParameter('autodetect_build_local_tracked', needed) for module_name in modules: module_obj = self.GetModuleByName(module_name) <IF_STMT> module_obj.profile = None",if module_obj:
"def set_job_on_hold(self, value, blocking=True): trigger = False if not self._job_on_hold.acquire(blocking=blocking): return False try: <IF_STMT> self._job_on_hold.set() else: self._job_on_hold.clear() if self._job_on_hold.counter == 0: trigger = True finally: self._job_on_hold.release() if trigger: self._continue_sending() return True",if value:
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: <IF_STMT> p.moveToFirstChild() elif p.hasNext(): p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break p.moveToParent() return p",if p.v.children:
"def best_image(width, height): image = images[0] for img in images: <IF_STMT> return img elif img.width >= width and img.width * img.height > image.width * image.height: image = img return image",if img.width == width and img.height == height:
"def _check_input_types(self): if len(self.base_features) == 0: return True input_types = self.primitive.input_types if input_types is not None: <IF_STMT> input_types = [input_types] for t in input_types: zipped = list(zip(t, self.base_features)) if all([issubclass(f.variable_type, v) for v, f in zipped]): return True else: return True return False",if type(input_types[0]) != list:
"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: if exc_info is None: exc_info = sys.exc_info() el<IF_STMT> app_log.error('Multiple exceptions in yield list', exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)","if not isinstance(e, self.quiet_exceptions):"
def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == 'resnet_tf': if step_num < hparams.lrn_step: lrn_rate = hparams.mom_lrn <IF_STMT> lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate,elif step_num < 30000:
"def topic_exists(self, arn): response = self._conn.get_all_topics() topics = response['ListTopicsResponse']['ListTopicsResult']['Topics'] current_topics = [] if len(topics) > 0: for topic in topics: topic_arn = topic['TopicArn'] current_topics.append(topic_arn) <IF_STMT> return True return False",if arn in current_topics:
"def assertStartsWith(self, expectedPrefix, text, msg=None): if not text.startswith(expectedPrefix): <IF_STMT> text = text[:len(expectedPrefix) + 5] + '...' standardMsg = '{} not found at the start of {}'.format(repr(expectedPrefix), repr(text)) self.fail(self._formatMessage(msg, standardMsg))",if len(expectedPrefix) + 5 < len(text):
"def validate_memory(self, value): for k, v in value.viewitems(): <IF_STMT> continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError('Process types can only contain [a-z]') if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value",if v is None:
"def open(self) -> 'KeyValueJsonDb': """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): <IF_STMT> raise IOError('%s exists and is not a file' % self._name) try: with open(self._name, 'r') as _in: self.set_records(json.load(_in)) except json.JSONDecodeError: self.commit() else: mkpath(os.path.dirname(self._name)) self.commit() return self",if not os.path.isfile(self._name):
"def _calculate(self): before = self.before.data after = self.after.data self.deleted = {} self.updated = {} self.created = after.copy() for path, f in before.items(): <IF_STMT> self.deleted[path] = f continue del self.created[path] if f.mtime < after[path].mtime: self.updated[path] = after[path]",if path not in after:
"def cache_sqs_queues_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): <IF_STMT> cache_sqs_queues_for_account.delay(account_id) elif account_id in config.get('celery.test_account_ids', []): cache_sqs_queues_for_account.delay(account_id) stats.count(f'{function}.success') return True",if config.get('environment') == 'prod':
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: <IF_STMT> config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if config is None:
"def PopulateProjectId(project_id=None): """"""Fills in a project_id from the boto config file if one is not provided."""""" if not project_id: default_id = boto.config.get_value('GSUtil', 'default_project_id') <IF_STMT> raise ProjectIdException('MissingProjectId') return default_id return project_id",if not default_id:
"def set(self, name, value): with self._object_cache_lock: old_value = self._object_cache.get(name) ret = not old_value or int(old_value.metadata.resource_version) < int(value.metadata.resource_version) <IF_STMT> self._object_cache[name] = value return (ret, old_value)",if ret:
"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) <IF_STMT> if i >= len(self.items): i = len(self.items) - 1 self.list.select_set(i)",elif was_selected:
"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) <IF_STMT> paths = add_directory_csv_files(path, paths) elif isfile(path) and path.endswith('.csv'): paths.append(path) return paths",if isdir(path):
"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): <IF_STMT> if isinstance(value, dict): return GeneralPrivateEndpointClient(key, value['api_version'], value['support_list_or_not'], value['resource_get_api_version']) return value() raise CLIError('Resource type must be one of {}'.format(', '.join(rp_mapping.keys())))",if str.lower(key) == str.lower(resource_provider):
"def compute_rule_hash(self, rule): buf = '%d-%d-%s-' % (rule.get('FromPort', 0) or 0, rule.get('ToPort', 0) or 0, rule.get('IpProtocol', '-1') or '-1') for a, ke in self.RULE_ATTRS: <IF_STMT> continue ev = [e[ke] for e in rule[a]] ev.sort() for e in ev: buf += '%s-' % e return zlib.crc32(buf.encode('ascii')) & 4294967295",if a not in rule:
"def analysis_sucess_metrics(analysis_time: float, allow_exception=False): try: anchore_engine.subsys.metrics.counter_inc(name='anchore_analysis_success') anchore_engine.subsys.metrics.histogram_observe('anchore_analysis_time_seconds', analysis_time, buckets=ANALYSIS_TIME_SECONDS_BUCKETS, status='success') except: <IF_STMT> raise else: logger.exception('Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing')",if allow_exception:
"def decide_file_icon(file): if file.state == File.ERROR: return FileItem.icon_error elif isinstance(file.parent, Track): <IF_STMT> return FileItem.icon_saved elif file.state == File.PENDING: return FileItem.match_pending_icons[int(file.similarity * 5 + 0.5)] else: return FileItem.match_icons[int(file.similarity * 5 + 0.5)] elif file.state == File.PENDING: return FileItem.icon_file_pending else: return FileItem.icon_file",if file.state == File.NORMAL:
"def deleteMenu(self, menuName): try: menu = self.getMenu(menuName) <IF_STMT> self.destroy(menu) self.destroyMenu(menuName) else: g.es(""can't delete menu:"", menuName) except Exception: g.es('exception deleting', menuName, 'menu') g.es_exception()",if menu:
"def parser(cls, buf): type_, code, csum = struct.unpack_from(cls._PACK_STR, buf) msg = cls(type_, code, csum) offset = cls._MIN_LEN if len(buf) > offset: cls_ = cls._ICMPV6_TYPES.get(type_, None) <IF_STMT> msg.data = cls_.parser(buf, offset) else: msg.data = buf[offset:] return (msg, None, None)",if cls_:
"def _load_dataset_area(self, dsid, file_handlers, coords): """"""Get the area for *dsid*."""""" try: return self._load_area_def(dsid, file_handlers) except NotImplementedError: if any((x is None for x in coords)): logger.warning(""Failed to load coordinates for '{}'"".format(dsid)) return None area = self._make_area_from_coords(coords) <IF_STMT> logger.debug('No coordinates found for %s', str(dsid)) return area",if area is None:
"def __getattr__(self, name): if Popen.verbose: sys.stdout.write('Getattr: %s...' % name) if name in Popen.__slots__: return object.__getattribute__(self, name) elif self.popen is not None: if Popen.verbose: print('from Popen') return getattr(self.popen, name) el<IF_STMT> return self.emu_wait else: raise Exception('subprocess emulation: not implemented: %s' % name)",if name == 'wait':
"def update(self, time_delta): super().update(time_delta) n = self.menu.selected_option if n == self.last: return self.last = n s = '' for i in range(len(self.files)): <IF_STMT> for l in open(self.files[i][1]): x = l.strip() if len(x) > 1 and x[0] == '#': x = '<b><u>' + x[1:] + ' </u></b>' s += x + '<br>' self.set_text(s)",if self.files[i][0] == n:
"def wrapper(*args, **kwargs): list_args, empty = _apply_defaults(func, args, kwargs) if len(dimensions) > len(list_args): raise TypeError('%s takes %i parameters, but %i dimensions were passed' % (func.__name__, len(list_args), len(dimensions))) for dim, value in zip(dimensions, list_args): if dim is None: continue <IF_STMT> val_dim = ureg.get_dimensionality(value) raise DimensionalityError(value, 'a quantity of', val_dim, dim) return func(*args, **kwargs)",if not ureg.Quantity(value).check(dim):
"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) if size: strlen *= size <IF_STMT> data = 'A' * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",if strlen < MAX_LEN:
def wait_send_all_might_not_block(self) -> None: with self._send_conflict_detector: <IF_STMT> raise trio.ClosedResourceError('file was already closed') try: await trio.lowlevel.wait_writable(self._fd_holder.fd) except BrokenPipeError as e: raise trio.BrokenResourceError from e,if self._fd_holder.closed:
"def parse_win_proxy(val): proxies = [] for p in val.split(';'): if '=' in p: tab = p.split('=', 1) <IF_STMT> tab[0] = 'SOCKS4' proxies.append((tab[0].upper(), tab[1], None, None)) else: proxies.append(('HTTP', p, None, None)) return proxies",if tab[0] == 'socks':
"def _super_function(args): passed_class, passed_self = args.get_arguments(['type', 'self']) if passed_self is None: return passed_class else: pyclass = passed_class if isinstance(pyclass, pyobjects.AbstractClass): supers = pyclass.get_superclasses() <IF_STMT> return pyobjects.PyObject(supers[0]) return passed_self",if supers:
"def update_output_mintime(job): try: return output_mintime[job] except KeyError: for job_ in chain([job], self.depending[job]): try: t = output_mintime[job_] except KeyError: t = job_.output_mintime <IF_STMT> output_mintime[job] = t return output_mintime[job] = None",if t is not None:
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split(':') if len(split_provider_id) == 2: _id = split_provider_id[1] cursor = self.get_by_id(_id) <IF_STMT> result.append(cursor) return result",if cursor:
"def stop(self): with self.lock: <IF_STMT> return self.task_queue.put(None) self.result_queue.put(None) process = self.process self.process = None self.task_queue = None self.result_queue = None process.join(timeout=0.1) if process.exitcode is None: os.kill(process.pid, signal.SIGKILL) process.join()",if not self.process:
"def on_api_command(self, command, data): if command == 'select': if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, 'Insufficient permissions') <IF_STMT> return flask.abort(409, 'No active prompt') choice = data['choice'] if not isinstance(choice, int) or not self._prompt.validate_choice(choice): return flask.abort(400, '{!r} is not a valid value for choice'.format(choice)) self._answer_prompt(choice)",if self._prompt is None:
"def application_openFiles_(self, nsapp, filenames): for filename in filenames: logging.info('[osx] receiving from macOS : %s', filename) <IF_STMT> if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES: sabnzbd.add_nzbfile(filename, keep=True)",if os.path.exists(filename):
"def test_error_through_destructor(self): rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy <IF_STMT> self.assertIsNone(cm.unraisable) elif cm.unraisable is not None: self.assertEqual(cm.unraisable.exc_type, OSError)",if not IOBASE_EMITS_UNRAISABLE:
"def http_wrapper(self, url, postdata={}): try: <IF_STMT> f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) if postdata: logging.error('with post data') else: logging.error('without post data') logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = '' return response",if postdata != {}:
"def check_single_file(fn, fetchuri): """"""Determine if a single downloaded file is something we can't handle"""""" with open(fn, 'r', errors='surrogateescape') as f: <IF_STMT> logger.error('Fetching ""%s"" returned a single HTML page - check the URL is correct and functional' % fetchuri) sys.exit(1)",if '<html' in f.read(100).lower():
"def update_properties(self, update_dict): signed_attribute_changed = False for k, value in update_dict.items(): if getattr(self, k) != value: setattr(self, k, value) signed_attribute_changed = signed_attribute_changed or k in self.payload_arguments if signed_attribute_changed: <IF_STMT> self.status = UPDATED self.timestamp = clock.tick() self.sign() return self",if self.status != NEW:
"def clean_items(event, items, variations): for item in items: if event != item.event: raise ValidationError(_('One or more items do not belong to this event.')) if item.has_variations: <IF_STMT> raise ValidationError(_('One or more items has variations but none of these are in the variations list.'))",if not any((var.item == item for var in variations)):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue if tt == 18: self.add_doc_id(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def connections(self): fds = self.open_files socket = 'socket:[' result = [] functions = [pwndbg.net.tcp, pwndbg.net.unix, pwndbg.net.netlink] for fd, path in fds.items(): if socket not in path: continue inode = path[len(socket):-1] inode = int(inode) for func in functions: for x in func(): <IF_STMT> x.fd = fd result.append(x) return tuple(result)",if x.inode == inode:
def _movement_finished(self): if self.in_ship_map: <IF_STMT> ship = self.session.world.ship_map.get(self._next_target.to_tuple()) if ship is not None and ship() is self: del self.session.world.ship_map[self._next_target.to_tuple()] super()._movement_finished(),if self._next_target is not None:
"def print_addresses(self): p = 3 tmp_str = '[' if self.get_len() >= 7: while 1: if p + 1 == self.get_ptr(): tmp_str += '#' tmp_str += self.get_ip_address(p) p += 4 <IF_STMT> break else: tmp_str += ', ' tmp_str += '] ' if self.get_ptr() % 4: tmp_str += 'nonsense ptr field: %d ' % self.get_ptr() return tmp_str",if p >= self.get_len():
"def source_shapes(self): """"""Prints debug information about the sources in this provider."""""" if logger.isEnabledFor(logging.DEBUG): for i, source in enumerate(self.sources): <IF_STMT> name = 'anonymous' else: name = self.keys[i] try: shape = source.shape() except NotImplementedError: shape = 'N/A' logger.debug('Data source ""%s"": entries=%s, shape=%s', name, len(source), shape)",if self.keys is None:
def swap_actions(actions): for mutexgroup in mutex_groups: mutex_actions = mutexgroup._group_actions <IF_STMT> targetindex = actions.index(mutexgroup._group_actions[0]) actions[targetindex] = mutexgroup actions = [action for action in actions if action not in mutex_actions] return actions,"if contains_actions(mutex_actions, actions):"
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: <IF_STMT> continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if init_service and init_service in dep_cnt['_deps']:
"def make_dump_list_by_name_list(name_list): info_list = [] for info_name in name_list: info = next((x for x in DUMP_LIST if x.info_name == info_name), None) <IF_STMT> raise RuntimeError('Unknown info name: ""{}""'.format(info_name)) info_list.append(info) return info_list",if not info:
"def create(self, private=False): try: if private: log.info('Creating private channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name, 'is_private': True}) else: log.info('Creating channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name}) except SlackAPIResponseError as e: <IF_STMT> raise RoomError(f'Unable to create channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e)",if e.error == 'user_is_bot':
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() <IF_STMT> continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) if j == -1: attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) if reply == '!done': return r",if len(i) == 0:
"def _load_logfile(self, lfn): enc_key = self.decryption_key_func() with open(os.path.join(self.logdir, lfn)) as fd: <IF_STMT> with DecryptingStreamer(fd, mep_key=enc_key, name='EventLog/DS(%s)' % lfn) as streamer: lines = streamer.read() streamer.verify(_raise=IOError) else: lines = fd.read() if lines: for line in lines.splitlines(): event = Event.Parse(line.strip()) self._events[event.event_id] = event",if enc_key:
"def set_ok_port(self, cookie, request): if cookie.port_specified: req_port = request_port(request) if req_port is None: req_port = '80' else: req_port = str(req_port) for p in cookie.port.split(','): try: int(p) except ValueError: debug('   bad port %s (not numeric)', p) return False <IF_STMT> break else: debug('   request port (%s) not found in %s', req_port, cookie.port) return False return True",if p == req_port:
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) <IF_STMT> dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",if nodeid not in self._nodes:
"def data_logging_status(self, trail_name, trail_details, api_client): for es in api_client.get_event_selectors(TrailName=trail_name)['EventSelectors']: has_wildcard = {u'Values': [u'arn:aws:s3:::'], u'Type': u'AWS::S3::Object'} in es['DataResources'] is_logging = trail_details['IsLogging'] <IF_STMT> return True return False",if has_wildcard and is_logging and self.is_fresh(trail_details):
"def pytest_deselected(items): if sb_config.dashboard: sb_config.item_count -= len(items) for item in items: test_id, display_id = _get_test_ids_(item) <IF_STMT> sb_config._results.pop(test_id)",if test_id in sb_config._results.keys():
"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: <IF_STMT> self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if output == input:
"def printWiki(): firstHeading = False for m in protocol: <IF_STMT> if firstHeading: output('|}') __printWikiHeader(m[1], m[2]) firstHeading = True else: output('|-') output('| <span style=""white-space:nowrap;""><tt>' + m[0] + '</tt></span> || || ' + m[1]) output('|}')",if m[0] == '':
"def test_getitem(self): n = 200 d = deque(range(n)) l = list(range(n)) for i in range(n): d.popleft() l.pop(0) <IF_STMT> d.append(i) l.append(i) for j in range(1 - len(l), len(l)): assert d[j] == l[j] d = deque('superman') self.assertEqual(d[0], 's') self.assertEqual(d[-1], 'n') d = deque() self.assertRaises(IndexError, d.__getitem__, 0) self.assertRaises(IndexError, d.__getitem__, -1)",if random.random() < 0.5:
"def get_num(line, char_ptr, num_chars): char_ptr = char_ptr + 1 numstr = '' good = '-.0123456789' while char_ptr < num_chars: digit = line[char_ptr] <IF_STMT> numstr = numstr + digit char_ptr = char_ptr + 1 else: break return numstr",if good.find(digit) != -1:
"def read_digits(source, start, first_code): body = source.body position = start code = first_code if code is not None and 48 <= code <= 57: while True: position += 1 code = char_code_at(body, position) <IF_STMT> break return position raise GraphQLSyntaxError(source, position, u'Invalid number, expected digit but got: {}.'.format(print_char_code(code)))",if not (code is not None and 48 <= code <= 57):
"def get_aws_metadata(headers, provider=None): if not provider: provider = boto.provider.get_default() metadata_prefix = provider.metadata_prefix metadata = {} for hkey in headers.keys(): <IF_STMT> val = urllib.unquote_plus(headers[hkey]) try: metadata[hkey[len(metadata_prefix):]] = unicode(val, 'utf-8') except UnicodeDecodeError: metadata[hkey[len(metadata_prefix):]] = val del headers[hkey] return metadata",if hkey.lower().startswith(metadata_prefix):
def _process_rtdest(self): LOG.debug('Processing RT NLRI destination...') if self._rtdest_queue.is_empty(): return else: processed_any = False while not self._rtdest_queue.is_empty(): next_dest = self._rtdest_queue.pop_first() if next_dest: next_dest.process() processed_any = True <IF_STMT> self._core_service.update_rtfilters(),if processed_any:
"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: if header_name in headers: <IF_STMT> self.assertTrue(url.endswith('/latest'), msg=url) else: self.assertTrue(url.endswith('/download_urls'), msg=url) return headers.get(header_name)",if self.revs_enabled:
"def add_external_deps(self, deps): for dep in deps: if hasattr(dep, 'el'): dep = dep.el <IF_STMT> raise InvalidArguments('Argument is not an external dependency') self.external_deps.append(dep) if isinstance(dep, dependencies.Dependency): self.process_sourcelist(dep.get_sources())","if not isinstance(dep, dependencies.Dependency):"
"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() if isinstance(r, bytes): r = r.decode('utf-8') msg = json.loads(r) stream = msg.get('stream') <IF_STMT> await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())",if stream is not None:
"def generate_and_check_random(): random_size = 256 while True: random = os.urandom(random_size) a = int.from_bytes(random, 'big') A = pow(g, a, p) <IF_STMT> a_for_hash = big_num_for_hash(A) u = int.from_bytes(sha256(a_for_hash, b_for_hash), 'big') if u > 0: return (a, a_for_hash, u)","if is_good_mod_exp_first(A, p):"
"def write(self, datagram, address): """"""Write a datagram."""""" try: return self.socket.sendto(datagram, address) except OSError as se: no = se.args[0] if no == EINTR: return self.write(datagram, address) elif no == EMSGSIZE: raise error.MessageLengthError('message too long') <IF_STMT> pass else: raise",elif no == EAGAIN:
"def doDir(elem): for child in elem.childNodes: <IF_STMT> continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))","if not isinstance(child, minidom.Element):"
"def add_reversed_tensor(i, X, reversed_X): if X in stop_mapping_at_tensors: return if X not in reversed_tensors: reversed_tensors[X] = {'id': (nid, i), 'tensor': reversed_X} else: tmp = reversed_tensors[X] if 'tensor' in tmp and 'tensors' in tmp: raise Exception('Wrong order, tensors already aggregated!') <IF_STMT> tmp['tensors'] = [tmp['tensor'], reversed_X] del tmp['tensor'] else: tmp['tensors'].append(reversed_X)",if 'tensor' in tmp:
"def walk(source, path, default, delimiter='.'): """"""Walk the sourch hash given the path and return the value or default if not found"""""" if not isinstance(source, dict): raise RuntimeError('The source is not a walkable dict: {} path: {}'.format(source, path)) keys = path.split(delimiter) max_depth = len(keys) cur_depth = 0 while cur_depth < max_depth: <IF_STMT> source = source[keys[cur_depth]] cur_depth = cur_depth + 1 else: return default return source",if keys[cur_depth] in source:
"def _from_txt_get_vulns(self): file_vulns = [] vuln_regex = 'SQL injection in a .*? was found at: ""(.*?)"", using HTTP method (.*?). The sent .*?data was: ""(.*?)""' vuln_re = re.compile(vuln_regex) for line in file(self.OUTPUT_FILE): mo = vuln_re.search(line) <IF_STMT> v = MockVuln('TestCase', None, 'High', 1, 'plugin') v.set_url(URL(mo.group(1))) v.set_method(mo.group(2)) file_vulns.append(v) return file_vulns",if mo:
"def __get__(self, instance, instance_type=None): if instance: if self.att_name not in instance._obj_cache: rel_obj = self.get_obj(instance) <IF_STMT> instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",if rel_obj:
"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): <IF_STMT> pos_end = pos else: if pos_end >= pos_start: ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",if func.type in support_set:
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: if name is None: all_plugins += [plugin for name, plugin in self.plugins.items() <IF_STMT>] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",if name not in self.plugins_callback_order and plugin.is_activated
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <IF_STMT> result.append(token.contents.replace('%', '%%')) elif token.token_type == TOKEN_VAR: result.append('%%(%s)s' % token.contents) vars.append(token.contents) msg = ''.join(result) if self.trimmed: msg = translation.trim_whitespace(msg) return (msg, vars)",if token.token_type == TOKEN_TEXT:
"def test_build_root_config_overwrite(self): cfg = build_root_config('tests.files.settings_overwrite') for key, val in DEFAULT_SPIDER_GLOBAL_CONFIG.items(): <IF_STMT> self.assertEqual(cfg['global'][key], ['zzz']) else: self.assertEqual(cfg['global'][key], val)",if key == 'spider_modules':
"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() <IF_STMT> if limit == 0: return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",if settings.MAX_PAGE_SIZE:
"def track_handler(handler): tid = handler.request.tid for event in events_monitored: <IF_STMT> e = Event(event, handler.request.execution_time) State.tenant_state[tid].RecentEventQ.append(e) State.tenant_state[tid].EventQ.append(e) break",if event['handler_check'](handler):
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_subscription().TryMerge(tmp) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:
"def GetCreateInstanceBinder(self, info): with self._lock: <IF_STMT> return self._createInstanceBinders[info] b = runtime.SymplCreateInstanceBinder(info) self._createInstanceBinders[info] = b return b",if self._createInstanceBinders.ContainsKey(info):
"def process_task(self, body, message): if 'control' in body: try: return self.control(body, message) except Exception: logger.exception('Exception handling control message:') return if len(self.pool): <IF_STMT> try: queue = UUID(body['uuid']).int % len(self.pool) except Exception: queue = self.total_messages % len(self.pool) else: queue = self.total_messages % len(self.pool) else: queue = 0 self.pool.write(queue, body) self.total_messages += 1 message.ack()",if 'uuid' in body and body['uuid']:
"def is_defined_in_base_class(self, var: Var) -> bool: if var.info: for base in var.info.mro[1:]: if base.get(var.name) is not None: return True <IF_STMT> return True return False",if var.info.fallback_to_any:
"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' <IF_STMT> tmp += '%' elif col == FOOD: tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == BARRIER:
def prompt_for_resume(config): logger = logging.getLogger('changeme') logger.error('A previous scan was interrupted. Type R to resume or F to start a fresh scan') answer = '' while not (answer == 'R' or answer == 'F'): prompt = '(R/F)> ' answer = '' try: answer = raw_input(prompt) except NameError: answer = input(prompt) <IF_STMT> logger.debug('Forcing a fresh scan') elif answer.upper() == 'R': logger.debug('Resuming previous scan') config.resume = True return config.resume,if answer.upper() == 'F':
"def f(view, s): if mode == modes.INTERNAL_NORMAL: <IF_STMT> if view.line(s.b).size() > 0: eol = view.line(s.b).b return R(s.b, eol) return s return s",if count == 1:
"def flush(self): if not self.cuts: return for move, (x, y, z), cent in douglas(self.cuts, self.tolerance, self.plane): <IF_STMT> self.write('%s X%.4f Y%.4f Z%.4f %s' % (move, x, y, z, cent)) self.lastgcode = None self.lastx = x self.lasty = y self.lastz = z else: self.move_common(x, y, z, gcode='G1') self.cuts = []",if cent:
"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() new_i.id = self.id cls.id = old_id for prop in cls.properties: <IF_STMT> if self.has(prop): val = getattr(self, prop) setattr(new_i, prop, val) new_i.members = [] return new_i",if prop is not 'members':
"def find_region_by_value(key, value): for region in cognitoidp_backends: backend = cognitoidp_backends[region] for user_pool in backend.user_pools.values(): if key == 'client_id' and value in user_pool.clients: return region <IF_STMT> return region return list(cognitoidp_backends)[0]",if key == 'access_token' and value in user_pool.access_tokens:
"def __init__(self, fixed: MQTTFixedHeader=None, variable_header: PacketIdVariableHeader=None): if fixed is None: header = MQTTFixedHeader(PUBREL, 2) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PubrelPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = None",if fixed.packet_type is not PUBREL:
"def _on_event_MetadataStatisticsUpdated(self, event, data): with self._selectedFileMutex: <IF_STMT> self._setJobData(self._selectedFile['filename'], self._selectedFile['filesize'], self._selectedFile['sd'], self._selectedFile['user'])",if self._selectedFile:
"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for parameter_range_key, parameter_range_value in parameter_range.__dict__.items(): <IF_STMT> continue if isinstance(parameter_range_value, list): for categorical_value in parameter_range_value: value_hp.validate(categorical_value) else: value_hp.validate(parameter_range_value)",if parameter_range_key == 'scaling_type':
"def visit_filter_projection(self, node, value): base = self.visit(node['children'][0], value) if not isinstance(base, list): return None comparator_node = node['children'][2] collected = [] for element in base: if self._is_true(self.visit(comparator_node, element)): current = self.visit(node['children'][1], element) <IF_STMT> collected.append(current) return collected",if current is not None:
"def _getSubstrings(self, va, size, ltyp): subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) <IF_STMT> subs.add((loc[L_VA], loc[L_SIZE])) if loc[L_TINFO]: subs = subs.union(set(loc[L_TINFO])) return list(subs)",if loc and loc[L_LTYPE] == LOC_STRING and (loc[L_VA] > va):
"def run(self): while not self._stopped: try: try: test_name = next(self.pending) except StopIteration: break mp_result = self._runtest(test_name) self.output.put((False, mp_result)) <IF_STMT> break except ExitThread: break except BaseException: self.output.put((True, traceback.format_exc())) break","if must_stop(mp_result.result, self.ns):"
"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v elif isinstance(v, (list, tuple, dict)): out = get_in_inputs(key, v) <IF_STMT> return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]",if out:
"def act_mapping(self, items, actions, mapping): """"""Executes all the actions on the list of pods."""""" success = True for action in actions: for key, method in mapping.items(): if key in action: params = action.get(key) ret = method(items, params) <IF_STMT> success = False return success",if not ret:
"def _apply(self, plan): desired = plan.desired changes = plan.changes self.log.debug('_apply: zone=%s, len(changes)=%d', desired.name, len(changes)) domain_name = desired.name[:-1] try: nsone_zone = self._client.loadZone(domain_name) except ResourceException as e: <IF_STMT> raise self.log.debug('_apply:   no matching zone, creating') nsone_zone = self._client.createZone(domain_name) for change in changes: class_name = change.__class__.__name__ getattr(self, '_apply_{}'.format(class_name))(nsone_zone, change)",if e.message != self.ZONE_NOT_FOUND_MESSAGE:
"def split_artists(self, json): if len(json) == 0: ([], []) elif len(json) == 1: artist = Artist.query.filter_by(name=json[0]['name']).first() return ([artist], []) my_artists = [] other_artists = [] for artist_dict in json: artist = Artist.query.filter_by(name=artist_dict['name']) <IF_STMT> my_artists.append(artist.first()) else: del artist_dict['thumb_url'] other_artists.append(artist_dict) return (my_artists, other_artists)",if artist.count():
"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) <IF_STMT> continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrvalue == 0:
"def close(self, code=errno.ECONNRESET): with self.shutdown_lock: <IF_STMT> super(RemoteIPRoute, self).close(code=code) self.closed = True try: self._mitogen_call.get() except mitogen.core.ChannelError: pass if self._mitogen_broker is not None: self._mitogen_broker.shutdown() self._mitogen_broker.join()",if not self.closed:
"def untokenize(self, iterable): for t in iterable: <IF_STMT> self.compat(t, iterable) break tok_type, token, start, end, line = t self.add_whitespace(start) self.tokens.append(token) self.prev_row, self.prev_col = end if tok_type in (NEWLINE, NL): self.prev_row += 1 self.prev_col = 0 return ''.join(self.tokens)",if len(t) == 2:
"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: if self.norm_means: x = np.add(x, self.bias[spk]) <IF_STMT> x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",if self.norm_vars:
"def get_party_total(self, args): self.party_total = frappe._dict() for d in self.receivables: self.init_party_total(d) for k in list(self.party_total[d.party]): <IF_STMT> self.party_total[d.party][k] += d.get(k, 0.0) self.set_party_details(d)","if k not in ['currency', 'sales_person']:"
"def get_databases(request): dbs = {} global_env = globals() for key, value in global_env.items(): try: cond = isinstance(value, GQLDB) except: cond = isinstance(value, SQLDB) <IF_STMT> dbs[key] = value return dbs",if cond:
"def check_twobit_file(dbkey, GALAXY_DATA_INDEX_DIR): twobit_file = '%s/twobit.loc' % GALAXY_DATA_INDEX_DIR twobit_path = '' twobits = {} for i, line in enumerate(open(twobit_file)): line = line.rstrip('\r\n') if line and (not line.startswith('#')): fields = line.split('\t') <IF_STMT> continue twobits[fields[0]] = fields[1] if dbkey in twobits: twobit_path = twobits[dbkey] return twobit_path",if len(fields) < 2:
"def action(scheduler, _): nonlocal state nonlocal has_result nonlocal result nonlocal first nonlocal time <IF_STMT> observer.on_next(result) try: if first: first = False else: state = iterate(state) has_result = condition(state) if has_result: result = state time = time_mapper(state) except Exception as e: observer.on_error(e) return if has_result: mad.disposable = scheduler.schedule_relative(time, action) else: observer.on_completed()",if has_result:
def orthogonalEnd(self): if self.type == Segment.LINE: O = self.AB.orthogonal() O.norm() return O else: O = self.B - self.C O.norm() <IF_STMT> return -O else: return O,if self.type == Segment.CCW:
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == 'ANY': <IF_STMT> self._definition = [] elif v in self._definition: self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",if v == 'ANY':
"def __enter__(self) -> None: try: <IF_STMT> signal.signal(signal.SIGALRM, self.handle_timeout) signal.alarm(self.seconds) except ValueError as ex: logger.warning(""timeout can't be used in the current context"") logger.exception(ex)",if threading.current_thread() == threading.main_thread():
"def __init__(self, fixed: MQTTFixedHeader=None): if fixed is None: header = MQTTFixedHeader(PINGRESP, 0) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PingRespPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = None self.payload = None",if fixed.packet_type is not PINGRESP:
"def _put_nowait(self, data, *, sender): if not self._running: logger.warning('Pub/Sub listener message after stop: %r, %r', sender, data) return self._queue.put_nowait((sender, data)) if self._waiter is not None: fut, self._waiter = (self._waiter, None) <IF_STMT> assert fut.cancelled(), ('Waiting future is in wrong state', self, fut) return fut.set_result(None)",if fut.done():
"def OnAssignBuiltin(self, cmd_val): buf = self._ShTraceBegin() if not buf: return for i, arg in enumerate(cmd_val.argv): <IF_STMT> buf.write(' ') buf.write(arg) for pair in cmd_val.pairs: buf.write(' ') buf.write(pair.var_name) buf.write('=') if pair.rval: _PrintShValue(pair.rval, buf) buf.write('\n') self.f.write(buf.getvalue())",if i != 0:
"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] if not (isinstance(k, str) or isinstance(k, unicode)): k = dumps(k) <IF_STMT> obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj",if Types.KEYS not in obj:
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == '{': length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break <IF_STMT> token = token.matching_bracket token = token.next_token return False",if token.OpensScope():
"def get_editable_dict(self): ret = {} for ref, ws_package in self._workspace_packages.items(): path = ws_package.root_folder <IF_STMT> path = os.path.join(path, CONANFILE) ret[ref] = {'path': path, 'layout': ws_package.layout} return ret",if os.path.isdir(path):
"def serialize(self, name=None): data = super(WebLink, self).serialize(name) data['contentType'] = self.contentType if self.width: <IF_STMT> raise InvalidWidthException(self.width) data['inputOptions'] = {} data['width'] = self.width data.update({'content': {'url': self.linkUrl, 'text': self.linkText}}) return data","if self.width not in [100, 50, 33, 25]:"
"def callback(lexer, match, context): text = match.group() extra = '' if start: context.next_indent = len(text) <IF_STMT> while context.next_indent < context.indent: context.indent = context.indent_stack.pop() if context.next_indent > context.indent: extra = text[context.indent:] text = text[:context.indent] else: context.next_indent += len(text) if text: yield (match.start(), TokenClass, text) if extra: yield (match.start() + len(text), TokenClass.Error, extra) context.pos = match.end()",if context.next_indent < context.indent:
"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): if subscriber_web_sock == web_sock: index = i break <IF_STMT> del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",if index is not None:
"def test_missing_dict_param(): expected_err = 'params dictionary did not contain value for placeholder' try: substitute_params('SELECT * FROM cust WHERE salesrep = %(name)s', {'foobar': 'John Doe'}) assert False, 'expected exception b/c dict did not contain replacement value' except ValueError as exc: <IF_STMT> raise",if expected_err not in str(exc):
"def one_gpr_reg_one_mem_scalable(ii): n, r = (0, 0) for op in _gen_opnds(ii): <IF_STMT> n += 1 elif op_gprv(op): r += 1 else: return False return n == 1 and r == 1",if op_agen(op) or (op_mem(op) and op.oc2 in ['v']):
"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, 'md_bg_color') and self.focus_behavior: if hasattr(self, 'theme_cls') and (not self.focus_color): self.md_bg_color = self.theme_cls.bg_normal el<IF_STMT> self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color",if not self.focus_color:
"def __init__(self, *args, **kwargs): BaseCellExporter.__init__(self, *args, **kwargs) self.comment = '#' for key in ['cell_marker']: <IF_STMT> self.metadata[key] = self.unfiltered_metadata[key] if self.fmt.get('rst2md'): raise ValueError(""The 'rst2md' option is a read only option. The reverse conversion is not implemented. Please either deactivate the option, or save to another format."")",if key in self.unfiltered_metadata:
def sendQueryQueueByAfterNate(self): for i in range(10): queryQueueByAfterNateRsp = self.session.httpClint.send(urls.get('queryQueue')) <IF_STMT> print(''.join(queryQueueByAfterNateRsp.get('messages')) or queryQueueByAfterNateRsp.get('validateMessages')) time.sleep(1) else: sendEmail(ticket.WAIT_ORDER_SUCCESS) sendServerChan(ticket.WAIT_ORDER_SUCCESS) raise ticketIsExitsException(ticket.WAIT_AFTER_NATE_SUCCESS),if not queryQueueByAfterNateRsp.get('status'):
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() <IF_STMT> continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",if not line:
"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) if not comment: r = r.replace('*/', '\\x2a/') return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): <IF_STMT> return '%s /* %s */' % (n, self.pretty(int(n))) else: return '%s (%s)' % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",if comment:
"def get_pricings(self, subscription_id: str): try: client = self.get_client(subscription_id) pricings_list = await run_concurrently(lambda: client.pricings.list()) <IF_STMT> return pricings_list.value else: return [] except Exception as e: print_exception(f'Failed to retrieve pricings: {e}') return []","if hasattr(pricings_list, 'value'):"
"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): name = target.id if name not in variables: doc = find_doc_for(target, body_lines) <IF_STMT> variables[name] = doc elif isinstance(target, ast.Tuple): for e in target.elts: add_doc(e, variables, body_lines)",if doc is not None:
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: <IF_STMT> done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",if right == len(text):
"def pxrun_nodes(self, *args, **kwargs): cell = self._px_cell if re.search('^\\s*%autopx\\b', cell): self._disable_autopx() return False else: try: result = self.view.execute(cell, silent=False, block=False) except: self.shell.showtraceback() return True else: <IF_STMT> try: result.get() except: self.shell.showtraceback() return True else: result.display_outputs() return False",if self.view.block:
"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: if matchSelf: yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children <IF_STMT> break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if s.siblingAbove is None:
"def decTaskGen(): cnt = intbv(0, min=-n, max=n) while 1: yield (clock.posedge, reset.negedge) <IF_STMT> cnt[:] = 0 count.next = 0 else: decTaskFunc(cnt, enable, reset, n) count.next = cnt",if reset == ACTIVE_LOW:
"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): <IF_STMT> layer = stack[0].f_locals['self'] if layer in layer_names: log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) return out",if 'self' in stack[0].f_locals:
"def to_json_dict(self): d = super().to_json_dict() d['bullet_list'] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: if isinstance(self.header, RenderedContent): d['header'] = self.header.to_json_dict() else: d['header'] = self.header if self.subheader is not None: <IF_STMT> d['subheader'] = self.subheader.to_json_dict() else: d['subheader'] = self.subheader return d","if isinstance(self.subheader, RenderedContent):"
"def add(request): form_type = 'servers' if request.method == 'POST': form = BookMarkForm(request.POST) <IF_STMT> form_type = form.save() messages.add_message(request, messages.INFO, 'Bookmark created') else: messages.add_message(request, messages.INFO, form.errors) if form_type == 'server': url = reverse('servers') else: url = reverse('metrics') return redirect(url) else: return redirect(reverse('servers'))",if form.is_valid():
"def fee_amount_in_quote(self, trading_pair: str, price: Decimal, order_amount: Decimal): fee_amount = Decimal('0') if self.percent > 0: fee_amount = price * order_amount * self.percent base, quote = trading_pair.split('-') for flat_fee in self.flat_fees: if interchangeable(flat_fee[0], base): fee_amount += flat_fee[1] * price <IF_STMT> fee_amount += flat_fee[1] return fee_amount","elif interchangeable(flat_fee[0], quote):"
"def load_batch(fpath): with open(fpath, 'rb') as f: <IF_STMT> d = pickle.load(f, encoding='latin1') else: d = pickle.load(f) data = d['data'] labels = d['labels'] return (data, labels)","if sys.version_info > (3, 0):"
"def clear_entries(options): """"""Clear pending entries"""""" with Session() as session: query = session.query(db.PendingEntry).filter(db.PendingEntry.approved == False) <IF_STMT> query = query.filter(db.PendingEntry.task_name == options.task_name) deleted = query.delete() console('Successfully deleted %i pending entries' % deleted)",if options.task_name:
"def attribute_table(self, attribute): """"""Return a tuple (schema, table) for attribute."""""" dimension = attribute.dimension if dimension: schema = self.naming.dimension_schema or self.naming.schema <IF_STMT> table = self.fact_name else: table = self.naming.dimension_table_name(dimension) else: table = self.fact_name schema = self.naming.schema return (schema, table)",if dimension.is_flat and (not dimension.has_details):
"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean('browsers', 'rating_confirm_multiple'): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _('_Remove Rating'), count, None) if dialog.run() != Gtk.ResponseType.YES: return reset = [] for song in songs: <IF_STMT> del song['~#rating'] reset.append(song) librarian.changed(reset)",if '~#rating' in song:
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True <IF_STMT> right += 1 else: done = True return (left, right)",elif not self.word_boundary_char(text[right]):
"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) if not chunk: self.transfer_finished = True return <IF_STMT> chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",if self._data_wrapper is not None:
"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 if self.split <= self.min or self.split >= self.max: self.split = self.save <IF_STMT> self.split = self.min else: self.split = self.max self.placeChilds()",elif self.split < 0.5:
"def readAtOffset(self, offset, size, shortok=False): ret = b'' self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) if x == b'': <IF_STMT> return None return ret ret += x return ret",if not shortok:
"def webfinger(environ, start_response, _): query = parse_qs(environ['QUERY_STRING']) try: rel = query['rel'] resource = query['resource'][0] except KeyError: resp = BadRequest('Missing parameter in request') else: <IF_STMT> resp = BadRequest('Bad issuer in request') else: wf = WebFinger() resp = Response(wf.response(subject=resource, base=OAS.baseurl)) return resp(environ, start_response)",if rel != [OIC_ISSUER]:
"def _tokenize(self, text): if format_text(text) == EMPTY_TEXT: return [self.additional_special_tokens[0]] split_tokens = [] if self.do_basic_tokenize: for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens): <IF_STMT> split_tokens.append(token) else: split_tokens += self.wordpiece_tokenizer.tokenize(token) else: split_tokens = self.wordpiece_tokenizer.tokenize(text) return split_tokens",if token in self.basic_tokenizer.never_split:
"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: if isinstance(command, str): command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() <IF_STMT> _errno, errmsg = ('UNKNOWN', e.args[0]) else: _errno, errmsg = e.args raise ConnectionError('Error %s while writing to socket. %s.' % (_errno, errmsg)) except Exception: self.disconnect() raise",if len(e.args) == 1:
"def to_value(self, value): ret = {} for key, val in value.items(): if key in ['attachments', 'custom_attributes', 'description_diff']: ret[key] = val <IF_STMT> ret[key] = {k: {'from': v[0], 'to': v[1]} for k, v in val.items()} else: ret[key] = {'from': val[0], 'to': val[1]} return ret",elif key == 'points':
"def to_child(cls, key=None, process=None): if process is not None: if type(process) is not dict: raise ValueError('Invalid value provided for ""process"" parameter, expected a dictionary') <IF_STMT> result = {} result.update(deepcopy(cls.__process__)) result.update(process) process = result  class Child(cls): __key__ = key __process__ = process __root__ = False Child.__name__ = cls.__name__ return Child",if cls.__process__:
"def _super_function(args): passed_class, passed_self = args.get_arguments(['type', 'self']) if passed_self is None: return passed_class else: pyclass = passed_class <IF_STMT> supers = pyclass.get_superclasses() if supers: return pyobjects.PyObject(supers[0]) return passed_self","if isinstance(pyclass, pyobjects.AbstractClass):"
"def get_data(row): data = [] for field_name, field_xpath in fields: result = row.xpath(field_xpath) <IF_STMT> result = ' '.join((text for text in map(six.text_type.strip, map(six.text_type, map(unescape, result))) if text)) else: result = None data.append(result) return data",if result:
"def say(jarvis, s): """"""Reads what is typed."""""" if not s: jarvis.say('What should I say?') else: voice_state = jarvis.is_voice_enabled() jarvis.enable_voice() jarvis.say(s) <IF_STMT> jarvis.disable_voice()",if not voice_state:
"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: <IF_STMT> fromlist = list(fromlist) fromlist.remove('*') fromlist.extend(getattr(module, '__all__', [])) for x in fromlist: if isinstance(getattr(module, x, None), types.ModuleType): from_name = '{}.{}'.format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module",if '*' in fromlist:
"def _read_pricing_file(self, region=None, pricing_file=None): if not self.__pricing_file_cache: <IF_STMT> logging.info('Reading pricing file...') with open(pricing_file) as data_file: self.__pricing_file_cache = json.load(data_file) else: self.__pricing_file_cache = self._download_pricing_file(region) return self.__pricing_file_cache",if pricing_file:
