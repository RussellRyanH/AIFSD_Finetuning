Unnamed: 0,formatted_method,if_statement,predicted_statement,CodeBLEU,BLEU-4,Exact Match
0,"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) <IF_STMT> return [] if ignore_non_errors and is_noerr(e): return [] raise",if ignore_timeouts and is_timeout(e):,if ignore_timeouts and is_noerr(e):,0.8638060396705811,0.8516228624291206,False
1,"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: if reuse_len is not None and reuse_len > 0: curr_out = curr_out[:reuse_len] <IF_STMT> new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",if prev_mem is None:,if prev_mem is None:,0.6159846065099827,0.8879659171421962,True
2,def filtered(gen): for example in gen: example_len = length_fn(example) if max_length is not None: <IF_STMT> continue if min_length is not None: if example_len < min_length: continue yield example,if example_len > max_length:,if example_len > max_length:,0.8796226661604958,0.8105932471967202,True
3,"def search(self, query): if not query: logger.debug('Empty search query') return [] logger.debug('Searching TuneIn for ""%s""' % query) args = '&query=' + query search_results = self._tunein('Search.ashx', args) results = [] for item in self._flatten(search_results): <IF_STMT> self._stations[item['guide_id']] = item results.append(item) return results","if item.get('type', '') == 'audio':",if 'guide_id' in item:,0.8888557218333881,0.8661072626070159,False
4,"def _check_script(self, script, directive): for var in compile_script(script): <IF_STMT> return False if var.can_contain('.'): reason = 'At least variable ""${var}"" can contain untrusted user input'.format(var=var.name) self.add_issue(directive=[directive] + var.providers, reason=reason) return True return False",if var.must_contain('/'):,if not var.can_match(directive):,0.715068066909157,0.8592899528284996,False
5,"def getAllDataLinkIDs(): linkDataIDs = set() dataType = _forestData.dataTypeBySocket for socketID, linkedIDs in _forestData.linkedSockets.items(): for linkedID in linkedIDs: <IF_STMT> linkDataIDs.add((socketID, linkedID, dataType[socketID], dataType[linkedID])) else: linkDataIDs.add((linkedID, socketID, dataType[linkedID], dataType[socketID])) return linkDataIDs",if socketID[1]:,if linkedID == socketID:,0.89002256057719,0.8169276475307028,False
6,"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): if curses: curses.setupterm() <IF_STMT> return True elif colorama: if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",if curses.tigetnum('colors') > 0:,if sys.stderr is curses.initialise.wrapped_stderr:,0.8983464917095669,0.8169276475307028,False
7,"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): if isinstance(ty, SimTypeBottom): l.warning('Found a bottom field in struct %s. Ignore and increment the offset using the default element size.', self.name) continue if not self._pack: align = ty.alignment <IF_STMT> offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",if offset_so_far % align != 0:,if align:,0.7405586327733444,0.9395648330058336,False
8,"def Restore(self): picker, obj = (self._window, self._pObject) value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: if issubclass(picker.__class__, wx.FileDialog): <IF_STMT> value = value[-1] picker.SetPath(value) return True return False",if type(value) == list:,if value[-1] == '/':,0.8868096028823963,0.8038019482772603,False
9,def dt_s_tup_to_string(dt_s_tup): dt_string = dt_s_tup[0] if dt_s_tup[1] > 0: <IF_STMT> dt_string = dt_string[:2] + 's' + dt_string[2:] else: dt_string = 's' + dt_string return dt_string,if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:,if len(dt_string) > 2:,0.5720731533972053,0.7886336751695258,False
10,"def writer(stream, items): sep = '' for item in items: stream.write(sep) sep = ' ' <IF_STMT> item = str(item) if not PY3K: if not isinstance(item, unicode): item = str(item) stream.write(item) stream.write('\n')","if not isinstance(item, str):","if not isinstance(item, unicode):",0.9091597958515799,0.828399516355805,False
11,"def _get_result_keys(self, config): result_key = config.get('result_key') if result_key is not None: <IF_STMT> result_key = [result_key] result_key = [jmespath.compile(rk) for rk in result_key] return result_key","if not isinstance(result_key, list):","if not isinstance(result_key, list):",0.8618556845692965,0.7801270245332924,True
12,"def _download_build_artifacts(self, build: Dict[str, Any]) -> None: arch = build['arch_tag'] snap_build = self._lp_load_url(build['self_link']) urls = snap_build.getFileUrls() if not urls: logger.error(f'Snap file not available for arch {arch!r}.') return for url in urls: file_name = _get_url_basename(url) self._download_file(url=url, dst=file_name) <IF_STMT> logger.info(f'Snapped {file_name}') else: logger.info(f'Fetched {file_name}')",if file_name.endswith('.snap'):,if arch == 'snap':,0.760483079457201,0.8723360571509826,False
13,"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy['Version'] = '2012-10-17' if self.resource_policy.get('Statement') is None: self.resource_policy['Statement'] = custom_statements else: if not isinstance(custom_statements, list): custom_statements = [custom_statements] statement = self.resource_policy['Statement'] if not isinstance(statement, list): statement = [statement] for s in custom_statements: <IF_STMT> statement.append(s) self.resource_policy['Statement'] = statement",if s not in statement:,if s not in statement:,0.8630645157346067,0.8621109017306224,True
14,"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] <IF_STMT> message = f'{idx}. {check.message}' else: message = None example = cast(Case, check.example) display_example(example, check.name, message, result.seed) if idx != len(checks): click.echo('\n')",if check.message:,if check.message:,0.8362952106115802,0.9253742688467129,True
15,"def build(opt): dpath = os.path.join(opt['datapath'], 'qangaroo') version = 'v1.1' if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):,if build_data.built(dpath):,0.9098134523371573,0.8787142254774354,True
16,"def call(self, step_input, states): new_states = [] for i in range(self.num_layers): out, new_state = self.lstm_cells[i](step_input, states[i]) step_input = layers.dropout(out, self.dropout_prob, dropout_implementation='upscale_in_train') <IF_STMT> else out new_states.append(new_state) return (step_input, new_states)",if self.dropout_prob > 0.0,if self.dropout_prob,0.8855827282877784,0.8696398662122882,False
17,"def jupyter_progress_bar(min=0, max=1.0): """"""Returns an ipywidget progress bar or None if we can't import it"""""" widgets = wandb.util.get_module('ipywidgets') try: <IF_STMT> from IPython.html import widgets assert hasattr(widgets, 'VBox') assert hasattr(widgets, 'Label') assert hasattr(widgets, 'FloatProgress') return ProgressWidget(widgets, min=min, max=max) except (ImportError, AssertionError): return None",if widgets is None:,"if hasattr(widgets, 'ProgressWidget'):",0.7867125475170696,0.8928756684056034,False
18,"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) <IF_STMT> if not os.path.exists(path): self.watches.pop(path).close()",if events | pyuv.fs.UV_RENAME:,if fsevent_handle == self.watch_handle:,0.5082709659840907,0.6540585844910979,False
19,"def _get_v1_id_from_tags(self, tags_obj, tag): """"""Get image id from array of tags"""""" if isinstance(tags_obj, dict): try: return tags_obj[tag] except KeyError: pass elif isinstance(tags_obj, []): try: for tag_dict in tags_obj: <IF_STMT> return tag_dict['layer'] except KeyError: pass return ''",if tag_dict['name'] == tag:,if tag_dict['tag'] == tag:,0.8717849797335835,0.8516228624291206,False
20,"def query_lister(domain, query='', max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes(domain, query, attr_names, next_token=next_token) for item in rs: <IF_STMT> if num_results == max_items: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",if max_items:,if max_items is not None:,0.9174584895551361,0.8649799950178215,False
21,"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError('TypeError', 'callbackfn must be a function') _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: <IF_STMT> kValue = array.get(unicode(k)) if to_boolean(callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",if array.has_property(unicode(k)):,if array.has_property(unicode(k)):,0.9504298939664363,0.9325718821645923,True
22,"def every_one_is(self, dst): msg = 'all members of %r should be %r, but the %dth is %r' for index, item in enumerate(self._src): if self._range: if index < self._range[0] or index > self._range[1]: continue error = msg % (self._src, dst, index, item) <IF_STMT> raise AssertionError(error) return True",if item != dst:,if error:,0.8344780344696481,0.9220450449751959,False
23,"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): <IF_STMT> del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if job_id in key:,if key in LoggerFactory.schedule_logger_dict[key]:,0.9113845515231018,0.8592377270804451,False
24,"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] <IF_STMT> typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",elif item[2]:,elif item[2]:,0.6765321048871619,0.9184043388013005,True
25,"def _read_data_from_all_categories(self, directory, config, categories): lines = [] for category in categories: data_file = os.path.join(directory, _DATASET_VERSION, category, config) <IF_STMT> with open(data_file) as f: ls = f.read().split('\n') for l in ls[::-1]: if not l: ls.remove(l) lines.extend(ls) return lines",if os.path.exists(data_file):,if os.path.exists(data_file):,0.6702056847290858,0.9024521756077707,True
26,"def find_handlers(self, forms): handlers = {} for form in forms.itervalues(): for action_name, _action_label in form.actions: <IF_STMT> handlers[action_name] = form else: raise HandlerError('More than one form defines the handler %s' % action_name) return handlers",if action_name not in handlers:,if action_name in self.handlers:,0.7177669333690256,0.8385130047130208,False
27,"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]: action = get_action_with_primary_id(payload) kwargs = {'task_description': action['description']} story_id = action['story_id'] for ref in payload['references']: <IF_STMT> kwargs['name_template'] = STORY_NAME_TEMPLATE.format(name=ref['name'], app_url=ref['app_url']) if action['changes']['complete']['new']: return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs) else: return None",if ref['id'] == story_id:,if ref['story_id'] == story_id:,0.869301489625742,0.8336104423443033,False
28,"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): <IF_STMT> continue edge = (nodes[i], nodes[j]) if graph.has_edge(edge): graph.del_edge(edge) graph.add_edge(edge, 1)",if i == j:,if i == j:,0.8501234554457631,0.7801270245332924,True
29,"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): <IF_STMT> lift_stmt[-1].append(op) return op.body if op.attr_key == 'virtual_thread': return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError('not reached')",if op.attr_key == 'storage_scope':,if op.attr_key == 'virtual_thread':,0.8904877145616927,0.8555308664663046,False
30,"def format_lazy_import(names): """"""Formats lazy import lines"""""" lines = '' for _, name, asname in names: pkg, _, _ = name.partition('.') <IF_STMT> line = '{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n' else: line = '{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n' lines += line.format(pkg=pkg, mod=name, asname=asname) return lines",if asname is None:,if asname == 'default':,0.797238151600203,0.8723360571509826,False
31,"def evaluateWord(self, argument): wildcard_count = argument[0].count('*') if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith('*'): return self.GetWordWildcard(argument[0][1:], method='endswith') if wildcard_count == 1 and argument[0].endswith('*'): return self.GetWordWildcard(argument[0][:-1], method='startswith') else: _regex = argument[0].replace('*', '.+') matched = False for w in self.words: matched = bool(re.search(_regex, w)) <IF_STMT> break return matched return self.GetWord(argument[0])",if matched:,if matched:,0.9581009495953634,0.9284304001296656,True
32,"def setup(self, ir: 'IR', aconf: Config) -> bool: if self.kind == 'ConsulResolver': self.resolve_with = 'consul' <IF_STMT> self.post_error('ConsulResolver is required to have a datacenter') return False elif self.kind == 'KubernetesServiceResolver': self.resolve_with = 'k8s' elif self.kind == 'KubernetesEndpointResolver': self.resolve_with = 'k8s' else: self.post_error(f'Resolver kind {self.kind} unknown') return False return True",if not self.get('datacenter'):,elif self.kind == 'datacenter':,0.7223527931720645,0.8879659171421962,False
33,"def get_success_url(self): """"""Continue to the flow index or redirect according `?back` parameter."""""" if 'back' in self.request.GET: back_url = self.request.GET['back'] <IF_STMT> back_url = '/' return back_url return reverse(self.success_url)","if not is_safe_url(url=back_url, allowed_hosts={self.request.get_host()}):",if not back_url.startswith('/'):,0.7672475311825261,0.833078701050083,False
34,"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: if url.startswith('https://'): url = url[8:] <IF_STMT> url = 'http://' + url if playlist: download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if not url.startswith('http://'):,elif url.startswith('http://'):,0.8923880533660082,0.8935248372106969,False
35,def __str__(self): buf = [''] if self.fileName: buf.append(self.fileName + ':') if self.line != -1: <IF_STMT> buf.append('line ') buf.append(str(self.line)) if self.column != -1: buf.append(':' + str(self.column)) buf.append(':') buf.append(' ') return str('').join(buf),if not self.fileName:,if self.column != -1:,0.9026408796871273,0.8228500218338367,False
36,"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith('win'): output = output.replace('\\\n', '') environ = {} for line in output.splitlines(0): line = line.rstrip() if not line: continue item = _ParseBashEnvStr(line) <IF_STMT> environ[item[0]] = item[1] return environ",if item:,if item:,0.7548993990087446,0.8996480074924822,True
37,"def remove_selected(self): """"""Removes selected items from list."""""" to_delete = [] for i in range(len(self)): if self[i].selected: to_delete.append(i) to_delete.reverse() for i in to_delete: self.pop(i) if len(to_delete) > 0: first_to_delete = to_delete[-1] <IF_STMT> self[0].selected = True elif first_to_delete > 0: self[first_to_delete - 1].selected = True",if first_to_delete == 0 and len(self) > 0:,if first_to_delete == 1:,0.8044795964162201,0.8752376177722327,False
38,"def update(self, update_tracks=True): self.enable_update_metadata_images(False) old_album_title = self.metadata['album'] self.metadata['album'] = config.setting['nat_name'] for track in self.tracks: <IF_STMT> track.metadata['album'] = self.metadata['album'] for file in track.linked_files: track.update_file_metadata(file) self.enable_update_metadata_images(True) super().update(update_tracks)",if old_album_title == track.metadata['album']:,if track.title == old_album_title:,0.8742619996171338,0.7886336751695258,False
39,"def on_input(self, target, message): if message.strip() == '': self.panel('No commit message provided') return if target: command = ['git', 'add'] <IF_STMT> command.append('--all') else: command.extend(('--', target)) self.run_command(command, functools.partial(self.add_done, message)) else: self.add_done(message, '')",if target == '*':,if target == 'all':,0.8585980087257873,0.8228500218338367,False
40,"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos <IF_STMT> self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() if position < editor.document().characterCount(): editor.set_cursor_position(position)",if not osp.isfile(filename):,if filename is None:,0.7335543970866921,0.7965020533851944,False
41,"def returnByType(self, results): new_results = {} for r in results: type_name = r.get('type', 'movie') + 's' <IF_STMT> new_results[type_name] = [] new_results[type_name].append(r) if 'movies' in new_results: new_results['movies'] = self.combineOnIMDB(new_results['movies']) return new_results",if type_name not in new_results:,if type_name not in new_results:,0.6672231591366035,0.7975010608178975,True
42,def cache_sns_topics_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get('environment') == 'prod': cache_sns_topics_for_account.delay(account_id) el<IF_STMT> cache_sns_topics_for_account.delay(account_id) stats.count(f'{function}.success') return True,"if account_id in config.get('celery.test_account_ids', []):",if config.get('environment') == 'test':,0.8909604466125013,0.7965020533851944,False
43,"def get(self, subject, topic): """"""Handles GET requests."""""" if subject in feconf.AVAILABLE_LANDING_PAGES: <IF_STMT> self.render_template('topic-landing-page.mainpage.html') else: raise self.PageNotFoundException else: raise self.PageNotFoundException",if topic in feconf.AVAILABLE_LANDING_PAGES[subject]:,if topic in feconf.AVAILABLE_LANDING_PAGES[subject]:,0.5295576118332382,0.7245511487202049,True
44,"def callback(compiled): <IF_STMT> logger.show_tabulated('Compiled', showpath(codepath), 'without writing to file.') else: with univ_open(destpath, 'w') as opened: writefile(opened, compiled) logger.show_tabulated('Compiled to', showpath(destpath), '.') if self.show: print(compiled) if run: if destpath is None: self.execute(compiled, path=codepath, allow_show=False) else: self.execute_file(destpath)",if destpath is None:,if destpath is None:,0.6108626685989751,0.8549644769692295,True
45,"def _find_start_index(self, string, start, end): while True: index = string.find('{', start, end) - 1 if index < 0: return -1 <IF_STMT> return index start = index + 2","if self._start_index_is_ok(string, index):",if string[index + 1] == '{':,0.6002867180976867,0.7590598306198806,False
46,"def _get_nlu_target_format(export_path: Text) -> Text: guessed_format = loading.guess_format(export_path) if guessed_format not in {MARKDOWN, RASA, RASA_YAML}: if rasa.shared.data.is_likely_json_file(export_path): guessed_format = RASA elif rasa.shared.data.is_likely_markdown_file(export_path): guessed_format = MARKDOWN <IF_STMT> guessed_format = RASA_YAML return guessed_format",elif rasa.shared.data.is_likely_yaml_file(export_path):,elif rasa.shared.data.is_likely_yaml_file(export_path):,0.9097848669943025,0.8827916928185874,True
47,"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: if p.v.children: p.moveToFirstChild() el<IF_STMT> p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break p.moveToParent() return p",if p.hasNext():,if p.hasNext():,0.909354671099589,0.8743414417652072,True
48,"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) <IF_STMT> continue name = name_fmt % attr if formatter is not None: value = formatter(attr, value) info_add(name, value)",if value is None:,if value is None:,0.6335508207046372,0.8385130047130208,True
49,"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, 'Description'): <IF_STMT> attr = desc.getAttributeNodeNS(namespace, name) if attr != None: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element","if desc.getAttributeNS(RDF_NAMESPACE, 'about') == aboutUri:",if aboutUri == desc.getAttribute('aboutUri'):,0.8333683692111752,0.8105932471967202,False
50,def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() <IF_STMT> dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),if self.timeout is not None:,if self.counter == self.count:,0.9213379657900601,0.8431339019329497,False
51,"def _parse_fixits(message, titer, line): """"""Parses fixit messages."""""" while OutputParser.message_line_re.match(line) is None and OutputParser.note_line_re.match(line) is None: message_text = line.strip() <IF_STMT> message.fixits.append(Note(message.path, message.line, line.find(message_text) + 1, message_text)) line = next(titer) return line",if message_text != '':,if message_text:,0.7723191989891768,0.8787142254774354,False
52,"def _connect_db(self, force_reconnect=False): thread_id = thread.get_ident() if force_reconnect and thread_id in ENGINES: del ENGINES[thread_id] conn = None try: engine = ENGINES[thread_id] conn = engine.connect() _test = conn.execute('SELECT 1') _test.fetchall() except (KeyError, MySQLdb.OperationalError): <IF_STMT> conn.close() engine = sqla.create_engine(self.db_url, pool_recycle=3600) ENGINES[thread_id] = engine conn = engine.connect() return conn",if conn:,if conn:,0.9131697280777684,0.9220450449751959,True
53,"def read(self, n): if self.current_frame: data = self.current_frame.read(n) <IF_STMT> self.current_frame = None return self.file_read(n) if len(data) < n: raise UnpicklingError('pickle exhausted before end of frame') return data else: return self.file_read(n)",if not data and n != 0:,if not data:,0.8195244524324464,0.8498644646741501,False
54,"def __setLoadCmd(self): base = self.__rawLoadCmd for _ in range(self.__machHeader.ncmds): command = LOAD_COMMAND.from_buffer_copy(base) <IF_STMT> segment = SEGMENT_COMMAND.from_buffer_copy(base) self.__setSections(segment, base[56:], 32) elif command.cmd == MACHOFlags.LC_SEGMENT_64: segment = SEGMENT_COMMAND64.from_buffer_copy(base) self.__setSections(segment, base[72:], 64) base = base[command.cmdsize:]",if command.cmd == MACHOFlags.LC_SEGMENT:,if command.cmd == MACHOFlags.LC_SEGMENT_32:,0.7155293420949627,0.8336104423443033,False
55,"def emit_post_sync_signal(created_models, verbosity, interactive, db): for app in models.get_apps(): app_name = app.__name__.split('.')[-2] <IF_STMT> print('Running post-sync handlers for application %s' % app_name) models.signals.post_syncdb.send(sender=app, app=app, created_models=created_models, verbosity=verbosity, interactive=interactive, db=db)",if verbosity >= 2:,if verbosity > 0:,0.7314834984613381,0.8038019482772603,False
56,"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get('origin', '') if url in repo.remotes: origin = url url = repo.remotes.get(origin) <IF_STMT> repo.pull(origin_uri=url) else: print('No pull URL.') else: print(command_help['git pull'])",if url:,if url:,0.7306371473767452,0.9076141716697395,True
57,"def version(self): try: return self._version except AttributeError: for line in self._get_metadata(self.PKG_INFO): <IF_STMT> self._version = safe_version(line.split(':', 1)[1].strip()) return self._version else: tmpl = ""Missing 'Version:' header and/or %s file"" raise ValueError(tmpl % self.PKG_INFO, self)",if line.lower().startswith('version:'):,if line.startswith('Version:'):,0.9069906095060816,0.8866029039778043,False
58,"def increment(self, metric, labels, delta): """"""Increment a value by |delta|."""""" with self._lock: key = self._get_key(metric.name, labels) <IF_STMT> start_time = self._store[key].start_time value = self._store[key].value + delta else: start_time = time.time() value = metric.default_value + delta self._store[key] = _StoreValue(metric, labels, start_time, value)",if key in self._store:,if key in self._store:,0.7131976067945397,0.8661072626070159,True
59,"def get_current_connections(session): """"""Retrieves open connections using the the given session"""""" res = session.sql('SHOW PROCESSLIST').execute() rows = res.fetch_all() connections = {} for row in rows: <IF_STMT> connections[row.get_string('User')] = [row.get_string('Host')] else: connections[row.get_string('User')].append(row.get_string('Host')) return connections",if row.get_string('User') not in connections:,if row.get_string('User') not in connections:,0.8883062863063222,0.8094220211349227,True
60,"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: <IF_STMT> etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: if not os.path.exists(fspath + '.spt'): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if env.cache_static:,if os.path.isfile(fspath):,0.700917730033414,0.9362597875749384,False
61,def thread_loop(self) -> None: while not self.stop_event.is_set(): time.sleep(1) new_trials = self.study.trials with self.lock: need_to_add_callback = self.new_trials is None self.new_trials = new_trials <IF_STMT> self.doc.add_next_tick_callback(self.update_callback),if need_to_add_callback:,if need_to_add_callback:,0.7979850579261304,0.8764445248055556,True
62,"def _cache_db_tables_iterator(tables, cache_alias, db_alias): no_tables = not tables cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,) db_aliases = settings.DATABASES if db_alias is None else (db_alias,) for db_alias in db_aliases: if no_tables: tables = connections[db_alias].introspection.table_names() <IF_STMT> for cache_alias in cache_aliases: yield (cache_alias, db_alias, tables)",if tables:,if tables:,0.8779868676181383,0.9184043388013005,True
63,"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: if hasattr(subscriber, '_pyroRelease'): subscriber._pyroRelease() <IF_STMT> try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)","if hasattr(subscriber, '_pyroUri'):","if hasattr(subscriber, '_pyroUri'):",0.8963182591211631,0.8120341702859789,True
64,"def test_constructor(job_id): with patch('apscheduler.job.Job._modify') as _modify: scheduler_mock = MagicMock(BaseScheduler) job = Job(scheduler_mock, id=job_id) assert job._scheduler is scheduler_mock assert job._jobstore_alias is None modify_kwargs = _modify.call_args[1] <IF_STMT> assert len(modify_kwargs['id']) == 32 else: assert modify_kwargs['id'] == job_id",if job_id is None:,if modify_kwargs['type'] == 'jobstore':,0.8776715751674016,0.8431339019329497,False
65,"def get_connection(self): if self.config.proxy_host != '': return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port) el<IF_STMT> return httplib.HTTPSConnection(self.config.simpledb_host) else: return httplib.HTTPConnection(self.config.simpledb_host)",if self.config.use_https:,if self.config.simpledb_port == 443:,0.5707340391017095,0.6540585844910979,False
66,"def notify_login(self, ipaddress=''): if app.NOTIFY_ON_LOGIN: update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT] title = common.notifyStrings[common.NOTIFY_LOGIN] <IF_STMT> self._notify_pht(title, update_text.format(ipaddress))",if update_text and title and ipaddress:,if title:,0.584396136695097,0.7487402156832422,False
67,"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: if item['layout'] == 'horiz': return 0.03 + ctrl.labelHeight * 3 <IF_STMT> return ctrl.labelHeight * len(item['options'])",elif item['layout'] == 'vert':,elif item['layout'] == 'vertical':,0.8886638254398045,0.8723360571509826,False
68,"def _get_errors_lines(self): """"""Return the number of lines that contains errors to highlight."""""" errors_lines = [] block = self.document().begin() while block.isValid(): user_data = get_user_data(block) <IF_STMT> errors_lines.append(block.blockNumber()) block = block.next() return errors_lines",if user_data.error:,if user_data.errors:,0.8016242206266552,0.8787142254774354,False
69,"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _('Processing...')) self.pbar.set_text(progress) if frac > 1: frac = 1.0 <IF_STMT> frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",if frac < 0:,elif frac < 0:,0.6608921021613609,0.8105932471967202,False
70,"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [''] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) if os.path.isdir(os.path.join(basedir, filename)): directories.append(filename) <IF_STMT> yield filename","elif os.path.exists(os.path.join(basedir, filename)):",if os.path.isfile(filename):,0.9409368100667528,0.9146232957723002,False
71,"def assistive(self): """"""Detects if item can be used as assistance"""""" if self.__assistive is None: assistive = False for effect in self.effects.values(): <IF_STMT> assistive = True break self.__assistive = assistive return self.__assistive",if effect.isAssistance is True:,if effect.is_assistance:,0.9013841206078421,0.8827916928185874,False
72,"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) <IF_STMT> min_dist = dist closest_unseen = (row, col) return closest_unseen",if dist < min_dist:,if dist < min_dist:,0.868690216666784,0.8902056737869248,True
73,"def _maybe_has_default_route(self): for route in self.iter_routes(): <IF_STMT> return True for iface in self.iter_interfaces(): for subnet in iface.get('subnets', []): for route in subnet.get('routes', []): if self._is_default_route(route): return True return False",if self._is_default_route(route):,if self._is_default_route(route):,0.6847594290160287,0.8743414417652072,True
74,"def data(self, data): if data is None: raise Exception('Data cannot be None') val = [] for d in data: if isinstance(d, str): val.append(bytes(d, 'utf-8')) <IF_STMT> val.append(d) else: raise Exception('Invalid type, data can only be an str or a bytes not {}: {}'.format(type(data), d)) self.__data = val","elif isinstance(d, bytes):","elif isinstance(d, bytes):",0.9288334719262681,0.9022045190074797,True
75,"def get_one_segment_function(data, context, echoerr): ext = data['ext'] function_name = context[-2][1].get('function') if function_name: module, function_name = get_function_strings(function_name, context, ext) func = import_segment(function_name, data, context, echoerr, module=module) <IF_STMT> yield func",if func:,if func:,0.8798049124819313,0.8798128223519914,True
76,"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): if isinstance(value, list): for item in value: <IF_STMT> self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)","if isinstance(item, AST):","if isinstance(item, AST):",0.9137624954378352,0.8547305998833805,True
77,"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] <IF_STMT> if v['FeatureType'] == 'val': if name.startswith('SCE_'): states.append((name, v['Value'])) elif name.startswith('SCLEX_'): lexers.append((name, v['Value'])) return (lexers, states)",if v['Category'] != 'Deprecated':,if v['FeatureType'] == 'lex' and v['Value']:,0.7735727748459773,0.7803242766062389,False
78,"def things(self, query): limit = query.pop('limit', 100) offset = query.pop('offset', 0) keys = set(self.docs) for k, v in query.items(): <IF_STMT> flat = common.flatten_dict(v)[0] k += '.' + web.rstrips(flat[0], '.key') v = flat[1] keys = set((k for k in self.filter_index(self.index, k, v) if k in keys)) keys = sorted(keys) return keys[offset:offset + limit]","if isinstance(v, dict):","if isinstance(v, dict):",0.9409243875833345,0.9134996171406936,True
79,"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: return None <IF_STMT> self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: return None",elif self._keys[hash_] == key:,if self._keys[hash_] == self._deleted:,0.9026556795840508,0.8516228624291206,False
80,"def test_204_invalid_content_length(self): with ExpectLog(gen_log, '.*Response with code 204 should not have body'): response = self.fetch('/?error=1') <IF_STMT> self.skipTest('requires HTTP/1.x') if self.http_client.configured_class != SimpleAsyncHTTPClient: self.skipTest('curl client accepts invalid headers') self.assertEqual(response.code, 599)",if not self.http1:,if response.code == 204:,0.9065999056812353,0.8169276475307028,False
81,"def __str__(self) -> str: text = '\n' for k, r in self.result.items(): text += '{}\n'.format('#' * 40) <IF_STMT> text += '# {} (failed)\n'.format(k) else: text += '# {} (succeeded)\n'.format(k) text += '{}\n'.format('#' * 40) for sub_r in r: text += '**** {}\n'.format(sub_r.name) text += '{}\n'.format(sub_r) return text",if r.failed:,if r is None:,0.689800090872829,0.8856327184319047,False
82,"def DeleteTask(): oid = request.form.get('oid', '') if oid: result = Mongo.coll['Task'].delete_one({'_id': ObjectId(oid)}) <IF_STMT> result = Mongo.coll['Result'].delete_many({'task_id': ObjectId(oid)}) if result: return 'success' return 'fail'",if result.deleted_count > 0:,if result:,0.906924893852286,0.839587623092576,False
83,"def _replace_vars(self, line, extracted, env_variables): for e in extracted: <IF_STMT> value = env_variables.get(e) if isinstance(value, dict) or isinstance(value, list): value = pprint.pformat(value) decorated = self._decorate_var(e) line = line.replace(decorated, str(value)) return line",if e in env_variables:,if e in env_variables:,0.7972260861076541,0.828399516355805,True
84,"def should_include(service): for f in filt: if f == 'status': state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == 'source': source = filt[f] if source == 'image' or source == 'build': <IF_STMT> return False else: raise UserError('Invalid value for source filter: %s' % source) else: raise UserError('Invalid filter: %s' % f) return True",if source not in service.options:,"if not has_image_with_state(containers, state):",0.661183598610572,0.9099929453837925,False
85,def state_callback_loop(): if usercallback: when = 1 while when and (not self.future_removed.done()) and (not self.session.shutdownstarttime): result = usercallback(self.get_state()) when = await result if iscoroutine(result) else result <IF_STMT> await sleep(when),if when > 0.0 and (not self.session.shutdownstarttime):,if when:,0.8156249758711598,0.8841121363289183,False
86,"def __get_new_timeout(self, timeout): """"""When using --timeout_multiplier=#.#"""""" self.__check_scope() try: timeout_multiplier = float(self.timeout_multiplier) <IF_STMT> timeout_multiplier = 0.5 timeout = int(math.ceil(timeout_multiplier * timeout)) return timeout except Exception: return timeout",if timeout_multiplier <= 0.5:,if timeout_multiplier < 0.5:,0.5901953882237644,0.7965020533851944,False
87,"def readexactly(self, n): buf = b'' while n: yield IORead(self.s) res = self.s.read(n) assert res is not None <IF_STMT> yield IOReadDone(self.s) break buf += res n -= len(res) return buf",if not res:,if len(res) == 0:,0.8925561097146278,0.8228500218338367,False
88,"def contract_rendering_pane(event): """"""Expand the rendering pane."""""" c = event.get('c') if c: vr = c.frame.top.findChild(QtWidgets.QWidget, 'viewrendered_pane') <IF_STMT> vr.contract() else: viewrendered(event)",if vr:,if vr:,0.7568223293455816,0.803154665668484,True
89,"def translate_headers(self, environ): """"""Translate CGI-environ header names to HTTP header names."""""" for cgiName in environ: <IF_STMT> yield (self.headerNames[cgiName], environ[cgiName]) elif cgiName[:5] == 'HTTP_': translatedHeader = cgiName[5:].replace('_', '-') yield (translatedHeader, environ[cgiName])",if cgiName in self.headerNames:,if cgiName in self.headerNames:,0.7846849167651057,0.8228500218338367,True
90,"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: <IF_STMT> string_value = str(string_value).strip() if string_value != 'NONE': param_value = int(string_value) except ValueError: self.pcluster_config.warn(""Unable to convert the value '{0}' to an Integer. Using default value for parameter '{1}'"".format(string_value, self.key)) return param_value",if string_value is not None:,if string_value:,0.6832805638707731,0.9202663016973823,False
91,"def monitor_filter(self): """"""Return filtered service objects list"""""" services = self.client.services.list(filters={'label': 'com.ouroboros.enable'}) monitored_services = [] for service in services: ouro_label = service.attrs['Spec']['Labels'].get('com.ouroboros.enable') <IF_STMT> monitored_services.append(service) self.data_manager.monitored_containers[self.socket] = len(monitored_services) self.data_manager.set(self.socket) return monitored_services","if not self.config.label_enable or ouro_label.lower() in ['true', 'yes']:",if ouro_label:,0.7830876014347591,0.8743414417652072,False
92,"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 <IF_STMT> nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if nei >= len(self._editableChildren):,if nei >= len(self._editableChildren):,0.8863027650469878,0.8856327184319047,True
93,"def linkify_cm_by_tp(self, timeperiods): for rm in self: mtp_name = rm.modulation_period.strip() mtp = timeperiods.find_by_name(mtp_name) <IF_STMT> err = ""Error: the business impact modulation '%s' got an unknown modulation_period '%s'"" % (rm.get_name(), mtp_name) rm.configuration_errors.append(err) rm.modulation_period = mtp",if mtp_name != '' and mtp is None:,if mtp is None:,0.7928983034910451,0.8431339019329497,False
94,def close_open_fds(keep=None): keep = [maybe_fileno(f) for f in keep or [] if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): <IF_STMT> try: os.close(fd) except OSError as exc: if exc.errno != errno.EBADF: raise,if fd not in keep:,if fd not in keep:,0.9086079949253487,0.8094220211349227,True
95,"def _append_child_from_unparsed_xml(father_node, unparsed_xml): """"""Append child xml nodes to a node."""""" dom_tree = parseString(unparsed_xml) if dom_tree.hasChildNodes(): first_child = dom_tree.childNodes[0] <IF_STMT> child_nodes = first_child.childNodes for _ in range(len(child_nodes)): childNode = child_nodes.item(0) father_node.appendChild(childNode) return raise DistutilsInternalError('Could not Append append elements to the Windows msi descriptor.')",if first_child.hasChildNodes():,if first_child.tagName == 'child':,0.7769511833328542,0.8723360571509826,False
96,"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) <IF_STMT> body = six.ensure_str(request.body) if old in body: request.body = body.replace(old, new) return request",if is_text_payload(request) and request.body:,if request.body:,0.6751083226825181,0.8590888738245122,False
97,"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, 'func_name_highlighting', True) self.disabled_modules = get_list_opt(options, 'disabled_modules', []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._luabuiltins import MODULES for mod, func in MODULES.iteritems(): <IF_STMT> self._functions.update(func) RegexLexer.__init__(self, **options)",if mod not in self.disabled_modules:,if mod in self.disabled_modules:,0.8976534365350045,0.828399516355805,False
98,"def GetBestSizeForParentSize(self, parentSize): """"""Finds the best width and height given the parent's width and height."""""" if len(self.GetChildren()) == 1: win = self.GetChildren()[0] <IF_STMT> temp_dc = wx.ClientDC(self) childSize = win.GetBestSizeForParentSize(parentSize) clientParentSize = self._art.GetPanelClientSize(temp_dc, self, wx.Size(*parentSize), None) overallSize = self._art.GetPanelSize(temp_dc, self, wx.Size(*clientParentSize), None) return overallSize return self.GetSize()","if isinstance(win, RibbonControl):",if win:,0.8430147655958216,0.9202663016973823,False
99,"def pid_from_name(name): processes = [] for pid in os.listdir('/proc'): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) <IF_STMT> return pid if name in cmdline.split(' ', 1)[0]: return pid except: pass raise ProcessException('No process with such name: %s' % name)",if name in pname:,if pname == 'pid':,0.9296846026120532,0.8627586293513119,False
100,"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element <IF_STMT> i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",if element[3] and element[4]:,elif element[3] == 'file':,0.911286129345381,0.8592377270804451,False
101,"def scan_block_scalar_indentation(self): chunks = [] max_indent = 0 end_mark = self.get_mark() while self.peek() in ' \r\n\x85\u2028\u2029': if self.peek() != ' ': chunks.append(self.scan_line_break()) end_mark = self.get_mark() else: self.forward() <IF_STMT> max_indent = self.column return (chunks, max_indent, end_mark)",if self.column > max_indent:,if self.column > max_indent:,0.8274482648555297,0.8474968231198384,True
102,"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' elif col == BARRIER: tmp += '%' <IF_STMT> tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == FOOD:,elif col == GRAY:,0.9431195319727762,0.924776563154472,False
103,"def prepare_data(entry): branch_wise_entries = {} gross_pay = 0 for d in entry: gross_pay += d.gross_pay <IF_STMT> branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay else: branch_wise_entries.setdefault(d.branch, {}).setdefault(d.mode_of_payment, d.net_pay) return (branch_wise_entries, gross_pay)",if branch_wise_entries.get(d.branch):,if d.branch in branch_wise_entries:,0.7771092842169443,0.7965020533851944,False
104,"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs): self.uuid = uuid self.cluster_state = cluster_state if self.cluster_state is not None: self.children = WeakSet((self.cluster_state.tasks.get(task_id) for task_id in children or () <IF_STMT>)) else: self.children = WeakSet() self._serializer_handlers = {'children': self._serializable_children, 'root': self._serializable_root, 'parent': self._serializable_parent} if kwargs: self.__dict__.update(kwargs)",if task_id in self.cluster_state.tasks,if task_id,0.7218662835900896,0.9144061946646023,False
105,"def listdir(self, d): try: return [p for p in os.listdir(d) <IF_STMT>] except OSError: return []","if os.path.basename(p) != 'CVS' and os.path.isdir(os.path.join(d, p))",if p.startswith('.') and p.endswith('.') and (not p.startswith('_')),0.4581582569909508,0.5530711031691576,False
106,"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: <IF_STMT> command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() if len(e.args) == 1: _errno, errmsg = ('UNKNOWN', e.args[0]) else: _errno, errmsg = e.args raise ConnectionError('Error %s while writing to socket. %s.' % (_errno, errmsg)) except Exception: self.disconnect() raise","if isinstance(command, str):","if not isinstance(command, list):",0.7549419644084302,0.8964173245779284,False
107,"def run(self): """"""Start the scanner"""""" logging.info('Dirscanner starting up') self.shutdown = False while not self.shutdown: with self.loop_condition: self.loop_condition.wait(self.dirscan_speed) <IF_STMT> self.scan()",if self.dirscan_speed and (not self.shutdown):,if self.loop_condition.is_set():,0.8812359796431336,0.8492326635760689,False
108,"def __aexit__(self, exc_type: type, exc_value: BaseException, tb: TracebackType) -> None: if exc_type is not None: await self.close() await self._task while not self._receive_queue.empty(): data = await self._receive_queue.get() if isinstance(data, bytes): self.response_data.extend(data) <IF_STMT> raise data","elif not isinstance(data, HTTPDisconnect):",elif exc_type is not None:,0.8702882383525105,0.8225938544151185,False
109,"def f(msg): text = extractor(msg) for px in prefix: <IF_STMT> chunks = text[len(px):].split(separator) return (chunks[0], (chunks[1:],) if pass_args else ()) return ((None,),)",if text.startswith(px):,if text.startswith(px):,0.8706147100063971,0.8318180062062374,True
110,def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) <IF_STMT> for ah in item: if type(ah) is not ActionHandle: raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs,"elif type(item) in (list, tuple, dict, set):","elif isinstance(item, list):",0.927870375560657,0.9022045190074797,False
111,"def find_class(self, module, name): sys.audit('pickle.find_class', module, name) if self.proto < 3 and self.fix_imports: if (module, name) in _compat_pickle.NAME_MAPPING: module, name = _compat_pickle.NAME_MAPPING[module, name] <IF_STMT> module = _compat_pickle.IMPORT_MAPPING[module] __import__(module, level=0) if self.proto >= 4: return _getattribute(sys.modules[module], name)[0] else: return getattr(sys.modules[module], name)",elif module in _compat_pickle.IMPORT_MAPPING:,"if (module, name) in _compat_pickle.IMPORT_MAPPING:",0.9191876431217255,0.845713978670975,False
112,"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: wr = util.wait_for_write(self.socket, self.socket.gettimeout()) <IF_STMT> raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))",if not wr:,if wr is None:,0.6034538798110358,0.7801270245332924,False
113,"def __new__(cls, *args, **kwargs): """"""Hack to ensure method defined as async are implemented as such."""""" coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction) for coroutine in coroutines: implemented_method = getattr(cls, coroutine[0]) <IF_STMT> raise RuntimeError('The method %s must be a coroutine' % implemented_method) return super().__new__(cls, *args, **kwargs)",if not inspect.iscoroutinefunction(implemented_method):,if not implemented_method:,0.8224255278104917,0.8928756684056034,False
114,"def add_directive(self, name, obj, content=None, arguments=None, **options): if isinstance(obj, clstypes) and issubclass(obj, Directive): <IF_STMT> raise ExtensionError('when adding directive classes, no additional arguments may be given') directives.register_directive(name, directive_dwim(obj)) else: obj.content = content obj.arguments = arguments obj.options = options directives.register_directive(name, obj)",if content or arguments or options:,if not arguments:,0.7744674975881128,0.884617925078158,False
115,"def create(self, w): if w.use_eventloop: w.timer = _Timer(max_interval=10.0) else: <IF_STMT> w.timer_cls = w.pool_cls.Timer w.timer = self.instantiate(w.timer_cls, max_interval=w.timer_precision, on_error=self.on_timer_error, on_tick=self.on_timer_tick)",if not w.timer_cls:,"if not hasattr(w, 'timer_cls'):",0.7070673436886159,0.7245511487202049,False
116,"def _config(_molecule_file, request): with open(_molecule_file) as f: d = util.safe_load(f) if hasattr(request, 'param'): <IF_STMT> d2 = util.safe_load(request.getfixturevalue(request.param)) else: d2 = request.getfixturevalue(request.param) d = util.merge_dicts(d, d2) return d","if isinstance(request.getfixturevalue(request.param), str):","if isinstance(request.param, str):",0.6336458522652526,0.833078701050083,False
117,"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): <IF_STMT> new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model","if isinstance(value, tf.keras.layers.Layer):","if isinstance(value, tf.keras.layers.Layer):",0.8989081615694023,0.8749766281017177,True
118,"def is_accepted_drag_event(self, event): if event.source() == self.table: return True mime = event.mimeData() if mime.hasUrls(): for url in mime.urls(): <IF_STMT> break filename = url.toLocalFile() extension = os.path.splitext(filename)[1].lower()[1:] if extension not in _dictionary_formats(): break else: return True return False",if not url.isLocalFile():,if url.is_file():,0.9107293922329541,0.9024521756077707,False
119,"def explain(self, other, depth=0): exp = super(UnionType, self).explain(other, depth) for ndx, subtype in enumerate(self.params['allowed_types']): <IF_STMT> exp += '\n{}and'.format(''.join(['\t'] * depth)) exp += '\n' + subtype.explain(other, depth=depth + 1) return exp",if ndx > 0:,if ndx == 0:,0.867482301978981,0.8228500218338367,False
120,"def test_k_is_stochastic_parameter(self): aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) if np.array_equal(observed, self.blur3x3): seen[0] += True <IF_STMT> seen[1] += True else: raise Exception('Unexpected result in MedianBlur@2') if all(seen): break assert np.all(seen)","elif np.array_equal(observed, self.blur5x5):","elif np.array_equal(observed, self.blur5x5):",0.9200589967739802,0.8815741981066073,True
121,def test_get_message(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: message_id = await self._send_message() message = await self.chat_thread_client.get_message(message_id) assert message.id == message_id assert message.type == ChatMessageType.TEXT assert message.content.message == 'hello world' <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():,if self.thread_id:,0.9257918032389332,0.9016857283765338,False
122,"def do_write_property(self, device, callback=None): try: iocb = device <IF_STMT> else self.form_iocb(device, request_type='writeProperty') deferred(self.request_io, iocb) self.requests_in_progress.update({iocb: {'callback': callback}}) iocb.add_callback(self.__general_cb) except Exception as error: log.exception('exception: %r', error)","if isinstance(device, IOCB)",if device is not None,0.8157919175969559,0.759907656827929,False
123,"def fit(self, dataset, force_retrain): if force_retrain: self.sub_unit_1['fitted'] = True self.sub_unit_1['calls'] += 1 self.sub_unit_2['fitted'] = True self.sub_unit_2['calls'] += 1 else: if not self.sub_unit_1['fitted']: self.sub_unit_1['fitted'] = True self.sub_unit_1['calls'] += 1 <IF_STMT> self.sub_unit_2['fitted'] = True self.sub_unit_2['calls'] += 1 return self",if not self.sub_unit_2['fitted']:,if not self.sub_unit_2['fitted']:,0.844578441971093,0.8783650674919876,True
124,"def _insert_with_loop(self): id_list = [] last_id = None return_id_list = self._return_id_list for row in self._rows: last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute() <IF_STMT> id_list.append(last_id) if return_id_list: return id_list else: return last_id",if return_id_list:,if last_id:,0.8372576794648823,0.8696398662122882,False
125,"def merge_block(self): """"""merges a block in the map"""""" for i in range(self.block.x): for j in range(self.block.x): c = self.block.get(i, j) <IF_STMT> self.map[i + self.block.pos.x, j + self.block.pos.y] = c",if c:,if c is not None:,0.7727395979618515,0.7909601595885504,False
126,"def configure_plex(config): core.PLEX_SSL = int(config['Plex']['plex_ssl']) core.PLEX_HOST = config['Plex']['plex_host'] core.PLEX_PORT = config['Plex']['plex_port'] core.PLEX_TOKEN = config['Plex']['plex_token'] plex_section = config['Plex']['plex_sections'] or [] if plex_section: <IF_STMT> plex_section = ','.join(plex_section) plex_section = [tuple(item.split(',')) for item in plex_section.split('|')] core.PLEX_SECTION = plex_section","if isinstance(plex_section, list):","if ',' in plex_section:",0.930645491357841,0.8474968231198384,False
127,"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) <IF_STMT> if xlib.XFilterEvent(e, e.xany.window): continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)","if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease):",if e.xany:,0.809221817511257,0.8318180062062374,False
128,"def format_message(self): bits = [self.message] if self.possibilities: <IF_STMT> bits.append('Did you mean %s?' % self.possibilities[0]) else: possibilities = sorted(self.possibilities) bits.append('(Possible options: %s)' % ', '.join(possibilities)) return '  '.join(bits)",if len(self.possibilities) == 1:,if len(self.possibilities) == 1:,0.6432238044298715,0.8038019482772603,True
129,"def _collect_logs(model): page_token = None all_logs = [] while True: paginated_logs = model.lookup_logs(now, later, page_token=page_token) page_token = paginated_logs.next_page_token all_logs.extend(paginated_logs.logs) <IF_STMT> break return all_logs",if page_token is None:,if not paginated_logs.logs:,0.816424746047558,0.80377750806414,False
130,"def run(self): while True: context_id_list_tuple = self._inflated_addresses.get(block=True) <IF_STMT> break c_id, inflated_address_list = context_id_list_tuple inflated_value_map = dict(inflated_address_list) if c_id in self._contexts: self._contexts[c_id].set_from_tree(inflated_value_map)",if context_id_list_tuple is _SHUTDOWN_SENTINEL:,if context_id_list_tuple is None:,0.5908929236711481,0.7498810286408993,False
131,"def _setup_prefix(self): path = self.module_path old = None while path != old: <IF_STMT> self.egg_name = os.path.basename(path) self.egg_info = os.path.join(path, 'EGG-INFO') self.egg_root = path break old = path path, base = os.path.split(path)",if path.lower().endswith('.egg'):,if os.path.isdir(path):,0.7061990031168413,0.8827916928185874,False
132,"def get_filename(self, prompt): okay = False val = '' while not okay: val = raw_input('%s: %s' % (prompt, val)) val = os.path.expanduser(val) if os.path.isfile(val): okay = True <IF_STMT> path = val val = self.choose_from_list(os.listdir(path)) if val: val = os.path.join(path, val) okay = True else: val = '' else: print('Invalid value: %s' % val) val = '' return val",elif os.path.isdir(val):,elif os.path.isdir(val):,0.7686035862361914,0.938501942261528,True
133,"def versions(self, sitename, data): if 'query' in data: q = json.loads(data['query']) itemid = self._get_itemid(q.get('key')) <IF_STMT> key = q['key'] return json.dumps([self.dummy_edit(key)]) return ConnectionMiddleware.versions(self, sitename, data)",if itemid:,if itemid:,0.8634710148429153,0.8466657105524215,True
134,"def read_stanza(self): while True: try: stanza_end = self._buffer.index(b'\n') stanza = self.decoder.decode(self._buffer[:stanza_end]) self._buffer = self._buffer[stanza_end + 1:] colon = stanza.index(':') return (stanza[:colon], stanza[colon + 1:]) except ValueError: bytes = self.read_bytes() <IF_STMT> return None else: self._buffer += bytes",if not bytes:,if bytes is None:,0.8369864632112634,0.8516228624291206,False
135,def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit <IF_STMT> if attrnames.has_key(mask): names.append(attrnames[mask]) else: names.append(hex(mask)) return names,if attrs & mask:,if attrs & mask:,0.7012352345888916,0.760856626273165,True
136,"def _set_http_cookie(): if conf.cookie: <IF_STMT> conf.http_headers[HTTP_HEADER.COOKIE] = '; '.join(map(lambda x: '='.join(x), conf.cookie.items())) else: conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie","if isinstance(conf.cookie, dict):","if isinstance(conf.cookie, dict):",0.840220660784737,0.7160350546947921,True
137,"def __ne__(self, other): if isinstance(other, WeakMethod): <IF_STMT> return self is not other return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref return True",if not self._alive or not other._alive:,if self is not other:,0.8110251332170516,0.7178970818142898,False
138,"def update_unread(self, order_id, reset=False): conn = Database.connect_database(self.PATH) with conn: cursor = conn.cursor() <IF_STMT> cursor.execute('UPDATE sales SET unread = unread + 1 WHERE id=?;', (order_id,)) else: cursor.execute('UPDATE sales SET unread=0 WHERE id=?;', (order_id,)) conn.commit() conn.close()",if reset is False:,if reset:,0.8857385518560803,0.8935248372106969,False
139,"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) <IF_STMT> field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",elif key == 'nw_dst':,elif key == 'nw_dst':,0.8520320674950466,0.8902056737869248,True
140,"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) <IF_STMT> if keep_nested is not None: if isinstance(v, dict): yield (k, dict(self.nested_filter(v.items(), keep_nested))) else: yield (k, v)",if k in keep_current:,if keep_current is not None:,0.7349131039438659,0.8248765135255685,False
141,"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: <IF_STMT> break elif p: p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",if p and p.isMarked():,if p.isMarked():,0.8995553938003387,0.9184043388013005,False
142,"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): <IF_STMT> if isinstance(v, NestedSpace): sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret",if k in striped_keys:,if k not in striped_keys:,0.8166936544015433,0.8677319190106252,False
143,"def update_gradients_full(self, dL_dK, X, X2=None): if self.ARD: phi1 = self.phi(X) <IF_STMT> self.variance.gradient = np.einsum('ij,iq,jq->q', dL_dK, phi1, phi1) else: phi2 = self.phi(X2) self.variance.gradient = np.einsum('ij,iq,jq->q', dL_dK, phi1, phi2) else: self.variance.gradient = np.einsum('ij,ij', dL_dK, self._K(X, X2)) * self.beta",if X2 is None or X is X2:,if X2 is None:,0.7607464092877705,0.8516228624291206,False
144,"def post(self): host_json = json.loads(request.data) host_os = host_json.get('os') if host_os: result = get_monkey_executable(host_os.get('type'), host_os.get('machine')) if result: executable_filename = result['filename'] real_path = MonkeyDownload.get_executable_full_path(executable_filename) <IF_STMT> result['size'] = os.path.getsize(real_path) return result return {}",if os.path.isfile(real_path):,if real_path:,0.856600099996934,0.8787142254774354,False
145,"def _encode_data(self, data, content_type): if content_type is MULTIPART_CONTENT: return encode_multipart(BOUNDARY, data) else: match = CONTENT_TYPE_RE.match(content_type) <IF_STMT> charset = match.group(1) else: charset = settings.DEFAULT_CHARSET return force_bytes(data, encoding=charset)",if match:,if match:,0.7331539649483697,0.8590888738245122,True
146,"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while 'e' in tokens[i + 1:]: i = tokens.index('e', i + 1) s = i - 1 e = i + 1 if not re.match('[0-9]', str(tokens[s])): continue if re.match('[+-]', str(tokens[e])): e += 1 <IF_STMT> e += 1 tokens[s:e] = [''.join(tokens[s:e])] i -= 1 return tokens","if re.match('[0-9]', str(tokens[e])):","elif re.match('[0-9]', str(tokens[e])):",0.9491438384057835,0.9196822664155297,False
147,"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) if value is not result: <IF_STMT> self[key] = result if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple): result.parent = self result.key = key return result",if replace:,if replace:,0.8997848669943025,0.8827916928185874,True
148,"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: <IF_STMT> if text.strip().lower() == 'none': text = None bp.cond = text break self.RespondDebuggerData()",if id(bp) == item_id:,if bp.id == item_id:,0.9122964742751649,0.8752376177722327,False
149,"def add(self, url: str, future_nzo: NzbObject, when: Optional[int]=None): """"""Add an URL to the URLGrabber queue, 'when' is seconds from now"""""" if future_nzo and when: future_nzo.url_tries += 1 <IF_STMT> self.fail_to_history(future_nzo, url, T('Maximum retries')) return future_nzo.url_wait = time.time() + when self.queue.put((url, future_nzo))",if future_nzo.url_tries > cfg.max_url_retries():,if future_nzo.url_tries >= self.max_retries:,0.9126198716708159,0.8661072626070159,False
150,def _is_datetime_string(series): if series.dtype == object: not_numeric = False try: pd.to_numeric(series) except Exception as e: not_numeric = True datetime_col = None <IF_STMT> try: datetime_col = pd.to_datetime(series) except Exception as e: return False if datetime_col is not None: return True return False,if not_numeric:,if not_numeric:,0.8303133610104703,0.9122561819614461,True
151,"def _getEventAndObservers(self, event): if isinstance(event, xpath.XPathQuery): observers = self._xpathObservers el<IF_STMT> observers = self._eventObservers else: event = xpath.internQuery(event) observers = self._xpathObservers return (event, observers)",if self.prefix == event[:len(self.prefix)]:,if event == xpath.event:,0.6125262041411419,0.7709002428237395,False
152,"def test_wildcard_import(): bonobo = __import__('bonobo') assert bonobo.__version__ for name in dir(bonobo): if name.startswith('_'): continue attr = getattr(bonobo, name) <IF_STMT> continue assert name in bonobo.__all__",if inspect.ismodule(attr):,"if not hasattr(attr, '__all__'):",0.7660041563319006,0.7801270245332924,False
153,"def relint_views(wid=None): windows = [sublime.Window(wid)] if wid else sublime.windows() for window in windows: for view in window.views(): <IF_STMT> hit(view, 'relint_views')",if view.buffer_id() in persist.assigned_linters and view.is_primary():,if view.is_visible():,0.8702403623800045,0.8289657839357887,False
154,def _check_for_unknown_gender(self): if self.obj.get_gender() == Person.UNKNOWN: d = GenderDialog(parent=self.window) gender = d.run() d.destroy() <IF_STMT> self.obj.set_gender(gender),if gender >= 0:,if gender:,0.7598967434563241,0.8065008590125561,False
155,"def add_to_path(self, fnames): """"""Add fnames to path"""""" indexes = [] for path in fnames: project = self.get_source_project(path) <IF_STMT> self.parent_widget.emit(SIGNAL('pythonpath_changed()')) indexes.append(self.get_index(path)) if indexes: self.reset_icon_provider() for index in indexes: self.update(index)",if project.add_to_pythonpath(path):,if project:,0.7908564692547323,0.8696398662122882,False
156,"def validate(self, value): if value.grid_id is not None: if not isinstance(value, self.proxy_class): self.error('FileField only accepts GridFSProxy values') <IF_STMT> self.error('Invalid GridFSProxy value')","if not isinstance(value.grid_id, ObjectId):",if value.grid_id not in self.proxy_class.grid_ids:,0.5743868330235244,0.7178970818142898,False
157,"def shortcut(self, input, ch_out, stride, name, if_first=False): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <IF_STMT> return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) else: return input",if if_first:,if if_first:,0.747391773844185,0.8966773400768917,True
158,"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) <IF_STMT> ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",elif code == Path.CURVE4:,elif code == Path.CURVE4:,0.6694548608926989,0.8474968231198384,True
159,"def _get_build_status(self, job_name, build_number): try: build_info = self.server.get_build_info(job_name, build_number) <IF_STMT> return 'building' else: return 'built' except jenkins.NotFoundException: return 'not found'",if build_info['building']:,if build_info.status == 'built':,0.5461626126565216,0.7378351342269067,False
160,"def _parse_param_value(name, datatype, default): if datatype == 'bool': if default.lower() == 'true': return True elif default.lower() == 'false': return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': <IF_STMT> return default else: return float(default) else: return str(default)",if type(default) == float:,if type(default) == float:,0.9495398387194216,0.914208565914368,True
161,"def get_fills(self, exchange_order_id): async with aiohttp.ClientSession() as client: response: aiohttp.ClientResponse = await client.get(f'{BASE_URL}{FILLS_ROUTE}', params={'orderId': exchange_order_id, 'limit': 100}) <IF_STMT> try: msg = await response.json() except ValueError: msg = await response.text() raise DydxAsyncAPIError(response.status, msg) return await response.json()",if response.status >= 300:,if response.status != 200:,0.6996346261479736,0.8474968231198384,False
162,"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() for key, value in list(semanticTags.items()): if not isinstance(key, int): raise TypeError('At least one key is not a valid int position') if not isinstance(value, list): raise TypeError('At least one value of the provided dict is not a list of string') for x in value: <IF_STMT> raise TypeError('At least one value of the provided dict is not a list of string') self.__semanticTags = semanticTags","if not isinstance(x, str):","if not isinstance(x, str):",0.8863105733615344,0.924776563154472,True
163,"def start_cutting_tool(self, event, axis, direction): toggle = event.EventObject self.cutting = toggle.Value if toggle.Value: for child in self.cutsizer.Children: child = child.Window <IF_STMT> child.Value = False self.cutting_axis = axis self.cutting_direction = direction else: self.cutting_axis = None self.cutting_direction = None self.cutting_dist = None",if child != toggle:,if child.Value:,0.5899520976452967,0.9099951253570094,False
164,"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) if patching.attribute_name is not None: keywargs.update(arg) <IF_STMT> extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",elif patching.new is DEFAULT:,elif patching.attribute_name == 'extra':,0.8941228402394865,0.8385130047130208,False
165,def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit if attrs & mask: <IF_STMT> names.append(attrnames[mask]) else: names.append(hex(mask)) return names,if attrnames.has_key(mask):,if mask in attrnames:,0.7227683309722484,0.7801270245332924,False
166,def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/params'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:,if 'init' not in item.keywords:,0.6137372760254911,0.6859238121837058,True
167,"def handle_socket(self, request): conn = request.connection while True: chunk = conn.recv(4) <IF_STMT> break slen = struct.unpack('>L', chunk)[0] chunk = conn.recv(slen) while len(chunk) < slen: chunk = chunk + conn.recv(slen - len(chunk)) obj = pickle.loads(chunk) record = logging.makeLogRecord(obj) self.log_output += record.msg + '\n' self.handled.release()",if len(chunk) < 4:,if not chunk:,0.6752640678155128,0.8953711787948615,False
168,"def on_source_foreach(self, model, path, iter, id): m_id = model.get_value(iter, self.COLUMN_ID) if m_id == id: if self._foreach_mode == 'get': self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED) <IF_STMT> self._foreach_take = iter",elif self._foreach_mode == 'set':,elif self._foreach_mode == 'set':,0.8757655021911628,0.7965020533851944,True
169,"def parts(): for l in lists.leaves: head_name = l.get_head_name() if head_name == 'System`List': yield l.leaves <IF_STMT> raise MessageException('Catenate', 'invrp', l)",elif head_name != 'System`Missing':,elif head_name == 'System`Error':,0.8528359205467702,0.7378351342269067,False
170,"def __fill_counter_values(self, command: str): result = [] regex = '(item[0-9]+\\.counter_value)' for token in re.split(regex, command): <IF_STMT> try: result.append(str(self.simulator_config.item_dict[token].value)) except (KeyError, ValueError, AttributeError): logger.error('Could not get counter value for ' + token) else: result.append(token) return ''.join(result)","if re.match(regex, token) is not None:",if token in self.simulator_config.item_dict:,0.9107064371852588,0.8474968231198384,False
171,"def IMPORTFROM(self, node): <IF_STMT> if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == '*': self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if node.module == '__future__':,if node.module == '__future__':,0.9409811714995081,0.8964173245779284,True
172,"def _split_batch_list(args, batch_list): new_list = [] for batch in batch_list.batches: new_list.append(batch) <IF_STMT> yield batch_pb2.BatchList(batches=new_list) new_list = [] if new_list: yield batch_pb2.BatchList(batches=new_list)",if len(new_list) == args.batch_size_limit:,if len(new_list) == 0:,0.8621960381209814,0.7498810286408993,False
173,"def get_branch_or_use_upstream(branch_name, arg, repo): if not branch_name: current_b = repo.current_branch upstream_b = current_b.upstream <IF_STMT> raise ValueError('No {0} branch specified and the current branch has no upstream branch set'.format(arg)) ret = current_b.upstream else: ret = get_branch(branch_name, repo) return ret",if not upstream_b:,if upstream_b is None:,0.6795563119182545,0.8592377270804451,False
174,"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): <IF_STMT> setattr(self, name, value) for name, value in settings.items(): if name not in default_settings: raise ImproperlyConfigured(""Invalid setting '{}' for {}"".format(name, self.__class__.__name__)) setattr(self, name, value)","if not hasattr(self, name):",if name not in settings:,0.8379308429843493,0.8248765135255685,False
175,"def _declare(self, name, obj, included=False, quals=0): if name in self._declarations: prevobj, prevquals = self._declarations[name] if prevobj is obj and prevquals == quals: return <IF_STMT> raise api.FFIError('multiple declarations of %s (for interactive usage, try cdef(xx, override=True))' % (name,)) assert '__dotdotdot__' not in name.split() self._declarations[name] = (obj, quals) if included: self._included_declarations.add(obj)",if not self._override:,if len(self._declarations) > 1:,0.6799408317435953,0.8902056737869248,False
176,"def include_file(name, fdir=tmp_dir, b64=False): try: if fdir is None: fdir = '' <IF_STMT> with io.open(os.path.join(fdir, name), 'rb') as f: return base64.b64encode(f.read()).decode('utf-8') else: with io.open(os.path.join(fdir, name), 'r', encoding='utf-8') as f: return f.read() except (OSError, IOError) as e: logger.error(""Could not include file '{}': {}"".format(name, e))",if b64:,if b64:,0.6770987568350469,0.9164531641034833,True
177,"def to_raw_json(self): parts = {} for p in self.parts: <IF_STMT> parts[p[0]] = [] parts[p[0]].append({'value': p[2], 'parameters': p[1]}) children = [x.to_raw_json() for x in self.children] return {'type': self.__class__.__name__, 'children': children, 'parts': parts}",if p[0] not in parts:,if p[0] not in parts:,0.8954431256052295,0.8036431532733102,True
178,"def process_output(output: str, filename: str, start_line: int) -> Tuple[Optional[str], bool]: error_found = False for line in output.splitlines(): t = get_revealed_type(line, filename, start_line) <IF_STMT> return (t, error_found) elif 'error:' in line: error_found = True return (None, True)",if t:,if t:,0.8262071209792449,0.8996480074924822,True
179,"def __init__(self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3): if row_width > self.max_row_keys: <IF_STMT> logger.error('Telegram does not support reply keyboard row width over %d.' % self.max_row_keys) row_width = self.max_row_keys self.resize_keyboard = resize_keyboard self.one_time_keyboard = one_time_keyboard self.selective = selective self.row_width = row_width self.keyboard = []",if not DISABLE_KEYLEN_ERROR:,if self.max_row_keys > 1:,0.9106802758942638,0.8692960007731574,False
180,"def realizeElementExpressions(innerElement): elementHasBeenRealized = False for exp in innerElement.expressions: if not hasattr(exp, 'realize'): continue before, during, after = exp.realize(innerElement) elementHasBeenRealized = True for n in before: newStream.append(n) <IF_STMT> newStream.append(during) for n in after: newStream.append(n) if elementHasBeenRealized is False: newStream.append(innerElement)",if during is not None:,if during is not None:,0.8726796824771741,0.8419539711731491,True
181,"def lex_number(self, pos): start = pos found_dot = False while pos < len(self.string) and (self.string[pos].isdigit() or self.string[pos] == '.'): <IF_STMT> if found_dot is True: raise ValueError(""Invalid number. Found multiple '.'"") found_dot = True pos += 1 val = self.string[start:pos] return Token(TokenType.LNUM, val, len(val))",if self.string[pos] == '.':,if self.string[pos].isdigit():,0.8603683219662924,0.9164531641034833,False
182,"def rename(src, dst): if _rename(src, dst): return try: os.rename(src, dst) except OSError as e: <IF_STMT> raise old = '%s-%08x' % (dst, random.randint(0, sys.maxsize)) os.rename(dst, old) os.rename(src, dst) try: os.unlink(old) except Exception: pass",if e.errno != errno.EEXIST:,if e.errno != errno.EEXIST:,0.9041568591926268,0.8336104423443033,True
183,"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, '__call__'): if num > 1: args = [point, index] else: args = [point] <IF_STMT> args.append(widget) try_callback(callback, *args) return",if pass_widget:,if widget:,0.9090833122190728,0.8743414417652072,False
184,def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({'test': 'insert'}) error = False except: <IF_STMT> raise if self.expect_exception: assert error,if not self.expect_exception:,if self.expect_exception:,0.8442722055524333,0.8318180062062374,False
185,"def handle(self, *args: Any, **options: Any) -> None: realm = self.get_realm(options) if options['all']: <IF_STMT> raise CommandError('You must specify a realm if you choose the --all option.') self.fix_all_users(realm) return self.fix_emails(realm, options['emails'])",if realm is None:,if not realm:,0.8989778237305389,0.8498644646741501,False
186,"def recv_tdi(self, nbits, pos): bits = 0 for n in range(nbits * 2): yield from self._wait_for_tck() <IF_STMT> bits = bits << 1 | (yield self.tdi.o) return bits",if (yield self.tck.o) == pos:,if self.tdi.o != 0:,0.6789669261645259,0.8038019482772603,False
187,"def _split_head(self): if not hasattr(self, '_severed_head'): <IF_STMT> tree = self._tree.copy() head = tree.get_heading_text() tree.remove_heading() self._severed_head = (head, tree) else: self._severed_head = (None, None) return self._severed_head",if self._tree:,if self._tree:,0.8253213593726577,0.8531413606256201,True
188,"def buildSearchTrie(self, choices): searchtrie = trie.Trie() for choice in choices: for token in self.tokenizeChoice(choice): <IF_STMT> searchtrie[token] = [] searchtrie[token].append(choice) return searchtrie",if not searchtrie.has_key(token):,if token not in searchtrie:,0.7417479441346356,0.7178970818142898,False
189,"def format_sql(sql, params): rv = [] if isinstance(params, dict): conv = _FormatConverter(params) if params: sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): <IF_STMT> rv.append('NULL') param = safe_repr(param) rv.append(param) return (sql, rv)",if param is None:,if param is None:,0.8672614287414779,0.8780099567239787,True
190,def on_completed2(): doner[0] = True if not qr: if len(ql) > 0: observer.on_next(False) observer.on_completed() <IF_STMT> observer.on_next(True) observer.on_completed(),elif donel[0]:,elif len(ql) == 0:,0.7588526900244627,0.7101158913184162,False
191,"def notify_digest(self, frequency, changes): notifications = defaultdict(list) users = {} for change in changes: for user in self.get_users(frequency, change): <IF_STMT> notifications[user.pk].append(change) users[user.pk] = user for user in users.values(): self.send_digest(user.profile.language, user.email, notifications[user.pk], subscription=user.current_subscription)",if change.project is None or user.can_access_project(change.project):,if user.profile.language == change.profile.language:,0.7565474951059741,0.8336104423443033,False
192,"def _any_listener_using(self, target_group_arn): for load_balancer in self.load_balancers.values(): for listener in load_balancer.listeners.values(): for rule in listener.rules: for action in rule.actions: <IF_STMT> return True return False",if action.data.get('target_group_arn') == target_group_arn:,if action.target_group_arn == target_group_arn:,0.624004684366553,0.7801270245332924,False
193,"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) for p, _ in ctr.most_common(): w, pos, l = p if (w, pos) not in self.composite_dict: self.composite_dict[w, pos] = l <IF_STMT> self.word_dict[w] = l return",if w not in self.word_dict:,"elif (w, pos) not in self.word_dict:",0.8117137832922432,0.8534652100396689,False
194,"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, 'config'), 'r') as f: for line in f: line = line.strip() <IF_STMT> section = line[1:-1].strip() config[section] = dict() elif section: key, value = line.replace(' ', '').split('=') config[section][key] = value return config",if line.startswith('['):,if line.startswith('#'):,0.7552486917705942,0.9202663016973823,False
195,"def send_signal(self, pid, signum): if pid in self.processes: process = self.processes[pid] hook_result = self.call_hook('before_signal', pid=pid, signum=signum) <IF_STMT> logger.debug(""before_signal hook didn't return True => signal %i is not sent to %i"" % (signum, pid)) else: process.send_signal(signum) self.call_hook('after_signal', pid=pid, signum=signum) else: logger.debug('process %s does not exist' % pid)",if signum != signal.SIGKILL and (not hook_result):,if hook_result is False:,0.8455162312601952,0.8832000938217648,False
196,"def validate_pos_return(self): if self.is_pos and self.is_return: total_amount_in_payments = 0 for payment in self.payments: total_amount_in_payments += payment.amount invoice_total = self.rounded_total or self.grand_total <IF_STMT> frappe.throw(_(""Total payments amount can't be greater than {}"").format(-invoice_total))",if total_amount_in_payments < invoice_total:,if total_amount_in_payments > invoice_total:,0.7669569175949008,0.8228500218338367,False
197,"def delete(key, inner_key=None): if inner_key is not None: try: del cache[key][inner_key] del use_count[key][inner_key] <IF_STMT> del cache[key] del use_count[key] wrapper.cache_size -= 1 except KeyError: return False else: return True else: try: wrapper.cache_size -= len(cache[key]) del cache[key] del use_count[key] except KeyError: return False else: return True",if not cache[key]:,if len(cache[key]) == 0:,0.9299625039002579,0.8780099567239787,False
198,"def insertionsort(array): size = array.getsize() array.reset('Insertion sort') for i in range(1, size): j = i - 1 while j >= 0: <IF_STMT> break array.swap(j, j + 1) j = j - 1 array.message('Sorted')","if array.compare(j, j + 1) <= 0:",if array.get(j) == array.get(j + 1):,0.7965826726393452,0.7925226565645306,False
199,"def publish_state(cls, payload, state): try: if isinstance(payload, LiveActionDB): <IF_STMT> cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)",if state == action_constants.LIVEACTION_STATUS_REQUESTED:,if state == 'running':,0.589293572287513,0.674945488826271,False
200,"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: if type(submob) == AnnularSector: sectors.append(submob) for r, submob in zip(np.arange(0, self.radius, dr), sectors): <IF_STMT> continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",if type(submob) != AnnularSector:,if r == 0:,0.8976322159585106,0.8555308664663046,False
201,"def is_suppressed_warning(type: str, subtype: str, suppress_warnings: List[str]) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: <IF_STMT> target, subtarget = warning_type.split('.', 1) else: target, subtarget = (warning_type, None) if target == type: if subtype is None or subtarget is None or subtarget == subtype or (subtarget == '*'): return True return False",if '.' in warning_type:,if '.' in warning_type:,0.9326036319128439,0.9128479730518225,True
202,"def set_many(self, mapping, timeout=None): timeout = self._normalize_timeout(timeout) pipe = self._client.pipeline(transaction=False) for key, value in _items(mapping): dump = self.dump_object(value) <IF_STMT> pipe.set(name=self.key_prefix + key, value=dump) else: pipe.setex(name=self.key_prefix + key, value=dump, time=timeout) return pipe.execute()",if timeout == -1:,if timeout is None:,0.8847663279295848,0.828399516355805,False
203,"def maybe_relative_path(path): if not os.path.isabs(path): return path dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) if dir == prevdir or not dir: return path names.append(name) try: <IF_STMT> names.reverse() return os.path.join(*names) except OSError: pass","if samefile(dir, os.curdir):",if os.path.isdir(names):,0.7042829552974734,0.9099951253570094,False
204,"def word_range(word): for ind in range(len(word)): temp = word[ind] for c in [chr(x) for x in range(ord('a'), ord('z') + 1)]: <IF_STMT> yield (word[:ind] + c + word[ind + 1:])",if c != temp:,if c == temp:,0.8263071451947136,0.8169276475307028,False
205,def validate(self): self.update_soil_edit('sand_composition') for soil_type in self.soil_types: <IF_STMT> frappe.throw(_('{0} should be a value between 0 and 100').format(soil_type)) if sum((self.get(soil_type) for soil_type in self.soil_types)) != 100: frappe.throw(_('Soil compositions do not add up to 100')),if self.get(soil_type) > 100 or self.get(soil_type) < 0:,"if not isinstance(soil_type, int):",0.8412102506247938,0.8385130047130208,False
206,"def on_click(self, event): run = self._is_running() if event['button'] == self.button_activate: self.py3.command_run(['xscreensaver-command', '-activate']) if event['button'] == self.button_toggle: <IF_STMT> self.py3.command_run(['xscreensaver-command', '-exit']) else: Popen(['xscreensaver', '-no-splash', '-no-capture-stderr'], stdout=PIPE, stderr=PIPE, preexec_fn=setpgrp)",if run:,if run:,0.9157754409665515,0.8590888738245122,True
207,"def maybe_relative_path(path): if not os.path.isabs(path): return path dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) <IF_STMT> return path names.append(name) try: if samefile(dir, os.curdir): names.reverse() return os.path.join(*names) except OSError: pass",if dir == prevdir or not dir:,if not os.path.isabs(name):,0.7341752015516542,0.8749766281017177,False
208,"def _format_micros(self, datestring): parts = datestring[:-1].split('.') if len(parts) == 1: <IF_STMT> return datestring[:-1] + '.000000Z' else: return datestring + '.000000Z' else: micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1] return '.'.join(parts[:-1] + ['{:06d}'.format(int(micros))]) + 'Z'",if datestring.endswith('Z'):,if len(parts) > 1:,0.7857498197970768,0.8516228624291206,False
209,"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] <IF_STMT> if line[0] == ' ': line = line[1:] f2.writelines(line + '\n')",if line != ' ' and line != '':,if line:,0.9227056847290859,0.9024521756077707,False
210,"def set(self, item, data): if not type(item) is slice: item = slice(item, item + len(data), None) virt_item = self.item2virtitem(item) if not virt_item: return off = 0 for s, n_item in virt_item: <IF_STMT> i = slice(off, n_item.stop + off - n_item.start, n_item.step) data_slice = data.__getitem__(i) s.content.__setitem__(n_item, data_slice) off = i.stop else: raise ValueError('TODO XXX') return","if isinstance(s, ProgBits):",if n_item.start < off:,0.8157221324083159,0.9001816649635144,False
211,"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): if part.get_content_maintype() == 'multipart': continue ctype = part.get_content_type() if ctype is None: ctype = OCTET_TYPE filename = part.get_filename() <IF_STMT> filename = PART_FN_TPL % partnum headers = dict(part) LOG.debug(headers) headers['Content-Type'] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",if not filename:,if filename is None:,0.7307094161350549,0.9001816649635144,False
212,"def _run_wes(args): """"""Run CWL using a Workflow Execution Service (WES) endpoint"""""" main_file, json_file, project_name = _get_main_and_json(args.directory) main_file = _pack_cwl(main_file) if args.host and 'stratus' in args.host: _run_wes_stratus(args, main_file, json_file) else: opts = ['--no-wait'] <IF_STMT> opts += ['--host', args.host] if args.auth: opts += ['--auth', args.auth] cmd = ['wes-client'] + opts + [main_file, json_file] _run_tool(cmd)",if args.host:,if args.host:,0.7801339018270731,0.9312457603037672,True
213,"def insertTestData(self, rows): for row in rows: if isinstance(row, Worker): self.workers[row.id] = dict(id=row.id, name=row.name, paused=0, graceful=0, info=row.info) <IF_STMT> row.id = row.buildermasterid * 10000 + row.workerid self.configured[row.id] = dict(buildermasterid=row.buildermasterid, workerid=row.workerid) elif isinstance(row, ConnectedWorker): self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)","elif isinstance(row, ConfiguredWorker):","elif isinstance(row, Configuration):",0.7048672558645303,0.8749766281017177,False
214,"def local_shape_to_shape_i(node): if node.op == T.shape: <IF_STMT> return shape_feature = node.fgraph.shape_feature ret = shape_feature.make_vector_shape(node.inputs[0]) copy_stack_trace(node.outputs[0], ret) return [ret]","if not hasattr(node.fgraph, 'shape_feature'):",if node.inputs[0] == node.outputs[0]:,0.8044711492039275,0.7098232254187811,False
215,"def get_config(): """"""Get INI parser with version.ini data."""""" ini_path = os.path.join(THIS_DIRECTORY, 'version.ini') <IF_STMT> ini_path = os.path.join(THIS_DIRECTORY, '../../version.ini') if not os.path.exists(ini_path): raise RuntimeError(""Couldn't find version.ini"") config = configparser.ConfigParser() config.read(ini_path) return config",if not os.path.exists(ini_path):,if not os.path.exists(ini_path):,0.895721764111362,0.8498644646741501,True
216,"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): <IF_STMT> kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) else: raise TypeError('pretrained must be a str or None')","if isinstance(m, nn.Conv2d):","if isinstance(m, nn.Conv2d):",0.9300500473238971,0.8815741981066073,True
217,"def isValidDateString(config_param_name, value, valid_value): try: <IF_STMT> return value day, month, year = value.split('-') if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if value == 'DD-MM-YYYY':,if valid_value:,0.9117409022911827,0.9325718821645923,False
218,"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError('py_obj must be a wandb.Image') else: <IF_STMT> box_keys = list(py_obj._boxes.keys()) else: box_keys = [] if hasattr(py_obj, 'masks') and py_obj.masks: mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)","if hasattr(py_obj, '_boxes') and py_obj._boxes:","if hasattr(py_obj, 'boxes') and py_obj._boxes:",0.9188075522967871,0.8338542560892604,False
219,"def _path_type(st, lst): parts = [] if st: if stat.S_ISREG(st.st_mode): parts.append('file') <IF_STMT> parts.append('dir') else: parts.append('other') if lst: if stat.S_ISLNK(lst.st_mode): parts.append('link') return ' '.join(parts)",elif stat.S_ISDIR(st.st_mode):,elif stat.S_ISDIR(st.st_mode):,0.9111093637532588,0.839587623092576,True
220,"def is_destructive(queries): """"""Returns if any of the queries in *queries* is destructive."""""" keywords = ('drop', 'shutdown', 'delete', 'truncate', 'alter') for query in sqlparse.split(queries): if query: <IF_STMT> return True elif query_starts_with(query, ['update']) is True and (not query_has_where_clause(query)): return True return False","if query_starts_with(query, keywords) is True:",if query in keywords:,0.8770123706889565,0.8661072626070159,False
221,"def _store_gsuite_membership_post(self): """"""Flush storing gsuite memberships."""""" if not self.member_cache: return self.session.flush() if self.membership_items: <IF_STMT> for item in self.membership_items: stmt = self.dao.TBL_MEMBERSHIP.insert(item) self.session.execute(stmt) else: stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items) self.session.execute(stmt)",if get_sql_dialect(self.session) == 'sqlite':,"if isinstance(self.membership_items, list):",0.8866989083875617,0.833078701050083,False
222,"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): <IF_STMT> block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) if i < 2: route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",if i > 0:,if i < 1:,0.916174616314197,0.8780099567239787,False
223,"def deep_dict(self, root=None): if root is None: root = self result = {} for key, value in root.items(): <IF_STMT> result[key] = self.deep_dict(root=self.__class__._get_next(key, root)) else: result[key] = value return result","if isinstance(value, dict):","if isinstance(value, dict):",0.8651438857409953,0.8446593249975184,True
224,"def _parse_param_list(self, content): r = Reader(content) params = [] while not r.eof(): header = r.read().strip() <IF_STMT> arg_name, arg_type = header.split(' : ')[:2] else: arg_name, arg_type = (header, '') desc = r.read_to_next_unindented_line() desc = dedent_lines(desc) params.append((arg_name, arg_type, desc)) return params",if ' : ' in header:,if ':' in header:,0.848930612798079,0.8627586293513119,False
225,"def _ungroup(sequence, groups=None): for v in sequence: <IF_STMT> if groups is not None: groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v","if isinstance(v, (list, tuple)):","if isinstance(v, list):",0.8786982748670554,0.8196189957582152,False
226,"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if 'resourcegroup' not in [x.lower() for x in obj.keys()]: if obj['id']: obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: <IF_STMT> _add_resource_group(obj[item_key])",if item_key != 'sourceVault':,if item_key in obj:,0.7238606638471577,0.8823051786911775,False
227,"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: <IF_STMT> ret = fvalue.haslayer(cls) if ret: return ret return self.payload.haslayer(cls)","if isinstance(fvalue, Packet):",if fvalue.islayer:,0.9550509863658306,0.9443716053164669,False
228,"def _post_attachment(self, message, channel, color, sub_fields=None): if channel is None: message_channels = self.channels else: message_channels = [channel] for message_channel in message_channels: attachment = {'fallback': message, 'text': message, 'color': color} <IF_STMT> attachment['fields'] = sub_fields self.slack_client.api_call('chat.postMessage', channel=message_channel, attachments=[attachment], as_user=True)",if sub_fields is not None:,if sub_fields:,0.7955163893174731,0.9024521756077707,False
229,"def create(cls, repository, args): key = cls() passphrase = os.environ.get('ATTIC_PASSPHRASE') if passphrase is not None: passphrase2 = passphrase else: passphrase, passphrase2 = (1, 2) while passphrase != passphrase2: passphrase = getpass('Enter passphrase: ') <IF_STMT> print('Passphrase must not be blank') continue passphrase2 = getpass('Enter same passphrase again: ') if passphrase != passphrase2: print('Passphrases do not match') key.init(repository, passphrase) if passphrase: print('Remember your passphrase. Your data will be inaccessible without it.') return key",if not passphrase:,if not passphrase:,0.9578525338245867,0.9366552662490459,True
230,"def _generate_create_date(self): if self.timezone is not None: tzinfo = tz.gettz(self.timezone) <IF_STMT> tzinfo = tz.gettz(self.timezone.upper()) if tzinfo is None: raise util.CommandError(""Can't locate timezone: %s"" % self.timezone) create_date = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo) else: create_date = datetime.datetime.now() return create_date",if tzinfo is None:,if tzinfo is None:,0.6444733431437952,0.8431339019329497,True
231,"def _read_header_lines(fp): """"""Read lines with headers until the start of body"""""" lines = deque() for line in fp: if is_empty(line): break <IF_STMT> fp.seek(fp.tell() - len(line)) break lines.append(line) return lines",if not _RE_HEADER.match(line):,if fp.tell() > len(line):,0.8741401998212465,0.8169276475307028,False
232,"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple <IF_STMT> if utils.is_media_file(uri) == True: files.append(uri) if len(files) == 0: return open_dropped_files(files)",if os.path.exists(uri) == True:,if uri:,0.9360150173192359,0.9099951253570094,False
233,"def remove_importlib(frame, options): if frame is None: return None for child in frame.children: remove_importlib(child, options=options) <IF_STMT> frame.self_time += child.self_time frame.add_children(child.children, after=child) child.remove_from_parent() return frame",if '<frozen importlib._bootstrap' in child.file_path:,if child.self_time:,0.8591237347330565,0.8466657105524215,False
234,"def __call__(self, graph): for layer_name, data in self.params: <IF_STMT> node = graph.get_node(layer_name) node.data = self.adjust_parameters(node, data) else: print_stderr('Ignoring parameters for non-existent layer: %s' % layer_name) return graph",if layer_name in graph:,if layer_name in graph.get_layers():,0.8343420666154934,0.8038019482772603,False
235,"def test_with_three_points(self): cba = ia.Polygon([(1, 2), (3, 4), (5, 5)]) for i, xy in enumerate(cba): assert i in [0, 1, 2] if i == 0: assert np.allclose(xy, (1, 2)) <IF_STMT> assert np.allclose(xy, (3, 4)) elif i == 2: assert np.allclose(xy, (5, 5)) assert i == 2",elif i == 1:,elif i == 1:,0.9071321165466102,0.8832000938217648,True
236,"def _serve(self): self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain) conn = MsgPackMessages(self._conn) while self.active: request = conn.recv() if not request: logger.warning('DNS: Recieved empty request. Shutdown') self.stop() break now = time.time() response = self.handler.process(request) if not response: response = [] used = time.time() - now <IF_STMT> logger.warning('DNS: Slow processing speed (%s)s', used) conn.send(response)",if used > 1:,if used > self.timeout:,0.9380614209925602,0.8902056737869248,False
237,"def read(cls, fp, **kwargs): major_version, minor_version, count = read_fmt('2HI', fp) items = [] for _ in range(count): length = read_fmt('I', fp)[0] - 4 <IF_STMT> with io.BytesIO(fp.read(length)) as f: items.append(Annotation.read(f)) return cls(major_version=major_version, minor_version=minor_version, items=items)",if length > 0:,if length > 0:,0.7605325364383206,0.8385130047130208,True
238,"def save_uploaded_files(): files = [] unzip = bool(request.form.get('unzip') in ['true', 'on']) for uploaded_file in request.files.getlist('files'): <IF_STMT> with zipfile.ZipFile(uploaded_file, 'r') as zf: for info in zf.infolist(): name = info.filename size = info.file_size data = zf.read(name) if size > 0: files.append(save_file(data, filename=name.split('/')[-1])) else: files.append(save_file(uploaded_file)) return files",if unzip and zipfile.is_zipfile(uploaded_file):,if unzip:,0.8551371443625915,0.9184043388013005,False
239,"def analyze_string_content(self, string, line_num, filename): output = {} if self.keyword_exclude and self.keyword_exclude.search(string): return output for identifier in self.secret_generator(string, filetype=determine_file_type(filename)): <IF_STMT> continue secret = PotentialSecret(self.secret_type, filename, identifier, line_num) output[secret] = secret return output",if self.is_secret_false_positive(identifier):,if identifier is None:,0.8882477682835359,0.8336104423443033,False
240,"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition['spec'] if 'DefaultValue' in spec: self.hyperparam_dict[name] = spec['DefaultValue'] <IF_STMT> raise ValueError('Required hyperparameter: %s is not set' % name)",elif 'IsRequired' in spec and spec['IsRequired']:,if name not in self.hyperparam_dict:,0.7056878141602032,0.8200123297196334,False
241,"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() <IF_STMT> self.code = self._get_delegate().get_code() return self.code",elif mod_type == imp.PKG_DIRECTORY:,elif mod_type == imp.PY_DELEGATE:,0.7495005346553069,0.8723360571509826,False
242,"def eigh_abstract_eval(operand, lower): if isinstance(operand, ShapedArray): <IF_STMT> raise ValueError('Argument to symmetric eigendecomposition must have shape [..., n, n],got shape {}'.format(operand.shape)) batch_dims = operand.shape[:-2] n = operand.shape[-1] v = ShapedArray(batch_dims + (n, n), operand.dtype) w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype)) else: v, w = (operand, operand) return (v, w)",if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:,if lower:,0.9406349958881933,0.9253742688467129,False
243,"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) <IF_STMT> return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if not param_match:,if param_match is None:,0.7664227651906019,0.9001816649635144,False
244,"def load_weights_from_unsupervised(self, unsupervised_model): update_state_dict = copy.deepcopy(self.network.state_dict()) for param, weights in unsupervised_model.network.state_dict().items(): if param.startswith('encoder'): new_param = 'tabnet.' + param else: new_param = param <IF_STMT> update_state_dict[new_param] = weights self.network.load_state_dict(update_state_dict)",if self.network.state_dict().get(new_param) is not None:,if weights is not None:,0.7861595918611503,0.7765145040967655,False
245,"def viewer_setup(self): for key, value in DEFAULT_CAMERA_CONFIG.items(): <IF_STMT> getattr(self.viewer.cam, key)[:] = value else: setattr(self.viewer.cam, key, value)","if isinstance(value, np.ndarray):","if isinstance(value, list):",0.7614433583906586,0.7160350546947921,False
246,"def colormap_changed(change): if change['new']: cmap_colors = [color[1:] for color in cmap.step.__dict__['_schemes'][colormap.value]] palette.value = ', '.join(cmap_colors) colorbar = getattr(cmap.step, colormap.value) colorbar_output = self.colorbar_widget with colorbar_output: colorbar_output.clear_output() display(colorbar) <IF_STMT> labels = [f'Class {i + 1}' for i in range(len(palette.value.split(',')))] legend_labels.value = ', '.join(labels)","if len(palette.value) > 0 and ',' in palette.value:",if change['new']:,0.9133780330900437,0.9122561819614461,False
247,"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [br for br in self.blockRenderers if br.layer is Layer.Blocks or br.layer not in layers] <IF_STMT> self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",if len(blockRenderers) < len(self.blockRenderers):,if not blockRenderers:,0.7653002495478327,0.9022045190074797,False
248,"def fromstring(cls, input): productions = [] for linenum, line in enumerate(input.split('\n')): line = line.strip() <IF_STMT> continue try: productions += _read_dependency_production(line) except ValueError: raise ValueError('Unable to parse line %s: %s' % (linenum, line)) if len(productions) == 0: raise ValueError('No productions found!') return DependencyGrammar(productions)",if line.startswith('#') or line == '':,if not line:,0.7273564488052319,0.8928756684056034,False
249,"def repl(m, base_path, rel_path=None): if m.group('comments'): tag = m.group('comments') else: tag = m.group('open') <IF_STMT> tag += RE_TAG_LINK_ATTR.sub(lambda m2: repl_absolute(m2, base_path), m.group('attr')) else: tag += RE_TAG_LINK_ATTR.sub(lambda m2: repl_relative(m2, base_path, rel_path), m.group('attr')) tag += m.group('close') return tag",if rel_path is None:,if rel_path is None:,0.7230533440322495,0.8474968231198384,True
250,"def encode(path): if isinstance(path, str_cls): try: path = path.encode(fs_encoding, 'strict') except UnicodeEncodeError: <IF_STMT> raise path = path.encode(fs_fallback_encoding, 'strict') return path",if not platform.is_linux():,if not fs_fallback_encoding:,0.691496391421171,0.7739321540474097,False
251,"def __iter__(self): base_iterator = super(ProcessIterable, self).__iter__() if getattr(self.queryset, '_coerced', False): for process in base_iterator: <IF_STMT> process = coerce_to_related_instance(process, process.flow_class.process_class) yield process else: for process in base_iterator: yield process","if isinstance(process, self.queryset.model):","if isinstance(process, Process):",0.5728677490807613,0.8390782502060267,False
252,"def footnotes_under(n: Element) -> Iterator[nodes.footnote]: if isinstance(n, nodes.footnote): yield n else: for c in n.children: <IF_STMT> continue elif isinstance(c, nodes.Element): yield from footnotes_under(c)","if isinstance(c, addnodes.start_of_file):","if isinstance(c, nodes.footnote):",0.8054708393509652,0.80377750806414,False
253,"def _process_submissions(self) -> None: """"""Process all submissions which have not been processed yet."""""" while self._to_be_processed: job = self._to_be_processed[0] job.process() <IF_STMT> heapq.heappush(self._steady_priority_queue, OrderedJobs(job.release_time, self._order, job)) self._to_be_processed.popleft() self._order += 1",if not self.batch_mode:,if job.release_time:,0.7321241511805874,0.8696398662122882,False
254,"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) <IF_STMT> continue if strip_delimiters: if ',' in line or ';' in line: continue yield line",if match:,if not match:,0.9180987873725525,0.8592899528284996,False
255,"def _get_payload_hash(self, method, data=None): if method in ('POST', 'PUT'): if data: <IF_STMT> return UNSIGNED_PAYLOAD return _hash(data) else: return UNSIGNED_PAYLOAD else: return _hash('')","if hasattr(data, 'next') or hasattr(data, '__next__'):","if data.get('payload', None) is None:",0.8626110963123585,0.7297349727547102,False
256,"def get_download_info(self): try: download_info = self.api.get_download_info(self.game) result = True except NoDownloadLinkFound as e: print(e) <IF_STMT> Config.unset('current_download') GLib.idle_add(self.parent.parent.show_error, _('Download error'), _('There was an error when trying to fetch the download link!\n{}'.format(e))) download_info = False result = False return (result, download_info)",if Config.get('current_download') == self.game.id:,if e.args[0] == 404:,0.7773294826798671,0.8627586293513119,False
257,"def find_id(self, doc_id): self._lock.acquire() try: doc = self._docs.get(doc_id) <IF_STMT> doc = copy.deepcopy(doc) doc['id'] = doc_id return doc finally: self._lock.release()",if doc:,if doc:,0.6862125469710932,0.803154665668484,True
258,"def assign_art(self, session, task): """"""Place the discovered art in the filesystem."""""" if task in self.art_candidates: candidate = self.art_candidates.pop(task) self._set_art(task.album, candidate, not self.src_removed) <IF_STMT> task.prune(candidate.path)",if self.src_removed:,if candidate.path:,0.641492716682948,0.8817739004515716,False
259,"def _replace_named(self, named, replace_scalar): for item in named: for name, value in self._get_replaced_named(item, replace_scalar): <IF_STMT> raise DataError('Argument names must be strings.') yield (name, value)",if not is_string(name):,"if not isinstance(name, str):",0.6211149438285558,0.7801270245332924,False
260,"def qtTypeIdent(conn, *args): res = None value = None for val in args: if not hasattr(val, '__len__'): val = str(val) if len(val) == 0: continue value = val <IF_STMT> value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res","if Driver.needsQuoting(val, True):","if isinstance(value, str):",0.6953741756341253,0.9151329413834155,False
261,"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: <IF_STMT> continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): if any((c.key not in chunk_result for c in tiled_n.chunks)): continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple((sum(nsplit) for nsplit in new_nsplits))) tiled_n._nsplits = new_nsplits",if n.op in failed_ops:,if n.key not in failed_ops:,0.8324620763872492,0.8559898693114286,False
262,"def _read_filter(self, data): if data: <IF_STMT> self.inner_sha.update(data) if self.expected_inner_md5sum: self.inner_md5.update(data) return data",if self.expected_inner_sha256:,if self.expected_inner_shasum:,0.7309213621942857,0.6703420896351792,False
263,"def find_previous_editable(self, *args): if self.editw == 0: if self._active_page > 0: self.switch_page(self._active_page - 1) if not self.editw == 0: for n in range(self.editw - 1, -1, -1): <IF_STMT> self.editw = n break",if self._widgets__[n].editable and (not self._widgets__[n].hidden):,if self.get_editable(n) == 0:,0.9096763397121073,0.8336104423443033,False
264,"def _get_event_for_message(self, message_id): with self.event_lock: <IF_STMT> raise RuntimeError('Event for message[{}] should have been created before accessing'.format(message_id)) return self._events[message_id]",if message_id not in self._events:,if message_id not in self._events:,0.7932803036667682,0.6750915335148621,True
265,"def _get_deepest(self, t): if isinstance(t, list): if len(t) == 1: return t[0] else: for part in t: res = self._get_deepest(part) <IF_STMT> return res return None return None",if res:,if res is not None:,0.8053242066964605,0.7765145040967655,False
266,"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: <IF_STMT> task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify elif self._chain_notify: return self._chain_notify return None",if action_node.notify:,if action_node.notify:,0.7161211138373021,0.8137489370974955,True
267,"def __init__(self, centered=None, shape_params=()): assert centered is None or isinstance(centered, (float, torch.Tensor)) assert isinstance(shape_params, (tuple, list)) assert all((isinstance(name, str) for name in shape_params)) if is_validation_enabled(): if isinstance(centered, float): assert 0 <= centered and centered <= 1 <IF_STMT> assert (0 <= centered).all() assert (centered <= 1).all() else: assert centered is None self.centered = centered self.shape_params = shape_params","elif isinstance(centered, torch.Tensor):","elif isinstance(centered, tuple):",0.888586168731134,0.9196822664155297,False
268,"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host['host'], squid_host['port']) if fulldata is not None: fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) <IF_STMT> self.publish_counter('%s.%s' % (nickname, matches.group(1)), float(matches.group(2)))",if matches:,if matches is not None:,0.790273409860581,0.8200123297196334,False
269,"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <IF_STMT> bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 9:,elif size <= 7:,0.8578714206177079,0.8964173245779284,False
270,"def wait_for_initial_conf(self, timeout=1.0): logger.info('Waiting for initial configuration') cur_timeout = timeout while not self.new_conf and (not self.interrupted): elapsed, _, _ = self.handleRequests(cur_timeout) if elapsed: cur_timeout -= elapsed <IF_STMT> continue cur_timeout = timeout sys.stdout.write('.') sys.stdout.flush()",if cur_timeout > 0:,if cur_timeout <= 0:,0.9062804468825392,0.8385130047130208,False
271,"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: if self.query_id == -1: self.query_id = query.query_id el<IF_STMT> raise ValueError('query in list must be same query_id')",if self.query_id != query.query_id:,if self.query_id != query.query_id:,0.7761329662332931,0.8555308664663046,True
272,"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: if matchSelf: yield s <IF_STMT> yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if recurseInAnon:,elif matchAnon:,0.9424281665959069,0.926934323706186,False
273,"def get_default_params(problem_type: str, penalty: str): if problem_type == REGRESSION: default_params = {'C': None, 'random_state': 0, 'fit_intercept': True} <IF_STMT> default_params['solver'] = 'auto' else: default_params = {'C': None, 'random_state': 0, 'solver': _get_solver(problem_type), 'n_jobs': -1, 'fit_intercept': True} model_params = list(default_params.keys()) return (model_params, default_params)",if penalty == L2:,if penalty == 'auto':,0.7991982995909953,0.8661072626070159,False
274,"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str): """"""Upload the contents of a local directory to a GCS Bucket."""""" for file_name in os.listdir(local_dir): path = os.path.join(local_dir, file_name) <IF_STMT> logging.info(""Skipping %s as it's not a file."", path) continue logging.info('Uploading: %s', path) gcs_blob = gcs_bucket.blob(f'{gcs_dir}/{file_name}') gcs_blob.upload_from_filename(path)",if not os.path.isfile(path):,if not os.path.isfile(path):,0.7403898109915008,0.8953711787948615,True
275,"def decode_query_ids(self, trans, conditional): if conditional.operator == 'and': self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split('.')[0] if left_base in self.FIELDS: field = self.FIELDS[left_base] <IF_STMT> conditional.right = trans.security.decode_id(conditional.right)",if field.id_decode:,if field.operator == 'and':,0.7196178969575786,0.8038019482772603,False
276,"def data_dir(self) -> Path: try: from appdirs import user_data_dir except ImportError: path = Path.home() / '.local' / 'share' <IF_STMT> return path / 'dephell' path = Path.home() / 'Library' / 'Application Support' if path.exists(): return path / 'dephell' self.pip_main(['install', 'appdirs']) from appdirs import user_data_dir return Path(user_data_dir('dephell'))",if path.exists():,if path.exists():,0.931738563984312,0.9202663016973823,True
277,"def setGameCard(self, isGameCard=False): if isGameCard: targetValue = 1 else: targetValue = 0 for nca in self: if isinstance(nca, Nca): <IF_STMT> continue Print.info('writing isGameCard for %s, %d' % (str(nca._path), targetValue)) nca.header.setIsGameCard(targetValue)",if nca.header.getIsGameCard() == targetValue:,if nca.header.isIsGameCard:,0.9073196628684445,0.8787142254774354,False
278,"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' <IF_STMT> if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",elif mode == 'key':,elif mode == 'end':,0.8834718061428868,0.9267806344830257,False
279,"def register_aggregate_groups(conn, *groups): seen = set() for group in groups: klasses = AGGREGATE_COLLECTION[group] for klass in klasses: name = getattr(klass, 'name', klass.__name__) <IF_STMT> seen.add(name) conn.create_aggregate(name, -1, klass)",if name not in seen:,if name not in seen:,0.7875632471772563,0.7765145040967655,True
280,"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: if isinstance(inputs[1], int): axis = int(inputs[1]) <IF_STMT> axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",elif _is_int_seq(inputs[1]):,"elif isinstance(inputs[1], list):",0.6622980742335027,0.8783650674919876,False
281,"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip('\n').split(' ')] bucket.append(walk) if len(bucket) == batch_size: yield bucket bucket = [] <IF_STMT> yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",if len(bucket):,elif epoch == 0:,0.7536039250890296,0.9069443196104878,False
282,"def _calculate_runtimes(states): results = {'runtime': 0.0, 'num_failed_states': 0, 'num_passed_states': 0} for state, resultset in states.items(): <IF_STMT> if resultset['result']: results['num_passed_states'] += 1 else: results['num_failed_states'] += 1 results['runtime'] += resultset['duration'] log.debug('Parsed state metrics: {}'.format(results)) return results","if isinstance(resultset, dict) and 'duration' in resultset:",if resultset['state'] == state:,0.9008730314774978,0.8431339019329497,False
283,"def _replicator_primary_device() -> snt_replicator.Replicator: for device_type in ('TPU', 'GPU', 'CPU'): devices = tf.config.experimental.list_logical_devices(device_type=device_type) <IF_STMT> devices = [d.name for d in devices] logging.info('Replicating over %s', devices) return snt_replicator.Replicator(devices=devices) assert False, 'No TPU/GPU or CPU found'",if devices:,if devices:,0.7348963190827804,0.8935248372106969,True
284,"def get_tag_values(self, event): http = event.interfaces.get('sentry.interfaces.Http') if not http: return [] if not http.headers: return [] headers = http.headers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: <IF_STMT> continue ua = Parse(value) if not ua: continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",if key != 'User-Agent':,if key.startswith('HTTP_'):,0.9262799206465225,0.9284304001296656,False
285,"def general(metadata, value): if metadata.get('commands') and value: <IF_STMT> v = quote(value) else: v = value return u'{0} {1}'.format(metadata['commands'][0], v) elif not value: return None elif not metadata.get('nargs'): return quote(value) else: return value",if not metadata.get('nargs'):,if metadata['commands'][0] == 'default':,0.8845292590939315,0.8336104423443033,False
286,"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: if self.action_input.selected_index == 0: self.back_to_parent() elif self.action_input.selected_index == 1: self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) <IF_STMT> self._apply_prefs() self.back_to_parent()",elif self.action_input.selected_index == 2:,elif self.action_input.selected_index == 2:,0.8771954048201461,0.782673117139774,True
287,def logic(): if reset == 1: lfsr.next = 1 el<IF_STMT> lfsr.next = lfsr << 1 lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16],if enable:,if reset == 2:,0.8105864259622235,0.7801270245332924,False
288,"def action_delete(self, request, attachments): deleted_attachments = [] desynced_posts = [] for attachment in attachments: <IF_STMT> deleted_attachments.append(attachment.pk) desynced_posts.append(attachment.post_id) if desynced_posts: with transaction.atomic(): for post in Post.objects.filter(id__in=desynced_posts): self.delete_from_cache(post, deleted_attachments) for attachment in attachments: attachment.delete() message = _('Selected attachments have been deleted.') messages.success(request, message)",if attachment.post:,if attachment.pk:,0.8596665417894048,0.9122561819614461,False
289,"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) if feature: feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] <IF_STMT> indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]","elif isinstance(index, slice):",if index.is_integer():,0.9531645590037253,0.9253742688467129,False
290,"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) <IF_STMT> continue elif self._pos == start: return b'' else: chunk = chunk[start - self._pos:] if stop is not None and self._pos > stop: chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",if self._pos < start:,if self._pos >= stop:,0.8366867699372075,0.8983343737277126,False
291,"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: <IF_STMT> continue if 'qemux86copy-' in root or 'qemux86-' in root: continue if 'do_build' not in name and 'do_populate_sdk' not in name: f.append(os.path.join(root, name)) return f",if 'meta-environment' in root or 'cross-canadian' in root:,if 'qemu-build-' in root or 'qemu-build-' in root:,0.8863936298182258,0.7899177245850753,False
292,"def _load_windows_store_certs(self, storename, purpose): certs = bytearray() try: for cert, encoding, trust in enum_certificates(storename): <IF_STMT> if trust is True or purpose.oid in trust: certs.extend(cert) except PermissionError: warnings.warn('unable to enumerate Windows certificate store') if certs: self.load_verify_locations(cadata=certs) return certs",if encoding == 'x509_asn':,if cert:,0.9279699742432305,0.9024521756077707,False
293,"def test_tokenizer_identifier_with_correct_config(self): for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]: tokenizer = tokenizer_class.from_pretrained('wietsedv/bert-base-dutch-cased') self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast)) <IF_STMT> self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False) else: self.assertEqual(tokenizer.do_lower_case, False) self.assertEqual(tokenizer.model_max_length, 512)","if isinstance(tokenizer, BertTokenizer):",if tokenizer.basic_tokenizer:,0.8082435211317958,0.8318180062062374,False
294,"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() if path_id is None: continue self.lock.acquire() self.modules[path_id] = module self.lock.release() <IF_STMT> resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",if resolve:,if resolve:,0.7533143026223289,0.9051034981560222,True
295,"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == 'SaltLoggingClass': for base in bases: if hasattr(base, 'trace'): include_trace = False <IF_STMT> include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr(base, 'garbage'):","if hasattr(base, 'garbage'):",0.918868720061478,0.9062841320510342,True
296,"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_owner_: res += prefix + 'owner: %s\n' % self.DebugFormatString(self.owner_) cnt = 0 for e in self.entries_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'entries%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 return res",if printElemNumber:,if printElemNumber:,0.6797854613403511,0.938501942261528,True
297,def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): if escaped: buf.append(c) elif c == '\\': escaped = True <IF_STMT> return ''.join(buf) else: buf.append(c) raise Exception('Unclosed tag ' + ''.join(buf)),elif c == '>':,elif c == '\n':,0.7833572900002521,0.8385130047130208,False
298,"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True): if shuffle: random.shuffle(train_nodes) total = train_nodes.shape[0] for i in range(0, total, batch_size): <IF_STMT> cur_nodes = train_nodes[i:i + batch_size] cur_labels = train_labels[cur_nodes] yield (cur_nodes, cur_labels)",if i + batch_size <= total:,if i + batch_size < total:,0.8571026995125037,0.7665936070959262,False
299,def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() if len(splitrow) > 0: <IF_STMT> infos.append(' '.join(splitrow[1:])) return infos,if splitrow[0] == 'INFO:':,if splitrow[0] == 'INFO':,0.8037080497215683,0.7498810286408993,False
300,"def _validate_client_public_key(self, username, key_data): """"""Validate a client public key for the specified user"""""" try: key = decode_ssh_public_key(key_data) except KeyImportError: return None options = None if self._client_keys: options = self._client_keys.validate(key, self._peer_addr) if options is None: result = self._owner.validate_public_key(username, key) if asyncio.iscoroutine(result): result = (yield from result) <IF_STMT> return None options = {} self._key_options = options return key",if not result:,if not result:,0.6869896710041223,0.9196822664155297,True
301,"def attach_related_versions(addons, addon_dict=None): if addon_dict is None: addon_dict = {addon.id: addon for addon in addons} all_ids = set(filter(None, (addon._current_version_id for addon in addons))) versions = list(Version.objects.filter(id__in=all_ids).order_by()) for version in versions: try: addon = addon_dict[version.addon_id] except KeyError: log.info('Version %s has an invalid add-on id.' % version.id) continue <IF_STMT> addon._current_version = version version.addon = addon",if addon._current_version_id == version.id:,if addon._current_version is None:,0.9194950464509963,0.8983343737277126,False
302,"def move_view(obj, evt): position = obj.GetCurrentCursorPosition() for other_axis, axis_number in self._axis_names.iteritems(): <IF_STMT> continue ipw3d = getattr(self, 'ipw_3d_%s' % other_axis) ipw3d.ipw.slice_position = position[axis_number]",if other_axis == axis_name:,if axis_number not in position:,0.8349805598164731,0.7297349727547102,False
303,"def func_wrapper(*args, **kwargs): warnings.simplefilter('always', DeprecationWarning) for old, new in arg_mapping.items(): <IF_STMT> warnings.warn(f""Keyword argument '{old}' has been deprecated in favour of '{new}'. '{old}' will be removed in a future version."", category=DeprecationWarning, stacklevel=2) val = kwargs.pop(old) kwargs[new] = val warnings.simplefilter('default', DeprecationWarning) return func(*args, **kwargs)",if old in kwargs:,if old in kwargs:,0.8403163713950567,0.8723360571509826,True
304,"def inner_connection_checker(self, *args, **kwargs): LOG.debug('in _connection_checker') for attempts in range(5): try: return func(self, *args, **kwargs) except exception.VolumeBackendAPIException as e: pattern = re.compile('.*Session id expired$') matches = pattern.match(six.text_type(e)) if matches: <IF_STMT> LOG.debug('Session might have expired. Trying to relogin') self._login() continue LOG.error('Re-throwing Exception %s', e) raise",if attempts < 4:,if attempts == 0:,0.924949684422016,0.8780099567239787,False
305,"def set(self, pcount): """"""Set channel prefetch_count setting."""""" if pcount != self.prev: new_value = pcount <IF_STMT> logger.warning('QoS: Disabled: prefetch_count exceeds %r', PREFETCH_COUNT_MAX) new_value = 0 logger.debug('basic.qos: prefetch_count->%s', new_value) self.callback(prefetch_count=new_value) self.prev = pcount return pcount",if pcount > PREFETCH_COUNT_MAX:,if new_value > PREFETCH_COUNT_MAX:,0.6587070451659093,0.8385130047130208,False
306,"def _build_gcs_object_key(self, key): if self.platform_specific_separator: <IF_STMT> gcs_object_key = os.path.join(self.prefix, self._convert_key_to_filepath(key)) else: gcs_object_key = self._convert_key_to_filepath(key) elif self.prefix: gcs_object_key = '/'.join((self.prefix, self._convert_key_to_filepath(key))) else: gcs_object_key = self._convert_key_to_filepath(key) return gcs_object_key",if self.prefix:,if self.prefix:,0.8933058614495357,0.8590888738245122,True
307,"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): <IF_STMT> name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)",if name not in skip:,if name not in skip:,0.9370415177232354,0.8954283587198554,True
308,def isCurveMonotonic(set_): for i in range(len(set_) - 1): <IF_STMT> return False if set_[i][1] >= set_[i + 1][1]: return False return True,if set_[i][0] >= set_[i + 1][0]:,if set_[i][0] >= set_[i + 1][0]:,0.8402862951292474,0.6885326214539055,True
309,"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color('Miscellaneous Topics:', 'yellow')) for pp in PAGEPATHS: <IF_STMT> continue content = os.listdir(pp) for pn in content: if '.' in pn: name = pn[:pn.index('.')] else: name = pn print(name)",if not os.path.isdir(pp):,if not os.path.isdir(pp):,0.9179372916534827,0.8749766281017177,True
310,"def test_send_error(self): allow_transfer_encoding_codes = (205, 304) for code in (101, 102, 204, 205, 304): self.con.request('SEND_ERROR', '/{}'.format(code)) res = self.con.getresponse() self.assertEqual(code, res.status) self.assertEqual(None, res.getheader('Content-Length')) self.assertEqual(None, res.getheader('Content-Type')) <IF_STMT> self.assertEqual(None, res.getheader('Transfer-Encoding')) data = res.read() self.assertEqual(b'', data)",if code not in allow_transfer_encoding_codes:,if code in allow_transfer_encoding_codes:,0.7270575648979549,0.8385130047130208,False
311,"def _length_hint(obj): """"""Returns the length hint of an object."""""" try: return len(obj) except (AttributeError, TypeError): try: get_hint = type(obj).__length_hint__ except AttributeError: return None try: hint = get_hint(obj) except TypeError: return None <IF_STMT> return None return hint","if hint is NotImplemented or not isinstance(hint, int_types) or hint < 0:",if hint is None:,0.7271453281132884,0.8516228624291206,False
312,"def _rmtree(self, path): for name in self._listdir(path): fullname = self._path_join(path, name) try: isdir = self._isdir(fullname) except self._os_error: isdir = False <IF_STMT> self._rmtree(fullname) else: try: self._remove(fullname) except self._os_error: pass try: self._rmdir(path) except self._os_error: pass",if isdir:,if isdir:,0.6825022882993391,0.8901732118131125,True
313,"def get_sources(self, sources=None): """"""Returns all sources from this provider."""""" self._load() if sources is None: sources = list(self.data.keys()) elif not isinstance(sources, (list, tuple)): sources = [sources] for source in sources: <IF_STMT> raise KeyError('Invalid data key: {}. Valid keys are: {}'.format(source, ', '.join((str(k) for k in self.data)))) return {k: self.data[k] for k in sources}",if source not in self.data:,if source not in self.data:,0.8357211984215862,0.879962308706789,True
314,"def do_shorts(opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != '': opt, optstring = (optstring[0], optstring[1:]) if short_has_arg(opt, shortopts): <IF_STMT> if not args: raise GetoptError('option -%s requires argument' % opt, opt) optstring, args = (args[0], args[1:]) optarg, optstring = (optstring, '') else: optarg = '' opts.append(('-' + opt, optarg)) return (opts, args)",if optstring == '':,if optstring == shortopts:,0.7439956316273285,0.9084940438173679,False
315,"def _sanitize_dict(self, config_dict, allow_val_change=None, ignore_keys: set=None): sanitized = {} for k, v in six.iteritems(config_dict): <IF_STMT> continue k, v = self._sanitize(k, v, allow_val_change) sanitized[k] = v return sanitized",if ignore_keys and k in ignore_keys:,if k in ignore_keys:,0.6544029697646316,0.8038019482772603,False
316,def x(data): count = 0 while count < 10: data.start_example(SOME_LABEL) b = data.draw_bits(1) <IF_STMT> count += 1 data.stop_example(discard=not b) data.mark_interesting(),if b:,if b:,0.6844685220732819,0.8137489370974955,True
317,def prompt_for_resume(config): logger = logging.getLogger('changeme') logger.error('A previous scan was interrupted. Type R to resume or F to start a fresh scan') answer = '' while not (answer == 'R' or answer == 'F'): prompt = '(R/F)> ' answer = '' try: answer = raw_input(prompt) except NameError: answer = input(prompt) if answer.upper() == 'F': logger.debug('Forcing a fresh scan') <IF_STMT> logger.debug('Resuming previous scan') config.resume = True return config.resume,elif answer.upper() == 'R':,elif answer.upper() == 'R':,0.9579300399451931,0.9180466554652996,True
318,"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): <IF_STMT> results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results","if isinstance(in_arrays, tuple):","if isinstance(in_arrays, list):",0.9018977525433898,0.8635707684233572,False
319,"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: <IF_STMT> raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))","if not util.wait_for_write(self.socket, self.socket.gettimeout()):",if self.timeout:,0.5982734442654947,0.8137489370974955,False
320,"def _read_jtl_chunk(self, jtl): data = jtl.read(1024 * 1024 * 10) if data: parts = data.rsplit('\n', 1) <IF_STMT> ready_chunk = self.buffer + parts[0] + '\n' self.buffer = parts[1] df = string_to_df(ready_chunk) self.stat_queue.put(df) return df else: self.buffer += parts[0] else: if self.jmeter_finished: self.agg_finished = True jtl.readline() return None",if len(parts) > 1:,if len(parts) > 1:,0.6496114003740063,0.8832000938217648,True
321,"def __new__(mcl, classname, bases, dictionary): slots = list(dictionary.get('__slots__', [])) for getter_name in [key for key in dictionary if key.startswith('get_')]: name = getter_name slots.append('__' + name) getter = dictionary.pop(getter_name) setter = dictionary.get(setter_name, None) <IF_STMT> del dictionary[setter_name] dictionary[name] = property(getter.setter) dictionary['__slots__'] = tuple(slots) return super().__new__(mcl, classname, bases, dictionary)","if setter is not None and isinstance(setter, collections.Callable):",if getter is None or setter is None:,0.8132053060465646,0.8145683092313328,False
322,"def tex_coords(self): """"""Array of texture coordinate data."""""" if 'multi_tex_coords' not in self.domain.attribute_names: <IF_STMT> domain = self.domain attribute = domain.attribute_names['tex_coords'] self._tex_coords_cache = attribute.get_region(attribute.buffer, self.start, self.count) self._tex_coords_cache_version = domain._version region = self._tex_coords_cache region.invalidate() return region.array else: return None",if self._tex_coords_cache_version != self.domain._version:,if 'tex_coords' in self.domain.attribute_names:,0.6164921167158444,0.8516228624291206,False
323,"def index(self, sub, start=0): """"""Returns the index of the closing bracket"""""" br = '([{<'[')]}>'.index(sub)] count = 0 for i in range(start, len(self.string)): char = self.string[i] <IF_STMT> count += 1 elif char == sub: if count > 0: count -= 1 else: return i err = 'Closing bracket {!r} missing in string {!r}'.format(sub, ''.join(self.original)) raise ParseError(err)",if char == br:,if char in br:,0.7653115557994181,0.9019629427251674,False
324,"def test_createFile(self): text = 'This is a test!' path = tempfile.mktemp() try: koDoc = self._koDocFromPath(path, load=False) koDoc.buffer = text koDoc.save(0) del koDoc koDoc2 = self._koDocFromPath(path) assert koDoc2.buffer == text finally: <IF_STMT> os.unlink(path)",if os.path.exists(path):,if os.path.exists(path):,0.6556466508131876,0.9120815172610118,True
325,"def __editScopeHasEdit(self, attributeHistory): with attributeHistory.context: tweak = GafferScene.EditScopeAlgo.acquireParameterEdit(attributeHistory.scene.node(), attributeHistory.context['scene:path'], attributeHistory.attributeName, IECoreScene.ShaderNetwork.Parameter('', self.__parameter), createIfNecessary=False) <IF_STMT> return False return tweak['enabled'].getValue()",if tweak is None:,if not tweak:,0.6677354772500034,0.7483293841345244,False
326,"def mail_migrator(app, schema_editor): Event_SettingsStore = app.get_model('pretixbase', 'Event_SettingsStore') for ss in Event_SettingsStore.objects.filter(key__in=['mail_text_order_approved', 'mail_text_order_placed', 'mail_text_order_placed_require_approval']): chgd = ss.value.replace('{date}', '{expire_date}') <IF_STMT> ss.value = chgd ss.save() cache.delete('hierarkey_{}_{}'.format('event', ss.object_id))",if chgd != ss.value:,if chgd:,0.8039302037190561,0.8466657105524215,False
327,"def __get_limits(self): dimension = len(self.__tree.get_root().data) nodes = self.__get_all_nodes() max, min = ([float('-inf')] * dimension, [float('+inf')] * dimension) for node in nodes: for d in range(dimension): if max[d] < node.data[d]: max[d] = node.data[d] <IF_STMT> min[d] = node.data[d] return (min, max)",if min[d] > node.data[d]:,if min[d] > node.data[d]:,0.8719924686472009,0.8627586293513119,True
328,"def get_complete_position(self, context: UserContext) -> int: for prefix_pattern in convert2list(self.get_filetype_var(context['filetype'], 'prefix_patterns')): m = re.search(self._object_pattern + prefix_pattern + '\\w*$', context['input']) <IF_STMT> continue self._prefix = re.sub('\\w*$', '', m.group(0)) m = re.search('\\w*$', context['input']) if m: return m.start() return -1",if m is None or prefix_pattern == '':,if not m:,0.6686439221892931,0.8749766281017177,False
329,"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): if curses: curses.setupterm() if curses.tigetnum('colors') > 0: return True <IF_STMT> if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",elif colorama:,if colorama:,0.9196435083895306,0.8827916928185874,False
330,"def setLabelColumnWidth(self, panel, width): for child in panel.GetChildren(): <IF_STMT> size = child.GetSize() size[0] = width child.SetBestSize(size)","if isinstance(child, wx.lib.stattext.GenStaticText):",if child.GetType() == Gtk.PanelType.Column:,0.7999417204356611,0.674945488826271,False
331,"def update(self, other): if other.M is None: <IF_STMT> self.items.update(other.items) else: for i in other.items: self.add(i) return if self.M is None: self.convert() self.M = array.array('B', list(map(max, list(zip(self.M, other.M)))))",if self.M is None:,if self.items is not None:,0.8828889355846108,0.7765145040967655,False
332,"def on_end_epoch(self, state): if self.write_epoch_metrics: <IF_STMT> self.writer.add_text('epoch', '<h4>Epoch {}</h4>'.format(state[torchbearer.EPOCH]) + self.table_formatter(str(state[torchbearer.METRICS])), 1) else: self.writer.add_text('epoch', self.table_formatter(str(state[torchbearer.METRICS])), state[torchbearer.EPOCH])",if self.visdom:,if self.epoch_metrics_enabled:,0.6047620352362983,0.762465858623486,False
333,"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get('events', []))): <IF_STMT> return False elif e.get('event') == ActionExecuted.type_name: return e.get('name') == ACTION_LISTEN_NAME return False",if e.get('event') == UserUttered.type_name:,if e.get('event') == ActionExecuted.type_name:,0.9061566072109126,0.8752376177722327,False
334,"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id <IF_STMT> continue if nw_id_ == nw_id: ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",if port.port_no == in_port:,if in_port is not None and in_port != nw_id_:,0.8487888026095947,0.7678891447146658,False
335,"def next_month(billing_cycle_anchor: datetime, dt: datetime) -> datetime: estimated_months = round((dt - billing_cycle_anchor).days * 12.0 / 365) for months in range(max(estimated_months - 1, 0), estimated_months + 2): proposed_next_month = add_months(billing_cycle_anchor, months) <IF_STMT> return proposed_next_month raise AssertionError(f'Something wrong in next_month calculation with billing_cycle_anchor: {billing_cycle_anchor}, dt: {dt}')",if 20 < (proposed_next_month - dt).days < 40:,if proposed_next_month:,0.816989653890438,0.9184043388013005,False
336,"def wait_complete(self): """"""Wait for futures complete done."""""" for future in concurrent.futures.as_completed(self._futures.keys()): try: error = future.exception() except concurrent.futures.CancelledError: break name = self._futures[future] <IF_STMT> err_msg = 'Extracting ""{0}"", got: {1}'.format(name, error) logger.error(err_msg)",if error is not None:,if error:,0.8221069965668168,0.8787142254774354,False
337,"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): if issubclass(target, mapperlib.Mapper): return target else: mapper = _mapper_or_none(target) <IF_STMT> return mapper else: return _MapperEventsHold(target) else: return target",if mapper is not None:,if mapper:,0.6458975227731515,0.8827916928185874,False
338,"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: <IF_STMT> gvariant += ' {}'.format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f' {arg}' elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","if isinstance(arg, bool):","if isinstance(arg, str):",0.8926418352829988,0.8953711787948615,False
339,"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) <IF_STMT> if support.match_test(test): print(test.id())","elif isinstance(test, unittest.TestCase):","elif isinstance(test, unittest.TestSuite):",0.545975730402846,0.6739047062564734,False
340,def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: <IF_STMT> all_disambiguations.extend(page.relations.disambiguation_links_norm) if page.relations.disambiguation_links is not None: all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),if page.relations.disambiguation_links_norm is not None:,if page.relations.disambiguation_links_norm is not None:,0.7921949743522618,0.6907573115737006,True
341,"def test_decode_invalid(self): testcases = [(b'xn--w&', 'strict', UnicodeError()), (b'xn--w&', 'ignore', 'xn-')] for puny, errors, expected in testcases: with self.subTest(puny=puny, errors=errors): <IF_STMT> self.assertRaises(UnicodeError, puny.decode, 'punycode', errors) else: self.assertEqual(puny.decode('punycode', errors), expected)","if isinstance(expected, Exception):","if isinstance(puny, UnicodeError):",0.8961539424828484,0.8390782502060267,False
342,"def find_globs(walker, patterns, matches): for root, dirs, files in walker: for d in dirs: d = join(root, d) for pattern in patterns: for p in Path(d).glob(pattern): matches.add(str(p)) sub_files = set() for p in matches: <IF_STMT> for f in files: sub_files.add(join(root, f)) matches.update(sub_files)",if root.startswith(p):,if p.endswith(p):,0.9323880222667543,0.9144061946646023,False
343,"def parse_stack_trace(self, it, line): """"""Iterate over lines and parse stack traces."""""" events = [] stack_traces = [] while self.stack_trace_re.match(line): event = self.parse_stack_trace_line(line) <IF_STMT> events.append(event) stack_traces.append(line) line = get_next(it) events.reverse() return (stack_traces, events, line)",if event:,if event:,0.770204100381495,0.8901732118131125,True
344,"def process(self): """"""Do processing necessary, storing result in feature."""""" summation = 0 histo = self.data['flat.notes.quarterLengthHistogram'] if not histo: raise NativeFeatureException('input lacks notes') maxKey = 0 for key in histo: if histo[key] > 0: summation += histo[key] <IF_STMT> maxKey = histo[key] self.feature.vector[0] = maxKey / summation",if histo[key] >= maxKey:,if histo[key] > maxKey:,0.9203638082977385,0.8806615362338783,False
345,"def load_resource(name): """"""return file contents for files within the package root folder"""""" try: <IF_STMT> return sublime.load_resource('Packages/Markdown Preview/{0}'.format(name)) else: filename = os.path.join(sublime.packages_path(), INSTALLED_DIRECTORY, os.path.normpath(name)) return load_utf8(filename) except: print(""Error while load_resource('%s')"" % name) traceback.print_exc() return ''",if is_ST3():,if name.startswith('.markdown'):,0.9225439891552062,0.8935248372106969,False
346,"def get_password(self, service, repo_url): if self.is_unlocked: asyncio.set_event_loop(asyncio.new_event_loop()) collection = secretstorage.get_default_collection(self.connection) attributes = {'application': 'Vorta', 'service': service, 'repo_url': repo_url} items = list(collection.search_items(attributes)) logger.debug('Found %i passwords matching repo URL.', len(items)) <IF_STMT> return items[0].get_secret().decode('utf-8') return None",if len(items) > 0:,if len(items) == 1:,0.6831987353440169,0.8385130047130208,False
347,"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) if fname == 'output': continue if fname == 'PureMVC_Python_1_0': continue if fname[-4:] == '.pyc': continue <IF_STMT> get_dir(p) else: res.append(p) return res",if os.path.isdir(p):,if os.path.isdir(p):,0.9366955939282584,0.9076141716697395,True
348,"def test_nic_names(self): p = subprocess.Popen(['ipconfig', '/all'], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: <IF_STMT> continue if nic not in out: self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)","if 'pseudo-interface' in nic.replace(' ', '-').lower():",if nic in self.nic_names:,0.9450895653139005,0.8661072626070159,False
349,"def vexop_to_simop(op, extended=True, fp=True): res = operations.get(op) if res is None and extended: attrs = op_attrs(op) <IF_STMT> raise UnsupportedIROpError('Operation not implemented') res = SimIROp(op, **attrs) if res is None: raise UnsupportedIROpError('Operation not implemented') if res._float and (not fp): raise UnsupportedIROpError('Floating point support disabled') return res",if attrs is None:,if attrs is None:,0.7581622336066234,0.8806615362338783,True
350,"def rule_builder_add_value(self, value, screenshot_name=None): rule_builder = self.components.rule_builder rule_builder.menu_button_column.wait_for_and_click() with self.rule_builder_rule_editor('add-column-value') as editor_element: filter_input = editor_element.find_element_by_css_selector(""input[type='text']"") filter_input.clear() filter_input.send_keys(value) <IF_STMT> self.screenshot(screenshot_name)",if screenshot_name:,if screenshot_name:,0.7830521737841585,0.8492326635760689,True
351,"def make_open_socket(self): s = socket.socket() try: s.bind(DEFAULT_BIND_ADDR_TUPLE) <IF_STMT> s.listen(1) self.assert_open(s, s.fileno()) except: s.close() s = None raise return s",if WIN or greentest.LINUX:,if self.listening:,0.7956821696152381,0.803154665668484,False
352,"def handle_ray_task_error(e): for s in e.traceback_str.split('\n')[::-1]: <IF_STMT> try: raise getattr(builtins, s.split(':')[0])(''.join(s.split(':')[1:])) except AttributeError as att_err: if 'module' in str(att_err) and builtins.__name__ in str(att_err): pass else: raise att_err raise e",if 'Error' in s or 'Exception' in s:,if s.startswith('ray_task_error'):,0.8908641970196324,0.8743414417652072,False
353,"def compare_multiple_events(i, expected_results, actual_results): events_in_a_row = [] j = i while j < len(expected_results) and isinstance(actual_results[j], actual_results[i].__class__): events_in_a_row.append(actual_results[j]) j += 1 message = '' for event in events_in_a_row: for k in range(i, j): passed, message = compare_events(expected_results[k], event) <IF_STMT> expected_results[k] = None break else: return (i, False, message) return (j, True, '')",if passed:,if passed:,0.9184050326625437,0.9312457603037672,True
354,"def ListSubscriptions(self, params): queryreturn = sqlQuery('SELECT label, address, enabled FROM subscriptions') data = '{""subscriptions"":[' for row in queryreturn: label, address, enabled = row label = shared.fixPotentiallyInvalidUTF8Data(label) <IF_STMT> data += ',' data += json.dumps({'label': label.encode('base64'), 'address': address, 'enabled': enabled == 1}, indent=4, separators=(',', ': ')) data += ']}' return data",if len(data) > 20:,if len(data) > 0:,0.8392504699561802,0.8902056737869248,False
355,"def compile(self, args): compiled_args = {} for key, value in six.iteritems(args): <IF_STMT> compiled_args[key] = str(value) else: compiled_args[key] = sjson_dumps(value) return self._minified_code % compiled_args",if key in self.clean_args:,"if isinstance(value, six.string_types):",0.7205340662463834,0.80377750806414,False
356,"def insert(self, pack_id, data): if pack_id not in self.queue and pack_id > self.begin_id: self.queue[pack_id] = PacketInfo(data) if self.end_id == pack_id: self.end_id = pack_id + 1 <IF_STMT> eid = self.end_id while eid < pack_id: self.miss_queue.add(eid) eid += 1 self.end_id = pack_id + 1 else: self.miss_queue.remove(pack_id)",elif self.end_id < pack_id:,elif self.end_id < self.end_id:,0.7848606350906843,0.8780099567239787,False
357,"def _target_generator(self): if self._internal_target_generator is None: <IF_STMT> return None from ....model_zoo.ssd.target import SSDTargetGenerator self._internal_target_generator = SSDTargetGenerator(iou_thresh=self._iou_thresh, stds=self._box_norm, negative_mining_ratio=-1, **self._kwargs) return self._internal_target_generator else: return self._internal_target_generator",if self._anchors_none:,if self._iou_thresh is None:,0.8710448160066231,0.7801270245332924,False
358,"def test_heapsort(self): for trial in range(100): size = random.randrange(50) data = [random.randrange(25) for i in range(size)] <IF_STMT> heap = data[:] self.module.heapify(heap) else: heap = [] for item in data: self.module.heappush(heap, item) heap_sorted = [self.module.heappop(heap) for i in range(size)] self.assertEqual(heap_sorted, sorted(data))",if trial & 1:,if trial == 0:,0.8064549800243856,0.8661072626070159,False
359,"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) <IF_STMT> code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",if res == _subprocess.WAIT_OBJECT_0:,if res == _subprocess.WAIT_FOR_OBJECT:,0.7879475980236563,0.8752376177722327,False
360,"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break <IF_STMT> continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",elif value is None:,elif value == 0:,0.8277970953474078,0.8806615362338783,False
361,"def isnotsurplus(self, item: T) -> bool: if not self.matchers: <IF_STMT> self.mismatch_description.append_text('not matched: ').append_description_of(item) return False return True",if self.mismatch_description:,if self.mismatch_description:,0.5794250379069639,0.7778111223054219,True
362,"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): <IF_STMT> return environ.get(list(config.values())[0]) elif list(config.keys()) == ['$file']: return open(list(config.values())[0]).read() else: return {key: resolve_env_secrets(value, environ) for key, value in config.items()} elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",if list(config.keys()) == ['$env']:,if list(config.keys()) == ['$env']:,0.6512479987969426,0.8902056737869248,True
363,"def __open__(filename, *args, **kwargs): if os.path.isfile(filename): return __realopen__(filename, *args, **kwargs) if not os.path.isabs(filename): datafilename = __papplet__.dataPath(filename) <IF_STMT> return __realopen__(datafilename, *args, **kwargs) sketchfilename = __papplet__.sketchPath(filename) if os.path.isfile(sketchfilename): return __realopen__(sketchfilename, *args, **kwargs) return __realopen__(filename, *args, **kwargs)",if os.path.isfile(datafilename):,if os.path.isfile(datafilename):,0.9016241111894617,0.8935248372106969,True
364,def run(self): while not self.completed: <IF_STMT> time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),if self.block:,if self.period is not None:,0.9073302118134431,0.8338542560892604,False
365,"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: <IF_STMT> raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if error_on_path:,if error_on_path:,0.9018895381399141,0.9350761925543661,True
366,"def structured_dot_grad(sparse_A, dense_B, ga): if sparse_A.type.format in ('csc', 'csr'): <IF_STMT> sdgcsx = sdg_csc CSx = CSC else: sdgcsx = sdg_csr CSx = CSR g_A_data = sdgcsx(csm_indices(sparse_A), csm_indptr(sparse_A), dense_B, ga) return CSx(g_A_data, csm_indices(sparse_A), csm_indptr(sparse_A), csm_shape(sparse_A)) else: raise NotImplementedError()",if sparse_A.type.format == 'csc':,if sparse_A.type.format == 'csc':,0.881516624480411,0.8555308664663046,True
367,"def step_async(self, actions): listify = True try: <IF_STMT> listify = False except TypeError: pass if not listify: self.actions = actions else: assert self.num_envs == 1, f'actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments' self.actions = [actions]",if len(actions) == self.num_envs:,"if not isinstance(actions, list):",0.7465260559405494,0.8832000938217648,False
368,"def tempFailureRetry(func, *args, **kwargs): while True: try: return func(*args, **kwargs) except (os.error, IOError) as ex: <IF_STMT> continue else: raise",if ex.errno == errno.EINTR:,if ex.errno == errno.EINTR:,0.5682581718418613,0.7245511487202049,True
369,"def test_learning_always_changes_generation(chars, order): learner = LStar(lambda s: len(s) == 1 and s[0] in chars) for c in order: prev = learner.generation s = bytes([c]) <IF_STMT> learner.learn(s) assert learner.generation > prev",if learner.dfa.matches(s) != learner.member(s):,if len(s) > 0:,0.8062847583649473,0.8228500218338367,False
370,"def test_costs_5D_noisy_names(signal_bkps_5D_noisy, cost_name): signal, bkps = signal_bkps_5D_noisy cost = cost_factory(cost_name) cost.fit(signal) cost.error(0, 100) cost.error(100, signal.shape[0]) cost.error(10, 50) cost.sum_of_costs(bkps) with pytest.raises(NotEnoughPoints): <IF_STMT> cost.min_size = 4 cost.error(1, 2) else: cost.error(1, 2)",if cost_name == 'cosine':,if cost_name == 'noisy':,0.8821650408902354,0.8169276475307028,False
371,"def remove_empty_dirs(dirname): logger.debug(""remove_empty_dirs '%s'"" % dirname) try: <IF_STMT> dirname = dirname.encode('utf-8') os.removedirs(dirname) logger.debug(""remove_empty_dirs '%s' done"" % dirname) except OSError as exc: if exc.errno == errno.ENOTEMPTY: logger.debug(""remove_empty_dirs '%s' not empty"" % dirname) pass else: raise except Exception as e: logger.exception(e) logger.error('remove_empty_dirs exception: ' + dirname) raise e","if not isinstance(dirname, str):","if isinstance(dirname, str):",0.7831638152606324,0.9022045190074797,False
372,"def get_unique_attribute(self, name: str): feat = None for f in self.features: <IF_STMT> if feat is not None: raise RuntimeError('The attribute was not unique.') feat = f if feat is None: raise RuntimeError('The attribute did not exist') return getattr(feat, name)","if self._return_feature(f) and hasattr(f, name):",if f.name == name:,0.8721665613074291,0.8627586293513119,False
373,"def get_allocated_address(self, config: ActorPoolConfig, allocated: allocated_type) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): if strategy == self: occupied = True break <IF_STMT> return addr raise NoIdleSlot(f'No idle slot for creating actor with label {self.label}, mark {self.mark}')",if not occupied:,if occupied:,0.9378605456111027,0.9237460349978159,False
374,"def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result for key, value in self.__dict__.items(): <IF_STMT> setattr(result, key, copy.copy(value)) else: setattr(result, key, copy.deepcopy(value, memo)) return result",if key in cls.dynamic_methods:,"if isinstance(value, dict):",0.6105725471340112,0.8390782502060267,False
375,def restore_forward(model): for child in model.children(): <IF_STMT> child.forward = child.old_forward child.old_forward = None else: restore_forward(child),"if is_leaf(child) and hasattr(child, 'old_forward'):","if isinstance(child, Forward):",0.8112020038272512,0.6964705665515707,False
376,"def add(self, obj, allow_duplicates=False): if allow_duplicates or obj not in self._constants: self._constant_pool.append(obj) self._constants[obj] = len(self) <IF_STMT> self._constant_pool.append(None)","if obj.__class__ in (Double, Long):",if self._constant_pool[obj] == self[obj]:,0.6134001975717881,0.7404008324993688,False
377,def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: idx = l.lower().find('copyright') if idx < 0: continue copyright = l[idx + 9:].strip() if not copyright: continue copyright = sanitise(copyright) <IF_STMT> continue ret.add(copyright) return ret,if not copyright.find('200') >= 0 and (not copyright.find('199') >= 0):,if not copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#'):,0.8795296406032455,0.8076797778670548,False
378,"def callback(lexer, match, context): text = match.group() extra = '' if start: context.next_indent = len(text) if context.next_indent < context.indent: while context.next_indent < context.indent: context.indent = context.indent_stack.pop() <IF_STMT> extra = text[context.indent:] text = text[:context.indent] else: context.next_indent += len(text) if text: yield (match.start(), TokenClass, text) if extra: yield (match.start() + len(text), TokenClass.Error, extra) context.pos = match.end()",if context.next_indent > context.indent:,if text[:context.indent] == context.indent:,0.6702045566103264,0.9001816649635144,False
379,"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) if not cmd.check(f'docker check for {self.path.k8s}'): if not cmd.stdout.strip(): log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) <IF_STMT> print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if log_cmd.check(f'docker logs for {self.path.k8s}'):,if log_cmd.run():,0.9300761099853636,0.9024521756077707,False
380,"def nodes(self): if not self._nodes: nodes = self.cluster_group.instances() self._nodes = [] master = self.master_node nodeid = 1 for node in nodes: if node.state not in ['pending', 'running']: continue <IF_STMT> self._nodes.insert(0, master) continue self._nodes.append(Node(node, self.key_location, 'node%.3d' % nodeid)) nodeid += 1 else: for node in self._nodes: log.debug('refreshing instance %s' % node.id) node.update() return self._nodes",if node.id == master.id:,if nodeid == master.id:,0.9195412263273274,0.8983343737277126,False
381,"def match(cls, agent_name, guid, uri, media=None): agent = Agents.get(agent_name) if agent is None: <IF_STMT> log.warn('Unsupported metadata agent: %s' % agent_name) unsupported_agents[agent_name] = True return False log.warn('Unsupported metadata agent: %s' % agent_name, extra={'duplicate': True}) return False return agent.fill(guid, uri, media)",if agent_name not in unsupported_agents:,if agent_name in unsupported_agents:,0.8993814803168851,0.8627586293513119,False
382,"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)): plug.setInput(randomNode['outFloat']) <IF_STMT> plug.setInput(randomNode['outColor']) GafferUI.NodeEditor.acquire(randomNode)","elif isinstance(plug, Gaffer.Color3fPlug):","elif isinstance(plug, Gaffer.ColorPlug):",0.891735311484454,0.807681595142239,False
383,"def post_arrow(self, arr: pa.Table, graph_type: str, opts: str=''): dataset_id = self.dataset_id tok = self.token sub_path = f'api/v2/upload/datasets/{dataset_id}/{graph_type}/arrow' try: resp = self.post_arrow_generic(sub_path, tok, arr, opts) out = resp.json() <IF_STMT> raise Exception('No success indicator in server response') return out except Exception as e: logger.error('Failed to post arrow to %s', sub_path, exc_info=True) raise e",if not 'success' in out or not out['success']:,if resp.status_code != 200:,0.8604848453479976,0.8944264839442453,False
384,"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): <IF_STMT> child = dict_to_XML('layer', val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: if tag == 'config': child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",if tag == 'layers':,"if isinstance(val, MutableLayer):",0.9339020096381625,0.9118021019905903,False
385,def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) <IF_STMT> error('node not found: ' + str(dir)) continue if not node in lst: lst.append(node) self.bld_incpaths_lst.append(node),if not node:,if node is None:,0.8164075376288007,0.8228500218338367,False
386,"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): if isinstance(val, compat.string_types): return '  %s' % val elif val < 1024 ** 2: return '  %.1f KB' % (val / 1024.0 ** 1) <IF_STMT> return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val",elif val < 1024 ** 3:,elif val > 1024.0 ** 3:,0.6888929050590337,0.8916211056218591,False
387,"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <IF_STMT> curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",if reuse_len is not None and reuse_len > 0:,if reuse_len is not None:,0.5951720452659314,0.8559898693114286,False
388,"def GROUP_CONCAT(builder, distinct, expr, sep=None): assert distinct in (None, True, False) result = (distinct and 'GROUP_CONCAT(DISTINCT ' or 'GROUP_CONCAT(', builder(expr)) if sep is not None: <IF_STMT> result = (result, ' SEPARATOR ', builder(sep)) else: result = (result, ', ', builder(sep)) return (result, ')')",if builder.provider.dialect == 'MySQL':,"if sep == ',':",0.69294773605481,0.8752376177722327,False
389,"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.custom_fields = [] self.obj_type = ContentType.objects.get_for_model(self.model) custom_fields = CustomField.objects.filter(content_types=self.obj_type) for cf in custom_fields: <IF_STMT> self.nullable_fields.append(cf.name) self.fields[cf.name] = cf.to_form_field(set_initial=False, enforce_required=False) self.custom_fields.append(cf.name)",if not cf.required:,if cf.nullable:,0.8645912022191177,0.8590888738245122,False
390,"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: <IF_STMT> return True if possible_child_hash not in self.items: return False possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash == item_hash:,if self.items[possible_child_hash].previous_hash != item_hash:,0.8484355995176522,0.7886336751695258,False
391,"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) <IF_STMT> self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) if not self.use_parallel_executor: self.assertTrue((in_data == out_data).all())",if self.use_parallel_executor and (not self.use_double_buffer):,if self.use_unordered_batch:,0.9141190322023034,0.8787142254774354,False
392,"def add_cells(self, cells): for cell in cells: <IF_STMT> id = len(self.cell_id_map) self.cell_id_map[cell] = id self.id_cell_map[id] = cell",if cell not in self.cell_id_map:,if cell not in self.id_cell_map:,0.7846759066918189,0.6577160909911662,False
393,"def _verify_out(marker='>>'): if shared: self.assertIn('libapp_lib.dylib', self.client.out) el<IF_STMT> self.assertIn('libapp_lib.a', self.client.out) else: self.assertIn('Built target app_lib', self.client.out) out = str(self.client.out).splitlines() for k, v in vals.items(): self.assertIn('%s %s: %s' % (marker, k, v), out)",if marker == '>>':,if shared:,0.9175832167248096,0.8787142254774354,False
394,"def Visit_expr(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance(child, pytree.Leaf) and child.value == '|':","if isinstance(child, pytree.Leaf) and child.value == 'expr':",0.4589411392016681,0.4527471870952894,False
395,"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) if not data: return <IF_STMT> self.limit = 0 self.after = Object(id=int(data[-1]['user']['id'])) for element in reversed(data): await self.members.put(self.create_member(element))",if len(data) < 1000:,if len(data) > 1:,0.6126952981288176,0.8431339019329497,False
396,"def assert_warns(expected): with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always') yield if sys.version_info >= (3, 0): <IF_STMT> try: exc_name = expected.__name__ except AttributeError: exc_name = str(expected) raise AssertionError('%s not triggerred' % exc_name)","if not any((isinstance(m.message, expected) for m in w)):",if expected is not None:,0.8129375347528222,0.7909601595885504,False
397,"def __init__(self, measures): """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class"""""" self.__class__.__name__ = 'Contingency' + measures.__class__.__name__ for k in dir(measures): <IF_STMT> continue v = getattr(measures, k) if not k.startswith('_'): v = self._make_contingency_fn(measures, v) setattr(self, k, v)",if k.startswith('__'):,if k.startswith('_'):,0.9018301796412864,0.8966773400768917,False
398,"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: omit = elem.tag == 'kw' and elem.get('type') != 'teardown' start = event == 'start' <IF_STMT> omitted_kws += 1 if not omitted_kws: yield (event, elem) elif not start: elem.clear() if omit and (not start): omitted_kws -= 1",if omit and start:,if omit and (not start):,0.6662987422670956,0.8677319190106252,False
399,"def read_block(buffer, i): offset = i * BLOCK_LENGTH % config.CAPTURE_BUFFER while True: if buffer[offset] == BLOCK_MARKER.END: return None while buffer[offset] == BLOCK_MARKER.WRITE: time.sleep(SHORT_SENSOR_SLEEP_TIME) buffer[offset] = BLOCK_MARKER.READ buffer.seek(offset + 1) length = struct.unpack('=H', buffer.read(2))[0] retval = buffer.read(length) <IF_STMT> break buffer[offset] = BLOCK_MARKER.NOP return retval",if buffer[offset] == BLOCK_MARKER.READ:,if retval == BLOCK_MARKER.NOP:,0.8034650746862808,0.8752376177722327,False
400,def _start(self): try: instance_info = self._get_instance_info() <IF_STMT> self._multipass_cmd.start(instance_name=self.instance_name) except errors.ProviderInfoError as instance_error: raise errors.ProviderInstanceNotFoundError(instance_name=self.instance_name) from instance_error,if not instance_info.is_running():,if instance_info and instance_info['instance_type'] == 'multipass':,0.49416686649583447,0.605478327168454,False
401,"def _river_driver(self): if self._cached_river_driver: return self._cached_river_driver else: <IF_STMT> self._cached_river_driver = MsSqlDriver(self.workflow, self.wokflow_object_class, self.field_name) else: self._cached_river_driver = OrmDriver(self.workflow, self.wokflow_object_class, self.field_name) return self._cached_river_driver",if app_config.IS_MSSQL:,if self.is_sql:,0.8904786561666947,0.8232490471721702,False
402,"def __LazyMap__(self, attr): try: <IF_STMT> debug_attr_print('%s.__LazyMap__(%s) added something' % (self._username_, attr)) return 1 except AttributeError: return 0",if self._LazyAddAttr_(attr):,if attr in self._lazy_map_:,0.5410528730769859,0.693395566222006,False
403,"def prepare(self, data=None, user=None): """"""Prepare activation for execution."""""" super(ManagedStartViewActivation, self).prepare.original() self.task.owner = user management_form_class = self.get_management_form_class() self.management_form = management_form_class(data=data, instance=self.task) if data: <IF_STMT> raise FlowRuntimeError('Activation metadata is broken {}'.format(self.management_form.errors)) self.task = self.management_form.save(commit=False)",if not self.management_form.is_valid():,if self.management_form.errors:,0.9252434463701256,0.8866029039778043,False
404,"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 <IF_STMT> RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",if not RegionSizeGuid:,if self <= 3:,0.7191837544468193,0.8336104423443033,False
405,"def _get_completion(self, document): try: completion_header = document.xpath(""//div[@id='complete_day']"")[0] completion_message = completion_header.getchildren()[0] <IF_STMT> return False elif 'day_complete_message' in completion_message.classes: return True except IndexError: return False",if 'day_incomplete_message' in completion_message.classes:,if 'completion_message' in completion_message.classes:,0.8006782067806474,0.7709002428237395,False
406,"def run(self): DISPATCH_SYNC = components.interfaces.nsIEventTarget.DISPATCH_SYNC try: <IF_STMT> return for match in findlib2.find_all_matches(self.regex, self.text): if self._stopped: return self.target.dispatch(lambda: self.callback(match), DISPATCH_SYNC) if self._stopped: return self.target.dispatch(lambda: self.callback(None), DISPATCH_SYNC) finally: self.callback = None self.target = None",if self._stopped:,if self.callback is None:,0.7986031274735361,0.8336104423443033,False
407,"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) <IF_STMT> return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)",elif 'regex' in literal_or_identifier:,"elif isinstance(k, regex_type):",0.9253030344481319,0.9042878500265974,False
408,"def process_image_pre_creation(sender, instance: Image, **kwargs): if instance.pk is not None: return for plugin in _plugin_instances: process_fn = getattr(plugin, 'process_image_pre_creation', None) <IF_STMT> continue try: process_fn(django_settings=settings, image_instance=instance) except Exception: logging.exception(""Error occurs while trying to access plugin's pin_pre_save for plugin %s"" % plugin)",if process_fn is None:,if process_fn is None:,0.7007194087751132,0.8661072626070159,True
409,def check_screenshots(self): if self.interactive: self._commit_screenshots() el<IF_STMT> self._validate_screenshots() self._commit_screenshots() elif self.allow_missing_screenshots: warnings.warn('No committed reference screenshots available. Ignoring.') else: self.fail('No committed reference screenshots available. Run interactive first.'),if self._has_reference_screenshots():,if self.screenshots:,0.6256035496647039,0.8531413606256201,False
410,"def on_task_abort(self, task, config): if 'abort' in config: <IF_STMT> return log.debug('sending abort notification') self.send_notification(config['abort']['title'], config['abort']['message'], config['abort']['via'], template_renderer=task.render)",if task.silent_abort:,if config['abort'] == 'no' or config['abort']['via'] == 'no':,0.5455041082636396,0.5697863655544793,False
411,"def block_users(self, user_ids): broken_items = [] self.logger.info('Going to block %d users.' % len(user_ids)) for user_id in tqdm(user_ids): <IF_STMT> self.error_delay() broken_items = user_ids[user_ids.index(user_id):] break self.logger.info('DONE: Total blocked %d users.' % self.total['blocks']) return broken_items",if not self.block(user_id):,if not self.total['blocks']:,0.8614736819721551,0.8592899528284996,False
412,"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] parent = self['editor'] for c in parent.get_children(): if hasattr(c, 'get_id'): if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) <IF_STMT> return r return None",if not r is None:,if r:,0.9469845290100836,0.926934323706186,False
413,"def addClasses(self, name): for n in name.split(): try: k, method = n.split('.') except ValueError: k = n method = None self.classes[k] = 1 <IF_STMT> self.methods.setdefault(k, {})[method] = 1",if method is not None:,if method:,0.6514697386916951,0.8696398662122882,False
414,"def Read(self, lex_mode): while True: t = self._Read(lex_mode) self.was_line_cont = t.id == Id.Ignored_LineCont <IF_STMT> break return t",if t.id != Id.Ignored_LineCont:,if self.was_line_cont:,0.5507931257342035,0.7778111223054219,False
415,"def _dir_guildfile(dir, ctx): from guild import guildfile try: return guildfile.for_dir(dir) except guildfile.NoModels: <IF_STMT> help_suffix = "" or '%s' for help"" % click_util.cmd_help(ctx) else: help_suffix = '' cli.error('%s does not contain a Guild file (guild.yml)\nTry specifying a project path or package name%s.' % (cwd_desc(dir), help_suffix)) except guildfile.GuildfileError as e: cli.error(str(e))",if ctx:,if ctx.args[0] == 'help':,0.9057699192346593,0.8902056737869248,False
416,"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: if not line.strip(): continue <IF_STMT> return elif line.startswith(b'Benutzer/Passwort Fehler'): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",if line.startswith(b'OK'):,if line.startswith(b'Benutzer/Passwort'):,0.8212958284650911,0.8743414417652072,False
417,"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f'Unexpected response type: `{type(response)}`') <IF_STMT> homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)",if stat.S_ISDIR(int(response.st_mode)):,if response.pathspec:,0.8689798632216058,0.8996480074924822,False
418,"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: <IF_STMT> x = np.add(x, self.bias[spk]) if self.norm_vars: x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",if self.norm_means:,if self.norm_means:,0.9373789279523093,0.9202663016973823,True
419,"def hasFixtures(self, ctx_callback=None): context = self.context if context is None: return False if self.implementsAnyFixture(context, ctx_callback=ctx_callback): return True factory = self.factory if factory: ancestors = factory.context.get(self, []) for ancestor in ancestors: <IF_STMT> return True return False","if self.implementsAnyFixture(ancestor, ctx_callback=ctx_callback):","if self.implementsAnyFixture(ancestor, ctx_callback=ctx_callback):",0.8906905029303115,0.8713933650206428,True
420,def UpdateControlState(self): active = self.demoModules.GetActiveID() for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] if moduleID == active: btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) <IF_STMT> self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),if moduleID == modModified:,elif moduleID == modActive:,0.8617270277272399,0.828399516355805,False
421,"def ignore_proxy_host(self): """"""Check if self.host is in the $no_proxy ignore list."""""" if urllib.proxy_bypass(self.host): return True no_proxy = os.environ.get('no_proxy') if no_proxy: entries = [parse_host_port(x) for x in no_proxy.split(',')] for host, port in entries: <IF_STMT> return True return False",if host.lower() == self.host and port == self.port:,if self.host.startswith(host):,0.933601732658486,0.9024521756077707,False
422,"def run(self, _): view = self.view if not view.settings().get('terminus_view'): return terminal = Terminal.from_id(view.id()) if terminal: terminal.close() panel_name = terminal.panel_name <IF_STMT> window = panel_window(view) if window: window.destroy_output_panel(panel_name) else: view.close()",if panel_name:,if panel_name:,0.9169695388460628,0.8696398662122882,True
423,"def get_docname_for_node(self, node: Node) -> str: while node: <IF_STMT> return self.env.path2doc(node['source']) elif isinstance(node, addnodes.start_of_file): return node['docname'] else: node = node.parent return None","if isinstance(node, nodes.document):","if isinstance(node, addnodes.path_of_file):",0.861113875662712,0.7947545184555568,False
424,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.add_version(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.8646149936089025,0.7378351342269067,True
425,"def _maybe_female(self, path_elements, female, strict): if female: if self.has_gender_differences: elements = path_elements + ['female'] try: return self._get_file(elements, '.png', strict=strict) except ValueError: <IF_STMT> raise elif strict: raise ValueError('Pokemon %s has no gender differences' % self.species_id) return self._get_file(path_elements, '.png', strict=strict)",if strict:,if self.species_id not in self.gender_differences:,0.9086518911526098,0.8380055871435848,False
426,"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() elif event.GetKeyCode() == wx.WXK_RETURN: self._update_value() <IF_STMT> self.SetValue('') if event.GetKeyCode() != wx.WXK_RETURN: event.Skip()",elif event.GetKeyCode() == wx.WXK_DELETE:,elif event.GetKeyCode() == wx.WXK_ESCAPE:,0.8873463457000027,0.760856626273165,False
427,"def sync_up_to_new_location(self, worker_ip): if worker_ip != self.worker_ip: logger.debug('Setting new worker IP to %s', worker_ip) self.set_worker_ip(worker_ip) self.reset() <IF_STMT> logger.warning('Sync up to new location skipped. This should not occur.') else: logger.warning('Sync attempted to same IP %s.', worker_ip)",if not self.sync_up():,if self.sync_failed:,0.6290809662687117,0.8966773400768917,False
428,"def _get_download_link(self, url, download_type='torrent'): links = {'torrent': '', 'magnet': ''} try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find('div', {'class': 'download'}) <IF_STMT> for download in downloads.findAll('a'): link = download['href'] if link.startswith('magnet'): links['magnet'] = link else: links['torrent'] = urljoin(self.urls['base_url'], link) except Exception: pass return links[download_type]",if downloads:,if downloads:,0.777754171187625,0.9220450449751959,True
429,"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): if '::1' in line: newline = re.sub('\\slocalhost\\s', ' ', line) <IF_STMT> logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), 'w') for line in lines: f.write(line) f.close()",if line != newline:,if newline:,0.7415031617049609,0.9298663600557577,False
430,"def prepare(self): if hasattr(self, 'prepared') and (not self.prepared): self.data = SafeUnpickler.loads(self.data) <IF_STMT> self.data['instance_id'] = self.instance_id self.prepared = True","if hasattr(self, 'instance_id'):",if self.instance_id:,0.59850508186648,0.7912619863720214,False
431,"def _test_compute_q0(self): sigma = 15 order = 250 logqs = np.arange(-290, -270, 1) count = 0 for logq in logqs: count += 1 sys.stdout.write('\t%0.5g: %0.10g' % (logq, pate.rdp_gaussian(logq, sigma, order))) sys.stdout.flush() <IF_STMT> print('')",if count % 5 == 0:,if count > 100:,0.7321091685312235,0.8617195878573112,False
432,"def valid_fieldnames(fieldnames): """"""check if fieldnames are valid"""""" for fieldname in fieldnames: <IF_STMT> return True elif fieldname in fieldname_map and fieldname_map[fieldname] == 'source': return True return False",if fieldname in canonical_field_names and fieldname == 'source':,if fieldname == 'name':,0.82633151477284,0.7965020533851944,False
433,"def ns_provide(self, id_): global controllers, layouts if id_ == '_leo_viewrendered': c = self.c vr = controllers.get(c.hash()) or ViewRenderedController(c) h = c.hash() controllers[h] = vr <IF_STMT> layouts[h] = c.db.get('viewrendered_default_layouts', (None, None)) return vr",if not layouts.get(h):,if h in layouts:,0.6813880614582042,0.8336104423443033,False
434,"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config <IF_STMT> defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if defaults is None:,if defaults is None:,0.8129260772126756,0.8983343737277126,True
435,"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): if k not in query: query[k] = {} <IF_STMT> query[k] = v else: query[k].update(v) return query","if isinstance(v, list):","elif isinstance(v, dict):",0.7557693354256668,0.8783650674919876,False
436,"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: <IF_STMT> data, passon = (data[:self.size], data[self.size:]) else: passon = b'' if data: self.data.append(data) return passon",if self.size:,if self.size > 0:,0.7212472015262077,0.8228500218338367,False
437,"def updateVar(name, data, mode=None): if mode: if mode == 'append': core.config.globalVariables[name].append(data) <IF_STMT> core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",elif mode == 'add':,elif mode == 'add':,0.8479530173567706,0.693395566222006,True
438,"def vi_pos_back_short(line, index=0, count=1): line = vi_list(line) try: for i in range(count): index -= 1 while vi_is_space(line[index]): index -= 1 in_word = vi_is_word(line[index]) <IF_STMT> while vi_is_word(line[index]): index -= 1 else: while not vi_is_word_or_space(line[index]): index -= 1 return index + 1 except IndexError: return 0",if in_word:,if in_word:,0.9070764684793349,0.9184043388013005,True
439,"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) <IF_STMT> for key, max_len in len_map.items(): example_len = example[key].shape if example_len > max_len: example[key] = np.resize(example[key], max_len) yield tuple(example)",if len_map is not None:,if len_map:,0.6889549727587141,0.8743414417652072,False
440,def decorate(f): f.__wrapped__ = Obj.__init__ f._uses_signature = Obj if Obj.__doc__: doclines = Obj.__doc__.splitlines() <IF_STMT> doc = f.__doc__ + '\n'.join(doclines[1:]) else: doc = '\n'.join(doclines) try: f.__doc__ = doc except AttributeError: pass return f,if f.__doc__:,if doclines[0] == '__doc__':,0.6390173897852798,0.8336104423443033,False
441,"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in ('toplevel', 'detailed'): if self.counting != 'detailed': category = category.split('/')[0] <IF_STMT> self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",if category not in self.errors_by_category:,if category not in self.errors_by_category:,0.590513236788926,0.7975010608178975,True
442,"def _delete_fields(self, data): data = self._del(data, ['speaker_ids', 'track_id', 'microlocation_id', 'session_type_id']) for _ in ['start_time_tz', 'end_time_tz']: <IF_STMT> data[_] = SESSION_POST[_[0:-3]].from_str(data[_]) data[_[0:-3]] = data.pop(_) return data",if _ in data:,if _ in SESSION_POST:,0.8690189714707551,0.7801270245332924,False
443,"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = '' strings = [] for char in word: if char in char_set: letters += char count += 1 else: <IF_STMT> strings.append(letters) letters = '' count = 0 if count > threshold: strings.append(letters) return strings",if count > threshold:,if count > threshold:,0.8111391839800727,0.8752376177722327,True
444,"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == '{': length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit <IF_STMT> break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",if token.ClosesScope():,if token.value == '}':,0.9312700475810929,0.8832000938217648,False
445,"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break <IF_STMT> raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",elif s.startswith('Proc-Type') and 'ENCRYPTED' in s:,elif mode == 'end':,0.9562585528460688,0.924776563154472,False
446,def main(self): self.model.clear() self.callman.unregister_all() active_handle = self.get_active('Person') if active_handle: active = self.dbstate.db.get_person_from_handle(active_handle) <IF_STMT> self.callman.register_obj(active) self.display_citations(active) else: self.set_has_data(False) else: self.set_has_data(False),if active:,if active:,0.8167429642662165,0.803154665668484,True
447,"def _validate(self) -> None: super(Tuple, self)._validate() if len(self.elements) == 0: <IF_STMT> raise CSTValidationError('A zero-length tuple must be wrapped in parentheses.')",if len(self.lpar) == 0:,if self.elements[0] == 0:,0.8343858871840774,0.7378351342269067,False
448,"def _session_from_arg(self, session_obj, lock_type=None): if not isinstance(session_obj, self.ISession): vm = self._machine_from_arg(session_obj) lock_type = lock_type or self.LockType.null <IF_STMT> return vm.create_session(lock_type) return None return session_obj",if vm:,if vm:,0.7719297535162916,0.839587623092576,True
449,"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue <IF_STMT> if not private and name.startswith('_'): continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",if name != '__init__':,if meth is not None:,0.89909417838126,0.8036431532733102,False
450,"def pdb(message=''): """"""Fall into pdb."""""" import pdb if app and (not app.useIpython): try: import PyQt5.QtCore as QtCore except ImportError: try: import PyQt4.QtCore as QtCore except ImportError: QtCore = None <IF_STMT> QtCore.pyqtRemoveInputHook() if message: print(message) pdb.set_trace()",if QtCore:,if QtCore:,0.935742901870759,0.8966773400768917,True
451,"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get('Logging'): <IF_STMT> if b['Name'] != b['Logging']['TargetBucket']: continue yield (b['Logging']['TargetBucket'], b['Logging']['TargetPrefix']) if not self_log and b['Name'].startswith('cf-templates-'): yield (b['Name'], '')",if self_log:,if b['Logging']['TargetPrefix']:,0.9228399294951519,0.8935248372106969,False
452,"def prepare_fields(self): for k, v in self.fields.items(): v._required = v.required v.required = False v.widget.is_required = False <IF_STMT> v._required = v.one_required v.one_required = False v.widget.enabled_locales = self.locales","if isinstance(v, I18nFormField):",if v.one_required:,0.6909184272121348,0.8590888738245122,False
453,"def __pack__(self): new_values = [] for i in xrange(len(self.__unpacked_data_elms__)): for key in self.__keys__[i]: new_val = getattr(self, key) old_val = self.__unpacked_data_elms__[i] <IF_STMT> break new_values.append(new_val) return struct.pack(self.__format__, *new_values)",if new_val != old_val:,if new_val != old_val:,0.8565981323112785,0.7965020533851944,True
454,"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join(constant.profile['APPDATA'], u'..', u'Local', u'Microsoft', u'Vault') <IF_STMT> for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault(os.path.join(main_vault_directory, vault_directory)) if cred: pwd_found.append(cred) return pwd_found",if os.path.exists(main_vault_directory):,if os.path.exists(main_vault_directory):,0.9236270933846101,0.8787142254774354,True
455,"def on_revision_plugin_revision_pre_save(**kwargs): instance = kwargs['instance'] if kwargs.get('created', False): update_previous_revision = not instance.previous_revision and instance.plugin and instance.plugin.current_revision and (instance.plugin.current_revision != instance) <IF_STMT> instance.previous_revision = instance.plugin.current_revision if not instance.revision_number: try: previous_revision = instance.plugin.revision_set.latest() instance.revision_number = previous_revision.revision_number + 1 except RevisionPluginRevision.DoesNotExist: instance.revision_number = 1",if update_previous_revision:,if update_previous_revision:,0.6667534363762995,0.9122561819614461,True
456,"def __setattr__(self, name, value): super().__setattr__(name, value) field = self._fields.get(name) if field: self.check_field_type(field, value) <IF_STMT> raise TypeError(f'cannot set immutable {name} on {self!r}')",if name in self.__ast_frozen_fields__:,elif not self._immutable:,0.8835907260023776,0.7848518349390632,False
457,"def _check_for_req_data(data): required_args = ['columns'] for arg in required_args: <IF_STMT> return (True, make_json_response(status=400, success=0, errormsg=gettext('Could not find required parameter ({}).').format(arg))) return (False, '')","if arg not in data or (isinstance(data[arg], list) and len(data[arg]) < 1):",if arg not in data:,0.6962600128170551,0.7406093667638122,False
458,"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) for p, _ in ctr.most_common(): w, pos, l = p <IF_STMT> self.composite_dict[w, pos] = l if w not in self.word_dict: self.word_dict[w] = l return","if (w, pos) not in self.composite_dict:",if w not in self.composite_dict:,0.7221806029584987,0.8677319190106252,False
459,"def render(type_, obj, context): if type_ == 'foreign_key': return None if type_ == 'column': if obj.name == 'y': return None <IF_STMT> return False else: return 'col(%s)' % obj.name if type_ == 'type' and isinstance(obj, MySpecialType): context.imports.add('from mypackage import MySpecialType') return 'MySpecialType()' return 'render:%s' % type_",elif obj.name == 'q':,elif obj.name == 'x':,0.8891884725400806,0.8806615362338783,False
460,"def test_knows_when_stepping_back_possible(self): iterator = bidirectional_iterator.BidirectionalIterator([0, 1, 2, 3]) commands = [0, 1, 0, 0, 1, 1, 0, 0, 0, 0] command_count = 0 results = [] for _ in iterator: <IF_STMT> iterator.step_back_on_next_iteration() results.append(iterator.can_step_back()) command_count += 1 assert results == [False, True, False, True, True, True, False, True, True, True]",if commands[command_count]:,if command_count == 0:,0.8741693042931937,0.8902056737869248,False
461,"def flask_debug_true(context): if context.is_module_imported_like('flask'): if context.call_function_name_qual.endswith('.run'): <IF_STMT> return bandit.Issue(severity=bandit.HIGH, confidence=bandit.MEDIUM, text='A Flask app appears to be run with debug=True, which exposes the Werkzeug debugger and allows the execution of arbitrary code.', lineno=context.get_lineno_for_call_arg('debug'))","if context.check_call_arg_value('debug', 'True'):",if context.get_lineno_for_call_arg('debug'):,0.66290393657777,0.8866029039778043,False
462,"def __exit__(self, exc_type, exc_val, exc_tb): if self._should_meta_profile: end_time = timezone.now() exception_raised = exc_type is not None if exception_raised: Logger.error('Exception when performing meta profiling, dumping trace below') traceback.print_exception(exc_type, exc_val, exc_tb) request = getattr(DataCollector().local, 'request', None) <IF_STMT> curr = request.meta_time or 0 request.meta_time = curr + _time_taken(self.start_time, end_time)",if request:,if request:,0.9416996873426919,0.9220450449751959,True
463,"def get_job_offer(ja_list): ja_joff_map = {} offers = frappe.get_all('Job Offer', filters=[['job_applicant', 'IN', ja_list]], fields=['name', 'job_applicant', 'status', 'offer_date', 'designation']) for offer in offers: <IF_STMT> ja_joff_map[offer.job_applicant] = [offer] else: ja_joff_map[offer.job_applicant].append(offer) return ja_joff_map",if offer.job_applicant not in ja_joff_map.keys():,if offer.job_applicant not in ja_joff_map:,0.8798194068496741,0.7909601595885504,False
464,"def _get_deepest(self, t): if isinstance(t, list): <IF_STMT> return t[0] else: for part in t: res = self._get_deepest(part) if res: return res return None return None",if len(t) == 1:,if len(t) == 1:,0.86544704984124,0.7886336751695258,True
465,"def test_main(self): root = os.path.dirname(mutagen.__path__[0]) skip = [os.path.join(root, 'docs'), os.path.join(root, 'venv')] for dirpath, dirnames, filenames in os.walk(root): <IF_STMT> continue for filename in filenames: if filename.endswith('.py'): path = os.path.join(dirpath, filename) self._check_encoding(path)",if any((dirpath.startswith(s + os.sep) or s == dirpath for s in skip)):,if dirpath in skip:,0.8747564145812776,0.8228500218338367,False
466,"def xview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.hsb.get() elif mode == 'moveto': frameWidth = self.innerframe.winfo_reqwidth() self._startX = value * float(frameWidth) else: clipperWidth = self._clipper.winfo_width() <IF_STMT> jump = int(clipperWidth * self._jfraction) else: jump = clipperWidth self._startX = self._startX + value * jump self.reposition()",if units == 'units':,if self._jfraction:,0.7589032079990127,0.9312457603037672,False
467,"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model(DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config='data/test_config/max_hist_config.yml', kwargs={}) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: <IF_STMT> if type(policy) == FormPolicy: assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5","if hasattr(policy.featurizer, 'max_history'):",if policy.featurizer:,0.9060426758024827,0.8743414417652072,False
468,"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base <IF_STMT> compare_entry = compare_entry.lower() compare_base = compare_base.lower() if self.compare_entries(compare_entry, compare_base): if entry not in sugg: sugg.append(entry) return sugg",if self.settings.get(IGNORE_CASE_SETTING):,"if isinstance(compare_entry, str):",0.7863287514316958,0.8713933650206428,False
469,"def marker_expr(remaining): if remaining and remaining[0] == '(': result, remaining = marker(remaining[1:].lstrip()) <IF_STMT> raise SyntaxError('unterminated parenthesis: %s' % remaining) remaining = remaining[1:].lstrip() else: lhs, remaining = marker_var(remaining) while remaining: m = MARKER_OP.match(remaining) if not m: break op = m.groups()[0] remaining = remaining[m.end():] rhs, remaining = marker_var(remaining) lhs = {'op': op, 'lhs': lhs, 'rhs': rhs} result = lhs return (result, remaining)",if remaining[0] != ')':,if remaining[0] == ')':,0.7859947543669186,0.9099929453837925,False
470,"def __repr__(self): """"""Dump the class data in the format of a .netrc file."""""" rep = '' for host in self.hosts.keys(): attrs = self.hosts[host] rep = rep + 'machine ' + host + '\n\tlogin ' + repr(attrs[0]) + '\n' <IF_STMT> rep = rep + 'account ' + repr(attrs[1]) rep = rep + '\tpassword ' + repr(attrs[2]) + '\n' for macro in self.macros.keys(): rep = rep + 'macdef ' + macro + '\n' for line in self.macros[macro]: rep = rep + line rep = rep + '\n' return rep",if attrs[1]:,if attrs[1]:,0.7476161279224351,0.9592693508045687,True
471,"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, 'id') controls_ids = required_key(item, 'controls') <IF_STMT> if controls_ids != 'all': msg = 'Policy {id_} contains invalid controls list {controls}.'.format(id_=id_, controls=str(controls_ids)) raise ValueError(msg) self.policies[id_] = controls_ids","if not isinstance(controls_ids, list):",if id_:,0.6910691516705506,0.8966773400768917,False
472,"def __set__(self, obj, value): if value is not None and self.field._currency_field.null and (not isinstance(value, MONEY_CLASSES + (Decimal,))): raise ValueError('Missing currency value') if isinstance(value, BaseExpression): <IF_STMT> value = self.prepare_value(obj, value.value) elif not isinstance(value, Func): validate_money_expression(obj, value) prepare_expression(value) else: value = self.prepare_value(obj, value) obj.__dict__[self.field.name] = value","if isinstance(value, Value):",if value.value is not None:,0.6582383964376685,0.8591169759078797,False
473,"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): is_list, property_type, is_strong = attributes[0:3] <IF_STMT> if not is_list: children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",if is_strong and property in self._properties:,if property_type == 'object':,0.7212077557160328,0.8474968231198384,False
474,"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction <IF_STMT> start = 0",if start < 0:,if i == start:,0.9120007841655257,0.9128479730518225,False
475,def setup_displace(self): self.displace_mod = None self.displace_strength = 0.02 for mod in self.obj.modifiers: <IF_STMT> self.displace_mod = mod self.displace_strength = mod.strength if not self.displace_mod: bpy.ops.object.modifier_add(type='DISPLACE') self.displace_mod = self.obj.modifiers[-1] self.displace_mod.show_expanded = False self.displace_mod.strength = self.displace_strength self.displace_mod.show_render = False self.displace_mod.show_viewport = False,if mod.type == 'DISPLACE':,"if isinstance(mod, bpy.ops.object.modifier):",0.9286349529842745,0.8815741981066073,False
476,"def set_json_body(cls, request_builder): old_body = request_builder.info.pop('data', {}) if isinstance(old_body, abc.Mapping): body = request_builder.info.setdefault('json', {}) for path in old_body: <IF_STMT> cls._sequence_path_resolver(path, old_body[path], body) else: body[path] = old_body[path] else: request_builder.info.setdefault('json', old_body)","if isinstance(path, tuple):","if isinstance(path, str):",0.8133033578270805,0.8446593249975184,False
477,"def build(opt): dpath = os.path.join(opt['datapath'], 'DBLL') version = None if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):,if build_data.built(dpath):,0.9090542280792658,0.8787142254774354,True
478,"def test_prefix_lm(self): num_tries = 100 original = 'This is a long test with lots of words to see if it works ok.' dataset = tf.data.Dataset.from_tensor_slices({'text': [original] * num_tries}) dataset = prep.prefix_lm(dataset) for data in test_utils.dataset_as_text(dataset): inputs = data['inputs'].replace('prefix: ', '') targets = data['targets'] reconstructed = ''.join(inputs) <IF_STMT> reconstructed += ' ' reconstructed += ''.join(targets) self.assertEqual(reconstructed, original)",if inputs:,if targets:,0.8902187802002827,0.9362597875749384,False
479,"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): if c in legal_indent_chars: leading_ws.append(c) else: break <IF_STMT> self.indchar = c elif c != self.indchar: self.strict_err_or_warn('found mixing of tabs and spaces', inputstring, i) return ''.join(leading_ws)",if self.indchar is None:,if i == 0:,0.7722981469835518,0.8627586293513119,False
480,"def __init__(self, text): self.mappings = {} self.attributes = collections.defaultdict(set) for stanza in _ParseTextProperties(text): processor_id, single_values, multiple_values = self._ParseStanza(stanza) if processor_id is None: continue <IF_STMT> logging.warn('Processor id %s seen twice in %s', processor_id, text) continue self.mappings[processor_id] = single_values for key, value in multiple_values.items(): self.attributes[key].add(value)",if processor_id in self.mappings:,if processor_id in self.mappings:,0.734871640177073,0.8752376177722327,True
481,def __iter__(self): for chunk in self.source: <IF_STMT> self.wait_counter = 0 yield chunk elif self.wait_counter < self.wait_cntr_max: self.wait_counter += 1 else: logger.warning('Data poller has been receiving no data for {} seconds.\nClosing data poller'.format(self.wait_cntr_max * self.poll_period)) break time.sleep(self.poll_period),if chunk is not None:,if self.wait_counter >= self.poll_max:,0.8825635452872729,0.8516228624291206,False
482,"def download(self, prefetch=False): while self.running: try: <IF_STMT> path, start, end = self.prefetch_queue.get(True, 1) else: path, start, end = self.download_queue.get(True, 1) self.download_data(path, start, end) if prefetch: self.prefetch_queue.task_done() else: self.download_queue.task_done() except Queue.Empty: pass",if prefetch:,if prefetch:,0.9107409276316333,0.8827916928185874,True
483,"def process_messages(self, found_files, messages): for message in messages: <IF_STMT> message.to_absolute_path(self.config.workdir) else: message.to_relative_path(self.config.workdir) if self.config.blending: messages = blender.blend(messages) filepaths = found_files.iter_module_paths(abspath=False) return postfilter.filter_messages(filepaths, self.config.workdir, messages)",if self.config.absolute_paths:,if self.config.absolute:,0.8825653956394237,0.8466657105524215,False
484,"def set_indentation_params(self, ispythonsource, guess=1): if guess and ispythonsource: i = self.guess_indent() <IF_STMT> self.indentwidth = i if self.indentwidth != self.tabwidth: self.usetabs = 0 self.editwin.set_tabwidth(self.tabwidth)",if 2 <= i <= 8:,if i > 0:,0.7121965669136885,0.7709002428237395,False
485,"def to_tree(self, tagname=None, value=None, namespace=None): namespace = getattr(self, 'namespace', namespace) if value is not None: <IF_STMT> tagname = '{%s}%s' % (namespace, tagname) el = Element(tagname) el.text = safe_string(value) return el",if namespace is not None:,if namespace:,0.9043786697284617,0.8787142254774354,False
486,"def execute(self, argv: List) -> bool: if not argv: print('ERROR: You must give at least one module to download.') return False for _arg in argv: result = module_server.search_module(_arg) CacheUpdater('hub_download', _arg).start() <IF_STMT> url = result[0]['url'] with log.ProgressBar('Download {}'.format(url)) as bar: for file, ds, ts in utils.download_with_progress(url): bar.update(float(ds) / ts) else: print('ERROR: Could not find a HubModule named {}'.format(_arg)) return True",if result:,if result:,0.9144286123322454,0.9395648330058336,True
487,"def visit_type_type(self, t: TypeType) -> ProperType: if isinstance(self.s, TypeType): typ = self.meet(t.item, self.s.item) <IF_STMT> typ = TypeType.make_normalized(typ, line=t.line) return typ elif isinstance(self.s, Instance) and self.s.type.fullname == 'builtins.type': return t elif isinstance(self.s, CallableType): return self.meet(t, self.s) else: return self.default(self.s)","if not isinstance(typ, NoneType):",if typ.fullname == 'builtins.type':,0.7918008352277339,0.8592377270804451,False
488,"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.name()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:,if len(items) > 1:,0.7413153949647944,0.760856626273165,True
489,"def get_icon(self): if self.icon is not None: <IF_STMT> try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass icon_name, extension = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() if theme.has_icon(icon_name): return theme.load_icon(icon_name, 24, 0)",if os.path.exists(self.icon):,if self.icon.endswith('.png'):,0.9030765768272055,0.8827916928185874,False
490,"def setup_logger(): """"""Set up logger and add stdout handler"""""" logging.setLoggerClass(IPDLogger) logger = logging.getLogger('icloudpd') has_stdout_handler = False for handler in logger.handlers: <IF_STMT> has_stdout_handler = True if not has_stdout_handler: formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S') stdout_handler = logging.StreamHandler(stream=sys.stdout) stdout_handler.setFormatter(formatter) stdout_handler.name = 'stdoutLogger' logger.addHandler(stdout_handler) return logger",if handler.name == 'stdoutLogger':,if handler.name == 'stdout':,0.8694078125008337,0.8780099567239787,False
491,"def process_extra_fields(self): if self.instance.pk is not None: if self.cleaned_data.get('initialize', None): self.instance.initialize() <IF_STMT> self.instance.update_from_templates()","if self.cleaned_data.get('update', None) or not self.instance.stores.count():","elif self.cleaned_data.get('update_from_templates', None):",0.6006905090710115,0.7166258375282707,False
492,"def testFunctions(self): from zim.formats.wiki import match_url, is_url for input, input_is_url, tail in self.examples: if input_is_url: <IF_STMT> self.assertEqual(match_url(input), input[:-len(tail)]) self.assertFalse(is_url(input)) else: self.assertEqual(match_url(input), input) self.assertTrue(is_url(input)) else: self.assertEqual(match_url(input), None) self.assertFalse(is_url(input))",if tail:,if tail:,0.9114841748225508,0.8645707301556367,True
493,"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings['room']['name'] if 'room' in settings else None file_ = settings['file'] if 'file' in settings else None <IF_STMT> if 'joined' in settings['event']: self._client.userlist.addUser(username, room, file_) elif 'left' in settings['event']: self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",if 'event' in settings:,if username:,0.6628048332359263,0.9253742688467129,False
494,"def restoreTerminals(self, state): for name in list(self.terminals.keys()): <IF_STMT> self.removeTerminal(name) for name, opts in state.items(): if name in self.terminals: term = self[name] term.setOpts(**opts) continue try: opts = strDict(opts) self.addTerminal(name, **opts) except: printExc('Error restoring terminal %s (%s):' % (str(name), str(opts)))",if name not in state:,if name in state:,0.9214900261977287,0.8592377270804451,False
495,"def htmlify(path, text): fname = os.path.basename(path) if any((fnmatch.fnmatchcase(fname, p) for p in _patterns)): sql = 'SELECT files.id FROM files WHERE path = ? LIMIT 1' row = _conn.execute(sql, (path,)).fetchone() <IF_STMT> return ClangHtmlifier(_tree, _conn, path, text, row[0]) return None",if row:,if row:,0.6917142657330376,0.9051034981560222,True
496,"def autoformat_filter_conv2d(fsize, in_depth, out_depth): if isinstance(fsize, int): return [fsize, fsize, in_depth, out_depth] elif isinstance(fsize, (tuple, list, tf.TensorShape)): <IF_STMT> return [fsize[0], fsize[1], in_depth, out_depth] else: raise Exception('filter length error: ' + str(len(fsize)) + ', only a length of 2 is supported.') else: raise Exception('filter format error: ' + str(type(fsize)))",if len(fsize) == 2:,if len(fsize) == 2:,0.6638142404943476,0.8879659171421962,True
497,def _rle_encode(string): new = b'' count = 0 for cur in string: <IF_STMT> count += 1 else: if count: new += b'\x00' + bytes([count]) count = 0 new += bytes([cur]) return new,if not cur:,if cur == b'\x00':,0.684595195375601,0.8336104423443033,False
498,"def is_clean(self): acceptable_statuses = {'external', 'unversioned'} root = self._capture_output('status', '--quiet') for elem in root.findall('./target/entry'): status = elem.find('./wc-status') <IF_STMT> continue log.debug('Path %s is %s', elem.get('path'), status.get('item')) return False return True","if status.get('item', None) in acceptable_statuses:",if status.get('status') not in acceptable_statuses:,0.7251893286168325,0.7909601595885504,False
499,"def process(self, body, message): try: <IF_STMT> raise TypeError('Received an unexpected type ""%s"" for payload.' % type(body)) response = self._handler.pre_ack_process(body) self._dispatcher.dispatch(self._process_message, response) except: LOG.exception('%s failed to process message: %s', self.__class__.__name__, body) finally: message.ack()","if not isinstance(body, self._handler.message_type):","if not isinstance(body, (bytes, bytes)):",0.8901182862432788,0.8094220211349227,False
500,"def page_file(self, page): try: page = self.notebook.get_page(page) <IF_STMT> return page.source else: return None except PageNotFoundError: return None","if hasattr(page, 'source') and isinstance(page.source, File):",if page.source:,0.522546731028657,0.7778111223054219,False
501,"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): <IF_STMT> pass elif silhouette <= best_silhouette: break best_silhouette = silhouette best_a = a best_k = k return (best_a, best_silhouette, best_k)",if best_silhouette is None:,if a == best_a:,0.8625654713147319,0.8592377270804451,False
502,"def _cancel_tasks_for_partitions(self, to_cancel_partitions): with self._lock: _LOGGER.debug('EventProcessor %r tries to cancel partitions %r', self._id, to_cancel_partitions) for partition_id in to_cancel_partitions: <IF_STMT> self._consumers[partition_id].stop = True _LOGGER.info('EventProcessor %r has cancelled partition %r', self._id, partition_id)",if partition_id in self._consumers:,if partition_id in self._consumers:,0.8417340169420292,0.8228500218338367,True
503,"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): <IF_STMT> result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) if refine: solid = solid.removeSplitter() return solid",if result is None:,if source == self._source:,0.916907148052107,0.8832000938217648,False
504,"def geli_detach(self, pool, clear=False): failed = 0 for ed in self.middleware.call_sync('datastore.query', 'storage.encrypteddisk', [('encrypted_volume', '=', pool['id'])]): dev = ed['encrypted_provider'] try: self.geli_detach_single(dev) except Exception as ee: self.logger.warn(str(ee)) failed += 1 <IF_STMT> try: self.geli_clear(dev) except Exception as e: self.logger.warn('Failed to clear %s: %s', dev, e) return failed",if clear:,if clear:,0.9157332228364196,0.9184043388013005,True
505,def compute_lengths(batch_sizes): tmp_batch_sizes = np.copy(batch_sizes) lengths = [] while True: c = np.count_nonzero(tmp_batch_sizes > 0) <IF_STMT> break lengths.append(c) tmp_batch_sizes = np.array([b - 1 for b in tmp_batch_sizes]) return np.array(lengths),if c == 0:,if c == 0:,0.7105781161327455,0.8169276475307028,True
506,"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: <IF_STMT> flatten_items.append(b'') elif isinstance(item, bytes): flatten_items.append(item) elif isinstance(item, int): flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b'\n'.join(flatten_items)",if item is None:,"if isinstance(item, b'\x00'):",0.8822037848478712,0.8196189957582152,False
507,"def update(self, new_config): jsonschema.validate(new_config, self.schema) config = {} for k, v in new_config.items(): <IF_STMT> config[k] = self[k] else: config[k] = v self._config = config self.changed()","if k in self.schema.get('secret', []) and v == SECRET_PLACEHOLDER:",if k in self:,0.8629957724448244,0.7886336751695258,False
508,"def _encode_numpy(values, uniques=None, encode=False, check_unknown=True): if uniques is None: if encode: uniques, encoded = np.unique(values, return_inverse=True) return (uniques, encoded) else: return np.unique(values) if encode: <IF_STMT> diff = _encode_check_unknown(values, uniques) if diff: raise ValueError('y contains previously unseen labels: %s' % str(diff)) encoded = np.searchsorted(uniques, values) return (uniques, encoded) else: return uniques",if check_unknown:,if check_unknown:,0.7994373099446385,0.9284304001296656,True
509,"def restore_dtype_and_merge(arr, input_dtype): if isinstance(arr, list): arr = [restore_dtype_and_merge(arr_i, input_dtype) for arr_i in arr] shapes = [arr_i.shape for arr_i in arr] <IF_STMT> arr = np.array(arr) if ia.is_np_array(arr): arr = iadt.restore_dtypes_(arr, input_dtype) return arr",if len(set(shapes)) == 1:,"if not isinstance(arr, np.ndarray):",0.6878768242699353,0.8385130047130208,False
510,"def proc_minute(d): if expanded[0][0] != '*': diff_min = nearest_diff_method(d.minute, expanded[0], 60) if diff_min is not None and diff_min != 0: <IF_STMT> d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return (True, d) return (False, d)",if is_prev:,if expanded[0][0] == '*':,0.660939649539441,0.8516228624291206,False
511,"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): <IF_STMT> self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","if isinstance(v, dict):","if isinstance(v, dict):",0.917732615127831,0.9118021019905903,True
512,"def __createItemAttribute(self, item, function, preload): """"""Create the new widget, add it, and remove the old one"""""" try: self.__stack.addWidget(function(item, preload)) <IF_STMT> oldWidget = self.__stack.widget(0) self.__stack.removeWidget(oldWidget) oldWidget.setParent(QtWidgets.QWidget()) except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if self.__stack.count() > 1:,if preload:,0.9060540755806095,0.8827916928185874,False
513,"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: <IF_STMT> url = url[8:] if not url.startswith('http://'): url = 'http://' + url if playlist: download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if url.startswith('https://'):,if url.startswith('http://'):,0.9086428790608074,0.8966773400768917,False
514,"def add_enc_zero(obj, enc_zero): if isinstance(obj, np.ndarray): return obj + enc_zero elif isinstance(obj, Iterable): return type(obj)((EncryptModeCalculator.add_enc_zero(o, enc_zero) <IF_STMT> else o + enc_zero for o in obj)) else: return obj + enc_zero","if isinstance(o, Iterable)",if o is not None,0.8686248153502997,0.7975010608178975,False
515,"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[w, pos] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred <IF_STMT> lemma = w lemmas.append(lemma) return lemmas",if lemma is None:,if lemma is None:,0.9155047093396158,0.9019629427251674,True
516,"def replace_to_6hex(color): """"""Validate and replace 3hex colors to 6hex ones."""""" if match('^#(?:[0-9a-fA-F]{3}){1,2}$', color): <IF_STMT> color = '#{0}{0}{1}{1}{2}{2}'.format(color[1], color[2], color[3]) return color else: exit(_('Invalid color {}').format(color))",if len(color) == 4:,if color[0] == '#':,0.6186527428616757,0.7886336751695258,False
517,"def computeMachineName(self): """"""Return the name of the current machine, i.e, HOSTNAME."""""" try: import os name = os.getenv('HOSTNAME') <IF_STMT> name = os.getenv('COMPUTERNAME') if not name: import socket name = socket.gethostname() except Exception: name = '' return name",if not name:,if not name:,0.7550463717321602,0.8749766281017177,True
518,"def _git_dirty_working_directory(q, include_untracked): try: cmd = ['git', 'status', '--porcelain'] if include_untracked: cmd += ['--untracked-files=normal'] else: cmd += ['--untracked-files=no'] status = _run_git_cmd(cmd) <IF_STMT> q.put(bool(status)) else: q.put(None) except (subprocess.CalledProcessError, OSError, FileNotFoundError): q.put(None)",if status is not None:,if status:,0.6437132902970765,0.8787142254774354,False
519,"def runAndWaitWork(server, work): work.touch() thr = threading.Thread(target=workThread, args=(server, work)) thr.setDaemon(True) thr.start() while True: if work.isTimedOut(): break if not thr.isAlive(): break <IF_STMT> break time.sleep(2)",if sys.stdin.closed:,if thr.isAlive():,0.8941247736278058,0.8524094630854673,False
520,"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) if ignore_timeouts and is_timeout(e): return [] <IF_STMT> return [] raise",if ignore_non_errors and is_noerr(e):,if ignore_non_errors and is_non_errors(e):,0.8523517282162696,0.8516228624291206,False
521,"def PrintHeader(self): if self.draw == False: return for val in self.parent.header: self.SetPrintFont(val['Font']) header_indent = val['Indent'] * self.pwidth text = val['Text'] htype = val['Type'] <IF_STMT> addtext = self.GetDate() elif htype == 'Date & Time': addtext = self.GetDateTime() else: addtext = '' self.OutTextPageWidth(text + addtext, self.pheader_margin, val['Align'], header_indent, True)",if htype == 'Date':,if htype == 'Date & Date':,0.8266619805495609,0.8505405945991492,False
522,"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): if result is None: result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) <IF_STMT> solid = solid.removeSplitter() return solid",if refine:,if refine:,0.82147486690596,0.9253742688467129,True
523,"def captured_updateNode(self, context): if not self.updating_name_from_pointer: font_datablock = self.get_bpy_data_from_name(self.fontname, bpy.data.fonts) <IF_STMT> self.font_pointer = font_datablock updateNode(self, context)",if font_datablock:,if font_datablock:,0.63200699807015,0.762465858623486,True
524,"def __add__(self, other): if isinstance(other, Vector2): <IF_STMT> _class = Vector2 else: _class = Point2 return _class(self.x + other.x, self.y + other.y) else: assert hasattr(other, '__len__') and len(other) == 2 return Vector2(self.x + other[0], self.y + other[1])",if self.__class__ is other.__class__:,if other.x == self.y:,0.7433354368183014,0.8516228624291206,False
525,"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: <IF_STMT> setting_values.update(self._flatten_settings_from_form(settings, field, form_values[field._name])) elif field._name in settings: setting_values[field._name] = form_values[field._name] return setting_values","if isinstance(field, _ContainerMixin):",if field._name in form_values:,0.7248642057540062,0.8555308664663046,False
526,"def add_include_dirs(self, args): ids = [] for a in args: if hasattr(a, 'includedirs'): a = a.includedirs <IF_STMT> raise InvalidArguments('Include directory to be added is not an include directory object.') ids.append(a) self.include_dirs += ids","if not isinstance(a, IncludeDirs):","if not isinstance(a, (list, tuple)):",0.6756184071076545,0.8148691130388024,False
527,"def _clip_array(array, config): if 'threshold' in config.keys(): threshold = config['threshold'] else: abs_array = np.max(np.abs(array)) <IF_STMT> return array threshold = np.percentile(np.abs(array), 99.99) return np.clip(array, -threshold, threshold)",if abs_array < 1.0:,if abs_array == 0:,0.7259485772383375,0.7886336751695258,False
528,def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: <IF_STMT> yield from dfs(w) elif w not in identified: while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v]:]) del stack[index[v]:] identified.update(scc) yield scc,if w not in index:,if w in identified:,0.8117991567154876,0.8692960007731574,False
529,"def create_balancer(self, name, members, protocol='http', port=80, algorithm=DEFAULT_ALGORITHM): balancer = self.ex_create_balancer_nowait(name, members, protocol, port, algorithm) timeout = 60 * 20 waittime = 0 interval = 2 * 15 if balancer.id is not None: return balancer else: while waittime < timeout: balancers = self.list_balancers() for i in balancers: <IF_STMT> return i waittime += interval time.sleep(interval) raise Exception('Failed to get id')",if i.name == balancer.name and i.id is not None:,if i.id is not None:,0.8622827704683926,0.8919483389981387,False
530,"def handle(self, scope: Scope, receive: Receive, send: Send) -> None: if self.methods and scope['method'] not in self.methods: <IF_STMT> raise HTTPException(status_code=405) else: response = PlainTextResponse('Method Not Allowed', status_code=405) await response(scope, receive, send) else: await self.app(scope, receive, send)",if 'app' in scope:,if self.http_error:,0.9214097187114164,0.8996480074924822,False
531,"def convert(data): result = [] for d in data: if isinstance(d, tuple) and len(d) == 2: result.append((d[0], None, d[1])) <IF_STMT> result.append(d) return result","elif isinstance(d, basestring):","elif isinstance(d, list):",0.8732825659903019,0.80377750806414,False
532,"def register_adapters(): global adapters_registered if adapters_registered is True: return try: import pkg_resources packageDir = pkg_resources.resource_filename('pyamf', 'adapters') except: packageDir = os.path.dirname(__file__) for f in glob.glob(os.path.join(packageDir, '*.py')): mod = os.path.basename(f).split(os.path.extsep, 1)[0] <IF_STMT> continue try: register_adapter(mod[1:].replace('_', '.'), PackageImporter(mod)) except ImportError: pass adapters_registered = True",if mod == '__init__' or not mod.startswith('_'):,if mod == 'adapters.py':,0.7971244170407479,0.8692960007731574,False
533,"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue if hasattr(module, 'aliases'): for alias in module.aliases(): <IF_STMT> modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()",if alias not in excluded_aliases:,if alias not in excluded_aliases:,0.866417897641292,0.88627064388393,True
534,"def clean_items(event, items, variations): for item in items: <IF_STMT> raise ValidationError(_('One or more items do not belong to this event.')) if item.has_variations: if not any((var.item == item for var in variations)): raise ValidationError(_('One or more items has variations but none of these are in the variations list.'))",if event != item.event:,if not any((event.item == item for item in variations)):,0.8147966551874455,0.8028252659413964,False
535,"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: <IF_STMT> return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",if idx == num:,"if not isinstance(element, int):",0.8056228430048267,0.8592377270804451,False
536,"def check(chip, xeddb, chipdb): all_inst = [] undoc = [] for inst in xeddb.recs: <IF_STMT> if inst.undocumented: undoc.append(inst) else: all_inst.append(inst) return (all_inst, undoc)",if inst.isa_set in chipdb[chip]:,if inst.id == chipdb.id:,0.8686907777228059,0.7709002428237395,False
537,"def get_all_topic_src_files(self): """"""Retrieves the file paths of all the topics in directory"""""" topic_full_paths = [] topic_names = os.listdir(self.topic_dir) for topic_name in topic_names: <IF_STMT> topic_full_path = os.path.join(self.topic_dir, topic_name) if topic_full_path != self.index_file: topic_full_paths.append(topic_full_path) return topic_full_paths",if not topic_name.startswith('.'):,if topic_name.endswith('.src'):,0.8767305920260197,0.8935248372106969,False
538,"def _get_element(dom_msi, tag_name, name=None, id_=None): """"""Get a xml element defined on Product."""""" product = dom_msi.getElementsByTagName('Product')[0] elements = product.getElementsByTagName(tag_name) for element in elements: <IF_STMT> if element.getAttribute('Name') == name and element.getAttribute('Id') == id_: return element elif id_: if element.getAttribute('Id') == id_: return element",if name and id_:,if name:,0.923371083435123,0.9122561819614461,False
539,"def __init__(self, *models): super().__init__() self.models = ModuleList(models) for m in models: <IF_STMT> raise ValueError('IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)') self.likelihood = LikelihoodList(*[m.likelihood for m in models])","if not hasattr(m, 'likelihood'):",if m.likelihood is None:,0.849834487191665,0.828399516355805,False
540,"def _sniff(filename, oxlitype): try: with open(filename, 'rb') as fileobj: header = fileobj.read(4) <IF_STMT> fileobj.read(1) ftype = fileobj.read(1) if binascii.hexlify(ftype) == oxlitype: return True return False except OSError: return False",if header == b'OXLI':,if header == b'SNIFF':,0.7915941497562136,0.8169276475307028,False
541,"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) if '/' not in key: key += '/tcp' <IF_STMT> result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result","if isinstance(v, list):","if isinstance(v, list):",0.7003166674669852,0.8713933650206428,True
542,"def input_data(self): gen = self.config.generator if gen and (not self.config['out'] or not self.config['in']): <IF_STMT> self._run_generator(gen, args=self.config.generator_args) if self._generated[0]: return self._generated[0] return self._normalize(self.problem.problem_data[self.config['in']]) if self.config['in'] else b''",if self._generated is None:,if self.config['generator']:,0.9169440756560195,0.8590888738245122,False
543,"def __new__(cls, *tasks, **kwargs): if not kwargs and tasks: <IF_STMT> tasks = tasks[0] if len(tasks) == 1 else tasks return reduce(operator.or_, tasks) return super(chain, cls).__new__(cls, *tasks, **kwargs)",if len(tasks) != 1 or is_list(tasks[0]):,"if isinstance(tasks, list):",0.8311258453893888,0.833078701050083,False
544,"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists('file_sources.json'): file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) <IF_STMT> ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if file_sources is None:,elif file_sources is None:,0.7218007096466046,0.8752376177722327,False
545,"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) <IF_STMT> self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE if c == curr: self._colourSelection = i",if c.IsOk():,if c == curr:,0.8088543330681196,0.8555308664663046,False
546,"def convert_obj_into_marshallable(self, obj): if isinstance(obj, self.marshalable_types): return obj if isinstance(obj, array.array): if obj.typecode == 'c': return obj.tostring() <IF_STMT> return obj.tounicode() return obj.tolist() return self.class_to_dict(obj)",if obj.typecode == 'u':,elif obj.typecode == 'b':,0.8417988681560242,0.7801270245332924,False
547,"def run(self): self.run_command('egg_info') from glob import glob for pattern in self.match: pattern = self.distribution.get_name() + '*' + pattern files = glob(os.path.join(self.dist_dir, pattern)) files = [(os.path.getmtime(f), f) for f in files] files.sort() files.reverse() log.info('%d file(s) matching %s', len(files), pattern) files = files[self.keep:] for t, f in files: log.info('Deleting %s', f) <IF_STMT> os.unlink(f)",if not self.dry_run:,if os.path.exists(f):,0.9302125930607512,0.9453264985232607,False
548,"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: if token.token_type == TOKEN_TEXT: result.append(token.contents.replace('%', '%%')) <IF_STMT> result.append('%%(%s)s' % token.contents) vars.append(token.contents) return (''.join(result), vars)",elif token.token_type == TOKEN_VAR:,elif token.token_type == TOKEN_VAR:,0.8784220062836683,0.8038019482772603,True
549,"def _handle_raise(self, values, is_NAs, origins): for is_NA, origin in zip(is_NAs, origins): <IF_STMT> msg = 'Missing values detected. If you want rows with missing values to be automatically deleted in a list-wise manner (not recommended), please set dropna=True in the Bambi Model initialization.' raise PatsyError(msg, origin) return values",if np.any(is_NA):,if is_NA:,0.7376714638743336,0.9237460349978159,False
550,"def add_node_data(node_array, ntwk): node_ntwk = nx.Graph() newdata = {} for idx, data in ntwk.nodes(data=True): <IF_STMT> newdata['value'] = node_array[int(idx) - 1] data.update(newdata) node_ntwk.add_node(int(idx), **data) return node_ntwk",if not int(idx) == 0:,if int(idx) > 0:,0.8885235635303688,0.7886336751695258,False
551,"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: if ';' in date_hdr: date_hdr = date_hdr.split(';')[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) <IF_STMT> return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",if msg_ts > time.time() + 24 * 3600 or msg_ts < 1:,if msg_ts < 0:,0.8723865710779773,0.8474968231198384,False
552,"def _route_db(self, model, **hints): chosen_db = None for router in self.routers: try: method = getattr(router, action) except AttributeError: pass else: chosen_db = method(model, **hints) <IF_STMT> return chosen_db try: return hints['instance']._state.db or DEFAULT_DB_ALIAS except KeyError: return DEFAULT_DB_ALIAS",if chosen_db:,if chosen_db:,0.8359088356165287,0.8996480074924822,True
553,"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): if not ignore_first_level: keys = [x.split('(')[0] for x in struct.keys()] res.extend(keys) for key in struct: <IF_STMT> logging.debug('Ignored: %s: %s', key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",if key in IGNORED_KEYS:,if key in IGNORED_FIRST_LEVEL:,0.910343913696682,0.8806615362338783,False
554,"def launch_app(self, fs_id): if fs_id in self.app_infos: row = self.get_row_by_fsid(fs_id) <IF_STMT> return app_info = self.app_infos[fs_id] filepath = os.path.join(row[SAVEDIR_COL], row[SAVENAME_COL]) gfile = Gio.File.new_for_path(filepath) app_info.launch([gfile], None) self.app_infos.pop(fs_id, None)",if not row:,if not row:,0.6313111633967297,0.8266114125804572,True
555,"def create_skipfile(files_changed, skipfile): json_pattern = re.compile('^\\{.*\\}') for line in files_changed.readlines(): <IF_STMT> for filename in json.loads(line): if '/COMMIT_MSG' in filename: continue skipfile.write('+*/%s\n' % filename) skipfile.write('-*\n')","if re.match(json_pattern, line):","if re.search(line, json_pattern):",0.8810214052220015,0.8120341702859789,False
556,"def zscore(self, client, request, N): check_input(request, N != 2) key = request[1] db = client.db value = db.get(key) if value is None: client.reply_bulk(None) elif not isinstance(value, self.zset_type): client.reply_wrongtype() else: score = value.score(request[2], None) <IF_STMT> score = str(score).encode('utf-8') client.reply_bulk(score)",if score is not None:,"if isinstance(score, bytes):",0.7421449841385394,0.8815741981066073,False
557,"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) elif isinstance(test, unittest.TestCase): <IF_STMT> print(test.id())",if support.match_test(test):,if test.id() != 'test':,0.5959029991035779,0.7082593633017147,False
558,"def Run(self): """"""The main run method of the client."""""" for thread in self._threads.values(): thread.start() logging.info(START_STRING) while True: dead_threads = [tn for tn, t in self._threads.items() if not t.isAlive()] <IF_STMT> raise FatalError('These threads are dead: %r. Shutting down...' % dead_threads) time.sleep(10)",if dead_threads:,if dead_threads:,0.8278670865569544,0.9099951253570094,True
559,"def _slice_queryset(queryset, order_by, per_page, start): page_len = int(per_page) + 1 if start: <IF_STMT> filter_name = '%s__lte' % order_by[1:] else: filter_name = '%s__gte' % order_by return queryset.filter(**{filter_name: start})[:page_len] return queryset[:page_len]",if order_by.startswith('-'):,if order_by.startswith('__lte'):,0.659168481421889,0.8743414417652072,False
560,"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 <IF_STMT> break else: dt = t2 - previous if dt <= 0.0: continue if precision is not None: precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",if 0 < dt:,if dt <= 0.0:,0.7310996995223393,0.9257921357402699,False
561,"def findWorkingDir(): frozen = getattr(sys, 'frozen', '') if not frozen: path = os.path.dirname(__file__) elif frozen in ('dll', 'console_exe', 'windows_exe', 'macosx_app'): path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))) elif frozen: <IF_STMT> path = getattr(sys, '_MEIPASS', '') else: path = os.path.dirname(sys.executable) else: path = '' return path","if getattr(sys, '_MEIPASS', '') is not None:",if sys.platform == 'win32':,0.6566183400964947,0.8692960007731574,False
562,"def CreateDataType(vmodlName, wsdlName, parent, version, props): with _lazyLock: dic = [vmodlName, wsdlName, parent, version, props] names = vmodlName.split('.') <IF_STMT> vmodlName = '.'.join((name[0].lower() + name[1:] for name in names)) _AddToDependencyMap(names) typeNs = GetWsdlNamespace(version) _dataDefMap[vmodlName] = dic _wsdlDefMap[typeNs, wsdlName] = dic _wsdlTypeMapNSs.add(typeNs)",if _allowCapitalizedNames:,if len(names) > 1:,0.7937047253032911,0.8661072626070159,False
563,"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f'Unexpected response type: `{type(response)}`') if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) <IF_STMT> yield rdf_client.User(username=username, homedir=homedir)",if username not in self._ignore_users:,if username in knowledge_base.username_names:,0.8623171716540752,0.8385130047130208,False
564,def process_question(qtxt): question = '' skip = False for letter in qtxt: if letter == '<': skip = True if letter == '>': skip = False if skip: continue <IF_STMT> if letter == ' ': letter = '_' question += letter.lower() return question,if letter.isalnum() or letter == ' ':,if not skip:,0.7975474385900831,0.8953711787948615,False
565,"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: if gap: self.write('') self.process(line) <IF_STMT> gap = True return 0",if not is_command(line):,if line.strip() == '\n':,0.6723961000216925,0.7886336751695258,False
566,"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() if time_now - record['update'] > self.ttl: record = None except KeyError: record = None <IF_STMT> record = {'r': 'unknown', 'dns': {}, 'g': 1, 'query_count': 0} return record",if not record:,if record is None:,0.6354573674034966,0.8627586293513119,False
567,"def gen_constant_folding(cw): types = ['Int32', 'Double', 'BigInteger', 'Complex'] for cur_type in types: cw.enter_block('if (constLeft.Value.GetType() == typeof(%s))' % (cur_type,)) cw.enter_block('switch (_op)') for op in ops: gen = getattr(op, 'genConstantFolding', None) <IF_STMT> gen(cw, cur_type) cw.exit_block() cw.exit_block()",if gen is not None:,if gen is not None:,0.8154607856830722,0.8200123297196334,True
568,"def unreferenced_dummy(self): for g, base in zip(self.evgroups, self.evbases): for ind, j in enumerate(g): <IF_STMT> debug_print('replacing unreferenced %d %s with dummy' % (base + ind, g[ind])) g[ind] = 'dummy' self.evnum[base + ind] = 'dummy'",if not self.indexobj[base + ind]:,if j == base + ind:,0.8419138692618253,0.7925226565645306,False
569,"def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]: for cls in self.__class__.__mro__: <IF_STMT> warnings.warn('PyDecoratorMixin is deprecated. Please check the implementation of %s' % cls, RemovedInSphinx50Warning, stacklevel=2) break else: warnings.warn('PyDecoratorMixin is deprecated', RemovedInSphinx50Warning, stacklevel=2) ret = super().handle_signature(sig, signode) signode.insert(0, addnodes.desc_addname('@', '@')) return ret",if cls.__name__ != 'DirectiveAdapter':,if cls.__name__ == sig:,0.8901868030276917,0.8752376177722327,False
570,"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get('next', {}).get('url', None) <IF_STMT> break n += 1 if n > max_next: vd.warning(f'stopping at max {max_next} pages') break vd.status(f'fetching next page from {src}') response = requests.get(src, stream=True)",if not src:,if src is None:,0.9396846513890733,0.8806615362338783,False
571,"def ordered_indices(self): with data_utils.numpy_seed(self.seed, self.epoch): indices = [np.random.permutation(len(dataset)) for dataset in self.datasets.values()] counters = [0 for _ in self.datasets] sampled_indices = [self._sample(indices, counters) for _ in range(self.total_num_instances)] <IF_STMT> sampled_indices.sort(key=lambda i: self.num_tokens(i)) return np.array(sampled_indices, dtype=np.int64)",if self.sort_indices:,if self.sort_tokens:,0.8670448840727478,0.8935248372106969,False
572,"def _build_columns(self): self.columns = [Column() for col in self.keys] for row in self: for col_idx, col_val in enumerate(row): col = self.columns[col_idx] col.append(col_val) <IF_STMT> col.is_quantity = False for idx, key_name in enumerate(self.keys): self.columns[idx].name = key_name self.x = Column() self.ys = []",if col_val is not None and (not is_quantity(col_val)):,if col.is_quantity:,0.7654363805667952,0.9099951253570094,False
573,"def tearDown(self): subprocess_list = self.subprocess_list processes = subprocess_list.processes self.schedule.reset() del self.schedule for proc in processes: <IF_STMT> terminate_process(proc.pid, kill_children=True, slow_stop=True) subprocess_list.cleanup() processes = subprocess_list.processes if processes: for proc in processes: if proc.is_alive(): terminate_process(proc.pid, kill_children=True, slow_stop=False) subprocess_list.cleanup() processes = subprocess_list.processes if processes: log.warning('Processes left running: %s', processes)",if proc.is_alive():,if proc.is_alive():,0.8787831373918591,0.9202663016973823,True
574,"def colorNetwork(cls, network, nodesInNetwork, nodeByID=None): for node in nodesInNetwork: node.use_custom_color = True neededCopies = sum((socket.execution.neededCopies for socket in node.outputs)) <IF_STMT> color = (0.7, 0.9, 0.7) else: color = (1.0, 0.3, 0.3) node.color = color",if neededCopies == 0:,if neededCopies == 0:,0.8200539108381617,0.8431339019329497,True
575,"def _init_warmup_scheduler(self, optimizer, states): updates_so_far = states.get('number_training_updates', 0) if self.warmup_updates > 0 and (updates_so_far <= self.warmup_updates or self.hard_reset): self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, self._warmup_lr) <IF_STMT> self.warmup_scheduler.load_state_dict(states['warmup_scheduler']) else: self.warmup_scheduler = None",if states.get('warmup_scheduler'):,if 'warmup_scheduler' in states:,0.6323663959440343,0.8105932471967202,False
576,"def inner(self, *iargs, **ikwargs): try: return getattr(super(VEXResilienceMixin, self), func)(*iargs, **ikwargs) except excs as e: for exc, handler in zip(excs, handlers): if isinstance(e, exc): v = getattr(self, handler)(*iargs, **ikwargs) <IF_STMT> raise return v assert False, 'this should be unreachable if Python is working correctly'",if v is raiseme:,if v is None:,0.8706426314630091,0.8752376177722327,False
577,"def unwrap_envelope(self, data, many): if many: if data['items']: <IF_STMT> self.context['total'] = len(data) return data else: self.context['total'] = data['total'] else: self.context['total'] = 0 data = {'items': []} return data['items'] return data","if isinstance(data, InstrumentedList) or isinstance(data, list):",if data['total'] == 0:,0.8041069361871251,0.8228500218338367,False
578,"def __subclasscheck__(self, cls): if self.__origin__ is not None: <IF_STMT> raise TypeError('Parameterized generics cannot be used with class or instance checks') return False if self is Generic: raise TypeError('Class %r cannot be used with class or instance checks' % self) return super().__subclasscheck__(cls)","if sys._getframe(1).f_globals['__name__'] not in ['abc', 'functools']:",if self is Generic:,0.8831073624235397,0.8692960007731574,False
579,"def __init__(self, pyversions, coverage_service): build_matrix = '' for version in pyversions: build_matrix += '\n{},'.format(version <IF_STMT> else 'py{}'.format(''.join(version.split('.')))) coverage_package = '' if coverage_service: coverage_package += '\n{}'.format(coverage_service.package) coverage_package += '\n' super(Tox, self).__init__('tox.ini', TEMPLATE.format(build_matrix=build_matrix, coverage_package=coverage_package))",if version.startswith('pypy'),if version.startswith('py'):,0.8404657853056998,0.8866029039778043,False
580,"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks except AttributeError: tasks = self.tasks if len(tasks): app = tasks[0]._app <IF_STMT> app = body._app return app if app is not None else current_app",if app is None and body is not None:,elif body:,0.7174667881302883,0.9024521756077707,False
581,"def logic(): for v in [True, False, None, 0, True, None, None, 1]: yield clk.posedge xd.next = v <IF_STMT> yd.next = zd.next = None elif v: yd.next = zd.next = 11 else: yd.next = zd.next = 0",if v is None:,if v == 0:,0.621488513876072,0.8555308664663046,False
582,"def run(self): eid = self.start_episode() obs = self.env.reset() while True: <IF_STMT> action = self.env.action_space.sample() self.log_action(eid, obs, action) else: action = self.get_action(eid, obs) obs, reward, done, info = self.env.step(action) self.log_returns(eid, reward, info=info) if done: self.end_episode(eid, obs) obs = self.env.reset() eid = self.start_episode()",if random.random() < self.off_pol_frac:,if self.env.action_space:,0.9082683937536941,0.9122561819614461,False
583,"def tearDown(self): os.chdir(self.orig_working_dir) sys.argv = self.orig_argv sys.stdout = self.orig_stdout sys.stderr = self.orig_stderr for dirname in ['lv_LV', 'ja_JP']: locale_dir = os.path.join(self.datadir, 'project', 'i18n', dirname) <IF_STMT> shutil.rmtree(locale_dir)",if os.path.isdir(locale_dir):,if os.path.exists(locale_dir):,0.8254178938302875,0.886661180281778,False
584,"def sentry_set_scope(process_context, entity, project, email=None, url=None): with sentry_sdk.hub.GLOBAL_HUB.configure_scope() as scope: scope.set_tag('process_context', process_context) scope.set_tag('entity', entity) scope.set_tag('project', project) <IF_STMT> scope.user = {'email': email} if url: scope.set_tag('url', url)",if email:,if email:,0.6784258436276928,0.8531413606256201,True
585,"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) <IF_STMT> continue if c.getYAxis() == Y_AXIS: nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",if not c.isVisible():,if c.getXAxis() == X_AXIS:,0.7982263735393879,0.8661072626070159,False
586,"def handle_starttag(self, tag, attrs): if tag == 'link' and ('rel', 'icon') in attrs or ('rel', 'shortcut icon') in attrs: href = None icon_type = None for attr, value in attrs: if attr == 'href': href = value elif attr == 'type': icon_type = value <IF_STMT> try: mimetype = extension_to_mimetype(href.rpartition('.')[2]) except KeyError: pass else: icon_type = mimetype if icon_type: self.icons.append((href, icon_type))",if href:,elif attr == 'type':,0.8085902306532726,0.9099929453837925,False
587,"def get_version(version_file=STATIC_VERSION_FILE): version_info = get_static_version_info(version_file) version = version_info['version'] if version == '__use_git__': version = get_version_from_git() <IF_STMT> version = get_version_from_git_archive(version_info) if not version: version = Version('unknown', None, None) return pep440_format(version) else: return version",if not version:,elif version == '__use_git_archive__':,0.8415361630497367,0.8336104423443033,False
588,"def _Sleep(self, seconds): if threading.current_thread() is not self._worker_thread: return self._original_sleep(seconds) self._time += seconds self._budget -= seconds while self._budget < 0: self._worker_thread_turn.clear() self._owner_thread_turn.set() self._worker_thread_turn.wait() <IF_STMT> raise FakeTimeline._WorkerThreadExit()",if self._worker_thread_done:,if self._owner_thread_turn.is_set():,0.8297318534144987,0.8701761846085435,False
589,def validate_attributes(self): if not (self.has_variants or self.variant_of): return if not self.variant_based_on: self.variant_based_on = 'Item Attribute' if self.variant_based_on == 'Item Attribute': attributes = [] <IF_STMT> frappe.throw(_('Attribute table is mandatory')) for d in self.attributes: if d.attribute in attributes: frappe.throw(_('Attribute {0} selected multiple times in Attributes Table').format(d.attribute)) else: attributes.append(d.attribute),if not self.attributes:,if not self.attributes:,0.9415531213862599,0.9022045190074797,True
590,"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get('Authorization'): credentails = parse_authorization_header(request.headers.get('Authorization')) if not credentails: return response_hash = response(credentails, passwd, dict(uri=request.script_root + request.path, body=request.data, method=request.method)) <IF_STMT> return True return False",if credentails.get('response') == response_hash:,if response_hash == user:,0.7026051153456384,0.8385130047130208,False
591,"def _get_index_type(return_index_type, ctx): if return_index_type is None: if ctx.running_mode == RunningMode.local: return_index_type = 'object' <IF_STMT> return_index_type = 'filename' else: return_index_type = 'bytes' return return_index_type",elif ctx.running_mode == RunningMode.local_cluster:,elif ctx.running_mode == RunningMode.file:,0.6642802240970447,0.7801270245332924,False
592,"def iter_event_handlers(self, resource: resources_.Resource, event: bodies.RawEvent) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn('SimpleRegistry.iter_event_handlers() is deprecated; use ResourceWatchingRegistry.iter_handlers().', DeprecationWarning) cause = _create_watching_cause(resource, event) for handler in self._handlers: if not isinstance(handler, handlers.ResourceWatchingHandler): pass <IF_STMT> yield handler","elif registries.match(handler=handler, cause=cause, ignore_fields=True):",if handler.cause == cause:,0.8938876870897834,0.8316518087941058,False
593,"def subprocess_post_check(completed_process: subprocess.CompletedProcess, raise_error: bool=True) -> None: if completed_process.returncode: <IF_STMT> print(completed_process.stdout, file=sys.stdout, end='') if completed_process.stderr is not None: print(completed_process.stderr, file=sys.stderr, end='') if raise_error: raise PipxError(f""{' '.join([str(x) for x in completed_process.args])!r} failed"") else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process.stdout is not None:,if completed_process.stdout is not None:,0.9250123724914103,0.8248765135255685,True
594,"def __pow__(self, power): if power == 1: return self if power == -1: from cirq.devices import line_qubit decomposed = protocols.decompose_once_with_qubits(self, qubits=line_qubit.LineQid.for_gate(self), default=None) <IF_STMT> return NotImplemented inverse_decomposed = protocols.inverse(decomposed, None) if inverse_decomposed is None: return NotImplemented return _InverseCompositeGate(self) return NotImplemented",if decomposed is None:,if decomposed is None:,0.8770113091562446,0.8627586293513119,True
595,"def tearDown(self): """"""Close the application after tests"""""" self.old_pos = self.dlg.rectangle self.dlg.menu_select('File->Exit') try: <IF_STMT> self.app.UntitledNotepad[""Do&n't Save""].click() self.app.UntitledNotepad.wait_not('visible') except Exception: pass finally: self.app.kill()","if self.app.UntitledNotepad[""Do&n't Save""].exists():",if self.app.UntitledNotepad.is_visible():,0.6266784011536777,0.8232490471721702,False
596,"def terminate_subprocess(proc, timeout=0.1, log=None): <IF_STMT> if log: log.info('Sending SIGTERM to %r', proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info('Sending SIGKILL to %r', proc) proc.kill() return proc.returncode",if proc.poll() is None:,if proc.poll() is None:,0.9206348117421506,0.8692960007731574,True
597,"def validate(self, detection, expectation): config = SigmaConfiguration() self.basic_rule['detection'] = detection with patch('yaml.safe_load_all', return_value=[self.basic_rule]): parser = SigmaCollectionParser('any sigma io', config, None) backend = SQLiteBackend(config, self.table) assert len(parser.parsers) == 1 for p in parser.parsers: <IF_STMT> self.assertEqual(expectation, backend.generate(p)) elif isinstance(expectation, Exception): self.assertRaises(type(expectation), backend.generate, p)","if isinstance(expectation, str):","if isinstance(expectation, str):",0.9283732265839355,0.8902579342581529,True
598,"def makelist(d): """"""Convert d into a list if all the keys of d are integers."""""" if isinstance(d, dict): <IF_STMT> return [makelist(d[k]) for k in sorted(d, key=int)] else: return web.storage(((k, makelist(v)) for k, v in d.items())) else: return d",if all((isint(k) for k in d)):,"if isinstance(d, list):",0.6000173682753803,0.8815741981066073,False
599,"def __share_local_dir(self, lpath, rpath, fast): result = const.ENoError for walk in self.__walk_normal_file(lpath): dirpath, dirnames, filenames = walk for filename in filenames: rpart = os.path.relpath(dirpath, lpath) if rpart == '.': rpart = '' subr = self.__share_local_file(joinpath(dirpath, filename), posixpath.join(rpath, rpart, filename), fast) <IF_STMT> result = subr return result",if subr != const.ENoError:,if subr is not const.ENoError:,0.7650869875069111,0.8649799950178215,False
600,"def _targets(self, sigmaparser): targets = set() for condfield in self.conditions: <IF_STMT> rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: if condvalue in rulefieldvalues: targets.update(self.conditions[condfield][condvalue]) return targets",if condfield in sigmaparser.values:,if condfield in sigmaparser.values:,0.8418277818829958,0.7886336751695258,True
601,"def _wrapped_view(request, *args, **kwargs): user = request.user if user.is_authenticated(): obj = _resolve_lookup(obj_lookup, kwargs) perm_obj = _resolve_lookup(perm_obj_lookup, kwargs) granted = access.has_perm_or_owns(user, perm, obj, perm_obj, owner_attr) <IF_STMT> return view_func(request, *args, **kwargs) return HttpResponseForbidden()",if granted or user.has_perm(perm):,if granted:,0.826097631620121,0.8827916928185874,False
602,"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier['part'] earlier_step = earlier['step'] found = False for current in current_parts: <IF_STMT> found = True break if not found: cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",if earlier_part == current['part'] and earlier_step == current['step']:,if current['part'] == earlier_part and current['step'] == earlier_step:,0.8125514340579131,0.8145683092313328,False
603,"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: if self.named_windows[wnd_name] == 0: self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) <IF_STMT> self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print('show_image: named_window ', wnd_name, ' not found.')",if wnd_name in self.capture_mouse_windows:,elif self.named_windows[wnd_name] == 1:,0.6425569286104603,0.8105932471967202,False
604,"def readlines(self, hint=None): body = self._get_body() rest = body[self.position:] self.position = len(body) result = [] while 1: next = rest.find('\r\n') <IF_STMT> result.append(rest) break result.append(rest[:next + 2]) rest = rest[next + 2:] return result",if next == -1:,if next == -1:,0.6403842867017102,0.8385130047130208,True
605,"def __lt__(self, other): olen = len(other) for i in range(olen): try: c = self[i] < other[i] except IndexError: return True if c: return c <IF_STMT> return False return len(self) < olen",elif other[i] < self[i]:,elif self[i] != other[i]:,0.8565099695581448,0.828399516355805,False
606,"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: if user and social.user != user: msg = 'This account is already in use.' raise AuthAlreadyAssociated(backend, msg) <IF_STMT> user = social.user return {'social': social, 'user': user, 'is_new': user is None, 'new_association': social is None}",elif not user:,if user is None:,0.9326911909193991,0.8902056737869248,False
607,"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: if self.utexc is False: self.utexc = np.zeros(ntexco, dtype=bool) <IF_STMT> self.utexc[indices] = True",if self.utexc is not True:,if indices is not None:,0.8235050108072777,0.8094220211349227,False
608,"def destination(self, type, name, arglist): classname = 'ResFunction' listname = 'functions' if arglist: t, n, m = arglist[0] <IF_STMT> classname = 'ResMethod' listname = 'resmethods' return (classname, listname)",if t == 'Handle' and m == 'InMode':,if t == 'ResFunction':,0.68105613037895,0.8105932471967202,False
609,"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) if register: to_store.append(self.view.substr(self.view.full_line(r))) if register: text = ''.join(to_store) <IF_STMT> text = text + '\n' state = State(self.view) state.registers[register] = [text]",if not text.endswith('\n'):,if not self.view.is_line(text):,0.9276650878606094,0.8635707684233572,False
610,"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue <IF_STMT> return b'' else: chunk = chunk[start - self._pos:] if stop is not None and self._pos > stop: chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",elif self._pos == start:,if start is None and self._pos >= stop:,0.7627816848157019,0.8368865274141842,False
611,def start(self): self.on_config_change() self.start_config_watch() try: if self.config['MITMf']['DNS']['tcp'].lower() == 'on': self.startTCP() else: self.startUDP() except socket.error as e: <IF_STMT> shutdown('\n[DNS] Unable to start DNS server on port {}: port already in use'.format(self.config['MITMf']['DNS']['port'])),if 'Address already in use' in e:,if e.errno == errno.EADDRINUSE:,0.6627044390091612,0.8228500218338367,False
612,"def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super(ParseElementEnhance, self).ignore(other) <IF_STMT> self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if self.expr is not None:,if self.expr is not None:,0.8797369373409095,0.7685107079449489,True
613,def test_relative_deploy_path_override(): s = Site(TEST_SITE_ROOT) s.load() res = s.content.resource_from_relative_path('blog/2010/december/merry-christmas.html') res.relative_deploy_path = 'blog/2010/december/happy-holidays.html' for page in s.content.walk_resources(): <IF_STMT> assert page.relative_deploy_path == 'blog/2010/december/happy-holidays.html' else: assert page.relative_deploy_path == Folder(page.relative_path),if res.source_file == page.source_file:,if page.relative_path == 'blog/2010/december/merry-christmas.html':,0.6069809440490666,0.7965020533851944,False
614,"def _parser(cls, buf): tlvs = [] while buf: tlv_type = LLDPBasicTLV.get_type(buf) tlv = cls._tlv_parsers[tlv_type](buf) tlvs.append(tlv) offset = LLDP_TLV_SIZE + tlv.len buf = buf[offset:] <IF_STMT> break assert len(buf) > 0 lldp_pkt = cls(tlvs) assert lldp_pkt._tlvs_len_valid() assert lldp_pkt._tlvs_valid() return (lldp_pkt, None, buf)",if tlv.tlv_type == LLDP_TLV_END:,if not buf:,0.8285587496955191,0.8875087479151215,False
615,"def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures): try: output = self.client.pull(repo, **pull_kwargs) if silent: with open(os.devnull, 'w') as devnull: yield from stream_output(output, devnull) else: yield from stream_output(output, sys.stdout) except (StreamOutputError, NotFound) as e: <IF_STMT> raise else: log.error(str(e))",if not ignore_pull_failures:,if ignore_pull_failures:,0.929476087078517,0.8996480074924822,False
616,def _collect_bytecode(ordered_code): bytecode_blocks = [] stack = [ordered_code] while stack: code = stack.pop() bytecode_blocks.append(code.co_code) for const in code.co_consts: <IF_STMT> stack.append(const) return bytecode_blocks,"if isinstance(const, blocks.OrderedCode):",if const.co_code == ordered_code:,0.8839275422811993,0.760856626273165,False
617,"def displayhook(value): if value is None: return builtins = modules['builtins'] builtins._ = None text = repr(value) try: local_stdout = stdout except NameError as e: raise RuntimeError('lost sys.stdout') from e try: local_stdout.write(text) except UnicodeEncodeError: bytes = text.encode(local_stdout.encoding, 'backslashreplace') <IF_STMT> local_stdout.buffer.write(bytes) else: text = bytes.decode(local_stdout.encoding, 'strict') local_stdout.write(text) local_stdout.write('\n') builtins._ = value","if hasattr(local_stdout, 'buffer'):",if PY3:,0.9458740646367678,0.926934323706186,False
618,"def _analyze(self): lines = open(self.log_path, 'r').readlines() prev_line = None for line in lines: if line.startswith('ERROR:') and prev_line and prev_line.startswith('='): self.errors.append(line[len('ERROR:'):].strip()) <IF_STMT> self.failures.append(line[len('FAIL:'):].strip()) prev_line = line",elif line.startswith('FAIL:') and prev_line and prev_line.startswith('='):,elif line.startswith('FAIL:') and prev_line and prev_line.startswith('='):,0.8733138282690681,0.7332023424667726,True
619,"def _flush(self): if self._data: if self._last is not None: text = ''.join(self._data) <IF_STMT> assert self._last.tail is None, 'internal error (tail)' self._last.tail = text else: assert self._last.text is None, 'internal error (text)' self._last.text = text self._data = []",if self._tail:,if self._last.tail is not None:,0.7021217630855526,0.8338542560892604,False
620,"def write(self, chunk): consumer = self._current_consumer server_side = consumer.server_side if server_side: server_side.data_received(chunk) else: consumer.message += chunk assert consumer.in_parser.execute(chunk, len(chunk)) == len(chunk) <IF_STMT> consumer.finished()",if consumer.in_parser.is_message_complete():,if consumer.finished:,0.9284955236860655,0.8764445248055556,False
621,"def _api_change_cat(name, output, kwargs): """"""API: accepts output, value(=nzo_id), value2(=category)"""""" value = kwargs.get('value') value2 = kwargs.get('value2') if value and value2: nzo_id = value cat = value2 <IF_STMT> cat = None result = sabnzbd.NzbQueue.change_cat(nzo_id, cat) return report(output, keyword='status', data=bool(result > 0)) else: return report(output, _MSG_NO_VALUE)",if cat == 'None':,if cat == 'category':,0.7364636468430698,0.8752376177722327,False
622,"def get_allocated_address(self, config: ActorPoolConfig, allocated: allocated_type) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): <IF_STMT> occupied = True break if not occupied: return addr raise NoIdleSlot(f'No idle slot for creating actor with label {self.label}, mark {self.mark}')",if strategy == self:,if strategy.is_available(self.mark):,0.9372603991943426,0.9220450449751959,False
623,"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: <IF_STMT> with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if delete:,if delete:,0.9416866824175106,0.9099951253570094,True
624,"def quick_load(tool_file, async_load=True): try: tool = self.load_tool(tool_file, tool_cache_data_dir) self.__add_tool(tool, load_panel_dict, elems) key = 'tool_%s' % str(tool.id) integrated_elems[key] = tool <IF_STMT> self._load_tool_panel() self._save_integrated_tool_panel() return tool.id except Exception: log.exception('Failed to load potential tool %s.', tool_file) return None",if async_load:,if async_load:,0.8443561887075877,0.8966773400768917,True
625,"def _get_default_ordering(self): try: ordering = super(DocumentChangeList, self)._get_default_ordering() except AttributeError: ordering = [] if self.model_admin.ordering: ordering = self.model_admin.ordering <IF_STMT> ordering = self.lookup_opts.ordering return ordering",elif self.lookup_opts.ordering:,elif self.lookup_opts.ordering:,0.7412055331930189,0.839587623092576,True
626,"def names(self, persistent=None): u = set() result = [] for s in [self.__storage(None), self.__storage(self.__category)]: for b in s: <IF_STMT> continue if b.name.startswith('__'): continue if b.name not in u: result.append(b.name) u.add(b.name) return result",if persistent is not None and b.persistent != persistent:,if b.persistent:,0.8660530837221649,0.8866029039778043,False
627,"def common_check_get_messages_query(self, query_params: Dict[str, object], expected: str) -> None: user_profile = self.example_user('hamlet') request = POSTRequestMock(query_params, user_profile) with queries_captured() as queries: get_messages_backend(request, user_profile) for query in queries: <IF_STMT> sql = str(query['sql']).replace(' /* get_messages */', '') self.assertEqual(sql, expected) return raise AssertionError('get_messages query not found')",if '/* get_messages */' in query['sql']:,if query['query_type'] == 'get_messages':,0.9118293211462838,0.8723360571509826,False
628,"def _activate_only_current_top_active(): for i in range(0, len(current_sequence().tracks) - 1): <IF_STMT> current_sequence().tracks[i].active = True else: current_sequence().tracks[i].active = False gui.tline_column.widget.queue_draw()",if i == current_sequence().get_first_active_track().id:,if current_sequence().tracks[i].top_id == gui.tline_column.current_top_id:,0.7926653929811814,0.7098232254187811,False
629,"def http_wrapper(self, url, postdata={}): try: if postdata != {}: f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) <IF_STMT> logging.error('with post data') else: logging.error('without post data') logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = '' return response",if postdata:,if exc.args[0] == '404':,0.8821459858490284,0.8856327184319047,False
630,"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None <IF_STMT> if hasattr(sys, 'getswitchinterval'): interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): if hasattr(sys, 'setswitchinterval'): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)",if not sys.platform.startswith('java'):,if not sys.platform.startswith('java'):,0.6130989776257075,0.8783650674919876,True
631,"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() <IF_STMT> if block_end: queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f","if f is not None and f.type in ('or', 'and', 'not'):",if f.block_end:,0.735117434896236,0.8318180062062374,False
632,"def smartsplit(code): """"""Split `code` at "" symbol, only if it is not escaped."""""" strings = [] pos = 0 while pos < len(code): <IF_STMT> word = '' pos += 1 while pos < len(code): if code[pos] == '""': break if code[pos] == '\\': word += '\\' pos += 1 word += code[pos] pos += 1 strings.append('""%s""' % word) pos += 1 return strings","if code[pos] == '""':","if code[pos] == '""':",0.881782614589697,0.914208565914368,True
633,"def get_folder_content(cls, name): """"""Return (folders, files) for the given folder in the root dir."""""" folders = set() files = set() for path in cls.LAYOUT: <IF_STMT> continue parts = path.split('/') if len(parts) == 2: files.add(parts[1]) else: folders.add(parts[1]) folders = list(folders) folders.sort() files = list(files) files.sort() return (folders, files)",if not path.startswith(name + '/'):,if path.startswith(name):,0.9417341495546665,0.9237460349978159,False
634,"def array_for(self, i): if 0 <= i < self._cnt: <IF_STMT> return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[i >> level & 31] level -= 5 assert isinstance(node, Node) return node._array affirm(False, u'Index out of Range')",if i >= self.tailoff():,if i >= self._tail:,0.7763303449732064,0.8806615362338783,False
635,"def __or__(self, other) -> 'MultiVector': """"""``self | other``, the inner product :math:`M \\cdot N`"""""" other, mv = self._checkOther(other) if mv: newValue = self.layout.imt_func(self.value, other.value) else: <IF_STMT> obj = self.__array__() return obj | other return self._newMV(dtype=np.result_type(self.value.dtype, other)) return self._newMV(newValue)","if isinstance(other, np.ndarray):",if self.layout.imt_func is not None:,0.9147443586393109,0.8380055871435848,False
636,"def parse_bzr_stats(status): stats = RepoStats() statustype = 'changed' for statusline in status: if statusline[:2] == '  ': setattr(stats, statustype, getattr(stats, statustype) + 1) <IF_STMT> statustype = 'staged' elif statusline == 'unknown:': statustype = 'new' else: statustype = 'changed' return stats",elif statusline == 'added:':,elif statusline == 'staged:':,0.9161849340594077,0.8661072626070159,False
637,"def write(self, timestamps, actualValues, predictedValues, predictionStep=1): assert len(timestamps) == len(actualValues) == len(predictedValues) for index in range(len(self.names)): timestamp = timestamps[index] actual = actualValues[index] prediction = predictedValues[index] writer = self.outputWriters[index] <IF_STMT> outputRow = [timestamp, actual, prediction] writer.writerow(outputRow) self.lineCounts[index] += 1",if timestamp is not None:,if predictionStep == 1:,0.861058198637703,0.8592377270804451,False
638,"def clean(self): """"""Delete old files in ""tmp""."""""" now = time.time() for entry in os.listdir(os.path.join(self._path, 'tmp')): path = os.path.join(self._path, 'tmp', entry) <IF_STMT> os.remove(path)",if now - os.path.getatime(path) > 129600:,if time.time() - now > self.cache_timeout:,0.7491254604620455,0.7337557032700684,False
639,"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen([self._find_binary(), 'info', os.path.realpath(path)], stdout=PIPE, stderr=PIPE).communicate() except OSError: pass else: <IF_STMT> for line in stdout.splitlines(): line = u(line).split(': ', 1) if len(line) == 2: info[line[0]] = line[1] return info",if stdout:,if stdout:,0.7295695986788593,0.9220450449751959,True
640,"def add(meta_list, info_list=None): if not info_list: info_list = meta_list if not isinstance(meta_list, (list, tuple)): meta_list = (meta_list,) if not isinstance(info_list, (list, tuple)): info_list = (info_list,) for info_f in info_list: <IF_STMT> for meta_f in meta_list: metadata[meta_f] = info[info_f] break",if info.get(info_f) is not None:,if info[info_f] is not None:,0.8438282029640257,0.8380055871435848,False
641,"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): <IF_STMT> log_r_term = model_site['log_prob'] if not model_site['is_observed']: log_r_term = log_r_term - guide_trace.nodes[name]['log_prob'] log_r.add((stacks[name], log_r_term.detach())) return log_r",if model_site['type'] == 'sample':,if model_site['is_observed']:,0.8923487113737492,0.8787142254774354,False
642,"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break if prog.match(line): text = line[len(key) + 1:] while 1: line = f.readline() <IF_STMT> break text = text + line return text.strip() return None",if not line or not line[0].isspace():,if not line:,0.9492326778500937,0.9196822664155297,False
643,"def build_iterator(data, infinite=True): """"""Build the iterator for inputs."""""" index = 0 size = len(data[0]) while True: if index + batch_size > size: <IF_STMT> index = 0 else: return yield (data[0][index:index + batch_size], data[1][index:index + batch_size]) index += batch_size",if infinite:,if infinite:,0.8328427661880113,0.9051034981560222,True
644,"def checkall(g, bg, dst_nodes, include_dst_in_src=True): for etype in g.etypes: ntype = g.to_canonical_etype(etype)[2] <IF_STMT> check(g, bg, ntype, etype, dst_nodes[ntype], include_dst_in_src) else: check(g, bg, ntype, etype, None, include_dst_in_src)",if dst_nodes is not None and ntype in dst_nodes:,if ntype in dst_nodes:,0.7739136503393109,0.7965020533851944,False
645,"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: if issubclass(n, m) and m is not n: break else: <IF_STMT> candidates.remove(m) candidates.append(m) return candidates",if m in candidates:,if m in candidates:,0.8933152213448453,0.9001816649635144,True
646,"def __keep_songs_enable(self, enabled): config.set('memory', 'queue_keep_songs', enabled) if enabled: self.queue.set_first_column_type(CurrentColumn) else: for col in self.queue.get_columns(): <IF_STMT> self.queue.set_first_column_type(None) break","if isinstance(col, CurrentColumn):",if col.type == CurrentColumn:,0.5984955471673198,0.7101158913184162,False
647,"def outlineView_heightOfRowByItem_(self, tree, item) -> float: default_row_height = self.rowHeight if item is self: return default_row_height heights = [default_row_height] for column in self.tableColumns: value = getattr(item.attrs['node'], str(column.identifier)) <IF_STMT> heights.append(value._impl.native.intrinsicContentSize().height) return max(heights)","if isinstance(value, toga.Widget):",if value is not None:,0.8886515461221647,0.7975010608178975,False
648,def condition(self): if self.__condition is None: <IF_STMT> self.__condition = self.flat_conditions[0] elif len(self.flat_conditions) == 0: self.__condition = lambda _: True else: self.__condition = lambda x: all((cond(x) for cond in self.flat_conditions)) return self.__condition,if len(self.flat_conditions) == 1:,if len(self.flat_conditions) == 1:,0.8861010074589847,0.828399516355805,True
649,"def _find_delimiter(f, block_size=2 ** 16): delimiter = b'\n' if f.tell() == 0: return 0 while True: b = f.read(block_size) if not b: return f.tell() <IF_STMT> return f.tell() - len(b) + b.index(delimiter) + 1",elif delimiter in b:,if b.index(delimiter) == 0:,0.8758375798173405,0.8385130047130208,False
650,"def serialize(self, name=None): data = super(SimpleText, self).serialize(name) data['contentType'] = self.contentType data['content'] = self.content if self.width: <IF_STMT> raise InvalidWidthException(self.width) data['inputOptions'] = {} data['width'] = self.width return data","if self.width not in [100, 50, 33, 25]:","if self.width not in ('0', '1', '2', '3'):",0.8603299049989727,0.6952219386678455,False
651,"def inference(self): self.attention_weight_dim = self.input_dims[0][-1] if self.keep_dim: self.output_dim = copy.deepcopy(self.input_dims[0]) else: self.output_dim = [] for idx, dim in enumerate(self.input_dims[0]): <IF_STMT> self.output_dim.append(dim) super(LinearAttentionConf, self).inference()",if idx != len(self.input_dims[0]) - 2:,if idx == self.attention_weight_dim:,0.6970814736735953,0.7709002428237395,False
652,"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): if status == MemcacheDeleteResponse.DELETED: result.append(DELETE_SUCCESSFUL) <IF_STMT> result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",elif status == MemcacheDeleteResponse.NOT_FOUND:,elif status == MemcacheDeleteResponse.MISSING:,0.8930939875129253,0.8038019482772603,False
653,def identify_page_at_cursor(self): for region in self.view.sel(): text_on_cursor = None pos = region.begin() scope_region = self.view.extract_scope(pos) <IF_STMT> text_on_cursor = self.view.substr(scope_region) return text_on_cursor.strip(string.punctuation) return None,if not scope_region.empty():,if scope_region:,0.7843494178965553,0.839587623092576,False
654,"def from_elem(cls, parent, when_elem): """"""Loads the proper when by attributes of elem"""""" when_value = when_elem.get('value', None) <IF_STMT> return ValueToolOutputActionConditionalWhen(parent, when_elem, when_value) else: when_value = when_elem.get('datatype_isinstance', None) if when_value is not None: return DatatypeIsInstanceToolOutputActionConditionalWhen(parent, when_elem, when_value) raise TypeError('When type not implemented')",if when_value is not None:,if when_value is not None:,0.7392534325954097,0.845713978670975,True
655,"def test_insert_entity_empty_string_rk(self, tables_cosmos_account_name, tables_primary_cosmos_account_key): await self._set_up(tables_cosmos_account_name, tables_primary_cosmos_account_key) try: entity = {'PartitionKey': 'pk', 'RowKey': ''} with pytest.raises(HttpResponseError): await self.table.create_entity(entity=entity) finally: await self._tear_down() <IF_STMT> sleep(SLEEP_DELAY)",if self.is_live:,if self.is_live:,0.9051987037173246,0.8764445248055556,True
656,"def provider_uris(self): login_urls = {} continue_url = self.request.get('continue_url') for provider in self.provider_info: <IF_STMT> login_url = self.uri_for('social-login', provider_name=provider, continue_url=continue_url) else: login_url = self.uri_for('social-login', provider_name=provider) login_urls[provider] = login_url return login_urls",if continue_url:,if continue_url:,0.8089667602775172,0.8696398662122882,True
657,"def expand_extensions(existing): for name in extension_names: ext = im('lizard_ext.lizard' + name.lower()).LizardExtension() <IF_STMT> else name existing.insert(len(existing) if not hasattr(ext, 'ordering_index') else ext.ordering_index, ext) return existing","if isinstance(name, str)","if not hasattr(ext, 'ordering_index'):",0.7937516894909005,0.7801270245332924,False
658,"def wrapper(self, *args, **kwargs): if not self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path + '/' <IF_STMT> uri += '?' + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)",if self.request.query:,if self.request.query:,0.796157687405439,0.8866029039778043,True
659,"def subword_map_by_joiner(subwords, marker=SubwordMarker.JOINER): """"""Return word id for each subword token (annotate by joiner)."""""" flags = [0] * len(subwords) for i, tok in enumerate(subwords): <IF_STMT> flags[i] = 1 if tok.startswith(marker): assert i >= 1 and flags[i - 1] != 1, 'Sentence `{}` not correct!'.format(' '.join(subwords)) flags[i - 1] = 1 marker_acc = list(accumulate([0] + flags[:-1])) word_group = [i - maker_sofar for i, maker_sofar in enumerate(marker_acc)] return word_group",if tok.endswith(marker):,if tok.startswith(marker):,0.9023282407320294,0.9460866645260309,False
660,"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue <IF_STMT> break i += direction if start < 0: start = 0",if self.select(i):,if self.items[i] == self._selected:,0.8287556580901558,0.9155272930874561,False
661,"def get_config(cls): config = {} profile_path = dingdangpath.config('profile.yml') if os.path.exists(profile_path): with open(profile_path, 'r') as f: profile = yaml.safe_load(f) <IF_STMT> if 'vid' in profile['iflytek_yuyin']: config['vid'] = profile['iflytek_yuyin']['vid'] return config",if 'iflytek_yuyin' in profile:,if 'iflytek_yuyin' in profile:,0.8914354186503842,0.8105932471967202,True
662,def get_signed_in_user(test_case): playback = not (test_case.is_live or test_case.in_recording) if playback: return MOCKED_USER_NAME else: account_info = test_case.cmd('account show').get_output_in_json() <IF_STMT> return account_info['user']['name'] return None,if account_info['user']['type'] != 'servicePrincipal':,if account_info['user']['name']:,0.6313406831981605,0.8318180062062374,False
663,"def rename_project(self, project, new_name): """"""Rename project, update the related projects if necessary"""""" old_name = project.name for proj in self.projects: relproj = proj.get_related_projects() <IF_STMT> relproj[relproj.index(old_name)] = new_name proj.set_related_projects(relproj) project.rename(new_name) self.save()",if old_name in relproj:,if old_name in relproj:,0.7797088552570853,0.8169276475307028,True
664,"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True if len(a) != b.len: return False <IF_STMT> return True return memcmp(a.ptr, b.ptr, a.len) == 0",if a.ptr == b.ptr:,if a.ptr != b.ptr:,0.9170674598276638,0.9019629427251674,False
665,def parse_variable(self): begin = self._pos while True: ch = self.read() <IF_STMT> return ScriptVariable(self._text[begin:self._pos - 1]) elif ch is None: self.__raise_eof() elif not isidentif(ch) and ch != ':': self.__raise_char(ch),if ch == '%':,if isidentif(ch) and ch == '\n':,0.7831661744872551,0.7590598306198806,False
666,"def h_file(self): filename = self.abspath() st = os.stat(filename) cache = self.ctx.hashes_md5_tstamp if filename in cache and cache[filename][0] == st.st_mtime: return cache[filename][1] if STRONGEST: ret = Utils.h_file(filename) else: <IF_STMT> raise IOError('Not a file') ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest() cache[filename] = (st.st_mtime, ret) return ret",if stat.S_ISDIR(st[stat.ST_MODE]):,if not os.path.isfile(filename):,0.9322223735736567,0.8928756684056034,False
667,"def add_widgets(self, *widgets_or_spacings): """"""Add widgets/spacing to dialog vertical layout"""""" layout = self.layout() for widget_or_spacing in widgets_or_spacings: <IF_STMT> layout.addSpacing(widget_or_spacing) else: layout.addWidget(widget_or_spacing)","if isinstance(widget_or_spacing, int):",if widget_or_spacing.isSpacing():,0.8463803038471345,0.8137489370974955,False
668,"def _str_index(self): idx = self['index'] out = [] if len(idx) == 0: return out out += ['.. index:: %s' % idx.get('default', '')] for section, references in idx.iteritems(): <IF_STMT> continue elif section == 'refguide': out += ['   single: %s' % ', '.join(references)] else: out += ['   %s: %s' % (section, ','.join(references))] return out",if section == 'default':,if section == 'default':,0.9007202701388458,0.8964173245779284,True
669,"def dictify_CPPDEFINES(env): cppdefines = env.get('CPPDEFINES', {}) if cppdefines is None: return {} if SCons.Util.is_Sequence(cppdefines): result = {} for c in cppdefines: <IF_STMT> result[c[0]] = c[1] else: result[c] = None return result if not SCons.Util.is_Dict(cppdefines): return {cppdefines: None} return cppdefines",if SCons.Util.is_Sequence(c):,if SCons.Util.is_Dict(c):,0.9166164084078965,0.9076141716697395,False
670,"def decoder(s): r = [] decode = [] for c in s: if c == '&' and (not decode): decode.append('&') elif c == '-' and decode: if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] <IF_STMT> decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",elif decode:,"elif c in ('+', '0'):",0.8657208596980371,0.8729118929672821,False
671,"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): <IF_STMT> continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if not Placeholder.check_resolved(v.size):,"if not isinstance(v, Graph):",0.8763677711086144,0.8627586293513119,False
672,"def one_gpr_reg_one_mem_scalable(ii): n, r = (0, 0) for op in _gen_opnds(ii): if op_agen(op) or (op_mem(op) and op.oc2 in ['v']): n += 1 <IF_STMT> r += 1 else: return False return n == 1 and r == 1",elif op_gprv(op):,elif op_reg(op) and op.oc1 in ['v']:,0.8944904067575413,0.8132493528194856,False
673,"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in(['fasta', 'base'], refs) if seq_file and os.path.exists(seq_file): return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), 'genomes', '*', gid)) <IF_STMT> return gdirs[0]",if len(gdirs) == 1 and os.path.exists(gdirs[0]):,if gdirs:,0.7482268528047308,0.9146232957723002,False
674,"def __modules(self): raw_output = self.__module_avail_output().decode('utf-8') for line in StringIO(raw_output): line = line and line.strip() if not line or line.startswith('-'): continue line_modules = line.split() for module in line_modules: <IF_STMT> module = module[0:-len(self.default_indicator)].strip() module_parts = module.split('/') module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield (module_name, module_version)",if module.endswith(self.default_indicator):,if module.startswith(self.default_indicator):,0.8839413921992347,0.9284304001296656,False
675,"def save(self): updates = self.cinder_obj_get_changes() if updates: <IF_STMT> metadata = updates.pop('metadata', None) self.metadata = db.backup_metadata_update(self._context, self.id, metadata, True) updates.pop('parent', None) db.backup_update(self._context, self.id, updates) self.obj_reset_changes()",if 'metadata' in updates:,if 'metadata' in updates:,0.8844687123232875,0.7801270245332924,True
676,"def test_set_tag(association_obj, sagemaker_session): tag = {'Key': 'foo', 'Value': 'bar'} association_obj.set_tag(tag) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags(ResourceArn=association_obj.source_arn)['Tags'] <IF_STMT> break time.sleep(5) assert len(actual_tags) > 0 assert actual_tags[0] == tag",if actual_tags:,if actual_tags:,0.741516538047785,0.8590888738245122,True
677,"def test_error_stream(environ, start_response): writer = start_response('200 OK', []) wsgi_errors = environ['wsgi.errors'] error_msg = None for method in ['flush', 'write', 'writelines']: if not hasattr(wsgi_errors, method): error_msg = ""wsgi.errors has no '%s' attr"" % method <IF_STMT> error_msg = 'wsgi.errors.%s attr is not callable' % method if error_msg: break return_msg = error_msg or 'success' writer(return_msg) return []","if not error_msg and (not callable(getattr(wsgi_errors, method))):",elif not callable(wsgi_errors.get(method)):,0.8868993149567811,0.9167056528641923,False
678,"def current_dict(cursor_offset, line): """"""If in dictionary completion, return the dict that should be used"""""" for m in current_dict_re.finditer(line): <IF_STMT> return LinePart(m.start(1), m.end(1), m.group(1)) return None",if m.start(2) <= cursor_offset and m.end(2) >= cursor_offset:,if m.start(1) <= cursor_offset and m.end(1) >= cursor_offset:,0.8518314011973375,0.6850564735741161,False
679,"def show_file_browser(self): """"""Show/hide the file browser."""""" if self.show_file_browser_action.isChecked(): sizes = self.panel.sizes() <IF_STMT> sizes[0] = sum(sizes) // 4 self.panel.setSizes(sizes) self.file_browser.show() else: self.file_browser.hide()",if sizes[0] == 0:,if len(sizes) > 0:,0.7135022904521995,0.7498810286408993,False
680,"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.nameEncoded()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:,if len(items) > 1:,0.7413153949647944,0.760856626273165,True
681,"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): <IF_STMT> tag.text = '' if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if not tag.text:,if tag.text == '':,0.8864976591171866,0.8944264839442453,False
682,"def getLabel(self, address=None): if address is None: address = self.address label = address if shared.config.has_section(address): label = shared.config.get(address, 'label') queryreturn = sqlQuery('select label from addressbook where address=?', address) <IF_STMT> for row in queryreturn: label, = row else: queryreturn = sqlQuery('select label from subscriptions where address=?', address) if queryreturn != []: for row in queryreturn: label, = row return label",if queryreturn != []:,if queryreturn != []:,0.946153908879609,0.9084940438173679,True
683,"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') <IF_STMT> self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",if 'momentum' in self.args:,if 'momentum' in self.args:,0.9312888416026989,0.8723360571509826,True
684,"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) if 'encoding' in new_kwargs: del new_kwargs['encoding'] <IF_STMT> del new_kwargs['errors'] return quote(*args, **new_kwargs)",if 'errors' in kwargs:,if 'errors' in new_kwargs:,0.8804520261912372,0.7801270245332924,False
685,"def setNextFormPrevious(self, backup=STARTING_FORM): try: if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]: self._FORM_VISIT_LIST.pop() <IF_STMT> self.setNextForm(self._FORM_VISIT_LIST.pop()) except IndexError: self.setNextForm(backup)",if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM:,elif self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-2]:,0.7977541891707682,0.630190855592386,False
686,"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char['text'].isspace(): if current_word: yield current_word current_word = [] <IF_STMT> yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word","elif current_word and self.char_begins_new_word(current_word, char):",elif self.keep_blank_chars and char['text'].isspace():,0.7516828536781788,0.8431339019329497,False
687,"def get(self): """"""return a secret by name"""""" results = self._get('secrets', self.name) results['decoded'] = {} results['exists'] = False if results['returncode'] == 0 and results['results'][0]: results['exists'] = True <IF_STMT> if 'data' in results['results'][0]: for sname, value in results['results'][0]['data'].items(): results['decoded'][sname] = base64.b64decode(value) if results['returncode'] != 0 and '""%s"" not found' % self.name in results['stderr']: results['returncode'] = 0 return results",if self.decode:,if results['exists']:,0.6787744302387012,0.9362597875749384,False
688,"def insert_use(self, edit): if self.is_first_use(): for location in ['^\\s*namespace\\s+[\\w\\\\]+[;{]', '<\\?php']: inserted = self.insert_first_use(location, edit) <IF_STMT> break else: self.insert_use_among_others(edit)",if inserted:,if inserted:,0.6028278257169626,0.7912619863720214,True
689,"def _new_rsa_key(spec): if 'name' not in spec: <IF_STMT> head, tail = os.path.split(spec['key']) spec['path'] = head spec['name'] = tail else: spec['name'] = spec['key'] return rsa_init(spec)",if '/' in spec['key']:,if spec['key']:,0.8765557802548084,0.8466657105524215,False
690,"def mimeData(self, indexes): if len(indexes) == 1: index = indexes[0] model = song = index.data(Qt.UserRole) <IF_STMT> try: model = song.album except (ProviderIOError, Exception): model = None return ModelMimeData(model)",if index.column() == Column.album:,if model is None:,0.6289286192504436,0.8105932471967202,False
691,"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: if url.endswith('ping') and self._first_ping: self._first_ping = False return EmptyCapabilitiesResponse() <IF_STMT> return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",elif 'Hello0' in url and '1.2.1' in url and ('v1' in url):,elif url.endswith('error'):,0.6363538271621788,0.8935248372106969,False
692,"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): if hasattr(model, '_create_content_base'): self.validate_base_model(model) <IF_STMT> self.validate_content_type(model)","if hasattr(model, '_feincms_content_models'):","elif hasattr(model, '_create_content_type'):",0.8936560138132055,0.8238874726594148,False
693,"def test_rules_widget(self): subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit) widgets = subreddit.widgets with self.use_cassette('TestSubredditWidgets.fetch_widgets'): rules = None for widget in widgets.sidebar: <IF_STMT> rules = widget break assert isinstance(rules, RulesWidget) assert rules == rules assert rules.id == rules assert rules.display assert len(rules) > 0 assert subreddit == rules.subreddit","if isinstance(widget, RulesWidget):","if isinstance(widget, RulesWidget):",0.9337903764431905,0.8953711787948615,True
694,"def __init__(self, exception): message = str(exception) with contextlib.suppress(IndexError): underlying_exception = exception.args[0] <IF_STMT> message = 'maximum retries exceeded trying to reach the store.\nCheck your network connection, and check the store status at {}'.format(_STORE_STATUS_URL) super().__init__(message=message)","if isinstance(underlying_exception, urllib3.exceptions.MaxRetryError):",if underlying_exception.args[0] == StoreStatusLimitExceededException:,0.7812253507337505,0.8385130047130208,False
695,"def wrapped(self, request): try: return self._finished except AttributeError: if self.node_ids: <IF_STMT> log.debug('%s is still going to be used, not terminating it. Still in use on:\n%s', self, pprint.pformat(list(self.node_ids))) return log.debug('Finish called on %s', self) try: return func(request) finally: self._finished = True",if not request.session.shouldfail and (not request.session.shouldstop):,if self.node_ids:,0.7366560965233532,0.9099951253570094,False
696,"def get_min_vertical_scroll() -> int: used_height = 0 prev_lineno = ui_content.cursor_position.y for lineno in range(ui_content.cursor_position.y, -1, -1): used_height += get_line_height(lineno) <IF_STMT> return prev_lineno else: prev_lineno = lineno return 0",if used_height > height - scroll_offsets_bottom:,if used_height < ui_content.scroll_height:,0.7839499867718112,0.8105932471967202,False
697,"def cookies(self): cookies = flask.Request.cookies.__get__(self) result = {} desuffixed = {} for key, value in cookies.items(): <IF_STMT> desuffixed[key[:-len(self.cookie_suffix)]] = value else: result[key] = value result.update(desuffixed) return result",if key.endswith(self.cookie_suffix):,if key.endswith(self.cookie_suffix):,0.8885581417958774,0.8645707301556367,True
698,"def update_vars(state1, state2): ops = [] for name in state1._fields: state1_vs = getattr(state1, name) <IF_STMT> ops += [tf.assign(_v1, _v2) for _v1, _v2 in zip(state1_vs, getattr(state2, name))] else: ops += [tf.assign(state1_vs, getattr(state2, name))] return tf.group(*ops)","if isinstance(state1_vs, list):","if isinstance(state2_vs, (list, tuple)):",0.6962805661862093,0.8431339019329497,False
699,"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: <IF_STMT> return {} mtime = self.getmtime(self.manifest_path) if self._mtime is None or mtime > self._mtime: self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",if not self.exists(self.manifest_path):,if self._manifest is None:,0.8841158709621342,0.8228500218338367,False
700,"def csvtitle(self): if isinstance(self.name, six.string_types): return '""' + self.name + '""' + char['sep'] * (len(self.nick) - 1) else: ret = '' for i, name in enumerate(self.name): ret = ret + '""' + name + '""' + char['sep'] * (len(self.nick) - 1) <IF_STMT> ret = ret + char['sep'] return ret",if i + 1 != len(self.name):,if i > 0:,0.8997196580818261,0.8902056737869248,False
701,"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): <IF_STMT> if final_dst is not None: raise ValueError('Multiple destinations!') final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",if dst.is_id('IRDst'):,if dst == linenb:,0.9162363861527171,0.8692960007731574,False
702,"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition('.') if dot: <IF_STMT> if module_name in dependencies: dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: logging.warning('Empty package name: %s', name)",if module_name:,if base_name:,0.7424171908862369,0.8966773400768917,False
703,def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == '[': m = -1 aas.append('') elif i == ']': m = 0 continue elif m == -1: aas[-1] = aas[-1] + i <IF_STMT> aas.append(i) return aas,elif m == 0:,elif m == 1:,0.7097701276736837,0.8692960007731574,False
704,"def logic(): count = intbv(0, min=0, max=MAXVAL + 1) while True: yield (clock.posedge, reset.posedge) if reset == 1: count[:] = 0 else: flag.next = 0 <IF_STMT> flag.next = 1 count[:] = 0 else: count += 1",if count == MAXVAL:,elif reset == 2:,0.8670890259783506,0.8516228624291206,False
705,"def _history_define_metric(self, hkey: str) -> Optional[wandb_internal_pb2.MetricRecord]: """"""check for hkey match in glob metrics, return defined metric."""""" if hkey.startswith('_'): return None for k, mglob in six.iteritems(self._metric_globs): if k.endswith('*'): <IF_STMT> m = wandb_internal_pb2.MetricRecord() m.CopyFrom(mglob) m.ClearField('glob_name') m.name = hkey return m return None",if hkey.startswith(k[:-1]):,if mglob:,0.8876049621822097,0.9099951253570094,False
706,"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_(beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment) <IF_STMT> model.half() if use_cuda: model.cuda()",if args.fp16:,if use_half:,0.783090864621873,0.839587623092576,False
707,"def _Dynamic_Rollback(self, transaction, transaction_response): txid = transaction.handle() self.__local_tx_lock.acquire() try: <IF_STMT> raise apiproxy_errors.ApplicationError(datastore_pb.Error.BAD_REQUEST, 'Transaction %d not found.' % (txid,)) txdata = self.__transactions[txid] assert txdata.thread_id == thread.get_ident(), 'Transactions are single-threaded.' del self.__transactions[txid] finally: self.__local_tx_lock.release()",if txid not in self.__transactions:,if txid not in self.__transactions:,0.7115353468715264,0.8094220211349227,True
708,"def get_job_dirs(path): regex = re.compile('[1-9][0-9]*-') jobdirs = [] for d in os.listdir(path): <IF_STMT> continue d = os.path.join(options.resultsdir, d) if os.path.isdir(d) and (not os.path.exists(os.path.join(d, PUBLISH_FLAGFILE))): jobdirs.append(d) return jobdirs",if not regex.match(d):,if regex.search(d):,0.8821049031969377,0.8645707301556367,False
709,"def traverse(node, functions=[]): if hasattr(node, 'grad_fn'): node = node.grad_fn if hasattr(node, 'variable'): node = graph.nodes_by_id.get(id(node.variable)) if node: node.functions = list(functions) del functions[:] if hasattr(node, 'next_functions'): functions.append(type(node).__name__) for f in node.next_functions: <IF_STMT> functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, 'saved_tensors'): for t in node.saved_tensors: traverse(t)",if f[0]:,"if hasattr(f, 1):",0.9398142418904121,0.8928756684056034,False
710,"def get_all_snap_points(self, forts): points = [] radius = Constants.MAX_DISTANCE_FORT_IS_REACHABLE for i in range(0, len(forts)): for j in range(i + 1, len(forts)): c1, c2 = self.get_enclosing_circles(forts[i], forts[j], radius) <IF_STMT> points.append((c1, c2, forts[i], forts[j])) return points",if c1 and c2:,if c1 != c2:,0.8143834847729988,0.8431339019329497,False
711,"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: <IF_STMT> continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))","if not isinstance(grandchild, minidom.Element):","if not isinstance(grandchild, minidom.Element):",0.9181126324917002,0.8385130047130208,True
712,"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: if ch == ' ': w += 1 <IF_STMT> w += abs(tab_width) - w % abs(tab_width) else: break return w",elif ch == '\t':,elif ch == '\t':,0.7615171730091267,0.8228500218338367,True
713,"def test_avg_group_by(self): ret = await Book.annotate(avg=Avg('rating')).group_by('author_id').values('author_id', 'avg') for item in ret: author_id = item.get('author_id') avg = item.get('avg') <IF_STMT> self.assertEqual(avg, 4.5) elif author_id == self.a2.pk: self.assertEqual(avg, 2.0)",if author_id == self.a1.pk:,if author_id == self.a1.pk:,0.7913376691952654,0.7965020533851944,True
714,"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() <IF_STMT> expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if stored_session:,if stored_session:,0.9361485033579389,0.8827916928185874,True
715,"def setInnerHTML(self, html): log.HTMLClassifier.classify(log.ThugLogging.url if log.ThugOpts.local else log.last_url, html) self.tag.clear() for node in bs4.BeautifulSoup(html, 'html.parser').contents: self.tag.append(node) name = getattr(node, 'name', None) if name is None: continue handler = getattr(log.DFT, 'handle_%s' % (name,), None) <IF_STMT> handler(node)",if handler:,if handler is not None:,0.7784805400443204,0.8461353780448981,False
716,def get_supported_period_type_map(cls): if cls.supported_period_map is None: cls.supported_period_map = {} cls.supported_period_map.update(cls.period_type_map) try: from dateutil import relativedelta <IF_STMT> cls.supported_period_map.update(cls.optional_period_type_map) except Exception: pass return cls.supported_period_map,if relativedelta is not None:,if relativedelta.all_days(cls.optional_period_type_map):,0.7990341103143381,0.8318180062062374,False
717,"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: <IF_STMT> if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()","if self._decide_whether_to_process(compare_id, redo, compares_done):",if compare_id not in compares_done:,0.8744553785229373,0.7178970818142898,False
718,"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if ':' in line: left, right = line.split(':', 1) left = left.strip().lower() right = right.strip() if left == field_name and len(right) > 0: if cant_be_number: <IF_STMT> return right else: return right return None",if not right.isdigit():,if left == field_name:,0.9458291041176381,0.8879659171421962,False
719,"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) if not s.consume('\\'): break <IF_STMT> pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return ''.join(res)",if s.consume_re(_newline_esc_re):,if s.last().group(1) == '\\':,0.7307869980723443,0.7965020533851944,False
720,"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == '550': if self.isfile(path): raise errors.DirectoryExpected(path) <IF_STMT> raise errors.DirectoryNotEmpty(path) raise",if not self.isempty(path):,elif self.isdir(path):,0.9247114470099476,0.9051034981560222,False
721,"def _normalize_store_path(self, resource_store): if resource_store['type'] == 'filesystem': <IF_STMT> resource_store['base_directory'] = os.path.join(self.root_directory, resource_store['base_directory']) return resource_store",if not os.path.isabs(resource_store['base_directory']):,if self.root_directory:,0.7466223842135429,0.7241577342575828,False
722,"def _apply_nested(name, val, nested): parts = name.split('.') cur = nested for i in range(0, len(parts) - 1): cur = cur.setdefault(parts[i], {}) <IF_STMT> conflicts_with = '.'.join(parts[0:i + 1]) raise ValueError('%r cannot be nested: conflicts with {%r: %s}' % (name, conflicts_with, cur)) cur[parts[-1]] = val","if not isinstance(cur, dict):",if cur[parts[-1] != val:,0.7561224483306689,0.8752376177722327,False
723,"def build_packages(targeted_packages, distribution_directory, is_dev_build=False): for package_root in targeted_packages: service_hierarchy = os.path.join(os.path.basename(package_root)) <IF_STMT> verify_update_package_requirement(package_root) print('Generating Package Using Python {}'.format(sys.version)) run_check_call([sys.executable, build_packing_script_location, '--dest', os.path.join(distribution_directory, service_hierarchy), package_root], root_dir)",if is_dev_build:,if is_dev_build:,0.8019908571495722,0.8531413606256201,True
724,"def resolve_root_node_address(self, root_node): if '[' in root_node: name, numbers = root_node.split('[', maxsplit=1) number = numbers.split(',', maxsplit=1)[0] <IF_STMT> number = number.split('-')[0] number = re.sub('[^0-9]', '', number) root_node = name + number return root_node",if '-' in number:,if '-' in number:,0.906693135198411,0.8336104423443033,True
725,"def _map_args(maps: dict, **kwargs): output = {} for name, val in kwargs.items(): if name in maps: assert isinstance(maps[name], str) output.update({maps[name]: val}) else: output.update({name: val}) for keys in maps.keys(): <IF_STMT> pass return output",if keys not in output.keys():,if keys not in output:,0.9186655750502538,0.8094220211349227,False
726,"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: self.select(start) break <IF_STMT> i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",if i >= len(self.items):,if i > len(self.items):,0.9420311138007719,0.9128479730518225,False
727,"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: <IF_STMT> if self.KEY in function.context: continue self._explore(function.entry_point, []) function.context[self.KEY] = True",if function.is_implemented:,if self.KEY in function.context:,0.8246516713209131,0.7098232254187811,False
728,"def load_model(self): if not os.path.exists(self.get_filename(absolute=True)): <IF_STMT> return ({}, {}) error('Model file with pre-trained convolution layers not found. Download it here...', 'https://github.com/alexjc/neural-enhance/releases/download/v%s/%s' % (__version__, self.get_filename())) print('  - Loaded file `{}` with trained model.'.format(self.get_filename())) return pickle.load(bz2.open(self.get_filename(), 'rb'))",if args.train:,if self.get_trained_layer() is None:,0.9117308785827574,0.8474968231198384,False
729,"def get_nonexisting_check_definition_extends(definition, indexed_oval_defs): for extdefinition in definition.findall('.//{%s}extend_definition' % oval_ns): extdefinitionref = extdefinition.get('definition_ref') referreddefinition = indexed_oval_defs.get(extdefinitionref) <IF_STMT> return extdefinitionref return None",if referreddefinition is None:,if referreddefinition:,0.8317318059802392,0.8137489370974955,False
730,"def pause(self): if self.is_playing: self.state = MusicPlayerState.PAUSED <IF_STMT> self._current_player.pause() self.emit('pause', player=self, entry=self.current_entry) return elif self.is_paused: return raise ValueError('Cannot pause a MusicPlayer in state %s' % self.state)",if self._current_player:,if self._current_player:,0.7210578273427205,0.8590888738245122,True
731,"def setNextFormPrevious(self, backup=STARTING_FORM): try: <IF_STMT> self._FORM_VISIT_LIST.pop() if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM: self.setNextForm(self._FORM_VISIT_LIST.pop()) except IndexError: self.setNextForm(backup)",if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]:,if self._THISFORM.FORM_NAME == self.STARTING_FORM:,0.816435507852087,0.630190855592386,False
732,"def get_expr_referrers(schema: s_schema.Schema, obj: so.Object) -> Dict[so.Object, str]: """"""Return schema referrers with refs in expressions."""""" refs = schema.get_referrers_ex(obj) result = {} for (mcls, fn), referrers in refs.items(): field = mcls.get_field(fn) <IF_STMT> result.update({ref: fn for ref in referrers}) return result","if issubclass(field.type, (Expression, ExpressionList)):",if field.is_referrer:,0.8051805039823676,0.9076141716697395,False
733,"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: <IF_STMT> continue requires_index = any((field.index, field.unique, isinstance(field, ForeignKeyField))) if requires_index: fields.append(field) return fields",if field.primary_key:,if field.index is None:,0.8192829965063746,0.760856626273165,False
734,"def ident_values(self): value = self._ident_values if value is False: value = None if not self.orig_prefix: wrapped = self.wrapped idents = getattr(wrapped, 'ident_values', None) <IF_STMT> value = [self._wrap_hash(ident) for ident in idents] self._ident_values = value return value",if idents:,if idents is not None:,0.8177908463758379,0.8294838585473985,False
735,def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) if not node: error('node not found: ' + str(dir)) continue <IF_STMT> lst.append(node) self.bld_incpaths_lst.append(node),if not node in lst:,if node not in lst:,0.8918908569952373,0.7931509362620145,False
736,"def application_openFiles_(self, nsapp, filenames): for filename in filenames: logging.info('[osx] receiving from macOS : %s', filename) if os.path.exists(filename): <IF_STMT> sabnzbd.add_nzbfile(filename, keep=True)",if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES:,if os.path.isfile(filename):,0.8896486515035436,0.8289657839357887,False
737,"def check(self, xp, nout): input = xp.asarray(self.x).astype(numpy.float32) with warnings.catch_warnings(): if self.ignore_warning: warnings.simplefilter('ignore', self.ignore_warning) <IF_STMT> self.check_positive(xp, self.func, input, self.eps, nout) else: self.check_negative(xp, self.func, input, self.eps, nout)",if self.result:,if self.is_positive:,0.8843310271421813,0.8531413606256201,False
738,"def _set_scheme(url, newscheme): scheme = _get_scheme(url) newscheme = newscheme or '' newseparator = ':' if newscheme in COLON_SEPARATED_SCHEMES else '://' if scheme == '': url = '%s:%s' % (newscheme, url) elif scheme is None and url: url = ''.join([newscheme, newseparator, url]) elif scheme: remainder = url[len(scheme):] <IF_STMT> remainder = remainder[3:] elif remainder.startswith(':'): remainder = remainder[1:] url = ''.join([newscheme, newseparator, remainder]) return url",if remainder.startswith('://'):,if remainder.startswith('://'):,0.7339478736946844,0.9425437476131634,True
739,"def parquet(tables, data_directory, ignore_missing_dependency, **params): try: import pyarrow as pa import pyarrow.parquet as pq except ImportError: msg = 'PyArrow dependency is missing' <IF_STMT> logger.warning('Ignored: %s', msg) return 0 else: raise click.ClickException(msg) data_directory = Path(data_directory) for table, df in read_tables(tables, data_directory): arrow_table = pa.Table.from_pandas(df) target_path = data_directory / '{}.parquet'.format(table) pq.write_table(arrow_table, str(target_path))",if ignore_missing_dependency:,if ignore_missing_dependency:,0.8953608308681594,0.9284304001296656,True
740,"def h2i(self, pkt, s): t = () if type(s) is str: t = time.strptime(s) t = t[:2] + t[2:-3] el<IF_STMT> y, m, d, h, min, sec, rest, rest, rest = time.gmtime(time.time()) t = (y, m, d, h, min, sec) else: t = s return t",if not s:,if type(s) is int:,0.6957507400216942,0.8806615362338783,False
741,"def filter_episodes(self, batch, cross_entropy): """"""Filter the episodes for the cross_entropy method"""""" accumulated_reward = [sum(rewards) for rewards in batch['rewards']] percentile = cross_entropy * 100 reward_bound = np.percentile(accumulated_reward, percentile) result = {k: [] for k in self.data_keys} episode_kept = 0 for i in range(len(accumulated_reward)): <IF_STMT> for k in self.data_keys: result[k].append(batch[k][i]) episode_kept += 1 return result",if accumulated_reward[i] >= reward_bound:,if reward_bound > episode_kept:,0.9342009288039376,0.8983343737277126,False
742,"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] if envvar in os.environ: value = os.environ[envvar] <IF_STMT> value = value.decode('utf8') return value else: raise InvalidConfigException(""{} - environment variable '{}' not set"".format(msg, var)) else: raise InvalidConfigException(""{} - environment variable name '{}' does not match pattern '{}'"".format(msg, var, _ENV_VAR_PAT_STR))",if six.PY2:,"if isinstance(value, bytes):",0.6693390207307048,0.9118021019905903,False
743,def _allocate_nbd(self): if not os.path.exists('/sys/block/nbd0'): self.error = _('nbd unavailable: module not loaded') return None while True: if not self._DEVICES: self.error = _('No free nbd devices') return None device = self._DEVICES.pop() <IF_STMT> break return device,if not os.path.exists('/sys/block/%s/pid' % os.path.basename(device)):,if not device:,0.8636631012589862,0.8675979125638379,False
744,"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() <IF_STMT> keys.add(k) dep = self.target_database[k] if 'generate_java' in dep.attr: dep.attr['generate_java'] = True queue.extend(dep.deps)",if k not in keys:,if k not in keys:,0.772758898160027,0.8338542560892604,True
745,"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get('partitions', [])) scanners = {} for part_name, part_scanner in list(syntax.get('scanner', {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get('formats', {}).items()): if isinstance(fstyle, basestring): <IF_STMT> key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return (partition_scanner, scanners, formats)",if fstyle.startswith('%(') and fstyle.endswith(')s'):,if fstyle.startswith('_'):,0.8932745087107583,0.933847757608669,False
746,"def rollback(self): for operation, values in self.current_transaction_state[::-1]: <IF_STMT> values.remove() elif operation == 'update': old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",if operation == 'insert':,if operation == 'delete':,0.8565510107108104,0.7801270245332924,False
747,"def _buildOffsets(offsetDict, localeData, indexStart): o = indexStart for key in localeData: <IF_STMT> for k in key.split('|'): offsetDict[k] = o else: offsetDict[key] = o o += 1",if '|' in key:,if '|' in key:,0.8145567016248242,0.7965020533851944,True
748,"def _check_start_pipeline_execution_errors(graphene_info, execution_params, execution_plan): if execution_params.step_keys: for step_key in execution_params.step_keys: <IF_STMT> raise UserFacingGraphQLError(graphene_info.schema.type_named('InvalidStepError')(invalid_step_key=step_key))",if not execution_plan.has_step(step_key):,if step_key not in execution_plan.steps:,0.5449084969261174,0.5856596027429395,False
749,"def __setattr__(self, option_name, option_value): if option_name in self._options: sort = self.OPTIONS[self.arch.name][option_name][0] <IF_STMT> self._options[option_name] = option_value else: raise ValueError('Value for option ""%s"" must be of type %s' % (option_name, sort)) else: super(CFGArchOptions, self).__setattr__(option_name, option_value)","if sort is None or isinstance(option_value, sort):",if sort == 'default':,0.6411386776466657,0.8385130047130208,False
750,"def value(self): quote = False if self.defects: quote = True else: for x in self: <IF_STMT> quote = True if quote: pre = post = '' if self[0].token_type == 'cfws' or self[0][0].token_type == 'cfws': pre = ' ' if self[-1].token_type == 'cfws' or self[-1][-1].token_type == 'cfws': post = ' ' return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",if x.token_type == 'quoted-string':,if x.token_type == 'quote':,0.9229179426768986,0.9099929453837925,False
751,"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: <IF_STMT> filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories","if isinstance(filename_data, list):","if isinstance(filename_data, tuple):",0.795172522017344,0.9062841320510342,False
752,"def _evaluateStack(s): op = s.pop() if op in '+-*/@^': op2 = _evaluateStack(s) op1 = _evaluateStack(s) result = opn[op](op1, op2) <IF_STMT> print(result) return result else: return op",if debug_flag:,if result:,0.773254215574936,0.8590888738245122,False
753,"def reconnect_user(self, user_id, host_id, server_id): if host_id == settings.local.host_id: return if server_id and self.server.id != server_id: return for client in self.clients.find({'user_id': user_id}): self.clients.update_id(client['id'], {'ignore_routes': True}) <IF_STMT> self.instance.disconnect_wg(client['id']) else: self.instance_com.client_kill(client['id'])",if len(client['id']) > 32:,if self.instance:,0.9084501070155765,0.8743414417652072,False
754,"def _get_library(self, name, args): library_database = self._library_manager.get_new_connection_to_library_database() try: last_updated = library_database.get_library_last_updated(name, args) if last_updated: <IF_STMT> self._library_manager.fetch_keywords(name, args, self._libraries_need_refresh_listener) return library_database.fetch_library_keywords(name, args) return self._library_manager.get_and_insert_keywords(name, args) finally: library_database.close()",if time.time() - last_updated > 10.0:,if last_updated > self._library_manager.last_updated_updated:,0.8205875450865094,0.7965020533851944,False
755,"def get_paths(self, path, commit): """"""Return a generator of all filepaths under path at commit."""""" _check_path_is_repo_relative(path) git_path = _get_git_path(path) tree = self.gl_repo.git_repo[commit.tree[git_path].id] assert tree.type == pygit2.GIT_OBJ_TREE for tree_entry in tree: tree_entry_path = os.path.join(path, tree_entry.name) <IF_STMT> for fp in self.get_paths(tree_entry_path, commit): yield fp else: yield tree_entry_path",if tree_entry.type == 'tree':,if os.path.isdir(tree_entry_path):,0.9045571904540268,0.9184043388013005,False
756,"def scan_resource_conf(self, conf): if 'properties' in conf: if 'attributes' in conf['properties']: <IF_STMT> if conf['properties']['attributes']['exp']: return CheckResult.PASSED return CheckResult.FAILED",if 'exp' in conf['properties']['attributes']:,if 'exp' in conf['properties']['attributes']:,0.597595165241553,0.7098232254187811,True
757,"def _set_parse_context(self, tag, tag_attrs): if not self._wb_parse_context: if tag == 'style': self._wb_parse_context = 'style' elif tag == 'script': <IF_STMT> self._wb_parse_context = 'script'",if self._allow_js_type(tag_attrs):,if tag_attrs['style'] == 'script':,0.7242237403360181,0.760856626273165,False
758,"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): <IF_STMT> continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",if item is None:,if item.is_modified:,0.9021733321246608,0.9024521756077707,False
759,def preorder(root): res = [] if not root: return res stack = [] stack.append(root) while stack: root = stack.pop() res.append(root.val) <IF_STMT> stack.append(root.right) if root.left: stack.append(root.left) return res,if root.right:,if root.right:,0.9226384437304782,0.8645707301556367,True
760,"def create(exported_python_target): if exported_python_target not in created: self.context.log.info('Creating setup.py project for {}'.format(exported_python_target)) subject = self.derived_by_original.get(exported_python_target, exported_python_target) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir if self._recursive: for dep in dependencies: <IF_STMT> create(dep)",if is_exported_python_target(dep):,if dep not in created:,0.9189799367813672,0.83268266078192,False
761,"def test_array_interface(self, data): result = np.array(data) np.testing.assert_array_equal(result[0], data[0]) result = np.array(data, dtype=object) expected = np.array(list(data), dtype=object) for a1, a2 in zip(result, expected): <IF_STMT> assert np.isnan(a1) and np.isnan(a2) else: tm.assert_numpy_array_equal(a2, a1)",if np.isscalar(a1):,"if isinstance(a1, np.ndarray):",0.8989778237305389,0.8498644646741501,False
762,"def valueChanged(plug): changed = plug.getInput() is not None if not changed and isinstance(plug, Gaffer.ValuePlug): <IF_STMT> changed = not Gaffer.NodeAlgo.isSetToUserDefault(plug) else: changed = not plug.isSetToDefault() return changed",if Gaffer.NodeAlgo.hasUserDefault(plug):,if plug.getInput() is not None:,0.6060419584285529,0.7685107079449489,False
763,"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) <IF_STMT> major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return (company, major, minor, arch, exe, args)",if version is not None:,if version is not None:,0.7166989072381588,0.8901199011963146,True
764,"def __iter__(self): for name, value in self.__class__.__dict__.items(): if isinstance(value, alias_flag_value): continue <IF_STMT> yield (name, self._has_flag(value.flag))","if isinstance(value, flag_value):","if isinstance(value, Flag):",0.5638610640130246,0.6964705665515707,False
765,"def connect(self): self.sock = sockssocket() self.sock.setproxy(*proxy_args) if type(self.timeout) in (int, float): self.sock.settimeout(self.timeout) self.sock.connect((self.host, self.port)) if isinstance(self, compat_http_client.HTTPSConnection): <IF_STMT> self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host) else: self.sock = ssl.wrap_socket(self.sock)","if hasattr(self, '_context'):",if self._context:,0.9097636619610447,0.8590888738245122,False
766,"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith('java'): if hasattr(sys, 'getswitchinterval'): interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): <IF_STMT> sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)","if hasattr(sys, 'setswitchinterval'):","if hasattr(sys, 'setswitchinterval'):",0.9267991486653911,0.8783650674919876,True
767,"def vars(self): ret = [] if op.intlist: varlist = op.intlist else: varlist = self.discover for name in varlist: if name in ('0', '1', '2', '8', 'CPU0', 'ERR', 'LOC', 'MIS', 'NMI'): varlist.remove(name) if not op.full and len(varlist) > 3: varlist = varlist[-3:] for name in varlist: if name in self.discover: ret.append(name) <IF_STMT> ret.append(self.intmap[name.lower()]) return ret",elif name.lower() in self.intmap:,elif name.lower() in self.intmap:,0.9103854184780812,0.9001816649635144,True
768,"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) <IF_STMT> count = len(inst) toReturn.append(gadget) added = True if callback: callback(gadget, added, float(i + 1) / len_gadgets) added = False return toReturn",if len(inst) > count:,if count == len_gadgets:,0.9125854744163948,0.8832000938217648,False
769,"def ident(self): value = self._ident if value is False: value = None if not self.orig_prefix: wrapped = self.wrapped ident = getattr(wrapped, 'ident', None) <IF_STMT> value = self._wrap_hash(ident) self._ident = value return value",if ident is not None:,if ident is not None:,0.7994017380017103,0.8094220211349227,True
770,"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: if isinstance(field, _ContainerMixin): setting_values.update(self._flatten_settings_from_form(settings, field, form_values[field._name])) <IF_STMT> setting_values[field._name] = form_values[field._name] return setting_values",elif field._name in settings:,"elif isinstance(field, _FormMixin):",0.9060301918336273,0.8749766281017177,False
771,"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != '__init__': <IF_STMT> continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",if not private and name.startswith('_'):,"if name in (__module__, '__init__', '__init__.__module__):",0.8952626429196389,0.7736680847834176,False
772,"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) <IF_STMT> return False if not b1: return True",if b1 != b2:,if b1 != b2:,0.8444329018889719,0.828399516355805,True
773,"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError <IF_STMT> self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",if self.ctl:,if value:,0.6953428304006363,0.9312457603037672,False
774,"def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]: self.invoke_threads() total_links = 0 for hyperlink in hyperlinks.values(): <IF_STMT> yield CheckResult(hyperlink.uri, hyperlink.docname, hyperlink.lineno, 'ignored', '', 0) else: self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False) total_links += 1 done = 0 while done < total_links: yield self.rqueue.get() done += 1 self.shutdown_threads()",if self.is_ignored_uri(hyperlink.uri):,if hyperlink.is_ignored():,0.9337464023800899,0.9202663016973823,False
775,"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: <IF_STMT> subscriber._pyroRelease() if hasattr(subscriber, '_pyroUri'): try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)","if hasattr(subscriber, '_pyroRelease'):","if hasattr(subscriber, '_pyroRelease'):",0.7753880265630236,0.8120341702859789,True
776,"def delete_arc(collection, document, origin, target, type): directory = collection real_dir = real_directory(directory) mods = ModificationTracker() projectconf = ProjectConfiguration(real_dir) document = path_join(real_dir, document) with TextAnnotations(document) as ann_obj: <IF_STMT> raise AnnotationsIsReadOnlyError(ann_obj.get_document()) _delete_arc_with_ann(origin, target, type, mods, ann_obj, projectconf) mods_json = mods.json_response() mods_json['annotations'] = _json_from_ann(ann_obj) return mods_json",if ann_obj._read_only:,if not ann_obj.get_document():,0.7568396503885588,0.8953711787948615,False
777,"def _select_from(self, parent_path, is_dir, exists, listdir): if not is_dir(parent_path): return with _cached(listdir) as listdir: yielded = set() try: successor_select = self.successor._select_from for starting_point in self._iterate_directories(parent_path, is_dir, listdir): for p in successor_select(starting_point, is_dir, exists, listdir): <IF_STMT> yield p yielded.add(p) finally: yielded.clear()",if p not in yielded:,if p not in yielded:,0.9143710877911029,0.845713978670975,True
778,"def _fractional_part(self, n, expr, evaluation): n_sympy = n.to_sympy() if n_sympy.is_constant(): <IF_STMT> positive_integer_part = Expression('Floor', n).evaluate(evaluation).to_python() result = n - positive_integer_part else: negative_integer_part = Expression('Ceiling', n).evaluate(evaluation).to_python() result = n - negative_integer_part else: return expr return from_python(result)",if n_sympy >= 0:,if evaluation.is_positive():,0.6142222319068005,0.8966773400768917,False
779,"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: if geometry[0] > 180 or geometry[0] < -180: raise ValueError('Longitude is out of bounds, check your JSON format or data') <IF_STMT> raise ValueError('Latitude is out of bounds, check your JSON format or data')",if geometry[1] > 90 or geometry[1] < -90:,if geometry[1] > -180 or geometry[1] < -180:,0.6556066775775129,0.8030129514032833,False
780,"def get_absolute_path(self, root, path): self.root = self.roots[0] for root in self.roots: abspath = os.path.abspath(os.path.join(root, path)) <IF_STMT> self.root = root break return abspath",if os.path.exists(abspath):,if os.path.isdir(abspath):,0.875838324670199,0.8318180062062374,False
781,"def do_setflow(self, l=''): try: <IF_STMT> l = str(self.flow_slider.GetValue()) else: l = l.lower() flow = int(l) if self.p.online: self.p.send_now('M221 S' + l) self.log(_('Setting print flow factor to %d%%.') % flow) else: self.logError(_('Printer is not online.')) except Exception as x: self.logError(_('You must enter a flow. (%s)') % (repr(x),))","if not isinstance(l, str) or not len(l):",if self.flow_slider:,0.909284049956636,0.9220450449751959,False
782,def sources(): for d in os.listdir(base): <IF_STMT> continue if d == 'indcat': continue if not os.path.isdir(base + d): continue yield d,if d.endswith('old'):,if d.startswith('.py') or d.startswith('.py') or d == 'dist':,0.7772709708755862,0.6364713244392607,False
783,"def create_accumulator(self) -> tf_metric_accumulators.TFCompilableMetricsAccumulator: configs = zip(self._metric_configs, self._loss_configs) padding_options = None if self._eval_config is not None: model_spec = model_util.get_model_spec(self._eval_config, self._model_name) <IF_STMT> padding_options = model_spec.padding_options return tf_metric_accumulators.TFCompilableMetricsAccumulator(padding_options, [len(m) + len(l) for m, l in configs], desired_batch_size=self._desired_batch_size)",if model_spec is not None and model_spec.HasField('padding_options'):,if model_spec is not None:,0.7998731496690383,0.8248765135255685,False
784,"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): <IF_STMT> if self.expr.resultsName: tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return (loc, tokens)",if self.defaultValue is not self.__optionalNotMatched:,if self.defaultValue is not None:,0.7217834353824293,0.8294838585473985,False
785,"def handleConnection(self): try: <IF_STMT> return True self.csock.close() except: ex_t, ex_v, ex_tb = sys.exc_info() tb = util.formatTraceback(ex_t, ex_v, ex_tb) log.warning('error during connect/handshake: %s; %s', ex_v, '\n'.join(tb)) self.csock.close() return False",if self.daemon._handshake(self.csock):,"if self.csock.connect(self.host, self.port):",0.8927852843468803,0.8390782502060267,False
786,"def getProc(su, innerTarget): if len(su) == 1: proc = ('first', 'last') el<IF_STMT> proc = ('first', 'last') elif su.isFirst(innerTarget): proc = ('first',) elif su.isLast(innerTarget): proc = ('last',) else: proc = () return proc",if su.isFirst(innerTarget) and su.isLast(innerTarget):,if su.isFirst(innerTarget):,0.6298565416082208,0.8866029039778043,False
787,"def get_color_dtype(data, column_names): has_color = all((column in data['points'] for column in column_names)) if has_color: color_data_types = [data['points'][column_name].dtype for column_name in column_names] <IF_STMT> raise TypeError(f'Data types of color values are inconsistent: got {color_data_types}') color_data_type = color_data_types[0] else: color_data_type = None return color_data_type",if len(set(color_data_types)) > 1:,if len(color_data_types) != 1:,0.6649529935603324,0.8692960007731574,False
788,"def close(self): children = [] for children_part, line_offset, last_line_offset_leaf in self.children_groups: <IF_STMT> try: _update_positions(children_part, line_offset, last_line_offset_leaf) except _PositionUpdatingFinished: pass children += children_part self.tree_node.children = children for node in children: node.parent = self.tree_node",if line_offset != 0:,if children_part:,0.9038160063314784,0.8866029039778043,False
789,"def get_multi(self, keys, index=None): with self._lmdb.begin() as txn: result = [] for key in keys: packed = txn.get(key.encode()) <IF_STMT> result.append((key, cbor.loads(packed))) return result",if packed is not None:,if packed is not None:,0.748958784633474,0.7406093667638122,True
790,"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: if p[0] != '.': subp = os.path.join(pth, p) p = os.path.join(prefix, p) <IF_STMT> res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",if recursive and os.path.isdir(subp):,if recursive:,0.7324445832303581,0.9024521756077707,False
791,"def __schedule(self, workflow_scheduler_id, workflow_scheduler): invocation_ids = self.__active_invocation_ids(workflow_scheduler_id) for invocation_id in invocation_ids: log.debug('Attempting to schedule workflow invocation [%s]', invocation_id) self.__attempt_schedule(invocation_id, workflow_scheduler) <IF_STMT> return",if not self.monitor_running:,if self.__is_scheduled(invocation_id):,0.7165334678076051,0.8706099548745285,False
792,"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: if self.size: data, passon = (data[:self.size], data[self.size:]) else: passon = b'' <IF_STMT> self.data.append(data) return passon",if data:,if len(data) > 0:,0.8765600526194581,0.8228500218338367,False
793,"def __getstate__(self): try: store_func, load_func = (self.store_function, self.load_function) self.store_function, self.load_function = (None, None) d = dict(((k, v) for k, v in self.__dict__.items() <IF_STMT> not in {'analyses'})) return d finally: self.store_function, self.load_function = (store_func, load_func)",if k,if k.lower(),0.7298638432141907,0.8935248372106969,False
794,"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local <IF_STMT> self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",if self.scroll_up_rect().collidepoint(p):,if self.scroll_up_rect().collidepoint(p):,0.864063222388007,0.8827916928185874,True
795,"def on_api_command(self, command, data): if command == 'select': if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, 'Insufficient permissions') if self._prompt is None: return flask.abort(409, 'No active prompt') choice = data['choice'] <IF_STMT> return flask.abort(400, '{!r} is not a valid value for choice'.format(choice)) self._answer_prompt(choice)","if not isinstance(choice, int) or not self._prompt.validate_choice(choice):","if not isinstance(choice, str):",0.7944751031844941,0.8627586293513119,False
796,"def register_predictors(self, model_data_arr): for integration in self._get_integrations(): <IF_STMT> integration.register_predictors(model_data_arr) else: logger.warning(f""There is no connection to {integration.name}. predictor wouldn't be registred."")",if integration.check_connection():,if integration.connection_to_model_data_arr is not None:,0.5705278210910496,0.7049592608322395,False
797,"def _pack_shears(shearData): shears = list() vidxs = list() for e_idx, entry in enumerate(shearData): <IF_STMT> shears.extend([float('nan'), float('nan')]) vidxs.extend([0, 0]) else: vidx1, vidx2, shear1, shear2 = entry shears.extend([shear1, shear2]) vidxs.extend([vidx1, vidx2]) return (np.asarray(shears, dtype=np.float32), np.asarray(vidxs, dtype=np.uint32))",if entry is None:,if e_idx == 0:,0.7654827713629709,0.8431339019329497,False
798,"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): <IF_STMT> continue yield (cog_name, cog_id)","if not isinstance(inner, dict):",if inner is None:,0.9361855183639987,0.8923575006167597,False
799,"def subFeaName(m, newNames, state): try: int(m[3], 16) except: return m[0] name = m[2] if name in newNames: <IF_STMT> print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4])) state['didChange'] = True return m[1] + newNames[name] + m[4] return m[0]",if name == 'uni0402':,if state['didChange']:,0.9317633166389637,0.9099951253570094,False
800,"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: if input_array is None: input_array = model.example_input_array <IF_STMT> input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn('Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given', UserWarning)",if input_array is not None:,"elif isinstance(input_array, list):",0.8181503978158482,0.884617925078158,False
801,"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) <IF_STMT> return True if not event.get_date_object(): return True return False",if not event.get_place_handle():,if not event:,0.9199447352053701,0.8592899528284996,False
802,"def format(m): if m > 1000: <IF_STMT> return (str(int(m / 1000)), 'km') else: return (str(round(m / 1000, 1)), 'km') return (str(m), 'm')",if m % 1000 == 0:,if m % 1000 == 0:,0.5985671792570446,0.7010615979282331,True
803,"def previous(self): try: idx = _jump_list_index next_index = idx + 1 <IF_STMT> next_index = 100 next_index = min(len(_jump_list) - 1, next_index) _jump_list_index = next_index return _jump_list[next_index] except (IndexError, KeyError) as e: return None",if next_index > 100:,if next_index > 100:,0.6926995248553092,0.8385130047130208,True
804,"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition['spec'] <IF_STMT> self.hyperparam_dict[name] = spec['DefaultValue'] elif 'IsRequired' in spec and spec['IsRequired']: raise ValueError('Required hyperparameter: %s is not set' % name)",if 'DefaultValue' in spec:,if 'DefaultValue' in spec and spec['DefaultValue']:,0.7390391224447861,0.80846720196545,False
805,"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: if self.action_input.selected_index == 0: self.back_to_parent() <IF_STMT> self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) elif self.action_input.selected_index == 2: self._apply_prefs() self.back_to_parent()",elif self.action_input.selected_index == 1:,elif self.action_input.selected_index == 1:,0.8342942799116739,0.7709002428237395,True
806,"def _split_anonymous_function(s): if s[:1] == '[' and s[-1:] == ']' and (':' in s): try: l = yaml_util.decode_yaml(s) except Exception: return (None, s[1:-1]) else: <IF_STMT> return (None, s[1:-1]) return None","if len(l) == 1 and isinstance(l[0], (six.string_types, int)):",if l and l['name'] == 'anonymous':,0.8913150472207305,0.7736680847834176,False
807,"def test_source_address(self): for addr, is_ipv6 in VALID_SOURCE_ADDRESSES: <IF_STMT> warnings.warn('No IPv6 support: skipping.', NoIPv6Warning) continue pool = HTTPConnectionPool(self.host, self.port, source_address=addr, retries=False) self.addCleanup(pool.close) r = pool.request('GET', '/source_address') self.assertEqual(r.data, b(addr[0]))",if is_ipv6 and (not HAS_IPV6_AND_DNS):,if not is_ipv6:,0.889259557999573,0.833078701050083,False
808,"def vim_G(self): """"""Put the cursor on the last character of the file."""""" if self.is_text_wrapper(self.w): <IF_STMT> self.do('end-of-buffer-extend-selection') else: self.do('end-of-buffer') self.done() else: self.quit()",if self.state == 'visual':,if self.w.get_cursor() == self.cursor_pos:,0.5764884015633105,0.7498810286408993,False
809,"def backend_supported(module, manager, **kwargs): if CollectionNodeModule.backend_supported(module, manager, **kwargs): if 'tid' not in kwargs: return True conn = manager.connection(did=kwargs['did']) template_path = 'partitions/sql/{0}/#{0}#{1}#'.format(manager.server_type, manager.version) SQL = render_template('/'.join([template_path, 'backend_support.sql']), tid=kwargs['tid']) status, res = conn.execute_scalar(SQL) <IF_STMT> return internal_server_error(errormsg=res) return res",if not status:,if status != 200:,0.8581528530026444,0.8516228624291206,False
810,"def _get_regex_config(self, data_asset_name: Optional[str]=None) -> dict: regex_config: dict = copy.deepcopy(self._default_regex) asset: Optional[Asset] = None if data_asset_name: asset = self._get_asset(data_asset_name=data_asset_name) if asset is not None: <IF_STMT> regex_config['pattern'] = asset.pattern if asset.group_names: regex_config['group_names'] = asset.group_names return regex_config",if asset.pattern:,if asset.pattern:,0.9302031639991769,0.8966773400768917,True
811,"def resolve(self, other): if other == ANY_TYPE: return self elif isinstance(other, ComplexType): f = self.first.resolve(other.first) s = self.second.resolve(other.second) <IF_STMT> return ComplexType(f, s) else: return None elif self == ANY_TYPE: return other else: return None",if f and s:,if f is not None and s is not None:,0.6004139125218569,0.7211389453884708,False
812,"def collect_pages(app): new_images = {} for full_path, basename in app.builder.images.iteritems(): base, ext = os.path.splitext(full_path) retina_path = base + '@2x' + ext <IF_STMT> new_images[retina_path] = app.env.images[retina_path][1] app.builder.images.update(new_images) return []",if retina_path in app.env.images:,if retina_path in app.env.images:,0.8345165626982638,0.8105932471967202,True
813,"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): <IF_STMT> return True if linenum > 0 and line[0] not in '\t ': return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",if not line:,if _has_newline(line):,0.9116586277976277,0.9312457603037672,False
814,"def reader(): try: imgs = mp4_loader(video_path, seg_num, seglen, mode) <IF_STMT> logger.error('{} frame length {} less than 1.'.format(video_path, len(imgs))) yield (None, None) except: logger.error('Error when loading {}'.format(mp4_path)) yield (None, None) imgs_ret = imgs_transform(imgs, mode, seg_num, seglen, short_size, target_size, img_mean, img_std) label_ret = video_path yield (imgs_ret, label_ret)",if len(imgs) < 1:,if len(imgs) > 1:,0.7607762423296867,0.8806615362338783,False
815,"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) <IF_STMT> for separator in (' & ', '; ', ' and ', ' vs. ', ' with ', ' y '): if separator in sortname: parts = sortname.split(separator) break else: parts = [sortname] separator = '' return separator.join(map(_reverse_sortname, parts)) return name",if ctg[0] == 'L' and unicodedata.name(c).find('LATIN') == -1:,if ctg == 'artist':,0.6927525483353915,0.9084940438173679,False
816,"def _to_local_path(path): """"""Convert local path to SFTP path"""""" if sys.platform == 'win32': path = os.fsdecode(path) <IF_STMT> path = path[1:] path = path.replace('/', '\\') return path",if path[:1] == '/' and path[2:3] == ':':,if path.startswith('/'):,0.6361911461898003,0.8531413606256201,False
817,"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == 'tacotron': text = tacotron_cleaner.cleaners.custom_english_cleaners(text) <IF_STMT> text = jaconv.normalize(text) elif t == 'vietnamese': if vietnamese_cleaners is None: raise RuntimeError('Please install underthesea') text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f'Not supported: type={t}') return text",elif t == 'jaconv':,elif t == 'jaconv':,0.7748477892633915,0.8752376177722327,True
818,"def cb_syncthing_system_data(self, daemon, mem, cpu, d_failed, d_total): if self.daemon.get_my_id() in self.devices: device = self.devices[self.daemon.get_my_id()] device['ram'] = sizeof_fmt(mem) device['cpu'] = '%3.2f%%' % cpu <IF_STMT> device['announce'] = _('disabled') else: device['announce'] = '%s/%s' % (d_total - d_failed, d_total)",if d_total == 0:,if d_failed == 0:,0.7330853261737651,0.8474968231198384,False
819,"def update_kls(self, sampled_kls): for i, kl in enumerate(sampled_kls): <IF_STMT> self.kl_coeff_val[i] *= 0.5 elif kl > 1.5 * self.kl_target: self.kl_coeff_val[i] *= 2.0 return self.kl_coeff_val",if kl < self.kl_target / 1.5:,if kl < 0.5 * self.kl_target:,0.826073587471344,0.7126109689973791,False
820,"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): if self.FindItemAtPosition((r, c)) is not None: f = False <IF_STMT> cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]",if f:,if f:,0.797029775276622,0.9220450449751959,True
821,"def get_session(self): if self._session is None: session = super(ChildResourceManager, self).get_session() <IF_STMT> session = session.get_session_for_resource(self.resource_type.resource) self._session = session return self._session",if self.resource_type.resource != constants.RESOURCE_ACTIVE_DIRECTORY:,if self.resource_type:,0.7071953275099748,0.803154665668484,False
822,"def _get_master_authorized_networks_config(self, raw_cluster): if raw_cluster.get('masterAuthorizedNetworksConfig'): config = raw_cluster.get('masterAuthorizedNetworksConfig') config['includes_public_cidr'] = False for block in config['cidrBlocks']: <IF_STMT> config['includes_public_cidr'] = True return config else: return {'enabled': False, 'cidrBlocks': [], 'includes_public_cidr': False}",if block['cidrBlock'] == '0.0.0.0/0':,if block['master_authorized_cidr']:,0.9077244176106418,0.8743414417652072,False
823,"def scan_folder(folder): scanned_files = [] for root, dirs, files in os.walk(folder): dirs[:] = [d for d in dirs if d != '__pycache__'] relative_path = os.path.relpath(root, folder) for f in files: <IF_STMT> continue relative_name = os.path.normpath(os.path.join(relative_path, f)).replace('\\', '/') scanned_files.append(relative_name) return sorted(scanned_files)",if f.endswith('.pyc'):,if f.startswith('.py'):,0.9258340754072145,0.9099951253570094,False
824,def read_progress(self): while True: processed_file = self.queue.get() self.threading_completed.append(processed_file) total_number = len(self.file_list) completed_number = len(self.threading_completed) if _progress_emitter: _progress_emitter.update_progress(completed_number * 100 // total_number) <IF_STMT> break,if total_number == completed_number:,if completed_number == total_number:,0.9315766104416253,0.8047418499398723,False
825,"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): <IF_STMT> return False continue if line.startswith('def ') or line.startswith('class '): return True if line.startswith(('#', '@', ' ', ')')): continue return False return False",if i > 0 and (not lines[i - 1].strip()):,if i == 0:,0.9437429244206965,0.9036816878108535,False
826,def __next__(self): try: data = next(self.iter_loader) except StopIteration: self._epoch += 1 <IF_STMT> self._dataloader.sampler.set_epoch(self._epoch) self.iter_loader = iter(self._dataloader) data = next(self.iter_loader) return data,"if hasattr(self._dataloader.sampler, 'set_epoch'):",if self._epoch % self.num_samples == 0:,0.6450054281255938,0.6885326214539055,False
827,"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: if dt is not None: <IF_STMT> return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)","if isinstance(dt, dgl.DGLGraph):","if isinstance(dt, dgl.DGLBatch):",0.8506697058282632,0.9196822664155297,False
828,"def f(self, info): for k in keys: <IF_STMT> for k2 in list(info.keys()): if k(k2): info.pop(k2) else: info.pop(k, None)",if callable(k):,"if isinstance(info, dict):",0.5960506551444573,0.7483293841345244,False
829,"def create(path, binary=False): for i in range(10): try: os.makedirs(os.path.dirname(path), exist_ok=True) <IF_STMT> return open(path, 'wb') else: return open(path, 'w', encoding='utf-8') if i > 0: log(True, f'Created {path} at attempt {i + 1}') except: time.sleep(0.5) else: raise Error(f'Failed to create {path}')",if binary:,if binary:,0.8956862254023312,0.9076141716697395,True
830,"def validate_update(self, update_query): structure = DotCollapsedDict(self.doc_class.structure) for op, fields in update_query.iteritems(): for field in fields: if op != '$unset' and op != '$rename': <IF_STMT> raise UpdateQueryError(""'%s' not found in %s's structure"" % (field, self.doc_class.__name__))",if field not in structure:,if field not in structure:,0.902111867226062,0.8200123297196334,True
831,"def check_enums_ATLAS_ISAEXT(lines): for i, isaext in enumerate(ATLAS_ISAEXT): got = lines.pop(0).strip() <IF_STMT> expect = 'none: 1' else: expect = '{0}: {1}'.format(isaext, 1 << i) if got != expect: raise RuntimeError('ATLAS_ISAEXT mismatch at position ' + str(i) + ': got >>' + got + '<<, expected >>' + expect + '<<')",if i == 0:,if isaext == 'none':,0.7026837750280562,0.8902056737869248,False
832,"def _test_export_session_csv(self, test_session=None): with self.app.test_request_context(): <IF_STMT> test_session = SessionFactory() field_data = export_sessions_csv([test_session]) session_row = field_data[1] self.assertEqual(session_row[0], 'example (accepted)') self.assertEqual(session_row[9], 'accepted')",if not test_session:,if test_session is None:,0.6941312318367404,0.7378351342269067,False
833,"def get_report_to_platform(self, args, scan_reports): if self.bc_api_key: <IF_STMT> repo_id = self.get_repository(args) self.setup_bridgecrew_credentials(bc_api_key=self.bc_api_key, repo_id=repo_id) if self.is_integration_configured(): self._upload_run(args, scan_reports)",if args.directory:,if self.is_bridgecrew_configured():,0.708395886959039,0.762465858623486,False
834,"def test_fvalue(self): if not getattr(self, 'skip_f', False): rtol = getattr(self, 'rtol', 1e-10) assert_allclose(self.res1.fvalue, self.res2.F, rtol=rtol) <IF_STMT> assert_allclose(self.res1.f_pvalue, self.res2.Fp, rtol=rtol) else: raise pytest.skip('TODO: document why this test is skipped')","if hasattr(self.res2, 'Fp'):","if hasattr(self, 'res1_f_pvalue'):",0.8996781793627869,0.8390782502060267,False
835,"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): if e.value is None: e.value = [] <IF_STMT> e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",elif type(e.value) is not list:,elif type(e) is str:,0.874536783897294,0.9215557939968955,False
836,"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) if not node.isdir: raise AssertionError('Not a folder: %s' % self.path) <IF_STMT> self.watcher.emit('created', self)",if self.watcher:,if self.watcher:,0.9286136207775035,0.8751809448365924,True
837,"def __init__(self, _inf=None, _tzinfos=None): if _inf: self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = _inf else: _tzinfos = {} self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = self._transition_info[0] _tzinfos[self._transition_info[0]] = self for inf in self._transition_info[1:]: <IF_STMT> _tzinfos[inf] = self.__class__(inf, _tzinfos)",if not _tzinfos.has_key(inf):,if inf not in _tzinfos:,0.8010773466331098,0.8380055871435848,False
838,"def test_sample_output(): comment = 'SAMPLE OUTPUT' skip_files = ['__init__.py'] errors = [] for _file in sorted(MODULE_PATH.iterdir()): <IF_STMT> with _file.open() as f: if comment not in f.read(): errors.append((comment, _file)) if errors: line = 'Missing sample error(s) detected!\n\n' for error in errors: line += '`{}` is not in module `{}`\n'.format(*error) print(line[:-1]) assert False",if _file.suffix == '.py' and _file.name not in skip_files:,if _file.name not in skip_files:,0.7896462936184225,0.8777008558345754,False
839,"def http_get(url, target): req = requests.get(url, stream=True) content_length = req.headers.get('Content-Length') total = int(content_length) if content_length is not None else None progress = tqdm(unit='B', total=total) with open(target, 'wb') as target_file: for chunk in req.iter_content(chunk_size=1024): <IF_STMT> progress.update(len(chunk)) target_file.write(chunk) progress.close()",if chunk:,if chunk:,0.9290460144153969,0.9024521756077707,True
840,"def _elements_to_datasets(self, elements, level=0): for element in elements: extra_kwds = {'identifier_%d' % level: element['name']} <IF_STMT> for inner_element in self._elements_to_datasets(element['elements'], level=level + 1): dataset = extra_kwds.copy() dataset.update(inner_element) yield dataset else: dataset = extra_kwds extra_kwds.update(element) yield extra_kwds",if 'elements' in element:,if 'elements' in element:,0.6427240916619298,0.8474968231198384,True
841,"def update_dict(a, b): for key, value in b.items(): if value is None: continue <IF_STMT> a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",if key not in a:,if key not in a:,0.7233003756140882,0.8200123297196334,True
842,"def scan(self, targets): for target in targets: target.print_infos() if self.is_interesting(target): self.target['other'].append(target) <IF_STMT> return target return None",if self.match(target):,elif self.is_interesting(target):,0.8422357737611639,0.762465858623486,False
843,"def printConnections(switches): """"""Compactly print connected nodes to each switch"""""" for sw in switches: output('%s: ' % sw) for intf in sw.intfList(): link = intf.link <IF_STMT> intf1, intf2 = (link.intf1, link.intf2) remote = intf1 if intf1.node != sw else intf2 output('%s(%s) ' % (remote.node, sw.ports[intf])) output('\n')",if link:,if link:,0.7692529324332547,0.9202663016973823,True
844,"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P) begin, nexti = (0, 0) for i, char in enumerate(sentence): pos = pos_list[i] if pos == 'B': begin = i <IF_STMT> yield sentence[begin:i + 1] nexti = i + 1 elif pos == 'S': yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",elif pos == 'E':,if begin < len(sentence):,0.9346015320552852,0.9084940438173679,False
845,"def check_files(self, paths=None): """"""Run all checks on the paths."""""" if paths is None: paths = self.paths report = self.options.report runner = self.runner report.start() try: for path in paths: <IF_STMT> self.input_dir(path) elif not self.excluded(path): runner(path) except KeyboardInterrupt: print('... stopped') report.stop() return report",if os.path.isdir(path):,if self.is_dir(path):,0.9440682511911143,0.9122561819614461,False
846,"def verts_of_loop(edge_loop): verts = [] for e0, e1 in iter_pairs(edge_loop, False): <IF_STMT> v0 = e0.shared_vert(e1) verts += [e0.other_vert(v0), v0] verts += [e1.other_vert(verts[-1])] if len(verts) > 1 and verts[0] == verts[-1]: return verts[:-1] return verts",if not verts:,if e0.shared_vert(e1) and e1.shared_vert(e0):,0.8537528459918255,0.8431339019329497,False
847,"def generator(self, data): for task in data: <IF_STMT> continue for bucket in task.bash_hash_entries(): yield (0, [int(task.p_pid), str(task.p_comm), int(bucket.times_found), str(bucket.key), str(bucket.data.path)])",if not (self._config.SCAN_ALL or str(task.p_comm) == 'bash'):,if task.p_pid == 0:,0.8568095415126489,0.7378351342269067,False
848,"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() if w: aList = w.sizes() <IF_STMT> n1, n2 = aList ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",if len(aList) == 2:,if len(aList) > 0:,0.8929464784638246,0.8902056737869248,False
849,"def geterrors(self): """"""Get all error messages."""""" notes = self.getnotes(origin='translator').split('\n') errordict = {} for note in notes: <IF_STMT> error = note.replace('(pofilter) ', '') errorname, errortext = error.split(': ', 1) errordict[errorname] = errortext return errordict",if '(pofilter) ' in note:,if note.startswith('(pofilter) '):,0.8891265537863657,0.8635707684233572,False
850,"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: if d in ['.', '..']: continue d_path = ''.join([path, '/', d]) d_new_path = ''.join([new_path, '/', d]) attr = self.getattr(d_path) <IF_STMT> self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)",if stat.S_ISDIR(attr['st_mode']):,if attr == 'dir':,0.750989834200972,0.8832000938217648,False
851,"def index(self, url_id: int) -> FlaskResponse: url = db.session.query(models.Url).get(url_id) if url and url.url: explore_url = '//superset/explore/?' <IF_STMT> explore_url += f'r={url_id}' return redirect(explore_url[1:]) return redirect(url.url[1:]) flash('URL to nowhere...', 'danger') return redirect('/')",if url.url.startswith(explore_url):,if url_id:,0.6146097836348213,0.8787142254774354,False
852,"def testShortCircuit(self): """"""Test that creation short-circuits to reuse existing references"""""" sd = {} for s in self.ss: sd[s] = 1 for t in self.ts: <IF_STMT> self.assertTrue(sd.has_key(safeRef(t.x))) self.assertTrue(safeRef(t.x) in sd) else: self.assertTrue(sd.has_key(safeRef(t))) self.assertTrue(safeRef(t) in sd)","if hasattr(t, 'x'):","if isinstance(t, (Tuple, Tuple)):",0.907214384453364,0.8431339019329497,False
853,"def wrapped(request, *args, **kwargs): if not request.user.is_authenticated(): request.session['_next'] = request.get_full_path() <IF_STMT> redirect_uri = reverse('sentry-auth-organization', args=[kwargs['organization_slug']]) else: redirect_uri = get_login_url() return HttpResponseRedirect(redirect_uri) return func(request, *args, **kwargs)",if 'organization_slug' in kwargs:,if 'organization_slug' in kwargs:,0.7422460718459367,0.7886336751695258,True
854,"def read_info(reader, dump=None): line_number_table_length = reader.read_u2() <IF_STMT> reader.debug('' * dump, 'Line numbers (%s total):' % line_number_table_length) line_numbers = [] for i in range(0, line_number_table_length): start_pc = reader.read_u2() line_number = reader.read_u2() if dump is not None: reader.debug('' * (dump + 1), '%s: %s' % (start_pc, line_number)) line_numbers.append((start_pc, line_number)) return LineNumberTable(line_numbers)",if dump is not None:,if dump is not None:,0.7859309728281842,0.8729118929672821,True
855,"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 if 0 < dt: break else: dt = t2 - previous if dt <= 0.0: continue <IF_STMT> precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",if precision is not None:,if precision:,0.7590510345226711,0.9506506804223306,False
856,def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() <IF_STMT> pass else: lineno = self.expr2.get_hi_lineno() if self.expr3 is None: pass else: lineno = self.expr3.get_hi_lineno() return lineno,if self.expr2 is None:,if self.expr2 is None:,0.6085268579162084,0.828399516355805,True
857,"def validate_cluster_resource_group(cmd, namespace): if namespace.cluster_resource_group is not None: client = get_mgmt_service_client(cmd.cli_ctx, ResourceType.MGMT_RESOURCE_RESOURCES) <IF_STMT> raise InvalidArgumentValueError(""Invalid --cluster-resource-group '%s': resource group must not exist."" % namespace.cluster_resource_group)",if client.resource_groups.check_existence(namespace.cluster_resource_group):,if not client.exists(namespace.cluster_resource_group):,0.6929601730331327,0.8120341702859789,False
858,"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: <IF_STMT> done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",if left == 0:,if left == len(text):,0.8203465991603354,0.9069443196104878,False
859,"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] if len(set(lengths)) > 1: raise ValueError('Not all values of X are of equal length.') x_len = lengths[0] else: x_len = len(X) if y is not None: <IF_STMT> raise ValueError('X and y are not of equal length.') if self.regression and y is not None and (y.ndim == 1): y = y.reshape(-1, 1) return (X, y)",if len(y) != x_len:,if x_len != len(y):,0.952547226837048,0.9204199807826591,False
860,"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): if node.tag_type == OPEN_TAG: open_tags += 1 <IF_STMT> open_tags -= 1 elif isinstance(node, HtmlDataFragment) and node.is_text_content and (open_tags == 0): text.append(html_body[node.start:node.end]) return text",elif node.tag_type == CLOSE_TAG:,elif node.tag_type == CLOSE_TAG:,0.7907276341630389,0.8627586293513119,True
861,"def _get_spyne_type(cls_name, k, v): try: v = NATIVE_MAP.get(v, v) except TypeError: return try: subc = issubclass(v, ModelBase) or issubclass(v, SelfReference) except: subc = False if subc: if issubclass(v, Array) and len(v._type_info) != 1: raise Exception('Invalid Array definition in %s.%s.' % (cls_name, k)) <IF_STMT> raise Exception('Please specify the number of dimensions') return v","elif issubclass(v, Point) and v.Attributes.dim is None:","elif not issubclass(v, Array) and len(v._type_info) != 2:",0.9180797457001582,0.8340333848097756,False
862,"def customize(cls, **kwargs): """"""return a class with some existing attributes customized"""""" for name, value in kwargs.iteritems(): <IF_STMT> raise TransportError('you cannot customize the protected attribute %s' % name) if not hasattr(cls, name): raise TransportError('Transport has no attribute %s' % name) NewSubClass = type('Customized_{}'.format(cls.__name__), (cls,), kwargs) return NewSubClass","if name in ['cookie', 'circuit', 'upstream', 'downstream', 'stream']:",if value is not None and name in cls.__protected_attributes:,0.7726010746785789,0.7990456121647639,False
863,"def test_UNrelativize(self): import URIlib relative = self.relative + self.full_relativize for base, rel, fullpath, common in relative: URI = uriparse.UnRelativizeURL(base, rel) fullURI = URIlib.URIParser(URI) <IF_STMT> fullpath = fullpath[:-1] self.failUnlessSamePath(os.path.normcase(fullURI.path), os.path.normcase(fullpath))","if fullpath[-1] in ('/', '\\'):",if fullpath.endswith('/'):,0.8515623038157114,0.8743414417652072,False
864,"def get_release_info(file_path=RELEASE_FILE): RELEASE_TYPE_REGEX = re.compile('^[Rr]elease [Tt]ype: (major|minor|patch)$') with open(file_path, 'r') as f: line = f.readline() match = RELEASE_TYPE_REGEX.match(line) <IF_STMT> print('The file RELEASE.md should start with `Release type` and specify one of the following values: major, minor or patch.') sys.exit(1) type_ = match.group(1) changelog = ''.join([line for line in f.readlines()]).strip() return (type_, changelog)",if not match:,if not match:,0.8076599577459638,0.9134996171406936,True
865,"def _get_next_history_entry(self): if self._history: hist_len = len(self._history) - 1 self.history_index = min(hist_len, self.history_index + 1) index = self.history_index <IF_STMT> self.history_index += 1 return self._history[index] return ''",if self.history_index == hist_len:,if self._history[index] is not None:,0.7731013980175492,0.7685107079449489,False
866,"def star_op(self): """"""Put a '*' op, with special cases for *args."""""" val = '*' if self.paren_level: i = len(self.code_list) - 1 if self.code_list[i].kind == 'blank': i -= 1 token = self.code_list[i] <IF_STMT> self.op_no_blanks(val) elif token.value == ',': self.blank() self.add_token('op-no-blanks', val) else: self.op(val) else: self.op(val)",if token.kind == 'lt':,if token.kind == 'no-blanks':,0.6230231926731272,0.8780099567239787,False
867,"def get_safe_settings(): """"""Returns a dictionary of the settings module, with sensitive settings blurred out."""""" settings_dict = {} for k in dir(settings): <IF_STMT> if HIDDEN_SETTINGS.search(k): settings_dict[k] = '********************' else: settings_dict[k] = getattr(settings, k) return settings_dict",if k.isupper():,if k.startswith('_'):,0.7647788376400547,0.8935248372106969,False
868,"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): <IF_STMT> cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if ref in self._editableChildren:,if ref in self._editableChildren:,0.9309140875038087,0.8856327184319047,True
869,"def _handle_dependents_type(types, type_str, type_name, rel_name, row): if types[type_str[0]] is None: <IF_STMT> type_name = 'index' rel_name = row['indname'] + ' ON ' + rel_name elif type_str[0] == 'o': type_name = 'operator' rel_name = row['relname'] else: type_name = types[type_str[0]] return (type_name, rel_name)",if type_str[0] == 'i':,if type_str[0] == 'i':,0.8967682593979349,0.8661072626070159,True
870,"def streamErrorHandler(self, conn, error): name, text = ('error', error.getData()) for tag in error.getChildren(): <IF_STMT> if tag.getName() == 'text': text = tag.getData() else: name = tag.getName() if name in stream_exceptions.keys(): exc = stream_exceptions[name] else: exc = StreamError raise exc((name, text))",if tag.getNamespace() == NS_XMPP_STREAMS:,if tag.getType() == 'stream':,0.7687885207577577,0.8627586293513119,False
871,"def _validate_names(self, settings: _SettingsType) -> None: """"""Make sure all settings exist."""""" unknown = [] for name in settings: <IF_STMT> unknown.append(name) if unknown: errors = [configexc.ConfigErrorDesc('While loading options', 'Unknown option {}'.format(e)) for e in sorted(unknown)] raise configexc.ConfigFileErrors('autoconfig.yml', errors)",if name not in configdata.DATA:,if name not in self.unknown_options:,0.7514843730250785,0.8338542560892604,False
872,"def can_haz(self, target, credentials): """"""Check whether key-values in target are present in credentials."""""" for requirement in target: key, match = requirement.split(':', 1) check = credentials.get(key) <IF_STMT> check = [check] if match in check: return True","if check is None or isinstance(check, basestring):","if not isinstance(check, list):",0.9065614473356993,0.8474968231198384,False
873,"def _recursive_fx_apply(input: dict, fx): for k, v in input.items(): <IF_STMT> v = torch.tensor(v) if isinstance(v, torch.Tensor): v = fx(v.float()) input[k] = v else: _recursive_fx_apply(v, fx)","if isinstance(v, list):","if isinstance(v, torch.Tensor):",0.8095950437048189,0.8196189957582152,False
874,"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: <IF_STMT> self._first_ping = False return EmptyCapabilitiesResponse() elif 'Hello0' in url and '1.2.1' in url and ('v1' in url): return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",if url.endswith('ping') and self._first_ping:,if self._first_ping:,0.6320053692055299,0.9144061946646023,False
875,"def server_thread_fn(): server_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) server_ctx.load_cert_chain('trio-test-1.pem') server = server_ctx.wrap_socket(server_sock, server_side=True, suppress_ragged_eofs=False) while True: data = server.recv(4096) print('server got:', data) <IF_STMT> print('server waiting for client to finish everything') client_done.wait() print('server attempting to send back close-notify') server.unwrap() print('server ok') break server.sendall(data)",if not data:,if not data:,0.8462997544270187,0.884617925078158,True
876,"def find_hostnames(data): hostnames = [] for i in re.finditer(hostname_regex, data): h = string.lower(i.group(1)) tld = h.split('.')[-1:][0] <IF_STMT> hostnames.append(h) return hostnames",if tld in tlds:,if tld == 'localhost':,0.8035844517773788,0.7378351342269067,False
877,"def Validate(self, win): textCtrl = self.GetWindow() text = textCtrl.GetValue().strip() sChar = Character.getInstance() try: <IF_STMT> raise ValueError(_t('You must supply a name for the Character!')) elif text in [x.name for x in sChar.getCharacterList()]: raise ValueError(_t('Character name already in use, please choose another.')) return True except ValueError as e: pyfalog.error(e) wx.MessageBox('{}'.format(e), _t('Error')) textCtrl.SetFocus() return False",if len(text) == 0:,if text == '':,0.8482137773982704,0.8964173245779284,False
878,def get_random_user_agent(agent_list=UA_CACHE): if not len(agent_list): ua_file = file(UA_FILE) for line in ua_file: line = line.strip() <IF_STMT> agent_list.append(line) ua = random.choice(UA_CACHE) return ua,if line:,if line:,0.731912306691108,0.8318180062062374,True
879,"def _validate_action_like_for_prefixes(self, key): for statement in self._statements: <IF_STMT> if isinstance(statement[key], string_types): self._validate_action_prefix(statement[key]) else: for action in statement[key]: self._validate_action_prefix(action)",if key in statement:,if key in statement:,0.5917583971155925,0.7098232254187811,True
880,"def predict(self, X): if self.regression: return self.predict_proba(X) else: y_pred = np.argmax(self.predict_proba(X), axis=1) <IF_STMT> y_pred = self.enc_.inverse_transform(y_pred) return y_pred",if self.use_label_encoder:,if self.enc_inverse:,0.8193789996153977,0.7912619863720214,False
881,"def _threaded_request_tracker(self, builder): while True: event_type = self._read_q.get() <IF_STMT> return payload = {'body': b''} request_id = builder.build_record(event_type, payload, '') self._write_q.put_nowait(request_id)",if event_type is False:,if event_type is None:,0.5660539268622061,0.7378351342269067,False
882,"def __call__(self, value): try: super(EmailValidator, self).__call__(value) except ValidationError as e: <IF_STMT> parts = value.split('@') try: parts[-1] = parts[-1].encode('idna').decode('ascii') except UnicodeError: raise e super(EmailValidator, self).__call__('@'.join(parts)) else: raise",if value and '@' in value:,if '@' in value:,0.8907284639338746,0.7965020533851944,False
883,"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 <IF_STMT> if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",elif not IfList:,if self.CurrentLineNumber == 1:,0.6084626918282235,0.8336104423443033,False
884,"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): if new_id != -1: palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: <IF_STMT> palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",if self.PALETTE is None:,if class_names:,0.7454173688395874,0.9202663016973823,False
885,"def Visit_star_expr(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR) _AppendTokenSubtype(child, format_token.Subtype.VARARGS_STAR)","if isinstance(child, pytree.Leaf) and child.value == '*':","if isinstance(child, ast.Expr):",0.5207179586187356,0.6475445426291286,False
886,"def create_if_compatible(cls, typ: Type, *, root: 'RootNode') -> Optional['Node']: if cls.compatible_types: target_type: Type = typ <IF_STMT> target_type = getattr(typ, '__origin__', None) or typ if cls._issubclass(target_type, cls.compatible_types): return cls(typ, root=root) return None",if cls.use_origin:,"if not isinstance(target_type, type):",0.9028534895452736,0.828399516355805,False
887,"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 if tokentype != 'id': continue while i + 1 < len(tokens) and tokens[i] == ('op', '.') and (tokens[i + 1][0] == 'id'): token += '.' + tokens[i + 1][1] i += 2 <IF_STMT> continue if token in pykeywords: continue if token[0] in '.0123456789': continue yield token",if token == '':,if token == 'op':,0.956708806273388,0.9192507668025022,False
888,"def create_config_filepath(cls, visibility=None): if cls.is_local(visibility): base_path = os.path.join('.') <IF_STMT> base_path = os.path.join(base_path, '.polyaxon') cls._create_dir(base_path) elif cls.CONFIG_PATH: pass else: base_path = polyaxon_user_path() cls._create_dir(base_path)",if cls.IS_POLYAXON_DIR:,if cls.CONFIG_PATH:,0.6805647239764564,0.8318180062062374,False
889,"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 <IF_STMT> bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 3:,elif size <= 5:,0.8210030663108111,0.8964173245779284,False
890,"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: <IF_STMT> records = as_dict(path + item, version, section) if records: result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",if item.endswith('/'):,if item.endswith('.json'):,0.9616454691790828,0.9443716053164669,False
891,"def api_read(self): result = {} files = ['my.cnf', 'debian.cnf'] directory_list = self.exec_payload('mysql_config_directory')['directory'] for _file in files: for directory in directory_list: mysql_conf = directory + _file content = self.shell.read(mysql_conf) <IF_STMT> result[mysql_conf] = content return result",if content:,if content:,0.832439094050311,0.8935248372106969,True
892,"def generate(self, count=100): self.pre_generate() counter = iter(range(count)) created = 0 while True: batch = list(islice(counter, self.batch_size)) <IF_STMT> break self.do_generate(batch, self.batch_size) from_size = created created += len(batch) print('Generate %s: %s-%s' % (self.resource, from_size, created)) self.after_generate()",if not batch:,if len(batch) == 0:,0.7621632339628721,0.8431339019329497,False
893,"def _normalize_fields(self, document, loader): for d in list(document.keys()): d2 = loader.expand_url(d, u'', scoped_id=False, vocab_term=True) <IF_STMT> document[d2] = document[d] del document[d]",if d != d2:,if d2 in document:,0.738270598963526,0.7378351342269067,False
894,"def load_cache(filename, get_key=mangle_key): cache = {} if not os.path.exists(filename): return cache f = open(filename, 'rb') l = 0 for line in f.readlines(): l += 1 fields = line.split(b' ') <IF_STMT> sys.stderr.write('Invalid file format in [%s], line %d\n' % (filename, l)) continue cache[get_key(fields[0][1:])] = fields[1].split(b'\n')[0] f.close() return cache",if fields == None or not len(fields) == 2 or fields[0][0:1] != b':':,if len(fields) != 2:,0.7301104386749626,0.8856327184319047,False
895,"def __lshift__(self, other): if not self.symbolic and type(other) is int: return RegisterOffset(self._bits, self.reg, self._to_signed(self.offset << other)) el<IF_STMT> return RegisterOffset(self._bits, self.reg, self.offset << other) else: return RegisterOffset(self._bits, self.reg, ArithmeticExpression(ArithmeticExpression.LShift, (self.offset, other)))",if self.symbolic:,if self.symbolic:,0.6608478793198422,0.8787142254774354,True
896,"def SaveSettings(self, force=False): if self.config is not None: frame.ShellFrameMixin.SaveSettings(self) <IF_STMT> frame.Frame.SaveSettings(self, self.config) self.shell.SaveSettings(self.config)",if self.autoSaveSettings or force:,if force:,0.6530930057724067,0.6997522298221912,False
897,"def _parse_gene(element): for genename_element in element: if 'type' in genename_element.attrib: ann_key = 'gene_%s_%s' % (genename_element.tag.replace(NS, ''), genename_element.attrib['type']) <IF_STMT> self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",if genename_element.attrib['type'] == 'primary':,if self.ParsedSeqRecord.annotations:,0.6481331612071893,0.8466657105524215,False
898,"def _write_pkg_file(self, file): with TemporaryFile(mode='w+') as tmpfd: _write_pkg_file_orig(self, tmpfd) tmpfd.seek(0) for line in tmpfd: <IF_STMT> file.write('Metadata-Version: 2.1\n') elif line.startswith('Description: '): file.write('Description-Content-Type: %s; charset=UTF-8\n' % long_description_content_type) file.write(line) else: file.write(line)",if line.startswith('Metadata-Version: '):,if line.startswith('Metadata Version: '):,0.8921978740821608,0.8105932471967202,False
899,"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: <IF_STMT> return self.value getcurrent().throw(*self._exception) else: if self.greenlet is not None: raise ConcurrentObjectUseError('This Waiter is already used by %r' % (self.greenlet,)) self.greenlet = getcurrent() try: return self.hub.switch() finally: self.greenlet = None",if self._exception is None:,if self._exception is not _NONE:,0.8522347765342789,0.879962308706789,False
900,"def connect(self, *args): """"""connects to the dropbox. args[0] is the username."""""" if len(args) != 1: return 'expected one argument!' try: dbci = get_dropbox_client(args[0], False, None, None) except Exception as e: return e.message else: <IF_STMT> return ""No Dropbox configured for '{u}'."".format(u=args[0]) else: self.client = dbci return True",if dbci is None:,if dbci is None:,0.6146076067426886,0.8832000938217648,True
901,"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: <IF_STMT> text = text.replace('\n', '<br>') return text",if '\n' in text:,if '\n' in text:,0.9527124580151902,0.9084940438173679,True
902,def t(ret): with IPDB() as ipdb: with ipdb.eventqueue() as evq: for msg in evq: <IF_STMT> ret.append(msg) return,if msg.get_attr('IFLA_IFNAME') == 'test1984':,if msg.type == 'message':,0.5345725546554401,0.7101158913184162,False
903,"def check_stmt(self, stmt): if is_future(stmt): for name, asname in stmt.names: <IF_STMT> self.found[name] = 1 else: raise SyntaxError('future feature %s is not defined' % name) stmt.valid_future = 1 return 1 return 0",if name in self.features:,if name in self.found:,0.8658681277574782,0.828399516355805,False
904,"def process_pypi_option(option, option_str, option_value, parser): if option_str.startswith('--no'): setattr(parser.values, option.dest, []) else: indexes = getattr(parser.values, option.dest, []) <IF_STMT> indexes.append(_PYPI) setattr(parser.values, option.dest, indexes)",if _PYPI not in indexes:,if option_str == '--no':,0.7758375669051228,0.7498810286408993,False
905,"def modify_address(self, name, address, domain): if not self.get_entries_by_name(name, domain): raise exception.NotFound infile = open(self.filename, 'r') outfile = tempfile.NamedTemporaryFile('w', delete=False) for line in infile: entry = self.parse_line(line) <IF_STMT> outfile.write('%s   %s   %s\n' % (address, self.qualify(name, domain), entry['type'])) else: outfile.write(line) infile.close() outfile.close() shutil.move(outfile.name, self.filename)","if entry and entry['name'].lower() == self.qualify(name, domain).lower():",if entry:,0.8841613887983591,0.9122561819614461,False
906,"def tms_to_quadkey(self, tms, google=False): quadKey = '' x, y, z = tms if not google: y = 2 ** z - 1 - y for i in range(z, 0, -1): digit = 0 mask = 1 << i - 1 if x & mask != 0: digit += 1 <IF_STMT> digit += 2 quadKey += str(digit) return quadKey",if y & mask != 0:,if y & mask != 0:,0.7887968205289896,0.8774402785224017,True
907,"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: if _typ == typ and key is _key: flag = 0 break <IF_STMT> self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",if flag:,if flag:,0.9174066795059833,0.9164531641034833,True
908,"def scan_error(self): """"""A string describing why the last scan failed, or None if it didn't."""""" self.acquire_lock() try: <IF_STMT> try: self._load_buf_data_once() except NotFoundInDatabase: pass return self._scan_error_cache finally: self.release_lock()",if self._scan_error_cache is None:,if self._scan_error_cache is not None:,0.6050403416317148,0.7765145040967655,False
909,"def _query(self): if self._mongo_query is None: self._mongo_query = self._query_obj.to_query(self._document) <IF_STMT> if '_cls' in self._mongo_query: self._mongo_query = {'$and': [self._cls_query, self._mongo_query]} else: self._mongo_query.update(self._cls_query) return self._mongo_query",if self._cls_query:,if self._cls_query:,0.6219314771763516,0.839587623092576,True
910,"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 <IF_STMT> n += 1 return n",if self.HasPinButton():,if self.HasShowButton():,0.9433250167472353,0.9202663016973823,False
911,"def testBind(self): try: with socket.socket(socket.PF_CAN, socket.SOCK_DGRAM, socket.CAN_J1939) as s: addr = (self.interface, socket.J1939_NO_NAME, socket.J1939_NO_PGN, socket.J1939_NO_ADDR) s.bind(addr) self.assertEqual(s.getsockname(), addr) except OSError as e: <IF_STMT> self.skipTest('network interface `%s` does not exist' % self.interface) else: raise",if e.errno == errno.ENODEV:,if e.errno == errno.EEXIST:,0.642191890240791,0.8385130047130208,False
912,"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes('\x00\x00\x01', self.current_size, self.current_size + 1024 * 1024 * 8) if pos is not None: padsize = pos - self.current_size <IF_STMT> yield PaddingBytes(self, 'pad[]', padsize // 8) chunk = Chunk(self, 'chunk[]') try: chunk['content/data'] except: pass yield chunk",if padsize:,if padsize > 0:,0.9454674600191918,0.8780099567239787,False
913,"def index_modulemd_files(repo_path): merger = Modulemd.ModuleIndexMerger() for fn in sorted(os.listdir(repo_path)): <IF_STMT> continue yaml_path = os.path.join(repo_path, fn) mmd = Modulemd.ModuleIndex() mmd.update_from_file(yaml_path, strict=True) merger.associate_index(mmd, 0) return merger.resolve()",if not fn.endswith('.yaml'):,if fn.startswith('.yaml'):,0.7441038571778853,0.8466657105524215,False
914,"def set_visible(self, visible=True): self._visible = visible if self._nswindow is not None: <IF_STMT> self.dispatch_event('on_resize', self._width, self._height) self.dispatch_event('on_show') self.dispatch_event('on_expose') self._nswindow.makeKeyAndOrderFront_(None) else: self._nswindow.orderOut_(None)",if visible:,if visible:,0.6823324111138658,0.8137489370974955,True
915,"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True if self.is_computed(): status = 'computed, ' if self.error() is None: <IF_STMT> status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.value() is self:,if self.value() is None:,0.9640661523689238,0.9019629427251674,False
916,"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == 'val': for key, value in segment.items(): if key == index[0]: return value <IF_STMT> if key.text == index[0]: return value raise Exception('Invalid state') elif index_type == 'index': return segment[index] elif index_type == 'textslice': return segment[index[0]:index[1]] elif index_type == 'key': return index[1] if strictdoc else index[0] else: raise Exception('Invalid state')","if hasattr(key, 'text'):",elif index_type == 'text':,0.6309949539024381,0.9053411402117831,False
917,"def _makeSafeAbsoluteURI(base, rel=None): if not ACCEPTABLE_URI_SCHEMES: return _urljoin(base, rel or u'') if not base: return rel or u'' if not rel: try: scheme = urlparse.urlparse(base)[0] except ValueError: return u'' <IF_STMT> return base return u'' uri = _urljoin(base, rel) if uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES: return u'' return uri",if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:,if scheme.scheme == 'file':,0.9242110603456049,0.8879659171421962,False
918,"def _write_packet(self, packet): try: for listener in self.early_outgoing_packet_listeners: listener.call_packet(packet) <IF_STMT> packet.write(self.socket, self.options.compression_threshold) else: packet.write(self.socket) for listener in self.outgoing_packet_listeners: listener.call_packet(packet) except IgnorePacket: pass",if self.options.compression_enabled:,if self.options.compression_threshold:,0.8830781602987949,0.8318180062062374,False
919,"def rangelist_to_set(rangelist): result = set() if not rangelist: return result for x in rangelist.split(','): <IF_STMT> result.add(int(x)) continue m = re.match('^(\\d+)-(\\d+)$', x) if m: start = int(m.group(1)) end = int(m.group(2)) result.update(set(range(start, end + 1))) continue msg = 'Cannot understand data input: %s %s' % (x, rangelist) raise ValueError(msg) return result","if re.match('^(\\d+)$', x):",if x.startswith('#'):,0.9332150687274616,0.926934323706186,False
920,"def test_device_property_logfile_isinstance(self): mock = MagicMock() with patch(builtin_string + '.open', mock): <IF_STMT> builtin_file = 'io.TextIOWrapper' else: builtin_file = builtin_string + '.file' with patch(builtin_file, MagicMock): handle = open('filename', 'r') self.dev.logfile = handle self.assertEqual(self.dev.logfile, handle)",if sys.version > '3':,if sys.platform == 'win32':,0.7980201459865853,0.8336104423443033,False
921,"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 if not start: start = stmt end = stmt <IF_STMT> pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",elif start:,if start:,0.9563837711305818,0.9425437476131634,False
922,"def reset_parameters(self): initialize = layers.get_initializer(self._hparams.initializer) if initialize is not None: for name, param in self.named_parameters(): <IF_STMT> initialize(param)",if name.split('.')[-1] == 'weight' and 'layer_norm' not in name:,if name == 'init':,0.5681939467445639,0.7404008324993688,False
923,"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import RequiredArgumentMissingError, MutuallyExclusiveArgumentError valid_combs = 'only --account-name, --name / --name / --name, --by-subscription is valid' if namespace.account_name is not None: if namespace.by_subscription is not None: raise MutuallyExclusiveArgumentError(valid_combs) <IF_STMT> raise RequiredArgumentMissingError('--name is also required') if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError('--name is also required')",if namespace.name is None:,if namespace.name is None:,0.9311897248677303,0.8944264839442453,True
924,"def DeleteDocuments(self, document_ids, response): """"""Deletes documents for the given document_ids."""""" for document_id in document_ids: <IF_STMT> document = self._documents[document_id] self._inverted_index.RemoveDocument(document) del self._documents[document_id] delete_status = response.add_status() delete_status.set_code(search_service_pb.SearchServiceError.OK)",if document_id in self._documents:,if document_id in self._documents:,0.8110385888527394,0.7886336751695258,True
925,"def generate_new_element(items, prefix, numeric=False): """"""Creates a random string with prefix, that is not in 'items' list."""""" while True: <IF_STMT> candidate = prefix + generate_random_numeric(8) else: candidate = prefix + generate_random_alphanumeric(8) if not candidate in items: return candidate LOG.debug('Random collision on %s' % candidate)",if numeric:,if numeric:,0.826400725172584,0.9164531641034833,True
926,"def generate_text_for_vocab(self, data_dir, tmp_dir): for i, sample in enumerate(self.generate_samples(data_dir, tmp_dir, problem.DatasetSplit.TRAIN)): if self.has_inputs: yield sample['inputs'] yield sample['targets'] <IF_STMT> break",if self.max_samples_for_vocab and i + 1 >= self.max_samples_for_vocab:,if i == len(sample['inputs']) - 1:,0.6104615570124681,0.6978429290017016,False
927,"def _get_ccp(config=None, config_path=None, saltenv='base'): """""" """""" if config_path: config = __salt__['cp.get_file_str'](config_path, saltenv=saltenv) <IF_STMT> raise SaltException('{} is not available'.format(config_path)) if isinstance(config, six.string_types): config = config.splitlines() ccp = ciscoconfparse.CiscoConfParse(config) return ccp",if config is False:,if not config:,0.6835924648112962,0.8446593249975184,False
928,"def rpush(key, *vals, **kwargs): ttl = kwargs.get('ttl') cap = kwargs.get('cap') if not ttl and (not cap): _client.rpush(key, *vals) else: pipe = _client.pipeline() pipe.rpush(key, *vals) <IF_STMT> pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",if cap:,if cap:,0.9368767704180561,0.8901732118131125,True
929,"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): <IF_STMT> if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s: mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",if mode == 'start':,if s.startswith('APNS-Type') and 'ENCRYPTED' in s:,0.836475181716728,0.9028586696953823,False
930,"def _add_communication_type(apps, schema_editor, communication_type): Worker = apps.get_model('orchestra', 'Worker') CommunicationPreference = apps.get_model('orchestra', 'CommunicationPreference') for worker in Worker.objects.all(): communication_preference, created = CommunicationPreference.objects.get_or_create(worker=worker, communication_type=communication_type) <IF_STMT> communication_preference.methods.slack = True communication_preference.methods.email = True communication_preference.save()",if created:,if created:,0.8710375293240366,0.8743414417652072,True
931,def get_postgresql_driver_name(): try: driver = os.getenv('CODECHECKER_DB_DRIVER') <IF_STMT> return driver try: import psycopg2 return 'psycopg2' except Exception: import pg8000 return 'pg8000' except Exception as ex: LOG.error(str(ex)) LOG.error('Failed to import psycopg2 or pg8000 module.') raise,if driver:,if driver:,0.5690696500516582,0.8901732118131125,True
932,"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, '_viewcode_modules', {}) for modname, entry in list(modules.items()): if entry is False: continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) <IF_STMT> modules.pop(modname)",if len(used) == 0:,if docname in modules:,0.7924388404315783,0.8878679585127796,False
933,"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: <IF_STMT> continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if not is_iterable(value):,if key == qkey:,0.9390444185686925,0.8832000938217648,False
934,"def _get_bucket_for_key(self, key: bytes) -> Optional[_DBValueTuple]: dbs: Iterable[PartitionDB] try: partition = self._key_index[key] dbs = [PartitionDB(partition, self._dbs[partition])] except KeyError: dbs = cast(Iterable[PartitionDB], self._dbs.items()) for partition, db in dbs: if db.key_may_exist(key)[0]: value = db.get(key) <IF_STMT> self._key_index[key] = partition return _DBValueTuple(db, value) return None",if value is not None:,if value is not None:,0.65097181125642,0.8492988135354755,True
935,def _clean(self): logger.info('Cleaning up...') if self._process is not None: if self._process.poll() is None: for _ in range(3): self._process.terminate() time.sleep(0.5) <IF_STMT> break else: self._process.kill() self._process.wait() logger.error('KILLED') if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info('Cleanup complete'),if self._process.poll() is not None:,if self._process.returncode == 0:,0.9318897258723919,0.8516228624291206,False
936,"def _calculate_runtimes(states): results = {'runtime': 0.0, 'num_failed_states': 0, 'num_passed_states': 0} for state, resultset in states.items(): if isinstance(resultset, dict) and 'duration' in resultset: <IF_STMT> results['num_passed_states'] += 1 else: results['num_failed_states'] += 1 results['runtime'] += resultset['duration'] log.debug('Parsed state metrics: {}'.format(results)) return results",if resultset['result']:,if resultset['duration'] == 0.0:,0.9001800117803963,0.8627586293513119,False
937,"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer if max != -1 and spaces > max: return LintProblem(token.start_mark.line + 1, next.start_mark.column, max_desc) <IF_STMT> return LintProblem(token.start_mark.line + 1, next.start_mark.column + 1, min_desc)",elif min != -1 and spaces < min:,if min != -1 and spaces < min:,0.922398985106936,0.8108704755314419,False
938,"def getfileinfo(name): finfo = FInfo() with io.open(name, 'rb') as fp: data = fp.read(512) <IF_STMT> finfo.Type = 'TEXT' fp.seek(0, 2) dsize = fp.tell() dir, file = os.path.split(name) file = file.replace(':', '-', 1) return (file, finfo, dsize, 0)",if 0 not in data:,if data == b'':,0.8415079043561371,0.8516228624291206,False
939,"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == 'layers': child = dict_to_XML('layer', val, name=key) <IF_STMT> child = dict_to_XML(key, val) else: if tag == 'config': child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem","elif isinstance(val, MutableMapping):",elif tag == 'data':,0.911894817016271,0.8964173245779284,False
940,"def _read_bytes(self, length): buffer = b'' while length: chunk = self.request.recv(length) <IF_STMT> log.debug('Connection closed') return False length -= len(chunk) buffer += chunk return buffer",if chunk == b'':,if not chunk:,0.7255367840770865,0.8120341702859789,False
941,"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) <IF_STMT> continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: if init_service and init_service in dep_cnt['_deps']: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if not dep_cnts:,if not dep_cnts:,0.9207811680186408,0.8815741981066073,True
942,"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: <IF_STMT> if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",if type(e) is Argument or (type(e) is Option and e.argcount):,if type(e) is Command or (type(e) is Option and e.argcount == 0):,0.919056964726783,0.8158535883569522,False
943,"def do_cli(manager, options): header = ['Name', 'Description'] table_data = [header] for filter_name, filter in get_filters(): <IF_STMT> continue filter_doc = inspect.getdoc(filter) or '' table_data.append([filter_name, filter_doc]) try: table = TerminalTable(options.table_type, table_data) except TerminalTableError as e: console('ERROR: %s' % str(e)) else: console(table.output)",if options.name and (not options.name in filter_name):,if filter_name == 'name':,0.894745558712242,0.8627586293513119,False
944,"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) if b1 != b2: return False <IF_STMT> return True",if not b1:,if b1 == b2:,0.9140968467583092,0.8418243449361874,False
945,"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() <IF_STMT> return True if not mother_handle: return True return False",if not father_handle:,if not father_handle:,0.9260525472081182,0.8815741981066073,True
946,"def caesar_cipher(s, k): result = '' for char in s: n = ord(char) if 64 < n < 91: n = (n - 65 + k) % 26 + 65 <IF_STMT> n = (n - 97 + k) % 26 + 97 result = result + chr(n) return result",if 96 < n < 123:,elif 97 < n < 96:,0.8199042437882665,0.8562773802729167,False
947,"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name if composite_file.description: rval = '{} ({})'.format(rval, composite_file.description) <IF_STMT> rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if composite_file.optional:,if composite_file.optional:,0.8355384593284713,0.926934323706186,True
948,"def __str__(self): t = '' if self._name != 'root': r = f'{t * (self._level - 1)}{self._name}:\n' else: r = '' level = self._level for i, (k, v) in enumerate(self._pointer.items()): <IF_STMT> r += f'{t * self._level}{v}\n' self._level += 1 else: r += f'{t * self._level}{k}: {v} ({type(v).__name__})\n' self._level = level return r[:-1]","if isinstance(v, Config):",if i == level:,0.9236052119874588,0.8944264839442453,False
949,"def __get_securitygroups(vm_): vm_securitygroups = config.get_cloud_config_value('securitygroups', vm_, __opts__, search_global=False) if not vm_securitygroups: return [] securitygroups = list_securitygroups() for i in range(len(vm_securitygroups)): vm_securitygroups[i] = six.text_type(vm_securitygroups[i]) <IF_STMT> raise SaltCloudNotFound(""The specified securitygroups '{0}' could not be found."".format(vm_securitygroups[i])) return vm_securitygroups",if vm_securitygroups[i] not in securitygroups:,if not securitygroups:,0.8808080510833688,0.8713933650206428,False
950,"def assert_walk_snapshot(self, field, filespecs_or_globs, paths, ignore_patterns=None, prepare=None): with self.mk_project_tree(ignore_patterns=ignore_patterns) as project_tree: scheduler = self.mk_scheduler(rules=create_fs_rules(), project_tree=project_tree) <IF_STMT> prepare(project_tree) result = self.execute(scheduler, Snapshot, self.specs(filespecs_or_globs))[0] self.assertEqual(sorted(getattr(result, field)), sorted(paths))",if prepare:,if prepare:,0.775160161147407,0.8531413606256201,True
951,"def _parse_rowids(self, rowids): xploded = [] rowids = [x.strip() for x in rowids.split(',')] for rowid in rowids: try: <IF_STMT> start = int(rowid.split('-')[0].strip()) end = int(rowid.split('-')[-1].strip()) xploded += range(start, end + 1) else: xploded.append(int(rowid)) except ValueError: continue return sorted(list(set(xploded)))",if '-' in rowid:,if '-' in rowid:,0.7535607332684358,0.8592377270804451,True
952,"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p <IF_STMT> lemma = self.composite_dict[w, pos] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas","if (w, pos) in self.composite_dict:",if w in self.composite_dict:,0.7837786791363719,0.9001816649635144,False
953,"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: <IF_STMT> self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True if remove and (not add): selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if box == self.level.bounds:,if remove:,0.9294275028881853,0.8935248372106969,False
954,"def _ensure_max_size(cls, image, max_size, interpolation): if max_size is not None: size = max(image.shape[0], image.shape[1]) <IF_STMT> resize_factor = max_size / size new_height = int(image.shape[0] * resize_factor) new_width = int(image.shape[1] * resize_factor) image = ia.imresize_single_image(image, (new_height, new_width), interpolation=interpolation) return image",if size > max_size:,if size > 0:,0.6351884480452946,0.8592377270804451,False
955,"def _1_0_cloud_ips(self, method, url, body, headers): if method == 'GET': return self.test_response(httplib.OK, self.fixtures.load('list_cloud_ips.json')) elif method == 'POST': <IF_STMT> body = json.loads(body) node = json.loads(self.fixtures.load('create_cloud_ip.json')) if 'reverse_dns' in body: node['reverse_dns'] = body['reverse_dns'] return self.test_response(httplib.ACCEPTED, json.dumps(node))",if body:,if body:,0.921094169554852,0.8935248372106969,True
956,"def get_formatted_stats(self): """"""Get percentage or number of rar's done"""""" if self.cur_setname and self.cur_setname in self.total_volumes: <IF_STMT> return '%02d/%02d' % (self.cur_volume, self.total_volumes[self.cur_setname]) return self.cur_volume",if self.total_volumes[self.cur_setname] >= self.cur_volume and self.cur_volume:,if self.total_volumes[self.cur_setname] > 0:,0.8661494343012077,0.7709002428237395,False
957,"def wdayset(self, year, month, day): dset = [None] * (self.yearlen + 7) i = datetime.date(year, month, day).toordinal() - self.yearordinal start = i for j in range(7): dset[i] = i i += 1 <IF_STMT> break return (dset, start, i)",if self.wdaymask[i] == self.rrule._wkst:,if i >= self.yearlen:,0.8289662162820892,0.8592377270804451,False
958,"def do_acquire_read_lock(self, wait=True): self.condition.acquire() try: <IF_STMT> while self.current_sync_operation is not None: self.condition.wait() elif self.current_sync_operation is not None: return False self.asynch += 1 finally: self.condition.release() if not wait: return True",if wait:,if wait:,0.9083762743143184,0.8743414417652072,True
959,"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): <IF_STMT> return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y","if not isinstance(y, (dict, OrderedDict)):","if not isinstance(y, dict):",0.8876182862779942,0.9053411402117831,False
960,"def update_forum_nums_topic_post(modeladmin, request, queryset): for forum in queryset: forum.num_topics = forum.count_nums_topic() forum.num_posts = forum.count_nums_post() <IF_STMT> forum.last_post = forum.topic_set.order_by('-last_reply_on')[0].last_post else: forum.last_post = '' forum.save()",if forum.num_topics:,if forum.last_post is None:,0.7121251346224252,0.7709002428237395,False
961,"def get_docname_for_node(self, node: Node) -> str: while node: if isinstance(node, nodes.document): return self.env.path2doc(node['source']) <IF_STMT> return node['docname'] else: node = node.parent return None","elif isinstance(node, addnodes.start_of_file):","elif isinstance(node, nodes.name):",0.8787830485950429,0.7947545184555568,False
962,"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) <IF_STMT> selected_machines.append(machine) return selected_machines",if self.locations and machine.location in self.locations:,"if self.tags_match(machine.tags, self.tags):",0.7983527361456915,0.833078701050083,False
963,"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: if value is True: return ['-%s' % name] <IF_STMT> if split_single_char_options: return ['-%s' % name, '%s' % value] else: return ['-%s%s' % (name, value)] elif value is True: return ['--%s' % dashify(name)] elif value is not False and value is not None: return ['--%s=%s' % (dashify(name), value)] return []","elif value not in (False, None):",elif value is False:,0.648324048520609,0.9069443196104878,False
964,"def indent(elem, level=0): i = '\n' + level * '  ' if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + '  ' if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i el<IF_STMT> elem.tail = i",if level and (not elem.tail or not elem.tail.strip()):,if level and (not elem.tail or not elem.tail.strip()):,0.9293081783050442,0.8423079304558274,True
965,"def _run_instances_op(self, op, instance_ids, **kwargs): while instance_ids: try: return self.manager.retry(op, InstanceIds=instance_ids, **kwargs) except ClientError as e: <IF_STMT> instance_ids.remove(extract_instance_id(e)) raise",if e.response['Error']['Code'] == 'IncorrectInstanceState':,if e.response['Error']['Code'] == 'NoSuchEntity':,0.8357505687223676,0.7391959451349216,False
966,"def runTest(self): self.poco(text='wait UI').click() bomb_count = 0 while True: blue_fish = self.poco('fish_emitter').child('blue') yellow_fish = self.poco('fish_emitter').child('yellow') bomb = self.poco('fish_emitter').child('bomb') fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) <IF_STMT> bomb_count += 1 if bomb_count > 3: return else: fish.click() time.sleep(2.5)",if fish is bomb:,if fish:,0.8325420699634807,0.8966773400768917,False
967,"def lineWidth(self, lw=None): """"""Set/get width of mesh edges. Same as `lw()`."""""" if lw is not None: <IF_STMT> self.GetProperty().EdgeVisibilityOff() self.GetProperty().SetRepresentationToSurface() return self self.GetProperty().EdgeVisibilityOn() self.GetProperty().SetLineWidth(lw) else: return self.GetProperty().GetLineWidth() return self",if lw == 0:,if self.GetProperty().GetLineWidth() == lw:,0.7693828619647983,0.8105932471967202,False
968,"def _current_date_updater(doc, field_name, value): if isinstance(doc, dict): <IF_STMT> doc[field_name] = helpers.get_current_timestamp() else: doc[field_name] = mongomock.utcnow()",if value == {'$type': 'timestamp'}:,if value:,0.7487706312490583,0.7447819789879647,False
969,"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) <IF_STMT> return if len(data) < 1000: self.limit = 0 self.after = Object(id=int(data[-1]['user']['id'])) for element in reversed(data): await self.members.put(self.create_member(element))",if not data:,if not data:,0.618887149901563,0.8713933650206428,True
970,"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract(page, start_index, end_index, self.template.ignored_regions) for item in arg_to_iter(extracted): if item: <IF_STMT> item[u'_template'] = self.template.id items.append(item) return items","if isinstance(item, (ItemProcessor, dict)):",if 'u' not in item:,0.8649878571902743,0.8036431532733102,False
971,"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] if node_type not in self.available_node_types: raise ValueError(f'Unknown node type tag: {node_type}.') node_specific_config = self.available_node_types[node_type] <IF_STMT> fields = node_specific_config[fields_key] return fields",if fields_key in node_specific_config:,if fields_key in node_specific_config:,0.9247774143721468,0.8692960007731574,True
972,"def _write_all(self, writer): """"""Writes messages and insert comments here and there."""""" for msg, comment in zip_longest(self.original_messages, self.original_comments, fillvalue=None): <IF_STMT> print('writing comment: ', comment) writer.log_event(comment) if msg is not None: print('writing message: ', msg) writer(msg)",if comment is not None:,if comment is not None:,0.9027734098605811,0.8200123297196334,True
973,"def run_tests(): x = 5 with switch(x) as case: <IF_STMT> print('zero') print('zero') elif case(1, 2): print('one or two') elif case(3, 4): print('three or four') else: print('default') print('another')",if case(0):,"if case(0, 1) == 0:",0.8748551564465297,0.7765145040967655,False
974,"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: if isinstance(value, datetime.date): ret = value.strftime('%d/%m/%y') <IF_STMT> ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.time): ret = value.strftime('%H:%M:%S') else: ret = value return ret","elif isinstance(value, datetime.datetime):","elif isinstance(value, datetime.datetime):",0.6491942495547209,0.8675979125638379,True
975,def database_app(request): if request.param == 'postgres_app': if not which('initdb'): pytest.skip('initdb must be on PATH for postgresql fixture') <IF_STMT> pytest.skip('psycopg2 must be installed for postgresql fixture') if request.param == 'sqlite_rabbitmq_app': if not os.environ.get('GALAXY_TEST_AMQP_INTERNAL_CONNECTION'): pytest.skip('rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset') return request.getfixturevalue(request.param),if not psycopg2:,if not which('psycopg2'):,0.9266781838151104,0.9000283069718913,False
976,"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) <IF_STMT> deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) if timeout < 0: n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",if timeout >= 0:,if timeout >= 0:,0.8969835570471322,0.9001816649635144,True
977,"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: if name is None: all_plugins += [plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated] else: plugin = self.plugins[name] <IF_STMT> all_plugins.append(plugin) return all_plugins",if plugin.is_activated:,if plugin.is_activated:,0.780687297037495,0.9312457603037672,True
978,"def get_expected_sql(self): sql_base_path = path.join(path.dirname(path.realpath(__file__)), 'sql') for version_mapping in get_version_mapping_directories(self.server['type']): <IF_STMT> continue complete_path = path.join(sql_base_path, version_mapping['name']) if not path.exists(complete_path): continue break data_sql = '' with open(path.join(complete_path, 'test_sql_output.sql')) as fp: data_sql = fp.read() return data_sql",if version_mapping['number'] > self.server_information['server_version']:,if version_mapping['name'] not in self.server['version_mapping']:,0.858599138946528,0.8200123297196334,False
979,"def _validate_headers(self, headers): if headers is None: return headers res = {} for key, value in headers.items(): if isinstance(value, (int, float)): value = str(value) <IF_STMT> raise ScriptError({'message': 'headers must be a table with strings as keys and values.Header: `{!r}:{!r}` is not valid'.format(key, value)}) res[key] = value return res","if not isinstance(key, (bytes, str)) or not isinstance(value, (bytes, str)):","if not isinstance(value, (list, tuple)):",0.7187859329514147,0.8703737209656045,False
980,"def _get_literal_value(self, pyval): if pyval == self.vm.lookup_builtin('builtins.True'): return True elif pyval == self.vm.lookup_builtin('builtins.False'): return False elif isinstance(pyval, str): prefix, value = parser_constants.STRING_RE.match(pyval).groups()[:2] value = value[1:-1] <IF_STMT> value = compat.bytestring(value) elif 'u' in prefix and self.vm.PY2: value = compat.UnicodeType(value) return value else: return pyval",if 'b' in prefix and (not self.vm.PY2):,if 's' in prefix and self.vm.PY1:,0.7516800583789994,0.8375707157974782,False
981,"def decode_query_ids(self, trans, conditional): if conditional.operator == 'and': self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split('.')[0] <IF_STMT> field = self.FIELDS[left_base] if field.id_decode: conditional.right = trans.security.decode_id(conditional.right)",if left_base in self.FIELDS:,if left_base in self.FIELDS:,0.7223707724448244,0.7886336751695258,True
982,"def testLastPhrases(self): for day in (11, 12, 13, 14, 15, 16, 17): start = datetime.datetime(2012, 11, day, 9, 0, 0) yr, mth, dy, _, _, _, wd, yd, isdst = start.timetuple() n = 4 - wd <IF_STMT> n -= 7 target = start + datetime.timedelta(days=n) self.assertExpectedResult(self.cal.parse('last friday', start.timetuple()), (target.timetuple(), 1), dateOnly=True)",if n >= 0:,if isdst:,0.6639935435786818,0.9298663600557577,False
983,"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: <IF_STMT> nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",if nbChars[0] is not None:,if nbChars[0] is not None:,0.692864443368162,0.8621109017306224,True
984,"def getpystone(): maxpystone = 0 for pyseed in [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]: duration, pystonefloat = pystones(pyseed) maxpystone = max(maxpystone, int(pystonefloat)) <IF_STMT> break return maxpystone",if duration > 0.1:,if duration > maxpystone:,0.8247944683771624,0.8105932471967202,False
985,"def _append_to_io_queue(self, data, stream_name): parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: <IF_STMT> for block in re.split('(.{%d,})' % (self._get_squeeze_threshold() + 1), part): if block: self._queued_io_events.append((block, stream_name))",if part:,if part:,0.8906135491633543,0.8590888738245122,True
986,"def qtTypeIdent(conn, *args): res = None value = None for val in args: <IF_STMT> val = str(val) if len(val) == 0: continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res","if not hasattr(val, '__len__'):","if isinstance(val, str):",0.9408201466225847,0.9134996171406936,False
987,"def SetVerbose(self, level): """"""Sets the verbose level."""""" try: <IF_STMT> level = int(level) if level >= 0 and level <= 3: self._verbose = level return except ValueError: pass self.Error('Verbose level (%s) must be between 0 and 3 inclusive.' % level)",if type(level) != types.IntType:,"if isinstance(level, int):",0.9047519785692506,0.884617925078158,False
988,"def step(self) -> None: """"""Performs a single optimization step."""""" for group in self.param_groups: for p in group['params']: <IF_STMT> continue p.add_(p.grad, alpha=-group['lr'] * self.num_data) return None",if p.grad is None:,if p.grad is None:,0.8636862927291948,0.7886336751695258,True
989,"def fill(self, values): if lupa.lua_type(values) != 'table': raise ScriptError({'argument': 'values', 'message': 'element:fill values is not a table', 'splash_method': 'fill'}) for key, value in values.items(): <IF_STMT> _mark_table_as_array(self.lua, value) values = self.lua.lua2python(values) return self.element.fill(values)",if lupa.lua_type(value) == 'table':,"if isinstance(value, (list, tuple)):",0.8091054680886041,0.8336104423443033,False
990,"def _gen_repr(self, buf): (print >> buf, 'def __repr__(self):') if self.argnames: fmt = COMMA.join(['%s'] * self.nargs) <IF_STMT> fmt = '(%s)' % fmt vals = ['repr(self.%s)' % name for name in self.argnames] vals = COMMA.join(vals) if self.nargs == 1: vals = vals + ',' (print >> buf, 'return ""%s(%s)"" %% (%s)' % (self.name, fmt, vals)) else: (print >> buf, 'return ""%s()""' % self.name)",if '(' in self.args:,if self.nargs == 1:,0.859032327264927,0.9114434865990403,False
991,"def render_observation(self): x = self.read_head_position label = 'Observation Grid: ' x_str = '' for j in range(-1, self.rows + 1): <IF_STMT> x_str += ' ' * len(label) for i in range(-2, self.input_width + 2): if i == x[0] and j == x[1]: x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += '\n' x_str = label + x_str return x_str",if j != -1:,if x_str:,0.6863755975223677,0.9434724611166208,False
992,"def get_module_comment(self, attrname: str) -> Optional[List[str]]: try: analyzer = ModuleAnalyzer.for_module(self.modname) analyzer.analyze() key = ('', attrname) <IF_STMT> return list(analyzer.attr_docs[key]) except PycodeError: pass return None",if key in analyzer.attr_docs:,if key in analyzer.attr_docs:,0.6402825479258158,0.7709002428237395,True
993,"def tms_to_quadkey(self, tms, google=False): quadKey = '' x, y, z = tms if not google: y = 2 ** z - 1 - y for i in range(z, 0, -1): digit = 0 mask = 1 << i - 1 <IF_STMT> digit += 1 if y & mask != 0: digit += 2 quadKey += str(digit) return quadKey",if x & mask != 0:,if x & mask != 0:,0.7946178989603621,0.8774402785224017,True
994,"def test_enumerate(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i * 4)) async for i, value in stream.enumerate(): current_event = stream.current_event assert i == current_event.key assert value == i * 4 <IF_STMT> break assert await channel_empty(stream.channel)",if i >= 99:,if current_event.key == i * 4:,0.9212623497927147,0.8262592696121884,False
995,"def print_messages(self): output_reports = self.config.get_output_report() for report in output_reports: output_format, output_files = report self.summary['formatter'] = output_format formatter = FORMATTERS[output_format](self.summary, self.messages, self.config.profile) <IF_STMT> self.write_to(formatter, sys.stdout) for output_file in output_files: with open(output_file, 'w+') as target: self.write_to(formatter, target)",if not output_files:,if self.config.verbose:,0.860667837635336,0.8966773400768917,False
996,"def eval_metrics(self): for task in self.task_list: <IF_STMT> return [metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY, metrics.Metrics.ROUGE_2_F, metrics.Metrics.ROUGE_L_F] return [metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY]",if 'summarize' in task.name:,if task.is_active:,0.575786682597721,0.7447819789879647,False
997,"def _getBuildRequestForBrdict(self, brdict): breq = self.breqCache.get(brdict['buildrequestid']) if not breq: breq = (yield BuildRequest.fromBrdict(self.master, brdict)) <IF_STMT> self.breqCache[brdict['buildrequestid']] = breq defer.returnValue(breq)",if breq:,if breq:,0.6297341613475745,0.803154665668484,True
998,"def _stash_splitter(states): keep, split = ([], []) if state_func is not None: for s in states: ns = state_func(s) <IF_STMT> split.append(ns) elif isinstance(ns, (list, tuple, set)): split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return (keep, split)","if isinstance(ns, SimState):","if isinstance(ns, (list, tuple, set)):",0.6178868221507579,0.8703737209656045,False
999,"def sequence_to_text(sequence): """"""Converts a sequence of IDs back to a string"""""" result = '' for symbol_id in sequence: <IF_STMT> s = _id_to_symbol[symbol_id] if len(s) > 1 and s[0] == '@': s = '{%s}' % s[1:] result += s return result.replace('}{', ' ')",if symbol_id in _id_to_symbol:,if symbol_id in _id_to_symbol:,0.8876037569803146,0.8723360571509826,True
1000,"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] <IF_STMT> source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",if mod_type == imp.PY_SOURCE:,if mod_type == imp.PKG_DIRECTORY:,0.7285872780483162,0.8723360571509826,False
1001,"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info('Checking for %s' % wafvendor) if self.wafdetections[wafvendor](self): detected.append(wafvendor) <IF_STMT> break self.knowledge['wafname'] = detected return detected",if not findall:,if findall:,0.8811950987568554,0.8901732118131125,False
1002,"def SessionId(self): """"""Returns the Session ID of the process"""""" if self.Session.is_valid(): process_space = self.get_process_address_space() <IF_STMT> return obj.Object('_MM_SESSION_SPACE', offset=self.Session, vm=process_space).SessionId return obj.NoneObject('Cannot find process session')",if process_space:,if process_space:,0.7057769826339815,0.8466657105524215,True
1003,"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] if c == '$' and s[i + 1] in '0123456789': s[i] = '\\' <IF_STMT> s[i] = '' i += 1 i += 1 return pattern[:0].join(s)",elif c == '\\' and s[i + 1] == '$':,elif c == '\\':,0.7922858925373283,0.9001816649635144,False
1004,"def __init__(self, coverage): self.coverage = coverage self.config = self.coverage.config self.source_paths = set() if self.config.source: for src in self.config.source: <IF_STMT> if not self.config.relative_files: src = files.canonical_filename(src) self.source_paths.add(src) self.packages = {} self.xml_out = None",if os.path.exists(src):,if src.endswith('.py'):,0.9287077812232533,0.8866029039778043,False
1005,"def populate_vol_format(self): rhel6_file_whitelist = ['raw', 'qcow2', 'qed'] model = self.widget('vol-format').get_model() model.clear() formats = self.vol_class.formats if hasattr(self.vol_class, 'create_formats'): formats = getattr(self.vol_class, 'create_formats') if self.vol_class == Storage.FileVolume and (not self.conn.rhel6_defaults_caps()): newfmts = [] for f in rhel6_file_whitelist: <IF_STMT> newfmts.append(f) formats = newfmts for f in formats: model.append([f, f])",if f in formats:,if f not in formats:,0.8812096502574144,0.8649799950178215,False
1006,"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None <IF_STMT> file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if os.path.exists('file_sources.json'):,if os.path.exists('file_sources.json'):,0.8150380818360817,0.9202663016973823,True
1007,"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] <IF_STMT> result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y",if len(x) > len(y):,if len(x) < len(y):,0.9243966818496393,0.9069443196104878,False
1008,def copy_dicts(dct): if '_remote_data' in dct: dsindex = dct['_remote_data']['_content'].dsindex newdct = dct.copy() newdct['_remote_data'] = {'_content': dsindex} return list(newdct.items()) elif '_data' in dct: newdct = dct.copy() newdata = copy_dicts(dct['_data']) <IF_STMT> newdct['_data'] = newdata return list(newdct.items()) return None,if newdata:,if newdata:,0.869378699051425,0.8996480074924822,True
1009,"def _import_epic_activity(self, project_data, taiga_epic, epic, options): offset = 0 while True: activities = self._client.get('/projects/{}/epics/{}/activity'.format(project_data['id'], epic['id']), {'envelope': 'true', 'limit': 300, 'offset': offset}) offset += 300 for activity in activities['data']: self._import_activity(taiga_epic, activity, options) <IF_STMT> break",if len(activities['data']) < 300:,if not activities['data']:,0.7830219251001921,0.8874708471648363,False
1010,"def __get__(self, instance, instance_type=None): if instance: <IF_STMT> rel_obj = self.get_obj(instance) if rel_obj: instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",if self.att_name not in instance._obj_cache:,if self.att_name not in instance._obj_cache:,0.8239937679312276,0.6907573115737006,True
1011,"def download_main(download, download_playlist, urls, playlist, output_dir, merge, info_only): for url in urls: if url.startswith('https://'): url = url[8:] if not url.startswith('http://'): url = 'http://' + url <IF_STMT> download_playlist(url, output_dir=output_dir, merge=merge, info_only=info_only) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if playlist:,if playlist:,0.662246846307953,0.8966773400768917,True
1012,"def _mksubs(self): self._subs = {} commit_dir = CommitDir(self, '.commit') self._subs['.commit'] = commit_dir tag_dir = TagDir(self, '.tag') self._subs['.tag'] = tag_dir for name, sha in git.list_refs(): <IF_STMT> name = name[11:] date = git.rev_get_date(sha.encode('hex')) n1 = BranchList(self, name, sha) n1.ctime = n1.mtime = date self._subs[name] = n1",if name.startswith('refs/heads/'):,if name.startswith('git-'):,0.8460693163143422,0.9184043388013005,False
1013,"def readAtOffset(self, offset, size, shortok=False): ret = b'' self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) <IF_STMT> if not shortok: return None return ret ret += x return ret",if x == b'':,if x == b'':,0.7921835789349905,0.8431339019329497,True
1014,"def remove_indent(self): """"""Remove one tab-width of blanks from the previous token."""""" w = abs(self.tab_width) if self.result: s = self.result[-1] <IF_STMT> self.result.pop() s = s.replace('\t', ' ' * w) if s.startswith('\n'): s2 = s[1:] self.result.append('\n' + s2[:-w]) else: self.result.append(s[:-w])",if s.isspace():,if s.startswith('\t'):,0.723285165920307,0.9051034981560222,False
1015,"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: if kwargs.get('in_place', False): self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: <IF_STMT> self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()",if '_create_temporary' in traceback.format_exc():,"if kwargs.get('in_place', False):",0.8936395461252257,0.833078701050083,False
1016,def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree['activities'].values(): <IF_STMT> _collect_manual_intervention_nodes(act['pipeline']) elif act['component']['code'] in MANUAL_INTERVENTION_COMP_CODES: manual_intervention_nodes.add(act['id']),if act['type'] == 'SubProcess':,if act['component']['code'] in MANUAL_INTERVENTION_PIPELINE_CODES:,0.5368490060306864,0.6026080978557137,False
1017,"def banned(): if request.endpoint == 'views.themes': return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() <IF_STMT> return (render_template('errors/403.html', error='You have been banned from this CTF'), 403) if team and team.banned: return (render_template('errors/403.html', error='Your team has been banned from this CTF'), 403)",if user and user.banned:,if user and user.banned:,0.9128223117421506,0.8692960007731574,True
1018,"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == 'ANY': if v == 'ANY': self._definition = [] <IF_STMT> self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",elif v in self._definition:,elif v in self._definition:,0.946228626885854,0.8983343737277126,True
1019,"def save(self, learner, file_name): """"""Save the model to location specified in file_name."""""" with open(file_name, 'wb') as f: <IF_STMT> learner.inference_cache_, tmp = (None, learner.inference_cache_) pickle.dump(learner, f, -1) learner.inference_cache_ = tmp else: pickle.dump(learner, f, -1)","if hasattr(learner, 'inference_cache_'):",if learner.inference_cache_:,0.8707004392679268,0.8901732118131125,False
1020,"def __init__(self, exprs, savelist=False): super(ParseExpression, self).__init__(savelist) if isinstance(exprs, _generatorType): exprs = list(exprs) if isinstance(exprs, basestring): self.exprs = [ParserElement._literalStringClass(exprs)] elif isinstance(exprs, collections.Iterable): exprs = list(exprs) <IF_STMT> exprs = map(ParserElement._literalStringClass, exprs) self.exprs = list(exprs) else: try: self.exprs = list(exprs) except TypeError: self.exprs = [exprs] self.callPreparse = False","if all((isinstance(expr, basestring) for expr in exprs)):","elif isinstance(exprs, basestring):",0.7113625826659561,0.9000283069718913,False
1021,"def find(self, back=False): flags = 0 <IF_STMT> flags = QTextDocument.FindBackward if self.csBox.isChecked(): flags = flags | QTextDocument.FindCaseSensitively text = self.searchEdit.text() if not self.findMain(text, flags): if text in self.editBoxes[self.ind].toPlainText(): cursor = self.editBoxes[self.ind].textCursor() if back: cursor.movePosition(QTextCursor.End) else: cursor.movePosition(QTextCursor.Start) self.editBoxes[self.ind].setTextCursor(cursor) self.findMain(text, flags)",if back:,if back:,0.8202989815057469,0.9076141716697395,True
1022,"def _load_storage(self): self._storage = {} for row in self('SELECT object, resource, amount FROM storage'): ownerid = int(row[0]) <IF_STMT> self._storage[ownerid].append(row[1:]) else: self._storage[ownerid] = [row[1:]]",if ownerid in self._storage:,if ownerid in self._storage:,0.6432113729455058,0.7709002428237395,True
1023,"def parse_chunked(self, unreader): size, rest = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() <IF_STMT> raise NoMoreData() yield rest[:size] rest = rest[size:] while len(rest) < 2: rest += unreader.read() if rest[:2] != b'\r\n': raise ChunkMissingTerminator(rest[:2]) size, rest = self.parse_chunk_size(unreader, data=rest[2:])",if not rest:,if len(rest) == 0:,0.8842360543443009,0.8902056737869248,False
1024,"def _augment_batch_(self, batch, random_state, parents, hooks): for column in batch.columns: <IF_STMT> for i, cbaoi in enumerate(column.value): column.value[i] = cbaoi.clip_out_of_image_() return batch","if column.name in ['keypoints', 'bounding_boxes', 'polygons', 'line_strings']:","if isinstance(column, ImageColumn):",0.8450766707300051,0.7848518349390632,False
1025,"def to_nim(self): if self.is_pointer == 2: s = 'cstringArray' if self.type == 'GLchar' else 'ptr pointer' else: s = self.type <IF_STMT> default = 'ptr ' + s s = self.NIM_POINTER_MAP.get(s, default) return s",if self.is_pointer == 1:,if self.is_pointer == 1:,0.7443450027721179,0.8385130047130208,True
1026,"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) <IF_STMT> self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",if os.path.isfile(file) or os.path.islink(file):,if os.path.isfile(file) or os.path.islink(file):,0.9047136777558212,0.8516228624291206,True
1027,"def remove(self, event): try: self._events_current_sweep.remove(event) <IF_STMT> assert event.in_sweep == True assert event.other.in_sweep == True event.in_sweep = False event.other.in_sweep = False return True except KeyError: if USE_DEBUG: assert event.in_sweep == False assert event.other.in_sweep == False return False",if USE_DEBUG:,if USE_DEBUG:,0.6921261516767043,0.8996480074924822,True
1028,"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue <IF_STMT> attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrname == 'salt_version':,if attrname == 'version':,0.6542690760484787,0.8592377270804451,False
1029,"def _init_auxiliary_head(self, auxiliary_head): """"""Initialize ``auxiliary_head``"""""" if auxiliary_head is not None: <IF_STMT> self.auxiliary_head = nn.ModuleList() for head_cfg in auxiliary_head: self.auxiliary_head.append(builder.build_head(head_cfg)) else: self.auxiliary_head = builder.build_head(auxiliary_head)","if isinstance(auxiliary_head, list):","if isinstance(auxiliary_head, nn.ModuleList):",0.6537118589195948,0.80377750806414,False
1030,"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) <IF_STMT> parts.append(param.type) out += [' : '.join(parts)] if param.desc and ''.join(param.desc).strip(): out += self._str_indent(param.desc) out += [''] return out",if param.type:,elif param.type:,0.9276821722174765,0.9099951253570094,False
1031,"def _set_handler(self, name, handle=None, obj=None, constructor_args=(), constructor_kwds={}): if handle is None: handle = obj is not None if handle: handler_class = self.handler_classes[name] <IF_STMT> newhandler = handler_class(obj) else: newhandler = handler_class(*constructor_args, **constructor_kwds) else: newhandler = None self._replace_handler(name, newhandler)",if obj is not None:,"if isinstance(obj, type):",0.7356426860803572,0.8783650674919876,False
1032,"def _extract_subtitles(src): subtitles = {} for caption in try_get(src, lambda x: x['captions'], list) or []: subtitle_url = url_or_none(caption.get('uri')) <IF_STMT> lang = caption.get('language', 'deu') subtitles.setdefault(lang, []).append({'url': subtitle_url}) return subtitles",if subtitle_url:,if subtitle_url:,0.8638683607448847,0.8696398662122882,True
1033,"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): <IF_STMT> keys = [x.split('(')[0] for x in struct.keys()] res.extend(keys) for key in struct: if key in IGNORED_KEYS: logging.debug('Ignored: %s: %s', key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",if not ignore_first_level:,if ignore_first_level:,0.9512964161434951,0.9220450449751959,False
1034,"def create_dir(path): curr_path = None for p in path: if curr_path is None: curr_path = os.path.abspath(p) else: curr_path = os.path.join(curr_path, p) <IF_STMT> os.mkdir(curr_path)",if not os.path.exists(curr_path):,if not os.path.exists(curr_path):,0.7866444682975172,0.8390623881636138,True
1035,"def dataToDumpFile(dumpFile, data): try: dumpFile.write(data) dumpFile.flush() except IOError as ex: if 'No space left' in getUnicode(ex): errMsg = 'no space left on output device' logger.error(errMsg) <IF_STMT> errMsg = 'permission denied when flushing dump data' logger.error(errMsg) else: errMsg = ""error occurred when writing dump data to file ('%s')"" % getUnicode(ex) logger.error(errMsg)",elif 'Permission denied' in getUnicode(ex):,elif 'Permission denied' in getUnicode(ex):,0.7468586683255947,0.8753524256584351,True
1036,"def elements(self, top): res = [] for part in self.parts: <IF_STMT> res.append(name_or_ref(part, top)) else: if isinstance(part, Extension): res.append(part.base) res.extend(part.elements(top)) return res","if isinstance(part, Element):","if isinstance(part, Name):",0.8639213395314034,0.7848518349390632,False
1037,"def _parse_param_value(name, datatype, default): if datatype == 'bool': if default.lower() == 'true': return True <IF_STMT> return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': if type(default) == float: return default else: return float(default) else: return str(default)",elif default.lower() == 'false':,elif default.lower() == 'false':,0.8074069715865545,0.914208565914368,True
1038,"def dvmethod(c, dx, doAST=False): for m in c.get_methods(): mx = dx.get_method(m) ms = DvMethod(mx) ms.process(doAST=doAST) <IF_STMT> assert ms.get_ast() is not None assert isinstance(ms.get_ast(), dict) assert 'body' in ms.get_ast() else: assert ms.get_source() is not None",if doAST:,if doAST:,0.6607400541904836,0.8935248372106969,True
1039,"def _repr_pretty_(self, p, cycle): if cycle: return '{{...}' with p.group(2, '{', '}'): p.breakable('') for idx, key in enumerate(self._items): <IF_STMT> p.text(',') p.breakable() value = self._items[key] p.pretty(key) p.text(': ') if isinstance(value, bytes): value = trimmed_repr(value) p.pretty(value) p.breakable('')",if idx:,if idx == 0:,0.7649511438889819,0.8474968231198384,False
1040,"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean('browsers', 'rating_confirm_multiple'): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _('_Remove Rating'), count, None) <IF_STMT> return reset = [] for song in songs: if '~#rating' in song: del song['~#rating'] reset.append(song) librarian.changed(reset)",if dialog.run() != Gtk.ResponseType.YES:,if dialog is None:,0.6680445465541214,0.8692960007731574,False
1041,"def get_or_create_place(self, place_name): """"""Return the requested place object tuple-packed with a new indicator."""""" LOG.debug('get_or_create_place: looking for: %s', place_name) for place_handle in self.db.iter_place_handles(): place = self.db.get_place_from_handle(place_handle) place_title = place_displayer.display(self.db, place) <IF_STMT> return (0, place) place = Place() place.set_title(place_name) place.name = PlaceName(value=place_name) self.db.add_place(place, self.trans) return (1, place)",if place_title == place_name:,if place_title == 'No place found':,0.8648729004334125,0.8443258653392445,False
1042,def _skip_trivial(constraint_data): if skip_trivial_constraints: <IF_STMT> if constraint_data.variables is None: return True elif constraint_data.body.polynomial_degree() == 0: return True return False,"if isinstance(constraint_data, LinearCanonicalRepn):","if isinstance(constraint_data, trivial_constraint):",0.5747951400716265,0.761827408333416,False
1043,"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): <IF_STMT> del data[i] if is_tuple: return tuple(data) else: return data else: return None",if i < len(data) and i > -1:,if data[i] == item:,0.934035769686599,0.9036816878108535,False
1044,"def test_case_insensitivity(self): with support.EnvironmentVarGuard() as env: env.set('PYTHONCASEOK', '1') <IF_STMT> self.skipTest('os.environ changes not reflected in _os.environ') loader = self.find_module() self.assertTrue(hasattr(loader, 'load_module'))",if b'PYTHONCASEOK' not in _bootstrap._os.environ:,if 'PYTHONCASEOK' in _os.environ:,0.6989403904342992,0.7378351342269067,False
1045,def field_spec(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.field_spec_ is None: self.field_spec_ = FieldSpec() finally: self.lazy_init_lock_.release() return self.field_spec_,if self.field_spec_ is None:,if self.field_spec_ is None:,0.6272973784763626,0.6929598487720369,True
1046,"def reduce(self, f, init): for x in range(self._idx, rt.count(self._w_array)): <IF_STMT> return rt.deref(init) init = f.invoke([init, rt.nth(self._w_array, rt.wrap(x))]) return init",if rt.reduced_QMARK_(init):,if rt.is_null(init):,0.7858745709858159,0.803154665668484,False
1047,def _find(event: E) -> None: values = list(self.values) for value in values[self._selected_index + 1:] + values: text = fragment_list_to_text(to_formatted_text(value[1])).lower() <IF_STMT> self._selected_index = self.values.index(value) return,if text.startswith(event.data.lower()):,if text in self.values:,0.8281506812756767,0.7801270245332924,False
1048,def check_permissions(): if platform_os() != 'Windows': <IF_STMT> print(localization.lang_check_permissions['permissions_granted']) else: print(localization.lang_check_permissions['permissions_denied']) exit() else: print(localization.lang_check_permissions['windows_warning']) exit(),if getuid() == 0:,if localization.lang_check_permissions['permissions_granted']:,0.5825744207931468,0.7241577342575828,False
1049,"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition('.') if dot: if module_name: <IF_STMT> dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: logging.warning('Empty package name: %s', name)",if module_name in dependencies:,if module_name in dependencies:,0.7217889584188819,0.8385130047130208,True
1050,"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) if content == 'Salted__': data = StringIO() <IF_STMT> self.encryptor.decrypt(db, data) else: raise EncryptionError('Encrpyted credential storage: {}'.format(self.db)) return json.loads(data.getvalue()) else: return json.load(db) except: return {'creds': []}",if self.encryptor:,if self.encryptor:,0.6708921203677312,0.8996480074924822,True
1051,"def _parse(self, stream, context): obj = [] try: context_for_subcon = context if self.subcon.conflags & self.FLAG_COPY_CONTEXT: context_for_subcon = context.__copy__() while True: subobj = self.subcon._parse(stream, context_for_subcon) <IF_STMT> break obj.append(subobj) except ConstructError as ex: raise ArrayError('missing terminator', ex) return obj","if self.predicate(subobj, context):",if subobj is None:,0.6899043739191744,0.8555308664663046,False
1052,"def is_active_for_user(self, user): is_active = super(AbstractUserFlag, self).is_active_for_user(user) if is_active: return is_active user_ids = self._get_user_ids() if hasattr(user, 'pk') and user.pk in user_ids: return True if hasattr(user, 'groups'): group_ids = self._get_group_ids() if group_ids: user_groups = set(user.groups.all().values_list('pk', flat=True)) <IF_STMT> return True return None",if group_ids.intersection(user_groups):,if user_groups and user_groups.pk in group_ids:,0.9220955356946345,0.8262592696121884,False
1053,"def lookup_member(self, member_name): document_choices = self.choices or [] for document_choice in document_choices: doc_and_subclasses = [document_choice] + document_choice.__subclasses__() for doc_type in doc_and_subclasses: field = doc_type._fields.get(member_name) <IF_STMT> return field",if field:,if field:,0.8298107325221642,0.8751809448365924,True
1054,"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True <IF_STMT> return True return False",if not mother_handle:,if not mother_handle:,0.916793287948859,0.8815741981066073,True
1055,"def init_weights(self): for m in self.modules(): if isinstance(m, nn.Linear): normal_init(m, std=0.01) if isinstance(m, nn.Conv3d): xavier_init(m, distribution='uniform') <IF_STMT> constant_init(m, 1)","if isinstance(m, nn.BatchNorm3d):","if isinstance(m, nn.BatchNorm2d):",0.8889948154256145,0.777225641181298,False
1056,def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == 'resnet_tf': <IF_STMT> lrn_rate = hparams.mom_lrn elif step_num < 30000: lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate,if step_num < hparams.lrn_step:,if step_num < 20000:,0.7139698675217585,0.8902056737869248,False
1057,"def token_producer(source): token = source.read_uint8() while token is not None: if is_push_data_token(token): yield DataToken(read_data(token, source)) <IF_STMT> yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",elif is_small_integer(token):,elif is_push_small_integer_token(token):,0.8482064220715578,0.8466657105524215,False
1058,"def user_info(oicsrv, userdb, sub, client_id='', user_info_claims=None): identity = userdb[sub] if user_info_claims: result = {} for key, restr in user_info_claims['claims'].items(): try: result[key] = identity[key] except KeyError: <IF_STMT> raise Exception(""Missing property '%s'"" % key) else: result = identity return OpenIDSchema(**result)",if restr == {'essential': True}:,if client_id not in identity:,0.8668597555361398,0.8380055871435848,False
1059,"def _helpSlot(self, *args): help_text = 'Filters are applied to packets in both direction.\n\n' filter_nb = 0 for filter in self._filters: help_text += '{}: {}'.format(filter['name'], filter['description']) filter_nb += 1 <IF_STMT> help_text += '\n\n' QtWidgets.QMessageBox.information(self, 'Help for filters', help_text)",if len(self._filters) != filter_nb:,if filter_nb > self._max_filters:,0.8830871373009238,0.8555308664663046,False
1060,"def find_user_theme(self, name: str) -> Theme: """"""Find a theme named as *name* from latex_theme_path."""""" for theme_path in self.theme_paths: config_path = path.join(theme_path, name, 'theme.conf') <IF_STMT> try: return UserTheme(name, config_path) except ThemeError as exc: logger.warning(exc) return None",if path.isfile(config_path):,if path.exists(config_path):,0.7650286480554928,0.8966773400768917,False
1061,"def decompress(self, value): if value: <IF_STMT> if value.country_code and value.national_number: return ['+%d' % value.country_code, national_significant_number(value)] else: return value.split('.') return [None, '']",if type(value) == PhoneNumber:,if '.' in value:,0.6067893390343855,0.7498810286408993,False
1062,"def update_prevdoc_status(self, flag): for quotation in list(set([d.prevdoc_docname for d in self.get('items')])): <IF_STMT> doc = frappe.get_doc('Quotation', quotation) if doc.docstatus == 2: frappe.throw(_('Quotation {0} is cancelled').format(quotation)) doc.set_status(update=True) doc.update_opportunity()",if quotation:,if flag:,0.7777364557318752,0.8590888738245122,False
1063,"def map(item): if item.deleted: return exploration = exp_fetchers.get_exploration_from_model(item) for state_name, state in exploration.states.items(): hints_length = len(state.interaction.hints) <IF_STMT> exp_and_state_key = '%s %s' % (item.id, state_name.encode('utf-8')) yield (python_utils.UNICODE(hints_length), exp_and_state_key)",if hints_length > 0:,if hints_length > 0:,0.813499771560749,0.8038019482772603,True
1064,"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) <IF_STMT> selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines","if self.tags and self._tags_match(machine.tags, self.tags):",if self.args.host and self._args.host == machine.name:,0.7728095302084176,0.7590598306198806,False
1065,"def _ripple_trim_compositors_move(self, delta): comp_ids = self.multi_data.moved_compositors_destroy_ids tracks_compositors = _get_tracks_compositors_list() track_moved = self.multi_data.track_affected for i in range(1, len(current_sequence().tracks) - 1): if not track_moved[i - 1]: continue track_comps = tracks_compositors[i - 1] for comp in track_comps: <IF_STMT> comp.move(delta)",if comp.destroy_id in comp_ids:,if comp.id in comp_ids:,0.7757094456149632,0.8728413392171659,False
1066,def stream_docker_log(log_stream): async for line in log_stream: if 'stream' in line and line['stream'].strip(): logger.debug(line['stream'].strip()) <IF_STMT> logger.debug(line['status'].strip()) elif 'error' in line: logger.error(line['error'].strip()) raise DockerBuildError,elif 'status' in line:,elif 'status' in line:,0.8755171609299462,0.7709002428237395,True
1067,"def create_keyfile(self, keyfile, size=64, force=False): if force or not os.path.exists(keyfile): keypath = os.path.dirname(keyfile) <IF_STMT> os.makedirs(keypath) subprocess.run(['dd', 'if=/dev/random', f'of={keyfile}', f'bs={size}', 'count=1'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)",if not os.path.exists(keypath):,if not os.path.exists(keypath):,0.7664396637489594,0.80377750806414,True
1068,"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num elif self.op == '-': self.current -= num <IF_STMT> self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '*':,elif self.op == '*':,0.9301459329147952,0.914208565914368,True
1069,"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = (expr.real, expr.imag) <IF_STMT> real = Integer(0) if -delta < imag.get_float_value() < delta: imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",if -delta < real.get_float_value() < delta:,if -delta < real.get_float_value() < delta:,0.8323408572915162,0.8688589397154922,True
1070,"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists('file_sources.json'): file_sources_as_dict = None with open('file_sources.json', 'r') as f: file_sources_as_dict = json.load(f) <IF_STMT> file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",if file_sources_as_dict is not None:,if file_sources_as_dict is not None:,0.9260201531866552,0.8527204701689132,True
1071,"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): <IF_STMT> if tag.user: tts[name] = '%ssort' % name if tag.internal: tts['~%s' % name] = '~%ssort' % name return tts",if tag.has_sort:,if tag.sort_type == 'sort':,0.7538805427621108,0.828399516355805,False
1072,"def __init__(self, **kwargs): if self.name is None: raise RuntimeError('RenderPrimitive cannot be used directly') self.option_values = {} for key, val in kwargs.items(): <IF_STMT> raise ValueError(""primitive `{0}' has no option `{1}'"".format(self.name, key)) self.option_values[key] = val for name, (description, default) in self.options.items(): if not name in self.option_values: self.option_values[name] = default",if not key in self.options:,if key not in self.options:,0.760621144283783,0.8677319190106252,False
1073,"def modify_bottle_params(self, output_stride=None): if output_stride is not None and output_stride % 2 != 0: raise Exception('output stride must to be even number') if output_stride is None: return else: stride = 2 for i, _cfg in enumerate(self.cfg): stride = stride * _cfg[-1] <IF_STMT> s = 1 self.cfg[i][-1] = s",if stride > output_stride:,if stride > output_stride:,0.8463858568275975,0.8879659171421962,True
1074,"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) <IF_STMT> ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",elif is_iterable(value):,"elif isinstance(value, list):",0.9443755511890264,0.9042878500265974,False
1075,"def make_shares(self, plaintext): share_arrays = [] for i, p in enumerate(plaintext): share_array = self.make_byte_shares(p) for sa in share_array: <IF_STMT> share_arrays.append(array.array('H')) current_share_array = sa current_share_array.append(sa) return share_arrays",if i == 0:,if i == len(p):,0.8041821144670986,0.7965020533851944,False
1076,"def populate(self, item): path = self.getItemPath(item) value = self.getValue(path) for name in sorted(value.__dict__.keys()): <IF_STMT> continue child = getattr(value, name, None) if hasattr(child, '__dict__'): item.addChild(name, True) else: item.addChild(name, False)",if name[:2] == '__' and name[-2:] == '__':,if name == 'children':,0.8856155087498332,0.8105932471967202,False
1077,def __repr__(self): try: if self._semlock._is_mine(): name = current_process().name <IF_STMT> name += '|' + threading.current_thread().name elif self._semlock._get_value() == 1: name = 'None' elif self._semlock._count() > 0: name = 'SomeOtherThread' else: name = 'SomeOtherProcess' except Exception: name = 'unknown' return '<Lock(owner=%s)>' % name,if threading.current_thread().name != 'MainThread':,if threading.current_thread():,0.7086346932716636,0.9122561819614461,False
1078,"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): """"""Add data to be displayed in the buffer."""""" self.values.extend(lines) if scroll_end: <IF_STMT> self.start_display_at = len(self.values) - len(self._my_widgets) elif scroll_if_editing: self.start_display_at = len(self.values) - len(self._my_widgets)",if not self.editing:,if self.scroll_if_editing:,0.9125146847680924,0.8743414417652072,False
1079,"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): if dep.repo is None: continue <IF_STMT> continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())","if not isinstance(dep.repo, WarehouseBaseRepo):","if not isinstance(dep.repo, WarehouseBaseRepo):",0.9119619759818886,0.8474968231198384,True
1080,"def _apply_flag_attrs(src_flag, dest_flag): baseline_flag = FlagDef('', {}, None) for name in dir(src_flag): <IF_STMT> continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) if dest_val == baseline_val: setattr(dest_flag, name, getattr(src_flag, name))",if name[:1] == '_':,if name.startswith('_'):,0.908602186207524,0.8866029039778043,False
1081,"def out(parent, attr, indent=0): val = getattr(parent, attr) prefix = '%s%s:' % (' ' * indent, attr.replace('_', '-')) if val is None: cli.out(prefix) else: <IF_STMT> val = [flag_util.encode_flag_val(c.value) for c in val] cli.out('%s %s' % (prefix, flag_util.encode_flag_val(val)))",if attr == 'choices':,"if isinstance(val, list):",0.7303254865871436,0.8783650674919876,False
1082,"def add_cand_to_check(cands): for cand in cands: x = cand.creator if x is None: continue <IF_STMT> heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",if x not in fan_out:,if x.rank > fan_out[x]:,0.5430181155539936,0.760856626273165,False
1083,"def task_tree_lines(task=None): if task is None: task = current_root_task() rendered_children = [] nurseries = list(task.child_nurseries) while nurseries: nursery = nurseries.pop() nursery_children = _rendered_nursery_children(nursery) <IF_STMT> nested = _render_subtree('(nested nursery)', rendered_children) nursery_children.append(nested) rendered_children = nursery_children return _render_subtree(task.name, rendered_children)",if rendered_children:,if nursery_children:,0.8711695543458999,0.8996480074924822,False
1084,"def lock_workspace(build_dir): _BUILDING_LOCK_FILE = '.blade.building.lock' lock_file_fd, ret_code = lock_file(os.path.join(build_dir, _BUILDING_LOCK_FILE)) if lock_file_fd == -1: <IF_STMT> console.fatal('There is already an active building in current workspace.') else: console.fatal('Lock exception, please try it later.') return lock_file_fd",if ret_code == errno.EAGAIN:,if ret_code == 0:,0.7399968221526063,0.8385130047130208,False
1085,"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data['features']), 2) for feature in response.data['features']: self.assertIn('bbox', feature) fid = feature['id'] if fid == 1: self.assertEqual(feature['bbox'], self.bl1.bbox_geometry.extent) <IF_STMT> self.assertEqual(feature['bbox'], self.bl2.bbox_geometry.extent) else: self.fail('Unexpected id: {0}'.format(fid)) BoxedLocation.objects.all().delete()",elif fid == 2:,elif fid == 2:,0.8439136984664161,0.8385130047130208,True
1086,"def result(): R, V = (rays, virtual_rays) if V is not None: if normalize: V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() <IF_STMT> raise ValueError('virtual rays must be linearly independent and with other rays span the ambient space.') return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if len(V) != d - R.dim() or (R + V).dim() != d:,if d < 1:,0.9227456740428528,0.8983343737277126,False
1087,"def search_host(self, search_string): results = [] for host_entry in self.config_data: if host_entry.get('type') != 'entry': continue if host_entry.get('host') == '*': continue searchable_information = host_entry.get('host') for key, value in six.iteritems(host_entry.get('options')): <IF_STMT> value = ' '.join(value) if isinstance(value, int): value = str(value) searchable_information += ' ' + value if search_string in searchable_information: results.append(host_entry) return results","if isinstance(value, list):","if isinstance(value, list):",0.9430451718702464,0.9134996171406936,True
1088,"def test_async_iterator(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i)) received = 0 async for value in stream: assert value == received received += 1 <IF_STMT> break assert await channel_empty(stream.channel)",if received >= 100:,if received >= 100:,0.8387803368069356,0.8431339019329497,True
1089,def has_google_credentials(): global _HAS_GOOGLE_CREDENTIALS if _HAS_GOOGLE_CREDENTIALS is None: provider = Provider('google') <IF_STMT> _HAS_GOOGLE_CREDENTIALS = False else: _HAS_GOOGLE_CREDENTIALS = True return _HAS_GOOGLE_CREDENTIALS,if provider.get_access_key() is None or provider.get_secret_key() is None:,if provider.get_credentials():,0.6654091214449809,0.8232490471721702,False
1090,"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() if a < b: return -1 <IF_STMT> return 0 else: raise TypeError('expected date or datetime object') return 1",elif a == b:,elif a > b:,0.6309649455026445,0.8385130047130208,False
1091,"def validate_weight(self, weight): try: add_acl_to_obj(self.context['user_acl'], self.category) except AttributeError: return weight if weight > self.category.acl.get('can_pin_threads', 0): <IF_STMT> raise ValidationError(_(""You don't have permission to pin threads globally in this category."")) else: raise ValidationError(_(""You don't have permission to pin threads in this category."")) return weight",if weight == 2:,"if weight < self.category.acl.get('can_pin_threads_globally', False):",0.8871112702591926,0.8527204701689132,False
1092,"def effective(line): for b in line: if not b.cond: return else: try: val = 5 <IF_STMT> if b.ignore: b.ignore -= 1 else: return (b, True) except: return (b, False) return",if val:,if val < b.cond:,0.5761146682731864,0.8228500218338367,False
1093,"def wheelEvent(self, event): """"""Handle a wheel event."""""" if QtCore.Qt.ControlModifier & event.modifiers(): d = {'c': self.leo_c} if isQt5: point = event.angleDelta() delta = point.y() or point.x() else: delta = event.delta() <IF_STMT> zoom_out(d) else: zoom_in(d) event.accept() return QtWidgets.QTextBrowser.wheelEvent(self, event)",if delta < 0:,if delta > 0.5:,0.897178046391833,0.8555308664663046,False
1094,"def test_evname_in_mp_events_testcases(): ok = True for evname in ins.mp_events: if evname == 'version': continue for i, args in enumerate(ins.mp_events[evname]['test_cases']): <IF_STMT> msg = 'Error, for evname %s the testase #%d does not match evname' print(msg % (evname, i)) ok = False if ok: print('test_evname_in_mp_events_testcases: passed')",if evname != args[0]:,if not args:,0.8870274867785349,0.897752847848028,False
1095,def check_database(): if len(EmailAddress.objects.all()) > 0: print('Are you sure you want to wipe the existing development database and reseed it? (Y/N)') <IF_STMT> destroy_database() else: return False else: return True,if raw_input().lower() == 'y':,if not is_admin():,0.5766298822727783,0.8446593249975184,False
1096,"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if self._requested_namespaces is not None and self._requested_namespaces != []: for requested_namespace in self._requested_namespaces: if requested_namespace[0] is '*': return [] <IF_STMT> requested_databases.append(requested_namespace[0]) return requested_databases",elif requested_namespace[0] not in IGNORE_DBS:,elif requested_namespace[0] not in self._ignored_databases:,0.9025620939969572,0.8380055871435848,False
1097,"def decorated(self, *args, **kwargs): start_time = time.perf_counter() stderr = '' saved_exception = None try: yield from fn(self, *args, **kwargs) except GitSavvyError as e: stderr = e.stderr saved_exception = e finally: end_time = time.perf_counter() util.debug.log_git(args, None, '<SNIP>', stderr, end_time - start_time) <IF_STMT> raise saved_exception from None",if saved_exception:,if saved_exception:,0.9243509317794805,0.9202663016973823,True
1098,"def is_suppressed_warning(type: str, subtype: str, suppress_warnings: List[str]) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: if '.' in warning_type: target, subtarget = warning_type.split('.', 1) else: target, subtarget = (warning_type, None) <IF_STMT> if subtype is None or subtarget is None or subtarget == subtype or (subtarget == '*'): return True return False",if target == type:,if target == type:,0.9127728253431694,0.9128479730518225,True
1099,"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) if j == -1: attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) <IF_STMT> return r",if reply == '!done':,if len(r) == 0:,0.9219711194992227,0.9031773220911172,False
1100,"def encrypt(self, plaintext): encrypted = [] for p in _string_to_bytes(plaintext): <IF_STMT> self._remaining_block = self._aes.encrypt(self._last_precipherblock) self._last_precipherblock = [] precipherbyte = self._remaining_block.pop(0) self._last_precipherblock.append(precipherbyte) cipherbyte = p ^ precipherbyte encrypted.append(cipherbyte) return _bytes_to_string(encrypted)",if len(self._remaining_block) == 0:,if len(self._remaining_block) == 0:,0.7721378443966007,0.8169276475307028,True
1101,"def find_symbol(self, r, globally=False): query = self.view.substr(self.view.word(r)) fname = self.view.file_name().replace('\\', '/') locations = self.view.window().lookup_symbol_in_index(query) if not locations: return try: <IF_STMT> location = [hit[2] for hit in locations if fname.endswith(hit[1])][0] return (location[0] - 1, location[1] - 1) else: return locations[0] except IndexError: return",if not globally:,if globally:,0.9212284801276815,0.9144061946646023,False
1102,"def __getslice__(self, i, j): try: <IF_STMT> j = len(self) if i < 0 or j < 0: raise dns.exception.FormError if i != j: super(WireData, self).__getitem__(i) super(WireData, self).__getitem__(j - 1) return WireData(super(WireData, self).__getslice__(i, j)) except IndexError: raise dns.exception.FormError",if j == sys.maxint:,if j == -1:,0.919573775463576,0.8555308664663046,False
1103,"def main(): r = redis.StrictRedis() curr_memory = prev_memory = r.info()['used_memory'] while True: <IF_STMT> print('Delta Memory : %d, Total Memory : %d' % (curr_memory - prev_memory, curr_memory)) time.sleep(1) prev_memory = curr_memory curr_memory = r.info()['used_memory']",if prev_memory != curr_memory:,if curr_memory > prev_memory:,0.8381658586710561,0.8385130047130208,False
1104,"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: <IF_STMT> self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if fname not in self._flags:,if fname in self._orig:,0.9265390469059656,0.8964173245779284,False
1105,"def urls(self, version=None): """"""Returns all URLS that are mapped to this interface"""""" urls = [] for _base_url, routes in self.api.http.routes.items(): for url, methods in routes.items(): for _method, versions in methods.items(): for interface_version, interface in versions.items(): <IF_STMT> if not url in urls: urls.append(('/v{0}'.format(version) if version else '') + url) return urls",if interface_version == version and interface == self:,if interface_version == version:,0.9445127168519967,0.8923575006167597,False
1106,"def _handle_data(self, text): if self._translate: <IF_STMT> self._data.append(text) else: self._translate = False self._data = [] self._comments = []",if not text.startswith('gtk-'):,if text:,0.8403406901614084,0.7778111223054219,False
1107,"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): <IF_STMT> continue log.info('changing mode of %s to %o', dirpath, mode) if not self.dry_run: os.chmod(dirpath, mode)",if os.path.islink(dirpath):,if not self.is_dir_supported(dirpath):,0.7691809064320871,0.8446593249975184,False
1108,"def language(self): if self.lang_data: lang_data = [s if s != 'None' else None for s in self.lang_data] <IF_STMT> return Language(lang_data[0], country=lang_data[1], script=lang_data[2])",if lang_data[0]:,if lang_data:,0.6604097716899995,0.8318180062062374,False
1109,"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 if col == nCol: for col in range(0, nCol, 2): <IF_STMT> break if col + 2 == nCol: col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)","if not self.layout.itemAt(row, col):",if sample[col] == label:,0.9405058674312593,0.8964173245779284,False
1110,"def align_comments(tlist): tidx, token = tlist.token_next_by(i=sql.Comment) while token: pidx, prev_ = tlist.token_prev(tidx) <IF_STMT> tlist.group_tokens(sql.TokenList, pidx, tidx, extend=True) tidx = pidx tidx, token = tlist.token_next_by(i=sql.Comment, idx=tidx)","if isinstance(prev_, sql.TokenList):",if prev_ == token:,0.7574977564424504,0.7886336751695258,False
1111,"def hook_GetVariable(ql, address, params): if params['VariableName'] in ql.env: var = ql.env[params['VariableName']] read_len = read_int64(ql, params['DataSize']) if params['Attributes'] != 0: write_int64(ql, params['Attributes'], 0) write_int64(ql, params['DataSize'], len(var)) <IF_STMT> return EFI_BUFFER_TOO_SMALL if params['Data'] != 0: ql.mem.write(params['Data'], var) return EFI_SUCCESS return EFI_NOT_FOUND",if read_len < len(var):,if read_len > 0:,0.9330051777128802,0.8592377270804451,False
1112,"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) if self._CheckMySQLConnection(): print('Successfully connected to MySQL with the given configuration.') return else: print('Error: Could not connect to MySQL with the given configuration.') retry = RetryBoolQuestion('Do you want to retry MySQL configuration?', True) <IF_STMT> raise ConfigInitError()",if not retry:,if not retry:,0.7363142739369224,0.916939218488858,True
1113,"def split_long_line_with_indent(line, max_per_line, indent): """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines."""""" words = line.split(' ') lines = [] current_line = words[0] for word in words[1:]: <IF_STMT> lines.append(current_line) current_line = ' ' * indent + word else: current_line = f'{current_line} {word}' lines.append(current_line) return '\n'.join(lines)",if len(f'{current_line} {word}') > max_per_line:,if max_per_line > indent:,0.8406643288258345,0.8964173245779284,False
1114,"def gen_cli(docs_dir): with open(os.path.join(docs_dir, 'CLI_template.md'), 'r') as cli_temp_file: temp_lines = cli_temp_file.readlines() lines = [] for line in temp_lines: matched = re.match('{onnx-tf.*}', line) <IF_STMT> command = matched.string.strip()[1:-1] output = subprocess.check_output(command.split(' ')).decode('UTF-8') lines.append(output) else: lines.append(line) with open(os.path.join(docs_dir, 'CLI.md'), 'w') as cli_file: cli_file.writelines(lines)",if matched:,if matched:,0.8554646153402624,0.9099951253570094,True
1115,"def read(self, size=None): if size == 0: return '' data = list() while size is None or size > 0: line = self.readline(size or -1) if not line: break <IF_STMT> size -= len(line) data.append(line) return ''.join(data)",if size is not None:,if len(line) > size:,0.6602881821193924,0.8516228624291206,False
1116,"def _get_format_and_pattern(file_path): file_path = Path(file_path) with file_path.open() as f: first_line = f.readline().strip() match = re.match('format *: *(.+)', first_line) <IF_STMT> return ('gztar', first_line, 1) return (match.group(1), f.readline().strip(), 2)",if match is None:,if match is None:,0.7440915219531838,0.8038019482772603,True
1117,def remove_old_snapshot(install_dir): logging.info('Removing any old files in {}'.format(install_dir)) for file in glob.glob('{}/*'.format(install_dir)): try: if os.path.isfile(file): os.unlink(file) <IF_STMT> shutil.rmtree(file) except Exception as error: logging.error('Error: {}'.format(error)) sys.exit(1),elif os.path.isdir(file):,elif os.path.isdir(file):,0.8905843760647225,0.8531413606256201,True
1118,"def _test_forever(self, tests): while True: for test_name in tests: yield test_name if self.bad: return <IF_STMT> return",if self.ns.fail_env_changed and self.environment_changed:,elif self.test_forever:,0.5929306323087685,0.8193228857188178,False
1119,"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): if line.startswith('#include') or line.startswith('%include'): line = line.split(' ')[1].strip('\'""\r\n') <IF_STMT> dep.append(line) return [i for i in dep if os.path.exists(i)]",if not ('<' in line or line in dep):,if line and line.startswith('#dependency'):,0.7317070352960238,0.8105932471967202,False
1120,"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() <IF_STMT> key.name = name if metadata is not None: key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",if name is not None:,if name is not None:,0.8954649125440907,0.7765145040967655,True
1121,"def range(self, dimension, data_range=True, dimension_range=True): if self.nodes and dimension in self.nodes.dimensions(): node_range = self.nodes.range(dimension, data_range, dimension_range) <IF_STMT> path_range = self._edgepaths.range(dimension, data_range, dimension_range) return max_range([node_range, path_range]) return node_range return super(Graph, self).range(dimension, data_range, dimension_range)",if self._edgepaths:,if self._edgepaths and dimension in self._edgepaths.dimensions():,0.7524079588643818,0.7865984197371234,False
1122,"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) <IF_STMT> break chan.send(data) if chan in r: data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",if len(data) == 0:,if len(data) == 0:,0.9372421105093857,0.9019629427251674,True
1123,"def output_layer(self, features, **kwargs): """"""Project features to the vocabulary size."""""" if self.adaptive_softmax is None: <IF_STMT> return F.linear(features, self.embed_tokens.weight) else: return F.linear(features, self.embed_out) else: return features",if self.share_input_output_embed:,if self.embed_tokens is not None:,0.5344521833695366,0.759907656827929,False
1124,"def generate(self, dest, vars): util.ensure_dir(dest) for relpath, src, template in self._file_templates: file_dest = os.path.join(dest, relpath) util.ensure_dir(os.path.dirname(file_dest)) <IF_STMT> shutil.copyfile(src, file_dest) else: _render_template(template, vars, file_dest)",if template is None:,if os.path.isfile(src):,0.8107752236678784,0.839587623092576,False
1125,"def _py_matching_callback(self, context, result, sender, device): d = HIDDevice.get_device(c_void_p(device)) if d not in self.devices: self.devices.add(d) for x in self.matching_observers: <IF_STMT> x.device_discovered(d)","if hasattr(x, 'device_discovered'):",if x.device_discovered(d):,0.8992573120200479,0.8641944207171431,False
1126,"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) <IF_STMT> del new_kwargs['encoding'] if 'errors' in kwargs: del new_kwargs['errors'] return quote(*args, **new_kwargs)",if 'encoding' in new_kwargs:,if 'encoding' in kwargs:,0.8804520261912372,0.7801270245332924,False
1127,"def Set(self, attr, value): hook = getattr(self, '_set_%s' % attr, None) if hook: <IF_STMT> raise ValueError('Can only update attribute %s using the context manager.' % attr) if attr not in self._pending_hooks: self._pending_hooks.append(attr) self._pending_parameters[attr] = value else: super(Configuration, self).Set(attr, value)",if self._lock > 0:,"if not hasattr(hook, 'contextmanager'):",0.900843857330637,0.8627586293513119,False
1128,"def on_profiles_loaded(self, profiles): cb = self.builder.get_object('cbProfile') model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() if name.endswith('.mod'): continue <IF_STMT> name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",if name.endswith('.sccprofile'):,if name.endswith('.py'):,0.7876539492180109,0.8696398662122882,False
1129,"def get_eval_task(self, worker_id): """"""Return next evaluation (task_id, Task) tuple"""""" with self._lock: <IF_STMT> return (-1, None) self._task_id += 1 task = self._eval_todo.pop() self._doing[self._task_id] = (worker_id, task, time.time()) return (self._task_id, task)",if not self._eval_todo:,if self._task_id >= len(self._eval_todo):,0.8422088552570853,0.8169276475307028,False
1130,"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) <IF_STMT> if not cmd.stdout.strip(): log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) if log_cmd.check(f'docker logs for {self.path.k8s}'): print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if not cmd.check(f'docker check for {self.path.k8s}'):,if cmd.check(f'docker ps for {self.path.k8s}'):,0.8913803528103463,0.8294838585473985,False
1131,"def disjoined(data): data_disjoined = None dim = len(data.shape) for d in range(dim): axes = list(range(dim)) axes.remove(d) data1d = multisum(data, axes) shape = [1 for k in range(dim)] shape[d] = len(data1d) data1d = data1d.reshape(tuple(shape)) <IF_STMT> data_disjoined = data1d else: data_disjoined = data_disjoined * data1d return data_disjoined",if d == 0:,if data_disjoined is None:,0.916452227059374,0.8806615362338783,False
1132,def safe_repr(val): try: <IF_STMT> val = _obj_with_safe_repr(val) ret = repr(val) if six.PY2: ret = ret.decode('utf-8') except UnicodeEncodeError: ret = red('a %r that cannot be represented' % type(val)) else: ret = green(ret) return ret,"if isinstance(val, dict):","if isinstance(val, dict):",0.9063204448010115,0.8635707684233572,True
1133,"def wrapper(*args, **kwargs): resp = view_func(*args, **kwargs) if isinstance(resp, dict): ctx_params = request.environ.get('webrec.template_params') <IF_STMT> resp.update(ctx_params) template = self.jinja_env.jinja_env.get_or_select_template(template_name) return template.render(**resp) else: return resp",if ctx_params:,if ctx_params:,0.6040688603420603,0.839587623092576,True
1134,"def post(self, request, *args, **kwargs): contact_id = kwargs.get('pk') self.object = get_object_or_404(Contact, id=contact_id) if self.request.user.role != 'ADMIN' and (not self.request.user.is_superuser) and (self.request.user != self.object.created_by) or self.object.company != self.request.company: raise PermissionDenied else: if self.object.address_id: self.object.address.delete() self.object.delete() <IF_STMT> return JsonResponse({'error': False}) return redirect('contacts:list')",if self.request.is_ajax():,if self.object.address_id:,0.9457440706839386,0.9099951253570094,False
1135,"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') <IF_STMT> text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text",if '<' in text:,if '<' in text:,0.9527124580151902,0.9084940438173679,True
1136,"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': <IF_STMT> return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True",if not everythingIsUnicode(v):,if not everythingIsUnicode(v):,0.9157079344592661,0.9167056528641923,True
1137,"def fill(self): try: while not self.stopping.wait(self.sample_wait) and len(self.queue) < self.queue.maxlen: self.queue.append(self.parent._read()) <IF_STMT> self.parent._fire_events() self.full.set() while not self.stopping.wait(self.sample_wait): self.queue.append(self.parent._read()) if isinstance(self.parent, EventsMixin): self.parent._fire_events() except ReferenceError: pass","if self.partial and isinstance(self.parent, EventsMixin):","if isinstance(self.parent, EventsMixin):",0.6170467257688126,0.8196189957582152,False
1138,"def _SetListviewTextItems(self, items): self.listview.DeleteAllItems() index = -1 for item in items: index = self.listview.InsertItem(index + 1, item[0]) data = item[1] <IF_STMT> data = '' self.listview.SetItemText(index, 1, data)",if data is None:,if data == '':,0.823225082221847,0.8038019482772603,False
1139,"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: body = six.ensure_str(request.body) <IF_STMT> request.body = body.replace(old, new) return request",if old in body:,if body:,0.7494785780310343,0.8590888738245122,False
1140,"def serialize(cls, value, *args, **kwargs): if value is None: return '' value_as_string = six.text_type(value) if SHOULD_NOT_USE_LOCALE: return value_as_string else: grouping = kwargs.get('grouping', None) has_decimal_places = value_as_string.find('.') != -1 <IF_STMT> string_format = '%d' else: decimal_places = len(value_as_string.split('.')[1]) string_format = '%.{}f'.format(decimal_places) return locale.format(string_format, value, grouping=grouping)",if not has_decimal_places:,if has_decimal_places:,0.8860173851371999,0.9164531641034833,False
1141,"def review_link(request, path_obj): try: <IF_STMT> if check_permission('translate', request): text = _('Review Suggestions') else: text = _('View Suggestions') return {'href': dispatch.translate(request, path_obj.pootle_path, matchnames=['hassuggestion']), 'text': text} except IOError: pass",if path_obj.has_suggestions():,if path_obj.pootle_path:,0.9019183975574582,0.8645707301556367,False
1142,"def _migrate_key(self, key): """"""migrate key from old .dat file"""""" key_path = os.path.join(self.home_path, 'keys.dat') if os.path.exists(key_path): try: key_data = json.loads(open(key_path, 'rb').read()) <IF_STMT> self.add_key(key, key_data.get(key)) except: self.error(f""Corrupt key file. Manual migration of '{key}' required."")",if key_data.get(key):,if key_data:,0.6966738656429293,0.8866029039778043,False
1143,"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: <IF_STMT> root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: if root_obj: args = self.get_callback_args(root_obj, callback) if args: yield args","if not isinstance(root_objs, Iterable):","if not isinstance(root_objs, list):",0.9343859610285618,0.8780099567239787,False
1144,"def GetDefFile(self, gyp_to_build_path): """"""Returns the .def file from sources, if any.  Otherwise returns None."""""" spec = self.spec if spec['type'] in ('shared_library', 'loadable_module', 'executable'): def_files = [s for s in spec.get('sources', []) if s.endswith('.def')] <IF_STMT> return gyp_to_build_path(def_files[0]) elif len(def_files) > 1: raise Exception('Multiple .def files') return None",if len(def_files) == 1:,if len(def_files) == 1:,0.6421862900958621,0.8832000938217648,True
1145,"def _validate_gallery(images): for image in images: image_path = image.get('image_path', '') if image_path: if not isfile(image_path): raise TypeError(f'{image_path!r} is not a valid image path.') else: raise TypeError(""'image_path' is required."") <IF_STMT> raise TypeError('Caption must be 180 characters or less.')","if not len(image.get('caption', '')) <= 180:",if len(image.get('caption')) < 180:,0.8895347656036984,0.8555308664663046,False
1146,"def VType(self): if 'DW_AT_type' in self.attributes: target = self.types[self.type_id] target_type = target.VType() <IF_STMT> target_type = [target_type, None] return ['Pointer', dict(target=target_type[0], target_args=target_type[1])] return ['Pointer', dict(target='Void')]","if not isinstance(target_type, list):","if not isinstance(target_type, list):",0.8082549804123861,0.7801270245332924,True
1147,"def addInPlace(self, value1, value2): for group in value2: for key in value2[group]: <IF_STMT> value1[group][key] = value2[group][key] else: value1[group][key] += value2[group][key] return value1",if key not in value1[group]:,if key not in value1[group]:,0.68836270217438,0.7297349727547102,True
1148,"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): <IF_STMT> query[k] = {} if isinstance(v, list): query[k] = v else: query[k].update(v) return query",if k not in query:,if k not in query:,0.7222571324643651,0.8248765135255685,True
1149,"def _handled_eventtype(self, eventtype, handler): if eventtype not in known_events: log.error('The event ""%s"" is not known', eventtype) return False if known_events[eventtype].__module__.startswith('deluge.event'): <IF_STMT> return True log.error('You cannot register custom notification providers for built-in event types.') return False return True",if handler.__self__ is self:,if handler.handled_eventtype:,0.896101732658486,0.9024521756077707,False
1150,"def get_ax_arg(uri): if not ax_ns: return u'' prefix = 'openid.' + ax_ns + '.type.' ax_name = None for name, values in self.request.arguments.iteritems(): <IF_STMT> part = name[len(prefix):] ax_name = 'openid.' + ax_ns + '.value.' + part break if not ax_name: return u'' return self.get_argument(ax_name, u'')",if values[-1] == uri and name.startswith(prefix):,if name.startswith(prefix):,0.9064195847964798,0.9184043388013005,False
1151,"def handle_starttag(self, tag, attrs): if tag == 'base': self.base_url = dict(attrs).get('href') if self.scan_tag(tag): for attr, value in attrs: <IF_STMT> if self.strip: value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",if self.scan_attr(attr):,if attr == 'href':,0.6929959642475473,0.8431339019329497,False
1152,"def test_long_steadystate_queue_popright(self): for size in (0, 1, 2, 100, 1000): d = deque(reversed(range(size))) append, pop = (d.appendleft, d.pop) for i in range(size, BIG): append(i) x = pop() <IF_STMT> self.assertEqual(x, i - size) self.assertEqual(list(reversed(list(d))), list(range(BIG - size, BIG)))",if x != i - size:,if x != i - size:,0.8653664212841481,0.8132493528194856,True
1153,"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) <IF_STMT> self.on_message(msg) return True self.close() except socket.error as err: if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK): pass else: self.on_error(err) return False",if msg:,if msg:,0.7046317830719453,0.8866029039778043,True
1154,"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = '' <IF_STMT> root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if len(root) > 0:,if i == 0:,0.8028609709952151,0.8923575006167597,False
1155,"def cmp(self, other): v_is_ptr = not isinstance(self, CTypesGenericPrimitive) w_is_ptr = isinstance(other, CTypesData) and (not isinstance(other, CTypesGenericPrimitive)) if v_is_ptr and w_is_ptr: return cmpfunc(self._convert_to_address(None), other._convert_to_address(None)) elif v_is_ptr or w_is_ptr: return NotImplemented else: if isinstance(self, CTypesGenericPrimitive): self = self._value <IF_STMT> other = other._value return cmpfunc(self, other)","if isinstance(other, CTypesGenericPrimitive):","elif isinstance(other, CTypesGenericPrimitive):",0.9341840393034563,0.8953711787948615,False
1156,"def get_external_addresses(self, label=None) -> List[str]: result = [] for c in self._conf['pools'].values(): <IF_STMT> if label == c['label']: result.append(c['external_address'][0]) else: result.append(c['external_address'][0]) return result",if label is not None:,if c['external_address']:,0.8864925863748809,0.8318180062062374,False
1157,"def coerce_text(v): if not isinstance(v, basestring_): <IF_STMT> attr = '__unicode__' else: attr = '__str__' if hasattr(v, attr): return unicode(v) else: return bytes(v) return v",if sys.version_info[0] < 3:,"if isinstance(v, unicode):",0.8223577235327206,0.8120341702859789,False
1158,"def check_localhost(self): """"""Warn if any socket_host is 'localhost'. See #711."""""" for k, v in cherrypy.config.items(): <IF_STMT> warnings.warn(""The use of 'localhost' as a socket host can cause problems on newer systems, since 'localhost' can map to either an IPv4 or an IPv6 address. You should use '127.0.0.1' or '[::1]' instead."")",if k == 'server.socket_host' and v == 'localhost':,if k == 'socket_host':,0.7944730733745953,0.8902056737869248,False
1159,"def add_songs(self, filenames, library): changed = [] for i in range(len(self)): <IF_STMT> song = library[self._list[i]] self._list[i] = song changed.append(song) if changed: self._emit_changed(changed, msg='add') return bool(changed)","if isinstance(self[i], str) and self._list[i] in filenames:",if self._list[i] in filenames:,0.8251676615344437,0.7886336751695258,False
1160,"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() if k not in keys: keys.add(k) dep = self.target_database[k] <IF_STMT> dep.attr['generate_java'] = True queue.extend(dep.deps)",if 'generate_java' in dep.attr:,if dep.attr:,0.933779371170236,0.9051034981560222,False
1161,"def get(self): name = request.args.get('filename') if name is not None: opts = dict() opts['type'] = 'episode' result = guessit(name, options=opts) res = dict() <IF_STMT> res['episode'] = result['episode'] else: res['episode'] = 0 if 'season' in result: res['season'] = result['season'] else: res['season'] = 0 if 'subtitle_language' in result: res['subtitle_language'] = str(result['subtitle_language']) return jsonify(data=res) else: return ('', 400)",if 'episode' in result:,if 'episode' in result:,0.6800031419414966,0.9019629427251674,True
1162,def _get_error_file(self) -> Optional[str]: error_file = None min_timestamp = sys.maxsize for replicas in self.role_replicas.values(): for replica in replicas: <IF_STMT> continue mtime = os.path.getmtime(replica.error_file) if mtime < min_timestamp: min_timestamp = mtime error_file = replica.error_file return error_file,if not os.path.exists(replica.error_file):,if replica.error_file is None:,0.794640151039403,0.8474968231198384,False
1163,"def findChapterNameForPosition(self, p): """"""Return the name of a chapter containing p or None if p does not exist."""""" cc, c = (self, self.c) if not p or not c.positionExists(p): return None for name in cc.chaptersDict: <IF_STMT> theChapter = cc.chaptersDict.get(name) if theChapter.positionIsInChapter(p): return name return 'main'",if name != 'main':,if name.startswith('main'):,0.9291697439860367,0.9202663016973823,False
1164,"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) if os.path.isfile(f_path): extension = os.path.splitext(f_path)[1] <IF_STMT> os.remove(f_path)",if extension in file_extensions:,if extension in file_extensions:,0.6057675812866427,0.7539352394200598,True
1165,"def execute_uncomment(self, event): cursor = self._editor.GetCurrentPos() line, pos = self._editor.GetCurLine() spaces = ' ' * self._tab_size comment = 'Comment' + spaces cpos = cursor - len(comment) lenline = len(line) if lenline > 0: idx = 0 while idx < lenline and line[idx] == ' ': idx += 1 <IF_STMT> self._editor.DeleteRange(cursor - pos + idx, len(comment)) self._editor.SetCurrentPos(cpos) self._editor.SetSelection(cpos, cpos) self.store_position()",if line[idx:len(comment) + idx].lower() == comment.lower():,if idx < lenline:,0.904947486358504,0.9084940438173679,False
1166,"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell(critical_suite_with_citations, empty_data_context): obs = SuiteEditNotebookRenderer.from_data_context(empty_data_context).render(critical_suite_with_citations, {'path': './foo/data'}) assert isinstance(obs, dict) found_expected = False for cell in obs['cells']: <IF_STMT> source_code = cell['source'] if 'batch_kwargs = {""path"": ""../.././foo/data""}' in source_code: found_expected = True break assert found_expected",if cell['cell_type'] == 'code':,if cell['type'] == 'code':,0.9055991314644508,0.8474968231198384,False
1167,"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile(max_size=self._storage.max_memory_size, suffix='.S3Boto3StorageFile', dir=setting('FILE_UPLOAD_TEMP_DIR')) if 'r' in self._mode: self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) <IF_STMT> self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",if self._storage.gzip and self.obj.content_encoding == 'gzip':,if self._mode.get('gzip'):,0.7262657109071242,0.8696398662122882,False
1168,"def _parse_filters(f_strs): filters = [] if not f_strs: return filters for f_str in f_strs: <IF_STMT> fname, fopts = f_str.split(':', 1) filters.append((fname, _parse_options([fopts]))) else: filters.append((f_str, {})) return filters",if ':' in f_str:,if ':' in f_str:,0.7789497447643805,0.8038019482772603,True
1169,"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = ', '.join(orig_text.replace(', ', ',').split(',')[:-1]) tags = [] for tag in self.tags_list: <IF_STMT> if orig_text[-1] not in (',', ' '): tags.append('%s,%s' % (text, tag)) tags.append('%s, %s' % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)","if ',' in orig_text:",if tag not in tags:,0.930319805908031,0.8703737209656045,False
1170,"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == '__pycache__': continue <IF_STMT> names.add(name.split('.')[0]) elif path.is_dir() and '.' not in name: names.add(name) if packages: packages = {package.lower().replace('-', '_') for package in packages} if len(names & packages) == len(packages): return packages return names",if name.endswith('.py'):,if path.is_file() and '.' in name:,0.9174867298126508,0.8615894073865318,False
1171,"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): <IF_STMT> if cred.kind == self.source.replace('ec2', 'aws'): credential = cred break elif cred.credential_type.kind != 'vault': credential = cred break return credential",if self.source in CLOUD_PROVIDERS:,if cred.credential_type is None:,0.8499826378291071,0.8692960007731574,False
1172,"def newickize(clade): """"""Convert a node tree to a Newick tree string, recursively."""""" label = clade.name or '' if label: unquoted_label = re.match(token_dict['unquoted node label'], label) <IF_STMT> label = ""'%s'"" % label.replace('\\', '\\\\').replace(""'"", ""\\'"") if clade.is_terminal(): return label + make_info_string(clade, terminal=True) else: subtrees = (newickize(sub) for sub in clade) return '(%s)%s' % (','.join(subtrees), label + make_info_string(clade))",if not unquoted_label or unquoted_label.end() < len(label):,if unquoted_label:,0.7355203777919457,0.9350761925543661,False
1173,"def __iter__(self): for name, value in self._vars.store.data.items(): source = self._sources[name] prefix = self._get_prefix(value) name = u'{0}{{{1}}}'.format(prefix, name) <IF_STMT> yield ArgumentInfo(name, value) else: yield VariableInfo(name, value, source)",if source == self.ARGUMENT_SOURCE:,if source is None:,0.7797082167763393,0.7965020533851944,False
1174,"def filepath_enumerate(paths): """"""Enumerate the file paths of all subfiles of the list of paths"""""" out = [] for path in paths: <IF_STMT> out.append(path) else: for root, dirs, files in os.walk(path): for name in files: out.append(os.path.normpath(os.path.join(root, name))) return out",if os.path.isfile(path):,if os.path.isdir(path):,0.9202559342072646,0.9051034981560222,False
1175,"def del_(self, key): hash_ = self.hash(key) node_ = self._table[hash_] pre_node = None while node_ is not None: <IF_STMT> if pre_node is None: self._table[hash_] = node_.next else: pre_node.next = node_.next self._len -= 1 pre_node = node_ node_ = node_.next",if node_.key == key:,if node_.key == key:,0.7982286793170927,0.8592377270804451,True
1176,"def _recurse(self, base_path, rel_source, rel_zip): submodules_path = Path(base_path) / 'submodules' if not submodules_path.is_dir(): return for submodule in submodules_path.iterdir(): source_path = submodule / rel_source <IF_STMT> continue output_path = submodule / rel_zip self._build_lambdas(source_path, output_path) self._recurse(submodule, rel_source, rel_zip)",if not source_path.is_dir():,if not source_path.exists():,0.9149846992856718,0.8713933650206428,False
1177,"def find_test_functions(collections): if not isinstance(collections, list): collections = [collections] functions = [] for collection in collections: if not isinstance(collection, dict): collection = vars(collection) keys = collection.keys() keys.sort() for key in keys: value = collection[key] <IF_STMT> functions.append(value) return functions","if isinstance(value, types.FunctionType) and hasattr(value, 'unittest'):","if isinstance(value, TestFunction):",0.8834788605257707,0.8815741981066073,False
1178,"def __init__(self, classifier, layer_name=None, transpose=None, distance=None, copy_weights=True): super().__init__() self.copy_weights = copy_weights if layer_name is not None: self.set_weights(getattr(classifier, layer_name)) else: for x in self.possible_layer_names: layer = getattr(classifier, x, None) <IF_STMT> self.set_weights(layer) break self.distance = classifier.distance if distance is None else distance self.transpose = transpose",if layer is not None:,if layer is not None:,0.659744658187601,0.8559898693114286,True
1179,def multi_dev_generator(self): for data in self._data_loader(): if len(self._tail_data) < self._base_number: self._tail_data += data <IF_STMT> yield self._tail_data self._tail_data = [],if len(self._tail_data) == self._base_number:,if len(self._tail_data) == self._base_number:,0.5746297272178537,0.7245511487202049,True
1180,"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) if answer == Gtk.ResponseType.YES: value = setting.Value.split('|') value.remove(edge) setting.Value = '|'.join(value) if updater: updater.UpdateSetting(setting) <IF_STMT> return False return True",if answer == Gtk.ResponseType.NO:,elif answer == Gtk.ResponseType.NO:,0.9135521428179874,0.8385130047130208,False
1181,"def _post_process_ttl(zone): for name in zone: for record_type in zone[name]: records = zone[name][record_type] if isinstance(records, list): ttl = min([x['ttl'] for x in records]) for record in records: <IF_STMT> logger.warning('Using lowest TTL {} for the record set. Ignoring value {}'.format(ttl, record['ttl'])) record['ttl'] = ttl",if record['ttl'] != ttl:,if ttl < record['ttl']:,0.8441969453440955,0.8752376177722327,False
1182,"def __init__(self, cmds, env, cleanup=[]): self.handle = None self.cmds = cmds self.env = env if cleanup: <IF_STMT> cleanup = [cleanup] else: try: cleanup = [c for c in cleanup if callable(c)] except: cleanup = [] self.cleanup = cleanup",if callable(cleanup):,"if not isinstance(cleanup, list):",0.5913484612104037,0.8592377270804451,False
1183,"def _parse_data_of_birth(cls, data_of_birth_string): if data_of_birth_string: format = '%m/%d/%Y' try: parsed_date = datetime.datetime.strptime(data_of_birth_string, format) return parsed_date except ValueError: <IF_STMT> raise",if data_of_birth_string.count('/') != 1:,if not datetime.datetime.isnan(data_of_birth_string):,0.883976012340697,0.8055344092731546,False
1184,"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == 'QTCORE': continue value = env['LIBPATH_' + var] if value: core = env[coreval] accu = [] for lib in value: <IF_STMT> continue accu.append(lib) env['LIBPATH_' + var] = accu",if lib in core:,if lib not in core:,0.8732972103944772,0.845713978670975,False
1185,"def throttle_status(server=None): result = AmonStruct() result.allow = False last_check = server.get('last_check') server_check_period = server.get('check_every', 60) if last_check: period_since_last_check = unix_utc_now() - last_check period_since_last_check = period_since_last_check + 15 <IF_STMT> result.allow = True else: result.allow = True return result",if period_since_last_check >= server_check_period:,if period_since_last_check > server_check_period:,0.6722027995903576,0.8555308664663046,False
1186,"def fetch_scatter_outputs(self, task): scatteroutputs = [] for var in task['body']: if var.startswith('call'): <IF_STMT> for output in self.tasks_dictionary[task['body'][var]['task']]['outputs']: scatteroutputs.append({'task': task['body'][var]['alias'], 'output': output[0]}) return scatteroutputs",if 'outputs' in self.tasks_dictionary[task['body'][var]['task']]:,if task['body'][var]['task']:,0.8952260412731787,0.839587623092576,False
1187,"def _add_constant_node(self, source_node): parent_ids = range(len(source_node.in_edges)) for idx in parent_ids: parent_node = self.tf_graph.get_node(source_node.in_edges[idx]) <IF_STMT> self._rename_Const(parent_node)",if parent_node.type == 'Const':,if parent_node is not None:,0.6538122088267706,0.6676160043354593,False
1188,"def enableCtrls(self): for data in self.storySettingsData: name = data['name'] if name in self.ctrls: <IF_STMT> set = self.getSetting(data['requires']) for i in self.ctrls[name]: i.Enable(set not in ['off', 'false', '0'])",if 'requires' in data:,if data['requires']:,0.9086684650186571,0.8645707301556367,False
1189,"def update_realtime(self, stdout='', stderr='', delete=False): wooey_cache = wooey_settings.WOOEY_REALTIME_CACHE if delete == False and wooey_cache is None: self.stdout = stdout self.stderr = stderr self.save() elif wooey_cache is not None: cache = django_cache[wooey_cache] <IF_STMT> cache.delete(self.get_realtime_key()) else: cache.set(self.get_realtime_key(), json.dumps({'stdout': stdout, 'stderr': stderr}))",if delete:,if delete:,0.6752567567056791,0.9076141716697395,True
1190,"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x['description'] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in(('metadata', 'batch'), x) if batches: if not isinstance(batches, (list, tuple)): batches = [batches] for batch in batches: <IF_STMT> dups.add(batch) if len(dups) > 0: raise ValueError('Batch names must be unique from sample descriptions.\nClashing batch names: %s' % sorted(list(dups)))",if batch in names:,if batch not in names:,0.9487431557431583,0.9032199913269796,False
1191,"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 <IF_STMT> self.split = self.save elif self.split < 0.5: self.split = self.min else: self.split = self.max self.placeChilds()",if self.split <= self.min or self.split >= self.max:,elif self.save > 0.5:,0.7020429113115485,0.8923575006167597,False
1192,"def can_read(self): if hasattr(self.file, '__iter__'): iterator = iter(self.file) head = next(iterator, None) <IF_STMT> self.repaired = [] return True if isinstance(head, str): self.repaired = itertools.chain([head], iterator) return True else: raise IOSourceError('Could not open source: %r (mode: %r)' % (self.file, self.options['mode'])) return False",if head is None:,if head is None:,0.7759669174203491,0.8692960007731574,True
1193,"def _print_message_content(self, peer, data): inheaders = 1 lines = data.splitlines() for line in lines: if inheaders and (not line): peerheader = 'X-Peer: ' + peer[0] <IF_STMT> peerheader = repr(peerheader.encode('utf-8')) print(peerheader) inheaders = 0 if not isinstance(data, str): line = repr(line) print(line)","if not isinstance(data, str):","if not isinstance(peerheader, str):",0.7610244703646938,0.8692960007731574,False
1194,"def connect(self): try: <IF_STMT> connection = pymysql.connect(read_default_file='/etc/mysql/conf.d/my.cnf') else: connection = pymysql.connect(read_default_file='~/.my.cnf') return connection except ValueError as e: Log.debug(self, str(e)) raise MySQLConnectionError except pymysql.err.InternalError as e: Log.debug(self, str(e)) raise MySQLConnectionError",if os.path.exists('/etc/mysql/conf.d/my.cnf'):,"if os.path.isfile(os.path.join(self.config.dir, 'mysql.conf')):",0.7084142563116962,0.8446593249975184,False
1195,"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) if dest.exists(): logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() <IF_STMT> shutil.copy(src, dest)",if src.exists():,if os.path.isfile(src):,0.9573865361604404,0.9188912094899004,False
1196,"def update(self, x, who=None, metadata=None): self._retain_refs(metadata) y = self._get_key(x) if self.keep == 'last': self._buffer.pop(y, None) self._metadata_buffer.pop(y, None) self._buffer[y] = x self._metadata_buffer[y] = metadata el<IF_STMT> self._buffer[y] = x self._metadata_buffer[y] = metadata return self.last",if y not in self._buffer:,if self.keep == 'first':,0.8957471161273486,0.8336104423443033,False
1197,"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k['c7n:match-type'] == 'credential': c_date = parse_date(k['last_rotated']) for ak in keys: if c_date == ak['CreateDate']: ak = dict(ak) ak['c7n:match-type'] = 'access' <IF_STMT> res.append(ak) elif k not in res: res.append(k) return res",if ak not in res:,if ak['c7n:match-type'] == 'access':,0.8151768741303057,0.8692960007731574,False
1198,"def _apply_flag_attrs(src_flag, dest_flag): baseline_flag = FlagDef('', {}, None) for name in dir(src_flag): if name[:1] == '_': continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) <IF_STMT> setattr(dest_flag, name, getattr(src_flag, name))",if dest_val == baseline_val:,if dest_val != baseline_val:,0.801132235723655,0.8336104423443033,False
1199,"def _ws_keep_reading(self): import websockets.exceptions while not self._reader_stopped: try: data = await self._ws.recv() <IF_STMT> data = data.encode('UTF-8') if len(data) == 0: self._error = 'EOF' break except websockets.exceptions.ConnectionClosedError: self._error = 'EOF' break self.num_bytes_received += len(data) self._make_output_available(data, block=False)","if isinstance(data, str):","if isinstance(data, bytes):",0.730092100367418,0.8713933650206428,False
1200,"def to_dict(self) -> Dict[str, Any]: result = {} for field_name in self.API_FIELDS: <IF_STMT> result['stream_id'] = self.id continue elif field_name == 'date_created': result['date_created'] = datetime_to_timestamp(self.date_created) continue result[field_name] = getattr(self, field_name) result['is_announcement_only'] = self.stream_post_policy == Stream.STREAM_POST_POLICY_ADMINS return result",if field_name == 'id':,if field_name == 'stream_id':,0.7385700387937664,0.8516228624291206,False
1201,"def all_masks(cls, images, run, run_key, step): all_mask_groups = [] for image in images: <IF_STMT> mask_group = {} for k in image._masks: mask = image._masks[k] mask_group[k] = mask.to_json(run) all_mask_groups.append(mask_group) else: all_mask_groups.append(None) if all_mask_groups and (not all((x is None for x in all_mask_groups))): return all_mask_groups else: return False",if image._masks:,if image.key == run_key:,0.9264409475859066,0.8832000938217648,False
1202,"def disconnect_all(listener): """"""Disconnect from all signals"""""" for emitter in listener._signal_data.emitters: for signal in emitter._signal_data.listeners: emitter._signal_data.listeners[signal] = [i for i in emitter._signal_data.listeners[signal] <IF_STMT>]","if getattr(i, '__self__', None) != listener",if i.signal_type == signal.EVENT_DISCONNECTED,0.7905175477048674,0.829360614494932,False
1203,"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) <IF_STMT> code = -signal.SIGTERM self.returncode = code return self.returncode",if code == TERMINATE:,if code == _subprocess.WAIT_OBJECT_1:,0.8545171601705489,0.8752376177722327,False
1204,"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _('Processing...')) self.pbar.set_text(progress) <IF_STMT> frac = 1.0 if frac < 0: frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",if frac > 1:,if frac > 1.0:,0.6294493583971884,0.8105932471967202,False
1205,def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == '[': m = -1 aas.append('') <IF_STMT> m = 0 continue elif m == -1: aas[-1] = aas[-1] + i elif m == 0: aas.append(i) return aas,elif i == ']':,elif i == ']':,0.816954274917487,0.8692960007731574,True
1206,"def link(token, base_url): """"""Validation for ``link``."""""" if get_keyword(token) == 'none': return 'none' parsed_url = get_url(token, base_url) if parsed_url: return parsed_url function = parse_function(token) if function: name, args = function prototype = (name, [a.type for a in args]) args = [getattr(a, 'value', a) for a in args] <IF_STMT> return ('attr()', args[0])","if prototype == ('attr', ['ident']):",if prototype == 'attr':,0.8798458386183231,0.8923575006167597,False
1207,"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) <IF_STMT> return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",elif self.current_provider.kind == directory.Provider.PROVIDER_URL:,elif self.current_provider.kind == directory.Provider.PROVIDER_URL:,0.8995349724990576,0.8169276475307028,True
1208,"def test_handle_single(self): self.skipTest(""Pops up windows and needs user input.. so disabled.Still worth keeping whilst we don't have unit tests for all plugins."") for id_, plugin in self.plugins.items(): <IF_STMT> self.h.plugin_enable(plugin, None) self.h.handle(id_, self.lib, self.parent, SONGS) self.h.plugin_disable(plugin)",if self.h.plugin_handle(plugin):,if self.h.plugin_is_enabled(plugin):,0.7339858183372172,0.8966773400768917,False
1209,"def __repr__(self): attrs = [] for k in self._keydata: <IF_STMT> attrs.append('p(%d)' % (self.size() + 1,)) elif hasattr(self, k): attrs.append(k) if self.has_private(): attrs.append('private') return '<%s @0x%x %s>' % (self.__class__.__name__, id(self), ','.join(attrs))",if k == 'p':,if k == 'size':,0.9094672621356864,0.8228500218338367,False
1210,"def apply(self, node, code, required): yield 'try:' yield from self.iterIndented(code) yield 'pass' yield 'except {}:'.format(self.exceptionString) outputVariables = node.getOutputSocketVariables() for i, s in enumerate(node.outputs): <IF_STMT> if hasattr(s, 'getDefaultValueCode'): yield f'{outputVariables[s.identifier]} = {s.getDefaultValueCode()}' else: yield f'{outputVariables[s.identifier]} = self.outputs[{i}].getDefaultValue()' yield 'pass'",if s.identifier in required:,if s.identifier in self.outputs:,0.9231622119742149,0.8592377270804451,False
1211,"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: if '*' in fromlist: fromlist = list(fromlist) fromlist.remove('*') fromlist.extend(getattr(module, '__all__', [])) for x in fromlist: <IF_STMT> from_name = '{}.{}'.format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module","if isinstance(getattr(module, x, None), types.ModuleType):",if x != '__all__':,0.9396902560814938,0.8832000938217648,False
1212,"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() <IF_STMT> r = r.decode('utf-8') msg = json.loads(r) stream = msg.get('stream') if stream is not None: await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())","if isinstance(r, bytes):","if isinstance(r, bytes):",0.8121694946043359,0.8815741981066073,True
1213,"def add_source(self, source, name=None): """"""Adds a new data source to an existing provider."""""" if self.randomize: <IF_STMT> raise ValueError('Cannot add a non-shuffleable source to an already shuffled provider.') super().add_source(source, name=name) if self.randomize is True: self._shuffle_len = self.entries",if not source.can_shuffle():,if source in self.entries:,0.8905839983928693,0.8516228624291206,False
1214,def __str__(self): buf = [''] if self.fileName: buf.append(self.fileName + ':') if self.line != -1: if not self.fileName: buf.append('line ') buf.append(str(self.line)) <IF_STMT> buf.append(':' + str(self.column)) buf.append(':') buf.append(' ') return str('').join(buf),if self.column != -1:,if not self.column:,0.8816046027509115,0.8446593249975184,False
1215,"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): if not line: return True if linenum > 0 and line[0] not in '\t ': return True if _has_newline(line): return True <IF_STMT> return True return False",if len(line.strip()) == 0:,if line[0] in '\n':,0.9009729716699493,0.8944264839442453,False
1216,"def scanHexEscape(self, prefix): code = 0 leng = 4 if prefix == 'u' else 2 for i in xrange(leng): <IF_STMT> ch = self.source[self.index] self.index += 1 code = code * 16 + HEX_CONV[ch] else: return '' return unichr(code)",if self.index < self.length and isHexDigit(self.source[self.index]):,if self.source[self.index] == prefix:,0.8863042941784218,0.8592377270804451,False
1217,"def _get_table_info(self, table_name): table_addr = self.addr_space.profile.get_symbol(table_name) table_size = self._get_table_info_distorm() <IF_STMT> table_size = self._get_table_info_other(table_addr, table_name) if table_size == 0: debug.error('Unable to get system call table size') return [table_addr, table_size]",if table_size == 0:,if table_size == 0:,0.8966428934490279,0.8105932471967202,True
1218,"def format_file_path(filepath): """"""Formats a path as absolute and with the correct platform separator."""""" try: is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN.match(filepath) filepath = os.path.realpath(os.path.abspath(filepath)) filepath = re.sub(BACKSLASH_REPLACE_PATTERN, '/', filepath) is_windows_drive = WINDOWS_DRIVE_PATTERN.match(filepath) <IF_STMT> filepath = filepath.capitalize() if is_windows_network_mount: filepath = '/' + filepath except: pass return filepath",if is_windows_drive:,if is_windows_drive:,0.9138764846064689,0.9164531641034833,True
1219,"def _match(self, cre, s): self.mo = cre.match(s) if __debug__: <IF_STMT> self._mesg(""\tmatched r'%r' => %r"" % (cre.pattern, self.mo.groups())) return self.mo is not None",if self.mo is not None and self.debug >= 5:,if self.mo:,0.8622077358995806,0.8318180062062374,False
1220,"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): if not line.startswith('#'): self.sanitize_allowlist.append(line.strip()) except OSError: <IF_STMT> log.warning(""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file)",if explicit:,if explicit:,0.7820999965126689,0.9076141716697395,True
1221,"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): if not self.flags.forc: raise RuntimeError('only contiguous arrays may be used as arguments to this operation') <IF_STMT> order = 'F' else: order = 'C' result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call(self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size) return result else: return self",if self.flags.f_contiguous:,if self.flags.forf:,0.6436982433140938,0.9253742688467129,False
1222,"def scan_spec_conf(self, conf): if 'metadata' in conf: if 'annotations' in conf['metadata'] and conf['metadata'].get('annotations'): for annotation in conf['metadata']['annotations']: for key in annotation: <IF_STMT> if 'docker/default' in annotation[key] or 'runtime/default' in annotation[key]: return CheckResult.PASSED return CheckResult.FAILED",if 'seccomp.security.alpha.kubernetes.io/defaultProfileName' in key:,if key.startswith('docker'):,0.6832228408386901,0.8935248372106969,False
1223,"def test_error_through_destructor(self): rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy if not IOBASE_EMITS_UNRAISABLE: self.assertIsNone(cm.unraisable) <IF_STMT> self.assertEqual(cm.unraisable.exc_type, OSError)",elif cm.unraisable is not None:,"if not isinstance(cm.unraisable, Exception):",0.8827879907597896,0.7391959451349216,False
1224,"def _dumpf(frame): if frame is None: return '<None>' else: addn = '(with trace!)' <IF_STMT> addn = ' **No Trace Set **' return 'Frame at %d, file %s, line: %d%s' % (id(frame), frame.f_code.co_filename, frame.f_lineno, addn)",if frame.f_trace is None:,if frame.f_code.co_filename == 'No Trace Set':,0.8721039495267897,0.7981256013410023,False
1225,"def containsBadbytes(self, value, bytecount=4): for b in self.badbytes: tmp = value <IF_STMT> b = ord(b) for i in range(bytecount): if tmp & 255 == b: return True tmp >>= 8 return False",if type(b) == str:,if tmp & 128 == 0:,0.6695614063025357,0.7865984197371234,False
1226,"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: <IF_STMT> peer.status = PEER_BAD elif peer.last_good > cutoff: peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",if peer.bad:,if peer.last_good < cutoff:,0.8496258659446179,0.8431339019329497,False
1227,"def afterTest(self, test): try: self.driver.quit() except AttributeError: pass except Exception: pass if self.options.headless: <IF_STMT> try: self.display.stop() except AttributeError: pass except Exception: pass",if self.headless_active:,if self.display:,0.5868300151027499,0.8318180062062374,False
1228,"def _written_variables_in_proxy(self, contract): variables = [] if contract.is_upgradeable: variables_name_written_in_proxy = self._variable_written_in_proxy() <IF_STMT> variables_in_contract = [contract.get_state_variable_from_name(v) for v in variables_name_written_in_proxy] variables_in_contract = [v for v in variables_in_contract if v] variables += variables_in_contract return list(set(variables))",if variables_name_written_in_proxy:,if variables_name_written_in_proxy:,0.7045335611890824,0.8901732118131125,True
1229,"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') if name.startswith(expr): <IF_STMT> found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",if name not in found_names:,if name not in found_names:,0.9230906012582839,0.8591169759078797,True
1230,"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): <IF_STMT> if not name.startswith('wait_until'): if is_resource_action(member): resource_methods[name] = member return resource_methods",if not name[0].isupper():,if is_resource_action(member):,0.870573226923419,0.8645707301556367,False
1231,def UpdateControlState(self): active = self.demoModules.GetActiveID() for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] <IF_STMT> btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) if moduleID == modModified: self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),if moduleID == active:,if active == moduleID:,0.8004006892822573,0.828399516355805,False
1232,"def test_controlcharacters(self): for i in range(128): c = chr(i) testString = 'string containing %s' % c if i >= 32 or c in '\r\n\t': data = plistlib.dumps(testString, fmt=plistlib.FMT_XML) <IF_STMT> self.assertEqual(plistlib.loads(data), testString) else: with self.assertRaises(ValueError): plistlib.dumps(testString, fmt=plistlib.FMT_XML) plistlib.dumps(testString, fmt=plistlib.FMT_BINARY)",if c != '\r':,if data:,0.6643384295981469,0.9051034981560222,False
1233,"def remove_usernames(self, username: SLT[str]) -> None: with self.__lock: <IF_STMT> raise RuntimeError(f""Can't set {self.username_name} in conjunction with (already set) {self.chat_id_name}s."") parsed_username = self._parse_username(username) self._usernames -= parsed_username",if self._chat_ids:,if self._usernames:,0.8795464625977154,0.8531413606256201,False
1234,"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize <IF_STMT> size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","elif isinstance(elem, int):","elif isinstance(elem, int):",0.9187987282534597,0.8928756684056034,True
1235,"def before_step(self, step, feed_dict): if step == 0: for _type, mem in self.memories.items(): <IF_STMT> self.gan.session.run(tf.assign(mem['var'], mem['source']))",if 'var' in mem and 'source' in mem:,if 'var' in mem and 'source' in mem:,0.5564589590125109,0.5636465749570341,True
1236,"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] <IF_STMT> if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if self.bitnum == 8:,if byte:,0.6155374729546124,0.9325718821645923,False
1237,"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for parameter_range_key, parameter_range_value in parameter_range.__dict__.items(): if parameter_range_key == 'scaling_type': continue <IF_STMT> for categorical_value in parameter_range_value: value_hp.validate(categorical_value) else: value_hp.validate(parameter_range_value)","if isinstance(parameter_range_value, list):","if isinstance(parameter_range_value, list):",0.8240366752116723,0.8120341702859789,True
1238,"def _trackA(self, tracks): try: track, start, end = self.featureA assert track in tracks return track except TypeError: for track in tracks: for feature_set in track.get_sets(): if hasattr(feature_set, 'features'): <IF_STMT> return track return None",if self.featureA in feature_set.features.values():,if feature_set.features[0].startswith(start):,0.9175356207746597,0.8901732118131125,False
1239,"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any((fnmatch(name, pattern) for pattern in basename_ignore)): continue path = path_so_far + '/' + name if path_so_far else name if any((fnmatch(path, pattern) for pattern in path_ignore)): continue full_name = os.path.join(directory, name) <IF_STMT> for file_path in walk(full_name, path): yield file_path elif os.path.isfile(full_name): yield path",if os.path.isdir(full_name):,if os.path.isdir(full_name):,0.6846258278655482,0.9284304001296656,True
1240,"def _poll_ipc_requests(self) -> None: try: <IF_STMT> return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: if os.path.isfile(filename): self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception('Problem processing ipc request', exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",if self._ipc_requests.empty():,if self.is_active():,0.9318083726167242,0.8935248372106969,False
1241,"def test_read1(self): self.test_write() blocks = [] nread = 0 with gzip.GzipFile(self.filename, 'r') as f: while True: d = f.read1() <IF_STMT> break blocks.append(d) nread += len(d) self.assertEqual(f.tell(), nread) self.assertEqual(b''.join(blocks), data1 * 50)",if not d:,if not d:,0.7184766122522377,0.8547305998833805,True
1242,"def _target_generator(self): if self._internal_target_generator is None: <IF_STMT> return None from ....model_zoo.rcnn.rpn.rpn_target import RPNTargetGenerator self._internal_target_generator = RPNTargetGenerator(num_sample=self._num_sample, pos_iou_thresh=self._pos_iou_thresh, neg_iou_thresh=self._neg_iou_thresh, pos_ratio=self._pos_ratio, stds=self._box_norm, **self._kwargs) return self._internal_target_generator else: return self._internal_target_generator",if self._net_none:,if self._num_sample == 0:,0.8823021495874958,0.7965020533851944,False
1243,"def time_left(self): """"""Return how many seconds are left until the timeout expires"""""" if self.is_non_blocking: return 0 elif self.is_infinite: return None else: delta = self.target_time - self.TIME() <IF_STMT> self.target_time = self.TIME() + self.duration return self.duration else: return max(0, delta)",if delta > self.duration:,if delta < 0:,0.7021917805836936,0.8592377270804451,False
1244,"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != '__init__': if not private and name.startswith('_'): continue <IF_STMT> continue setattr(cls, name, decorator(meth)) return cls",if name in butnot:,if not meth:,0.9063220266985611,0.8547305998833805,False
1245,"def load_vocab(vocab_file: str) -> List: """"""Loads a vocabulary file into a dictionary."""""" vocab = collections.OrderedDict() with io.open(vocab_file, 'r', encoding='UTF-8') as file: for num, line in enumerate(file): items = convert_to_unicode(line.strip()).split('\t') <IF_STMT> break token = items[0] index = items[1] if len(items) == 2 else num token = token.strip() vocab[token] = int(index) return vocab",if len(items) > 2:,if len(items) == 1:,0.7002220900028155,0.8944264839442453,False
1246,"def slice_fill(self, slice_): """"""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"""""" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): <IF_STMT> new_slice_.append(0) elif offset < len(slice_): new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",if self.squeeze_dims[i]:,if i == 0:,0.9356260947905131,0.9001816649635144,False
1247,"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) if remote_version > local_version: <IF_STMT> update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",if auto:,if auto:,0.5975600822405508,0.8743414417652072,True
1248,"def iter_content(self, chunk_size_bytes): while True: try: data = self._fp.read(chunk_size_bytes) except IOError as e: raise Fetcher.PermanentError('Problem reading chunk from {}: {}'.format(self._fp.name, e)) <IF_STMT> break yield data",if not data:,if not data:,0.855994834857934,0.8196189957582152,True
1249,"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: if isinstance(arg, bool): gvariant += ' {}'.format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f' {arg}' <IF_STMT> gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","elif isinstance(arg, str):","elif isinstance(arg, str):",0.8261474743807432,0.8953711787948615,True
1250,"def _element_keywords(cls, backend, elements=None): """"""Returns a dictionary of element names to allowed keywords"""""" if backend not in Store.loaded_backends(): return {} mapping = {} backend_options = Store.options(backend) elements = elements if elements is not None else backend_options.keys() for element in elements: <IF_STMT> continue element = element if isinstance(element, tuple) else (element,) element_keywords = [] options = backend_options['.'.join(element)] for group in Options._option_groups: element_keywords.extend(options[group].allowed_keywords) mapping[element[0]] = element_keywords return mapping",if '.' in element:,if not element:,0.9445644731439218,0.9318557434050335,False
1251,"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print('V', value) <IF_STMT> param_node.selected_mode = 'int' param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = 'float' param_node.float_ = value","if isinstance(value, int):","if isinstance(value, int):",0.6701894759820669,0.8592899528284996,True
1252,"def _get_oshape(indices_shape, depth, axis): oshape = [] true_axis = len(indices_shape) if axis == -1 else axis ndim = len(indices_shape) + 1 indices_index = 0 for i in range(0, ndim): <IF_STMT> oshape.append(depth) else: oshape.append(indices_shape[indices_index]) indices_index += 1 return oshape",if i == true_axis:,if indices_index >= true_axis:,0.9046268494887921,0.8592377270804451,False
1253,"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split('_', 1)[1]): <IF_STMT> value = value[len(prefix):] value = value.lstrip('_') if hasattr(self.group, value): return getattr(self.group, value) else: raise ValueError('No such constant: %s_%s' % (self.prefix, value)) else: return value",if value.startswith(prefix):,if value.startswith(prefix):,0.937403662376129,0.9184043388013005,True
1254,"def shuffle_unison_inplace(list_of_lists, random_state=None): if list_of_lists: assert all((len(l) == len(list_of_lists[0]) for l in list_of_lists)) <IF_STMT> random_state.permutation(len(list_of_lists[0])) else: p = np.random.permutation(len(list_of_lists[0])) return [l[p] for l in list_of_lists] return None",if random_state is not None:,if random_state:,0.8333434343511066,0.8645707301556367,False
1255,"def _load_module(self): spec = self.default_module_spec module_identifier = self.module_identifier if module_identifier: impls = self.get_module_implementation_map() <IF_STMT> raise ModuleNotFound('Invalid module identifier %r in %s' % (module_identifier, force_ascii(repr(self)))) spec = impls[module_identifier] cls = load(spec, context_explanation='Loading module for %s' % force_ascii(repr(self))) options = getattr(self, self.module_options_field, None) or {} return cls(self, options)",if module_identifier not in impls:,if module_identifier not in impls:,0.7481707520422884,0.8649799950178215,True
1256,"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): <IF_STMT> if isinstance(request, numbers.Integral): data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)",if shapes[i] is not None:,if i in shapes:,0.7996748258941563,0.8592377270804451,False
1257,"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k['c7n:match-type'] == 'credential': c_date = parse_date(k['last_rotated']) for ak in keys: <IF_STMT> ak = dict(ak) ak['c7n:match-type'] = 'access' if ak not in res: res.append(ak) elif k not in res: res.append(k) return res",if c_date == ak['CreateDate']:,if c_date.tzinfo() < c_date.tzinfo():,0.9327123246230433,0.8723360571509826,False
1258,"def _is_legacy_mode(self, node): """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode."""""" script_mode = False py_version = 'py2' for kw in node.keywords: <IF_STMT> script_mode = bool(kw.value.value) if isinstance(kw.value, ast.NameConstant) else True if kw.arg == 'py_version': py_version = kw.value.s if isinstance(kw.value, ast.Str) else 'py3' return not (py_version.startswith('py3') or script_mode)",if kw.arg == 'script_mode':,if kw.arg == 'script_mode':,0.7503380623345934,0.8902056737869248,True
1259,"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue <IF_STMT> continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) if status: statuses_by_refs[entity_ref].append(status) return statuses_by_refs",if entity_ref not in statuses_by_refs:,if entity_ref in statuses_by_refs:,0.8086014623301705,0.8806615362338783,False
1260,"def items(self): dict = {} for userdir in self.XDG_DIRS.keys(): prefix = self.get(userdir).strip('""').split('/')[0] <IF_STMT> path = os.getenv('HOME') + '/' + '/'.join(self.get(userdir).strip('""').split('/')[1:]) else: path = self.get(userdir).strip('""') dict[userdir] = path return dict.items()",if prefix:,if prefix == 'HOME':,0.6893673697897895,0.8169276475307028,False
1261,"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: <IF_STMT> prefix_words_are_adj = False if prefix_words_are_adj: return (words[-1:], words[:-1]) else: return ([string], []) else: return ([string], [])",if att not in common_attributes:,if att in common_attributes:,0.9111036500598915,0.8723360571509826,False
1262,"def extract_custom(extractor, *args, **kw): for match in extractor(*args, **kw): msg = match[2] <IF_STMT> unused = '<unused singular (hash=%s)>' % md5(msg[1].encode('utf8')).hexdigest() msg = (unused, msg[1], msg[2]) match = (match[0], match[1], msg, match[3]) yield match","if isinstance(msg, tuple) and msg[0] == '':",if len(msg) == 3:,0.703794248387534,0.8385130047130208,False
1263,"def test_convex_decomposition(self): mesh = g.get_mesh('quadknot.obj') engines = [('vhacd', g.trimesh.interfaces.vhacd.exists)] for engine, exists in engines: <IF_STMT> g.log.warning('skipping convex decomposition engine %s', engine) continue g.log.info('Testing convex decomposition with engine %s', engine) meshes = mesh.convex_decomposition(engine=engine) self.assertTrue(len(meshes) > 1) for m in meshes: self.assertTrue(m.is_watertight) g.log.info('convex decomposition succeeded with %s', engine)",if not exists:,if not exists:,0.8280564555172054,0.9022045190074797,True
1264,"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(' , ') else: hasConst = not (self._const.__class__ in native_numeric_types and self._const == 0) if hasConst: idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ <IF_STMT> ostream.write(' - ') else: ostream.write(' + ')",if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0):,if _lt == 0:,0.661282973275094,0.8661072626070159,False
1265,"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] <IF_STMT> return tuple(data) else: return data else: return None",if is_tuple:,if is_tuple:,0.9272639726760018,0.9425437476131634,True
1266,"def process_error(self, data): if data.get('error'): <IF_STMT> raise AuthCanceled(self, data.get('error_description', '')) raise AuthFailed(self, data.get('error_description') or data['error']) elif 'denied' in data: raise AuthCanceled(self, data['denied'])",if 'denied' in data['error'] or 'cancelled' in data['error']:,if 'cancelled' in data:,0.862246146283451,0.760856626273165,False
1267,"def retry_http_digest_auth(self, req, auth): token, challenge = auth.split(' ', 1) chal = parse_keqv_list(parse_http_list(challenge)) auth = self.get_authorization(req, chal) if auth: auth_val = 'Digest %s' % auth <IF_STMT> return None req.add_unredirected_header(self.auth_header, auth_val) resp = self.parent.open(req) return resp","if req.headers.get(self.auth_header, None) == auth_val:","if req.headers.get(self.auth_header, None) == auth_val:",0.7606189450327198,0.8248765135255685,True
1268,"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() if type(sockname) is str: <IF_STMT> os.remove(sockname) self.sock = None",if os.path.exists(sockname):,if os.path.exists(sockname):,0.9125249164137686,0.8590888738245122,True
1269,"def to_nurbs(self, curves): result = [] for i, c in enumerate(curves): nurbs = SvNurbsCurve.to_nurbs(c) <IF_STMT> raise Exception(f'Curve #{i} - {c} - can not be converted to NURBS!') result.append(nurbs) return result",if nurbs is None:,if nurbs is None:,0.7236596057397946,0.8228500218338367,True
1270,"def handle_1_roomid_raffle(self, i): if i[1] in ['handle_1_room_TV', 'handle_1_room_captain']: <IF_STMT> await self.notify('post_watching_history', i[0]) await self.notify(i[1], i[0], i[2]) else: print('hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh', i)","if await self.notify('check_if_normal_room', i[0], -1):","if i[0] in ['post_watching_history', 'post_watching_history_captain']:",0.5756829571204167,0.6907573115737006,False
1271,"def init_ps_var_partition(self): ps_vars = {} for v in self._non_embed_vars.values(): if v.name not in self._var_to_ps: self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num) ps_id = self._var_to_ps[v.name] <IF_STMT> ps_vars[ps_id] = [v] else: ps_vars[ps_id].append(v) self._ps_vars = ps_vars",if ps_id not in ps_vars:,if ps_id not in ps_vars:,0.8234440216074661,0.7975010608178975,True
1272,"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if 'meta-environment' in root or 'cross-canadian' in root: continue if 'qemux86copy-' in root or 'qemux86-' in root: continue <IF_STMT> f.append(os.path.join(root, name)) return f",if 'do_build' not in name and 'do_populate_sdk' not in name:,if name.endswith('.py'):,0.9319584624158577,0.9051034981560222,False
1273,"def setSelectedLabelState(self, p): c = self.c if p and c.edit_widget(p): <IF_STMT> g.trace(self.trace_n, c.edit_widget(p), p) self.trace_n += 1 self.setDisabledHeadlineColors(p)",if 0:,if self.trace_n < self.trace_len:,0.794787698145897,0.7098232254187811,False
1274,"def filter_tasks(self, task_types=None, task_states=None, task_text=None): tasks = self.api.tasks(self.id).get('tasks', {}) if tasks and tasks.get('task'): return [Task(self, task) for task in tasks.get('task', []) if (not task_types or task['type'].lower() in task_types) and (not task_states or task['state'].lower() in task_states) and (not task_text or task_text.lower() in str(task).lower())] else: return []",if not task_types or task['type'].lower() in task_types,if task['type'].lower() in task_types,1.0,1.0,False
1275,"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): <IF_STMT> if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if vector:,if item not in vector:,0.8884951999424376,0.8954283587198554,False
1276,def _transmit_from_storage(self) -> None: for blob in self.storage.gets(): if blob.lease(self._timeout + 5): envelopes = [TelemetryItem(**x) for x in blob.get()] result = self._transmit(list(envelopes)) <IF_STMT> blob.lease(1) else: blob.delete(),if result == ExportResult.FAILED_RETRYABLE:,if result:,0.6509324985066939,0.8590888738245122,False
1277,"def load_dictionary(file): oui = {} with open(file, 'r') as f: for line in f: <IF_STMT> data = line.split('(hex)') key = data[0].replace('-', ':').lower().strip() company = data[1].strip() oui[key] = company return oui",if '(hex)' in line:,if line.startswith('#'):,0.7569761004764006,0.8787142254774354,False
1278,"def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True): indices = np.arange(data_ary.shape[0]) if shuffle: indices = rgen.permutation(indices) if n_batches > 1: remainder = data_ary.shape[0] % n_batches <IF_STMT> minis = np.array_split(indices[:-remainder], n_batches) minis[-1] = np.concatenate((minis[-1], indices[-remainder:]), axis=0) else: minis = np.array_split(indices, n_batches) else: minis = (indices,) for idx_batch in minis: yield idx_batch",if remainder:,if remainder > 0:,0.6869055287004886,0.8879659171421962,False
1279,"def canonical_custom_headers(self, headers): hoi = [] custom_headers = {} for key in headers: lk = key.lower() if headers[key] is not None: <IF_STMT> custom_headers[lk] = ','.join((v.strip() for v in headers.get_all(key))) sorted_header_keys = sorted(custom_headers.keys()) for key in sorted_header_keys: hoi.append('%s:%s' % (key, custom_headers[key])) return '\n'.join(hoi)",if lk.startswith('x-amz-'):,if lk in custom_headers:,0.9348798738361153,0.8723360571509826,False
1280,"def validate(self, data): if not data.get('reason'): message = data.get('message') if not message: if 'message' not in data: msg = serializers.Field.default_error_messages['required'] <IF_STMT> msg = serializers.Field.default_error_messages['null'] else: msg = serializers.CharField.default_error_messages['blank'] raise serializers.ValidationError({'message': [msg]}) return data",elif message is None:,elif 'null' in data:,0.6668885682972701,0.8385130047130208,False
1281,def tearDown(self): try: os.chdir(self.cwd) <IF_STMT> os.remove(self.pythonexe) test_support.rmtree(self.parent_dir) finally: BaseTestCase.tearDown(self),if self.pythonexe != sys.executable:,if self.pythonexe:,0.4285492433230673,0.5253819788848316,False
1282,"def update(self, value, label): if self._disabled: return try: self._progress.value = value self._label.value = label <IF_STMT> self._displayed = True display_widget(self._widget) except Exception as e: self._disabled = True logger.exception(e) wandb.termwarn('Unable to render progress bar, see the user log for details')",if not self._displayed:,if self._displayed:,0.6773287633389172,0.9051034981560222,False
1283,"def GetBinaryOperationBinder(self, op): with self._lock: <IF_STMT> return self._binaryOperationBinders[op] b = runtime.SymplBinaryOperationBinder(op) self._binaryOperationBinders[op] = b return b",if self._binaryOperationBinders.ContainsKey(op):,if op in self._binaryOperationBinders:,0.7751789009840774,0.674945488826271,False
1284,"def apply(self, l, b, evaluation): """"""FromDigits[l_, b_]"""""" if l.get_head_name() == 'System`List': value = Integer(0) for leaf in l.leaves: value = Expression('Plus', Expression('Times', value, b), leaf) return value elif isinstance(l, String): value = FromDigits._parse_string(l.get_string_value(), b) <IF_STMT> evaluation.message('FromDigits', 'nlst') else: return value else: evaluation.message('FromDigits', 'nlst')",if value is None:,if value is None:,0.7770732499141386,0.8752376177722327,True
1285,"def hsconn_sender(self): while not self.stop_event.is_set(): try: request = self.send_queue.get(True, 6.0) if self.socket is not None: self.socket.sendall(request) <IF_STMT> self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",if self.send_queue is not None:,if self.send_queue.task_done:,0.8963123275395943,0.8466657105524215,False
1286,"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: <IF_STMT> return False elif not rline.endswith(eline): return False return True",if eline not in rline:,if not rline.startswith(eline):,0.947257936781017,0.9100365300271298,False
1287,"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): <IF_STMT> normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): if isinstance(m, nn.Conv2d): normal_init(m, std=0.001, bias=0)","if isinstance(m, nn.ConvTranspose2d):","if isinstance(m, nn.Conv1d):",0.8956191307508865,0.8390782502060267,False
1288,"def filter_rel_attrs(field_name, **rel_attrs): clean_dict = {} for k, v in rel_attrs.items(): <IF_STMT> splitted_key = k.split('__') key = '__'.join(splitted_key[1:]) clean_dict[key] = v else: clean_dict[k] = v return clean_dict",if k.startswith(field_name + '__'):,if field_name == '__':,0.828170528536526,0.8038019482772603,False
1289,"def cancel(self): with self._condition: <IF_STMT> self._squash(state_root=self._previous_state_hash, context_ids=[self._previous_context_id], persist=False, clean_up=True) self._cancelled = True self._condition.notify_all()",if not self._cancelled and (not self._final) and self._previous_context_id:,if not self._cancelled:,0.7873846252854022,0.6475445426291286,False
1290,"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): <IF_STMT> level_ref += len(levels) if not 0 <= level_ref < len(levels): raise PatsyError('specified level %r is out of range' % (level_ref,)) return level_ref raise PatsyError('specified level %r not found' % (level_ref,))",if level_ref < 0:,if level_ref < 0:,0.9163019271223048,0.8752376177722327,True
1291,"def parse_node(self, node, alias_map=None, conv=None): sql, params, unknown = self._parse(node, alias_map, conv) if unknown and conv and params: params = [conv.db_value(i) for i in params] if isinstance(node, Node): if node._negated: sql = 'NOT %s' % sql <IF_STMT> sql = ' '.join((sql, 'AS', node._alias)) if node._ordering: sql = ' '.join((sql, node._ordering)) return (sql, params)",if node._alias:,if node._alias:,0.6820798695540098,0.9325718821645923,True
1292,"def parse_object_id(_, values): if values: for key in values: <IF_STMT> val = values[key] if len(val) > 10: try: values[key] = utils.ObjectIdSilent(val) except: values[key] = None",if key.endswith('_id'):,"if key in ('id', 'name'):",0.7944368223864336,0.759907656827929,False
1293,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_max_rows(d.getVarInt32()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.896805147837443,0.7965020533851944,True
1294,"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if 'identifiers' in rule and rule['identifiers'] is not None: for i_type, i_value in rule['identifiers'].items(): <IF_STMT> if not checks.is_cce_value_valid('CCE-' + str(i_value)): return True return False",if i_type[0:3] == 'cce':,if i_type == 'CCE':,0.8058733310683894,0.828399516355805,False
1295,"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield (simple_colorize(fromdesc, 'description'), simple_colorize(todesc, 'description')) for i, line in enumerate(diffs): if line is None: <IF_STMT> yield (simple_colorize('---', 'separator'), simple_colorize('---', 'separator')) else: yield line",if i > 0:,if i == 0:,0.8957958524996009,0.8336104423443033,False
1296,"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if '%' not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'): template = DateEpoch(lineBeginOnly=key != 'EPOCH') <IF_STMT> template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template","elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'):","elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'):",0.7456706134096542,0.8665222382201849,True
1297,"def ref_max_pooling_2d(x, kernel, stride, ignore_border, pad): y = [] for xx in x.reshape((-1,) + x.shape[-3:]): <IF_STMT> xx = xx[np.newaxis] y += [refs.pooling_2d(xx, 'max', kernel, stride, pad, ignore_border)[np.newaxis]] y = np.vstack(y) if x.ndim == 2: y = np.squeeze(y, 1) return y.reshape(x.shape[:-3] + y.shape[1:])",if xx.ndim == 2:,if xx.ndim == 2:,0.9304056474542659,0.8723360571509826,True
1298,"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color('Miscellaneous Topics:', 'yellow')) for pp in PAGEPATHS: if not os.path.isdir(pp): continue content = os.listdir(pp) for pn in content: <IF_STMT> name = pn[:pn.index('.')] else: name = pn print(name)",if '.' in pn:,if pn.startswith('.'):,0.9108681130851786,0.8966773400768917,False
1299,"def justify_toggle_auto(self, event=None): c = self if c.editCommands.autojustify == 0: c.editCommands.autojustify = abs(c.config.getInt('autojustify') or 0) <IF_STMT> g.es('Autojustify on, @int autojustify == %s' % c.editCommands.autojustify) else: g.es('Set @int autojustify in @settings') else: c.editCommands.autojustify = 0 g.es('Autojustify off')",if c.editCommands.autojustify:,if c.editCommands.autojustify:,0.858311966606194,0.8996480074924822,True
1300,"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <IF_STMT> result.append(token.contents.replace('%', '%%')) elif token.token_type == TOKEN_VAR: result.append('%%(%s)s' % token.contents) vars.append(token.contents) return (''.join(result), vars)",if token.token_type == TOKEN_TEXT:,if token.token_type == TOKEN_VAR:,0.873614313975976,0.8038019482772603,False
1301,"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: if operation['type'] == 'crop': width = operation['right'] - operation['left'] height = operation['bottom'] - operation['top'] <IF_STMT> width = operation['width'] height = operation['height'] return (width, height)",if operation['type'] == 'resize':,elif operation['type'] == 'crop_rect':,0.8967208045489905,0.8431339019329497,False
1302,"def get_eval_matcher(self): if isinstance(self.data['match'], str): <IF_STMT> values = ['explicitDeny', 'implicitDeny'] else: values = ['allowed'] vf = ValueFilter({'type': 'value', 'key': 'EvalDecision', 'value': values, 'op': 'in'}) else: vf = ValueFilter(self.data['match']) vf.annotate = False return vf",if self.data['match'] == 'denied':,if self.data['match'] == 'explicitDeny':,0.8882936434268563,0.8385130047130208,False
1303,"def test_training(self): if not self.model_tester.is_training: return config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common() config.return_dict = True for model_class in self.all_model_classes: <IF_STMT> continue model = model_class(config) model.to(torch_device) model.train() inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True) loss = model(**inputs).loss loss.backward()",if model_class in MODEL_MAPPING.values():,if model_class is None:,0.8307482941030547,0.8385130047130208,False
1304,"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: <IF_STMT> reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == 'amd64': reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",if self.arch == 'i386':,if self.arch == 'i386':,0.8844118404945511,0.8474968231198384,True
1305,"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len('hello'), len(base64mime.encode('hello', eol=''))) for size in range(15): <IF_STMT> bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len('x' * size), bsize)",if size == 0:,if size <= 1:,0.8257025548018597,0.8964173245779284,False
1306,"def __new__(cls, dependencies): deps = check.list_param(dependencies, 'dependencies', of_type=DependencyDefinition) seen = {} for dep in deps: key = dep.solid + ':' + dep.output <IF_STMT> raise DagsterInvalidDefinitionError('Duplicate dependencies on solid ""{dep.solid}"" output ""{dep.output}"" used in the same MultiDependencyDefinition.'.format(dep=dep)) seen[key] = True return super(MultiDependencyDefinition, cls).__new__(cls, deps)",if key in seen:,if key in seen:,0.79349664309275,0.8752376177722327,True
1307,"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) if len(a) == 1: return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: <IF_STMT> raise pass elif self.strict > 1: raise PermError('Empty domain-spec on exp=') return None",if self.strict > 1:,if self.strict == 0:,0.9005787969197306,0.8474968231198384,False
1308,"def build(self): if self.args.get('sle_id'): self.process_sle_against_current_voucher() else: entries_to_fix = self.get_future_entries_to_fix() i = 0 while i < len(entries_to_fix): sle = entries_to_fix[i] i += 1 self.process_sle(sle) <IF_STMT> self.get_dependent_entries_to_fix(entries_to_fix, sle) if self.exceptions: self.raise_exceptions() self.update_bin()",if sle.dependant_sle_voucher_detail_no:,if self.args.get('dependent_entries'):,0.933332553594836,0.8787142254774354,False
1309,"def ValidateStopLatitude(self, problems): if self.stop_lat is not None: value = self.stop_lat try: if not isinstance(value, (float, int)): self.stop_lat = util.FloatStringToFloat(value, problems) except (ValueError, TypeError): problems.InvalidValue('stop_lat', value) del self.stop_lat else: <IF_STMT> problems.InvalidValue('stop_lat', value)",if self.stop_lat > 90 or self.stop_lat < -90:,if value < 0:,0.9159702570327608,0.8418243449361874,False
1310,"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" 'the ignore method' ignore = getattr(self, 'ignore') for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) <IF_STMT> for k1 in self.combinations[k]: if not hasattr(self, k1): setattr(self, k1, ignore)",if k in self.combinations:,if k in self.combinations:,0.7819603607761969,0.8692960007731574,True
1311,"def split(self, duration, include_remainder=True): duration = _seconds_or_timedelta(duration) if duration <= timedelta(seconds=0): raise ValueError('cannot call split with a non-positive timedelta') start = self.start while start < self.end: if start + duration <= self.end: yield MayaInterval(start, start + duration) <IF_STMT> yield MayaInterval(start, self.end) start += duration",elif include_remainder:,elif start + duration >= self.end:,0.9055128161377523,0.841020165317327,False
1312,"def get_first_field(layout, clz): for layout_object in layout.fields: if issubclass(layout_object.__class__, clz): return layout_object <IF_STMT> gf = get_first_field(layout_object, clz) if gf: return gf","elif hasattr(layout_object, 'get_field_names'):","elif issubclass(layout_object, BaseLayout):",0.8678468554237412,0.7848518349390632,False
1313,"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if '%' not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: <IF_STMT> template = DateEpoch(lineBeginOnly=key != 'EPOCH') elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'): template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template","if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'):","if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'):",0.7333575601059894,0.8665222382201849,True
1314,"def findOwningViewController(self, object): while object: <IF_STMT> description = fb.evaluateExpressionValue(object).GetObjectDescription() print('Found the owning view controller.\n{}'.format(description)) cmd = 'echo {} | tr -d ""\n"" | pbcopy'.format(object) os.system(cmd) return else: object = self.nextResponder(object) print('Could not find an owning view controller')",if self.isViewController(object):,if fb.evaluateExpressionValue(object).GetType() == 'ViewController':,0.8756277969924293,0.8555308664663046,False
1315,"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) <IF_STMT> return i idx = i else: idx += 1 return idx","if not isinstance(i, int):",if i < num:,0.7787800018944006,0.8592377270804451,False
1316,"def promtool(**kwargs): key = 'prometheus:promtool' try: path = pathlib.Path(util.setting(key)) except TypeError: yield checks.Warning('Missing setting for %s in %s ' % (key, settings.PROMGEN_CONFIG_FILE), id='promgen.W001') else: <IF_STMT> yield checks.Warning('Unable to execute file %s' % path, id='promgen.W003')","if not os.access(path, os.X_OK):",if not os.path.exists(path):,0.9212846951308222,0.8675979125638379,False
1317,"def parse_config(schema, config): schemaparser = ConfigParser() schemaparser.readfp(StringIO(schema)) cfgparser = ConfigParser() cfgparser.readfp(StringIO(config)) result = {} for section in cfgparser.sections(): result_section = {} schema = {} <IF_STMT> schema = dict(schemaparser.items(section)) for key, value in cfgparser.items(section): converter = converters[schema.get(key, 'string')] result_section[key] = converter(value) result[section] = result_section return result",if section in schemaparser.sections():,if section in schemaparser:,0.7633941631251787,0.8806615362338783,False
1318,"def validate_arguments(args): if args.num_pss < 1: print('Value error: must have ore than one parameter servers.') exit(1) if not GPU_IDS: num_cpus = multiprocessing.cpu_count() <IF_STMT> print('Value error: there are %s available CPUs but you are requiring %s.' % (num_cpus, args.cpu_trainers)) exit(1) if not os.path.isfile(args.file): print('Value error: model trainning file does not exist') exit(1)",if args.cpu_trainers > num_cpus:,if num_cpus != args.cpu_trainers:,0.9361910600372632,0.8944264839442453,False
1319,"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return 'raw' elif IndexedDataset.exists(path): with open(index_file_path(path), 'rb') as f: magic = f.read(8) if magic == IndexedDataset._HDR_MAGIC: return 'cached' <IF_STMT> return 'mmap' else: return None elif FastaDataset.exists(path): return 'fasta' else: return None",elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:,elif magic == IndexedDataset._MAGIC:,0.6417239432126861,0.8474968231198384,False
1320,"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if 'resourcegroup' not in [x.lower() for x in obj.keys()]: <IF_STMT> obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != 'sourceVault': _add_resource_group(obj[item_key])",if obj['id']:,if 'id' in obj.keys():,0.9299710026940873,0.8692960007731574,False
1321,"def reformatBody(self, event=None): """"""Reformat all paragraphs in the body."""""" c, p = (self, self.p) undoType = 'reformat-body' w = c.frame.body.wrapper c.undoer.beforeChangeGroup(p, undoType) w.setInsertPoint(0) while 1: progress = w.getInsertPoint() c.reformatParagraph(event, undoType=undoType) ins = w.getInsertPoint() s = w.getAllText() w.setInsertPoint(ins) <IF_STMT> break c.undoer.afterChangeGroup(p, undoType)",if ins <= progress or ins >= len(s):,if s == '':,0.8813892326712962,0.8692960007731574,False
1322,"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) <IF_STMT> content.append(project.readme.to_rst().path.name) path = project.package.path for fname in ('setup.cfg', 'setup.py'): if (path / fname).exists(): content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append('/'.join(fpath.parts)) return '\n'.join(content)",if project.readme.markup != 'rst':,if project.readme.to_rst:,0.9084804720124847,0.9122561819614461,False
1323,"def __init__(self, response): error = '{} {}'.format(response.status_code, response.reason) extra = [] try: response_json = response.json() <IF_STMT> error = ' '.join((error['message'] for error in response_json['error_list'])) extra = [error['extra'] for error in response_json['error_list'] if 'extra' in error] except JSONDecodeError: pass super().__init__(response=response, error=error, extra=extra)",if 'error_list' in response_json:,if 'error_list' in response_json:,0.8300934776593138,0.8692960007731574,True
1324,"def handle_event(self, fileno=None, events=None): if self._state == RUN: <IF_STMT> self._it = self._process_result(0) try: next(self._it) except (StopIteration, CoroStop): self._it = None",if self._it is None:,if self._it is None:,0.7350684089966301,0.7378351342269067,True
1325,"def find_query(self, needle, haystack): try: import pinyin haystack_py = pinyin.get_initial(haystack, '') needle_len = len(needle) start = 0 result = [] while True: found = haystack_py.find(needle, start) <IF_STMT> break result.append((found, needle_len)) start = found + needle_len return result except: return None",if found < 0:,if found == -1:,0.8037519089120233,0.8661072626070159,False
1326,"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if 'Last-Modified' not in rv.headers: try: result = date if callable(result): result = result(rv) if not isinstance(result, basestring): from werkzeug.http import http_date result = http_date(result) <IF_STMT> rv.headers['Last-Modified'] = result except Exception: logging.getLogger(__name__).exception('Error while calculating the lastmodified value for response {!r}'.format(rv)) return rv",if result:,if result:,0.9515385312627507,0.926934323706186,True
1327,"def check_require(require_modules, require_lines): for require_module in require_modules: st = try_import(require_module) if st == 0: continue <IF_STMT> print('installed {}: {}\n'.format(require_module, require_lines[require_module])) elif st == 2: print('failed installed {}: {}\n'.format(require_module, require_lines[require_module]))",elif st == 1:,elif st == 1:,0.6059316514862809,0.8169276475307028,True
1328,"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith('.'): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, '__init__.py')): self.bundle_package(itempath) <IF_STMT> self.bundle_module(itempath)",elif nm.endswith('.py'):,"elif os.path.exists(os.path.join(itempath, '__init__.py')):",0.9425898692775403,0.8938510701271204,False
1329,"def _find_root(): test_dirs = ['Src', 'Build', 'Package', 'Tests', 'Util'] root = os.getcwd() test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) while not test: last_root = root root = os.path.dirname(root) <IF_STMT> raise Exception('Root not found') test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) return root",if root == last_root:,if root == last_root:,0.9060935868004729,0.8780099567239787,True
1330,"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = (c.rootPosition(), [], []) while p: if p.v in seen: p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, '@ignore'): p.moveToNodeAfterTree() <IF_STMT> result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",elif p.h.startswith('@mark-for-unit-tests'):,"elif g.match_word(p.h, 0, '@mark-for-unit-test'):",0.9181220822318764,0.8692960007731574,False
1331,"def startTagFrameset(self, token): self.parser.parseError('unexpected-start-tag', {'name': 'frameset'}) if len(self.tree.openElements) == 1 or self.tree.openElements[1].name != 'body': assert self.parser.innerHTML elif not self.parser.framesetOK: pass else: <IF_STMT> self.tree.openElements[1].parent.removeChild(self.tree.openElements[1]) while self.tree.openElements[-1].name != 'html': self.tree.openElements.pop() self.tree.insertElement(token) self.parser.phase = self.parser.phases['inFrameset']",if self.tree.openElements[1].parent:,if self.tree.openElements[1].parent:,0.8056625503585778,0.8866029039778043,True
1332,"def try_split(self, split_text: List[str]): ret = [] for i in split_text: <IF_STMT> continue val = int(i, 2) if val > 255 or val < 0: return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f'binary successful, returning {ret.__repr__()}') return ret",if len(i) == 0:,if i == '':,0.9221229069802458,0.8692960007731574,False
1333,"def generator(self, data): for sock in data: <IF_STMT> offset = sock.obj_offset else: offset = sock.obj_vm.vtop(sock.obj_offset) yield (0, [Address(offset), int(sock.Pid), int(sock.LocalPort), int(sock.Protocol), str(protos.protos.get(sock.Protocol.v(), '-')), str(sock.LocalIpAddress), str(sock.CreateTime)])",if not self._config.PHYSICAL_OFFSET:,if sock.obj_vm is None:,0.8931657298746627,0.7886336751695258,False
1334,"def __init__(self, num_bits=4, always_apply=False, p=0.5): super(Posterize, self).__init__(always_apply, p) if isinstance(num_bits, (list, tuple)): <IF_STMT> self.num_bits = [to_tuple(i, 0) for i in num_bits] else: self.num_bits = to_tuple(num_bits, 0) else: self.num_bits = to_tuple(num_bits, num_bits)",if len(num_bits) == 3:,if len(num_bits) == 2:,0.7027612154114021,0.828399516355805,False
1335,"def tearDown(self): """"""Just in case yn00 creates some junk files, do a clean-up."""""" del_files = [self.out_file, '2YN.dN', '2YN.dS', '2YN.t', 'rst', 'rst1', 'rub'] for filename in del_files: <IF_STMT> os.remove(filename) if os.path.exists(self.working_dir): for filename in os.listdir(self.working_dir): filepath = os.path.join(self.working_dir, filename) os.remove(filepath) os.rmdir(self.working_dir)",if os.path.exists(filename):,if os.path.exists(filename):,0.9264111757826212,0.9099951253570094,True
1336,"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [(idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) <IF_STMT>] else: res = [(idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) if line.startswith(searchfor)] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return ''",if line.lstrip().startswith(searchfor.lstrip()),if line.startswith(searchfor),0.9276501912228148,0.9184043388013005,False
1337,"def ComboBoxDroppedHeightTest(windows): """"""Check if each combobox height is the same as the reference"""""" bugs = [] for win in windows: if not win.ref: continue <IF_STMT> continue if win.DroppedRect().height() != win.ref.DroppedRect().height(): bugs.append(([win], {}, testname, 0)) return bugs",if win.Class() != 'ComboBox' or win.ref.Class() != 'ComboBox':,if win.ref.isVisible():,0.8590745835668039,0.8996480074924822,False
1338,"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() <IF_STMT> if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if start < last_end:,if start < last_end:,0.8203239187038472,0.8832000938217648,True
1339,"def unpickle_from_file(file_path, gzip=False): """"""Unpickle obj from file_path with gzipping."""""" with tf.io.gfile.GFile(file_path, 'rb') as f: <IF_STMT> obj = pickle.load(f) else: with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf: obj = pickle.load(gzipf) return obj",if not gzip:,if gzip:,0.8140721349339689,0.8743414417652072,False
1340,"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, 'user', None) result = {'ip_address': request.META['REMOTE_ADDR']} if user and user.is_authenticated(): result.update({'email': user.email, 'id': user.id}) <IF_STMT> result['name'] = user.name else: result = {} return mark_safe(json.dumps(result))",if user.name:,elif user.is_staff():,0.8417231061154556,0.8901732118131125,False
1341,"def get_item_address(self, item): """"""Get an item's address as a collection of names"""""" result = [] while True: name = self.tree_ctrl.GetItemPyData(item) <IF_STMT> break else: result.insert(0, name) item = self.tree_ctrl.GetItemParent(item) return result",if name is None:,if name is None:,0.705609026405274,0.8228500218338367,True
1342,"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: <IF_STMT> dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen",if self.map[row][col] == UNSEEN:,if closest_unseen is None:,0.8962359152949058,0.8902056737869248,False
1343,"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: <IF_STMT> input_array = model.example_input_array if input_array is not None: input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn('Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given', UserWarning)",if input_array is None:,if input_array is None:,0.8653800767818007,0.8661072626070159,True
1344,"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: if not len(scene_exception) == 2: continue scene_name, scene_season = scene_exception.split('|') <IF_STMT> scene_exceptions.append(scene_name) return scene_exceptions",if season == scene_season:,if scene_season == season:,0.8044081441496199,0.7801270245332924,False
1345,def _clean_temp_files(): for pattern in _temp_files: for path in glob.glob(pattern): <IF_STMT> os.remove(path) else: shutil.rmtree(path),if os.path.islink(path) or os.path.isfile(path):,if os.path.isfile(path):,0.5871848207364566,0.7241577342575828,False
1346,"def wait_for_completion(self, job_id, offset, max_results, start_time, timeout): """"""Wait for job completion and return the first page."""""" while True: result = self.get_query_results(job_id=job_id, page_token=None, start_index=offset, max_results=max_results) <IF_STMT> return result if time.time() - start_time > timeout: raise Exception(""Timeout: the query doesn't finish within %d seconds."" % timeout) time.sleep(1)",if result['jobComplete']:,if result:,0.9409792853779001,0.9202663016973823,False
1347,"def get_data(self, element, ranges, style): <IF_STMT> groups = element.groupby(element.kdims).items() else: groups = [(element.label, element)] plots = [] axis = 'x' if self.invert_axes else 'y' for key, group in groups: if element.kdims: label = ','.join([d.pprint_value(v) for d, v in zip(element.kdims, key)]) else: label = key data = {axis: group.dimension_values(group.vdims[0]), 'name': label} plots.append(data) return plots",if element.kdims:,if element.groupby:,0.7828831019570501,0.9325718821645923,False
1348,"def get_files(self, dirname): if not self._data.has_key(dirname): self._create(dirname) else: new_time = self._changed(dirname) <IF_STMT> self._update(dirname, new_time) dcLog.debug('==> ' + '\t\n'.join(self._data[dirname]['flist'])) return self._data[dirname]['flist']",if new_time:,if new_time:,0.8039577331545625,0.8137489370974955,True
1349,"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): <IF_STMT> self.module_names.add(name[:-3]) elif '.' not in name: self.module_names.add(name)",if name.endswith('.py'):,if name.endswith('.py') and name.endswith('.py'):,0.828244883602079,0.7098232254187811,False
1350,"def logic(): for i in range(100): yield (clock.posedge, reset.negedge) <IF_STMT> count.next = 0 elif enable: count.next = (count + 1) % n raise StopSimulation",if reset == ACTIVE_LOW:,if reset == 0:,0.6037427569098037,0.7801270245332924,False
1351,"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): if k[:1] == '-': keynames[i] = k[1:] negate.add(k[1:])  def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): <IF_STMT> composite[i] = -v return composite return getit",if k in negate:,if k not in negate:,0.9183070490037594,0.8527204701689132,False
1352,"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: <IF_STMT> self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) if wnd_name in self.capture_mouse_windows: self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print('show_image: named_window ', wnd_name, ' not found.')",if self.named_windows[wnd_name] == 0:,if self.named_windows[wnd_name] == 0:,0.8792349111191979,0.8105932471967202,True
1353,def check_action_permitted(self): if self._action == 'sts:GetCallerIdentity': return True policies = self._access_key.collect_policies() permitted = False for policy in policies: iam_policy = IAMPolicy(policy) permission_result = iam_policy.is_action_permitted(self._action) if permission_result == PermissionResult.DENIED: self._raise_access_denied() <IF_STMT> permitted = True if not permitted: self._raise_access_denied(),elif permission_result == PermissionResult.PERMITTED:,elif permission_result == PermissionResult.PERMITTED:,0.8326923641166144,0.8555308664663046,True
1354,"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key]['inverse'] is True: <IF_STMT> value = datetime.now() - limit elif datetime.now() + limit < value: value = datetime.now() + limit elif value > limit: value = limit return value",if datetime.now() - limit > value:,if datetime.now() - limit > value:,0.9232229655470888,0.8443258653392445,True
1355,"def replace_dataset_ids(path, key, value): """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job."""""" current_case = input_values if key == 'id': for i, p in enumerate(path): if isinstance(current_case, (list, dict)): current_case = current_case[p] <IF_STMT> return (key, translate_values.get(current_case['id'], value)) return (key, value)",if src == current_case.get('src'):,if i == 0:,0.6742373797045049,0.8832000938217648,False
1356,"def load_ext(name, funcs): ExtModule = namedtuple('ExtModule', funcs) ext_list = [] lib_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) for fun in funcs: <IF_STMT> ext_list.append(extension.load(fun, name, lib_dir=lib_root).op) else: ext_list.append(extension.load(fun, name, lib_dir=lib_root).op_) return ExtModule(*ext_list)","if fun in ['nms', 'softnms']:",if name == fun:,0.890670528536526,0.8038019482772603,False
1357,"def execute_action(self): selected_actions = self.model_action.get_selected_results_with_index() if selected_actions and self.args_for_action: for name, _, act_idx in selected_actions: try: action = self.actions[act_idx] <IF_STMT> action.act([arg for arg, _, _ in self.args_for_action], self) except Exception as e: debug.log('execute_action', e)",if action:,if action.act_name == name:,0.6579361678453971,0.8431339019329497,False
1358,"def __getattr__(self, attr): proxy = self.__proxy if proxy and hasattr(proxy, attr): return getattr(proxy, attr) attrmap = self.__attrmap if attr in attrmap: source = attrmap[attr] <IF_STMT> value = source() else: value = _import_object(source) setattr(self, attr, value) self.__log.debug('loaded lazy attr %r: %r', attr, value) return value raise AttributeError(""'module' object has no attribute '%s'"" % (attr,))",if callable(source):,if callable(source):,0.9271264696938374,0.9325718821645923,True
1359,"def forward(self, x): x = x.unsqueeze(1) for conv in self.conv_layers: residual = x x = conv(x) <IF_STMT> tsz = x.size(2) r_tsz = residual.size(2) residual = residual[..., ::r_tsz // tsz][..., :tsz] x = (x + residual) * self.residual_scale if self.log_compression: x = x.abs() x = x + 1 x = x.log() return x",if self.skip_connections and x.size(1) == residual.size(1):,if self.use_tsz:,0.8177801006574824,0.9312457603037672,False
1360,"def __Prefix_Step2a(self, token): for prefix in self.__prefix_step2a: <IF_STMT> token = token[len(prefix):] self.prefix_step2a_success = True break return token",if token.startswith(prefix) and len(token) > 5:,if token.startswith(prefix):,0.7846331361630884,0.7778111223054219,False
1361,"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False elif isinstance(s, np.ndarray) and s.size == 0: return False <IF_STMT> return False return True","elif isinstance(s, collections.abc.Sequence) and len(s) == 0:","elif isinstance(s, (list, tuple)):",0.8605811459548586,0.8474968231198384,False
1362,"def get_all_comments(self, gallery_id, post_no, comment_cnt): comment_page_cnt = (comment_cnt - 1) // self.options.comments_per_page + 1 comments = [] headers = {'X-Requested-With': 'XMLHttpRequest'} data = {'ci_t': self._session.cookies['ci_c'], 'id': gallery_id, 'no': post_no} for i in range(comment_page_cnt): data['comment_page'] = i + 1 response = self.request_comment(headers, data) batch = self.parse_comments(response.text) <IF_STMT> break comments = batch + comments return comments",if not batch:,if not batch:,0.9673899707188607,0.9167056528641923,True
1363,def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: <IF_STMT> if e.no_match_group_specs or e.error_group_specs: raise e if e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS: raise e logger.error(str(e)),if self.base.conf.strict:,if e.module_base:,0.7653876966149633,0.8466657105524215,False
1364,"def find_field_notnull_differ(self, meta, table_description, table_name): if not self.can_detect_notnull_differ: return for field in all_local_fields(meta): attname = field.db_column or field.attname <IF_STMT> continue null = self.get_field_db_nullable(field, table_name) if field.null != null: action = field.null and 'DROP' or 'SET' self.add_difference('notnull-differ', table_name, attname, action)","if (table_name, attname) in self.new_db_fields:",if attname == table_description:,0.70916250844859,0.8627586293513119,False
1365,"def _change_moving_module(self, changes, dest): if not self.source.is_folder(): pymodule = self.pycore.resource_to_pyobject(self.source) source = self.import_tools.relatives_to_absolutes(pymodule) pymodule = self.tools.new_pymodule(pymodule, source) source = self._change_occurrences_in_module(dest, pymodule) source = self.tools.new_source(pymodule, source) <IF_STMT> changes.add_change(ChangeContents(self.source, source))",if source != self.source.read():,if source != dest:,0.7619883719218771,0.8200754821669128,False
1366,"def get(quality_name): """"""Returns a quality object based on canonical quality name."""""" found_components = {} for part in quality_name.lower().split(): component = _registry.get(part) <IF_STMT> raise ValueError('`%s` is not a valid quality string' % part) if component.type in found_components: raise ValueError('`%s` cannot be defined twice in a quality' % component.type) found_components[component.type] = component if not found_components: raise ValueError('No quality specified') result = Quality() for type, component in found_components.items(): setattr(result, type, component) return result",if not component:,if not component:,0.9562548627166221,0.9357502588195495,True
1367,def _unselected(self): selected = self._selected k = 0 z = selected[k] k += 1 for i in range(self._n): if i == z: <IF_STMT> z = selected[k] k += 1 else: z = -1 else: yield i,if k < len(selected):,if selected[k] == -1:,0.8258842054322068,0.8516228624291206,False
1368,"def render_headers(self) -> bytes: if not hasattr(self, '_headers'): parts = [b'Content-Disposition: form-data; ', format_form_param('name', self.name)] <IF_STMT> filename = format_form_param('filename', self.filename) parts.extend([b'; ', filename]) if self.content_type is not None: content_type = self.content_type.encode() parts.extend([b'\r\nContent-Type: ', content_type]) parts.append(b'\r\n\r\n') self._headers = b''.join(parts) return self._headers",if self.filename:,if self.filename is not None:,0.647475136749562,0.845713978670975,False
1369,"def app_middleware(next, root, info, **kwargs): app_auth_header = 'HTTP_AUTHORIZATION' prefix = 'bearer' request = info.context if request.path == API_PATH: if not hasattr(request, 'app'): request.app = None auth = request.META.get(app_auth_header, '').split() <IF_STMT> auth_prefix, auth_token = auth if auth_prefix.lower() == prefix: request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",if len(auth) == 2:,if auth:,0.9458663540093586,0.9220450449751959,False
1370,"def _shortest_hypernym_paths(self, simulate_root): if self.offset == '00000000': return {self: 0} queue = deque([(self, 0)]) path = {} while queue: s, depth = queue.popleft() <IF_STMT> continue path[s] = depth depth += 1 queue.extend(((hyp, depth) for hyp in s._hypernyms())) if simulate_root: root = Synset(self._wordnet_corpus_reader, None, self.pos(), '00000000', '') path[root] = max(path.values()) + 1 return path",if s in path:,if s is None:,0.6823657047977049,0.8983343737277126,False
1371,"def _populate_class_variables(): lookup = {} reverse_lookup = {} characters_for_re = [] for codepoint, name in list(codepoint2name.items()): character = chr(codepoint) <IF_STMT> characters_for_re.append(character) lookup[character] = name reverse_lookup[name] = character re_definition = '[%s]' % ''.join(characters_for_re) return (lookup, reverse_lookup, re.compile(re_definition))",if codepoint != 34:,if character not in lookup:,0.7174768224551267,0.8294838585473985,False
1372,"def prepare_data_status(self, view: sublime.View, data: Dict[str, Any]) -> Any: """"""Prepare the returned data for status"""""" if data['success'] and 'No docstring' not in data['doc'] and (data['doc'] != 'list\n'): self.signature = data['doc'] <IF_STMT> return try: self.signature = self.signature.splitlines()[2] except KeyError: return return self._show_status(view)",if self._signature_excluded(self.signature):,if not self.signature:,0.801070587918936,0.8902579342581529,False
1373,def _setup_once_tables(cls): if cls.run_define_tables == 'once': cls.define_tables(cls.metadata) <IF_STMT> cls.metadata.create_all(cls.bind) cls.tables.update(cls.metadata.tables),if cls.run_create_tables == 'once':,if cls.bind:,0.5006017471477509,0.6315552371794037,False
1374,"def _send_recursive(self, files): for base in files: <IF_STMT> self._send_files([base]) continue last_dir = asbytes(base) for root, dirs, fls in os.walk(base): self._chdir(last_dir, asbytes(root)) self._send_files([os.path.join(root, f) for f in fls]) last_dir = asbytes(root) for i in range(len(os.path.split(last_dir))): self._send_popd()",if not os.path.isdir(base):,if not os.path.isdir(base):,0.9238814510828744,0.8713933650206428,True
1375,"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) if not is_registered(self.model): inline_fields = () for inline in self.inlines: inline_model, follow_field = self._reversion_introspect_inline_admin(inline) if inline_model: self._reversion_autoregister(inline_model, ()) <IF_STMT> inline_fields += (follow_field,) self._reversion_autoregister(self.model, inline_fields)",if follow_field:,if follow_field:,0.9207692039246764,0.8787142254774354,True
1376,"def dispatch_hook(key, hooks, hook_data, **kwargs): """"""Dispatches a hook dictionary on a given piece of data."""""" hooks = hooks or dict() hooks = hooks.get(key) if hooks: if hasattr(hooks, '__call__'): hooks = [hooks] for hook in hooks: _hook_data = hook(hook_data, **kwargs) <IF_STMT> hook_data = _hook_data return hook_data",if _hook_data is not None:,if _hook_data is not None:,0.912936363305355,0.8621109017306224,True
1377,"def __call__(self, image, crop=True): if isinstance(image, PTensor): return self.crop_to_output(numpy_to_paddle(self(paddle_to_numpy(image), crop=False))) else: warp = cv.warpAffine(image, self.transform_matrix, image.shape[1::-1], borderMode=cv.BORDER_REPLICATE) <IF_STMT> return self.crop_to_output(warp) else: return warp",if crop:,if crop:,0.6660563873587796,0.839587623092576,True
1378,"def _analyze(self): lines = open(self.log_path, 'r').readlines() prev_line = None for line in lines: <IF_STMT> self.errors.append(line[len('ERROR:'):].strip()) elif line.startswith('FAIL:') and prev_line and prev_line.startswith('='): self.failures.append(line[len('FAIL:'):].strip()) prev_line = line",if line.startswith('ERROR:') and prev_line and prev_line.startswith('='):,if line.startswith('ERROR:') and prev_line and prev_line.startswith('='):,0.8591018001742481,0.7332023424667726,True
1379,"def end(self, name): self.soup.endData() completed_tag = self.soup.tagStack[-1] namespace, name = self._getNsTag(name) nsprefix = None if namespace is not None: for inverted_nsmap in reversed(self.nsmaps): <IF_STMT> nsprefix = inverted_nsmap[namespace] break self.soup.handle_endtag(name, nsprefix) if len(self.nsmaps) > 1: self.nsmaps.pop()",if inverted_nsmap is not None and namespace in inverted_nsmap:,if namespace in inverted_nsmap:,0.9272941365466495,0.8474968231198384,False
1380,"def _bind_parameters(operation, parameters): string_parameters = {} for name, value in parameters.iteritems(): if value is None: string_parameters[name] = 'NULL' <IF_STMT> string_parameters[name] = ""'"" + _escape(value) + ""'"" else: string_parameters[name] = str(value) return operation % string_parameters","elif isinstance(value, basestring):","elif isinstance(value, str):",0.7103804398116733,0.8675979125638379,False
1381,"def plugin_on_song_ended(self, song, skipped): if song is not None: rating = song('~#rating') invrating = 1.0 - rating delta = min(rating, invrating) / 2.0 <IF_STMT> rating -= delta else: rating += delta song['~#rating'] = rating",if skipped:,if skipped:,0.7013404310432994,0.8935248372106969,True
1382,"def on_activated_async(self, view): if settings['modified_lines_only']: self.freeze_last_version(view) if settings['enabled']: match_trailing_spaces(view) <IF_STMT> active_views[view.id()] = view.visible_region() self.update_on_region_change(view)",if not view.id() in active_views:,if view.visible_region():,0.8399272275183861,0.7241577342575828,False
1383,"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term):] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u('%s is contained here:') % py.io.saferepr(term, maxsize=42)] for line in diff: <IF_STMT> continue if line.startswith(u('- ')): continue if line.startswith(u('+ ')): newdiff.append(u('  ') + line[2:]) else: newdiff.append(line) return newdiff",if line.startswith(u('Skipping')):,if line.startswith(u(' ')):,0.9431306837603753,0.9182210682909737,False
1384,"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) elif fn.endswith('.png'): os.remove(fn_full) <IF_STMT> os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",elif fn.endswith('.md'):,elif fn.endswith('.png.png'):,0.8865971466460489,0.8645707301556367,False
1385,"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = (0, 0) for ts in self.time_steps: if ts.raw_reward is not None: raw_rewards += ts.raw_reward <IF_STMT> processed_rewards += ts.processed_reward return (raw_rewards, processed_rewards)",if ts.processed_reward is not None:,if ts.processed_reward is not None:,0.8112934935052207,0.8294838585473985,True
1386,"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] <IF_STMT> s = s.decode(encoding) if withyear: s = '%s %s' % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if encoding is not None:,if encoding:,0.7322584929911103,0.8827916928185874,False
1387,"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get('Authorization'): credentails = parse_authorization_header(request.headers.get('Authorization')) <IF_STMT> return response_hash = response(credentails, passwd, dict(uri=request.script_root + request.path, body=request.data, method=request.method)) if credentails.get('response') == response_hash: return True return False",if not credentails:,if not credentails:,0.9159518568536834,0.8675979125638379,True
1388,"def wrapped(self, request): try: return self._finished except AttributeError: <IF_STMT> if not request.session.shouldfail and (not request.session.shouldstop): log.debug('%s is still going to be used, not terminating it. Still in use on:\n%s', self, pprint.pformat(list(self.node_ids))) return log.debug('Finish called on %s', self) try: return func(request) finally: self._finished = True",if self.node_ids:,if self._finished:,0.8641274136907493,0.9184043388013005,False
1389,"def run_tests(): x = 5 with switch(x) as case: if case(0): print('zero') print('zero') elif case(1, 2): print('one or two') <IF_STMT> print('three or four') else: print('default') print('another')","elif case(3, 4):","elif case(3, 4):",0.8905431881326735,0.8266114125804572,True
1390,def task_done(self): with self._cond: <IF_STMT> raise ValueError('task_done() called too many times') if self._unfinished_tasks._semlock._is_zero(): self._cond.notify_all(),if not self._unfinished_tasks.acquire(False):,if self._unfinished_tasks._semlock._is_zero():,0.8104695556077298,0.7241577342575828,False
1391,"def _set_uid(self, val): if val is not None: <IF_STMT> self.bus.log('pwd module not available; ignoring uid.', level=30) val = None elif isinstance(val, text_or_bytes): val = pwd.getpwnam(val)[2] self._uid = val",if pwd is None:,if not pwd:,0.877611927869734,0.8390782502060267,False
1392,"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) <IF_STMT> exe, args = exe_data return (company, major, minor, arch, exe, args)",if exe_data is not None:,if exe_data is not None:,0.7782437516777401,0.8901199011963146,True
1393,"def run(algs): for alg in algs: vcs = alg.get('variantcaller') if vcs: if isinstance(vcs, dict): vcs = reduce(operator.add, vcs.values()) <IF_STMT> vcs = [vcs] return any((vc.startswith(prefix) for vc in vcs if vc))","if not isinstance(vcs, (list, tuple)):","elif not isinstance(vcs, list):",0.6205610908120028,0.8228500218338367,False
1394,"def wrapper(self, *args, **kwargs): if not self.request.path.endswith('/'): <IF_STMT> uri = self.request.path + '/' if self.request.query: uri += '?' + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)","if self.request.method in ('GET', 'HEAD'):",if self.request.path:,0.7638277103152753,0.8743414417652072,False
1395,"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: <IF_STMT> continue if line.startswith(b'OK'): return elif line.startswith(b'Benutzer/Passwort Fehler'): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",if not line.strip():,if not line:,0.9121738912025151,0.8390782502060267,False
1396,"def Walk(self, hMenu=None): if not hMenu: hMenu = self.handle n = user32.GetMenuItemCount(hMenu) mi = MENUITEMINFO() for i in range(n): mi.fMask = 2 user32.GetMenuItemInfoA(hMenu, i, 1, byref(mi)) handle = user32.GetSubMenu(hMenu, i) <IF_STMT> yield (handle, self.ListItems(handle)) for i in self.Walk(handle): yield i",if handle:,if handle:,0.871573016044134,0.9099951253570094,True
1397,"def setSelection(self, labels): input = self.__validateInput(labels) if len(input) == 0 and (not self.__allowEmptySelection): return if self.__allowMultipleSelection: self.__selectedLabels[:] = input self.__selectionChanged() el<IF_STMT> raise RuntimeError('Parameter must be single item or a list with one element.') else: self.__selectedLabels[:] = input self.__selectionChanged() self.__validateState()",if len(input) > 1:,"if not isinstance(input, list):",0.9055869887731312,0.8627586293513119,False
1398,"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) <IF_STMT> raise ParsingError('""momentum"" must be numeric.')","if not isinstance(self.momentum, (int, float)):","if not isinstance(self.momentum, int):",0.9296625895199283,0.8692960007731574,False
1399,"def get_order(self, aBuf): if not aBuf: return (-1, 1) first_char = wrap_ord(aBuf[0]) if 129 <= first_char <= 159 or 224 <= first_char <= 252: charLen = 2 else: charLen = 1 if len(aBuf) > 1: second_char = wrap_ord(aBuf[1]) <IF_STMT> return (second_char - 159, charLen) return (-1, charLen)",if first_char == 202 and 159 <= second_char <= 241:,if second_char > 159:,0.6663109752276118,0.8856327184319047,False
1400,"def saveSpecial(self, **kwargs): for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST: item = config.get_config('misc', kw) value = kwargs.get(kw) msg = item.set(value) <IF_STMT> return badParameterResponse(msg) config.save_config() raise Raiser(self.__root)",if msg:,if msg:,0.7547774287701479,0.8645707301556367,True
1401,"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): <IF_STMT> kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: if isinstance(kwargs.get('event_data', {}).get(key), str): if len(kwargs['event_data'][key]) > 1024: kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)",if key not in valid_keys:,if key in valid_keys:,0.896373987188623,0.8038019482772603,False
1402,"def toggleFactorReload(self, value=None): self.serviceFittingOptions['useGlobalForceReload'] = value if value is not None else not self.serviceFittingOptions['useGlobalForceReload'] fitIDs = set() for fit in set(self._loadedFits): <IF_STMT> continue if fit.calculated: fit.factorReload = self.serviceFittingOptions['useGlobalForceReload'] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",if fit is None:,if fit.ID in fitIDs:,0.7694905743743957,0.8336104423443033,False
1403,"def closest_unseen(self, row1, col1, filter=None): min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): <IF_STMT> if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen","if filter is None or (row, col) not in filter:",if filter is not None and filter[row][col] == filter:,0.8989485917214324,0.7867820991813698,False
1404,"def getAlphaClone(lookfor, eager=None): if isinstance(lookfor, int): <IF_STMT> item = get_gamedata_session().query(AlphaClone).get(lookfor) else: item = get_gamedata_session().query(AlphaClone).options(*processEager(eager)).filter(AlphaClone.ID == lookfor).first() else: raise TypeError('Need integer as argument') return item",if eager is None:,if eager is None:,0.8518718663257837,0.7801270245332924,True
1405,def _rle_encode(string): new = b'' count = 0 for cur in string: if not cur: count += 1 else: <IF_STMT> new += b'\x00' + bytes([count]) count = 0 new += bytes([cur]) return new,if count:,if count:,0.7745027830185384,0.8901732118131125,True
1406,def result_iterator(): try: for future in fs: <IF_STMT> yield future.result() else: yield future.result(end_time - time.time()) finally: for future in fs: future.cancel(),if timeout is None:,if end_time is None:,0.620516729215264,0.7498810286408993,False
1407,"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == 'val': for key, value in segment.items(): <IF_STMT> return value if hasattr(key, 'text'): if key.text == index[0]: return value raise Exception('Invalid state') elif index_type == 'index': return segment[index] elif index_type == 'textslice': return segment[index[0]:index[1]] elif index_type == 'key': return index[1] if strictdoc else index[0] else: raise Exception('Invalid state')",if key == index[0]:,if strictdoc:,0.9280335793240164,0.9362597875749384,False
1408,"def _reset_sequences(self, db_name): conn = connections[db_name] if conn.features.supports_sequence_reset: sql_list = conn.ops.sequence_reset_by_name_sql(no_style(), conn.introspection.sequence_list()) <IF_STMT> try: cursor = conn.cursor() for sql in sql_list: cursor.execute(sql) except Exception: transaction.rollback_unless_managed(using=db_name) raise transaction.commit_unless_managed(using=db_name)",if sql_list:,if sql_list:,0.694238185517738,0.8645707301556367,True
1409,"def translate_to_statements(self, statements, conditional_write_vars): lines = [] for stmt in statements: <IF_STMT> self.temporary_vars.add((stmt.var, stmt.dtype)) line = self.translate_statement(stmt) if stmt.var in conditional_write_vars: subs = {} condvar = conditional_write_vars[stmt.var] lines.append('if %s:' % condvar) lines.append(indent(line)) else: lines.append(line) return lines",if stmt.op == ':=' and (not stmt.var in self.variables):,if stmt.var not in self.temporary_vars:,0.864070527117712,0.8294838585473985,False
1410,"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: ext = os.path.splitext(os.path.normcase(py_file))[1] if ext != PYTHON_SOURCE_EXTENSION: continue <IF_STMT> bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if self.compile:,if self.optimize > 0:,0.7500757117674959,0.828399516355805,False
1411,"def logic(): for i in range(100): yield (clock.posedge, reset.negedge) if reset == ACTIVE_LOW: count.next = 0 el<IF_STMT> count.next = (count + 1) % n raise StopSimulation",if enable:,if reset == ACTIVE_HIGH:,0.8716639985250696,0.7965020533851944,False
1412,"def _is_subnet_of(a, b): try: <IF_STMT> raise TypeError('%s and %s are not of the same version' % (a, b)) return b.network_address <= a.network_address and b.broadcast_address >= a.broadcast_address except AttributeError: raise TypeError('Unable to test subnet containment between %s and %s' % (a, b))",if a._version != b._version:,if a.version != b.version:,0.8912406638065,0.8692960007731574,False
1413,"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: if item.endswith('/') and (not is_dir): continue match = path if item.startswith('/') else basename <IF_STMT> return True return False","if fnmatch.fnmatch(match, item.strip('/')):",if match.lower().startswith(basename):,0.7860559499754847,0.8827916928185874,False
1414,"def __recv_null(self): """"""Receive a null byte."""""" while 1: c = self.sock.recv(1) if c == '': self.close() raise EOFError('Socket Closed') <IF_STMT> return",if c == '\x00':,if c == '':,0.6412398210433737,0.7871773473399102,False
1415,"def onMessage(self, payload, isBinary): if isBinary: self.result = 'Expected text message with payload, but got binary.' el<IF_STMT> self.result = 'Expected text message with payload of length %d, but got %d.' % (self.DATALEN, len(payload)) else: self.behavior = Case.OK self.result = 'Received text message of length %d.' % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len(payload) != self.DATALEN:,if len(payload) != self.DATALEN:,0.7294385453207922,0.8944264839442453,True
1416,"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: <IF_STMT> continue d_path = ''.join([path, '/', d]) d_new_path = ''.join([new_path, '/', d]) attr = self.getattr(d_path) if stat.S_ISDIR(attr['st_mode']): self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)","if d in ['.', '..']:",if not d.endswith('.py'):,0.8767459871138132,0.8953711787948615,False
1417,"def dir_box_click(self, double): if double: name = self.list_box.get_selected_name() path = os.path.join(self.directory, name) suffix = os.path.splitext(name)[1] <IF_STMT> self.directory = path else: self.double_click_file(name) self.update()",if suffix not in self.suffixes and os.path.isdir(path):,if suffix == '.py':,0.7631982552583616,0.760856626273165,False
1418,"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: if value is not None: <IF_STMT> return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError('Module %r has no mapped classes registered under the name %r' % (self.__parent.name, key))","if isinstance(value, _ModuleMarker):","if isinstance(value, _ModuleMarker):",0.9277135304075171,0.8902579342581529,True
1419,"def poll_thread(): time.sleep(0.5) if process.wait() and process_state: time.sleep(0.25) <IF_STMT> stdout, stderr = process._communicate(None) logger.error('Web server process exited unexpectedly', 'app', stdout=stdout, stderr=stderr) time.sleep(1) restart_server(1)",if not check_global_interrupt():,if process._communicate:,0.8792428671355219,0.839587623092576,False
1420,"def apply_dateparser_timezone(utc_datetime, offset_or_timezone_abb): for name, info in _tz_offsets: <IF_STMT> tz = StaticTzInfo(name, info['offset']) return utc_datetime.astimezone(tz)",if info['regex'].search(' %s' % offset_or_timezone_abb):,if name in offset_or_timezone_abb:,0.7580905908037154,0.6540585844910979,False
1421,"def _load_wordlist(filename): if filename is None: return {} path = None for dir in (CONFIG_DIR, ASSETS_DIR): path = os.path.realpath(os.path.join(dir, filename)) <IF_STMT> break words = {} with open(path, encoding='utf-8') as f: pairs = [word.strip().rsplit(' ', 1) for word in f] pairs.sort(reverse=True, key=lambda x: int(x[1])) words = {p[0]: int(p[1]) for p in pairs} return words",if os.path.exists(path):,if path is None:,0.6860888000012155,0.8983343737277126,False
1422,"def terminate_processes_matching_names(match_strings, kill=False): """"""Terminates processes matching particular names (case sensitive)."""""" if isinstance(match_strings, str): match_strings = [match_strings] for process in psutil.process_iter(): try: process_info = process.as_dict(attrs=['name', 'pid']) process_name = process_info['name'] except (psutil.AccessDenied, psutil.NoSuchProcess, OSError): continue <IF_STMT> terminate_process(process_info['pid'], kill)",if any((x == process_name for x in match_strings)):,if match_strings(process_name):,0.9153853530524183,0.9073052766733322,False
1423,"def has_scheme(self, inp): if '://' in inp: return True else: authority = inp.replace('/', '#').replace('?', '#').split('#')[0] <IF_STMT> _, host_or_port = authority.split(':', 1) if re.match('^\\d+$', host_or_port): return False else: return False return True",if ':' in authority:,if ':' in authority:,0.6466636732107177,0.828399516355805,True
1424,def close(self): with BrowserContext._BROWSER_LOCK: BrowserContext._BROWSER_REFCNT -= 1 <IF_STMT> logger.info('Destroying browser main loop') BrowserContext._BROWSER_LOOP.destroy() BrowserContext._BROWSER_LOOP = None,if BrowserContext._BROWSER_REFCNT == 0:,if BrowserContext._BROWSER_REFCNT == 0:,0.5448912153851561,0.674945488826271,True
1425,"def _mock_get_merge_ticks(self, order_book_id_list, trading_date, last_dt=None): for tick in self._ticks: <IF_STMT> continue if self.env.data_proxy.get_future_trading_date(tick.datetime).date() != trading_date.date(): continue if last_dt and tick.datetime <= last_dt: continue yield tick",if tick.order_book_id not in order_book_id_list:,if tick.order_book_id_list != order_book_id_list:,0.768200521569978,0.7886336751695258,False
1426,"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' <IF_STMT> source += '[branch %s] ' % ss['branch'] if ss['revision']: source += str(ss['revision']) else: source += 'HEAD' if ss['patch'] is not None: source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['branch']:,if ss['branch']:,0.7464827187039841,0.9425437476131634,True
1427,"def test_open_read_bytes(self, sftp): """"""Test reading bytes from a file"""""" f = None try: self._create_file('file', 'xxx') f = (yield from sftp.open('file', 'rb')) self.assertEqual((yield from f.read()), b'xxx') finally: <IF_STMT> yield from f.close() remove('file')",if f:,if f:,0.9237432003276359,0.8827916928185874,True
1428,"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) if len(data) == 0: break chan.send(data) <IF_STMT> data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",if chan in r:,elif chan in r:,0.9485183237898799,0.9019629427251674,False
1429,"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search(""url\\('/ks-waf-error\\.png'\\)"", page, re.I) is not None <IF_STMT> break return retval",if retval:,if retval:,0.8116829700443436,0.8590888738245122,True
1430,"def __init__(self, raw): ticker_ticks = {} for tick in raw['results']: <IF_STMT> ticker_ticks[tick['T']].append(tick) else: ticker_ticks[tick['T']] = [tick] super().__init__({ticker: Aggsv2({'results': ticks}) for ticker, ticks in ticker_ticks.items()})",if ticker_ticks.get(tick['T']):,if tick['T'] in ticker_ticks:,0.7535299109665649,0.7801270245332924,False
1431,"def _makefiles(self, f): if isinstance(f, dict): for k, v in list(f.items()): <IF_STMT> self.makedir(dirname=k, content=v) elif isinstance(v, str): self.make_file(filename=k, content=v) else: raise ValueError('Unexpected:', k, v) elif isinstance(f, str): self._make_empty_file(f) elif isinstance(f, list): self.make_list(f) else: raise ValueError('Unknown type:', f)","if isinstance(v, list):","if isinstance(v, str):",0.68219492942629,0.8783650674919876,False
1432,"def migrate_command_storage(apps, schema_editor): model = apps.get_model('terminal', 'CommandStorage') init_storage_data(model) setting = get_setting(apps, schema_editor, 'TERMINAL_COMMAND_STORAGE') if not setting: return values = get_storage_data(setting) for name, meta in values.items(): tp = meta.pop('TYPE') <IF_STMT> continue model.objects.create(name=name, type=tp, meta=meta)","if not tp or name in ['default', 'null']:",if tp == 'terminal':,0.9043499449520372,0.8385130047130208,False
1433,"def build_vertices(self, ulines): vertex_idx = 0 vertices = collections.OrderedDict() for line in ulines: for vt in line: <IF_STMT> continue new_vertex = (vt.u, vt.v, 0.0) if new_vertex in vertices: continue vt.index = vertex_idx vertex_idx += 1 vertices[new_vertex] = 1 return (vertex_idx, list(vertices.keys()))",if vt.replacement is not None:,if vt.u == 0 or vt.v == 0:,0.878233891748693,0.7944725405795466,False
1434,"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {'objects': 0, 'containers': 0, 'accounts': 0} qdir = 'quarantined' for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) if os.path.exists(qtgt): linkcount = os.lstat(qtgt).st_nlink <IF_STMT> qcounts[qtype] += linkcount - 2 return qcounts",if linkcount > 2:,if linkcount > 2:,0.7464607751467066,0.8780099567239787,True
1435,"def _format_arg(self, name, trait_spec, value): if name == 'mask_file': return '' if name == 'op_string': <IF_STMT> if isdefined(self.inputs.mask_file): return self.inputs.op_string % self.inputs.mask_file else: raise ValueError('-k %s option in op_string requires mask_file') return super(ImageStats, self)._format_arg(name, trait_spec, value)",if '-k %s' in self.inputs.op_string:,if self.inputs.op_string:,0.860442636259299,0.8996480074924822,False
1436,"def _update_theme_style(self, *args): self.line_color_normal = self.theme_cls.divider_color if not any([self.error, self._text_len_error]): if not self.focus: self._current_hint_text_color = self.theme_cls.disabled_hint_text_color self._current_right_lbl_color = self.theme_cls.disabled_hint_text_color <IF_STMT> self._current_error_color = self.theme_cls.disabled_hint_text_color",if self.helper_text_mode == 'persistent':,if not self.focus:,0.872909708453573,0.80377750806414,False
1437,"def createFields(self): for item in self.format: <IF_STMT> yield item[0](self, *item[1:-1], **item[-1]) else: yield item[0](self, *item[1:])","if isinstance(item[-1], dict):",if item[-1] == '-':,0.5584629768306472,0.6540585844910979,False
1438,"def execute(self, statement, arguments=None): while True: try: <IF_STMT> self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: if 'locked' not in getSafeExString(ex): raise else: break if statement.lstrip().upper().startswith('SELECT'): return self.cursor.fetchall()",if arguments:,if arguments:,0.9054155817565519,0.8696398662122882,True
1439,"def set_income_account_for_fixed_assets(self): disposal_account = depreciation_cost_center = None for d in self.get('items'): <IF_STMT> if not disposal_account: disposal_account, depreciation_cost_center = get_disposal_account_and_cost_center(self.company) d.income_account = disposal_account if not d.cost_center: d.cost_center = depreciation_cost_center",if d.is_fixed_asset:,if d.income_account is None:,0.8752963235032599,0.8105932471967202,False
1440,"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 <IF_STMT> nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",if nbChars[1] is not None:,if nbChars[1] is not None:,0.7213862687649873,0.8621109017306224,True
1441,"def _get_service_full_name(self, name, help_command_table): if help_command_table and name not in self._NON_SERVICE_COMMANDS: <IF_STMT> return self._HIGH_LEVEL_SERVICE_FULL_NAMES[name] service = help_command_table.get(name) if service: return service.service_model.metadata['serviceFullName']",if name in self._HIGH_LEVEL_SERVICE_FULL_NAMES:,if name in self._HIGH_LEVEL_SERVICE_FULL_NAMES:,0.8126766995176532,0.7498810286408993,True
1442,"def print_addresses(self): p = 3 tmp_str = '[' if self.get_len() >= 7: while 1: <IF_STMT> tmp_str += '#' tmp_str += self.get_ip_address(p) p += 4 if p >= self.get_len(): break else: tmp_str += ', ' tmp_str += '] ' if self.get_ptr() % 4: tmp_str += 'nonsense ptr field: %d ' % self.get_ptr() return tmp_str",if p + 1 == self.get_ptr():,if self.get_ptr() % 4:,0.8733640233507228,0.8983343737277126,False
1443,def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({'test': 'insert'}) error = False except: if not self.expect_exception: raise <IF_STMT> assert error,if self.expect_exception:,if self.expect_exception:,0.8473956883106025,0.8524094630854673,True
1444,"def create_composite_mounter_by_args(args): """"""Creates a CompositeMounter by the images in given args."""""" logging.info('Mount images...') mounter = composite_mounter.CompositeMounter() for partition in composite_mounter.SUPPORTED_PARTITIONS: image_source = vars(args)[partition] <IF_STMT> logging.info('  %s=%s', partition, image_source) mounter.add_by_mount_target(partition, image_source) if mounter.is_empty(): raise RuntimeError('Must give at least one image source.') return mounter",if image_source:,if image_source:,0.8399428377940749,0.9144061946646023,True
1445,"def _get_containing_class(self, pyname): if isinstance(pyname, pynames.DefinedName): scope = pyname.get_object().get_scope() parent = scope.parent <IF_STMT> return parent.pyobject",if parent is not None and parent.get_kind() == 'Class':,if parent is not None:,0.7201165441204892,0.6349495142258627,False
1446,"def test_chunkcoding(self): tstring_lines = [] for b in self.tstring: lines = b.split(b'\n') last = lines.pop() assert last == b'' lines = [line + b'\n' for line in lines] tstring_lines.append(lines) for native, utf8 in zip(*tstring_lines): u = self.decode(native)[0] self.assertEqual(u, utf8.decode('utf-8')) <IF_STMT> self.assertEqual(native, self.encode(u)[0])",if self.roundtriptest:,if self.is_native:,0.8332512315054856,0.92086883132209,False
1447,"def set_default_variants(apps, schema_editor): Product = apps.get_model('product', 'Product') for product in Product.objects.iterator(): first_variant = product.variants.first() <IF_STMT> product.default_variant = first_variant product.save(update_fields=['default_variant', 'updated_at'])",if first_variant:,if first_variant is not None:,0.8103743464850588,0.7049592608322395,False
1448,def json(self): try: if self.is_json(): raw_data = self.raw_data() <IF_STMT> raw_data = raw_data.decode('utf-8') return json.loads(raw_data) except ValueError: pass,"if not isinstance(raw_data, text_type):","if isinstance(raw_data, bytes):",0.7007953008168937,0.7331765459202478,False
1449,"def clear_react(self, message: discord.Message, emoji: MutableMapping=None) -> None: try: await message.clear_reactions() except discord.Forbidden: <IF_STMT> return with contextlib.suppress(discord.HTTPException): async for key in AsyncIter(emoji.values(), delay=0.2): await message.remove_reaction(key, self.bot.user) except discord.HTTPException: return",if not emoji:,if not self.bot.user.is_active:,0.5975750039467144,0.8446593249975184,False
1450,"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split('_', 1)[1]): if value.startswith(prefix): value = value[len(prefix):] value = value.lstrip('_') <IF_STMT> return getattr(self.group, value) else: raise ValueError('No such constant: %s_%s' % (self.prefix, value)) else: return value","if hasattr(self.group, value):","if hasattr(self.group, value):",0.8596682774020732,0.8953711787948615,True
1451,"def value(self): quote = False if self.defects: quote = True else: for x in self: if x.token_type == 'quoted-string': quote = True if quote: pre = post = '' if self[0].token_type == 'cfws' or self[0][0].token_type == 'cfws': pre = ' ' <IF_STMT> post = ' ' return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",if self[-1].token_type == 'cfws' or self[-1][-1].token_type == 'cfws':,elif self[0][0].token_type == 'cfws':,0.7234917331891754,0.9036816878108535,False
1452,"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: if root_path: config_root_path = drive.get('root_path') if config_root_path and root_path == config_root_path: return drive <IF_STMT> config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",elif volume_guid_path:,if volume_guid_path:,0.9218869534056262,0.8901732118131125,False
1453,"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) <IF_STMT> edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if parsed_drawing:,if parsed_drawing:,0.7475518202992956,0.926934323706186,True
1454,"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' <IF_STMT> return 'null' else: return unicode(k)",elif k is None:,elif k is None:,0.9206392515494587,0.8856327184319047,True
1455,"def find_multiple_stats(stats, name, _found=None, _on_found=None): if _found is None: _found = [] for child_stats in stats: if child_stats.name == name: _found.append(child_stats) <IF_STMT> _on_found(_found) find_multiple_stats(child_stats, name, _found) return _found",if callable(_on_found):,if _on_found is not None:,0.6837720108505593,0.7839800200378249,False
1456,"def _run_generated_code(self, code, globs, locs, fails_under_py3k=True): import warnings from zope.interface._compat import PYTHON3 with warnings.catch_warnings(record=True) as log: warnings.resetwarnings() <IF_STMT> exec(code, globs, locs) self.assertEqual(len(log), 0) return True else: try: exec(code, globs, locs) except TypeError: return False else: if fails_under_py3k: self.fail(""Didn't raise TypeError"")",if not PYTHON3:,if PYTHON3:,0.7029291600431724,0.9099951253570094,False
1457,"def _get_node(self, node_id): self.non_terminated_nodes({}) with self.lock: <IF_STMT> return self.cached_nodes[node_id] instance = self.compute.instances().get(project=self.provider_config['project_id'], zone=self.provider_config['availability_zone'], instance=node_id).execute() return instance",if node_id in self.cached_nodes:,if node_id in self.cached_nodes:,0.7787984296036061,0.674945488826271,True
1458,def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok['text'] <IF_STMT> if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break,if self.opHash.has_key(tval):,if tval in self.opHash:,0.6998354700545156,0.8661072626070159,False
1459,"def _create_or_get_helper(self, infer_mode: Optional[bool]=None, **kwargs) -> Helper: prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) if is_training and self._train_helper is None: self._train_helper = helper <IF_STMT> self._infer_helper = helper return helper",elif not is_training and self._infer_helper is None:,elif is_infer and self._infer_helper is None:,0.8413820590874624,0.8711151332295498,False
1460,"def get_ldset(self, ldsets): ldset = None if self._properties['ldset_name'] == '': nldset = len(ldsets) if nldset == 0: msg = _('Logical Disk Set could not be found.') raise exception.NotFound(msg) else: ldset = None else: <IF_STMT> msg = _('Logical Disk Set `%s` could not be found.') % self._properties['ldset_name'] raise exception.NotFound(msg) ldset = ldsets[self._properties['ldset_name']] return ldset",if self._properties['ldset_name'] not in ldsets:,if nldset == 0:,0.9108225971197759,0.8983343737277126,False
1461,"def calc_fractal_serial(q, maxiter): z = np.zeros(q.shape, complex) output = np.resize(np.array(0), q.shape) for i in range(len(q)): for iter in range(maxiter): z[i] = z[i] * z[i] + q[i] <IF_STMT> output[i] = iter break return output",if abs(z[i]) > 2.0:,if z[i] > 0.5:,0.9057505051761269,0.8385130047130208,False
1462,"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) <IF_STMT> if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))","if isinstance(inst, (_Block, _Instantiator)):","if isinstance(inst, _Block):",0.9036409286673633,0.8196189957582152,False
1463,"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip('\n').split(' ')] bucket.append(walk) <IF_STMT> yield bucket bucket = [] if len(bucket): yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",if len(bucket) == batch_size:,if len(bucket) == batch_size:,0.7468832110951412,0.9036816878108535,True
1464,def _traverse(op): if op in visited: return visited.add(op) if tag.is_injective(op.tag): if op not in s.outputs: s[op].compute_inline() for tensor in op.input_tensors: <IF_STMT> _traverse(tensor.op) callback(op),"if isinstance(tensor.op, tvm.te.ComputeOp):",if tensor.op is not None:,0.8722898662265809,0.7519196511661765,False
1465,"def unwatch_run(self, run_id, handler): with self._dict_lock: <IF_STMT> self._handlers_dict[run_id] = [(start_cursor, callback) for start_cursor, callback in self._handlers_dict[run_id] if callback != handler] if not self._handlers_dict[run_id]: del self._handlers_dict[run_id] run_id_dict = self._run_id_dict del run_id_dict[run_id] self._run_id_dict = run_id_dict",if run_id in self._run_id_dict:,if run_id not in self._handlers_dict:,0.8717644988998793,0.8148691130388024,False
1466,"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) <IF_STMT> print('Successfully connected to MySQL with the given configuration.') return else: print('Error: Could not connect to MySQL with the given configuration.') retry = RetryBoolQuestion('Do you want to retry MySQL configuration?', True) if not retry: raise ConfigInitError()",if self._CheckMySQLConnection():,if self._MySQLConnected():,0.7981862413336954,0.9312457603037672,False
1467,"def get_courses_without_topic(topic): data = [] for entry in frappe.db.get_all('Course'): course = frappe.get_doc('Course', entry.name) topics = [t.topic for t in course.topics] <IF_STMT> data.append(course.name) return data",if not topics or topic not in topics:,if topic in topics:,0.8324366221864676,0.7801270245332924,False
1468,"def _error_handler(action, **keywords): if keywords: file_type = keywords.get('file_type', None) if file_type: raise exceptions.FileTypeNotSupported(constants.FILE_TYPE_NOT_SUPPORTED_FMT % (file_type, action)) else: <IF_STMT> keywords.pop('on_demand') msg = 'Please check if there were typos in ' msg += 'function parameters: %s. Otherwise ' msg += 'unrecognized parameters were given.' raise exceptions.UnknownParameters(msg % keywords) else: raise exceptions.UnknownParameters('No parameters found!')",if 'on_demand' in keywords:,if 'on_demand' in keywords:,0.7264797391725518,0.8944264839442453,True
1469,"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) <IF_STMT> to_store.append(self.view.substr(self.view.full_line(r))) if register: text = ''.join(to_store) if not text.endswith('\n'): text = text + '\n' state = State(self.view) state.registers[register] = [text]",if register:,if self.view.is_line(r):,0.6554227100807676,0.8935248372106969,False
1470,"def has_actor(self, message: HasActorMessage) -> ResultMessage: actor_ref = message.actor_ref for address, item in self._allocated_actors.items(): ref = create_actor_ref(address, actor_ref.uid) <IF_STMT> return ResultMessage(message.message_id, True, protocol=message.protocol) return ResultMessage(message.message_id, False, protocol=message.protocol)",if ref in item:,if ref == item:,0.7606846099270744,0.8038019482772603,False
1471,"def toggleMetaButton(self, event): """"""Process clicks on toggle buttons"""""" clickedBtn = event.EventObject if wx.GetMouseState().GetModifiers() == wx.MOD_CONTROL: activeBtns = [btn for btn in self.metaButtons if btn.GetValue()] <IF_STMT> clickedBtn.setUserSelection(clickedBtn.GetValue()) self.itemView.filterItemStore() else: clickedBtn.setUserSelection(True) else: for btn in self.metaButtons: btn.setUserSelection(btn == clickedBtn) self.itemView.filterItemStore()",if activeBtns:,if len(activeBtns) == 0:,0.8341858381003384,0.8592377270804451,False
1472,"def __init__(self, hub=None): if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() if config.resolver_nameservers: _resolver.network_resolver.nameservers[:] = config.resolver_nameservers <IF_STMT> _resolver.network_resolver.lifetime = config.resolver_timeout assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",if config.resolver_timeout:,if config.resolver_timeout:,0.7326990299685254,0.8645707301556367,True
1473,"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] <IF_STMT> m = RE_CHECKBOX.match(first.text) if m is not None: first.text = self.markdown.htmlStash.store(get_checkbox(m.group('state')), safe=True) + m.group('line') found = True return found",if first.tag == 'p' and first.text is not None:,if first.text:,0.6655484906725863,0.8996480074924822,False
1474,"def _check_mswin_locale(locale): msloc = None try: msloc = _LOCALE_NAMES[locale[:5]][:2] locale = locale[:5] except KeyError: try: msloc = _LOCALE_NAMES[locale[:2]][:2] locale = locale[:2] except KeyError: <IF_STMT> return ('en_GB', '1252') return (None, None) return (locale, msloc)",if locale[:2] == 'en' and locale[:5] != 'en_US':,if locale == 'en_GB':,0.8950956772072034,0.8385130047130208,False
1475,"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = (self.c, self, self.w) if w: <IF_STMT> g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) if protect: k.mb_prefix = s","if hasattr(g.app.gui, 'set_minibuffer_label'):",if c:,0.8815167907505902,0.8935248372106969,False
1476,"def getProc(su, innerTarget): if len(su) == 1: proc = ('first', 'last') elif su.isFirst(innerTarget) and su.isLast(innerTarget): proc = ('first', 'last') <IF_STMT> proc = ('first',) elif su.isLast(innerTarget): proc = ('last',) else: proc = () return proc",elif su.isFirst(innerTarget):,elif su.isFirst(innerTarget):,0.7546002662114832,0.8935248372106969,True
1477,def await_test_end(self): iterations = 0 while True: if iterations > 100: self.log.debug('Await: iteration limit reached') return status = self.master.get_status() <IF_STMT> return iterations += 1 time.sleep(1.0),if status.get('status') == 'ENDED':,if status == 'DONE':,0.6804308194291806,0.7886336751695258,False
1478,"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): <IF_STMT> text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","if isinstance(text, CodeViewText):","if isinstance(text, CodeViewText):",0.8687184279070813,0.833078701050083,True
1479,"def validate_party_details(self): if self.party: <IF_STMT> frappe.throw(_('Invalid {0}: {1}').format(self.party_type, self.party)) if self.party_account and self.party_type in ('Customer', 'Supplier'): self.validate_account_type(self.party_account, [erpnext.get_party_account_type(self.party_type)])","if not frappe.db.exists(self.party_type, self.party):","if not self.party_type in ('Customer', 'Supplier'):",0.8289623006221012,0.6436045454907839,False
1480,"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): <IF_STMT> pieces.append(force_text(getattr(self, piece)())) elif piece: pieces.append(re_escaped.sub('\\1', piece)) return ''.join(pieces)",if i % 2:,if i == 0:,0.8493088711599228,0.7378351342269067,False
1481,"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] <IF_STMT> s[i] = '\\' elif c == '\\' and s[i + 1] == '$': s[i] = '' i += 1 i += 1 return pattern[:0].join(s)",if c == '$' and s[i + 1] in '0123456789':,if c == '$':,0.7523219626382959,0.9001816649635144,False
1482,"def download(self, url, filename, **kwargs): try: r = self.get(url, timeout=10, stream=True, **kwargs) <IF_STMT> return False with open(filename, 'wb') as f: for chunk in r.iter_content(chunk_size=1024): if chunk: f.write(chunk) helpers.chmod_as_parent(filename) except Exception as e: sickrage.app.log.debug('Failed to download file from {} - ERROR: {}'.format(url, e)) if os.path.exists(filename): os.remove(filename) return False return True",if r.status_code >= 400:,if not r:,0.7146799639356279,0.9081987180086649,False
1483,"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedFilesWithExtension('js'): items.append('<script type=""text/javascript"" src=""' + item.pathAbsoluteFromProjectEncoded() + '""></script>') if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:,if len(items) > 1:,0.7464930091908393,0.8105932471967202,True
1484,def work(self): while True: timeout = self.timeout if idle.is_set(): timeout = self.idle_timeout log.debug('Wait for {}'.format(timeout)) fetch.wait(timeout) <IF_STMT> log.info('Stop fetch worker') break self.fetch(),if shutting_down.is_set():,if fetch.is_set():,0.6898089129054931,0.8318180062062374,False
1485,"def check_apns_certificate(ss): mode = 'start' for s in ss.split('\n'): if mode == 'start': <IF_STMT> mode = 'key' elif mode == 'key': if 'END RSA PRIVATE KEY' in s or 'END PRIVATE KEY' in s: mode = 'end' break elif s.startswith('Proc-Type') and 'ENCRYPTED' in s: raise ImproperlyConfigured('Encrypted APNS private keys are not supported') if mode != 'end': raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",if 'BEGIN RSA PRIVATE KEY' in s or 'BEGIN PRIVATE KEY' in s:,if 'RSA PRIVATE KEY' in s or 'RSA PRIVATE KEY' in s:,0.8534841538744908,0.8183335711992558,False
1486,"def compare_lists(self, l1, l2, key): l2_lookup = {o.get(key): o for o in l2} for obj1 in l1: obj2 = l2_lookup.get(obj1.get(key)) for k in obj1: <IF_STMT> self.assertEqual(obj1.get(k), obj2.get(k))",if k not in 'id' and obj1.get(k):,if k in obj2:,0.7485799645424609,0.8136663942210373,False
1487,"def before_get_object(self, view_kwargs): if view_kwargs.get('id') is not None: try: user_favourite_event = find_user_favourite_event_by_id(event_id=view_kwargs['id']) except NoResultFound: raise ObjectNotFound({'source': '/data/relationships/event'}, 'Object: not found') else: <IF_STMT> view_kwargs['id'] = user_favourite_event.id else: view_kwargs['id'] = None",if user_favourite_event is not None:,if user_favourite_event:,0.8888942894055136,0.8743414417652072,False
1488,"def close(self): super().close() if not sys.is_finalizing(): for sig in list(self._signal_handlers): self.remove_signal_handler(sig) el<IF_STMT> warnings.warn(f'Closing the loop {self!r} on interpreter shutdown stage, skipping signal handlers removal', ResourceWarning, source=self) self._signal_handlers.clear()",if self._signal_handlers:,if self.shutdown:,0.8791911248301855,0.8645707301556367,False
1489,"def install_script(self, script, install_options=None): try: fname = utils.do_script(script, python_exe=osp.join(self.target, 'python.exe'), architecture=self.architecture, verbose=self.verbose, install_options=install_options) except RuntimeError: <IF_STMT> print('Failed!') raise",if not self.verbose:,if self.verbose:,0.7663743656840311,0.8086627571031982,False
1490,"def GetRouterForUser(self, username): """"""Returns a router corresponding to a given username."""""" for index, router in enumerate(self.routers): router_id = str(index) <IF_STMT> logging.debug('Matched router %s to user %s', router.__class__.__name__, username) return router logging.debug('No router ACL rule match for user %s. Using default router %s', username, self.default_router.__class__.__name__) return self.default_router","if self.auth_manager.CheckPermissions(username, router_id):",if router.username == username:,0.755019138230276,0.8832000938217648,False
1491,"def charset(self): """"""The charset from the content type."""""" header = self.environ.get('CONTENT_TYPE') if header: ct, options = parse_options_header(header) charset = options.get('charset') <IF_STMT> if is_known_charset(charset): return charset return self.unknown_charset(charset) return self.default_charset",if charset:,if charset:,0.6678801743241346,0.8743414417652072,True
1492,def isFinished(self): if self.count > self.epiLen: self.res() return True else: if self.count == 1: self.pertGlasPos(0) <IF_STMT> self.env.reset() self.pertGlasPos(1) self.count += 1 return False,if self.count == self.epiLen / 2 + 1:,elif self.count == 2:,0.6039858732246033,0.7709002428237395,False
1493,"def mtimes_of_files(dirnames: List[str], suffix: str) -> Iterator[float]: for dirname in dirnames: for root, dirs, files in os.walk(dirname): for sfile in files: <IF_STMT> try: yield path.getmtime(path.join(root, sfile)) except OSError: pass",if sfile.endswith(suffix):,if sfile.endswith(suffix):,0.914244546413809,0.8743414417652072,True
1494,"def get_all_hashes(self): event_hashes = [] sample_hashes = [] for a in self.event.attributes: h = None if a.type in ('md5', 'sha1', 'sha256'): h = a.value event_hashes.append(h) elif a.type in ('filename|md5', 'filename|sha1', 'filename|sha256'): h = a.value.split('|')[1] event_hashes.append(h) <IF_STMT> h = a.value.split('|')[1] sample_hashes.append(h) return (event_hashes, sample_hashes)",elif a.type == 'malware-sample':,"elif a.type in ('sample|md5', 'sample|sha256', 'sample'):",0.9204197807316252,0.8375707157974782,False
1495,"def _validate(self, event): if self.type is None: return new = self.value if not isinstance(new, self.type) and new is not None: <IF_STMT> self.value = event.old types = repr(self.type) if isinstance(self.type, tuple) else self.type.__name__ raise ValueError('LiteralInput expected %s type but value %s is of type %s.' % (types, new, type(new).__name__))",if event:,if event.old is not None:,0.9282501477112791,0.8703737209656045,False
1496,"def update_dict(a, b): for key, value in b.items(): <IF_STMT> continue if key not in a: a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",if value is None:,if key == 'default':,0.8373926097253204,0.8474968231198384,False
1497,"def on_pre_save(self, view): extOrClause = '|'.join(s.get('format_on_save_extensions')) extRegex = '\\.(' + extOrClause + ')$' if s.get('format_on_save') and re.search(extRegex, view.file_name()): lints_regions = ['lint-keyword-underline', 'lint-keyword-outline'] for linter in lints_regions: <IF_STMT> return view.run_command('js_format')",if len(view.get_regions(linter)):,if linter.get('format_on_save') and linter.get('format_on_save'):,0.8034723836072045,0.8260567476092244,False
1498,"def readMemory(self, va, size): for mva, mmaxva, mmap, mbytes in self._map_defs: if mva <= va < mmaxva: mva, msize, mperms, mfname = mmap <IF_STMT> raise envi.SegmentationViolation(va) offset = va - mva return mbytes[offset:offset + size] raise envi.SegmentationViolation(va)",if not mperms & MM_READ:,if msize != size:,0.760363922936161,0.8555308664663046,False
1499,"def assertFilepathsEqual(self, p1, p2): if sys.platform == 'win32': <IF_STMT> p1 = [normcase(normpath(x)) for x in p1] p2 = [normcase(normpath(x)) for x in p2] else: assert isinstance(p1, (str, unicode)) p1 = normcase(normpath(p1)) p2 = normcase(normpath(p2)) self.assertEqual(p1, p2)","if isinstance(p1, (list, tuple)):","if isinstance(p1, list):",0.8811904482438838,0.8749766281017177,False
1500,"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) if isdir(path): paths = add_directory_csv_files(path, paths) <IF_STMT> paths.append(path) return paths",elif isfile(path) and path.endswith('.csv'):,elif isdir(path):,0.689237634013879,0.8645707301556367,False
1501,"def _verifySubs(self): for inst in self.subs: <IF_STMT> raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))","if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):","if not isinstance(inst, (_Block, _Instantiator)):",0.8193713366202477,0.7406093667638122,False
1502,"def __annotations_bytes(self): if self.annotations: a = [] for k, v in self.annotations.items(): if len(k) != 4: raise errors.ProtocolError('annotation key must be of length 4') <IF_STMT> k = k.encode('ASCII') a.append(struct.pack('!4sH', k, len(v))) a.append(v) return b''.join(a) return b''","if sys.version_info >= (3, 0):","if isinstance(k, str):",0.7464400305457529,0.8749766281017177,False
1503,"def session(self, profile: str='default', region: str=None) -> boto3.Session: region = self._get_region(region, profile) try: session = self._cache_lookup(self._session_cache, [profile, region], self._boto3.Session, [], {'region_name': region, 'profile_name': profile}) except ProfileNotFound: <IF_STMT> raise session = self._boto3.Session(region_name=region) self._cache_set(self._session_cache, [profile, region], session) return session",if profile != 'default':,if region is None:,0.7343803602186768,0.8555308664663046,False
1504,"def spans_score(gold_spans, system_spans): correct, gi, si = (0, 0, 0) while gi < len(gold_spans) and si < len(system_spans): if system_spans[si].start < gold_spans[gi].start: si += 1 <IF_STMT> gi += 1 else: correct += gold_spans[gi].end == system_spans[si].end si += 1 gi += 1 return Score(len(gold_spans), len(system_spans), correct)",elif gold_spans[gi].start < system_spans[si].start:,elif gold_spans[gi].end > system_spans[si].end:,0.7924731580311217,0.8806615362338783,False
1505,"def to_api(tag, raw_value): try: api_tag, converter = _QL_TO_SC[tag] if tag else ('q', None) except KeyError: <IF_STMT> raise self.error(""Unsupported '%s' tag. Try: %s"" % (tag, ', '.join(SUPPORTED))) return (None, None) else: value = str(converter(raw_value) if converter else raw_value) return (api_tag, value)",if tag not in SUPPORTED:,if tag not in SUPPORTED:,0.7270685390158806,0.845713978670975,True
1506,"def unpack(self, buf): dpkt.Packet.unpack(self, buf) buf = buf[self.__hdr_len__:] if self.type & 128: self.len = 0 self.data = b'' else: <IF_STMT> self.len = struct.unpack('>H', buf[:2])[0] buf = buf[2:] else: self.len = struct.unpack('B', buf[:1])[0] buf = buf[1:] self.data = buf[:self.len]",if self.type == USER_TO_USER:,if self.type & 128:,0.9230868696338559,0.8592377270804451,False
1507,"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): <IF_STMT> return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:,if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:,0.8092151071791922,0.8169276475307028,True
1508,"def _text(bitlist): out = '' for typ, text in bitlist: if not typ: out += text <IF_STMT> out += '\\fI%s\\fR' % text elif typ in ['strong', 'code']: out += '\\fB%s\\fR' % text else: raise ValueError('unexpected tag %r inside text' % (typ,)) out = out.strip() out = re.sub(re.compile('^\\s+', re.M), '', out) return out",elif typ == 'em':,"elif typ in ['strong', 'code']:",0.8506310114995099,0.879962308706789,False
1509,"def process(self, buckets): with self.executor_factory(max_workers=3) as w: futures = {} results = [] for b in buckets: futures[w.submit(self.process_bucket, b)] = b for f in as_completed(futures): <IF_STMT> b = futures[f] self.log.error('error modifying bucket:%s\n%s', b['Name'], f.exception()) results += filter(None, [f.result()]) return results",if f.exception():,if f.exception():,0.9250190974181789,0.9099951253570094,True
1510,"def check_settings(self): if self.settings_dict['TIME_ZONE'] is not None: <IF_STMT> raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because USE_TZ is False."" % self.alias) elif self.features.supports_timezones: raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because its engine handles time zones conversions natively."" % self.alias)",if not settings.USE_TZ:,if self.settings_dict['TIME_ZONE'] == 'USE_TZ':,0.5980051777128801,0.8592377270804451,False
1511,"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: if each: <IF_STMT> key, value = each.split('=', 1) else: key, value = (each, '') result[key] = value namespace.webhook_properties = result",if '=' in each:,if '=' in each:,0.6779408801798597,0.8431339019329497,True
1512,"def _expand_query_values(original_query_list): query_list = [] for key, value in original_query_list: <IF_STMT> query_list.append((key, value)) else: key_fmt = key + '[%s]' value_list = _to_kv_list(value) query_list.extend(((key_fmt % k, v) for k, v in value_list)) return query_list","if isinstance(value, basestring):","if isinstance(value, dict):",0.7153642531134131,0.8635707684233572,False
1513,"def tags(): """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}."""""" tags = {} for n, c in list_refs(): if n.startswith('refs/tags/'): name = n[10:] <IF_STMT> tags[c] = [] tags[c].append(name) return tags",if not c in tags:,if c not in tags:,0.7483146674068553,0.8200123297196334,False
1514,"def test_colorspiral(self): """"""Set of 625 colours, with jitter, using get_colors()."""""" boxedge = 20 boxes_per_row = 25 rows = 0 for i, c in enumerate(get_colors(625)): self.c.setFillColor(c) x1 = boxedge * (i % boxes_per_row) y1 = rows * boxedge self.c.rect(x1, y1, boxedge, boxedge, fill=1, stroke=0) <IF_STMT> rows += 1 self.finish()",if not (i + 1) % boxes_per_row:,if i % boxes_per_row == 0:,0.7931832993682505,0.8534652100396689,False
1515,"def oldest_pending_update_in_days(): """"""Return the datestamp of the oldest pending update"""""" pendingupdatespath = os.path.join(prefs.pref('ManagedInstallDir'), 'UpdateNotificationTracking.plist') try: pending_updates = FoundationPlist.readPlist(pendingupdatespath) except FoundationPlist.NSPropertyListSerializationException: return 0 oldest_date = now = NSDate.date() for category in pending_updates: for name in pending_updates[category]: this_date = pending_updates[category][name] <IF_STMT> oldest_date = this_date return now.timeIntervalSinceDate_(oldest_date) / (24 * 60 * 60)",if this_date < oldest_date:,if this_date > oldest_date:,0.836261937166973,0.8923575006167597,False
1516,"def _try_read_gpg(path): path = os.path.expanduser(path) cmd = _gpg_cmd() + [path] log.debug('gpg cmd: %s', cmd) try: p = subprocess.Popen(cmd, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE) except OSError as e: log.error(""cannot decode %s with command '%s' (%s)"", path, ' '.join(cmd), e) else: out, err = p.communicate() <IF_STMT> log.error(err.decode(errors='replace').strip()) return None return out.decode(errors='replace')",if p.returncode != 0:,if err:,0.684812247169935,0.9237460349978159,False
1517,"def sort_nested_dictionary_lists(d): for k, v in d.items(): <IF_STMT> for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v, list):","if isinstance(v, list):",0.9077318845729747,0.8592899528284996,True
1518,"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, '__call__'): <IF_STMT> args = [point, index] else: args = [point] if pass_widget: args.append(widget) try_callback(callback, *args) return",if num > 1:,if index:,0.8947020097204058,0.8645707301556367,False
1519,"def _add_cs(master_cs, sub_cs, prefix, delimiter='.', parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) <IF_STMT> new_parameter.name = prefix elif not prefix == '': new_parameter.name = '{}{}{}'.format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",if new_parameter.name == '':,if delimiter == '':,0.6938771932915365,0.8555308664663046,False
1520,"def tearDown(self): """"""Shutdown the server."""""" try: <IF_STMT> self.server.stop() if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:,if self.server:,0.5103819074241628,0.7241577342575828,True
1521,"def app_uninstall_all(self, excludes=[], verbose=False): """"""Uninstall all apps"""""" our_apps = ['com.github.uiautomator', 'com.github.uiautomator.test'] output, _ = self.shell(['pm', 'list', 'packages', '-3']) pkgs = re.findall('package:([^\\s]+)', output) pkgs = set(pkgs).difference(our_apps + excludes) pkgs = list(pkgs) for pkg_name in pkgs: <IF_STMT> print('uninstalling', pkg_name, ' ', end='', flush=True) ok = self.app_uninstall(pkg_name) if verbose: print('OK' if ok else 'FAIL') return pkgs",if verbose:,if verbose:,0.9398494656228501,0.9325718821645923,True
1522,"def httpapi(self, arg, opts): sc = HttpAPIStatsCollector() headers = ['#Item', 'Value'] table = [] for k, v in sc.get().getStats().items(): if isinstance(v, dict): v = json.dumps(v) row = [] row.append('#%s' % k) <IF_STMT> row.append(formatDateTime(v)) else: row.append(v) table.append(row) self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))",if k[-3:] == '_at':,"if isinstance(v, datetime):",0.7984140922886552,0.8875087479151215,False
1523,"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = '' for line in string.split(entry, '\n'): <IF_STMT> GN = string.strip(line[5:]) if GN[-1] == '.': GN = GN[0:-1] return GN if line[0:2] == '//': break return GN",if line[0:5] == 'GN   ':,if line.startswith('GN'):,0.9162184044609855,0.9184043388013005,False
1524,"def replace_dir_vars(path, d): """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})"""""" dirvars = {} for var in sorted(list(d.keys()), key=len): if var.endswith('dir') and var.lower() == var: value = d.getVar(var) <IF_STMT> dirvars[value] = var for dirpath in sorted(list(dirvars.keys()), reverse=True): path = path.replace(dirpath, '${%s}' % dirvars[dirpath]) return path",if value.startswith('/') and (not '\n' in value) and (value not in dirvars):,if value:,0.8182615384065681,0.926934323706186,False
1525,"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, 'r') as f: lines = f.readlines() if len(lines) < 1: return with open(source, 'w') as f: <IF_STMT> f.write(lines[0]) for line in lines[1:]: f.write(line)",if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):,if len(lines) > 1:,0.9448403763361795,0.9036816878108535,False
1526,"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: <IF_STMT> all_plugins += [plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",if name is None:,if name in self.plugins:,0.8467894808320586,0.8923575006167597,False
1527,"def test_query_level(self): """"""Tests querying at a level other than max"""""" l2 = set() for p in self.tile_paths: l2.add(p[0:2]) for path in iterate_base4(2): <IF_STMT> self.assertTrue(self.tree.query_path(path)) else: self.assertFalse(self.tree.query_path(path)) self.assertTrue(self.tree.query_path((0,))) self.assertTrue(self.tree.query_path((1,))) self.assertTrue(self.tree.query_path((2,))) self.assertFalse(self.tree.query_path((3,)))",if path in l2:,if path in l2:,0.9077280889894528,0.8228500218338367,True
1528,"def program_exists(name): paths = (os.getenv('PATH') or os.defpath).split(os.pathsep) for p in paths: fn = '%s/%s' % (p, name) <IF_STMT> return not os.path.isdir(fn) and os.access(fn, os.X_OK)",if os.path.exists(fn):,if os.path.exists(fn):,0.7912218375555603,0.8466657105524215,True
1529,"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) <IF_STMT> keywargs.update(arg) elif patching.new is DEFAULT: extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",if patching.attribute_name is not None:,if patching.new is KEY:,0.731231958006856,0.8336104423443033,False
1530,"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) <IF_STMT> rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.ENABLED:,if k == neighbors.NEIGH_ENABLED:,0.9104849111191979,0.8105932471967202,False
1531,"def calcUniqueStates(self): self.uniqueStates = {} for k in self.holdUniqueStates.keys(): v = self.holdUniqueStates[k] <IF_STMT> self.uniqueStates[k] = v.keys()[0] log.debug('Map style [%s] to state [%s]', k, v.keys()[0]) log.debug('Style [%s] maps to states [%s]', k, ', '.join(v.keys())) self.holdUniqueStates = None",if len(v.keys()) == 1:,"if isinstance(v, dict):",0.8206181013886692,0.8749766281017177,False
1532,"def init_logger(): configured_loggers = [log_config.get('root', {})] + [logger for logger in log_config.get('loggers', {}).values()] used_handlers = {handler for log in configured_loggers for handler in log.get('handlers', [])} for handler_id, handler in list(log_config['handlers'].items()): <IF_STMT> del log_config['handlers'][handler_id] elif 'filename' in handler.keys(): filename = handler['filename'] logfile_path = Path(filename).expanduser().resolve() handler['filename'] = str(logfile_path) logging.config.dictConfig(log_config)",if handler_id not in used_handlers:,if handler_id in used_handlers:,0.8325483428431532,0.8856327184319047,False
1533,"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: <IF_STMT> selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines",if self._args.host and self._args.host == machine.name:,"if self.tags and self._tags_match(machine.tags, self.tags):",0.7762377418372727,0.7765145040967655,False
1534,def init(self): r = self.get_redis() if r: key = 'pocsuite_target' info_msg = '[PLUGIN] try fetch targets from redis...' logger.info(info_msg) targets = r.get(key) count = 0 if targets: for target in targets: <IF_STMT> count += 1 info_msg = '[PLUGIN] get {0} target(s) from redis'.format(count) logger.info(info_msg),if self.add_target(target):,if target.is_active():,0.937403662376129,0.9184043388013005,False
1535,def tearDown(self): suffix = str(os.getgid()) cli = monitoring_v3.MetricServiceClient() for md in cli.list_metric_descriptors('projects/{}'.format(PROJECT)): <IF_STMT> try: cli.delete_metric_descriptor(md.name) except Exception: pass,if 'OpenCensus' in md.name and suffix in md.name:,if md.name.endswith(suffix):,0.8604989508153104,0.7912619863720214,False
1536,"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) if c.IsOk(): self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE <IF_STMT> self._colourSelection = i",if c == curr:,if curr == c:,0.879033748337565,0.8474968231198384,False
1537,"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) <IF_STMT> feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",if feature:,if feature:,0.6933561176279832,0.926934323706186,True
1538,"def _get_data_from_buffer(obj): try: view = memoryview(obj) except TypeError: <IF_STMT> view = memoryview(buffer(obj)) warnings.warn('using old buffer interface to unpack %s; this leads to unpacking errors if slicing is used and will be removed in a future version' % type(obj), RuntimeWarning, stacklevel=3) else: raise if view.itemsize != 1: raise ValueError('cannot unpack from multi-byte object') return view",if PY2:,"if isinstance(obj, bytes):",0.6342561742552802,0.9167056528641923,False
1539,"def import_modules(modules, safe=True): """"""Safely import a list of *modules*"""""" all = [] for mname in modules: if mname.endswith('.*'): to_load = expand_star(mname) else: to_load = [mname] for module in to_load: try: all.append(import_module(module)) except ImportError: <IF_STMT> raise return all",if not safe:,if safe:,0.9013471413117189,0.9024521756077707,False
1540,"def pack(types, *args): if len(types) != len(args): raise Exception('number of arguments does not match format string') port = StringIO() for type, value in zip(types, args): if type == 'V': write_vuint(port, value) <IF_STMT> write_vint(port, value) elif type == 's': write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",elif type == 'v':,elif type == 'I':,0.9347438341998876,0.9001816649635144,False
1541,"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): <IF_STMT> os.mkdir(folder) init_path = join(folder, '__init__.py') if not exists(init_path): create_file(init_path)",if not exists(folder):,if not exists(folder):,0.7627315028980035,0.8390782502060267,True
1542,"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] <IF_STMT> raise ValueError(f'Unknown node type tag: {node_type}.') node_specific_config = self.available_node_types[node_type] if fields_key in node_specific_config: fields = node_specific_config[fields_key] return fields",if node_type not in self.available_node_types:,if node_type not in self.available_node_types:,0.7303842910872876,0.845713978670975,True
1543,"def _maybe_fix_sequence_in_union(aliases: List[Alias], typecst: cst.SubscriptElement) -> cst.SubscriptElement: slc = typecst.slice if isinstance(slc, cst.Index): val = slc.value <IF_STMT> return cst.ensure_type(typecst.deep_replace(val, _get_clean_type_from_subscript(aliases, val)), cst.SubscriptElement) return typecst","if isinstance(val, cst.Subscript):","if isinstance(val, cst.Union):",0.731529572283573,0.8120341702859789,False
1544,"def cancel_download(self, downloads): if isinstance(downloads, Download): downloads = [downloads] for download in downloads: <IF_STMT> self.cancel_current_download() else: self.__paused = True new_queue = queue.Queue() while not self.__queue.empty(): queued_download = self.__queue.get() if download == queued_download: download.cancel() else: new_queue.put(queued_download) self.__queue = new_queue self.__paused = False",if download == self.__current_download:,if download.is_current():,0.7017319528846666,0.9122561819614461,False
1545,"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) if account.discriminator == 'easaccount': create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) <IF_STMT> set_labels_for_imapuids(account, db_session) db_session.commit()",if account.discriminator == 'gmailaccount':,if account.discriminator == 'imapuids':,0.7869627033598887,0.8228500218338367,False
1546,"def __init__(self, fmt=None, *args): if not isinstance(fmt, BaseException): Error.__init__(self, fmt, *args) else: e = fmt cls = e.__class__ fmt = '%s.%s: %s' % (cls.__module__, cls.__name__, e) tb = sys.exc_info()[2] <IF_STMT> fmt += '\n' fmt += ''.join(traceback.format_tb(tb)) Error.__init__(self, fmt)",if tb:,if tb:,0.7101557121198309,0.9051034981560222,True
1547,"def setLabel(self, label): if label is None: <IF_STMT> self.label.scene().removeItem(self.label) self.label = None else: if self.label is None: self.label = TextItem() self.label.setParentItem(self) self.label.setText(label) self._updateLabel()",if self.label is not None:,if self.label is not None:,0.7240428182469193,0.7406093667638122,True
1548,"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) <IF_STMT> self.handle_request() if self.event is not None and self.event.is_set(): break",if rd:,if rd:,0.8733129239222457,0.8466657105524215,True
1549,"def generateCompressedFile(inputfile, outputfile, formatstring): try: <IF_STMT> in_file = open(inputfile, 'rb') in_data = in_file.read() out_file = open(inputfile + '.xz', 'wb') out_file.write(xz.compress(in_data)) in_file.close() out_file.close() else: tarout = tarfile.open(outputfile, formatstring) tarout.add(inputfile, arcname=os.path.basename(inputfile)) tarout.close() except Exception as e: print(e) return False return True",if formatstring == 'w:xz':,if os.path.isfile(inputfile):,0.7436705733866792,0.9076141716697395,False
1550,"def _datastore_get_handler(signal, sender, keys, **kwargs): txn = current_transaction() if txn: for key in keys: <IF_STMT> raise PreventedReadError('Attempted to read key (%s:%s) inside a transaction where it was marked protected' % (key.kind(), key.id_or_name())) txn._fetched_keys.update(set(keys))",if key in txn._protected_keys:,if key.kind() in txn._fetched_keys:,0.8952763083086865,0.8385130047130208,False
1551,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_access_token(d.getPrefixedString()) continue if tt == 16: self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.6067590041327626,0.7965020533851944,True
1552,"def write_vuint(port, x): if x < 0: raise Exception('vuints must not be negative') elif x == 0: port.write('\x00') else: while x: seven_bits = x & 127 x >>= 7 <IF_STMT> port.write(chr(128 | seven_bits)) else: port.write(chr(seven_bits))",if x:,if x & 128:,0.6508767864488934,0.8474968231198384,False
1553,"def _expand_srcs(self): """"""Expand src to [(src, full_path)]"""""" result = [] for src in self.srcs: full_path = self._source_file_path(src) <IF_STMT> full_path = self._target_file_path(src) result.append((src, full_path)) return result",if not os.path.exists(full_path):,if not full_path:,0.799099819974301,0.8196189957582152,False
1554,def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/ops'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:,if 'init' not in item.keywords:,0.6137372760254911,0.6859238121837058,True
1555,"def set_shape(self, shape): """"""Sets a shape."""""" if self._shape is not None: logger.warning('Modifying the shape of Placeholder ""%s"".', self.name) if not isinstance(shape, (list, tuple)): shape = (shape,) shape = tuple((x if x != 'None' else None for x in shape)) for x in shape: <IF_STMT> raise ParsingError('All entries in ""shape"" must be integers, or in special cases None. Shape is: {}'.format(shape)) self._shape = shape","if not isinstance(x, (int, type(None))):","if not isinstance(x, int):",0.8983204143012519,0.914208565914368,False
1556,"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if ':' in line: left, right = line.split(':', 1) left = left.strip().lower() right = right.strip() <IF_STMT> if cant_be_number: if not right.isdigit(): return right else: return right return None",if left == field_name and len(right) > 0:,if left.startswith(field_name):,0.7224739248071563,0.9164531641034833,False
1557,"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) if value is None: <IF_STMT> raise ValidationError(_('%(attr)s attribute cannot be blank') % {'attr': attribute.code}) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError(_('%(attr)s attribute %(err)s') % {'attr': attribute.code, 'err': e})",if attribute.required:,if not attribute.is_blank():,0.7022795154761171,0.8902579342581529,False
1558,"def append(self, s): buf = self.buf if buf is None: strbuf = self.strbuf <IF_STMT> self.strbuf = strbuf + s return buf = self._create_buffer() buf.append(s) sz = buf.__len__() if not self.overflowed: if sz >= self.overflow: self._set_large_buffer()",if len(strbuf) + len(s) < STRBUF_LIMIT:,if strbuf:,0.8176955662949297,0.8966773400768917,False
1559,"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import RequiredArgumentMissingError, MutuallyExclusiveArgumentError valid_combs = 'only --account-name, --name / --name / --name, --by-subscription is valid' if namespace.account_name is not None: <IF_STMT> raise MutuallyExclusiveArgumentError(valid_combs) if namespace.name is None: raise RequiredArgumentMissingError('--name is also required') if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError('--name is also required')",if namespace.by_subscription is not None:,if namespace.by_subscription is not None:,0.9261566065155191,0.8753524256584351,True
1560,"def Handle(self, args, context=None): for client_id in args.client_ids: cid = str(client_id) data_store.REL_DB.RemoveClientLabels(cid, context.username, args.labels) labels_to_remove = set(args.labels) existing_labels = data_store.REL_DB.ReadClientLabels(cid) for label in existing_labels: labels_to_remove.discard(label.name) <IF_STMT> idx = client_index.ClientIndex() idx.RemoveClientLabels(cid, labels_to_remove)",if labels_to_remove:,if len(labels_to_remove) > 0:,0.8918165045053482,0.828399516355805,False
1561,"def delete_snapshot(self, snapshot): snap_name = self._get_snap_name(snapshot['id']) LOG.debug('Deleting snapshot (%s)', snapshot['id']) self.client_login() try: self.client.delete_snapshot(snap_name, self.backend_type) except exception.DotHillRequestError as ex: <IF_STMT> return LOG.exception('Deleting snapshot %s failed', snapshot['id']) raise exception.Invalid(ex) finally: self.client_logout()",if 'The volume was not found on this system.' in ex.args:,if ex.response['Error']['Code'] == 'InvalidSnapshot':,0.6406908147721444,0.8169276475307028,False
1562,def jobs(self): total_processed = 0 for jobEntity in self.jobItems.query_entities(): yield AzureJob.fromEntity(jobEntity) total_processed += 1 <IF_STMT> logger.debug('Processed %d total jobs' % total_processed) logger.debug('Processed %d total jobs' % total_processed),if total_processed % 1000 == 0:,if total_processed % 100 == 0:,0.7987156787385281,0.7510201430702309,False
1563,def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() <IF_STMT> self.stop(),if self.counter == self.count:,elif dt < self.timeout:,0.6698747335718637,0.8693386622895124,False
1564,"def get_instance(cls, pool_size=None): if cls._instance is not None: return cls._instance with cls._SINGLETON_LOCK: <IF_STMT> cls._instance = cls(ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size) return cls._instance",if cls._instance is None:,if cls._instance is None:,0.8015958620454258,0.7801270245332924,True
1565,"def set_state(self, state): if self._inhibit_play: <IF_STMT> self.__set_inhibit_play(False) self.bin.set_state(state) return self._wanted_state = state else: self.bin.set_state(state)","if state not in (Gst.State.PLAYING, Gst.State.PAUSED):",if state == self._wanted_state:,0.7859315290722461,0.630190855592386,False
1566,"def seen_add(options): seen_name = options.add_value if is_imdb_url(seen_name): console('IMDB url detected, try to parse ID') imdb_id = extract_id(seen_name) <IF_STMT> seen_name = imdb_id else: console('Could not parse IMDB ID') db.add(seen_name, 'cli_add', {'cli_add': seen_name}) console('Added %s as seen. This will affect all tasks.' % seen_name)",if imdb_id:,if imdb_id:,0.6275969558198776,0.9144061946646023,True
1567,"def test_204_invalid_content_length(self): with ExpectLog(gen_log, '.*Response with code 204 should not have body'): response = self.fetch('/?error=1') if not self.http1: self.skipTest('requires HTTP/1.x') <IF_STMT> self.skipTest('curl client accepts invalid headers') self.assertEqual(response.code, 599)",if self.http_client.configured_class != SimpleAsyncHTTPClient:,if not response.headers:,0.8946894952048579,0.8390782502060267,False
1568,"def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> None: src_class = target.cls_model id_ = target.datasource_id if id_: ds = db.session.query(src_class).filter_by(id=int(id_)).first() <IF_STMT> target.perm = ds.perm target.schema_perm = ds.schema_perm",if ds:,if ds:,0.6489477442655319,0.8645707301556367,True
1569,"def on_modified_async(self, view): if self.is_command_line(view): <IF_STMT> view.run_command('text_pastry_selection_preview')","if view.size() > 6 and view.substr(sublime.Region(0, 6)).lower() == 'search':",if view.command('text_pastry_selection_preview'):,0.4384856142750722,0.5410822690539396,False
1570,"def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text): """"""Returns tokenized answer spans that better match the annotated answer."""""" tok_answer_text = ' '.join(tokenizer.tokenize(orig_answer_text)) for new_start in range(input_start, input_end + 1): for new_end in range(input_end, new_start - 1, -1): text_span = ' '.join(doc_tokens[new_start:new_end + 1]) <IF_STMT> return (new_start, new_end) return (input_start, input_end)",if text_span == tok_answer_text:,if text_span.startswith(tok_answer_text):,0.7542788351823955,0.9253742688467129,False
1571,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_url(d.getPrefixedString()) continue if tt == 18: self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.649883111136198,0.8592377270804451,True
1572,"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: <IF_STMT> if obj['id']: obj['resourceGroup'] = _parse_id(obj['id'])['resource-group'] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != 'sourceVault': _add_resource_group(obj[item_key])",if 'resourcegroup' not in [x.lower() for x in obj.keys()]:,if 'resourceGroup' not in obj:,0.6608196409021994,0.8200123297196334,False
1573,"def build(opt): dpath = os.path.join(opt['datapath'], DECODE) version = DECODE_VERSION if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):,if build_data.built(dpath):,0.9080842033252996,0.8787142254774354,True
1574,"def toterminal(self, tw): last_style = None for i, entry in enumerate(self.reprentries): <IF_STMT> tw.line('') entry.toterminal(tw) if i < len(self.reprentries) - 1: next_entry = self.reprentries[i + 1] if entry.style == 'long' or (entry.style == 'short' and next_entry.style == 'long'): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",if entry.style == 'long':,if entry.style == 'long':,0.9145314534766,0.8692960007731574,True
1575,"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): <IF_STMT> found += 1 if found == 2: if division in '\n'.join(lines): break lines.insert(i + 1, '') lines.insert(i + 2, division) break return '\n'.join(lines)","if line.startswith('""""""'):",if line.startswith(division):,0.9314942243646044,0.9144061946646023,False
1576,def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: if self.base.conf.strict: <IF_STMT> raise e if e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS: raise e logger.error(str(e)),if e.no_match_group_specs or e.error_group_specs:,if e.module_name != 'libdnf.module.ModulePackageContainer':,0.8804534291966162,0.760856626273165,False
1577,"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len('hello'), len(base64mime.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <IF_STMT> bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len('x' * size), bsize)",elif size <= 9:,elif size <= 7:,0.8578714206177079,0.8964173245779284,False
1578,"def is_valid(self): """"""Determines whether file is valid for this reader"""""" blocklist = self.open() valid = True for line in blocklist: line = decode_bytes(line) <IF_STMT> try: start, end = self.parse(line) if not re.match('^(\\d{1,3}\\.){4}$', start + '.') or not re.match('^(\\d{1,3}\\.){4}$', end + '.'): valid = False except Exception: valid = False break blocklist.close() return valid",if not self.is_ignored(line):,if line:,0.7639822376787175,0.9325718821645923,False
1579,"def next(self): while self.index < len(self.data): uid = self._read_next_word() dont_care = self._read_next_word() entry = self._read_next_string() total_size = int(4 + 4 + len(entry)) count = int(total_size / self.SIZE) if count == 0: mod = self.SIZE - total_size else: mod = self.SIZE - int(total_size - count * self.SIZE) <IF_STMT> remainder = self._read_next_block(mod) yield (uid, entry)",if mod > 0:,if dont_care:,0.8404544365970505,0.9325718821645923,False
1580,"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) if param.type: parts.append(param.type) out += [' : '.join(parts)] <IF_STMT> out += self._str_indent(param.desc) out += [''] return out",if param.desc and ''.join(param.desc).strip():,if param.desc:,0.9390122335459555,0.9051034981560222,False
1581,"def assert_backend(self, expected_translated, language='cs'): """"""Check that backend has correct data."""""" translation = self.get_translation(language) translation.commit_pending('test', None) store = translation.component.file_format_cls(translation.get_filename(), None) messages = set() translated = 0 for unit in store.content_units: id_hash = unit.id_hash self.assertFalse(id_hash in messages, 'Duplicate string in in backend file!') <IF_STMT> translated += 1 self.assertEqual(translated, expected_translated, 'Did not found expected number of translations ({} != {}).'.format(translated, expected_translated))",if unit.is_translated():,if unit.is_translated:,0.8530987282131689,0.938501942261528,False
1582,"def status(self, name, error='No matching script logs found'): with self.script_lock: <IF_STMT> return self.script_running[1:] elif self.script_last and self.script_last[1] == name: return self.script_last[1:] else: raise ValueError(error)",if self.script_running and self.script_running[1] == name:,if self.script_running and self.script_running[1] == name:,0.8337582337108639,0.7232927766551176,True
1583,"def dict_no_value_from_proto_list(obj_list): d = dict() for item in obj_list: possible_dict = json.loads(item.value_json) <IF_STMT> logger.warning(""key '{}' has no 'value' attribute"".format(item.key)) continue d[item.key] = possible_dict['value'] return d","if not isinstance(possible_dict, dict) or 'value' not in possible_dict:",if 'value' not in possible_dict:,0.6427287198781064,0.759907656827929,False
1584,"def visit(self, node): """"""dispatcher on node's class/bases name."""""" cls = node.__class__ try: visitmethod = self.cache[cls] except KeyError: for subclass in cls.__mro__: visitmethod = getattr(self, subclass.__name__, None) <IF_STMT> break else: visitmethod = self.__object self.cache[cls] = visitmethod visitmethod(node)",if visitmethod is not None:,if visitmethod is None:,0.7798885344365358,0.8516228624291206,False
1585,"def _get_adapter(mcls, reversed_mro: Tuple[type, ...], collection: Dict[Any, Dict[type, Adapter]], kwargs: Dict[str, Any]) -> Optional[Adapter]: registry_key = mcls.get_registry_key(kwargs) adapters = collection.get(registry_key) if adapters is None: return None result = None seen: Set[Adapter] = set() for base in reversed_mro: for adaptee, adapter in adapters.items(): found = mcls._match_adapter(base, adaptee, adapter) <IF_STMT> result = found seen.add(found) return result",if found and found not in seen:,if found not in seen:,0.8320699849944848,0.8842429471030616,False
1586,"def test_pt_BR_rg(self): for _ in range(100): to_test = self.fake.rg() <IF_STMT> assert re.search('^\\d{8}X', to_test) else: assert re.search('^\\d{9}$', to_test)",if 'X' in to_test:,if sys.platform == 'win32':,0.5540129346802121,0.693395566222006,False
1587,"def get_user_extra_data_by_client_id(self, client_id, username): extra_data = {} current_client = self.clients.get(client_id, None) if current_client: for readable_field in current_client.get_readable_fields(): attribute = list(filter(lambda f: f['Name'] == readable_field, self.users.get(username).attributes)) <IF_STMT> extra_data.update({attribute[0]['Name']: attribute[0]['Value']}) return extra_data",if len(attribute) > 0:,if attribute:,0.915248234945853,0.8787142254774354,False
1588,"def augment(self, resources): super().augment(resources) for r in resources: md = r.get('SAMLMetadataDocument') <IF_STMT> continue root = sso_metadata(md) r['IDPSSODescriptor'] = root['IDPSSODescriptor'] return resources",if not md:,if not md:,0.6973672854773493,0.7848518349390632,True
1589,"def __init__(self, mode=0, decode=None): self.regex = self.REGEX[mode] self.decode = decode if decode: self.header = _('### This log has been decoded with automatic search pattern\n### If some paths are not decoded you can manually decode them with:\n') self.header += ""### 'backintime --quiet "" <IF_STMT> self.header += '--profile ""%s"" ' % decode.config.profileName() self.header += ""--decode <path>'\n\n"" else: self.header = ''",if int(decode.config.currentProfile()) > 1:,if decode.config:,0.6541285638798413,0.938501942261528,False
1590,"def _get_dynamic_attr(self, attname, obj, default=None): try: attr = getattr(self, attname) except AttributeError: return default if callable(attr): try: code = six.get_function_code(attr) except AttributeError: code = six.get_function_code(attr.__call__) <IF_STMT> return attr(obj) else: return attr() return attr",if code.co_argcount == 2:,if code == 'dynamic':,0.6725081179512933,0.8385130047130208,False
1591,"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 <IF_STMT> continue while i + 1 < len(tokens) and tokens[i] == ('op', '.') and (tokens[i + 1][0] == 'id'): token += '.' + tokens[i + 1][1] i += 2 if token == '': continue if token in pykeywords: continue if token[0] in '.0123456789': continue yield token",if tokentype != 'id':,if tokentype != 'op':,0.8415654370977608,0.9192507668025022,False
1592,"def _add_disk_config(self, context, images): for image in images: metadata = image['metadata'] <IF_STMT> raw_value = metadata[INTERNAL_DISK_CONFIG] value = utils.bool_from_str(raw_value) image[API_DISK_CONFIG] = disk_config_to_api(value)",if INTERNAL_DISK_CONFIG in metadata:,if internal_DISK_CONFIG in metadata:,0.6975563577611701,0.7498810286408993,False
1593,"def test_edgeql_expr_valid_setop_07(self): expected_error_msg = 'cannot be applied to operands' for val in get_test_values(): query = f'SELECT 1 IF {val} ELSE 2;' <IF_STMT> await self.assert_query_result(query, [1]) else: with self.assertRaisesRegex(edgedb.QueryError, expected_error_msg, msg=query): async with self.con.transaction(): await self.con.execute(query)",if val == '<bool>True':,if val == '1':,0.7341675090382558,0.8474968231198384,False
1594,"def get_all_url_infos() -> Dict[str, UrlInfo]: """"""Returns dict associating URL to UrlInfo."""""" url_infos = {} for path in _checksum_paths().values(): dataset_url_infos = load_url_infos(path) for url, url_info in dataset_url_infos.items(): <IF_STMT> raise AssertionError('URL {} is registered with 2+ distinct size/checksum tuples. {} vs {}'.format(url, url_info, url_infos[url])) url_infos.update(dataset_url_infos) return url_infos","if url_infos.get(url, url_info) != url_info:",if len(url_infos) != 2:,0.7909066168704809,0.8806615362338783,False
1595,"def global_fixes(): """"""Yield multiple (code, function) tuples."""""" for function in list(globals().values()): <IF_STMT> arguments = _get_parameters(function) if arguments[:1] != ['source']: continue code = extract_code_from_function(function) if code: yield (code, function)",if inspect.isfunction(function):,"if isinstance(function, ast.Function):",0.8961128766363374,0.8390782502060267,False
1596,"def createSocket(self): skt = Port.createSocket(self) if self.listenMultiple: skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) <IF_STMT> skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) return skt","if hasattr(socket, 'SO_REUSEPORT'):",if self.listen:,0.833395886959039,0.762465858623486,False
1597,"def _asStringList(self, sep=''): out = [] for item in self._toklist: if out and sep: out.append(sep) <IF_STMT> out += item._asStringList() else: out.append(str(item)) return out","if isinstance(item, ParseResults):","elif isinstance(item, _StringList):",0.8792107502721447,0.80377750806414,False
1598,"def parse_c_comments(lexer, tok, ntok): if tok != '/' or ntok != '*': return False quotes = lexer.quotes lexer.quotes = '' while True: tok = lexer.get_token() ntok = lexer.get_token() <IF_STMT> lexer.quotes = quotes break else: lexer.push_token(ntok) return True",if tok == '*' and ntok == '/':,if tok == '/' and ntok == '*':,0.8131695256308586,0.7749516243532728,False
1599,"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) <IF_STMT> v.append(sibling) if sibling.getFirstChild(): sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",if c2:,if c2:,0.9267186842285633,0.8966773400768917,True
1600,"def __view_beside(self, onsideof, **kwargs): bounds = self.info['bounds'] min_dist, found = (-1, None) for ui in UiObject(self.session, Selector(**kwargs)): dist = onsideof(bounds, ui.info['bounds']) <IF_STMT> min_dist, found = (dist, ui) return found",if dist >= 0 and (min_dist < 0 or dist < min_dist):,if dist < min_dist:,0.8653570034052335,0.8169276475307028,False
1601,"def __eq__(self, other): if isinstance(other, numeric_range): empty_self = not bool(self) empty_other = not bool(other) <IF_STMT> return empty_self and empty_other else: return self._start == other._start and self._step == other._step and (self._get_by_index(-1) == other._get_by_index(-1)) else: return False",if empty_self or empty_other:,if self is None or other is None:,0.8437560375175317,0.7637260583416604,False
1602,"def _buffered_generator(self, size): buf = [] c_size = 0 push = buf.append while 1: try: while c_size < size: c = next(self._gen) push(c) if c: c_size += 1 except StopIteration: <IF_STMT> return yield concat(buf) del buf[:] c_size = 0",if not c_size:,if c_size >= size:,0.9172857067218491,0.8627586293513119,False
1603,"def connect(self): with self._conn_lock: <IF_STMT> raise Exception('Error, database not properly initialized before opening connection') with self.exception_wrapper(): self.__local.conn = self._connect(self.database, **self.connect_kwargs) self.__local.closed = False self.initialize_connection(self.__local.conn)",if self.deferred:,if self.__local.conn is None:,0.8728758986722558,0.7801270245332924,False
1604,"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): <IF_STMT> subst[name] = var elif subst[name] is not var: subst[name].PasteVariable(var) return subst",if name not in subst:,if name not in subst:,0.788174192947519,0.7839800200378249,True
1605,"def remove(self, tag): """"""Removes a tag recursively from all containers."""""" new_contents = [] self.content_size = 0 for element in self.contents: if element.name != tag: new_contents.append(element) <IF_STMT> element.remove(tag) self.content_size += element.size() self.contents = new_contents","if isinstance(element, Container):","if isinstance(element, Container):",0.6666805007890658,0.8635707684233572,True
1606,"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): if isinstance(prop_value, dict) and prop_value: func_name = list(prop_value.keys())[0] <IF_STMT> func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props",if func_name.startswith('_'):,if func_name in self.fake_func_mapping:,0.8236360754417799,0.8661072626070159,False
1607,"def visit_try_stmt(self, o: 'mypy.nodes.TryStmt') -> str: a = [o.body] for i in range(len(o.vars)): a.append(o.types[i]) <IF_STMT> a.append(o.vars[i]) a.append(o.handlers[i]) if o.else_body: a.append(('Else', o.else_body.body)) if o.finally_body: a.append(('Finally', o.finally_body.body)) return self.dump(a, o)",if o.vars[i]:,if o.handlers:,0.9189965658730899,0.8696398662122882,False
1608,"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False <IF_STMT> return False elif isinstance(v, _bytes): return False return True","elif isinstance(i, _bytes):","elif isinstance(i, unicode):",0.9359840560158037,0.9167056528641923,False
1609,"def msg_ser(inst, sformat, lev=0): if sformat in ['urlencoded', 'json']: if isinstance(inst, Message): res = inst.serialize(sformat, lev) else: res = inst elif sformat == 'dict': if isinstance(inst, Message): res = inst.serialize(sformat, lev) <IF_STMT> res = inst elif isinstance(inst, str): res = inst else: raise MessageException('Wrong type: %s' % type(inst)) else: raise PyoidcError('Unknown sformat', inst) return res","elif isinstance(inst, dict):","elif isinstance(inst, dict):",0.8932628111314213,0.9182210682909737,True
1610,"def start_container_if_stopped(self, container, attach_logs=False, quiet=False): if not container.is_running: <IF_STMT> log.info('Starting %s' % container.name) if attach_logs: container.attach_log_stream() return self.start_container(container)",if not quiet:,if quiet:,0.8449678444172898,0.7912619863720214,False
1611,"def layer_op(self, input_image, mask=None): if not isinstance(input_image, dict): self._set_full_border(input_image) input_image = np.pad(input_image, self.full_border, mode=self.mode) return (input_image, mask) for name, image in input_image.items(): self._set_full_border(image) <IF_STMT> tf.logging.warning('could not pad, dict name %s not in %s', name, self.image_name) continue input_image[name] = np.pad(image, self.full_border, mode=self.mode) return (input_image, mask)",if name not in self.image_name:,if name not in self.image_name:,0.8278475608581536,0.8591169759078797,True
1612,"def __Suffix_Noun_Step2b(self, token): for suffix in self.__suffix_noun_step2b: <IF_STMT> token = token[:-2] self.suffix_noun_step2b_success = True break return token",if token.endswith(suffix) and len(token) >= 5:,if token.endswith(suffix):,0.7777792714287889,0.7778111223054219,False
1613,"def __projectBookmark(widget, location): script = None while widget is not None: <IF_STMT> script = widget.scriptNode() if isinstance(script, Gaffer.ScriptNode): break widget = widget.parent() if script is not None: p = script.context().substitute(location) if not os.path.exists(p): try: os.makedirs(p) except OSError: pass return p else: return os.getcwd()","if hasattr(widget, 'scriptNode'):",if widget.isVisible():,0.9423600843684268,0.9164531641034833,False
1614,"def events_to_str(event_field, all_events): result = [] for flag, string in all_events: c_flag = flag if event_field & c_flag: result.append(string) event_field = event_field & ~c_flag <IF_STMT> break if event_field: result.append(hex(event_field)) return '|'.join(result)",if not event_field:,if event_field == 0:,0.9119932744915101,0.828399516355805,False
1615,"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get('Logging'): if self_log: if b['Name'] != b['Logging']['TargetBucket']: continue yield (b['Logging']['TargetBucket'], b['Logging']['TargetPrefix']) <IF_STMT> yield (b['Name'], '')",if not self_log and b['Name'].startswith('cf-templates-'):,elif b['Name']:,0.925913115288744,0.8827916928185874,False
1616,"def extract_file(tgz, tarinfo, dst_path, buffer_size=10 << 20, log_function=None): """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'."""""" src = tgz.extractfile(tarinfo) if src is None: return dst = tf.compat.v1.gfile.GFile(dst_path, 'wb') while 1: buf = src.read(buffer_size) if not buf: break dst.write(buf) <IF_STMT> log_function(len(buf)) dst.close() src.close()",if log_function is not None:,if log_function:,0.9218788397492198,0.9144061946646023,False
1617,"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): if k in ('lccn', 'oclc', 'isbn'): fields[k] = v continue <IF_STMT> fields['title'] = [read_short_title(v)] return fields",if k == 'full_title':,if v:,0.6759042193408152,0.8590888738245122,False
1618,def disconnect_application(self): if not self.is_app_running(self.APP_BACKDROP): self.socket.send(commands.CloseCommand(destination_id=False)) start_time = time.time() while not self.is_app_running(None): try: self.socket.send_and_wait(commands.StatusCommand()) except cast_socket.ConnectionTerminatedException: break current_time = time.time() <IF_STMT> raise TimeoutException() time.sleep(self.WAIT_INTERVAL) else: logger.debug('Closing not necessary. Backdrop is running ...'),if current_time - start_time > self.timeout:,if current_time - start_time > self.WAIT_INTERVAL:,0.8549048618665794,0.7865984197371234,False
1619,"def matches(self, cursor_offset, line, **kwargs): cs = lineparts.current_string(cursor_offset, line) if cs is None: return None matches = set() username = cs.word.split(os.path.sep, 1)[0] user_dir = os.path.expanduser(username) for filename in self.safe_glob(os.path.expanduser(cs.word)): if os.path.isdir(filename): filename += os.path.sep <IF_STMT> filename = username + filename[len(user_dir):] matches.add(filename) return matches",if cs.word.startswith('~'):,if filename.startswith(user_dir):,0.940412623136763,0.9164531641034833,False
1620,"def eventFilter(self, obj, event): if event.type() == QEvent.MouseButtonPress: button = event.button() <IF_STMT> self._app.browser.back() return True elif button == Qt.ForwardButton: self._app.browser.forward() return True return False",if button == Qt.BackButton:,if button == Qt.BackButton:,0.744027032148894,0.7801270245332924,True
1621,"def reset_parameters(self): for m in self.modules(): if isinstance(m, nn.Embedding): continue <IF_STMT> nn.init.constant_(m.weight, 0.1) nn.init.constant_(m.bias, 0) else: for p in m.parameters(): nn.init.normal_(p, 0, 0.1)","elif isinstance(m, nn.LayerNorm):","if isinstance(m, nn.BatchNorm2d):",0.5990711315636889,0.80377750806414,False
1622,"def get_scalding_core(self): lib_dir = os.path.join(self.scalding_home, 'lib') for j in os.listdir(lib_dir): <IF_STMT> p = os.path.join(lib_dir, j) logger.debug('Found scalding-core: %s', p) return p raise luigi.contrib.hadoop.HadoopJobError('Could not find scalding-core.')",if j.startswith('scalding-core-'):,if os.path.isdir(j):,0.8854991094320005,0.8590888738245122,False
1623,"def save(self): """"""Saves a new set of golden output frames to disk."""""" for pixels, (relative_to_assets, filename) in zip(self.iter_render(), self._iter_paths()): full_directory_path = os.path.join(self._ASSETS_DIR, relative_to_assets) <IF_STMT> os.makedirs(full_directory_path) path = os.path.join(full_directory_path, filename) _save_pixels(pixels, path)",if not os.path.exists(full_directory_path):,if not os.path.exists(full_directory_path):,0.7266620829281657,0.8547305998833805,True
1624,"def _fix_var_naming(operators, names, mod='input'): new_names = [] map = {} for op in operators: <IF_STMT> iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name if len(map) == len(names): break for name in names: new_names.append(map[name]) return new_names",if mod == 'input':,if mod == 'input':,0.8791155847370453,0.9001816649635144,True
1625,"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] <IF_STMT> typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",elif item[1]:,elif item[1]:,0.6640294276543497,0.9184043388013005,True
1626,"def init_errorhandler(): for ex in default_exceptions: if ex < 500: app.register_error_handler(ex, error_http) <IF_STMT> app.register_error_handler(ex, internal_error) if services.ldap:  @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug('LDAP server not accessible while trying to login to opds feed') return error_http(FailedDependency())",elif ex == 500:,elif ex < 500:,0.9199052015481637,0.8385130047130208,False
1627,"def decode(self, ids): ids = pad_decr(ids) tokens = [] for int_id in ids: <IF_STMT> tokens.append(self._vocab_list[int_id]) else: tokens.append(self._oov_token) return self._decode_token_separator.join(tokens)",if int_id < len(self._vocab_list):,if int_id in self._vocab_list:,0.8399032415976804,0.7245511487202049,False
1628,"def remove_contest(contest_id): with SessionGen() as session: contest = session.query(Contest).filter(Contest.id == contest_id).first() if not contest: print('No contest with id %s found.' % contest_id) return False contest_name = contest.name <IF_STMT> print(""Not removing contest `%s'."" % contest_name) return False session.delete(contest) session.commit() print(""Contest `%s' removed."" % contest_name) return True",if not ask(contest):,if not contest_name:,0.8812659497725571,0.9000283069718913,False
1629,def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() if self.expr2 is None: pass else: lineno = self.expr2.get_hi_lineno() <IF_STMT> pass else: lineno = self.expr3.get_hi_lineno() return lineno,if self.expr3 is None:,if self.expr3 is None:,0.6818749568457027,0.828399516355805,True
1630,"def _send_internal(self, bytes_): if self.pendings: self.pendings += bytes_ bytes_ = self.pendings try: self._reconnect() self.socket.sendall(bytes_) self.pendings = None except Exception: self._close() <IF_STMT> self.pendings = None else: self.pendings = bytes_",if self.pendings and len(self.pendings) > self.bufmax:,if bytes_ == 0:,0.6761661039879284,0.8105932471967202,False
1631,"def _unpack(self, fmt, byt): d = unpack(self._header['byteorder'] + fmt, byt)[0] if fmt[-1] in self.MISSING_VALUES: nmin, nmax = self.MISSING_VALUES[fmt[-1]] if d < nmin or d > nmax: <IF_STMT> return StataMissingValue(nmax, d) else: return None return d",if self._missing_values:,if d < nmin:,0.6014932212148416,0.8474968231198384,False
1632,"def tuple_iter(self): for x in range(self.center.x - self.max_radius, self.center.x + self.max_radius + 1): for y in range(self.center.y - self.max_radius, self.center.y + self.max_radius + 1): <IF_STMT> yield (x, y)","if self.min_radius <= self.center.distance((x, y)) <= self.max_radius:",if x < self.center.x and y < self.center.y:,0.6184238954974413,0.7136906481243283,False
1633,"def _parse_gene(element): for genename_element in element: <IF_STMT> ann_key = 'gene_%s_%s' % (genename_element.tag.replace(NS, ''), genename_element.attrib['type']) if genename_element.attrib['type'] == 'primary': self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",if 'type' in genename_element.attrib:,if genename_element.tag.startswith('gene'):,0.8988344381743371,0.8466657105524215,False
1634,"def invalidateDependentSlices(self, iFirstCurve): if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType): c.invalidate() <IF_STMT> break",elif i == iFirstCurve:,elif c.getSymbol().getSymbolType() == SymbolType.PieSliceSymbolType:,0.8602921117408557,0.8047418499398723,False
1635,"def gen_app_versions(self): for app_config in apps.get_app_configs(): name = app_config.verbose_name app = app_config.module version = self.get_app_version(app) <IF_STMT> yield (app.__name__, name, version)",if version:,if name and version:,0.636286471979399,0.7378351342269067,False
1636,"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError('Empty chown path') checkpath = root parts = path.split(os.sep) for part in parts: if part in ('.', '..'): raise PackagerError('. and .. is not allowed in chown path') checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1:] <IF_STMT> raise PackagerError(f'chown path {relpath} does not exist') if os.path.islink(checkpath): raise PackagerError(f'chown path {relpath} is a soft link')",if not os.path.exists(checkpath):,if not os.path.exists(relpath):,0.9613278644719196,0.9286099819098308,False
1637,"def create_or_update_tag_at_scope(cmd, resource_id=None, tags=None, tag_name=None): rcf = _resource_client_factory(cmd.cli_ctx) if resource_id is not None: <IF_STMT> raise IncorrectUsageError('Tags could not be empty.') Tags = cmd.get_models('Tags') tag_obj = Tags(tags=tags) return rcf.tags.create_or_update_at_scope(scope=resource_id, properties=tag_obj) return rcf.tags.create_or_update(tag_name=tag_name)",if not tags:,if tags is None:,0.900535206283516,0.828399516355805,False
1638,"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base if self.settings.get(IGNORE_CASE_SETTING): compare_entry = compare_entry.lower() compare_base = compare_base.lower() <IF_STMT> if entry not in sugg: sugg.append(entry) return sugg","if self.compare_entries(compare_entry, compare_base):",if compare_entry == base:,0.6877204695805768,0.8431339019329497,False
1639,"def createFields(self): yield String(self, 'dict_start', 2) while not self.eof: addr = self.absolute_address + self.current_size <IF_STMT> for field in parsePDFType(self): yield field else: break yield String(self, 'dict_end', 2)","if self.stream.readBytes(addr, 2) != '>>':",if addr < self.absolute_address:,0.7977246355621349,0.8038019482772603,False
1640,"def Visit_and_test(self, node): for child in node.children: self.Visit(child) <IF_STMT> _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance(child, pytree.Leaf) and child.value == 'and':","if isinstance(child, pytree.Leaf) and child.value == 'and':",0.4589411392016681,0.4527471870952894,True
1641,"def getfiledata(directories): columns = None data = [] counter = 1 for directory in directories: for f in os.listdir(directory): if not os.path.isfile(os.path.join(directory, f)): continue counter += 1 st = os.stat(os.path.join(directory, f)) <IF_STMT> columns = ['rowid', 'name', 'directory'] + [x for x in dir(st) if x.startswith('st_')] data.append([counter, f, directory] + [getattr(st, x) for x in columns[3:]]) return (columns, data)",if columns is None:,if columns is None:,0.8055891624702889,0.9069443196104878,True
1642,"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) if value is None: continue name = name_fmt % attr <IF_STMT> value = formatter(attr, value) info_add(name, value)",if formatter is not None:,if formatter is not None:,0.6542924765508104,0.8094220211349227,True
1643,"def main(args): ap = argparse.ArgumentParser() ap.add_argument('job_ids', nargs='+', type=int, help='ID of a running job') ns = ap.parse_args(args) _stash = globals()['_stash'] ':type : StaSh' for job_id in ns.job_ids: <IF_STMT> print('killing job {} ...'.format(job_id)) worker = _stash.runtime.worker_registry.get_worker(job_id) worker.kill() time.sleep(1) else: print('error: no such job with id: {}'.format(job_id)) break",if job_id in _stash.runtime.worker_registry:,if _stash.runtime.job_registry.has_job(job_id):,0.8360035238489569,0.9202663016973823,False
1644,"def _check_choice(self): if self.type == 'choice': if self.choices is None: raise OptionError(""must supply a list of choices for type 'choice'"", self) <IF_STMT> raise OptionError(""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self) elif self.choices is not None: raise OptionError('must not supply choices for type %r' % self.type, self)","elif type(self.choices) not in (types.TupleType, types.ListType):","elif not isinstance(self.choices, str):",0.6772773867570664,0.8944264839442453,False
1645,"def add_file(pipe, srcpath, tgtpath): with open(srcpath, 'rb') as handle: <IF_STMT> write(pipe, enc('M 100755 inline %s\n' % tgtpath)) else: write(pipe, enc('M 100644 inline %s\n' % tgtpath)) data = handle.read() write(pipe, enc('data %d\n' % len(data))) write(pipe, enc(data)) write(pipe, enc('\n'))","if os.access(srcpath, os.X_OK):",if os.path.exists(tgtpath):,0.9216174877808472,0.9024521756077707,False
1646,"def cdf(self, x): if x == numpy.inf: return 1.0 else: <IF_STMT> raise RuntimeError('Invalid value.') c = 0.0 for i in xrange(x + 1): c += self.probability(i) return c",if x != int(x):,if x < 0:,0.8631377359409368,0.8105932471967202,False
1647,"def convert_to_strings(self, out, seq_len): results = [] for b, batch in enumerate(out): utterances = [] for p, utt in enumerate(batch): size = seq_len[b][p] <IF_STMT> transcript = ''.join(map(lambda x: self.int_to_char[x.item()], utt[0:size])) else: transcript = '' utterances.append(transcript) results.append(utterances) return results",if size > 0:,if size > 0:,0.8509503789005568,0.8592377270804451,True
1648,"def get_date_range(self): if not hasattr(self, 'start') or not hasattr(self, 'end'): args = (self.today.year, self.today.month) form = self.get_form() <IF_STMT> args = (int(form.cleaned_data['year']), int(form.cleaned_data['month'])) self.start = self.get_start(*args) self.end = self.get_end(*args) return (self.start, self.end)",if form.is_valid():,if form.cleaned_data:,0.703018307573777,0.8827916928185874,False
1649,"def save_stats(self): LOGGER.info('Saving task-level statistics.') has_headers = os.path.isfile(paths.TABLE_COUNT_PATH) with open(paths.TABLE_COUNT_PATH, 'a') as csvfile: headers = ['start_time', 'database_name', 'number_tables'] writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\n', fieldnames=headers) <IF_STMT> writer.writeheader() writer.writerow({'start_time': self.start_time, 'database_name': self.database_name, 'number_tables': self.count})",if not has_headers:,if has_headers:,0.8385609723917872,0.8866029039778043,False
1650,"def _CheckCanaryCommand(self): <IF_STMT> return with self._lock: if OpenStackVirtualMachine.command_works: return logging.info('Testing OpenStack CLI command is installed and working') cmd = os_utils.OpenStackCLICommand(self, 'image', 'list') stdout, stderr, _ = cmd.Issue() if stderr: raise errors.Config.InvalidValue('OpenStack CLI test command failed. Please make sure the OpenStack CLI client is installed and properly configured') OpenStackVirtualMachine.command_works = True",if OpenStackVirtualMachine.command_works:,if not os_utils.IsRunning(self):,0.8541665159276335,0.9152819145440988,False
1651,"def test_windows_hidden(self): if not sys.platform == 'win32': self.skipTest('sys.platform is not windows') return hidden_mask = 2 with tempfile.NamedTemporaryFile() as f: success = ctypes.windll.kernel32.SetFileAttributesW(f.name, hidden_mask) <IF_STMT> self.skipTest('unable to set file attributes') self.assertTrue(hidden.is_hidden(f.name))",if not success:,if not success:,0.7880917038944346,0.8498644646741501,True
1652,"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <IF_STMT> y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return b''.join(y)",elif r:,if e:,0.6655959445914909,0.938501942261528,False
1653,def _is_xml(accepts): if accepts.startswith(b'application/'): has_xml = accepts.find(b'xml') <IF_STMT> semicolon = accepts.find(b';') if semicolon < 0 or has_xml < semicolon: return True return False,if has_xml > 0:,if has_xml > 0:,0.8703705566704263,0.7709002428237395,True
1654,"def times(self, value: int): if value is None: self._times = None else: try: candidate = int(value) except ValueError: raise BarException(f'cannot set repeat times to: {value!r}') if candidate < 0: raise BarException(f'cannot set repeat times to a value less than zero: {value}') <IF_STMT> raise BarException('cannot set repeat times on a start Repeat') self._times = candidate",if self.direction == 'start':,if candidate > self.start_repeat:,0.7713307018365836,0.9001816649635144,False
1655,"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): if 'self' in stack[0].f_locals: layer = stack[0].f_locals['self'] <IF_STMT> log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) return out",if layer in layer_names:,if layer in layer_names:,0.6932146964638962,0.8431339019329497,True
1656,"def do_begin(self, byte): if byte.isspace(): return if byte != '<': <IF_STMT> self._leadingBodyData = byte return 'bodydata' self._parseError(""First char of document [{!r}] wasn't <"".format(byte)) return 'tagstart'",if self.beExtremelyLenient:,if self._leadingBodyData is None:,0.8135230888765947,0.7886336751695258,False
1657,"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) <IF_STMT> r = r.replace('*/', '\\x2a/') return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): if comment: return '%s /* %s */' % (n, self.pretty(int(n))) else: return '%s (%s)' % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",if not comment:,if r.startswith('*/'):,0.8604464185580041,0.9362597875749384,False
1658,"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model(DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config='data/test_config/max_hist_config.yml', kwargs={}) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: if hasattr(policy.featurizer, 'max_history'): <IF_STMT> assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5",if type(policy) == FormPolicy:,if policy.featurizer.max_history == 1:,0.8900192248446881,0.8105932471967202,False
1659,"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + '  --->>  ' + _distro_dir) user_input = read_input_uninstall() <IF_STMT> for index, _distro_dir in enumerate(distro_list): if index == user_input: config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log('No distro installed on ' + config.usb_disk)",if user_input is not False:,if user_input is not None:,0.8222524420389586,0.8677319190106252,False
1660,"def set_random_avatar(user): galleries = get_available_galleries(include_default=True) if not galleries: raise RuntimeError('no avatar galleries are set') avatars_list = [] for gallery in galleries: <IF_STMT> avatars_list = gallery['images'] break else: avatars_list += gallery['images'] random_avatar = random.choice(avatars_list) store.store_new_avatar(user, Image.open(random_avatar.image))",if gallery['name'] == DEFAULT_GALLERY:,if gallery['type'] == 'avatar':,0.6177114459275518,0.8474968231198384,False
1661,"def make_query(self, key, filters): meta = self.get_meta(key) q = {meta.facet_key: self.normalize_key(meta.path)} if filters: if filters.get('has_fulltext') == 'true': q['has_fulltext'] = 'true' <IF_STMT> q['publish_year'] = filters['publish_year'] return q",if filters.get('publish_year'):,if filters.get('publish_year') == 'true':,0.8922787215570978,0.7965020533851944,False
1662,"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == 'likelihood.noise_covar.raw_noise': self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) <IF_STMT> self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == 'covar_module.base_kernel.raw_lengthscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive)",elif name == 'mean_module.constant':,elif name == 'likelihood.covar.raw_bias':,0.913752103484968,0.8516228624291206,False
1663,"def _test_pooling(input_shape, **kwargs): _test_pooling_iteration(input_shape, **kwargs) if is_gpu_available(): <IF_STMT> input_shape = [input_shape[ii] for ii in (0, 3, 1, 2)] kwargs['data_format'] = 'NCHW' _test_pooling_iteration(input_shape, **kwargs)",if len(input_shape) == 4:,"if isinstance(input_shape, tuple):",0.8761923983753038,0.80377750806414,False
1664,def init(self): r = self.get_redis() if r: key = 'pocsuite_target' info_msg = '[PLUGIN] try fetch targets from redis...' logger.info(info_msg) targets = r.get(key) count = 0 <IF_STMT> for target in targets: if self.add_target(target): count += 1 info_msg = '[PLUGIN] get {0} target(s) from redis'.format(count) logger.info(info_msg),if targets:,if targets:,0.8026814401539069,0.9184043388013005,True
1665,"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs['setting'] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, '') value = kwargs['value'] if setting in DEFAULTS.keys(): if value is not None: setattr(json_api_settings, setting, value) <IF_STMT> delattr(json_api_settings, setting)","elif hasattr(json_api_settings, setting):",elif setting in JSON_API_SETTINGS_DEFAULTS.keys():,0.905684627614938,0.8200754821669128,False
1666,"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) <IF_STMT> try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass","elif hasattr(self.metadata, attrname):",elif attrname != 'version':,0.6670501320970832,0.8627586293513119,False
1667,"def test_02_looking_at_listdir_path_(name): for dline in listdir.json(): <IF_STMT> assert dline['type'] in ('DIRECTORY', 'FILE'), listdir.text assert dline['uid'] == 0, listdir.text assert dline['gid'] == 0, listdir.text assert dline['name'] == name, listdir.text break else: raise AssertionError(f'/{path}/{name} not found')",if dline['path'] == f'{path}/{name}':,if dline['type'] == 'DIRECTORY':,0.8985455018995226,0.8431339019329497,False
1668,"def DeletePlugin(): oid = request.form.get('oid', '') if oid: result = Mongo.coll['Plugin'].find_one_and_delete({'_id': ObjectId(oid)}, remove=True) <IF_STMT> result['filename'] = result['filename'] + '.py' if os.path.exists(file_path + result['filename']): os.remove(file_path + result['filename']) return 'success' return 'fail'",if not result['filename'].find('.') > -1:,if result:,0.928249087460772,0.8787142254774354,False
1669,"def iterparent(self, node): """"""Iterator wrapper to get allowed parent and child all at once."""""" for child in node: <IF_STMT> yield (node, child) yield from self.iterparent(child)","if not self.header_rgx.match(child.tag) and child.tag not in ['pre', 'code']:","if isinstance(child, Node):",0.8758685021967991,0.8196189957582152,False
1670,def _get_matched_layout(command): cmd = command.script.split(' ') for source_layout in source_layouts: is_all_match = True for cmd_part in cmd: if not all([ch in source_layout or ch in '-_' for ch in cmd_part]): is_all_match = False break <IF_STMT> return source_layout,if is_all_match:,if is_all_match:,0.7962997055643477,0.9098803919843304,True
1671,"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: if n.op in failed_ops: continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): <IF_STMT> continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple((sum(nsplit) for nsplit in new_nsplits))) tiled_n._nsplits = new_nsplits",if any((c.key not in chunk_result for c in tiled_n.chunks)):,if not tiled_n.has_shape():,0.882912996397422,0.8815741981066073,False
1672,"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: if len(all_items) < target: raise ItemNotFoundError('insufficient items with name %r' % name) else: raise AttributeError('insufficient non-disabled items with name %s' % name) on = [] off = [] for o in items: <IF_STMT> on.append(o) else: off.append(o) return (on, off)",if o.selected:,if o.disabled:,0.8398331028564437,0.940591574039961,False
1673,def parse_flow_sequence_entry_mapping_value(self): if self.check_token(ValueToken): token = self.get_token() <IF_STMT> self.states.append(self.parse_flow_sequence_entry_mapping_end) return self.parse_flow_node() else: self.state = self.parse_flow_sequence_entry_mapping_end return self.process_empty_scalar(token.end_mark) else: self.state = self.parse_flow_sequence_entry_mapping_end token = self.peek_token() return self.process_empty_scalar(token.start_mark),"if not self.check_token(FlowEntryToken, FlowSequenceEndToken):",if token.is_identifier():,0.702476990025186,0.8590888738245122,False
1674,"def serialize_config(self, session, key, tid, language): cache_key = gen_cache_key(key, tid, language) cache_obj = None if cache_key not in self.cache: <IF_STMT> cache_obj = db_admin_serialize_node(session, tid, language) elif key == 'notification': cache_obj = db_get_notification(session, tid, language) self.cache[cache_key] = cache_obj return self.cache[cache_key]",if key == 'node':,if key == 'node':,0.9108256161760007,0.8627586293513119,True
1675,"def get_lldp_neighbors(self): commands = ['show lldp neighbors'] output = self.device.run_commands(commands)[0]['lldpNeighbors'] lldp = {} for n in output: <IF_STMT> lldp[n['port']] = [] lldp[n['port']].append({'hostname': n['neighborDevice'], 'port': n['neighborPort']}) return lldp",if n['port'] not in lldp.keys():,if 'port' not in lldp:,0.8638486674914023,0.7765145040967655,False
1676,def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name if self.option('full-path'): name = str(venv.path) <IF_STMT> self.line('<info>{} (Activated)</info>'.format(name)) continue self.line(name),if venv == current_env:,if name in current_env.active_venv_names:,0.7714513488709867,0.8169276475307028,False
1677,"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): if list(config.keys()) == ['$env']: return environ.get(list(config.values())[0]) <IF_STMT> return open(list(config.values())[0]).read() else: return {key: resolve_env_secrets(value, environ) for key, value in config.items()} elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",elif list(config.keys()) == ['$file']:,elif list(config.keys()) == ['NAME']:,0.6650656293939576,0.8902056737869248,False
1678,"def _is_valid_16bit_as_path(cls, buf): two_byte_as_size = struct.calcsize('!H') while buf: type_, num_as = struct.unpack_from(cls._SEG_HDR_PACK_STR, six.binary_type(buf)) if type_ is not cls._AS_SET and type_ is not cls._AS_SEQUENCE: return False buf = buf[struct.calcsize(cls._SEG_HDR_PACK_STR):] <IF_STMT> return False buf = buf[num_as * two_byte_as_size:] return True",if len(buf) < num_as * two_byte_as_size:,if num_as * two_byte_as_size != 2:,0.7757772928101256,0.8177978265964414,False
1679,"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: <IF_STMT> newParent._element.text = '' if self._element.text is not None: newParent._element.text += self._element.text self._element.text = '' base.Node.reparentChildren(self, newParent)",if not newParent._element.text:,if self._element.text is None:,0.810413687136982,0.7965020533851944,False
1680,"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: if operation: return None operation = definition <IF_STMT> return definition return operation",elif definition.name and definition.name.value == operation_name:,elif operation_name == definition.name:,0.7986324719032858,0.8105932471967202,False
1681,"def reprSmart(vw, item): ptype = type(item) if ptype is int: <IF_STMT> return str(item) elif vw.isValidPointer(item): return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) elif ptype is dict: return '{%s}' % ','.join(['%s:%s' % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]) else: return repr(item)",if -1024 < item < 1024:,if vw.isValidString(item):,0.6504949654427212,0.926934323706186,False
1682,"def cleanDataCmd(cmd): newcmd = 'AbracadabrA ** <?php ' if cmd[:6] != 'php://': <IF_STMT> cmds = cmd.split('&') for c in cmds: if len(c) > 0: newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += '?> **' return newcmd",if reverseConn not in cmd:,if '&' in cmd:,0.638078751612842,0.8879659171421962,False
1683,"def render_tasks(self) -> List: results = [] for task in self.tasks.values(): job_entry = self.jobs.get(task.job_id) <IF_STMT> if not self.should_render_job(job_entry): continue files = self.get_file_counts([task]) entry = (task.job_id, task.task_id, task.state, task.type.name, task.target, files, task.pool, task.end_time) results.append(entry) return results",if job_entry:,if job_entry:,0.6787699487267738,0.8966773400768917,True
1684,"def __call__(self, environ, start_response): for key in ('REQUEST_URL', 'REQUEST_URI', 'UNENCODED_URL'): if key not in environ: continue request_uri = unquote(environ[key]) script_name = unquote(environ.get('SCRIPT_NAME', '')) <IF_STMT> environ['PATH_INFO'] = request_uri[len(script_name):].split('?', 1)[0] break return self.app(environ, start_response)",if request_uri.startswith(script_name):,if request_uri.startswith(script_name):,0.833961915835851,0.8866029039778043,True
1685,"def _add_role_information(self, function_dict, role_id): function_dict['role_arn'] = role_id role_name = role_id.split('/')[-1] function_dict['execution_role'] = await self.facade.awslambda.get_role_with_managed_policies(role_name) if function_dict.get('execution_role'): statements = [] for policy in function_dict['execution_role'].get('policies'): <IF_STMT> statements += policy['Document']['Statement'] function_dict['execution_role']['policy_statements'] = statements",if 'Document' in policy and 'Statement' in policy['Document']:,if policy['Policy']['Name'] == role_name:,0.8786170460157277,0.8228500218338367,False
1686,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_ts(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.8646149936089025,0.7378351342269067,True
1687,"def format_counts(results, json_output=False, human_readable=False): if json_output: for result in results: yield json.dumps(result) else: for result in results: space_consumed = result.get('spaceConsumed') <IF_STMT> space_consumed = _sizeof_fmt(int(result.get('spaceConsumed'))) yield ('%12s %12s %18s %s' % (result.get('directoryCount'), result.get('fileCount'), space_consumed, result.get('path')))",if human_readable:,if human_readable:,0.8566012721830824,0.8935248372106969,True
1688,"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) <IF_STMT> bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if bbox is None:,elif bbox is None:,0.7356405201341145,0.8856327184319047,False
1689,"def __getitem__(self, k) -> 'SimMemView': if isinstance(k, slice): if k.step is not None: raise ValueError('Slices with strides are not supported') <IF_STMT> raise ValueError('Must specify start index') elif k.stop is not None: raise ValueError('Slices with stop index are not supported') else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",elif k.start is None:,elif k.start is None:,0.9425220396161145,0.9084940438173679,True
1690,"def _parse(self, stream, context): obj = [] try: if self.subcon.conflags & self.FLAG_COPY_CONTEXT: while True: subobj = self.subcon._parse(stream, context.__copy__()) obj.append(subobj) <IF_STMT> break else: while True: subobj = self.subcon._parse(stream, context) obj.append(subobj) if self.predicate(subobj, context): break except ConstructError as ex: raise ArrayError('missing terminator', ex) return obj","if self.predicate(subobj, context):","if self.predicate(subobj, context):",0.920821970337939,0.8953711787948615,True
1691,"def before_run(self, run_context): if 'featurizer' in self.model_portion and (self.need_to_refresh or self.refresh_base_model): <IF_STMT> self.refresh_base_model = True self.init_fn(None, run_context.session, self.model_portion, self.refresh_base_model) self.need_to_refresh = False self.refresh_base_model = False",if self.model_portion == 'whole_featurizer':,if self.refresh_base_model:,0.8245823338623381,0.8531413606256201,False
1692,"def run(self): while True: task = self.requestQueue.get() if task is None: break try: <IF_STMT> raise SCons.Errors.BuildError(task.targets[0], errstr=interrupt_msg) task.execute() except: task.exception_set() ok = False else: ok = True self.resultsQueue.put((task, ok))",if self.interrupted():,if task.targets:,0.6488772933303804,0.8743414417652072,False
1693,"def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]: """"""Returns all documents that have need had a recent evergreen notification."""""" documents = db_session.query(Document).filter(Document.evergreen == True).all() overdue_documents = [] now = datetime.utcnow() for d in documents: next_reminder = d.evergreen_last_reminder_at + timedelta(days=d.evergreen_reminder_interval) <IF_STMT> overdue_documents.append(d) return overdue_documents",if now > next_reminder:,if next_reminder > now:,0.8277953765984569,0.8661072626070159,False
1694,"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): if not exists(folder): os.mkdir(folder) init_path = join(folder, '__init__.py') <IF_STMT> create_file(init_path)",if not exists(init_path):,if not exists(init_path):,0.7624690018804829,0.867561858851747,True
1695,"def generate(): for leaf in u.leaves: <IF_STMT> val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector","if isinstance(leaf, Integer):","if isinstance(leaf, SymbolInt):",0.8333722355013401,0.8902579342581529,False
1696,"def replace(self, old, new): v_m = self.var_map size = v_m[self.size] if not (size.is_const() or size.is_ident()): size.replace(old, new) el<IF_STMT> v_m[new.value()] = new self.size = new.value() else: v_m[old] = new",if new.is_ident():,if new.is_ident():,0.8231497471812779,0.8696398662122882,True
1697,def method_for_doctype(doctype): method = 'xhtml' if doctype: if doctype.startswith('html'): method = 'html' <IF_STMT> method = 'xhtml' elif doctype.startswith('svg'): method = 'xml' else: method = 'xhtml' return method,elif doctype.startswith('xhtml'):,elif doctype.startswith('xhtml'):,0.5601121774617644,0.8645707301556367,True
1698,"def delete(self, trans, **kwd): idnum = kwd[self.tagged_item_id] item = self._get_item_from_id(trans, idnum, check_writable=True) if item is not None: ex_obj = self.get_item_extended_metadata_obj(trans, item) <IF_STMT> self.unset_item_extended_metadata_obj(trans, item) self.delete_extended_metadata(trans, ex_obj)",if ex_obj is not None:,if ex_obj is None:,0.6926463686159212,0.7965020533851944,False
1699,"def check_testv(self, testv): test_good = True f = open(self.home, 'rb+') for offset, length, operator, specimen in testv: data = self._read_share_data(f, offset, length) <IF_STMT> test_good = False break f.close() return test_good","if not testv_compare(data, operator, specimen):",if not data:,0.7836620399433707,0.8498644646741501,False
1700,"def get_history_user(self, instance): """"""Get the modifying user from instance or middleware."""""" try: return instance._history_user except AttributeError: request = None try: <IF_STMT> request = self.thread.request except AttributeError: pass return self.get_user(instance=instance, request=request)",if self.thread.request.user.is_authenticated:,if self.thread:,0.7606386328309735,0.8787142254774354,False
1701,"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) <IF_STMT> strlen *= size if strlen < MAX_LEN: data = 'A' * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",if size:,if size:,0.7218025660560177,0.9434724611166208,True
1702,"def __setattr__(self, name, value): if name == 'path': if value and value != '': if value[0] != '/': raise ValueError('The page path should always start with a slash (""/"").') elif name == 'load_time': <IF_STMT> raise ValueError('Page load time must be specified in integer milliseconds.') object.__setattr__(self, name, value)","if value and (not isinstance(value, int)):",if value and value < 0:,0.6492377648101129,0.8505405945991492,False
1703,"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True if self.is_computed(): status = 'computed, ' <IF_STMT> if self.value() is self: status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.error() is None:,if self.error():,0.9723444648262235,0.9350761925543661,False
1704,"def _exclude_node(self, name): if 'exclude_nodes' in self.node_filters: <IF_STMT> self.loggit.info('Excluding node ""{0}"" due to node_filters'.format(name)) return True return False",if name in self.node_filters['exclude_nodes']:,if self.node_filters['exclude_nodes'] == name:,0.5622447234300942,0.7098232254187811,False
1705,"def enumerate_projects(): """"""List projects in _DEFAULT_APP_DIR."""""" src_path = os.path.join(_DEFAULT_APP_DIR, 'src') projects = {} for project in os.listdir(src_path): projects[project] = [] project_path = os.path.join(src_path, project) for file in os.listdir(project_path): <IF_STMT> projects[project].append(file[:-8]) return projects",if file.endswith('.gwt.xml'):,if file.endswith('.py') and file.endswith('.py'):,0.9166352144042197,0.8336104423443033,False
1706,"def zip_readline_read_test(self, f, compression): self.make_test_archive(f, compression) with zipfile.ZipFile(f, 'r') as zipfp, zipfp.open(TESTFN) as zipopen: data = b'' while True: read = zipopen.readline() <IF_STMT> break data += read read = zipopen.read(100) if not read: break data += read self.assertEqual(data, self.data)",if not read:,if not read:,0.9199625833183642,0.884617925078158,True
1707,"def f(view, s): if mode == modes.NORMAL: return sublime.Region(0) elif mode == modes.VISUAL: <IF_STMT> return sublime.Region(s.a + 1, 0) else: return sublime.Region(s.a, 0) elif mode == modes.INTERNAL_NORMAL: return sublime.Region(view.full_line(s.b).b, 0) elif mode == modes.VISUAL_LINE: if s.a < s.b: return sublime.Region(0, s.b) else: return sublime.Region(0, s.a) return s",if s.a < s.b:,if s.a < s.b:,0.9177715405863683,0.8856327184319047,True
1708,def response(self): try: response = requests.get(str(self)) rjson = response.json() <IF_STMT> raise Exception(response.text) return rjson except Exception as e: raise ResponseFanartError(str(e)),"if not isinstance(rjson, dict):",if rjson is None:,0.736566079276929,0.7378351342269067,False
1709,"def __get_type(self, cexpr): """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument"""""" child = cexpr for p in reversed(self.parents): assert p, 'Failed to get type at ' + helper.to_hex(self.__function_address) if p.cexpr.op == idaapi.cot_call: return 'Arg' if not p.is_expr(): return 'R' if p.cexpr.op == idaapi.cot_asg: <IF_STMT> return 'W' return 'R' child = p.cexpr",if p.cexpr.x == child:,if child.is_expr():,0.9480136758506614,0.9415839804285298,False
1710,"def _extract_lemma(self, parse: Parse) -> str: special_feats = [x for x in self.SPECIAL_FEATURES if x in parse.tag] if len(special_feats) == 0: return parse.normal_form for other in parse.lexeme: tag = other.tag <IF_STMT> continue if tag.case == 'nomn' and tag.gender == parse.tag.gender and (tag.number == 'sing'): return other.word return parse.normal_form",if any((x not in tag for x in special_feats)):,if tag in special_feats:,0.9397775316345836,0.8879659171421962,False
1711,"def evaluateWord(self, argument): wildcard_count = argument[0].count('*') if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith('*'): return self.GetWordWildcard(argument[0][1:], method='endswith') <IF_STMT> return self.GetWordWildcard(argument[0][:-1], method='startswith') else: _regex = argument[0].replace('*', '.+') matched = False for w in self.words: matched = bool(re.search(_regex, w)) if matched: break return matched return self.GetWord(argument[0])",if wildcard_count == 1 and argument[0].endswith('*'):,elif wildcard_count == 2 and argument[0].endswith('*'):,0.9101248328096455,0.8474966375582541,False
1712,def getAllEntries(self): entries = [] for bucket in self.buckets: last = None for entry in bucket.entries: if last is not None: last.size = entry.virtualOffset - last.virtualOffset last = entry entries.append(entry) <IF_STMT> entries[-1].size = bucket.endOffset - entries[-1].virtualOffset return entries,if len(entries) != 0:,if len(entries) > 1:,0.8791523931648116,0.8592377270804451,False
1713,def clean(self): if self._ctx: <IF_STMT> libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx) else: libcrypto.EVP_CIPHER_CTX_reset(self._ctx) libcrypto.EVP_CIPHER_CTX_free(self._ctx),"if hasattr(libcrypto, 'EVP_CIPHER_CTX_cleanup'):",if libcrypto.EVP_CIPHER_CTX_is_valid(self._ctx):,0.4637651234674891,0.5253819788848316,False
1714,"def _addTab(self, name, label, idx=None): label = getLanguageString(label) tab = Tab(self, name, label) tab.idx = self._makeTab(tab, idx) if idx != None: newIdxList = {} for tIdx, t in list(self._tabs_by_idx.items()): <IF_STMT> t.idx += 1 newIdxList[t.idx] = t self._tabs_by_idx = newIdxList self._tabs_by_idx[tab.idx] = tab self._tabs_by_name[tab.name] = tab return tab",if int(tIdx) >= idx:,if t.idx == idx:,0.8149655278634274,0.8856327184319047,False
1715,"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) <IF_STMT> self.users[current_user.id] = dict(session_count=1, key=_key) else: if _new_login: user['session_count'] += 1 user['key'] = _key",if user is None:,if user is None:,0.7697029396216813,0.7801270245332924,True
1716,"def stop(self): try: self.rpcserver.stop() <IF_STMT> self.backend_rpcserver.stop() if self.cluster_rpcserver: self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",if self.backend_rpcserver:,if self.backend_rpcserver:,0.9003049547783142,0.8232490471721702,True
1717,"def __genmenuOnlyAllocated(menu): for submenu in menu.Submenus: __genmenuOnlyAllocated(submenu) if menu.OnlyUnallocated == True: tmp['cache'].addMenuEntries(menu.AppDirs) menuentries = [] for rule in menu.Rules: menuentries = rule.do(tmp['cache'].getMenuEntries(menu.AppDirs), rule.Type, 2) for menuentry in menuentries: <IF_STMT> menuentry.Parents.append(menu) menu.MenuEntries.append(menuentry)",if menuentry.Add == True:,if menuentry.Type == 'Submenu':,0.8897985053185035,0.83689703343176,False
1718,"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, 'func_name_highlighting', True) self.disabled_modules = get_list_opt(options, 'disabled_modules', []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._lua_builtins import MODULES for mod, func in iteritems(MODULES): <IF_STMT> self._functions.update(func) RegexLexer.__init__(self, **options)",if mod not in self.disabled_modules:,if mod in self.disabled_modules:,0.8966412907860166,0.828399516355805,False
1719,"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() <IF_STMT> break elif r: y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return ''.join(y)",if r is None:,if r == e:,0.7549647988270782,0.9036816878108535,False
1720,"def get_menu_items(node): aList = [] for child in node.children: for tag in ('@menu', '@item'): if child.h.startswith(tag): name = child.h[len(tag) + 1:].strip() <IF_STMT> aList.append(('%s %s' % (tag, name), get_menu_items(child), None)) else: b = g.splitLines(''.join(child.b)) aList.append((tag, name, b[0] if b else '')) break return aList",if tag == '@menu':,if child.b == '':,0.6981025104201384,0.8752376177722327,False
1721,"def import_suffix_generator(a_block, datatype=False): if datatype is False: for name, suffix in iteritems(a_block.component_map(Suffix)): <IF_STMT> yield (name, suffix) else: for name, suffix in iteritems(a_block.component_map(Suffix)): if suffix.import_enabled() is True and suffix.get_datatype() is datatype: yield (name, suffix)",if suffix.import_enabled() is True:,if suffix.import_enabled() is True and suffix.get_datatype() is datatype:,0.8907889108725644,0.7513169266927657,False
1722,"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError('Empty chown path') checkpath = root parts = path.split(os.sep) for part in parts: if part in ('.', '..'): raise PackagerError('. and .. is not allowed in chown path') checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1:] if not os.path.exists(checkpath): raise PackagerError(f'chown path {relpath} does not exist') <IF_STMT> raise PackagerError(f'chown path {relpath} is a soft link')",if os.path.islink(checkpath):,if not os.path.islink(relpath):,0.9619358113164722,0.9297257490286697,False
1723,"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get('partitions', [])) scanners = {} for part_name, part_scanner in list(syntax.get('scanner', {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get('formats', {}).items()): <IF_STMT> if fstyle.startswith('%(') and fstyle.endswith(')s'): key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return (partition_scanner, scanners, formats)","if isinstance(fstyle, basestring):","if isinstance(fstyle, str):",0.8693211599508515,0.9182210682909737,False
1724,"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or '' for line in commit_msg.splitlines(): parts = line.strip('# ').split(':', 1) key, val = parts if len(parts) > 1 else (parts[0], '') <IF_STMT> ci_names = val.replace(',', ' ').lower().split() if val else [] if len(ci_names) == 0 or ci.lower() in ci_names: result = True return result",if key == 'CI_KEEP_ALIVE':,if key == 'alive':,0.7324834732339108,0.8983343737277126,False
1725,"def get_note_title_file(note): mo = note_title_re.match(note.get('content', '')) if mo: fn = mo.groups()[0] fn = fn.replace(' ', '_') fn = fn.replace('/', '_') if not fn: return '' <IF_STMT> fn = unicode(fn, 'utf-8') else: fn = unicode(fn) if note_markdown(note): fn += '.mkdn' else: fn += '.txt' return fn else: return ''","if isinstance(fn, str):","if isinstance(fn, str):",0.7685408530450865,0.9062841320510342,True
1726,"def post(self, orgname, teamname): if _syncing_setup_allowed(orgname): try: team = model.team.get_organization_team(orgname, teamname) except model.InvalidTeamException: raise NotFound() config = request.get_json() status, err = authentication.check_group_lookup_args(config) <IF_STMT> raise InvalidRequest('Could not sync to group: %s' % err) model.team.set_team_syncing(team, authentication.federated_service, config) return team_view(orgname, team) raise Unauthorized()",if not status:,if status != 200:,0.8515757795534459,0.8661072626070159,False
1727,"def _marshalData(self): if self._cache == None: d = self._data s = '' s = time.strftime('%H:%M:%S', (0, 0, 0) + d + (0, 0, -1)) f = d[2] - int(d[2]) <IF_STMT> s += ('%g' % f)[1:] s += 'Z' self._cache = s return self._cache",if f != 0:,if f > 0:,0.8571233055832089,0.8752376177722327,False
1728,"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): if level_ref < 0: level_ref += len(levels) <IF_STMT> raise PatsyError('specified level %r is out of range' % (level_ref,)) return level_ref raise PatsyError('specified level %r not found' % (level_ref,))",if not 0 <= level_ref < len(levels):,if level_ref >= len(levels):,0.9173860754417799,0.8661072626070159,False
1729,"def iterfieldselect(source, field, where, complement, missing): it = iter(source) hdr = next(it) yield tuple(hdr) indices = asindices(hdr, field) getv = operator.itemgetter(*indices) for row in it: try: v = getv(row) except IndexError: v = missing <IF_STMT> yield tuple(row)",if bool(where(v)) != complement:,if v == where:,0.6950388492140698,0.862572866657237,False
1730,"def _test_wait_read_invalid_switch(self, sleep): sock1, sock2 = socket.socketpair() try: p = gevent.spawn(util.wrap_errors(AssertionError, socket.wait_read), sock1.fileno()) gevent.get_hub().loop.run_callback(switch_None, p) <IF_STMT> gevent.sleep(sleep) result = p.get() assert isinstance(result, AssertionError), result assert 'Invalid switch' in str(result), repr(str(result)) finally: sock1.close() sock2.close()",if sleep is not None:,if sleep:,0.7421423781801766,0.8901732118131125,False
1731,"def train(config, args): gan = setup_gan(config, inputs, args) test_batches = [] for i in range(args.steps): gan.step() <IF_STMT> correct_prediction = 0 total = 0 for x, y in gan.inputs.testdata(): prediction = gan.generator(x) correct_prediction += (torch.argmax(prediction, 1) == torch.argmax(y, 1)).sum() total += y.shape[0] accuracy = float(correct_prediction) / total * 100 print('accuracy: ', accuracy) return sum_metrics",if i % args.sample_every == 0 and i > 0:,if i % args.steps == 0:,0.8606570432277726,0.8665222382201849,False
1732,"def process_response(self, request, response, spider): if not response.body: return response for fmt, func in six.iteritems(self._formats): new_response = func(response) <IF_STMT> logger.debug('Decompressed response with format: %(responsefmt)s', {'responsefmt': fmt}, extra={'spider': spider}) return new_response return response",if new_response:,if new_response is not None:,0.736071899596723,0.8094220211349227,False
1733,"def detect_ssl_option(self): for option in self.ssl_options(): <IF_STMT> for other_option in self.ssl_options(): if option != other_option: if scan_argv(self.argv, other_option) is not None: raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option","if scan_argv(self.argv, option) is not None:",if option != 'default':,0.6505282124673226,0.8385130047130208,False
1734,"def load(cls, storefile, template_store): if not hasattr(storefile, 'read'): storefile = open(storefile, 'rb') store = cls.convertfile(storefile, template_store) for unit in store.units: <IF_STMT> continue if cls.needs_target_sync: unit.target = unit.source unit.rich_target = unit.rich_source return store",if unit.isheader():,if unit.source is None:,0.7906725174474154,0.8336104423443033,False
1735,"def _pre_get_table(self, _ctx, table_name): vsctl_table = self._get_table(table_name) schema_helper = self.schema_helper schema_helper.register_table(vsctl_table.table_name) for row_id in vsctl_table.row_ids: <IF_STMT> schema_helper.register_table(row_id.table) if row_id.name_column: schema_helper.register_columns(row_id.table, [row_id.name_column]) if row_id.uuid_column: schema_helper.register_columns(row_id.table, [row_id.uuid_column]) return vsctl_table",if row_id.table:,if row_id.table:,0.8961622051255811,0.8645707301556367,True
1736,"def __init__(self, pin=None, pull_up=False): super(InputDevice, self).__init__(pin) try: self.pin.function = 'input' pull = 'up' if pull_up else 'down' <IF_STMT> self.pin.pull = pull except: self.close() raise self._active_state = False if pull_up else True self._inactive_state = True if pull_up else False",if self.pin.pull != pull:,if pull:,0.7696286547431092,0.9051034981560222,False
1737,"def _increment_operations_count(self, operation, executed): with self._lock: <IF_STMT> self._executed_operations += 1 self._executed[operation.job_type] += 1 else: self._skipped[operation.job_type] += 1",if executed:,if executed:,0.8296256864240797,0.7778111223054219,True
1738,"def emit(self, type, info=None): ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: <IF_STMT> self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev]) elif type in self.__event_types_at_proxy: self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev])",if type in self.__proxy_properties__:,if type in self.__event_types_at_proxy:,0.8970437062010936,0.8169276475307028,False
1739,def validate_pull_secret(namespace): if namespace.pull_secret is None: warning = 'No --pull-secret provided: cluster will not include samples or operators from ' + 'Red Hat or from certified partners.' logger.warning(warning) else: try: <IF_STMT> raise Exception() except: raise InvalidArgumentValueError('Invalid --pull-secret.'),"if not isinstance(json.loads(namespace.pull_secret), dict):","if namespace.pull_secret not in ['samples', 'operators']:",0.8805637091907448,0.8132493528194856,False
1740,"def pack(types, *args): if len(types) != len(args): raise Exception('number of arguments does not match format string') port = StringIO() for type, value in zip(types, args): if type == 'V': write_vuint(port, value) elif type == 'v': write_vint(port, value) <IF_STMT> write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",elif type == 's':,elif type == 'b':,0.9311742942739225,0.9001816649635144,False
1741,"def data(self): if self._data is not None: return self._data el<IF_STMT> with open(self.path, 'rb') as jsonfile: data = jsonfile.read().decode('utf8') data = json.loads(data) self._data = data return self._data else: return dict()",if os.path.exists(self.path):,if self.path:,0.9054279662362543,0.8743414417652072,False
1742,"def interact(self): self.output.write('\n') while True: try: request = self.getline('help> ') <IF_STMT> break except (KeyboardInterrupt, EOFError): break request = strip(request) if len(request) > 2 and request[0] == request[-1] in (""'"", '""') and (request[0] not in request[1:-1]): request = request[1:-1] if lower(request) in ('q', 'quit'): break self.help(request)",if not request:,if not request:,0.8951906027300073,0.9000283069718913,True
1743,"def api_attachment_metadata(self): resp = [] for part in self.parts: <IF_STMT> continue k = {'content_type': part.block.content_type, 'size': part.block.size, 'filename': part.block.filename, 'id': part.block.public_id} content_id = part.content_id if content_id: if content_id[0] == '<' and content_id[-1] == '>': content_id = content_id[1:-1] k['content_id'] = content_id resp.append(k) return resp",if not part.is_attachment:,if part.block is None:,0.8618800946034878,0.8752376177722327,False
1744,"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term):] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u('%s is contained here:') % py.io.saferepr(term, maxsize=42)] for line in diff: if line.startswith(u('Skipping')): continue <IF_STMT> continue if line.startswith(u('+ ')): newdiff.append(u('  ') + line[2:]) else: newdiff.append(line) return newdiff",if line.startswith(u('- ')):,if verbose:,0.9434113048204886,0.933847757608669,False
1745,"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: <IF_STMT> API_CACHE = {} if API_CACHE.get(url) is None: API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",if API_CACHE is None:,if API_CACHE is None:,0.8200940290988069,0.8431339019329497,True
1746,"def __str__(self, prefix='', printElemNumber=0): res = '' if self.has_index_name_: res += prefix + 'index_name: %s\n' % self.DebugFormatString(self.index_name_) cnt = 0 for e in self.prefix_value_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'prefix_value%s: %s\n' % (elm, self.DebugFormatString(e)) cnt += 1 if self.has_value_prefix_: res += prefix + 'value_prefix: %s\n' % self.DebugFormatBool(self.value_prefix_) return res",if printElemNumber:,if printElemNumber:,0.8348285165800228,0.9374009563674955,True
1747,"def add_group(x, nl, in_group, mw): if len(x) == 0: return x if len(x) > 1 and (not in_group): <IF_STMT> return ['[['] + x + [']]'] mw.warn('Equation will multiplex and may produce inaccurate results (see manual)') return ['['] + x + [']']","if supports_group(x, nl):",if nl:,0.8982587724105318,0.9122561819614461,False
1748,def unfulfilled_items(self): unfulfilled_items = 0 for order_item in self.items.all(): <IF_STMT> aggr = order_item.deliver_item.aggregate(delivered=Sum('quantity')) unfulfilled_items += order_item.quantity - (aggr['delivered'] or 0) return unfulfilled_items,if not order_item.canceled:,if order_item.deliver_item:,0.8228716834682637,0.8318180062062374,False
1749,"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): <IF_STMT> data = self.tagged_blocks.get_data(key) for pattern in data: if pattern.pattern_id == pattern_id: return pattern return None",if key in self.tagged_blocks:,if self.tagged_blocks.has_data(key):,0.8798820538836052,0.8787142254774354,False
1750,"def query_lister(domain, query='', max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes(domain, query, attr_names, next_token=next_token) for item in rs: if max_items: <IF_STMT> raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",if num_results == max_items:,if num_results >= max_items:,0.9156839122517824,0.8780099567239787,False
1751,"def find_deprecated_settings(source): from celery.utils import deprecated for name, opt in flatten(NAMESPACES): <IF_STMT> deprecated.warn(description='The {0!r} setting'.format(name), deprecation=opt.deprecate_by, removal=opt.remove_by, alternative='Use the {0.alt} instead'.format(opt)) return source","if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):",if name in source:,0.6662717633887155,0.7709002428237395,False
1752,"def tearDown(self): """"""Shutdown the server."""""" try: <IF_STMT> self.server.stop(2.0) if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:,if self.server:,0.5103819074241628,0.7241577342575828,True
1753,"def broadcast_events(self, events): LOGGER.debug('Broadcasting events: %s', events) with self._subscribers_cv: subscribers = {conn: sub.copy() for conn, sub in self._subscribers.items()} if subscribers: for connection_id, subscriber in subscribers.items(): <IF_STMT> subscriber_events = [event for event in events if subscriber.is_subscribed(event)] event_list = EventList(events=subscriber_events) self._send(connection_id, event_list.SerializeToString())",if subscriber.is_listening():,"if isinstance(events, list):",0.9342953717280986,0.8875087479151215,False
1754,"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen([self._find_binary(), 'info', os.path.realpath(path)], stdout=PIPE, stderr=PIPE).communicate() except OSError: pass else: if stdout: for line in stdout.splitlines(): line = u(line).split(': ', 1) <IF_STMT> info[line[0]] = line[1] return info",if len(line) == 2:,if len(line) == 2:,0.7695127703478927,0.8780099567239787,True
1755,"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False <IF_STMT> return True if len(a) != b.len: return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",if a is None:,if a.ptr == b.ptr:,0.916855852007381,0.9019629427251674,False
1756,"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: <IF_STMT> raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs",if type(ah) is not ActionHandle:,if type(ah) is ActionHandle:,0.9152576925228176,0.8879659171421962,False
1757,"def startElement(self, name, attrs, connection): if name == 'Parameter': <IF_STMT> self[self._current_param.name] = self._current_param self._current_param = Parameter(self) return self._current_param",if self._current_param:,if self._current_param.name not in self:,0.7830534283685705,0.6750915335148621,False
1758,"def _find_class_in_descendants(self, search_key): for cls in self.primitive_classes: cls_key = (cls.__name__, cls.__module__) self.class_cache[cls_key] = cls <IF_STMT> return cls",if cls_key == search_key:,if cls_key in search_key:,0.5632503981056431,0.7101158913184162,False
1759,"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) if c2: v.append(sibling) <IF_STMT> sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",if sibling.getFirstChild():,if sibling.getFirstChild():,0.9192030858167652,0.8966773400768917,True
1760,"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): if i > 0: block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) <IF_STMT> route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",if i < 2:,if i < len(blocks) - 1:,0.9083010974711021,0.841020165317327,False
1761,"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: <IF_STMT> continue match = path if item.startswith('/') else basename if fnmatch.fnmatch(match, item.strip('/')): return True return False",if item.endswith('/') and (not is_dir):,if is_dir and item.startswith('.'):,0.8606741358620618,0.8169276475307028,False
1762,"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): if line.startswith('""""""'): found += 1 if found == 2: <IF_STMT> break lines.insert(i + 1, '') lines.insert(i + 2, division) break return '\n'.join(lines)",if division in '\n'.join(lines):,if i == 0:,0.918593838818174,0.8661072626070159,False
1763,"def buildImage(opt): dpath = os.path.join(opt['datapath'], 'COCO-IMG-2015') version = '1' if not build_data.built(dpath, version_string=version): print('[building image data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES[:1]: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):,if build_data.built(dpath):,0.9131714831871888,0.8827916928185874,True
1764,"def colorformat(text): if text[0:1] == '#': col = text[1:] <IF_STMT> return col elif len(col) == 3: return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == '': return '' assert False, 'wrong color format %r' % text",if len(col) == 6:,if len(col) == 2:,0.6341995397255188,0.8723360571509826,False
1765,"def tree_print(tree): for key in tree: print(key, end=' ') tree_element = tree[key] for subElem in tree_element: print(' -> ', subElem, end=' ') <IF_STMT> print('\n ') print()",if type(subElem) != str:,if subElem:,0.8225013865326334,0.8590888738245122,False
1766,"def is_dse_cluster(path): try: with open(os.path.join(path, 'CURRENT'), 'r') as f: name = f.readline().strip() cluster_path = os.path.join(path, name) filename = os.path.join(cluster_path, 'cluster.conf') with open(filename, 'r') as f: data = yaml.load(f) <IF_STMT> return True except IOError: return False",if 'dse_dir' in data:,if data['name'] == 'dse':,0.8845324402337951,0.8474968231198384,False
1767,"def delete_old_target_output_files(classpath_prefix): """"""Delete existing output files or symlinks for target."""""" directory, basename = os.path.split(classpath_prefix) pattern = re.compile('^{basename}(([0-9]+)(\\.jar)?|classpath\\.txt)$'.format(basename=re.escape(basename))) files = [filename for filename in os.listdir(directory) if pattern.match(filename)] for rel_path in files: path = os.path.join(directory, rel_path) <IF_STMT> safe_delete(path)",if os.path.islink(path) or os.path.isfile(path):,if os.path.isfile(path):,0.8842869385134562,0.9220630473024066,False
1768,"def test_files(self): dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): if n.endswith('.py') and (not n.startswith('bad')): names.append(os.path.join(test_dir, n)) for filename in names: <IF_STMT> print('Testing %s' % filename) source = read_pyfile(filename) self.check_roundtrip(source)",if test_support.verbose:,if self.verbose:,0.8944671738131387,0.9144061946646023,False
1769,"def __str__(self): if self.HasError(): return self.ErrorAsStr() else: string = self._action if self._target is not None: string += ' ""{target}""'.format(target=self._target) <IF_STMT> path = self._filename if self._lineno is not None: path += ':{lineno}'.format(lineno=self._lineno) string += ' ({path})'.format(path=path) return string",if self._filename is not None:,if self._filename is not None:,0.6404230329366254,0.8338542560892604,True
1770,"def extra_action_out(self, input_dict, state_batches, model, action_dist): with self._no_grad_context(): <IF_STMT> stats_dict = extra_action_out_fn(self, input_dict, state_batches, model, action_dist) else: stats_dict = parent_cls.extra_action_out(self, input_dict, state_batches, model, action_dist) return self._convert_to_non_torch_type(stats_dict)",if extra_action_out_fn:,if extra_action_out_fn:,0.8559591546065602,0.8590888738245122,True
1771,"def _retract_bindings(fstruct, inv_bindings, fs_class, visited): if id(fstruct) in visited: return visited.add(id(fstruct)) if _is_mapping(fstruct): items = fstruct.items() elif _is_sequence(fstruct): items = enumerate(fstruct) else: raise ValueError('Expected mapping or sequence') for fname, fval in items: if isinstance(fval, fs_class): <IF_STMT> fstruct[fname] = inv_bindings[id(fval)] _retract_bindings(fval, inv_bindings, fs_class, visited)",if id(fval) in inv_bindings:,if id(fval) in inv_bindings:,0.8040961828168622,0.8752376177722327,True
1772,"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): <IF_STMT> continue if not isinstance(dep.repo, WarehouseBaseRepo): continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())",if dep.repo is None:,if dep.name in repos:,0.9252616392815519,0.8474968231198384,False
1773,"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) <IF_STMT> continue deval = detyper(val) if deval is None: continue ctx[key] = deval self._detyped = ctx return ctx",if detyper is None:,if detyper is None:,0.9388204205785045,0.8806615362338783,True
1774,"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: <IF_STMT> field.delete() return if isinstance(self.data, FileStorage) and (not is_empty(self.data.stream)): if not field.grid_id: func = field.put else: func = field.replace func(self.data.stream, filename=self.data.filename, content_type=self.data.content_type)",if self._should_delete:,if field.grid_id:,0.9000292691386147,0.8996480074924822,False
1775,"def _load(container): if isinstance(container, str): <IF_STMT> with open(container, 'rb') as f: return pickle.load(f) else: return pickle.loads(container) elif isinstance(container, IOBase): return pickle.load(container) else: l.error('Cannot unpickle container of type %s', type(container)) return None",if all((c in string.printable for c in container)) and os.path.exists(container):,if os.path.isfile(container):,0.636121789593163,0.8827916928185874,False
1776,"def append_row(self, row): self.allocate_future_payments(row) self.set_invoice_details(row) self.set_party_details(row) self.set_ageing(row) if self.filters.get('group_by_party'): self.update_sub_total_row(row, row.party) <IF_STMT> self.append_subtotal_row(self.previous_party) self.previous_party = row.party self.data.append(row)",if self.previous_party and self.previous_party != row.party:,if self.previous_party:,0.8712923530907464,0.7778111223054219,False
1777,def gg1(): while 1: tt = 3 while tt > 0: trace.append(tt) val = (yield) <IF_STMT> tt = 10 trace.append('breaking early...') break tt -= 1 trace.append('try!'),if val is not None:,if val == 'continue':,0.6833971426097293,0.7965020533851944,False
1778,"def migrate_common_facts(facts): """"""Migrate facts from various roles into common"""""" params = {'node': 'portal_net', 'master': 'portal_net'} if 'common' not in facts: facts['common'] = {} for role in params.keys(): if role in facts: for param in params[role]: <IF_STMT> facts['common'][param] = facts[role].pop(param) return facts",if param in facts[role]:,if param in facts[role]:,0.9109290000403774,0.8692960007731574,True
1779,"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] <IF_STMT> if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] if self.do_manders: results += ['Manders'] if self.do_rwc: results += ['RWC'] if self.do_costes: results += ['Costes'] return results return []",if self.do_corr_and_slope:,if self.do_correlation:,0.6455596165608829,0.9298663600557577,False
1780,"def access_modes(self): """"""access_modes property"""""" if self._access_modes is None: self._access_modes = self.get_access_modes() <IF_STMT> self._access_modes = list(self._access_modes) return self._access_modes","if not isinstance(self._access_modes, list):","if isinstance(self._access_modes, list):",0.7344998015032298,0.7331765459202478,False
1781,"def unwrap_envelope(self, data, many): if many: <IF_STMT> if isinstance(data, InstrumentedList) or isinstance(data, list): self.context['total'] = len(data) return data else: self.context['total'] = data['total'] else: self.context['total'] = 0 data = {'items': []} return data['items'] return data",if data['items']:,if data['total'] is None:,0.9116573552756452,0.8431339019329497,False
1782,"def to_string(self, fmt='{:.4f}'): result_str = '' for key in self.measures: result = self.m_dict[key][0]() result_str += ','.join((fmt.format(x) for x in result)) <IF_STMT> else fmt.format(result) result_str += ',' return result_str[:-1]","if isinstance(result, tuple)",if len(result) > 1,0.8836564798369626,0.8105932471967202,False
1783,"def on_torrent_created(self, result): if not result: return self.dialog_widget.btn_create.setEnabled(True) self.dialog_widget.edit_channel_create_torrent_progress_label.setText('Created torrent') if 'torrent' in result: self.create_torrent_notification.emit({'msg': 'Torrent successfully created'}) <IF_STMT> self.add_torrent_to_channel(result['torrent']) self.close_dialog()",if self.dialog_widget.add_to_channel_checkbox.isChecked():,if 'torrent' in result:,0.8802995348184113,0.7629273292796576,False
1784,"def save(self): for var_name in self.default_config: <IF_STMT> if var_name in self.file_config: del self.file_config[var_name] else: self.file_config[var_name] = getattr(self, var_name) with open(self.config_path, 'w') as f: f.write(json.dumps(self.file_config, indent=2))","if getattr(self, var_name, None) == self.default_config[var_name]:","if hasattr(self, var_name):",0.8860222885298626,0.8196189957582152,False
1785,"def get_class_parameters(kwarg): ret = {'attrs': []} for key in ('rsc', 'fsc', 'usc'): <IF_STMT> ret['attrs'].append(['TCA_HFSC_%s' % key.upper(), {'m1': get_rate(kwarg[key].get('m1', 0)), 'd': get_time(kwarg[key].get('d', 0)), 'm2': get_rate(kwarg[key].get('m2', 0))}]) return ret",if key in kwarg:,if key in kwarg:,0.6829473142508117,0.8038019482772603,True
1786,"def forward(self, x): f_x = x if self.exp: f_x = self.exp_swish(self.exp_bn(self.exp(f_x))) f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x))) f_x = self.se(f_x) f_x = self.lin_proj_bn(self.lin_proj(f_x)) if self.has_skip: <IF_STMT> f_x = drop_connect(f_x, effnet_cfg.EN.DC_RATIO) f_x = x + f_x return f_x",if self.training and effnet_cfg.EN.DC_RATIO > 0.0:,if self.drop_connect:,0.9330134674895163,0.8935248372106969,False
1787,"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + '  --->>  ' + _distro_dir) user_input = read_input_uninstall() if user_input is not False: for index, _distro_dir in enumerate(distro_list): <IF_STMT> config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log('No distro installed on ' + config.usb_disk)",if index == user_input:,if user_input == _distro_dir:,0.7806100461645579,0.8879659171421962,False
1788,"def IMPORTFROM(self, node): if node.module == '__future__': <IF_STMT> self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == '*': self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if not self.futuresAllowed:,if self.futuresAllowed:,0.9446896296913001,0.9325718821645923,False
1789,"def _split_and_load(batch, ctx_list): """"""Split data to 1 batch each device."""""" new_batch = [] for _, data in enumerate(batch): <IF_STMT> new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)] else: new_data = [data.as_in_context(ctx_list[0])] new_batch.append(new_data) return new_batch","if isinstance(data, (list, tuple)):","if isinstance(data, (list, tuple)):",0.8619082388078203,0.8474968231198384,True
1790,"def wait_success(self, timeout=60 * 10): for i in range(timeout // 10): time.sleep(10) status = self.query_job() print('job {} status is {}'.format(self.job_id, status)) <IF_STMT> return True if status and status in [StatusSet.CANCELED, StatusSet.TIMEOUT, StatusSet.FAILED]: return False return False",if status and status == StatusSet.SUCCESS:,"if status and status in [StatusSet.SUCCESS, StatusSet.FAILED]:",0.9295310818668928,0.7884896239035346,False
1791,"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): <IF_STMT> rel_root = os.path.relpath(src_root, src_dir) else: rel_root = '' if skip_variables and rel_root.startswith('variables'): continue dst_root = os.path.join(dst_dir, rel_root) if not os.path.exists(dst_root): os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if src_root != src_dir:,if os.path.isdir(src_root):,0.8887183713680312,0.9122561819614461,False
1792,"def _make_padded_shapes(self, dataset, decoders): padded_shapes = dataset.output_shapes for i, hparams_i in enumerate(self._hparams.datasets): <IF_STMT> continue if not hparams_i['pad_to_max_seq_length']: continue text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(dataset, hparams_i, decoders[i], self.text_name(i), self.text_id_name(i)) padded_shapes.update(text_and_id_shapes) return padded_shapes",if not _is_text_data(hparams_i['data_type']):,if not hparams_i:,0.9132482987636366,0.8390782502060267,False
1793,"def format_errors(messages): errors = {} for k, v in messages.items(): key = camelize(k, uppercase_first_letter=False) <IF_STMT> errors[key] = format_errors(v) elif isinstance(v, list): errors[key] = v[0] return errors","if isinstance(v, dict):","if isinstance(v, dict):",0.6656071849467833,0.8266114125804572,True
1794,"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): <IF_STMT> for item in value: if isinstance(item, AST): self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)","if isinstance(value, list):","if isinstance(value, list):",0.8963206349727189,0.8547305998833805,True
1795,"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get('allow_css_overrides'): filename = self.view.file_name() filetypes = self.settings.get('markdown_filetypes') if filename and filetypes: for filetype in filetypes: <IF_STMT> css_filename = filename.rpartition(filetype)[0] + '.css' if os.path.isfile(css_filename): return u'<style>%s</style>' % load_utf8(css_filename) return ''",if filename.endswith(filetype):,if filetype.startswith('css'):,0.9402686136119998,0.8966773400768917,False
1796,"def clean(self): super().clean() if self.cluster.site is not None: for device in self.cleaned_data.get('devices', []): <IF_STMT> raise ValidationError({'devices': '{} belongs to a different site ({}) than the cluster ({})'.format(device, device.site, self.cluster.site)})",if device.site != self.cluster.site:,if device.site != self.cluster.site:,0.893887140544528,0.8169276475307028,True
1797,"def _setProcessPriority(process, nice_val, disable_gc): org_nice_val = Computer._process_original_nice_value try: process.nice(nice_val) Computer.in_high_priority_mode = nice_val != org_nice_val <IF_STMT> gc.disable() else: gc.enable() return True except psutil.AccessDenied: print2err('WARNING: Could not set process {} priority to {}'.format(process.pid, nice_val)) return False",if disable_gc:,if disable_gc:,0.7891451108616072,0.8935248372106969,True
1798,"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <IF_STMT> warnings.warn('{}: setting results name {!r} on {} expression may only return a single token for an And alternative, in future will return the full list of tokens'.format('warn_multiple_tokens_in_named_alternation', name, type(self).__name__), stacklevel=3) return super()._setResultsName(name, listAllMatches)","if any((isinstance(e, And) for e in self.exprs)):",if name in self.named_alternations:,0.8880532557133701,0.8661072626070159,False
1799,"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) if project.readme.markup != 'rst': content.append(project.readme.to_rst().path.name) path = project.package.path for fname in ('setup.cfg', 'setup.py'): <IF_STMT> content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append('/'.join(fpath.parts)) return '\n'.join(content)",if (path / fname).exists():,if fname.startswith(path):,0.9472402416546073,0.9122561819614461,False
1800,"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): if j < segmentCount: <IF_STMT> controlPointIndices[index] = 1 index += s[1] return controlPointIndices",if glyph.contours[i].segments[j].type == 'line':,if s[0] == va[j]:,0.8832698386202231,0.8627586293513119,False
1801,"def MergeFrom(self, other): if self.message_class is not None: if other.Parse(self.message_class): self.message.MergeFrom(other.message) elif other.message_class is not None: <IF_STMT> self.message = other.message_class() self.message_class = other.message_class self.message.MergeFrom(other.message) else: self.message += other.message",if not self.Parse(other.message_class):,if self.message is None:,0.9005367081186445,0.8105932471967202,False
1802,def remove_old_snapshot(install_dir): logging.info('Removing any old files in {}'.format(install_dir)) for file in glob.glob('{}/*'.format(install_dir)): try: <IF_STMT> os.unlink(file) elif os.path.isdir(file): shutil.rmtree(file) except Exception as error: logging.error('Error: {}'.format(error)) sys.exit(1),if os.path.isfile(file):,if os.path.isfile(file):,0.9018806723610188,0.8531413606256201,True
1803,"def writexml(self, stream, indent='', addindent='', newl='', strip=0, nsprefixes={}, namespace=''): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue if not isinstance(v, str): v = str(v) <IF_STMT> v = ' '.join(v.split()) val = escape(v) w(val)",if strip:,"if isinstance(v, list):",0.8981614226865408,0.897752847848028,False
1804,"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) <IF_STMT> if attribute.required: raise ValidationError(_('%(attr)s attribute cannot be blank') % {'attr': attribute.code}) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError(_('%(attr)s attribute %(err)s') % {'attr': attribute.code, 'err': e})",if value is None:,if value is None:,0.6332992529240745,0.8627586293513119,True
1805,"def PyJsHoisted_BinaryExpression_(node, parent, this, arguments, var=var): var = Scope({u'node': node, u'this': this, u'arguments': arguments, u'parent': parent}, var) var.registers([u'node', u'parent']) if PyJsStrictEq(var.get(u'node').get(u'operator'), Js(u'in')): <IF_STMT> return var.get(u'true') if var.get(u't').callprop(u'isFor', var.get(u'parent')): return var.get(u'true') return Js(False)","if var.get(u't').callprop(u'isVariableDeclarator', var.get(u'parent')):","if var.get(u't').callprop(u'isFor', var.get(u'parent')):",0.9203436888648856,0.8592899528284996,False
1806,"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) <IF_STMT> raise TypeError('on must be a name or field, not: {0}'.format(n)) elif n not in fields: raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))","if not isinstance(n, _strtypes):","elif not isinstance(n, (str, unicode)):",0.882766551540332,0.9002470831246567,False
1807,"def encode(self, msg): """"""Encodes the message to the stream encoding."""""" stream = self.stream rv = msg + '\n' if PY2 and is_unicode(rv) or not (PY2 or is_unicode(rv) or _is_text_stream(stream)): enc = self.encoding <IF_STMT> enc = getattr(stream, 'encoding', None) or 'utf-8' rv = rv.encode(enc, 'replace') return rv",if enc is None:,if not enc:,0.7441313551096665,0.9022045190074797,False
1808,"def color_convert(self, to_color_space, preserve_alpha=True): if to_color_space == self.color_space and preserve_alpha: return self else: pixels = pixels_as_float(self.pixels) converted = convert_color(pixels, self.color_space, to_color_space, preserve_alpha) <IF_STMT> return None return Image(converted, to_color_space)",if converted is None:,if converted is None:,0.8233618255767912,0.8105932471967202,True
1809,"def seek(self, pos): if self.closed: raise IOError('Cannot seek on a closed file') for n, idx in enumerate(self._indexes[::-1]): if idx.offset <= pos: <IF_STMT> self._idxiter = iter(self._indexes[-(n + 1):]) self._nextidx() break else: raise Exception('Cannot seek to pos') self._curfile.seek(pos - self._curidx.offset)",if idx != self._curidx:,if n < len(self._indexes):,0.8443166531227162,0.8592377270804451,False
1810,"def load_from_json(self, node_data: dict, import_version: float): if import_version <= 0.08: self.image_pointer = unpack_pointer_property_name(bpy.data.images, node_data, 'image_name') <IF_STMT> proposed_name = node_data.get('image_name') self.info(f'image data not found in current {proposed_name}')",if not self.image_pointer:,if self.image_pointer is None:,0.7119274143675552,0.7965020533851944,False
1811,"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == 'Average': self._local_aggregators.append(_AverageAggregator()) <IF_STMT> self._local_aggregators.append(_CountAggregator()) elif operator == 'Max': self._local_aggregators.append(_MaxAggregator()) elif operator == 'Min': self._local_aggregators.append(_MinAggregator()) elif operator == 'Sum': self._local_aggregators.append(_SumAggregator())",elif operator == 'Count':,elif operator == 'Count':,0.9278318386594478,0.8692960007731574,True
1812,"def attrgetter(item): items = [None] * len(attribute) for i, attribute_part in enumerate(attribute): item_i = item for part in attribute_part: item_i = environment.getitem(item_i, part) <IF_STMT> item_i = postprocess(item_i) items[i] = item_i return items",if postprocess is not None:,if postprocess:,0.8299979536717872,0.8866029039778043,False
1813,def work(self): while True: timeout = self.timeout <IF_STMT> timeout = self.idle_timeout log.debug('Wait for {}'.format(timeout)) fetch.wait(timeout) if shutting_down.is_set(): log.info('Stop fetch worker') break self.fetch(),if idle.is_set():,if self.idle_timeout is not None:,0.871485124239581,0.7297349727547102,False
1814,"def testCoreInterfaceIntInputData(): result_testing = False for _ in range(10): hsyncnet_instance = hsyncnet([[1], [2], [3], [20], [21], [22]], 2, initial_type.EQUIPARTITION, ccore=True) analyser = hsyncnet_instance.process() <IF_STMT> result_testing = True break assert result_testing",if len(analyser.allocate_clusters(0.1)) == 2:,if analyser.get_input_data():,0.8753344104577808,0.8787142254774354,False
1815,"def _gen(): buf = [] iterable = dataset() try: while len(buf) < buffer_size: buf.append(next(iterable)) while 1: i = random.randint(0, buffer_size - 1) n = next(iterable) yield buf[i] buf[i] = n except StopIteration: <IF_STMT> random.shuffle(buf) for i in buf: yield i",if len(buf):,if random.random() < 0.5:,0.9007651522722303,0.8661072626070159,False
1816,"def debug_tree(tree): l = [] for elt in tree: if isinstance(elt, (int, long)): l.append(_names.get(elt, elt)) <IF_STMT> l.append(elt) else: l.append(debug_tree(elt)) return l","elif isinstance(elt, str):","elif isinstance(elt, str):",0.8722546728647367,0.7848518349390632,True
1817,"def reverse_code(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None: PreregistrationUser = apps.get_model('zerver', 'PreregistrationUser') for user in PreregistrationUser.objects.all(): <IF_STMT> user.invited_as_admin = True else: user.invited_as_admin = False user.save(update_fields=['invited_as_admin'])",if user.invited_as == 2:,if user.is_staff:,0.881800663295744,0.8466657105524215,False
1818,"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith('>>%s' % section_name): in_section = True elif in_section: <IF_STMT> break out.append(line.rstrip('\r\n')) return out",if line.startswith('>>END'):,if line.startswith('>>%s' % section_name):,0.9018898912861807,0.8555308664663046,False
1819,"def determine_block_hints(self, text): hints = '' if text: if text[0] in ' \n\x85\u2028\u2029': hints += str(self.best_indent) <IF_STMT> hints += '-' elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029': hints += '+' return hints",if text[-1] not in '\n\x85\u2028\u2029':,elif text[0] in ' \n\x85\u2028\u2029':,0.8433032302754092,0.8148691130388024,False
1820,def database_app(request): if request.param == 'postgres_app': if not which('initdb'): pytest.skip('initdb must be on PATH for postgresql fixture') if not psycopg2: pytest.skip('psycopg2 must be installed for postgresql fixture') if request.param == 'sqlite_rabbitmq_app': <IF_STMT> pytest.skip('rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset') return request.getfixturevalue(request.param),if not os.environ.get('GALAXY_TEST_AMQP_INTERNAL_CONNECTION'):,if not GALAXY_TEST_AMQP_INTERNAL_CONNECTION:,0.9193865171484437,0.9000283069718913,False
1821,"def do_rollout(agent, env, num_steps, render=False): total_rew = 0 ob = env.reset() for t in range(num_steps): a = agent.act(ob) ob, reward, done, _info = env.step(a) total_rew += reward <IF_STMT> env.render() if done: break return (total_rew, t + 1)",if render and t % 3 == 0:,if render:,0.9407769806963221,0.9024521756077707,False
1822,"def _handle_subrepos(self, ctx, dirty_trees): substate = util.parse_hgsubstate(ctx['.hgsubstate'].data().splitlines()) sub = util.OrderedDict() if '.hgsub' in ctx: sub = util.parse_hgsub(ctx['.hgsub'].data().splitlines()) for path, sha in substate.iteritems(): <IF_STMT> continue d = os.path.dirname(path) dirty_trees.add(d) tree = self._dirs.setdefault(d, dulobjs.Tree()) tree.add(os.path.basename(path), dulobjs.S_IFGITLINK, sha)",if path in sub and (not sub[path].startswith('[git]')):,if path in sub:,0.7270609303192073,0.8474968231198384,False
1823,"def get_property_file_image_choices(self, pipeline): columns = pipeline.get_measurement_columns() image_names = [] for column in columns: object_name, feature, coltype = column[:3] choice = feature[len(C_FILE_NAME) + 1:] <IF_STMT> image_names.append(choice) return image_names",if object_name == 'Image' and feature.startswith(C_FILE_NAME):,if choice in C_FILE_NAME:,0.8329812665789713,0.8038019482772603,False
1824,"def check_all_decorator_order(): """"""Check that in all test files, the slow decorator is always last."""""" errors = [] for fname in os.listdir(PATH_TO_TESTS): <IF_STMT> filename = os.path.join(PATH_TO_TESTS, fname) new_errors = check_decorator_order(filename) errors += [f'- {filename}, line {i}' for i in new_errors] if len(errors) > 0: msg = '\n'.join(errors) raise ValueError(f'The parameterized decorator (and its variants) should always be first, but this is not the case in the following files:\n{msg}')",if fname.endswith('.py'):,if fname.endswith('.py'):,0.8494218506407437,0.9469051104650212,True
1825,"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: if col and col.get_title() == _('Active'): <IF_STMT> client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",if self.watchdirs[watchdir_id]['enabled']:,if client.autoadd.is_watchdir_enabled(watchdir_id):,0.7896590081385492,0.8696398662122882,False
1826,"def get_conv_output_size(input_size, kernel_size, stride, padding, dilation): ndim = len(input_size) output_size = [] for i in range(ndim): size = (input_size[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) // stride[i] + 1 <IF_STMT> output_size.append(1) else: output_size.append(size) return output_size",if kernel_size[i] == -1:,if size == 0:,0.9515755680750231,0.8692960007731574,False
1827,"def from_location(cls, location, basename, metadata=None, **kw): project_name, version, py_version, platform = [None] * 4 basename, ext = os.path.splitext(basename) if ext.lower() in ('.egg', '.egg-info'): match = EGG_NAME(basename) <IF_STMT> project_name, version, py_version, platform = match.group('name', 'ver', 'pyver', 'plat') return cls(location, metadata, project_name=project_name, version=version, py_version=py_version, platform=platform, **kw)",if match:,if match:,0.6856860413209281,0.9184043388013005,True
1828,"def __new__(metacls, typename, bases, namespace): annotations = namespace.get('__annotations__', {}) for t in annotations.values(): <IF_STMT> for ut in t.__args__: _assert_tensorizer_type(ut) else: _assert_tensorizer_type(t) return super().__new__(metacls, typename, bases, namespace)","if getattr(t, '__origin__', '') is Union:","if hasattr(t, '__args__'):",0.884763043750242,0.8266114125804572,False
1829,"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get('content-type') if ct: ct, options = parse_options_header(ct) charset = options.get('charset') <IF_STMT> return self.json(charset) elif ct.startswith('text/'): return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",if ct in JSON_CONTENT_TYPES:,if ct.startswith('application/json/'):,0.910550874352738,0.9076141716697395,False
1830,"def get_full_path(path): if '://' not in path: path = os.path.join(self.AUTO_COLL_TEMPL, path, '') <IF_STMT> path = os.path.join(abs_path, path) return path",if abs_path:,if abs_path:,0.6738523657141827,0.803154665668484,True
1831,"def __getitem__(self, name_or_path): if isinstance(name_or_path, integer_types): return list.__getitem__(self, name_or_path) elif isinstance(name_or_path, tuple): try: val = self for fid in name_or_path: <IF_STMT> raise KeyError val = val[fid] return val except (KeyError, IndexError): raise KeyError(name_or_path) else: raise TypeError(self._INDEX_ERROR % name_or_path)","if not isinstance(val, FeatStruct):",if fid not in val:,0.9091668440332246,0.8380055871435848,False
1832,"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: if isinstance(s, (tree.Scope, tree.Flow)): return scan(s) or s <IF_STMT> return scan(s) return None","elif s.type in ('suite', 'decorated'):","elif isinstance(s, tree.Function):",0.6381577343265149,0.8196189957582152,False
1833,"def _get_key(self): if not self.key: self._channel.send(u'pake', self.msg1) pake_msg = self._channel.get(u'pake') self.key = self.sp.finish(pake_msg) self.verifier = self.derive_key(u'wormhole:verifier') <IF_STMT> return confkey = self.derive_key(u'wormhole:confirmation') nonce = os.urandom(CONFMSG_NONCE_LENGTH) confmsg = make_confmsg(confkey, nonce) self._channel.send(u'_confirm', confmsg)",if not self._send_confirm:,if not self.verifier:,0.6519959532404428,0.8498644646741501,False
1834,"def executeScript(self, script): if len(script) > 0: commands = [] for l in script: extracted = self.extract_command(l) <IF_STMT> commands.append(extracted) for command in commands: cmd, argv = command self.dispatch_command(cmd, argv)",if extracted:,if extracted is not None:,0.7146863215765531,0.7909601595885504,False
1835,"def create_path(n, fullname, meta): if meta: meta.create_path(fullname) else: unlink(fullname) if stat.S_ISDIR(n.mode): mkdirp(fullname) <IF_STMT> os.symlink(n.readlink(), fullname)",elif stat.S_ISLNK(n.mode):,elif stat.S_ISREG(n.mode):,0.8661018097386779,0.7670387248467656,False
1836,def get_cycle(self): if self.has_cycle(): cross_node = self.path[-1] <IF_STMT> return self.path[self.path.index(cross_node):] else: return self.path return [],if self.path.count(cross_node) > 1:,if cross_node in self.path:,0.5347234810323278,0.6540585844910979,False
1837,"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): <IF_STMT> depth += 1 elif str_in[pos] == end_tag: depth -= 1 if depth == 0: break sel = str_in[start_pos + 1:pos] return sel",if str_in[pos] == start_tag:,if str_in[pos] == start_tag:,0.9171622143171478,0.9001816649635144,True
1838,"def device(self): """"""Device on which the data array of this variable reside."""""" if self._device is None: <IF_STMT> self._device = backend.CpuDevice() else: self._device = backend.get_device_from_array(self._data[0]) return self._device",if self._data[0] is None:,if len(self._data) == 0:,0.6453332328352932,0.7965020533851944,False
1839,"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException('Item does not exist!') <IF_STMT> raise ItemNotFoundException(e.get_dbus_message()) if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED): raise SecretServiceNotAvailableException(e.get_dbus_message()) raise",if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT:,if e.get_dbus_name() == DBUS_NOT_FOUND:,0.9073666330415846,0.828399516355805,False
1840,"def run(self): """"""Continual loop evaluating when_statements"""""" while len(self.library) > 0: for name, expression in self.library.items(): <IF_STMT> del self.library[name] else: expression.evaluate() sleep(0.01) return",if expression.remove_me == True:,"if isinstance(expression, WhenStatement):",0.8655276763188899,0.7947545184555568,False
1841,"def _source_target_path(source, source_path, source_location): target_path_attr = source.target_path or source.resdef.target_path if source.preserve_path: <IF_STMT> log.warning(""target-path '%s' specified with preserve-path - ignoring"", target_path_attr) return os.path.relpath(os.path.dirname(source_path), source_location) else: return target_path_attr or source.resdef.target_path or ''",if target_path_attr:,if target_path_attr:,0.5899738137649256,0.8787142254774354,True
1842,"def _load_user_from_header(self, header): if self._header_callback: user = self._header_callback(header) <IF_STMT> app = current_app._get_current_object() user_loaded_from_header.send(app, user=user) return user return None",if user is not None:,if user:,0.7249293059941937,0.7912619863720214,False
1843,"def setup(cls): """"""Check dependencies and warn about firewalling"""""" pathCheck('brctl', moduleName='bridge-utils') for table in ('arp', 'ip', 'ip6'): cmd = 'sysctl net.bridge.bridge-nf-call-%stables' % table out = quietRun(cmd).strip() <IF_STMT> warn('Warning: Linux bridge may not work with', out, '\n')",if out.endswith('1'):,if out:,0.7210371003884993,0.8966773400768917,False
1844,"def _browse_your_music(web_client, variant): if not web_client.logged_in: return [] if variant in ('tracks', 'albums'): items = flatten([page.get('items', []) for page in web_client.get_all(f'me/{variant}', params={'market': 'from_token', 'limit': 50}) if page]) <IF_STMT> return list(translator.web_to_track_refs(items)) else: return list(translator.web_to_album_refs(items)) else: return []",if variant == 'tracks':,if variant == 'tracks':,0.6392481002566285,0.8516228624291206,True
1845,"def reset_styling(self): for edge in self.fsm_graph.edges_iter(): style_attr = self.fsm_graph.style_attributes.get('edge', {}).get('default') edge.attr.update(style_attr) for node in self.fsm_graph.nodes_iter(): <IF_STMT> style_attr = self.fsm_graph.style_attributes.get('node', {}).get('inactive') node.attr.update(style_attr) for sub_graph in self.fsm_graph.subgraphs_iter(): style_attr = self.fsm_graph.style_attributes.get('graph', {}).get('default') sub_graph.graph_attr.update(style_attr)",if 'point' not in node.attr['shape']:,if node.state == 'inactive':,0.9011627095971232,0.8228500218338367,False
1846,"def set_message_type_visibility(self, message_type: MessageType): try: rows = {i for i, msg in enumerate(self.proto_analyzer.messages) <IF_STMT>} if message_type.show: self.ui.tblViewProtocol.show_rows(rows) else: self.ui.tblViewProtocol.hide_rows(rows) except Exception as e: logger.exception(e)",if msg.message_type == message_type,if msg.message_type_visibility == message_type,0.5903972661936234,0.7801270245332924,False
1847,"def POP(cpu, *regs): for reg in regs: val = cpu.stack_pop(cpu.address_bit_size // 8) <IF_STMT> cpu._set_mode_by_val(val) val = val & ~1 reg.write(val)","if reg.reg in ('PC', 'R15'):",if cpu.mode_by_val:,0.9189609490084247,0.8137489370974955,False
1848,"def processMovie(self, atom): for field in atom: if 'track' in field: self.processTrack(field['track']) <IF_STMT> self.processMovieHeader(field['movie_hdr'])",if 'movie_hdr' in field:,if 'movie_hdr' in field:,0.5922061406643189,0.6889656775362827,True
1849,"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) <IF_STMT> if auto: update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",if remote_version > local_version:,if local_version > remote_version:,0.5721584400916987,0.8038019482772603,False
1850,"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 <IF_STMT> values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",if i >= 0 and i < len(selections):,if i >= 0:,0.9347870502907988,0.8964173245779284,False
1851,"def find_int_identifiers(directory): results = find_rules(directory, has_int_identifier) print('Number of rules with integer identifiers: %d' % len(results)) for result in results: rule_path = result[0] product_yaml_path = result[1] product_yaml = None <IF_STMT> product_yaml = yaml.open_raw(product_yaml_path) fix_file(rule_path, product_yaml, fix_int_identifier)",if product_yaml_path is not None:,if product_yaml_path:,0.8410691516705506,0.8966773400768917,False
1852,def condition(self): if self.__condition is None: if len(self.flat_conditions) == 1: self.__condition = self.flat_conditions[0] <IF_STMT> self.__condition = lambda _: True else: self.__condition = lambda x: all((cond(x) for cond in self.flat_conditions)) return self.__condition,elif len(self.flat_conditions) == 0:,elif self.flat_conditions[0].condition is None:,0.7105630188775178,0.828399516355805,False
1853,"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: <IF_STMT> continue scene_name, scene_season = scene_exception.split('|') if season == scene_season: scene_exceptions.append(scene_name) return scene_exceptions",if not len(scene_exception) == 2:,if scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith('|') or scene_exception.startswith(',0.7890718441212701,0.4660385632403427,False
1854,"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): <IF_STMT> break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",if idx >= len(selections):,if index == 0:,0.938464634546024,0.9036816878108535,False
1855,"def to_tool_path(self, path_or_uri_like, **kwds): if '://' not in path_or_uri_like: path = path_or_uri_like else: uri_like = path_or_uri_like <IF_STMT> raise Exception('Invalid URI passed to get_tool_source') scheme, rest = uri_like.split(':', 2) if scheme not in self.resolver_classes: raise Exception('Unknown tool scheme [{}] for URI [{}]'.format(scheme, uri_like)) path = self.resolver_classes[scheme]().get_tool_source_path(uri_like) return path",if ':' not in path_or_uri_like:,if not uri_like.startswith('://'):,0.9356777787394033,0.9042878500265974,False
1856,def mainWindow(): global MW if not MW: for i in qApp.topLevelWidgets(): <IF_STMT> MW = i return MW return None else: return MW,if i.objectName() == 'MainWindow':,if qApp.isWindowVisible(i):,0.8275235887704615,0.8318180062062374,False
1857,"def async_get_service(hass, config, discovery_info=None): """"""Get the demo notification service."""""" for account, account_dict in hass.data[DATA_ALEXAMEDIA]['accounts'].items(): for key, _ in account_dict['devices']['media_player'].items(): <IF_STMT> _LOGGER.debug('%s: Media player %s not loaded yet; delaying load', hide_email(account), hide_serial(key)) return False return AlexaNotificationService(hass)",if key not in account_dict['entities']['media_player']:,if account == key:,0.9010452928833748,0.8474968231198384,False
1858,"def _migrate_bool(self, name: str, true_value: str, false_value: str) -> None: if name not in self._settings: return values = self._settings[name] if not isinstance(values, dict): return for scope, val in values.items(): <IF_STMT> new_value = true_value if val else false_value self._settings[name][scope] = new_value self.changed.emit()","if isinstance(val, bool):","if isinstance(val, bool):",0.7157704277159455,0.8902579342581529,True
1859,"def send(self, data, flags=0): self._checkClosed() if self._sslobj: <IF_STMT> raise ValueError('non-zero flags not allowed in calls to send() on %s' % self.__class__) return self._sslobj.write(data) else: return socket.send(self, data, flags)",if flags != 0:,if flags != 0:,0.7043142065291721,0.8105932471967202,True
1860,"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) <IF_STMT> if init_service and init_service in dep_cnt['_deps']: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if dep_cnt:,if dep_cnt:,0.920130884543557,0.9076141716697395,True
1861,"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith('/'): records = as_dict(path + item, version, section) if records: result[item[:-1]] = records <IF_STMT> idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",elif is_dict.match(item):,elif item.endswith('/'):,0.9360937025787228,0.9443716053164669,False
1862,"def PrintColGroup(col_names, schema): """"""Print HTML colgroup element, used for JavaScript sorting."""""" print('  <colgroup>') for i, col in enumerate(col_names): if col.endswith('_HREF'): continue <IF_STMT> css_class = 'number' else: css_class = 'case-insensitive' print('<col id=""{}"" type=""{}"" />'.format(col, css_class)) print('  </colgroup>')",if schema.IsNumeric(col):,if i == 0:,0.9061132145960792,0.8516228624291206,False
1863,"def check_region(self, region): for other in self.regions: <IF_STMT> continue if other.start < region.start < other.end or other.start < region.end < other.end: raise Exception('%r overlaps with %r' % (region, other))",if other is region:,if region.start == other.start:,0.8132427639238106,0.8169276475307028,False
1864,"def _write_value(self, rng, value, scalar): if rng.api and value: <IF_STMT> value = value[0][0] else: rng = rng.resize(len(value), len(value[0])) rng.raw_value = value",if scalar:,if scalar:,0.8390907643547721,0.8232490471721702,True
1865,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.9001195620339053,0.8555308664663046,True
1866,"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): sv_faces = [] for i, face in enumerate(dcel_mesh.faces): <IF_STMT> 'Face ({}) has inner components! Sverchok cant show polygons with holes.'.format(i) if not face.outer or del_flag in face.flags: continue if only_select and (not face.select): continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",if face.inners and face.outer:,if face.inner:,0.804709452521658,0.9220450449751959,False
1867,"def _get_x_for_y(self, xValue, x, y): x_value = str(xValue) for anime in self.xmlMap.findall('anime'): try: <IF_STMT> return int(anime.get(y, 0)) except ValueError as e: continue return 0","if anime.get(x, False) == x_value:","if anime.get(x, 0) == x_value:",0.8495099164443174,0.7506346798217074,False
1868,"def dir_copy(src_dir, dest_dir, merge_if_exists=True): try: if not os.path.exists(dest_dir): shutil.copytree(src_dir, dest_dir) <IF_STMT> merge_dir(src_dir, dest_dir) except OSError as e: if e.errno == errno.ENOTDIR: shutil.copy(src_dir, dest_dir) else: logging.error('Could not copy %s to %s', src_dir, dest_dir)",elif merge_if_exists:,elif merge_if_exists:,0.8779056421858201,0.8866029039778043,True
1869,"def mapping(self): m = {} if getGdriveCredentialsFile() is not None: m['gdrive'] = '' unknown = 0 for f in self.scan: bits = f.split('#', 2) if len(bits) == 1: label = os.path.basename(f) else: label = bits[1] <IF_STMT> label = 'L' + str(unknown) unknown += 1 m[label] = bits[0] return m",if not label or len(label) == 0 or label == '':,if unknown:,0.8247861882353629,0.926934323706186,False
1870,"def get_tag_values(self, event): http = event.interfaces.get('sentry.interfaces.Http') if not http: return [] if not http.headers: return [] headers = http.headers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: if key != 'User-Agent': continue ua = Parse(value) <IF_STMT> continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",if not ua:,if not ua:,0.9337029071266691,0.9118021019905903,True
1871,"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): if g: <IF_STMT> t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield (k, tuple(t))",if i in direct_upstreams:,if i in direct_upstreams:,0.9129560506040695,0.8879659171421962,True
1872,def process_question(qtxt): question = '' skip = False for letter in qtxt: <IF_STMT> skip = True if letter == '>': skip = False if skip: continue if letter.isalnum() or letter == ' ': if letter == ' ': letter = '_' question += letter.lower() return question,if letter == '<':,if letter == '<':,0.9027837743725917,0.8832000938217648,True
1873,"def _module_repr_from_spec(spec): """"""Return the repr to use for the module."""""" name = '?' if spec.name is None else spec.name if spec.origin is None: if spec.loader is None: return '<module {!r}>'.format(name) else: return '<module {!r} ({!r})>'.format(name, spec.loader) el<IF_STMT> return '<module {!r} from {!r}>'.format(name, spec.origin) else: return '<module {!r} ({})>'.format(spec.name, spec.origin)",if spec.has_location:,if spec.name is None:,0.6537192998173509,0.8902056737869248,False
1874,"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) if self.any_match: if result: return not self.inverse el<IF_STMT> return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if not result:,if result:,0.9201854681847015,0.9051034981560222,False
1875,"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith('java'): <IF_STMT> interval = sys.getswitchinterval() sys.setswitchinterval(1e-06) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith('java'): if hasattr(sys, 'setswitchinterval'): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)","if hasattr(sys, 'getswitchinterval'):","if hasattr(sys, 'setswitchinterval'):",0.6327918088873746,0.8783650674919876,False
1876,"def record_expected_exportable_production(self, ticks): """"""Record the amount of production that should be transferred to other islands."""""" for (quota_holder, resource_id), amount in self._low_priority_requests.items(): <IF_STMT> self._settlement_manager_id[quota_holder] = WorldObject.get_object_by_id(int(quota_holder[1:].split(',')[0])).settlement_manager.worldid self.trade_storage[self._settlement_manager_id[quota_holder]][resource_id] += ticks * amount",if quota_holder not in self._settlement_manager_id:,if quota_holder not in self._settlement_manager_id:,0.8403657557938311,0.7975010608178975,True
1877,"def _method_events_callback(self, values): try: previous_echoed = values['child_result_list'][-1].decode().split('\n')[-2].strip() <IF_STMT> return 'echo foo2\n' elif previous_echoed.endswith('foo2'): return 'echo foo3\n' elif previous_echoed.endswith('foo3'): return 'exit\n' else: raise Exception('Unexpected output {0!r}'.format(previous_echoed)) except IndexError: return 'echo foo1\n'",if previous_echoed.endswith('foo1'):,if previous_echoed.endswith('foo1'):,0.6415348592460679,0.8787142254774354,True
1878,"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None): if cluster_identifier: cluster_snapshots = [] for snapshot in self.snapshots.values(): <IF_STMT> cluster_snapshots.append(snapshot) if cluster_snapshots: return cluster_snapshots if snapshot_identifier: if snapshot_identifier in self.snapshots: return [self.snapshots[snapshot_identifier]] raise ClusterSnapshotNotFoundError(snapshot_identifier) return self.snapshots.values()",if snapshot.cluster.cluster_identifier == cluster_identifier:,if snapshot.identifier == cluster_identifier:,0.764962041509086,0.828399516355805,False
1879,def get_snippet_edit_handler(model): if model not in SNIPPET_EDIT_HANDLERS: <IF_STMT> edit_handler = model.edit_handler else: panels = extract_panel_definitions_from_model_class(model) edit_handler = ObjectList(panels) SNIPPET_EDIT_HANDLERS[model] = edit_handler.bind_to(model=model) return SNIPPET_EDIT_HANDLERS[model],"if hasattr(model, 'edit_handler'):","if hasattr(model, 'edit_handler'):",0.8128637602497301,0.80377750806414,True
1880,"def start(): if os.environ.get('RUN_MAIN') != 'true': try: exit_code = restart_with_reloader() <IF_STMT> os.kill(os.getpid(), -exit_code) else: sys.exit(exit_code) except KeyboardInterrupt: pass",if exit_code < 0:,if exit_code == 0:,0.5509136143963926,0.7098232254187811,False
1881,"def discover(self, *objlist): ret = [] for l in self.splitlines(): if len(l) < 5: continue <IF_STMT> continue try: int(l[2]) int(l[3]) except: continue ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",if l[0] == 'Filename':,if l[0] == 'DISCARD':,0.9061811189184065,0.828399516355805,False
1882,"def ipfs_publish(self, lib): with tempfile.NamedTemporaryFile() as tmp: self.ipfs_added_albums(lib, tmp.name) try: <IF_STMT> cmd = 'ipfs add --nocopy -q '.split() else: cmd = 'ipfs add -q '.split() cmd.append(tmp.name) output = util.command_output(cmd) except (OSError, subprocess.CalledProcessError) as err: msg = 'Failed to publish library. Error: {0}'.format(err) self._log.error(msg) return False self._log.info('hash of library: {0}', output)",if self.config['nocopy']:,if lib.version == '2.0':,0.7018128343492523,0.8923575006167597,False
1883,"def spends(self): spends = defaultdict(list) utxos = self.mempool_utxos() for tx_hash, tx in self.txs.items(): for n, input in enumerate(tx.inputs): if input.is_generation(): continue prevout = (input.prev_hash, input.prev_idx) <IF_STMT> hashX, value = utxos.pop(prevout) else: hashX, value = self.db_utxos[prevout] spends[hashX].append(prevout) return spends",if prevout in utxos:,if prevout in utxos:,0.8195565119507816,0.8592377270804451,True
1884,"def terminate(self): if self.returncode is None: try: os.kill(self.pid, TERM_SIGNAL) except OSError as exc: if getattr(exc, 'errno', None) != errno.ESRCH: <IF_STMT> raise",if self.wait(timeout=0.1) is None:,if exc.errno != errno.EINTR:,0.863063574443697,0.7871773473399102,False
1885,def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value <IF_STMT> thisDynamic = thisDynamic[1:] if thisDynamic[-1] == 'z': thisDynamic = thisDynamic[:-1] if thisDynamic in dynamicStrToScalar: return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None],if 's' in thisDynamic:,if thisDynamic.startswith('z'):,0.7362896110222755,0.9076141716697395,False
1886,"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): if '.' in valname: realvalname, key = valname.split('.', 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: <IF_STMT> self.__dict__[name] = config[name] del self._raw_config",if name in self.values:,if name in self.__dict__:,0.9132674691572696,0.8555308664663046,False
1887,"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary <IF_STMT> continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",if dictionary.path in paths:,if dictionary.path in paths:,0.8111160817623712,0.8555308664663046,True
1888,"def __getitem__(self, key, _get_mode=False): if not _get_mode: <IF_STMT> return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)","if isinstance(key, (int, long)):","if isinstance(key, str):",0.9028496699813134,0.8713933650206428,False
1889,"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: <IF_STMT> raise ItemNotFoundError('insufficient items with name %r' % name) else: raise AttributeError('insufficient non-disabled items with name %s' % name) on = [] off = [] for o in items: if o.selected: on.append(o) else: off.append(o) return (on, off)",if len(all_items) < target:,if not items:,0.7064443373412546,0.9224532597476077,False
1890,"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in(['fasta', 'base'], refs) <IF_STMT> return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), 'genomes', '*', gid)) if len(gdirs) == 1 and os.path.exists(gdirs[0]): return gdirs[0]",if seq_file and os.path.exists(seq_file):,if seq_file:,0.792364179416404,0.9122561819614461,False
1891,"def _PrintFuncs(self, names): status = 0 for name in names: <IF_STMT> print(name) else: status = 1 return status",if name in self.funcs:,if name.endswith('.py'):,0.8153611424725407,0.7912619863720214,False
1892,"def package_files(self): seen_package_directories = () directories = self.distribution.package_dir or {} empty_directory_exists = '' in directories packages = self.distribution.packages or [] for package in packages: if package in directories: package_directory = directories[package] elif empty_directory_exists: package_directory = os.path.join(directories[''], package) else: package_directory = package <IF_STMT> seen_package_directories += (package_directory + '.',) yield package_directory",if not package_directory.startswith(seen_package_directories):,if package_directory not in seen_package_directories:,0.8906511829979258,0.8729118929672821,False
1893,"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) <IF_STMT> with open(conf_file) as conf_handle: value = fn(conf_handle) if value: return value return None",if conf_file:,if conf_file:,0.8675770754397701,0.8531413606256201,True
1894,"def on_text(self, text): if text != self.chosen_text: self.fail_test('Expected ""{}"", received ""{}""'.format(self.chosen_text, text)) else: self.checks_passed += 1 <IF_STMT> self.pass_test() else: self._select_next_text()",if self.checks_passed >= self.number_of_checks:,if self.checks_passed >= self.max_checks:,0.5872337688346263,0.7378351342269067,False
1895,"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): <IF_STMT> continue x = item() if isinstance(x, ast.AST): self.assertEqual(type(x._fields), tuple)",if name == 'Index':,"if not hasattr(item, '_fields'):",0.8619934297042794,0.7378351342269067,False
1896,"def apply(self, response): updated_headers = self.update_headers(response) if updated_headers: response.headers.update(updated_headers) warning_header_value = self.warning(response) <IF_STMT> response.headers.update({'Warning': warning_header_value}) return response",if warning_header_value is not None:,if warning_header_value:,0.8472851346058529,0.7778111223054219,False
1897,"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) if self.use_parallel_executor and (not self.use_double_buffer): self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) <IF_STMT> self.assertTrue((in_data == out_data).all())",if not self.use_parallel_executor:,if self.use_parallel_executor:,0.8985704706171926,0.8866029039778043,False
1898,def finalize(self): if self._started: <IF_STMT> self._queue.put(None) self._queue.join() self._consumer.join() self._started = False self._finalized = True,if not self._finalized:,if self._consumer:,0.8180453131834874,0.7241577342575828,False
1899,"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: <IF_STMT> if e.code == 405: return 3 if e.code == 501: return 1 raise return 2","if hasattr(e, 'code'):","if e.response.get('Error', None):",0.824353298910121,0.8266114125804572,False
1900,"def _check_data(self, source, expected_bytes, expected_duration): received_bytes = 0 received_seconds = 0.0 bytes_to_read = 1024 while True: data = source.get_audio_data(bytes_to_read) <IF_STMT> break received_bytes += data.length received_seconds += data.duration self.assertEqual(data.length, len(data.data)) self.assertAlmostEqual(expected_duration, received_seconds, places=1) self.assertAlmostEqual(expected_bytes, received_bytes, delta=5)",if data is None:,if not data:,0.670253092559689,0.8713933650206428,False
1901,"def __randomize_interval_task(self): for job in self.aps_scheduler.get_jobs(): <IF_STMT> self.aps_scheduler.modify_job(job.id, next_run_time=datetime.now() + timedelta(seconds=randrange(job.trigger.interval.total_seconds() * 0.75, job.trigger.interval.total_seconds())))","if isinstance(job.trigger, IntervalTrigger):",if job.trigger.interval.total_seconds() > 0.5:,0.5572779986945777,0.630190855592386,False
1902,"def find_approximant(x): c = 0.0001 it = sympy.ntheory.continued_fraction_convergents(sympy.ntheory.continued_fraction_iterator(x)) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 <IF_STMT> return i if tol < machine_epsilon: break return x",if abs(i - x) <= tol:,if tol > machine_epsilon:,0.8738497603820852,0.8385130047130208,False
1903,"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): if line.endswith('\r\n'): lines[i] = line[:-2] + '\n' <IF_STMT> lines[i] = line[:-1] + '\n'",elif line.endswith('\r'):,elif line.endswith('\n'):,0.7326052815211038,0.8466657105524215,False
1904,"def payment_control_render(self, request: HttpRequest, payment: OrderPayment): template = get_template('pretixplugins/paypal/control.html') sale_id = None for trans in payment.info_data.get('transactions', []): for res in trans.get('related_resources', []): <IF_STMT> sale_id = res['sale']['id'] ctx = {'request': request, 'event': self.event, 'settings': self.settings, 'payment_info': payment.info_data, 'order': payment.order, 'sale_id': sale_id} return template.render(ctx)",if 'sale' in res and 'id' in res['sale']:,if res['sale']:,0.8545622398047812,0.9144061946646023,False
1905,"def for_name(self, name): try: name_resources = self._resources[name] except KeyError: raise LookupError(name) else: for res in name_resources: try: inst = res.inst() except Exception as e: <IF_STMT> log.exception('error initializing %s', res) else: log.error('error initializing %s: %s', res, e) else: yield inst",if log.getEffectiveLevel() <= logging.DEBUG:,if log.getEffectiveLevel() > logging.DEBUG:,0.7496514541819362,0.8627586293513119,False
1906,"def describe(self, done=False): description = ShellCommand.describe(self, done) if done: <IF_STMT> description = ['compile'] description.append('%d projects' % self.getStatistic('projects', 0)) description.append('%d files' % self.getStatistic('files', 0)) warnings = self.getStatistic('warnings', 0) if warnings > 0: description.append('%d warnings' % warnings) errors = self.getStatistic('errors', 0) if errors > 0: description.append('%d errors' % errors) return description",if not description:,if description is None:,0.781201002481454,0.8902056737869248,False
1907,"def parse_list(tl): ls = [] nm = [] while True: term, nmt, tl = parse_term(tl) ls.append(term) <IF_STMT> nm.append(nmt) if tl[0] != ',': break tl = tl[1:] return (ls, nm, tl)",if nmt is not None:,if nmt:,0.9257981070686151,0.8787142254774354,False
1908,"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return 'raw' elif IndexedDataset.exists(path): with open(index_file_path(path), 'rb') as f: magic = f.read(8) <IF_STMT> return 'cached' elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]: return 'mmap' else: return None elif FastaDataset.exists(path): return 'fasta' else: return None",if magic == IndexedDataset._HDR_MAGIC:,if magic == CachedIndexedDataset.Index._HDR_MAGIC[:8]:,0.6251798255656273,0.8474968231198384,False
1909,"def _get(self): fut = item = None with self._mutex: <IF_STMT> fut = Future() fut.add_done_callback(lambda f: self._get_complete() if not f.cancelled() else None) self._getters.append(fut) else: item = self._get_item() self._get_complete() return (item, fut)",if not self._queue or self._getters:,if self._is_future():,0.9075080780277898,0.8787142254774354,False
1910,"def validate(self): dates = [] for d in self.get('leave_block_list_dates'): <IF_STMT> frappe.msgprint(_('Date is repeated') + ':' + d.block_date, raise_exception=1) dates.append(d.block_date)",if d.block_date in dates:,if d.block_date not in dates:,0.638783811483699,0.6907573115737006,False
1911,"def on_choose_watch_dir_clicked(self): if self.window().watchfolder_enabled_checkbox.isChecked(): previous_watch_dir = self.window().watchfolder_location_input.text() or '' watch_dir = QFileDialog.getExistingDirectory(self.window(), 'Please select the watch folder', previous_watch_dir, QFileDialog.ShowDirsOnly) <IF_STMT> return self.window().watchfolder_location_input.setText(watch_dir)",if not watch_dir:,if watch_dir is None:,0.6627427020224864,0.7732323369196639,False
1912,"def log_generator(self, limit=6000, **kwargs): skip = 0 while True: logs = self.log(limit=limit, skip=skip, **kwargs) if not logs: break for entry in logs: yield entry <IF_STMT> break skip = skip + limit",if len(logs) < limit:,if skip >= limit:,0.8842213023606528,0.828399516355805,False
1913,"def _setUpClass(cls): global solver import pyomo.environ from pyomo.solvers.tests.io.writer_test_cases import testCases for test_case in testCases: <IF_STMT> solver[test_case.name, test_case.io] = True","if (test_case.name, test_case.io) in solver and test_case.available:",if test_case.io:,0.7446334522735809,0.803154665668484,False
1914,"def _get_file_data(self, normpath, normrev): data = self.client.cat(normpath, normrev) if has_expanded_svn_keywords(data): keywords = self.client.propget('svn:keywords', normpath, normrev, recurse=True) <IF_STMT> data = collapse_svn_keywords(data, force_bytes(keywords[normpath])) return data",if normpath in keywords:,if keywords:,0.6794742640421105,0.839587623092576,False
1915,"def add_controller_list(path): if not os.path.exists(os.path.join(path, '__init__.py')): bb.fatal('Controllers directory %s exists but is missing __init__.py' % path) files = sorted([f for f in os.listdir(path) if f.endswith('.py') and (not f.startswith('_'))]) for f in files: module = 'oeqa.controllers.' + f[:-3] <IF_STMT> controllerslist.append(module) else: bb.warn('Duplicate controller module found for %s, only one added. Layers should create unique controller module names' % module)",if module not in controllerslist:,"if os.path.exists(os.path.join(path, module)):",0.9058953627782298,0.9224532597476077,False
1916,"def on_session2(event): new_xmpp.get_roster() new_xmpp.send_presence() logging.info(roster[0]) data = roster[0]['roster']['items'] logging.info(data) for jid, item in data.items(): <IF_STMT> new_xmpp.send_presence(ptype='subscribe', pto=jid) new_xmpp.update_roster(jid, name=item['name'], groups=item['groups']) new_xmpp.disconnect()",if item['subscription'] != 'none':,if item['jid'] == jid:,0.769553906780778,0.7498810286408993,False
1917,"def _parse_class_simplified(symbol): results = {} name = symbol.name + '(' name += ', '.join([analyzer.expand_attribute(base) for base in symbol.bases]) name += ')' for sym in symbol.body: <IF_STMT> result = _parse_function_simplified(sym, symbol.name) results.update(result) elif isinstance(sym, ast.ClassDef): result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, 'c') return results","if isinstance(sym, ast.FunctionDef):","if isinstance(sym, ast.FunctionDef):",0.678061185332934,0.9167056528641923,True
1918,"def check_args(args): """"""Checks that the args are coherent."""""" check_args_has_attributes(args) if args.v: non_version_attrs = [v for k, v in args.__dict__.items() if k != 'v'] print('non_version_attrs', non_version_attrs) <IF_STMT> fail('Cannot show the version number with another command.') return if args.i is None: fail('Cannot draw ER diagram of no database.') if args.o is None: fail('Cannot draw ER diagram with no output file.')",if len([v for v in non_version_attrs if v is not None]) != 0:,if non_version_attrs:,0.6803998202056811,0.938501942261528,False
1919,"def handle(self, *args, **options): if not settings.ST_BASE_DIR.endswith('spirit'): raise CommandError('settings.ST_BASE_DIR is not the spirit root folder, are you overriding it?') for root, dirs, files in os.walk(settings.ST_BASE_DIR): <IF_STMT> continue with utils.pushd(root): call_command('makemessages', stdout=self.stdout, stderr=self.stderr, **options) self.stdout.write('ok')",if 'locale' not in dirs:,if not root.endswith('.py'):,0.8597292086580588,0.8675979125638379,False
1920,"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: <IF_STMT> return scan(s) or s elif s.type in ('suite', 'decorated'): return scan(s) return None","if isinstance(s, (tree.Scope, tree.Flow)):","if s.type in ('suite', 'decorated_suite'):",0.615605630771592,0.7685107079449489,False
1921,def run_sync(self): count = 0 while count < self.args.num_messages: batch = self.receiver.fetch_next(max_batch_size=self.args.num_messages - count) <IF_STMT> for msg in batch: msg.complete() count += len(batch),if self.args.peeklock:,if len(batch) > 0:,0.7369922208211845,0.7709002428237395,False
1922,"def __getitem__(self, item): if self._datas is not None: ret = [] for data in self._datas: <IF_STMT> ret.append(data[self._offset]) else: ret.append(data.iloc[self._offset]) self._offset += 1 return ret else: return self._get_data(item)","if isinstance(data, np.ndarray):","if isinstance(data, list):",0.8899493748011535,0.833078701050083,False
1923,"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == '550': <IF_STMT> raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise",if self.isfile(path):,if self.isdir(path):,0.926613277241581,0.9076141716697395,False
1924,"def replaces_in_file(file, replacement_list): rs = [(re.compile(regexp), repl) for regexp, repl in replacement_list] file_tmp = file + '.' + str(os.getpid()) + '.tmp' with open(file, 'r') as f: with open(file_tmp, 'w') as f_tmp: for line in f: for r, replace in rs: match = r.search(line) <IF_STMT> line = replace + '\n' f_tmp.write(line) shutil.move(file_tmp, file)",if match:,if match:,0.935869861323361,0.9312457603037672,True
1925,"def _get_path_check_mem(self, i, size): if size > 0: <IF_STMT> p = self._get_path(i, -1) else: p = self._get_path(i, size) if p.startswith('/dev/shm'): env.meminfo.add(size) else: p = self._get_path(i, size) return p",if env.meminfo.rss + size > env.meminfo.mem_limit_soft:,if size == 0:,0.6165326917305218,0.8105932471967202,False
1926,"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] parent = self['editor'] for c in parent.get_children(): <IF_STMT> if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) if not r is None: return r return None","if hasattr(c, 'get_id'):","if isinstance(c, Gtk.Widget):",0.9058068997350348,0.9118021019905903,False
1927,"def _deserialize(cls, io): flags = VideoFlags() flags.byte = U8.read(io) if flags.bit.type == VIDEO_FRAME_TYPE_COMMAND_FRAME: data = VideoCommandFrame.deserialize(io) el<IF_STMT> data = AVCVideoData.deserialize(io) else: data = io.read() return cls(flags.bit.type, flags.bit.codec, data)",if flags.bit.codec == VIDEO_CODEC_ID_AVC:,if flags.bit.type == AVC_FRAME_TYPE_AUDIO_DATA:,0.7250319103461093,0.8105932471967202,False
1928,"def asciiLogData(data, maxlen=64, replace=False): ellipses = ' ...' try: <IF_STMT> dd = data[:maxlen] + ellipses else: dd = data return dd.decode('utf8', errors='replace' if replace else 'strict') except: return '0x' + binLogData(data, maxlen)",if len(data) > maxlen - len(ellipses):,if maxlen > 0:,0.8290253697214403,0.8336104423443033,False
1929,"def _check_units(self, new_unit_system): if self.unit_system is None: self.unit_system = new_unit_system el<IF_STMT> raise ValueError('Unit system mismatch %d v. %d' % (self.unit_system, new_unit_system))",if self.unit_system != new_unit_system:,if self.unit_system != new_unit_system:,0.5754859338305277,0.7498810286408993,True
1930,"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) <IF_STMT> print('%s: %s' % (filename, visitor.get_stats())) if fix: print('Fixing: %s' % filename) fix_file(filename)",if visitor.needs_fix():,if visitor:,0.9087876007336358,0.8531413606256201,False
1931,"def assign_attributes_to_variants(variant_attributes): for value in variant_attributes: pk = value['pk'] defaults = value['fields'] defaults['variant_id'] = defaults.pop('variant') defaults['assignment_id'] = defaults.pop('assignment') assigned_values = defaults.pop('values') assoc, created = AssignedVariantAttribute.objects.update_or_create(pk=pk, defaults=defaults) <IF_STMT> assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))",if created:,if created:,0.7946207562779268,0.899160928885317,True
1932,"def _info(self, userlist): for strng in userlist: group_matched = False for env in self.base.comps.environments_by_pattern(strng): self.output.display_groups_in_environment(env) group_matched = True for group in self.base.comps.groups_by_pattern(strng): self.output.display_pkgs_in_groups(group) group_matched = True <IF_STMT> logger.error(_('Warning: Group %s does not exist.'), strng) return (0, [])",if not group_matched:,if group_matched:,0.8797974321625539,0.9024521756077707,False
1933,def parse_implements_interfaces(parser): types = [] if parser.token.value == 'implements': advance(parser) while True: types.append(parse_named_type(parser)) <IF_STMT> break return types,"if not peek(parser, TokenKind.NAME):",if parser.token.value == 'implements':,0.813956116498394,0.693395566222006,False
1934,"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 <IF_STMT> yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",elif leaf == SymbolFalse:,elif leaf == SymbolFalse:,0.6472361053719129,0.8661072626070159,True
1935,"def update_gstin(context): dirty = False for key, value in iteritems(frappe.form_dict): if key != 'party': address_name = frappe.get_value('Address', key) <IF_STMT> address = frappe.get_doc('Address', address_name) address.gstin = value.upper() address.save(ignore_permissions=True) dirty = True if dirty: frappe.db.commit() context.updated = True",if address_name:,if address_name:,0.8191978680620657,0.8996480074924822,True
1936,"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): <IF_STMT> if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True","if isinstance(v, dict) and k != 'headers':","if isinstance(v, dict):",0.912858611937874,0.9100365300271298,False
1937,"def check_graph(graph): for c in graph: <IF_STMT> raise RuntimeError('cannot have fuse') for inp in c.inputs: if isinstance(inp.op, Fuse): raise RuntimeError('cannot have fuse')","if isinstance(c.op, Fuse):","if isinstance(c.op, Fuse):",0.8577336063420359,0.7947545184555568,True
1938,"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: <IF_STMT> if isinstance(value, _ModuleMarker): return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError('Module %r has no mapped classes registered under the name %r' % (self.__parent.name, key))",if value is not None:,if self.__parent.mapped:,0.9317047205065841,0.9076141716697395,False
1939,"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue <IF_STMT> ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",if nw_id_ == nw_id:,if nw_id_ == nw_id:,0.8041980929606755,0.8627586293513119,True
1940,"def _parse(self, contents): entries = [] for line in contents.splitlines(): if not len(line.strip()): entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') <IF_STMT> entries.append(('all_comment', [line])) continue entries.append(('option', [head.split(None), tail])) return entries",if not len(head):,if head == '' or tail == '':,0.6790576647494483,0.7300365518218032,False
1941,"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, '_UNDOCUMENTED', False): continue <IF_STMT> continue if getattr(command, 'positional_arg', False): continue names.append(key) return names",if startswith is not None and (not key.startswith(startswith)):,if startswith and key.startswith(startswith):,0.8799346028823963,0.8038019482772603,False
1942,"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] <IF_STMT> val = tf.sparse.to_dense(val) if val.dtype == tf.int64: val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",if tf.keras.backend.is_sparse(val):,"if isinstance(val, tf.sparse.Tensor):",0.7849938607706015,0.9000283069718913,False
1943,"def _get_lang_zone(self, lang): if lang not in self._lang_zone_from_lang: <IF_STMT> self._lang_zone_from_lang[lang] = MultiLangZone(self.mgr, lang) else: self._lang_zone_from_lang[lang] = LangZone(self.mgr, lang) return self._lang_zone_from_lang[lang]",if self.mgr.is_multilang(lang):,if self.mgr.get_multi_lang_zones():,0.8177452300408353,0.8137489370974955,False
1944,"def dispatch(self, request, *args, **kwargs): try: return super(Handler, self).dispatch(request, *args, **kwargs) except Http404 as e: <IF_STMT> try: request.original_path_info = request.path_info request.path_info = settings.FEINCMS_CMS_404_PAGE response = super(Handler, self).dispatch(request, *args, **kwargs) response.status_code = 404 return response except Http404: raise e else: raise",if settings.FEINCMS_CMS_404_PAGE:,if e.status_code == 404:,0.9081489233907407,0.8661072626070159,False
1945,"def _maybe_update_dropout(self, step): for i in range(len(self.dropout_steps)): <IF_STMT> self.model.update_dropout(self.dropout[i]) logger.info('Updated dropout to %f from step %d' % (self.dropout[i], step))",if step > 1 and step == self.dropout_steps[i] + 1:,if self.dropout_steps[i] == step:,0.5822812903781682,0.7245511487202049,False
1946,"def bulk_move(*args, **kwargs): for arg in args: <IF_STMT> raise PopupException(_('Source path and destination path cannot be same')) request.fs.rename(urllib.unquote(arg['src_path']), urllib.unquote(arg['dest_path']))",if arg['src_path'] == arg['dest_path']:,if arg['src_path'] != arg['dest_path']:,0.586118697806089,0.7245511487202049,False
1947,"def asisWrite(self, root): at, c = (self, self.c) try: c.endEditing() c.init_error_dialogs() fileName = at.initWriteIvars(root, root.atAsisFileNodeName()) <IF_STMT> at.addToOrphanList(root) return at.openOutputStream() for p in root.self_and_subtree(copy=False): at.writeAsisNode(p) contents = at.closeOutputStream() at.replaceFile(contents, at.encoding, fileName, root) except Exception: at.writeException(fileName, root)","if not at.precheck(fileName, root):",if fileName == 'orphan':,0.7758672371875455,0.8474968231198384,False
1948,"def next_event(it): """"""read an event from an eventstream"""""" while True: try: line = next(it) except StopIteration: return <IF_STMT> return json.loads(line.split(':', 1)[1])",if line.startswith('data:'):,if line.startswith('event:'):,0.5754272053153755,0.8232490471721702,False
1949,"def process_formdata(self, valuelist): if valuelist: if valuelist[0] == '__None': self.data = None else: <IF_STMT> self.data = None return try: obj = self.queryset.get(pk=valuelist[0]) self.data = obj except DoesNotExist: self.data = None",if self.queryset is None:,if len(valuelist) == 0:,0.6678259773451128,0.8228500218338367,False
1950,"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <IF_STMT> warnings.warn('{}: setting results name {!r} on {} expression will return a list of all parsed tokens in an And alternative, in prior versions only the first token was returned'.format('warn_multiple_tokens_in_named_alternation', name, type(self).__name__), stacklevel=3) return super()._setResultsName(name, listAllMatches)","if any((isinstance(e, And) for e in self.exprs)):",if name in self.named_alternations:,0.8903519839306692,0.8723360571509826,False
1951,"def add(request): form_type = 'servers' if request.method == 'POST': form = BookMarkForm(request.POST) if form.is_valid(): form_type = form.save() messages.add_message(request, messages.INFO, 'Bookmark created') else: messages.add_message(request, messages.INFO, form.errors) <IF_STMT> url = reverse('servers') else: url = reverse('metrics') return redirect(url) else: return redirect(reverse('servers'))",if form_type == 'server':,if form_type == 'servers':,0.929973028544161,0.8592377270804451,False
1952,"def __init__(self, post_id, artist, page, tzInfo=None, dateFormat=None): self.imageUrls = list() self.imageResizedUrls = list() self.imageId = int(post_id) self._tzInfo = tzInfo self.dateFormat = dateFormat if page is not None: post_json = demjson.decode(page) <IF_STMT> artist_id = post_json['data']['item']['user']['id'] self.artist = SketchArtist(artist_id, page, tzInfo, dateFormat) else: self.artist = artist self.parse_post(post_json['data']['item'])",if artist is None:,if 'user' in post_json['data']['item']:,0.7156551892136047,0.8806615362338783,False
1953,"def _create_batch_iterator(self, mark_as_delete: Callable[[Any], None], to_key: Callable[[Any], Any], to_value: Callable[[Any], Any], batch: Iterable[EventT]) -> Iterable[Tuple[Any, Any]]: for event in batch: key = to_key(event.key) <IF_STMT> mark_as_delete(key) continue yield (key, to_value(event.value))",if event.message.value is None:,if mark_as_delete:,0.8754926496328154,0.8743414417652072,False
1954,"def test_lc_numeric_nl_langinfo(self): tested = False for loc in candidate_locales: try: setlocale(LC_NUMERIC, loc) setlocale(LC_CTYPE, loc) except Error: continue for li, lc in ((RADIXCHAR, 'decimal_point'), (THOUSEP, 'thousands_sep')): <IF_STMT> tested = True if not tested: self.skipTest('no suitable locales')","if self.numeric_tester('nl_langinfo', nl_langinfo(li), lc, loc):","if lc in (LC_NUMERIC, lc):",0.8853955888496567,0.8248765135255685,False
1955,def _level_up_logging(self): for handler in self.log.handlers: <IF_STMT> if handler.level != logging.DEBUG: handler.setLevel(logging.DEBUG) self.log.debug('Leveled up log file verbosity'),"if issubclass(handler.__class__, logging.FileHandler):","if hasattr(handler, 'level'):",0.5773910798783792,0.7331765459202478,False
1956,def _show_axes_changed(self): marker = self.marker if self._vtk_control is not None and marker is not None: <IF_STMT> marker.interactor = None marker.enabled = False else: marker.interactor = self.interactor marker.enabled = True self.render(),if not self.show_axes:,if self.interactor is None:,0.5760685917934052,0.8228500218338367,False
1957,"def handle_keypress(self, rawKey, modifiers, key, *args): if self.recordKeyboard and self.__delayPassed(): <IF_STMT> self.insideKeys = True self.targetParent.start_key_sequence() modifierCount = len(modifiers) if modifierCount > 1 or (modifierCount == 1 and Key.SHIFT not in modifiers) or (Key.SHIFT in modifiers and len(rawKey) > 1): self.targetParent.append_hotkey(rawKey, modifiers) elif key not in MODIFIERS: self.targetParent.append_key(key)",if not self.insideKeys:,if key in MODIFIERS:,0.907097584882792,0.8856327184319047,False
1958,"def transform(self, data): with timer('transform %s' % self.name, logging.DEBUG): if self.operator in {'lat', 'latitude'}: return self.series(data).apply(GeoIP.get_latitude) <IF_STMT> return self.series(data).apply(GeoIP.get_longitude) elif self.operator in {'acc', 'accuracy'}: return self.series(data).apply(GeoIP.get_accuracy) raise NameError('Unknown GeoIP operator [lat, lon, acc]: %s' % self.operator)","elif self.operator in {'lon', 'longitude'}:","elif self.operator in {'lon', 'longitude'}:",0.9158645962934581,0.8294838585473985,True
1959,def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = 'bl_%s' % self.businessline_id <IF_STMT> sidebar_selected += '_s_%s' % self.service_id if self.environment_id: sidebar_selected += '_env_%s' % self.environment_id return sidebar_selected,if self.service_id:,if self.service_id:,0.6333577914968522,0.8645707301556367,True
1960,"def _run_response_middleware(self, request, response, request_name=None): named_middleware = self.named_response_middleware.get(request_name, deque()) applicable_middleware = self.response_middleware + named_middleware if applicable_middleware: for middleware in applicable_middleware: _response = middleware(request, response) <IF_STMT> _response = await _response if _response: response = _response break return response",if isawaitable(_response):,if _response:,0.7250690215138612,0.9024521756077707,False
1961,"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: if self._should_delete: field.delete() return <IF_STMT> if not field.grid_id: func = field.put else: func = field.replace func(self.data.stream, filename=self.data.filename, content_type=self.data.content_type)","if isinstance(self.data, FileStorage) and (not is_empty(self.data.stream)):",if self._should_put:,0.8931625503585778,0.8866029039778043,False
1962,"def _import_hash(self, operator): for key in sorted(operator.import_hash.keys()): module_list = ', '.join(sorted(operator.import_hash[key])) <IF_STMT> exec('from {} import {}'.format(key[4:], module_list)) else: exec('from {} import {}'.format(key, module_list)) for var in operator.import_hash[key]: self.operators_context[var] = eval(var)",if key.startswith('tpot.'):,if key.startswith('import'):,0.8113178298978866,0.8787142254774354,False
1963,"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) <IF_STMT> extension = os.path.splitext(f_path)[1] if extension in file_extensions: os.remove(f_path)",if os.path.isfile(f_path):,if os.path.isfile(f_path):,0.6087016694347747,0.8137489370974955,True
1964,def clearBuffer(self): if self.shouldLose == -1: return if self.producer: self.producer.resumeProducing() if self.buffer: <IF_STMT> self.logFile.write('loopback receiving %s\n' % repr(self.buffer)) buffer = self.buffer self.buffer = b'' self.target.dataReceived(buffer) if self.shouldLose == 1: self.shouldLose = -1 self.target.connectionLost(failure.Failure(main.CONNECTION_DONE)),if self.logFile:,if self.logFile:,0.9171975882133436,0.8901732118131125,True
1965,"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + ': ') if bytes: sys.stderr.write(data.decode('utf8')) else: sys.stderr.write(data) <IF_STMT> self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",if data[-1] == '\n':,if self._line_line:,0.7807576733318687,0.8590888738245122,False
1966,def stop(self): self.queue_com.state_lock.acquire() try: <IF_STMT> self.queue_com.state = STOPPED self.remove() return True return False finally: self.queue_com.state_lock.release(),if self.queue_com.state == RUNNING and self.stop_task():,if self.queue_com.state == STOPPED:,0.7857447228623933,0.6540585844910979,False
1967,"def _handle_special_args(self, pyobjects): if len(pyobjects) == len(self.arguments.args): if self.arguments.vararg: pyobjects.append(rope.base.builtins.get_list()) <IF_STMT> pyobjects.append(rope.base.builtins.get_dict())",if self.arguments.kwarg:,elif self.arguments.varname:,0.6153675166226078,0.7539221180326288,False
1968,"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos if not osp.isfile(filename): self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() <IF_STMT> editor.set_cursor_position(position)",if position < editor.document().characterCount():,if editor is not None:,0.7494120057719563,0.7897643356713118,False
1969,"def _create_sentence_objects(self): """"""Returns a list of Sentence objects from the raw text."""""" sentence_objects = [] sent_tokenizer = SentenceTokenizer(locale=self.language.code) seq = Sequence(self.raw) seq = sent_tokenizer.transform(seq) for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]): sent = seq.text[start_index:end_index].strip() <IF_STMT> continue s = Sentence(sent, start_index=start_index, end_index=end_index) s.detected_languages = self.detected_languages sentence_objects.append(s) return sentence_objects",if not sent:,if not sent:,0.7977293574858176,0.9022045190074797,True
1970,"def to_json_schema(self, parent=None): schema = {} if not parent: schema['title'] = self.title <IF_STMT> schema['description'] = self.description if self.has_default: schema['default'] = self.default schema['_required_'] = self.required if self.null: schema['type'] = ['string', 'null'] else: schema['type'] = 'string' if self.enum is not None: schema['enum'] = self.enum return schema",if self.description:,if self.description is not None:,0.7320341547632035,0.8591169759078797,False
1971,def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return for f in os.listdir(dirname): <IF_STMT> continue path = dirname + os.sep + f if os.path.isdir(path): rmdir(path) else: os.unlink(path) os.rmdir(dirname),"if f in ('.', '..'):",if f.startswith('.'):,0.9022284884944497,0.8827916928185874,False
1972,"def convert_whole_dir(path=Path('marian_ckpt/')): for subdir in tqdm(list(path.ls())): dest_dir = f'marian_converted/{subdir.name}' <IF_STMT> continue convert(source_dir, dest_dir)",if (dest_dir / 'pytorch_model.bin').exists():,if not os.path.exists(dest_dir):,0.6067898524470474,0.6475445426291286,False
1973,"def colorformat(text): if text[0:1] == '#': col = text[1:] if len(col) == 6: return col <IF_STMT> return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == '': return '' assert False, 'wrong color format %r' % text",elif len(col) == 3:,elif len(col) == 3:,0.8771919639679431,0.8723360571509826,True
1974,"def _init_rel_seek(self): """"""Sets the file object's position to the relative location set above."""""" rs, fo = (self._rel_seek, self._file_obj) if rs == 0.0: fo.seek(0, os.SEEK_SET) else: fo.seek(0, os.SEEK_END) size = fo.tell() <IF_STMT> self._cur_pos = size else: target = int(size * rs) fo.seek(target, os.SEEK_SET) self._align_to_newline() self._cur_pos = fo.tell()",if rs == 1.0:,if rs == 0.0:,0.8813098291490875,0.8832000938217648,False
1975,"def parse_command_line(self, argv=None): """"""Parse the command line"""""" if self.config: parser = argparse.ArgumentParser(add_help=False) self.settings['config'].add_argument(parser) opts, _ = parser.parse_known_args(argv) if opts.config is not None: self.set('config', opts.config) self.params.update(self.import_from_module()) parser = self.parser() opts = parser.parse_args(argv) for k, v in opts.__dict__.items(): <IF_STMT> continue self.set(k.lower(), v)",if v is None:,if k.startswith('_'):,0.936149990453359,0.9099951253570094,False
1976,"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client('shield', region_name='us-east-1') protections = get_type_protections(client, self.manager.get_model()) protected_resources = {p['ResourceArn'] for p in protections} state = self.data.get('state', False) results = [] for arn, r in zip(self.manager.get_arns(resources), resources): r['c7n:ShieldProtected'] = shielded = arn in protected_resources <IF_STMT> results.append(r) elif not shielded and (not state): results.append(r) return results",if shielded and state:,if shielded and state:,0.895044389314191,0.8923575006167597,True
1977,"def removeTrailingWs(self, aList): i = 0 while i < len(aList): <IF_STMT> j = i i = self.skip_ws(aList, i) assert j < i if i >= len(aList) or aList[i] == '\n': del aList[j:i] i = j else: i += 1",if self.is_ws(aList[i]):,if aList[i] == '\n':,0.8654346578085343,0.8627586293513119,False
1978,"def predict(request: Request): form = await request.form() files, entry = convert_input(form) try: <IF_STMT> return JSONResponse(ALL_FEATURES_PRESENT_ERROR, status_code=400) try: resp = model.predict(data_dict=[entry]).to_dict('records')[0] return JSONResponse(resp) except Exception as e: logger.error('Error: {}'.format(str(e))) return JSONResponse(COULD_NOT_RUN_INFERENCE_ERROR, status_code=500) finally: for f in files: os.remove(f.name)",if entry.keys() & input_features != input_features:,if entry is None:,0.7842165611029828,0.8555308664663046,False
1979,"def reset(self): logger.debug('Arctic.reset()') with self._lock: <IF_STMT> self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): if hasattr(l, '_reset') and callable(l._reset): logger.debug('Library reset() %s' % l) l._reset()",if self.__conn is not None:,if self.__conn:,0.7595847000801196,0.8590888738245122,False
1980,"def read(self): if op.isfile(self.fileName): with textfile_open(self.fileName, 'rt') as fid: items = json.load(fid) <IF_STMT> items = dict() else: items = dict() self._items.clear() self._items.update(items) self._haveReadData = True",if items is None:,if items is None:,0.7470568259348063,0.7886336751695258,True
1981,"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): if text[end - 2:end] == '#}': return text[i:end] <IF_STMT> unclosed_end = end end += 1 raise TokenizationException('Unclosed comment', text[i:unclosed_end])",if not unclosed_end and text[end] == '<':,elif text[end - 2:end] == '#}':,0.9002361537271113,0.8177978265964414,False
1982,"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: value_ = self._late_binding_returnvalues.get(key, KeyError) <IF_STMT> value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) if schema is not None: value_ = schema.validate(value_) self._late_binding_returnvalues[key] = value_ return value_ else: return value",if value_ is KeyError:,if value_ is not None:,0.7344547620612262,0.8294838585473985,False
1983,"def connect(*args, **ckwargs): if 'give_content_type' in kwargs: <IF_STMT> kwargs['give_content_type'](args[6]['content-type']) else: kwargs['give_content_type']('') if 'give_connect' in kwargs: kwargs['give_connect'](*args, **ckwargs) status = code_iter.next() etag = etag_iter.next() timestamp = timestamps_iter.next() if status == -1: raise HTTPException() return FakeConn(status, etag, body=kwargs.get('body', ''), timestamp=timestamp)",if len(args) >= 7 and 'content_type' in args[6]:,if args[6]['content-type']:,0.9266847352358728,0.9051034981560222,False
1984,"def _reset(self): self._handle_connect() if self.rewarder_session: <IF_STMT> env_id = random.choice(self._sample_env_ids) logger.info('Randomly sampled env_id={}'.format(env_id)) else: env_id = None self.rewarder_session.reset(env_id=env_id) else: logger.info('No rewarder session exists, so cannot send a reset via the rewarder channel') self._reset_mask() return [None] * self.n",if self._sample_env_ids:,if self._sample_env_ids:,0.6391486068178344,0.8996480074924822,True
1985,"def _create_architecture_list(architectures, current_arch): if not architectures: return [_Architecture(build_on=[current_arch])] build_architectures: List[str] = [] architecture_list: List[_Architecture] = [] for item in architectures: if isinstance(item, str): build_architectures.append(item) <IF_STMT> architecture_list.append(_Architecture(build_on=item.get('build-on'), run_on=item.get('run-on'))) if build_architectures: architecture_list.append(_Architecture(build_on=build_architectures)) return architecture_list","if isinstance(item, dict):","elif isinstance(item, dict):",0.9288482632536761,0.8592899528284996,False
1986,"def inspect(self, pokemon): for caught_pokemon in self.cache: same_latitude = '{0:.4f}'.format(pokemon['latitude']) == '{0:.4f}'.format(caught_pokemon['latitude']) same_longitude = '{0:.4f}'.format(pokemon['longitude']) == '{0:.4f}'.format(caught_pokemon['longitude']) <IF_STMT> return if len(self.cache) >= 200: self.cache.pop(0) self.cache.append(pokemon)",if same_latitude and same_longitude:,if same_latitude != caught_pokemon['latitude'] or same_longitude != caught_pokemon['longitude']:,0.8805171545814702,0.6850564735741161,False
1987,"def parley(self): for x in [0, 1]: a = self.agents[x].act() <IF_STMT> if '[DONE]' in a['text']: self.agents[x - 1].observe({'id': 'World', 'text': 'The other agent has ended the chat.'}) self.episodeDone = True else: self.agents[x - 1].observe(a)",if a is not None:,if a['id'] == 'World':,0.6254379065417063,0.8431339019329497,False
1988,"def _prepare_subset(full_data: torch.Tensor, full_targets: torch.Tensor, num_samples: int, digits: Sequence): classes = {d: 0 for d in digits} indexes = [] for idx, target in enumerate(full_targets): label = target.item() if classes.get(label, float('inf')) >= num_samples: continue indexes.append(idx) classes[label] += 1 <IF_STMT> break data = full_data[indexes] targets = full_targets[indexes] return (data, targets)",if all((classes[k] >= num_samples for k in classes)):,if classes[label] >= num_samples:,0.9365383566466031,0.8902056737869248,False
1989,"def get_work_root(self, flags): _flags = flags.copy() _flags['is_toplevel'] = True target = self._get_target(_flags) if target: _flags['target'] = target.name tool = self.get_tool(_flags) <IF_STMT> return target.name + '-' + tool else: raise SyntaxError('Failed to determine work root. Could not resolve tool for target ' + target.name) else: raise SyntaxError('Failed to determine work root. Could not resolve target')",if tool:,if tool:,0.6510656787484475,0.933847757608669,True
1990,"def run_command(self, data): """"""Run editor commands."""""" parts = data.split(' ') cmd = parts[0].lower() if cmd in self.operations.keys(): return self.run_operation(cmd) args = ' '.join(parts[1:]) self.logger.debug(""Looking for command '{0}'"".format(cmd)) if cmd in self.modules.modules.keys(): self.logger.debug(""Trying to run command '{0}'"".format(cmd)) self.get_editor().store_action_state(cmd) <IF_STMT> return False else: self.set_status(""Command '{0}' not found."".format(cmd)) return False return True","if not self.run_module(cmd, args):","if self.run_command(cmd, args):",0.8486720749684645,0.9081987180086649,False
1991,"def get_main_chain_layers(self): """"""Return a list of layer IDs in the main chain."""""" main_chain = self.get_main_chain() ret = [] for u in main_chain: for v, layer_id in self.adj_list[u]: <IF_STMT> ret.append(layer_id) return ret",if v in main_chain and u in main_chain:,if v == u and layer_id not in ret:,0.8529914738015351,0.7178403422697396,False
1992,"def hash(self, context): with context: <IF_STMT> return IECore.MurmurHash() h = GafferDispatch.TaskNode.hash(self, context) h.append(self['fileName'].hash()) h.append(self['in'].hash()) h.append(self.__parameterHandler.hash()) return h",if not self['fileName'].getValue() or self['in'].source() == self['in']:,if self.is_native:,0.8513991863402641,0.7778111223054219,False
1993,"def consume_buf(): ty = state['ty'] - 1 for i in xrange(state['buf'].shape[1] // N): tx = x // N + i src = state['buf'][:, i * N:(i + 1) * N, :] <IF_STMT> with self.tile_request(tx, ty, readonly=False) as dst: mypaintlib.tile_convert_rgba8_to_rgba16(src, dst, self.EOTF) if state['progress']: try: state['progress'].completed(ty - ty0) except Exception: logger.exception('Progress.completed() failed') state['progress'] = None","if src[:, :, 3].any():",if tx != 0:,0.9633727247784947,0.9001816649635144,False
1994,"def check_permissions(self, obj): request = self.context.get('request') for Perm in permissions: perm = Perm() if not perm.has_permission(request, self): return False <IF_STMT> return False return True","if not perm.has_object_permission(request, self, obj):","if not perm.has_permission(obj, self):",0.8151672906168543,0.7801270245332924,False
1995,"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): if op.attr_key == 'storage_scope': lift_stmt[-1].append(op) return op.body <IF_STMT> return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError('not reached')",if op.attr_key == 'virtual_thread':,"if isinstance(op, tvm.tir.ForList):",0.9354868495499606,0.8783650674919876,False
1996,def task_done(self): with self._cond: if not self._unfinished_tasks.acquire(False): raise ValueError('task_done() called too many times') <IF_STMT> self._cond.notify_all(),if self._unfinished_tasks._semlock._is_zero():,if self._unfinished_tasks.acquire(False):,0.8604633730312883,0.8065008590125561,False
1997,"def get_json(self): if not hasattr(self, '_json'): self._json = None <IF_STMT> self._json = json.loads(self.request.body) return self._json","if self.request.headers.get('Content-Type', '').startswith('application/json'):",if self.request.method == 'GET':,0.6220160903837607,0.6540585844910979,False
1998,"def userfullname(): """"""Get the user's full name."""""" global _userfullname <IF_STMT> uid = os.getuid() entry = pwd_from_uid(uid) if entry: _userfullname = entry[4].split(',')[0] or entry[0] if not _userfullname: _userfullname = 'user%d' % uid return _userfullname",if not _userfullname:,if not _userfullname:,0.7093595596247951,0.8635707684233572,True
1999,"def test_scatter(self): for rank in range(self.world_size): tensor = [] <IF_STMT> tensor = [torch.tensor(i) for i in range(self.world_size)] result = comm.get().scatter(tensor, rank, size=()) self.assertTrue(torch.is_tensor(result)) self.assertEqual(result.item(), self.rank)",if self.rank == rank:,if rank == 0:,0.781962861339512,0.7886336751695258,False
2000,"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) if method is None: throw(DecompileError('Unsupported operation: %s' % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) <IF_STMT> decompiler.stack.append(x)",if x is not None:,if x is not None:,0.9353971455786554,0.8610291728339028,True
2001,"def print_scenario_ran(self, scenario): if scenario.passed: self.wrt('OK') elif scenario.failed: reason = self.scenarios_and_its_fails[scenario] <IF_STMT> self.wrt('FAILED') else: self.wrt('ERROR') self.wrt('\n')","if isinstance(reason.exception, AssertionError):",if reason == 'FAILED':,0.7496138806341371,0.674945488826271,False
2002,"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): if option != other_option: <IF_STMT> raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option","if scan_argv(self.argv, other_option) is not None:",if option != other_option:,0.6562100306491409,0.8385130047130208,False
2003,"def print_po_snippet(en_loc_old_lists, context): for m, localized, old in zip(*en_loc_old_lists): if m == '': continue <IF_STMT> localized = old print('#: {file}:{line}\nmsgid ""{context}{en_month}""\nmsgstr ""{localized_month}""\n'.format(context=context, file=filename, line=print_po_snippet.line, en_month=m, localized_month=localized)) print_po_snippet.line += 1",if m == localized:,if localized == '':,0.7127354368167664,0.8169276475307028,False
2004,"def set_status(self, dict_new): for i, value in dict_new.items(): self.dict_bili[i] = value <IF_STMT> self.dict_bili['pcheaders']['cookie'] = value self.dict_bili['appheaders']['cookie'] = value",if i == 'cookie':,if i == 0:,0.5907435804988381,0.7098232254187811,False
2005,"def makeSomeFiles(pathobj, dirdict): pathdict = {} for key, value in dirdict.items(): child = pathobj.child(key) <IF_STMT> pathdict[key] = child child.setContent(value) elif isinstance(value, dict): child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError('only strings and dicts allowed as values') return pathdict","if isinstance(value, bytes):","if isinstance(value, str):",0.8498855917657171,0.8815741981066073,False
2006,"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) if len_map is not None: for key, max_len in len_map.items(): example_len = example[key].shape <IF_STMT> example[key] = np.resize(example[key], max_len) yield tuple(example)",if example_len > max_len:,if example_len > max_len:,0.6608617711092631,0.8228500218338367,True
2007,"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: if 'dir' in kw: return not kw['dir'] ^ isdir(self.strpath) <IF_STMT> return not kw['file'] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",if 'file' in kw:,if 'file' in kw:,0.8676409789106598,0.8228500218338367,True
2008,"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): if i > 0 and (not lines[i - 1].strip()): return False continue if line.startswith('def ') or line.startswith('class '): return True <IF_STMT> continue return False return False","if line.startswith(('#', '@', ' ', ')')):",if line.startswith('#'):,0.9371698524645068,0.9395648330058336,False
2009,"def askCheckReadFile(self, localFile, remoteFile): if not kb.bruteMode: message = ""do you want confirmation that the remote file '%s' "" % remoteFile message += 'has been successfully downloaded from the back-end ' message += 'DBMS file system? [Y/n] ' <IF_STMT> return self._checkFileLength(localFile, remoteFile, True) return None","if readInput(message, default='Y', boolean=True):",if self.confirm(message):,0.7946005448620317,0.9202663016973823,False
2010,"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) <IF_STMT> exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return (company, major, minor, arch, exe, args)",if arch is not None:,if arch is not None:,0.7670979905319789,0.8901199011963146,True
2011,"def _get_matching_bracket(self, s, pos): if s[pos] != '{': return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == '{': depth += 1 <IF_STMT> depth -= 1 if depth == 0: break pos += 1 if pos < end and s[pos] == '}': return pos return None",elif c == '}':,elif c == '}':,0.9279162394957898,0.9053411402117831,True
2012,"def pred(field, value, item): for suffix, p in _BUILTIN_PREDS.iteritems(): if field.endswith(suffix): f = field[:field.index(suffix)] <IF_STMT> return False return p(getattr(item, f), value) if not hasattr(item, field) or getattr(item, field) is None: return False if isinstance(value, type(lambda x: x)): return value(getattr(item, field)) return getattr(item, field) == value","if not hasattr(item, f) or getattr(item, f) is None:","if not hasattr(item, f):",0.8428158535868164,0.8806615362338783,False
2013,"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): if isinstance(m, nn.ConvTranspose2d): normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): <IF_STMT> normal_init(m, std=0.001, bias=0)","if isinstance(m, nn.Conv2d):","if isinstance(m, nn.BatchNorm2d):",0.821187230331144,0.8390782502060267,False
2014,"def test_byteswap(self): if self.typecode == 'u': example = '\U00100100' else: example = self.example a = array.array(self.typecode, example) self.assertRaises(TypeError, a.byteswap, 42) if a.itemsize in (1, 2, 4, 8): b = array.array(self.typecode, example) b.byteswap() <IF_STMT> self.assertEqual(a, b) else: self.assertNotEqual(a, b) b.byteswap() self.assertEqual(a, b)",if a.itemsize == 1:,if a.itemsize == 1:,0.6390755680750231,0.8692960007731574,True
2015,"def _remove_blocks_from_variables(variables): new_variables = [] for name, variable in variables: <IF_STMT> new_variables.extend(variable.locals) new_variables.append((name, variable.result)) else: new_variables.append((name, variable)) return new_variables",if variable.is_block():,"if isinstance(variable, Block):",0.7557417694228852,0.761827408333416,False
2016,def scope(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.scope_ is None: self.scope_ = Scope() finally: self.lazy_init_lock_.release() return self.scope_,if self.scope_ is None:,if self.scope_ is None:,0.6272973784763626,0.6929598487720369,True
2017,"def translate(): assert Lex.next() is AttributeList reader.read() attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: if k == 'attrlist': v = subs_attrs(v) <IF_STMT> parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",if v:,elif k == 'attrs':,0.8392416201770251,0.8627586293513119,False
2018,"def parse(self, response): try: content = response.content.decode('utf-8', 'ignore') content = json.loads(content, strict=False) except: self.logger.error('Fail to parse the response in json format') return for item in content['data']: <IF_STMT> img_url = self._decode_url(item['objURL']) elif 'hoverURL' in item: img_url = item['hoverURL'] else: continue yield dict(file_url=img_url)",if 'objURL' in item:,if 'objURL' in item:,0.9190184415842777,0.8692960007731574,True
2019,"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() if name == 'MOV': if instr.mnemonic.startswith('lsr'): return 'LSR' elif instr.mnemonic.startswith('lsl'): return 'LSL' <IF_STMT> return 'ASR' return OP_NAME_MAP.get(name, name)",elif instr.mnemonic.startswith('asr'):,elif instr.mnemonic.startswith('asr'):,0.8919581626072006,0.839587623092576,True
2020,"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) <IF_STMT> if isinstance(target, six.string_types) and os.path.isfile(target): target = _load_regions(target) else: target = [target] return target",if target:,"if not isinstance(target, list):",0.6674384882732974,0.8661072626070159,False
2021,def reader_leaves(self): self.mutex.acquire() try: self.active_readers -= 1 <IF_STMT> self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() finally: self.mutex.release(),if self.active_readers == 0 and self.waiting_writers != 0:,if self.waiting_readers > 0:,0.5810243534158214,0.693395566222006,False
2022,"def _bpe_to_words(sentence, delimiter='@@'): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = '' delimiter_len = len(delimiter) for subwords in sentence: <IF_STMT> word += subwords[:-delimiter_len] else: word += subwords words.append(word) word = '' return words",if len(subwords) >= delimiter_len and subwords[-delimiter_len:] == delimiter:,if subwords[-delimiter_len] == delimiter:,0.8569381027057603,0.8592377270804451,False
2023,"def _make_var_names(exog): if hasattr(exog, 'name'): var_names = exog.name elif hasattr(exog, 'columns'): var_names = exog.columns else: raise ValueError('exog is not a Series or DataFrame or is unnamed.') try: var_names = ' '.join(var_names) except TypeError: from statsmodels.base.data import _make_exog_names <IF_STMT> var_names = 'x1' else: var_names = ' '.join(_make_exog_names(exog)) return var_names",if exog.ndim == 1:,if exog is None:,0.918289383054508,0.8879659171421962,False
2024,"def __start_element_handler(self, name, attrs): if name == 'mime-type': <IF_STMT> for extension in self.extensions: self[extension] = self.type self.type = attrs['type'].lower() self.extensions = [] elif name == 'glob': pattern = attrs['pattern'] if pattern.startswith('*.'): self.extensions.append(pattern[1:].lower())",if self.type:,if self.type:,0.9180321417059751,0.8866029039778043,True
2025,"def nodes(self, id=None, name=None): for node_dict in self.node_ls(id=id, name=name): node_id = node_dict['ID'] node = DockerNode(self, node_id, inspect=node_dict) <IF_STMT> continue yield node",if self._node_prefix and (not node.name.startswith(self._node_prefix)):,if node.is_alive():,0.6104254044944419,0.8232490471721702,False
2026,"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() <IF_STMT> e.value = 0 return self",if type(e) is Command or (type(e) is Option and e.argcount == 0):,elif type(e) is Option:,0.8083750392314143,0.9128479730518225,False
2027,"def vi_search(self, rng): for i in rng: line_history = self._history.history[i] pos = line_history.get_line_text().find(self._vi_search_text) <IF_STMT> self._history.history_cursor = i self.l_buffer.line_buffer = list(line_history.line_buffer) self.l_buffer.point = pos self.vi_undo_restart() return True self._bell() return False",if pos >= 0:,if pos != -1:,0.665439397811071,0.8169276475307028,False
2028,"def visitIf(self, node, scope): for test, body in node.tests: <IF_STMT> if type(test.value) in self._const_types: if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)","if isinstance(test, ast.Const):","if isinstance(test, ast.If):",0.8726726266412134,0.8266114125804572,False
2029,"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host['host'], squid_host['port']) <IF_STMT> fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) if matches: self.publish_counter('%s.%s' % (nickname, matches.group(1)), float(matches.group(2)))",if fulldata is not None:,"if isinstance(fulldata, str):",0.7825245857052767,0.8547305998833805,False
2030,"def convert(x, base, exponents): out = [] for e in exponents: d = int(x / base ** e) x -= d * base ** e out.append(digits[d]) <IF_STMT> break return out",if x == 0 and e < 0:,if d == 0:,0.717714409098892,0.8228500218338367,False
2031,"def print_doc(manager, options): plugin_name = options.doc plugin = plugins.get(plugin_name, None) if plugin: <IF_STMT> console('Plugin %s does not have documentation' % plugin_name) else: console('') console(trim(plugin.instance.__doc__)) console('') else: console('Could not find plugin %s' % plugin_name)",if not plugin.instance.__doc__:,if not plugin.instance.__doc__:,0.6306169815975483,0.8635707684233572,True
2032,"def _set_attrs(self, attrs): for attr in self.ATTRS: if attr in attrs: setattr(self, attr, attrs[attr]) del attrs[attr] el<IF_STMT> setattr(self, attr, NO_DEFAULT) else: setattr(self, attr, None) if attrs: attrs = sorted(attrs.keys()) raise OptionError('invalid keyword arguments: %s' % ', '.join(attrs), self)",if attr == 'default':,if attr == NO_DEFAULT:,0.8081242253319278,0.8592377270804451,False
2033,"def _get_set_scope(ir_set: irast.Set, scope_tree: irast.ScopeTreeNode) -> irast.ScopeTreeNode: if ir_set.path_scope_id: new_scope = scope_tree.root.find_by_unique_id(ir_set.path_scope_id) <IF_STMT> raise errors.InternalServerError(f'dangling scope pointer to node with uid:{ir_set.path_scope_id} in {ir_set!r}') else: new_scope = scope_tree return new_scope",if new_scope is None:,if new_scope is None:,0.696867798672428,0.8169276475307028,True
2034,"def test_leave_one_out(self): correct = 0 k = 3 model = kNN.train(xs, ys, k) predictions = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1] for i in range(len(predictions)): model = kNN.train(xs[:i] + xs[i + 1:], ys[:i] + ys[i + 1:], k) prediction = kNN.classify(model, xs[i]) self.assertEqual(prediction, predictions[i]) <IF_STMT> correct += 1 self.assertEqual(correct, 13)",if prediction == ys[i]:,if i == k:,0.8791429666467856,0.9114434865990403,False
2035,"def import_files(self, files): """"""Import a list of MORE (.csv) files."""""" c = self.c if files: changed = False self.tab_width = c.getTabWidth(c.p) for fileName in files: g.setGlobalOpenDir(fileName) p = self.import_file(fileName) <IF_STMT> p.contract() p.setDirty() c.setChanged(True) changed = True if changed: c.redraw(p)",if p:,if p:,0.7956200878361936,0.9076141716697395,True
2036,"def getPageTemplate(payload, place): retVal = (kb.originalPage, kb.errorIsNone) if payload and place: <IF_STMT> page, _, _ = Request.queryPage(payload, place, content=True, raise404=False) kb.pageTemplates[payload, place] = (page, kb.lastParserStatus is None) retVal = kb.pageTemplates[payload, place] return retVal","if (payload, place) not in kb.pageTemplates:","if kb.pageTemplates[payload, place] is None:",0.8885536470784438,0.8148691130388024,False
2037,"def _skip_trivial(constraint_data): if skip_trivial_constraints: if isinstance(constraint_data, LinearCanonicalRepn): if constraint_data.variables is None: return True el<IF_STMT> return True return False",if constraint_data.body.polynomial_degree() == 0:,if constraint_data.variables.is_trivial:,0.5925226774472251,0.7912619863720214,False
2038,"def get_unique_attribute(self, name: str): feat = None for f in self.features: if self._return_feature(f) and hasattr(f, name): <IF_STMT> raise RuntimeError('The attribute was not unique.') feat = f if feat is None: raise RuntimeError('The attribute did not exist') return getattr(feat, name)",if feat is not None:,if name in f.unique_attributes:,0.8946421211066253,0.8627586293513119,False
2039,"def hideEvent(self, event): """"""Reimplement Qt method"""""" if not self.light: for plugin in self.widgetlist: <IF_STMT> plugin.visibility_changed(True) QMainWindow.hideEvent(self, event)",if plugin.isAncestorOf(self.last_focused_widget):,if plugin.isVisible():,0.8003006472304778,0.7778111223054219,False
2040,"def move_stdout_to_stderr(self): to_remove = [] to_add = [] for consumer_level, consumer in self.consumers: <IF_STMT> to_remove.append((consumer_level, consumer)) to_add.append((consumer_level, sys.stderr)) for item in to_remove: self.consumers.remove(item) self.consumers.extend(to_add)",if consumer == sys.stdout:,if consumer.is_stdout():,0.7986622617362898,0.8466657105524215,False
2041,"def create(exported_python_target): if exported_python_target not in created: self.context.log.info('Creating setup.py project for {}'.format(exported_python_target)) subject = self.derived_by_original.get(exported_python_target, exported_python_target) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir <IF_STMT> for dep in dependencies: if is_exported_python_target(dep): create(dep)",if self._recursive:,if dependencies:,0.7171208836919112,0.8866029039778043,False
2042,"def __add__(self, other): other = ArithmeticExpression.try_unpack_const(other) if not self.symbolic and type(other) is int: return SpOffset(self._bits, self._to_signed(self.offset + other)) el<IF_STMT> return SpOffset(self._bits, self.offset + other) else: return SpOffset(self._bits, ArithmeticExpression(ArithmeticExpression.Add, (self.offset, other)))",if self.symbolic:,if self.symbolic:,0.6425263663174782,0.8787142254774354,True
2043,"def check_connection(conn): tables = [r[0] for r in conn.execute(""select name from sqlite_master where type='table'"").fetchall()] for table in tables: try: conn.execute(f'PRAGMA table_info({escape_sqlite(table)});') except sqlite3.OperationalError as e: <IF_STMT> raise SpatialiteConnectionProblem(e) else: raise ConnectionProblem(e)",if e.args[0] == 'no such module: VirtualSpatialIndex':,if e.errno == errno.EEXIST:,0.8842701993781766,0.828399516355805,False
2044,"def _get_github_client(self) -> 'Github': from github import Github if self.access_token_secret is not None: access_token = Secret(self.access_token_secret).get() else: access_token = prefect.context.get('secrets', {}).get('GITHUB_ACCESS_TOKEN') <IF_STMT> access_token = os.getenv('GITHUB_ACCESS_TOKEN') return Github(access_token)",if access_token is None:,if access_token is None:,0.8634560769415202,0.8038019482772603,True
2045,"def make_tab(lists): if hasattr(lists, 'tolist'): lists = lists.tolist() ut = [] for rad in lists: <IF_STMT> ut.append('\t'.join(['%s' % x for x in rad])) else: ut.append('%s' % rad) return '\n'.join(ut)","if type(rad) in [list, tuple]:","if isinstance(rad, list):",0.897032181123353,0.8446593249975184,False
2046,"def _ensure_ffi_initialized(cls): with cls._init_lock: <IF_STMT> cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES) cls._lib_loaded = True cls.lib.SSL_library_init() cls.lib.OpenSSL_add_all_algorithms() cls.lib.SSL_load_error_strings() cls._register_osrandom_engine()",if not cls._lib_loaded:,if not cls._lib_loaded:,0.8330061346948142,0.7160350546947921,True
2047,def writer_leaves(self): self.mutex.acquire() try: self.active_writers -= 1 if self.waiting_writers != 0: self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() <IF_STMT> t = self.waiting_readers self.waiting_readers = 0 self.active_readers += t while t > 0: self.can_read.release() t -= 1 finally: self.mutex.release(),elif self.waiting_readers != 0:,if self.waiting_readers != 0:,0.9089954516978932,0.8592377270804451,False
2048,"def _spans(self, operands): spans = {} k = 0 j = 0 for mode in (self.FLOAT, self.MPMATH): for i, operand in enumerate(operands[k:]): if operand[0] > mode: break j = i + k + 1 <IF_STMT> j = 0 spans[mode] = slice(k, j) k = j spans[self.SYMBOLIC] = slice(k, len(operands)) return spans",if k == 0 and j == 1:,if j > len(operands):,0.8208083418878946,0.8944264839442453,False
2049,"def _report_error(self, completion_routine, response=None, message=None): if response: <IF_STMT> status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: if completion_routine: return completion_routine(status) raise IOError(response.text) elif completion_routine: completion_routine(status) return location.Status(200, response.content)",if not response.ok:,if message is None:,0.9081757082246937,0.8661072626070159,False
2050,"def readinto(self, buf): if self.current_frame: n = self.current_frame.readinto(buf) if n == 0 and len(buf) != 0: self.current_frame = None n = len(buf) buf[:] = self.file_read(n) return n <IF_STMT> raise UnpicklingError('pickle exhausted before end of frame') return n else: n = len(buf) buf[:] = self.file_read(n) return n",if n < len(buf):,elif n == 0:,0.9288415171915568,0.8832000938217648,False
2051,"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] <IF_STMT> return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent if mydict is None: return",if value is not None:,if value is not None:,0.8260111255938409,0.8338542560892604,True
2052,"def _handle_Mul(self, expr): arg0, arg1 = expr.args expr_0 = self._expr(arg0) if expr_0 is None: return None expr_1 = self._expr(arg1) if expr_1 is None: return None try: <IF_STMT> mask = (1 << expr.result_size(self.tyenv)) - 1 return expr_0 * expr_1 & mask else: return expr_0 * expr_1 except TypeError as e: self.l.warning(e) return None","if isinstance(expr_0, int) and isinstance(expr_1, int):","if isinstance(expr_0, int) and isinstance(expr_1, int):",0.6262922337633394,0.8641006343550574,True
2053,"def end_request(self, request_id): """"""Removes the information associated with given request_id."""""" with self._lock: del self._request_wsgi_environ[request_id] del self._request_id_to_server_configuration[request_id] <IF_STMT> del self._request_id_to_instance[request_id]",if request_id in self._request_id_to_instance:,if request_id in self._request_id_to_instance:,0.846240079211878,0.7391959451349216,True
2054,def generate(): <IF_STMT> decoder = zlib.decompressobj(16 + zlib.MAX_WBITS) while True: chunk = self.raw.read(chunk_size) if not chunk: break if self._gzipped: chunk = decoder.decompress(chunk) yield chunk,if self._gzipped:,if self._gzipped:,0.8444594486099584,0.8588510825398774,True
2055,def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name <IF_STMT> name = str(venv.path) if venv == current_env: self.line('<info>{} (Activated)</info>'.format(name)) continue self.line(name),if self.option('full-path'):,if name == 'default':,0.9149102271339261,0.828399516355805,False
2056,"def addAggregators(sheet, cols, aggrnames): """"""Add each aggregator in list of *aggrnames* to each of *cols*."""""" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: <IF_STMT> c.aggregators = [] if aggr and aggr not in c.aggregators: c.aggregators += [aggr]","if not hasattr(c, 'aggregators'):","if not hasattr(c, 'aggregators'):",0.9271293246837433,0.8964173245779284,True
2057,"def on_pre_output_coercion(directive_args: Dict[str, Any], next_directive: Callable, value: Any, ctx: Optional[Any], info: 'ResolveInfo'): value = await next_directive(value, ctx, info) if value is None: return value try: py_enum = _ENUM_MAP[directive_args['name']] <IF_STMT> return [None if item is None else py_enum(item).name for item in value] return py_enum(value).name except Exception: pass return value","if isinstance(value, list):","if isinstance(value, list):",0.7098283403961079,0.9062841320510342,True
2058,def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: <IF_STMT> for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,if re_han.match(blk):,if blk in Force_Split_Words:,0.9214915347619347,0.8752376177722327,False
2059,"def refresh_archive_action(self): archive_name = self.selected_archive_name() if archive_name is not None: params = BorgInfoArchiveThread.prepare(self.profile(), archive_name) <IF_STMT> thread = BorgInfoArchiveThread(params['cmd'], params, parent=self.app) thread.updated.connect(self._set_status) thread.result.connect(self.refresh_archive_result) self._toggle_all_buttons(False) thread.start()",if params['ok']:,if params:,0.7453697428149768,0.8466657105524215,False
2060,"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): if not name[0].isupper(): if not name.startswith('wait_until'): <IF_STMT> resource_methods[name] = member return resource_methods",if is_resource_action(member):,if inspect.isclass(member) and member.name == 'resource':,0.8731341907221187,0.7590598306198806,False
2061,"def _get_compressor(compress_type, compresslevel=None): if compress_type == ZIP_DEFLATED: <IF_STMT> return zlib.compressobj(compresslevel, zlib.DEFLATED, -15) return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15) elif compress_type == ZIP_BZIP2: if compresslevel is not None: return bz2.BZ2Compressor(compresslevel) return bz2.BZ2Compressor() elif compress_type == ZIP_LZMA: return LZMACompressor() else: return None",if compresslevel is not None:,if compresslevel is not None:,0.9065489299166085,0.8380055871435848,True
2062,"def parse_header(plyfile, ext): line = [] properties = [] num_points = None while b'end_header' not in line and line != b'': line = plyfile.readline() if b'element' in line: line = line.split() num_points = int(line[2]) <IF_STMT> line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return (num_points, properties)",elif b'property' in line:,elif b'properties' in line:,0.8874320510824822,0.8806615362338783,False
2063,"def download_release_artifacts(self, version): try: os.mkdir(self.artifacts_dir) except FileExistsError: pass for job_name in self.build_ids: build_number = self.build_ids.get(job_name) build_status = self._get_build_status(job_name, build_number) <IF_STMT> self._download_job_artifact(job_name, build_number, version) else: print('Build for {} is not fininished'.format(job_name)) print(""\tRun 'build' action to check status of {}"".format(job_name))",if build_status == 'built':,if build_status == 'fininished':,0.8924496221573246,0.8592377270804451,False
2064,"def update_metadata(self): for attrname in dir(self): <IF_STMT> continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrname.startswith('__'):,if attrname.startswith('_'):,0.8433580411292781,0.9099951253570094,False
2065,"def check_heuristic_in_sql(): heurs = set() excluded = ['Equal assembly or pseudo-code', 'All or most attributes'] for heur in HEURISTICS: name = heur['name'] if name in excluded: continue sql = heur['sql'] <IF_STMT> print('SQL command not correctly associated to %s' % repr(name)) print(sql) assert sql.find(name) != -1 heurs.add(name) print('Heuristics:') import pprint pprint.pprint(heurs)",if sql.lower().find(name.lower()) == -1:,if not sql.find(name) != -1:,0.9170820001821912,0.8753524256584351,False
2066,def gettext(rv): for child in rv.childNodes: <IF_STMT> yield child.nodeValue if child.nodeType == child.ELEMENT_NODE: for item in gettext(child): yield item,if child.nodeType == child.TEXT_NODE:,if child.nodeType == child.TEXT_NODE:,0.7715350543716011,0.7245511487202049,True
2067,"def update(self): """"""Update properties over dbus."""""" self._check_dbus() _LOGGER.info('Updating service information') self._services.clear() try: systemd_units = await self.sys_dbus.systemd.list_units() for service_data in systemd_units[0]: <IF_STMT> continue self._services.add(ServiceInfo.read_from(service_data)) except (HassioError, IndexError): _LOGGER.warning(""Can't update host service information!"")",if not service_data[0].endswith('.service') or service_data[2] != 'loaded':,if service_data == 'host':,0.9010272556982974,0.828399516355805,False
2068,"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: <IF_STMT> comment = source[0, source.index('*/') + 2] elif re.search('^\\s*\\/\\/', source): comment = re.search('^\\s*\\/\\/', source).group(0) else: comment = None if comment: source = re.sub('^\\s+', '', source[len(comment):]) trailing_comments.append(comment) return '\n'.join(trailing_comments) + source","if re.search('^\\s*\\/\\*', source):",if '/' in source:,0.8084499720048537,0.8944264839442453,False
2069,"def _getSourceStamp_sync(self, ssid): if ssid in self.sourcestamps: ssdict = self.sourcestamps[ssid].copy() ssdict['ssid'] = ssid patchid = ssdict['patchid'] <IF_STMT> ssdict.update(self.patches[patchid]) ssdict['patchid'] = patchid else: ssdict['patch_body'] = None ssdict['patch_level'] = None ssdict['patch_subdir'] = None ssdict['patch_author'] = None ssdict['patch_comment'] = None return ssdict else: return None",if patchid:,if patchid in self.patches:,0.6587908586846001,0.8723360571509826,False
2070,"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): if self.defaultValue is not self.__optionalNotMatched: <IF_STMT> tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return (loc, tokens)",if self.expr.resultsName:,if self.expr.resultsName:,0.8950367542534687,0.9076141716697395,True
2071,"def _find_exceptions(): for _name, obj in iteritems(globals()): try: is_http_exception = issubclass(obj, HTTPException) except TypeError: is_http_exception = False if not is_http_exception or obj.code is None: continue __all__.append(obj.__name__) old_obj = default_exceptions.get(obj.code, None) <IF_STMT> continue default_exceptions[obj.code] = obj","if old_obj is not None and issubclass(obj, old_obj):",if old_obj is not None and old_obj != obj:,0.8887236453767502,0.7453094826679977,False
2072,"def generator(self, data): for proc_as, key_buf_ptr in data: key_buf = proc_as.read(key_buf_ptr, 24) <IF_STMT> continue key = ''.join(('%02X' % ord(k) for k in key_buf)) yield (0, [str(key)])",if not key_buf:,if not key_buf:,0.6170738003775714,0.8266114125804572,True
2073,"def calculateEnableMargins(self): self.cnc.resetEnableMargins() for block in self.blocks: <IF_STMT> CNC.vars['xmin'] = min(CNC.vars['xmin'], block.xmin) CNC.vars['ymin'] = min(CNC.vars['ymin'], block.ymin) CNC.vars['zmin'] = min(CNC.vars['zmin'], block.zmin) CNC.vars['xmax'] = max(CNC.vars['xmax'], block.xmax) CNC.vars['ymax'] = max(CNC.vars['ymax'], block.ymax) CNC.vars['zmax'] = max(CNC.vars['zmax'], block.zmax)",if block.enable:,if block.enableMargins:,0.7035146305495048,0.8866029039778043,False
2074,"def __init__(self, client, job_id, callback=None): self.client = client self.job_id = job_id with client._jobs_lock: job = client._jobs.get(job_id) self.event = None <IF_STMT> self.event = job.get('__ready') if self.event is None: self.event = job['__ready'] = Event() job['__callback'] = callback",if job:,if job is not None:,0.7475237069374817,0.8248765135255685,False
2075,"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: <IF_STMT> tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if not os.path.exists(fspath + '.spt'):,if env.cache_static:,0.9521255271271976,0.9325718821645923,False
2076,"def set_conf(): """"""Collapse all object_trail config into cherrypy.request.config."""""" base = cherrypy.config.copy() for name, obj, conf, segleft in object_trail: base.update(conf) <IF_STMT> base['tools.staticdir.section'] = '/' + '/'.join(fullpath[0:fullpath_len - segleft]) return base",if 'tools.staticdir.dir' in conf:,if segleft > 0:,0.8540140790188308,0.8169276475307028,False
2077,"def __init__(self): self.setLayers(None, None) self.interface = None self.event_callbacks = {} self.__stack = None self.lock = threading.Lock() members = inspect.getmembers(self, predicate=inspect.ismethod) for m in members: <IF_STMT> fname = m[0] fn = m[1] self.event_callbacks[fn.event_callback] = getattr(self, fname)","if hasattr(m[1], 'event_callback'):","if isinstance(m, tuple):",0.6605888011813421,0.8713933650206428,False
2078,def multi_dev_generator(self): for data in self._data_loader(): <IF_STMT> self._tail_data += data if len(self._tail_data) == self._base_number: yield self._tail_data self._tail_data = [],if len(self._tail_data) < self._base_number:,if data is not None:,0.5293509107883705,0.6907573115737006,False
2079,"def replace_field_to_value(layout, cb): for i, lo in enumerate(layout.fields): if isinstance(lo, Field) or issubclass(lo.__class__, Field): layout.fields[i] = ShowField(cb, *lo.fields, attrs=lo.attrs, wrapper_class=lo.wrapper_class) elif isinstance(lo, basestring): layout.fields[i] = ShowField(cb, lo) <IF_STMT> replace_field_to_value(lo, cb)","elif hasattr(lo, 'get_field_names'):","elif isinstance(lo, Field):",0.6635301508003437,0.8589552172592859,False
2080,"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException('Item does not exist!') if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT: raise ItemNotFoundException(e.get_dbus_message()) <IF_STMT> raise SecretServiceNotAvailableException(e.get_dbus_message()) raise","if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED):",if e.get_dbus_name() == DBUS_NO_SUCH_SECRET_SERVICE:,0.9045056117626832,0.8228500218338367,False
2081,"def results_iter(self): if self.connection.ops.oracle: from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] <IF_STMT> date = typecast_timestamp(str(date)) yield date",elif needs_string_cast:,if needs_string_cast:,0.8254819998963656,0.9122561819614461,False
2082,"def handle_label(self, path, **options): verbosity = int(options.get('verbosity', 1)) result = finders.find(path, all=options['all']) path = smart_unicode(path) if result: if not isinstance(result, (list, tuple)): result = [result] output = u'\n  '.join((smart_unicode(os.path.realpath(path)) for path in result)) self.stdout.write(smart_str(u""Found '%s' here:\n  %s\n"" % (path, output))) el<IF_STMT> self.stderr.write(smart_str(""No matching file found for '%s'.\n"" % path))",if verbosity >= 1:,if verbosity == 0:,0.9442285297458759,0.8902056737869248,False
2083,"def name(self): """"""Get the enumeration name of this storage class."""""" if self._name_map is None: self._name_map = {} for key, value in list(StorageClass.__dict__.items()): <IF_STMT> self._name_map[value] = key return self._name_map[self]","if isinstance(value, StorageClass):","if isinstance(value, StorageClass):",0.8573021078174704,0.8390782502060267,True
2084,"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) if r == self._zero: return int(q) el<IF_STMT> q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError('{} is not in numeric range'.format(value))",if self._start >= value > self._stop:,if self._start >= value < self._stop:,0.9097250024521356,0.8443258653392445,False
2085,"def extract_cookie(cookie_header, cookie_name): inx = cookie_header.find(cookie_name) if inx >= 0: end_inx = cookie_header.find(';', inx) <IF_STMT> value = cookie_header[inx:end_inx] else: value = cookie_header[inx:] return value return ''",if end_inx > 0:,if end_inx >= 0:,0.7516067996351756,0.7965020533851944,False
2086,"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) <IF_STMT> size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","elif isinstance(elem, np.ndarray):","elif isinstance(elem, np.ndarray):",0.7164438991744927,0.8928756684056034,True
2087,"def createFields(self): size = self.size / 8 if size > 2: <IF_STMT> yield UInt8(self, 'cs', '10ms units, values from 0 to 199') yield Bits(self, '2sec', 5, 'seconds/2') yield Bits(self, 'min', 6, 'minutes') yield Bits(self, 'hour', 5, 'hours') yield Bits(self, 'day', 5, '(1-31)') yield Bits(self, 'month', 4, '(1-12)') yield Bits(self, 'year', 7, '(0 = 1980, 127 = 2107)')",if size > 4:,if size % 2 == 0:,0.9249682493268272,0.8754021059663507,False
2088,"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('incap_ses|visid_incap', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None retval |= re.search('Incapsula', headers.get('X-CDN', ''), re.I) is not None <IF_STMT> break return retval",if retval:,if retval:,0.7871537341459545,0.8996480074924822,True
2089,"def _get_order_information(self, node_id, timeout=1200, check_interval=5): mask = {'billingItem': '', 'powerState': '', 'operatingSystem': {'passwords': ''}, 'provisionDate': ''} for i in range(0, timeout, check_interval): res = self.connection.request('SoftLayer_Virtual_Guest', 'getObject', id=node_id, object_mask=mask).object <IF_STMT> return res time.sleep(check_interval) raise SoftLayerException('Timeout on getting node details')","if res.get('provisionDate', None):",if res:,0.7564258007730689,0.9051034981560222,False
2090,"def _process_param_change(self, msg): msg = super(Select, self)._process_param_change(msg) labels, values = (self.labels, self.values) if 'value' in msg: msg['value'] = [labels[indexOf(v, values)] for v in msg['value'] if isIn(v, values)] if 'options' in msg: msg['options'] = labels <IF_STMT> self.value = [v for v in self.value if isIn(v, values)] return msg","if any((not isIn(v, values) for v in self.value)):",if self.value:,0.6969689446932614,0.9237460349978159,False
2091,"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip('\\') for a, o in self.objects.items(): if not o.name: continue if o.name.lower() == name.lower(): return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] <IF_STMT> name = m[0] return self.get_object_from_name(name, False)",if m:,if len(m) == 1:,0.9399962640212782,0.8856327184319047,False
2092,"def run(self): for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'User': if v['email'] == '': v['email'] = None if v['ip'] == '0.0.0.0': v['ip'] = None return self.objs",if k.startswith('_'):,if k.startswith('_'):,0.8311931891689034,0.8743414417652072,True
2093,"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): if descriptor in ent_desc: <IF_STMT> pass else: res.append(ent_id) return res",if ent_id in res:,if ent_desc['type'] == 'providers':,0.8911326937582862,0.7886336751695258,False
2094,"def test_add_participant(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: share_history_time = datetime.utcnow() share_history_time = share_history_time.replace(tzinfo=TZ_UTC) new_participant = ChatThreadParticipant(user=self.new_user, display_name='name', share_history_time=share_history_time) await self.chat_thread_client.add_participant(new_participant) <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id)",if not self.is_playback():,if self.thread_id:,0.7808266905352482,0.8701761846085435,False
2095,"def url(regex, view, kwargs=None, name=None, prefix=''): if isinstance(view, (list, tuple)): urlconf_module, app_name, namespace = view return RegexURLResolver(regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace) else: if isinstance(view, basestring): if not view: raise ImproperlyConfigured('Empty URL pattern view name not permitted (for pattern %r)' % regex) <IF_STMT> view = prefix + '.' + view return RegexURLPattern(regex, view, kwargs, name)",if prefix:,if prefix:,0.873221257798545,0.933847757608669,True
2096,"def tx(): while not sub_ready.ready(): pub.send(b'test BEGIN') eventlet.sleep(0.005) for i in range(1, 101): msg = 'test {0}'.format(i).encode() <IF_STMT> pub.send(msg) else: pub.send(b'test LAST') sub_last.wait() eventlet.sleep(0.001) pub.send(b'done DONE')",if i != 50:,if pub.send(msg):,0.8018873679347438,0.8590888738245122,False
2097,"def remove_tmp_snapshot_file(self, files): for filepath in files: path = Path(filepath) if path.is_dir() and path.exists(): shutil.rmtree(path) <IF_STMT> path.unlink()",elif path.is_file() and path.exists():,elif path.is_file():,0.5972442424558406,0.8305389167974837,False
2098,"def f(view, s): if mode == modes.INTERNAL_NORMAL: if count == 1: <IF_STMT> eol = view.line(s.b).b return R(s.b, eol) return s return s",if view.line(s.b).size() > 0:,if s.b.is_whitespace():,0.8627398125186139,0.8318180062062374,False
2099,"def get_ids(self, **kwargs): id = [] if 'id' in kwargs: id = kwargs['id'] <IF_STMT> id = id.split(',') try: id = list(map(int, id)) except Exception: decorators.error('Invalid id') return id","if not isinstance(id, list):","if ',' in id:",0.6242642456836311,0.8105932471967202,False
2100,def param_value(self): for token in self: <IF_STMT> return token.stripped_value if token.token_type == 'quoted-string': for token in token: if token.token_type == 'bare-quoted-string': for token in token: if token.token_type == 'value': return token.stripped_value return '',if token.token_type == 'value':,if token.token_type == 'value':,0.7811961992517376,0.8385130047130208,True
2101,"def get_all_start_methods(self): if sys.platform == 'win32': return ['spawn'] else: methods = ['spawn', 'fork'] if sys.platform == 'darwin' else ['fork', 'spawn'] <IF_STMT> methods.append('forkserver') return methods",if reduction.HAVE_SEND_HANDLE:,if self.is_running():,0.6738054419719098,0.8466657105524215,False
2102,"def _process_watch(self, watched_event): logger.debug('process_watch: %r', watched_event) with handle_exception(self._tree._error_listeners): <IF_STMT> assert self._parent is None, 'unexpected CREATED on non-root' self.on_created() elif watched_event.type == EventType.DELETED: self.on_deleted() elif watched_event.type == EventType.CHANGED: self._refresh_data() elif watched_event.type == EventType.CHILD: self._refresh_children()",if watched_event.type == EventType.CREATED:,if watched_event.type == EventType.CREATED:,0.8950642306663229,0.8385130047130208,True
2103,"def assert_open(self, sock, *rest): if isinstance(sock, fd_types): self.__assert_fd_open(sock) else: fileno = sock.fileno() assert isinstance(fileno, fd_types), fileno sockname = sock.getsockname() assert isinstance(sockname, tuple), sockname <IF_STMT> self.__assert_fd_open(fileno) else: self._assert_sock_open(sock) if rest: self.assert_open(rest[0], *rest[1:])",if not WIN:,if fileno:,0.9211465073768357,0.8827916928185874,False
2104,"def detype(self): """"""De-types the instance, allowing it to be exported to the environment."""""" style = self.style if self._detyped is None: self._detyped = ':'.join([key + '=' + ';'.join([LsColors.target_value <IF_STMT> else ansi_color_name_to_escape_code(v, cmap=style) for v in val]) for key, val in sorted(self._d.items())]) return self._detyped",if key in self._targets,if v is None,0.7853981841682149,0.8723360571509826,False
2105,"def gather_metrics(dry_run=False): today = datetime.date.today() first = today.replace(day=1) last_month = first - datetime.timedelta(days=1) filename = 'form_types_{}.csv'.format(last_month.strftime('%Y-%m')) with connection.cursor() as cursor: cursor.execute(REGISTRATION_METRICS_SQL) <IF_STMT> for row in cursor.fetchall(): logger.info(encode_row(row)) else: write_raw_data(cursor=cursor, filename=filename)",if dry_run:,if dry_run:,0.906761040187068,0.8787142254774354,True
2106,"def cat(tensors, dim=0): assert isinstance(tensors, list), 'input to cat must be a list' if len(tensors) == 1: return tensors[0] from .autograd_cryptensor import AutogradCrypTensor if any((isinstance(t, AutogradCrypTensor) for t in tensors)): <IF_STMT> tensors[0] = AutogradCrypTensor(tensors[0], requires_grad=False) return tensors[0].cat(*tensors[1:], dim=dim) else: return get_default_backend().cat(tensors, dim=dim)","if not isinstance(tensors[0], AutogradCrypTensor):",if len(tensors) == 1:,0.9054218318780733,0.8723360571509826,False
2107,"def is_installed(self, dlc_title='') -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info('version', dlc_title) installed = True if dlc_version else False if not installed: status = self.legacy_get_dlc_status(dlc_title) installed = True if status in ['installed', 'updatable'] else False el<IF_STMT> installed = True return installed",if self.install_dir and os.path.exists(self.install_dir):,"if status in ['installed', 'updatable']:",0.8751954033024067,0.8559898693114286,False
2108,"def on_copy(self): source_objects = self.__getSelection() for source in source_objects: <IF_STMT> new_obj = model.Phrase('', '') else: new_obj = model.Script('', '') new_obj.copy(source) self.cutCopiedItems.append(new_obj)","if isinstance(source, model.Phrase):",if source.is_phrase():,0.8370789179495434,0.8232490471721702,False
2109,"def FetchFn(type_name): """"""Fetches all hunt results of a given type."""""" offset = 0 while True: results = data_store.REL_DB.ReadHuntResults(hunt_id, offset=offset, count=self._RESULTS_PAGE_SIZE, with_type=type_name) <IF_STMT> break for r in results: msg = r.AsLegacyGrrMessage() msg.source_urn = source_urn yield msg offset += self._RESULTS_PAGE_SIZE",if not results:,if not results:,0.7113824968651381,0.8815741981066073,True
2110,"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: <IF_STMT> return 'TINYBLOB' if length <= self.LENGTH_LIMIT_BLOB: return 'BLOB' if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_TINYBLOB:,if length <= self.LENGTH_LIMIT_TINYBLOB:,0.8664813237850955,0.7886336751695258,True
2111,"def decode(cls, data): while data: length, atype = unpack(cls.Header.PACK, data[:cls.Header.LEN]) <IF_STMT> raise AttributesError('Buffer underrun %d < %d' % (len(data), length)) payload = data[cls.Header.LEN:length] yield (atype, payload) data = data[int((length + 3) / 4) * 4:]",if len(data) < length:,if length < cls.Header.LEN:,0.7348357565136909,0.8474968231198384,False
2112,"def test_join_diffs(db, series_of_diffs, expected): diffs = [] for changes in series_of_diffs: tracker = DBDiffTracker() for key, val in changes.items(): <IF_STMT> del tracker[key] else: tracker[key] = val diffs.append(tracker.diff()) DBDiff.join(diffs).apply_to(db) assert db == expected",if val is None:,if val is None:,0.8188307512511809,0.8336104423443033,True
2113,"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' elif col == BARRIER: tmp += '%' elif col == FOOD: tmp += '*' <IF_STMT> tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == UNSEEN:,elif col == WAND:,0.8191683994133222,0.924776563154472,False
2114,"def _report_error(self, completion_routine, response=None, message=None): if response: if not response.ok: status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: <IF_STMT> return completion_routine(status) raise IOError(response.text) elif completion_routine: completion_routine(status) return location.Status(200, response.content)",if completion_routine:,if completion_routine:,0.938209576937583,0.9122561819614461,True
2115,"def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None): """"""Yields examples."""""" with tf.io.gfile.GFile(src_path) as f_d, tf.io.gfile.GFile(tgt_path) as f_s: for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)): <IF_STMT> yield (i, {_DOCUMENT: doc_text.strip().replace('<unk>', 'UNK'), _SUMMARY: sum_text.strip().replace('<unk>', 'UNK')}) else: yield (i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()})",if replace_unk:,if replace_unk:,0.9305533519981917,0.9024521756077707,True
2116,"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') <IF_STMT> if '\n' in text: text = text.replace('\n', '<br>') return text",if newline:,if newline:,0.9570608003415977,0.9415839804285298,True
2117,"def _handle_url_click(self, event): url = _extract_click_text(self.info_text, event, 'url') if url is not None: <IF_STMT> import webbrowser webbrowser.open(url) elif os.path.sep in url: os.makedirs(url, exist_ok=True) open_path_in_system_file_manager(url) else: self._start_show_package_info(url)",if url.startswith('http:') or url.startswith('https:'):,if os.path.isfile(url):,0.6946872133076328,0.8590888738245122,False
2118,"def SConsignFile(self, name='.sconsign', dbm_module=None): if name is not None: name = self.subst(name) <IF_STMT> name = os.path.join(str(self.fs.SConstruct_dir), name) if name: name = os.path.normpath(name) sconsign_dir = os.path.dirname(name) if sconsign_dir and (not os.path.exists(sconsign_dir)): self.Execute(SCons.Defaults.Mkdir(sconsign_dir)) SCons.SConsign.File(name, dbm_module)",if not os.path.isabs(name):,if self.fs.SConstruct_dir:,0.7119513692822852,0.8901732118131125,False
2119,"def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None: super().on_train_start(trainer, pl_module) submodule_dict = dict(pl_module.named_modules()) self._hook_handles = [] for name in self._get_submodule_names(pl_module): <IF_STMT> rank_zero_warn(f'{name} is not a valid identifier for a submodule in {pl_module.__class__.__name__}, skipping this key.') continue handle = self._register_hook(name, submodule_dict[name]) self._hook_handles.append(handle)",if name not in submodule_dict:,if name not in submodule_dict:,0.8861211252192933,0.8492988135354755,True
2120,"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): super().validate_configuration(configuration) if configuration is None: configuration = self.configuration try: assert 'value_set' in configuration.kwargs, 'value_set is required' assert isinstance(configuration.kwargs['value_set'], (list, set, dict)), 'value_set must be a list or a set' <IF_STMT> assert '$PARAMETER' in configuration.kwargs['value_set'], 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key.' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True","if isinstance(configuration.kwargs['value_set'], dict):",if 'evaluation_parameter' in configuration.kwargs:,0.9348576874948181,0.9036816878108535,False
2121,"def check_refcounts(expected, timeout=10): start = time.time() while True: try: _check_refcounts(expected) break except AssertionError as e: <IF_STMT> raise e else: time.sleep(0.1)",if time.time() - start > timeout:,if time.time() - start > timeout:,0.7981805526995738,0.6748913185157768,True
2122,"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() <IF_STMT> break if prog.match(line): text = line[len(key) + 1:] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",if not line:,if not line or not line[0].isspace():,0.9227598005711842,0.8794127725861569,False
2123,def _is_perf_file(file_path): f = get_file(file_path) for line in f: <IF_STMT> continue r = event_regexp.search(line) if r: f.close() return True f.close() return False,if line[0] == '#':,if line.startswith('#'):,0.7951012009381532,0.8318180062062374,False
2124,"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for page, soup in soups.items(): for a in soup.find_all('a'): <IF_STMT> continue pantsref = a['pantsref'] if pantsref not in precomputed.pantsref: raise TaskError(f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it') a['href'] = rel_href(page, precomputed.pantsref[pantsref])",if not a.has_attr('pantsref'):,if 'pantsref' not in a:,0.8176128773748219,0.8649799950178215,False
2125,"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: <IF_STMT> self.query_id = query.query_id elif self.query_id != query.query_id: raise ValueError('query in list must be same query_id')",if self.query_id == -1:,if self.query_id is None:,0.8877239908964,0.8555308664663046,False
2126,"def _draw_number(screen, x_offset, y_offset, number, token=Token.Clock, transparent=False): """"""Write number at position."""""" fg = Char(' ', token) bg = Char(' ', Token) for y, row in enumerate(_numbers[number]): screen_row = screen.data_buffer[y + y_offset] for x, n in enumerate(row): <IF_STMT> screen_row[x + x_offset] = fg elif not transparent: screen_row[x + x_offset] = bg",if n == '#':,if n == token:,0.9032750496701633,0.8923575006167597,False
2127,"def init(self): self.sock.setblocking(True) if self.parser is None: <IF_STMT> self.sock = ssl.wrap_socket(self.sock, server_side=True, **self.cfg.ssl_options) self.parser = http.RequestParser(self.cfg, self.sock)",if self.cfg.is_ssl:,if self.cfg.ssl_options:,0.86437915169987,0.7778111223054219,False
2128,"def intersect_face(pt): nonlocal vis_faces2D for f, vs in vis_faces2D: v0 = vs[0] for v1, v2 in iter_pairs(vs[1:], False): <IF_STMT> return f return None","if intersect_point_tri_2d(pt, v0, v1, v2):",if v0 == v1 and v2 == pt:,0.8579777515815449,0.6625231057044074,False
2129,"def IMPORTFROM(self, node): if node.module == '__future__': if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: <IF_STMT> self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == '__future__': importation.used = (self.scope, node) self.addBinding(node, importation)",if alias.name == '*':,if alias.asname and alias.name == 'star':,0.7352787595071706,0.8641006343550574,False
2130,"def PyObject_Bytes(obj): if type(obj) == bytes: return obj if hasattr(obj, '__bytes__'): res = obj.__bytes__() <IF_STMT> raise TypeError('__bytes__ returned non-bytes (type %s)' % type(res).__name__) return PyBytes_FromObject(obj)","if not isinstance(res, bytes):","if not isinstance(res, bytes):",0.7096269849804909,0.7886336751695258,True
2131,"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text()  @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) <IF_STMT> return self.current_provider.on_file(query)",elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:,elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:,0.9049511190845795,0.8260567476092244,True
2132,"def remove(self, name): for s in [self.__storage(self.__category), self.__storage(None)]: for i, b in enumerate(s): <IF_STMT> del s[i] if b.persistent: self.__save() return raise KeyError(name)",if b.name == name:,if b.name == name:,0.7805248488247604,0.760856626273165,True
2133,"def _wrapper(data, axis=None, keepdims=False): if not keepdims: return func(data, axis=axis) else: <IF_STMT> axis = axis if isinstance(axis, int) else axis[0] out_shape = list(data.shape) out_shape[axis] = 1 else: out_shape = [1 for _ in range(len(data.shape))] return func(data, axis=axis).reshape(out_shape)",if axis is not None:,if axis is not None:,0.8589538647505786,0.8338542560892604,True
2134,"def authn_info(self): res = [] for astat in self.assertion.authn_statement: context = astat.authn_context try: authn_instant = astat.authn_instant except AttributeError: authn_instant = '' <IF_STMT> try: aclass = context.authn_context_class_ref.text except AttributeError: aclass = '' try: authn_auth = [a.text for a in context.authenticating_authority] except AttributeError: authn_auth = [] res.append((aclass, authn_auth, authn_instant)) return res",if context:,if context:,0.7605353339412333,0.926934323706186,True
2135,"def _persist_metadata(self, dirname, filename): metadata_path = '{0}/{1}.json'.format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: <IF_STMT> self.merge_json({'GraphImages': self.posts}, metadata_path) else: self.save_json({'GraphImages': self.posts}, metadata_path) if self.stories: if self.latest: self.merge_json({'GraphStories': self.stories}, metadata_path) else: self.save_json({'GraphStories': self.stories}, metadata_path)",if self.latest:,if self.latest:,0.9340124335708764,0.8966773400768917,True
2136,"def update_record_image_detail(input_image_record, updated_image_detail, session=None): if not session: session = db.Session image_record = {} image_record.update(input_image_record) image_record.pop('created_at', None) image_record.pop('last_updated', None) if image_record['image_type'] == 'docker': for tag_record in updated_image_detail: <IF_STMT> image_record['image_detail'].append(tag_record) return update_record(image_record, session=session) return image_record",if tag_record not in image_record['image_detail']:,if tag_record['image_type'] == 'docker':,0.9067899607697214,0.8385130047130208,False
2137,"def backup(self): for ds in [('activedirectory', 'AD'), ('ldap', 'LDAP'), ('nis', 'NIS')]: <IF_STMT> try: ds_cache = self.middleware.call_sync('cache.get', f'{ds[1]}_cache') with open(f'/var/db/system/.{ds[1]}_cache_backup', 'wb') as f: pickle.dump(ds_cache, f) except KeyError: self.logger.debug('No cache exists for directory service [%s].', ds[0])",if self.middleware.call_sync(f'{ds[0]}.config')['enable']:,if ds[0] in self.cache_dir_service_names:,0.8201366473756269,0.8431339019329497,False
2138,"def parse_setup_cfg(self): if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: if six.PY2: contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) <IF_STMT> return {} return parsed return {}",if not parsed:,if parsed is None:,0.8836732646437944,0.8555308664663046,False
2139,"def parts(): for l in lists.leaves: head_name = l.get_head_name() <IF_STMT> yield l.leaves elif head_name != 'System`Missing': raise MessageException('Catenate', 'invrp', l)",if head_name == 'System`List':,if head_name == 'System`List':,0.5578623755732253,0.7378351342269067,True
2140,"def _get_callback_and_order(self, hook): if callable(hook): return (hook, None) elif isinstance(hook, tuple) and len(hook) == 2: callback, order = hook <IF_STMT> raise ValueError('Hook callback is not a callable') try: int(order) except ValueError: raise ValueError('Hook order is not a number') return (callback, order) else: raise ValueError('Invalid hook definition, neither a callable nor a 2-tuple (callback, order): {!r}'.format(hook))",if not callable(callback):,if not callable(callback):,0.6332500013353997,0.9182210682909737,True
2141,"def _resize_masks(self, results): """"""Resize masks with ``results['scale']``"""""" for key in results.get('mask_fields', []): if results[key] is None: continue <IF_STMT> results[key] = results[key].rescale(results['scale']) else: results[key] = results[key].resize(results['img_shape'][:2])",if self.keep_ratio:,"if isinstance(results[key], ImageMask):",0.736833414477501,0.8196189957582152,False
2142,"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) if not c.isVisible(): continue <IF_STMT> nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",if c.getYAxis() == Y_AXIS:,if self.chart.getCurveWidth() == c.getCurveHeight() - 1:,0.8054742876136989,0.8221293984712319,False
2143,"def _check_token(self): if settings.app.sso_client_cache and self.server_auth_token: doc = self.sso_client_cache_collection.find_one({'user_id': self.user.id, 'server_id': self.server.id, 'device_id': self.device_id, 'device_name': self.device_name, 'auth_token': self.server_auth_token}) <IF_STMT> self.has_token = True",if doc:,if doc:,0.8513059158805044,0.8318180062062374,True
2144,"def parse_header(plyfile, ext): line = [] properties = [] num_points = None while b'end_header' not in line and line != b'': line = plyfile.readline() <IF_STMT> line = line.split() num_points = int(line[2]) elif b'property' in line: line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return (num_points, properties)",if b'element' in line:,if b'num_points' in line:,0.8962656608116144,0.8806615362338783,False
2145,"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = (self.pyflakes_enabled, self.pep8_enabled) for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() <IF_STMT> if current_finfo is not finfo: finfo.run_code_analysis(run_pyflakes, run_pep8)",if (run_pyflakes or run_pep8) and current_finfo is not None:,if self.data[finfo.editor].is_code_analysis:,0.7799918575755803,0.8466657105524215,False
2146,"def __modules(self): raw_output = self.__module_avail_output().decode('utf-8') for line in StringIO(raw_output): line = line and line.strip() <IF_STMT> continue line_modules = line.split() for module in line_modules: if module.endswith(self.default_indicator): module = module[0:-len(self.default_indicator)].strip() module_parts = module.split('/') module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield (module_name, module_version)",if not line or line.startswith('-'):,if not line:,0.7195879920802501,0.9042878500265974,False
2147,"def _set_trailing_size(self, size): if self.is_free(): next_chunk = self.next_chunk() <IF_STMT> self.state.memory.store(next_chunk.base, size, self.state.arch.bytes)",if next_chunk is not None:,if next_chunk.base != 0:,0.5106329922909493,0.570282226440554,False
2148,"def _execute_for_all_tables(self, app, bind, operation, skip_tables=False): app = self.get_app(app) if bind == '__all__': binds = [None] + list(app.config.get('SQLALCHEMY_BINDS') or ()) elif isinstance(bind, string_types) or bind is None: binds = [bind] else: binds = bind for bind in binds: extra = {} <IF_STMT> tables = self.get_tables_for_bind(bind) extra['tables'] = tables op = getattr(self.Model.metadata, operation) op(bind=self.get_engine(app, bind), **extra)",if not skip_tables:,if skip_tables:,0.9025530894449105,0.9350761925543661,False
2149,"def getFileName(): extension = '.json' file = '%s-stats' % self.clusterName counter = 0 while True: suffix = str(counter).zfill(3) + extension fullName = os.path.join(self.statsPath, file + suffix) <IF_STMT> return fullName counter += 1",if not os.path.exists(fullName):,if os.path.exists(fullName):,0.7788773764178687,0.8866029039778043,False
2150,def logic(): if goRight == ACTIVE: dir.next = DirType.RIGHT run.next = True elif goLeft == ACTIVE: dir.next = DirType.LEFT run.next = True if stop == ACTIVE: run.next = False if run: <IF_STMT> q.next[4:1] = q[3:] q.next[0] = not q[3] else: q.next[3:] = q[4:1] q.next[3] = not q[0],if dir == DirType.LEFT:,if q[0] == q[3]:,0.6301419433092854,0.8856327184319047,False
2151,"def test_broadcast(self): """"""Test example broadcast functionality."""""" self.create_lang_connection('1000000000', 'en') self.create_lang_connection('1000000001', 'en') self.create_lang_connection('1000000002', 'en') self.create_lang_connection('1000000003', 'es') self.create_lang_connection('1000000004', 'es') app.lang_broadcast() self.assertEqual(2, len(self.outbound)) for message in self.outbound: if message.text == 'hello': self.assertEqual(3, len(message.connections)) <IF_STMT> self.assertEqual(2, len(message.connections))",elif message.text == 'hola':,elif message.text == 'hello2':,0.918285764215125,0.8418243449361874,False
2152,"def get_ovf_env(dirname): env_names = ('ovf-env.xml', 'ovf_env.xml', 'OVF_ENV.XML', 'OVF-ENV.XML') for fname in env_names: full_fn = os.path.join(dirname, fname) <IF_STMT> try: contents = util.load_file(full_fn) return (fname, contents) except Exception: util.logexc(LOG, 'Failed loading ovf file %s', full_fn) return (None, False)",if os.path.isfile(full_fn):,if os.path.isfile(full_fn):,0.7792306119730308,0.8996480074924822,True
2153,"def _calc_offsets_children(self, offset, is_last): if self.elems: elem_last = self.elems[-1] for elem in self.elems: offset = elem._calc_offsets(offset, elem is elem_last) offset += _BLOCK_SENTINEL_LENGTH elif not self.props or self.id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL: <IF_STMT> offset += _BLOCK_SENTINEL_LENGTH return offset",if not is_last:,if is_last:,0.9122869453137901,0.8966773400768917,False
2154,"def publish_state(cls, payload, state): try: <IF_STMT> if state == action_constants.LIVEACTION_STATUS_REQUESTED: cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)","if isinstance(payload, LiveActionDB):",if state != action_constants.LIVEACTION_STATUS_RUNNING:,0.5831256900294433,0.693395566222006,False
2155,"def log_predictive_density(self, x_test, y_test, Y_metadata=None): if isinstance(x_test, list): x_test, y_test, ind = util.multioutput.build_XY(x_test, y_test) <IF_STMT> Y_metadata = {'output_index': ind, 'trials': np.ones(ind.shape)} return super(MultioutputGP, self).log_predictive_density(x_test, y_test, Y_metadata)",if Y_metadata is None:,if Y_metadata is None:,0.753945880371541,0.7965020533851944,True
2156,"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: <IF_STMT> break else: if m in candidates: candidates.remove(m) candidates.append(m) return candidates","if issubclass(n, m) and m is not n:",if n is m:,0.9270070180136183,0.8923575006167597,False
2157,"def apply(self, operations, rotations=None, **kwargs): rotations = rotations or [] for i, operation in enumerate(operations): <IF_STMT> raise DeviceError('Operation {} cannot be used after other Operations have already been applied on a {} device.'.format(operation.name, self.short_name)) for operation in operations: self._apply_operation(operation) self._pre_rotated_state = self._state for operation in rotations: self._apply_operation(operation)","if i > 0 and isinstance(operation, (QubitStateVector, BasisState)):",if i % self.max_operations == 0:,0.8187959570899357,0.8505405945991492,False
2158,"def __str__(self): txt = str(self._called) if self.call_gas or self.call_value: gas = f'gas: {self.call_gas}' if self.call_gas else '' value = f'value: {self.call_value}' if self.call_value else '' salt = f'salt: {self.call_salt}' if self.call_salt else '' <IF_STMT> options = [gas, value, salt] txt += '{' + ','.join([o for o in options if o != '']) + '}' return txt + '(' + ','.join([str(a) for a in self._arguments]) + ')'",if gas or value or salt:,if self._arguments:,0.7938266125958976,0.9460866645260309,False
2159,"def pop(self): """"""Pop a nonterminal.  (Internal)"""""" popdfa, popstate, popnode = self.stack.pop() newnode = self.convert(self.grammar, popnode) if newnode is not None: <IF_STMT> dfa, state, node = self.stack[-1] node.children.append(newnode) else: self.rootnode = newnode",if self.stack:,if self.stack:,0.7664460243316045,0.8827916928185874,True
2160,"def pollpacket(self, wait): self._stage0() if len(self.buffer) < self.bufneed: r, w, x = select.select([self.sock.fileno()], [], [], wait) <IF_STMT> return None try: s = self.sock.recv(BUFSIZE) except socket.error: raise EOFError if len(s) == 0: raise EOFError self.buffer += s self._stage0() return self._stage1()",if len(r) == 0:,if not r:,0.9180570065897339,0.884617925078158,False
2161,"def increaseToolReach(self): if self.draggingFace is not None: d = (1, -1)[self.draggingFace & 1] <IF_STMT> d = -d self.draggingY += d x, y, z = self.editor.mainViewport.cameraPosition pos = [x, y, z] pos[self.draggingFace >> 1] += d self.editor.mainViewport.cameraPosition = tuple(pos) else: self.cloneCameraDistance = self.editor._incrementReach(self.cloneCameraDistance) return True",if self.draggingFace >> 1 != 1:,if self.draggingFace & 1:,0.6396593953349197,0.8780099567239787,False
2162,"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True <IF_STMT> selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if remove and (not add):,if add:,0.9216618739276293,0.8901732118131125,False
2163,"def __init__(self, *args, **kwargs): super(ProjectForm, self).__init__(*args, **kwargs) if self.instance.id: <IF_STMT> self.fields['localfiletype'].widget.attrs['disabled'] = True self.fields['localfiletype'].required = False if self.instance.treestyle != 'auto' and self.instance.translationproject_set.count() and (self.instance.treestyle == self.instance._detect_treestyle()): self.fields['treestyle'].widget.attrs['disabled'] = True self.fields['treestyle'].required = False",if Store.objects.filter(translation_project__project=self.instance).count():,if self.instance.localfiletype != 'auto':,0.921060022195298,0.8336104423443033,False
2164,"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): <IF_STMT> raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if return_type is str:,if return_type is None:,0.9085981434403536,0.9204199807826591,False
2165,"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) if len(inst) > count: count = len(inst) toReturn.append(gadget) added = True <IF_STMT> callback(gadget, added, float(i + 1) / len_gadgets) added = False return toReturn",if callback:,if callback:,0.9514996617538101,0.9253742688467129,True
2166,"def send_all(self, data: bytes): with self._conflict_detector: <IF_STMT> raise _core.ClosedResourceError('this pipe is already closed') if not data: await _core.checkpoint() return try: written = await _core.write_overlapped(self._handle_holder.handle, data) except BrokenPipeError as ex: raise _core.BrokenResourceError from ex assert written == len(data)",if self._handle_holder.closed:,if self._closed:,0.7925309395584555,0.9024521756077707,False
2167,"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': <IF_STMT> value = self.sv_get()[0][0] print('V', value) if isinstance(value, int): param_node.selected_mode = 'int' param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = 'float' param_node.float_ = value",if self.use_prop or self.get_prop_name():,if self.sv_get():,0.8505740031168413,0.8827916928185874,False
2168,"def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map): active_inst_idx_list = [] for inst_idx, inst_position in inst_idx_to_position_map.items(): is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position]) <IF_STMT> active_inst_idx_list += [inst_idx] return active_inst_idx_list",if not is_inst_complete:,if is_inst_complete:,0.6513303822561118,0.8232490471721702,False
2169,"def compare_member_req_resp_without_key(self, request, response): for user_response in resp_json(response)['data']: for user_request in request: <IF_STMT> assert user_request['role'] == user_response['role']",if user_request['user_id'] == user_response['user_id']:,if user_request['key'] == user_response['key']:,0.5745869394499267,0.693395566222006,False
2170,"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): if name.endswith('.py'): self.module_names.add(name[:-3]) <IF_STMT> self.module_names.add(name)",elif '.' not in name:,"elif os.path.isfile(os.path.join(dir, name)):",0.8623049303895655,0.7541859578343534,False
2171,"def _read_filter(self, data): if data: if self.expected_inner_sha256: self.inner_sha.update(data) <IF_STMT> self.inner_md5.update(data) return data",if self.expected_inner_md5sum:,elif self.expected_inner_md5:,0.8036486349215584,0.6703420896351792,False
2172,"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) <IF_STMT> break if s.consume_re(_newline_esc_re): pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return ''.join(res)",if not s.consume('\\'):,if s.last().group(0) == _basicstr_re:,0.7956356514200396,0.7886336751695258,False
2173,"def process_response(self, request, response): if response.status_code == 404 and request.path_info.endswith('/') and (not is_valid_path(request.path_info)) and is_valid_path(request.path_info[:-1]): newurl = request.path[:-1] <IF_STMT> with safe_query_string(request): newurl += '?' + request.META.get('QUERY_STRING', '') return HttpResponsePermanentRedirect(newurl) else: return response",if request.GET:,if request.META.get('QUERY_STRING'):,0.6379365616580128,0.8866029039778043,False
2174,"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] <IF_STMT> k = dumps(k) if Types.KEYS not in obj: obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj","if not (isinstance(k, str) or isinstance(k, unicode)):","if isinstance(k, str):",0.7627444816722125,0.8498644646741501,False
2175,"def __repr__(self): if self._in_repr: return '<recursion>' try: self._in_repr = True <IF_STMT> status = 'computed, ' if self.error() is None: if self.value() is self: status += '= self' else: status += '= ' + repr(self.value()) else: status += 'error = ' + repr(self.error()) else: status = ""isn't computed"" return '%s (%s)' % (type(self), status) finally: self._in_repr = False",if self.is_computed():,if self.computed():,0.9733626987692185,0.9374009563674955,False
2176,"def allocate_network(ipv='ipv4'): global dtcd_uuid global network_pool global allocations network = None try: cx = httplib.HTTPConnection('localhost:7623') cx.request('POST', '/v1/network/%s/' % ipv, body=dtcd_uuid) resp = cx.getresponse() <IF_STMT> network = netaddr.IPNetwork(resp.read().decode('utf-8')) cx.close() except Exception: pass if network is None: network = network_pool[ipv].pop() allocations[network] = True return network",if resp.status == 200:,if resp.status_code == 200:,0.760243086022589,0.8752376177722327,False
2177,def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith(' '): ind += 1 else: if start < ind: ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: <IF_STMT> lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if ':' in line and len(line) > 0:,if line.startswith(':'):,0.943206567659347,0.9425437476131634,False
2178,"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: <IF_STMT> print('killing %s' % member) proc = hosts[member]['proc'] if sys.platform in ('win32', 'cygwin'): os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print('%s already dead?' % member)",if ha_tools_debug:,if ha_tools_debug:,0.9227561060629172,0.8996480074924822,True
2179,"def check(self): for path in self.paths: response = self.http_request(method='GET', path=path) <IF_STMT> continue if any(map(lambda x: x in response.text, ['report.db.server.name', 'report.db.server.sa.pass', 'report.db.server.user.pass'])): self.valid = path return True return False",if response is None:,if response is None:,0.5857458057358939,0.8105932471967202,True
2180,"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) if r.ok: run_logs = r.json()['data']['records'] result.extend([i['logs'][0]['stats']['id'] for i in run_logs]) last_date = r.json()['data']['lastTimestamp'] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f'pares keep ids data since {since_time}') time.sleep(1) <IF_STMT> break return result",if not last_date:,if since_time > last_date:,0.8820888685821413,0.8780099567239787,False
2181,"def button_press_cb(self, tdw, event): self._update_zone_and_cursors(tdw, event.x, event.y) if self._zone in (_EditZone.CREATE_FRAME, _EditZone.REMOVE_FRAME): button = event.button <IF_STMT> self._click_info = (button, self._zone) return False return super(FrameEditMode, self).button_press_cb(tdw, event)",if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS:,if button in self._click_info:,0.7015251096361808,0.7965020533851944,False
2182,"def first_timestep(): assignment = self.has_previous.assign(value=tf_util.constant(value=True, dtype='bool'), read_value=False) with tf.control_dependencies(control_inputs=(assignment,)): <IF_STMT> current = x else: current = tf.expand_dims(input=x, axis=self.axis + 1) multiples = tuple((self.length if dims == self.axis + 1 else 1 for dims in range(self.output_spec().rank + 1))) return tf.tile(input=current, multiples=multiples)",if self.concatenate:,if self.axis == 0:,0.9157110877562529,0.8661072626070159,False
2183,"def main() -> None: onefuzz = Onefuzz() jobs = onefuzz.jobs.list() for job in jobs: print('job:', str(job.job_id)[:8], ':'.join([job.config.project, job.config.name, job.config.build])) for task in onefuzz.tasks.list(job_id=job.job_id): <IF_STMT> continue print('', str(task.task_id)[:8], task.config.task.type, task.config.task.target_exe)","if task.state in ['stopped', 'stopping']:",if task.config.task is None:,0.834563791703787,0.8169276475307028,False
2184,"def update_stack(self, full_name, template_url, parameters, tags): """"""Updates an existing stack in CloudFormation."""""" try: logger.info('Attempting to update stack %s.', full_name) self.conn.cloudformation.update_stack(full_name, template_url=template_url, parameters=parameters, tags=tags, capabilities=['CAPABILITY_IAM']) return SUBMITTED except BotoServerError as e: <IF_STMT> logger.info('Stack %s did not change, not updating.', full_name) return SKIPPED raise",if 'No updates are to be performed.' in e.message:,if e.status_code == 404:,0.9337600075300383,0.8723360571509826,False
2185,"def header_tag_files(env, files, legal_header, script_files=False): """"""Apply the legal_header to the list of files"""""" try: import apply_legal_header except: xbc.cdie('XED ERROR: mfile.py could not find scripts directory') for g in files: print('G: ', g) for f in mbuild.glob(g): print('F: ', f) <IF_STMT> apply_legal_header.apply_header_to_data_file(legal_header, f) else: apply_legal_header.apply_header_to_source_file(legal_header, f)",if script_files:,if script_files:,0.9232538249293152,0.9202663016973823,True
2186,"def cleanDataCmd(cmd): newcmd = 'AbracadabrA ** <?php ' if cmd[:6] != 'php://': if reverseConn not in cmd: cmds = cmd.split('&') for c in cmds: <IF_STMT> newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += '?> **' return newcmd",if len(c) > 0:,if c:,0.6243893628385375,0.926934323706186,False
2187,"def test_form(self): n_qubits = 6 random_operator = get_fermion_operator(random_interaction_operator(n_qubits)) chemist_operator = chemist_ordered(random_operator) for term, _ in chemist_operator.terms.items(): <IF_STMT> pass else: self.assertTrue(term[0][1]) self.assertTrue(term[2][1]) self.assertFalse(term[1][1]) self.assertFalse(term[3][1]) self.assertTrue(term[0][0] > term[2][0]) self.assertTrue(term[1][0] > term[3][0])",if len(term) == 2 or not len(term):,if term[0][1] == 'B':,0.9022737903220204,0.8169276475307028,False
2188,"def do(server, handler, config, modargs): data = [] clients = server.get_clients(handler.default_filter) if not clients: return for client in clients: tags = config.tags(client.node()) <IF_STMT> tags.remove(*modargs.remove) if modargs.add: tags.add(*modargs.add) data.append({'ID': client.node(), 'TAGS': tags}) config.save(project=modargs.write_project, user=modargs.write_user) handler.display(Table(data))",if modargs.remove:,if modargs.remove:,0.9410346457453282,0.8935248372106969,True
2189,"def validate(self): if self.data.get('state') == 'enabled': <IF_STMT> raise PolicyValidationError('redshift logging enablement requires `bucket` and `prefix` specification on %s' % (self.manager.data,)) return self",if 'bucket' not in self.data:,if self.data['bucket'] and self.data['prefix']:,0.8381399964936536,0.760856626273165,False
2190,"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return exists, contents = read_sysconfig_file(fn) updated_am = 0 for k, v in adjustments.items(): if v is None: continue v = str(v) if len(v) == 0 and (not allow_empty): continue contents[k] = v updated_am += 1 if updated_am: lines = [str(contents)] <IF_STMT> lines.insert(0, util.make_header()) util.write_file(fn, '\n'.join(lines) + '\n', 420)",if not exists:,if exists:,0.9522822095671817,0.9350761925543661,False
2191,"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, 'Description'): if desc.getAttributeNS(RDF_NAMESPACE, 'about') == aboutUri: attr = desc.getAttributeNodeNS(namespace, name) <IF_STMT> yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",if attr != None:,if attr:,0.7508418265175243,0.8743414417652072,False
2192,"def get_store_name_from_connection_string(connection_string): if is_valid_connection_string(connection_string): segments = dict((seg.split('=', 1) for seg in connection_string.split(';'))) endpoint = segments.get('Endpoint') <IF_STMT> return endpoint.split('//')[1].split('.')[0] return None",if endpoint:,if endpoint:,0.7823039496771046,0.8137489370974955,True
2193,"def insertLoopTemplate(self, layout): col = layout.column(align=True) for socket in self.activeNode.outputs: <IF_STMT> props = col.operator('an.insert_loop_for_iterator', text='Loop through {}'.format(repr(socket.getDisplayedName())), icon='MOD_ARRAY') props.nodeIdentifier = self.activeNode.identifier props.socketIndex = socket.getIndex()",if not socket.hide and isList(socket.bl_idname):,if socket.isLoop():,0.888011484292082,0.8466657105524215,False
2194,"def do_task(self, task): self.running_task += 1 result = (yield gen.Task(self.fetcher.fetch, task)) type, task, response = result.args self.processor.on_task(task, response) while not self.processor.inqueue.empty(): _task, _response = self.processor.inqueue.get() self.processor.on_task(_task, _response) while not self.processor.result_queue.empty(): _task, _result = self.processor.result_queue.get() <IF_STMT> self.result_worker.on_result(_task, _result) self.running_task -= 1",if self.result_worker:,if _result:,0.8884483303859299,0.9099951253570094,False
2195,"def _parse_config_result(data): command_list = ' ; '.join([x.strip() for x in data[0]]) config_result = data[1] if isinstance(config_result, list): result = '' <IF_STMT> for key in config_result[0]: result += config_result[0][key] config_result = result else: config_result = config_result[0] return [command_list, config_result]","if isinstance(config_result[0], dict):","if isinstance(config_result[0], dict):",0.7681677014285376,0.8815741981066073,True
2196,"def load_api_handler(self, mod_name): for name, hdl in API_HANDLERS: name = name.lower() <IF_STMT> handler = self.mods.get(name) if not handler: handler = hdl(self.emu) self.mods.update({name: handler}) return handler return None",if mod_name and name == mod_name.lower():,if name.startswith(mod_name):,0.713368600564937,0.8645707301556367,False
2197,def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: proc = PipelineProcess.objects.get(id=proc_id) if not proc.is_alive or proc.is_frozen: continue for dr in self.doctors: <IF_STMT> dr.cure(proc) break,if dr.confirm(proc):,if dr.is_alive:,0.9185137201925369,0.8841121363289183,False
2198,"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError('You can either use {} with one positional argument or with keyword arguments, not both.'.format(cls.__name__)) <IF_STMT> return super().__new__(cls) if isinstance(args[0], cls): return cls return super().__new__(cls, *args, **kwargs)",if not args[0]:,"if isinstance(args[0], cls):",0.9196554206398463,0.8815741981066073,False
2199,"def __lt__(self, other): try: A, B = (self[0], other[0]) <IF_STMT> if A == B: return self[2] < other[2] return A < B return self[1] < other[1] except IndexError: return NotImplemented",if A and B:,if A == B:,0.7476876500125131,0.8228500218338367,False
2200,"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): if str.lower(key) == str.lower(resource_provider): <IF_STMT> return GeneralPrivateEndpointClient(key, value['api_version'], value['support_list_or_not'], value['resource_get_api_version']) return value() raise CLIError('Resource type must be one of {}'.format(', '.join(rp_mapping.keys())))","if isinstance(value, dict):",if 'api_version' in value:,0.9039881506455381,0.8169276475307028,False
2201,"def test_progressbar_format_pos(runner, pos, length): with _create_progress(length, length_known=length != 0, pos=pos) as progress: result = progress.format_pos() <IF_STMT> assert result == f'{pos}/{length}' else: assert result == str(pos)",if progress.length_known:,if runner.is_running():,0.7294932899638977,0.8531413606256201,False
2202,"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue <IF_STMT> flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if not v.has_attribute(SplitTarget):,if height != width:,0.7936461449087273,0.8627586293513119,False
2203,"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: <IF_STMT> tmp += '.' elif col == BARRIER: tmp += '%' elif col == FOOD: tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",if col == LAND:,if col == PLAYER:,0.9472769362135423,0.924776563154472,False
2204,"def reset(self): logger.debug('Arctic.reset()') with self._lock: if self.__conn is not None: self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): <IF_STMT> logger.debug('Library reset() %s' % l) l._reset()","if hasattr(l, '_reset') and callable(l._reset):",if l.is_alive():,0.7102686287711428,0.8590888738245122,False
2205,"def add_cand_to_check(cands): for cand in cands: x = cand.creator <IF_STMT> continue if x not in fan_out: heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",if x is None:,if x is None:,0.8762684819202554,0.7709002428237395,True
2206,"def on_task_modify(self, task, config): for entry in task.entries: <IF_STMT> size = entry['torrent'].size / 1024 / 1024 log.debug('%s size: %s MB' % (entry['title'], size)) entry['content_size'] = size",if 'torrent' in entry:,if 'torrent' in entry and 'size' in entry:,0.8227525781718714,0.6952219386678455,False
2207,"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] if self.do_manders: results += ['Manders'] if self.do_rwc: results += ['RWC'] <IF_STMT> results += ['Costes'] return results return []",if self.do_costes:,if self.do_costes:,0.9638710200696547,0.9298663600557577,True
2208,"def create_root(cls, site=None, title='Root', request=None, **kwargs): if not site: site = Site.objects.get_current() root_nodes = cls.objects.root_nodes().filter(site=site) if not root_nodes: article = Article() revision = ArticleRevision(title=title, **kwargs) <IF_STMT> revision.set_from_request(request) article.add_revision(revision, save=True) article.save() root = cls.objects.create(site=site, article=article) article.add_object_relation(root) else: root = root_nodes[0] return root",if request:,if request:,0.7961383007752711,0.9122561819614461,True
2209,"def get(self, key): filename = self._get_filename(key) try: with open(filename, 'rb') as f: pickle_time = pickle.load(f) <IF_STMT> return pickle.load(f) else: os.remove(filename) return None except (IOError, OSError, pickle.PickleError): return None",if pickle_time == 0 or pickle_time >= time():,if pickle_time < self.max_age:,0.7836095510192942,0.8105932471967202,False
2210,"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): <IF_STMT> self.add_body(message, value) elif name == 'EMAIL_ATTACHMENT': self.add_attachment(message, value) else: self.set_option(message, name, value, target) return message",if name == 'EMAIL_BODY':,if name == 'BODY':,0.8826306665571422,0.8105932471967202,False
2211,"def updateVar(name, data, mode=None): if mode: <IF_STMT> core.config.globalVariables[name].append(data) elif mode == 'add': core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",if mode == 'append':,if mode == 'add':,0.8302662129052603,0.693395566222006,False
2212,"def insert_errors(el, errors, form_id=None, form_index=None, error_class='error', error_creator=default_error_creator): el = _find_form(el, form_id=form_id, form_index=form_index) for name, error in errors.items(): <IF_STMT> continue for error_el, message in _find_elements_for_name(el, name, error): assert isinstance(message, (basestring, type(None), ElementBase)), 'Bad message: %r' % message _insert_error(error_el, message, error_class, error_creator)",if error is None:,if name == 'error':,0.8789623466224611,0.8661072626070159,False
2213,"def read(self, item, recursive=False, sort=False): item = _normalize_path(item) if item in self._store: <IF_STMT> del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) else: return self._read_dir(item, recursive=recursive, sort=sort)",if item in self._expire_time and self._expire_time[item] < datetime.now():,if recursive:,0.6368806723610189,0.8531413606256201,False
2214,"def _stash_splitter(states): keep, split = ([], []) if state_func is not None: for s in states: ns = state_func(s) if isinstance(ns, SimState): split.append(ns) <IF_STMT> split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return (keep, split)","elif isinstance(ns, (list, tuple, set)):","elif isinstance(ns, list):",0.938523784651566,0.9022045190074797,False
2215,"def run(self): while self.runflag: <IF_STMT> with self.lock: tasks = list(self.queue) self.queue.clear() while len(tasks) > 0: pathname, remotepath = tasks.pop(0) self.bcloud_app.upload_page.add_bg_task(pathname, remotepath) self.last = time() else: sleep(1)",if time() - self.last > 5 and self.qsize() > 0:,if self.last - self.last > self.max_run_time:,0.8742942549269476,0.7424213297217366,False
2216,"def _append_patch(self, patch_dir, patch_files): for patch in patch_files: <IF_STMT> tmp = patch patch = {} for key in tmp.keys(): patch[os.path.join(patch_dir, key)] = tmp[key] self.patches.append(patch) else: self.patches.append(os.path.join(patch_dir, patch))",if type(patch) is dict:,"if isinstance(patch, dict):",0.781083339432022,0.833078701050083,False
2217,"def __remote_port(self): port = 22 if self.git_has_remote: m = re.match('^(.*?)?@([^/:]*):?([0-9]+)?', self.git_remote.url) if m: <IF_STMT> port = m.group(3) return int(port)",if m.group(3):,if m.group(1) == '127.0.0.1':,0.8471622427591206,0.7245511487202049,False
2218,"def _create_or_get_helper(self, infer_mode: Optional[bool]=None, **kwargs) -> Helper: prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) <IF_STMT> self._train_helper = helper elif not is_training and self._infer_helper is None: self._infer_helper = helper return helper",if is_training and self._train_helper is None:,if is_training and self._train_helper is None:,0.8628190570111989,0.8732949145753399,True
2219,"def flushChangeClassifications(self, schedulerid, less_than=None): if less_than is not None: classifications = self.classifications.setdefault(schedulerid, {}) for changeid in list(classifications): <IF_STMT> del classifications[changeid] else: self.classifications[schedulerid] = {} return defer.succeed(None)",if changeid < less_than:,if less_than < changeid:,0.8523605228838118,0.7965020533851944,False
2220,"def pid_from_name(name): processes = [] for pid in os.listdir('/proc'): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) if name in pname: return pid <IF_STMT> return pid except: pass raise ProcessException('No process with such name: %s' % name)","if name in cmdline.split(' ', 1)[0]:",elif cmdline == 'pid':,0.9088108552792172,0.8555308664663046,False
2221,"def spew(): seenUID = False start() for part in query: <IF_STMT> seenUID = True if part.type == 'body': yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, 'spew_' + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and (not seenUID): space() yield self.spew_uid(id, msg, write, flush) finish() flush()",if part.type == 'uid':,if uid:,0.9363925090418388,0.9350761925543661,False
2222,def rx(): while True: rx_i = rep.recv() <IF_STMT> rep.send(b'done') break rep.send(b'i'),if rx_i == b'1000':,if rx_i == b'':,0.551895172312741,0.5316967153331754,False
2223,"def test_search_incorrect_base_exception_1(self): self.connection_1c.bind() try: result = self.connection_1c.search('o=nonexistant', '(cn=*)', search_scope=SUBTREE, attributes=['cn', 'sn']) <IF_STMT> _, result = self.connection_1c.get_response(result) self.fail('exception not raised') except LDAPNoSuchObjectResult: pass",if not self.connection_1c.strategy.sync:,if result:,0.6271679848460217,0.8318180062062374,False
2224,"def value_from_datadict(self, data, files, prefix): count = int(data['%s-count' % prefix]) values_with_indexes = [] for i in range(0, count): <IF_STMT> continue values_with_indexes.append((int(data['%s-%d-order' % (prefix, i)]), self.child_block.value_from_datadict(data, files, '%s-%d-value' % (prefix, i)))) values_with_indexes.sort() return [v for i, v in values_with_indexes]","if data['%s-%d-deleted' % (prefix, i)]:",if i >= count:,0.734461890550304,0.8592377270804451,False
2225,"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error('# channels not specified') if not self._sampwidth: raise Error('sample width not specified') <IF_STMT> raise Error('sampling rate not specified') self._write_header(datasize)",if not self._framerate:,if not self._samprate:,0.8639992233832658,0.8446593249975184,False
2226,def wait_til_ready(cls): while True: now = time.time() next_iteration = now // 1.0 + 1 <IF_STMT> break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0),if cls.connector.ready:,if next_iteration >= cls._max_til_ready_iterations:,0.9051786379568624,0.7498810286408993,False
2227,"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: <IF_STMT> if resp.match(key[:-1], val): break elif not resp.match(key, val): break else: actions[action] = opts return actions",if key[-1] == '!':,if key.endswith('='):,0.9330129533235614,0.9024521756077707,False
2228,"def close(self, wait=True, abort=False): """"""Close the socket connection."""""" if not self.closed and (not self.closing): self.closing = True self.server._trigger_event('disconnect', self.sid, run_async=False) if not abort: self.send(packet.Packet(packet.CLOSE)) self.closed = True self.queue.put(None) <IF_STMT> self.queue.join()",if wait:,if wait:,0.9445866765220818,0.9060643337943414,True
2229,"def model_parse(self): for name, submodel in self.model.named_modules(): for op_type in SUPPORTED_OP_TYPE: <IF_STMT> self.target_layer[name] = submodel self.already_pruned[name] = 0","if isinstance(submodel, op_type):",if op_type in submodel.named_modules:,0.8371956104508438,0.7098232254187811,False
2230,"def pack_identifier(self): """"""Return a combined identifier for the whole pack if this has more than one episode."""""" if self.id_type == 'ep': <IF_STMT> return 'S%02dE%02d-E%02d' % (self.season, self.episode, self.episode + self.episodes - 1) else: return self.identifier else: return self.identifier",if self.episodes > 1:,if self.episode < self.episodes - 1:,0.5966848880122756,0.8177978265964414,False
2231,"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode('utf-8') <IF_STMT> v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",if type(v) == unicode:,elif type(v) == unicode:,0.912749909301808,0.8592377270804451,False
2232,"def _enable_contours_changed(self, value): """"""Turns on and off the contours."""""" if self.module_manager is None: return if value: self.actor.inputs = [self.contour] <IF_STMT> self.actor.mapper.scalar_mode = 'use_cell_data' else: self.actor.inputs = [self.grid_plane] self.actor.mapper.scalar_mode = 'default' self.render()",if self.contour.filled_contours:,if self.grid_plane is None:,0.8877278782223494,0.828399516355805,False
2233,"def _apply_abs_paths(data, script_dir): for flag_data in data.values(): <IF_STMT> continue default = flag_data.get('default') if not default or not isinstance(default, six.string_types) or os.path.sep not in default: continue abs_path = os.path.join(script_dir, default) if os.path.exists(abs_path): flag_data['default'] = abs_path","if not isinstance(flag_data, dict):",if flag_data['type'] != 'flag':,0.8505184795905621,0.8431339019329497,False
2234,"def button_release(self, mapper): self.pressed = False if self.waiting_task and self.active is None and (not self.action): mapper.cancel_task(self.waiting_task) self.waiting_task = None <IF_STMT> self.normalaction.button_press(mapper) mapper.schedule(0.02, self.normalaction.button_release) elif self.active: self.active.button_release(mapper) self.active = None",if self.normalaction:,if self.normalaction:,0.8854850176542803,0.8743414417652072,True
2235,"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break <IF_STMT> p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",elif p:,elif p:,0.9294214161434952,0.9220450449751959,True
2236,"def status(self, name, error='No matching script logs found'): with self.script_lock: if self.script_running and self.script_running[1] == name: return self.script_running[1:] <IF_STMT> return self.script_last[1:] else: raise ValueError(error)",elif self.script_last and self.script_last[1] == name:,elif self.script_last and self.script_last[1] == name:,0.7444188943715246,0.7232927766551176,True
2237,"def _stderr_supports_color(): try: if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty(): <IF_STMT> curses.setupterm() if curses.tigetnum('colors') > 0: return True elif colorama: if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr', object()): return True except Exception: pass return False",if curses:,if curses:,0.7988155237938951,0.8827916928185874,True
2238,"def main(): configFilename = 'twitterbot.ini' if sys.argv[1:]: configFilename = sys.argv[1] try: <IF_STMT> raise Exception() load_config(configFilename) except Exception as e: print('Error while loading ini file %s' % configFilename, file=sys.stderr) print(e, file=sys.stderr) print(__doc__, file=sys.stderr) sys.exit(1) bot = TwitterBot(configFilename) return bot.run()",if not os.path.exists(configFilename):,if not os.path.exists(configFilename):,0.7028318571096425,0.8815741981066073,True
2239,def safe_to_kill(request): if os.path.exists(DRAIN_FILE): with open(DRAIN_FILE) as f: dt = datetime.datetime.fromtimestamp(float(f.read())) delta = datetime.datetime.now() - dt <IF_STMT> return Response(status_int=200) else: return Response(status_int=400) else: return Response(status_int=400),if delta.seconds > 2:,if delta < 0:,0.6343901146333588,0.7886336751695258,False
2240,"def get_class_name(item): class_name, module_name = (None, None) for parent in reversed(item.listchain()): <IF_STMT> class_name = parent.name elif isinstance(parent, pytest.Module): module_name = parent.module.__name__ break if class_name and '.tasks.' not in module_name: return '{}.{}'.format(module_name, class_name) else: return module_name","if isinstance(parent, pytest.Class):","if isinstance(parent, pytest.Class):",0.8949504983178527,0.8713933650206428,True
2241,"def getAllFitsLite(): fits = eos.db.getFitListLite() shipMap = {f.shipID: None for f in fits} for shipID in shipMap: ship = eos.db.getItem(shipID) <IF_STMT> shipMap[shipID] = (ship.name, ship.getShortName()) fitsToPurge = set() for fit in fits: try: fit.shipName, fit.shipNameShort = shipMap[fit.shipID] except (KeyError, TypeError): fitsToPurge.add(fit) for fit in fitsToPurge: fits.remove(fit) return fits",if ship is not None:,if ship:,0.8290054219573741,0.9253742688467129,False
2242,"def _process(self, event_data): self.machine.callbacks(self.machine.prepare_event, event_data) _LOGGER.debug('%sExecuted machine preparation callbacks before conditions.', self.machine.name) try: for trans in self.transitions[event_data.state.name]: event_data.transition = trans <IF_STMT> event_data.result = True break except Exception as err: event_data.error = err raise finally: self.machine.callbacks(self.machine.finalize_event, event_data) _LOGGER.debug('%sExecuted machine finalize callbacks', self.machine.name) return event_data.result",if trans.execute(event_data):,if event_data.transition.state == self.STATE_DONE:,0.6908548117228864,0.8752376177722327,False
2243,"def fetch_comments(self, force=False, limit=None): comments = [] if force is True or self.badges['comments'] > 0: query_params = {'filter': 'commentCard,copyCommentCard'} <IF_STMT> query_params['limit'] = limit comments = self.client.fetch_json('/cards/' + self.id + '/actions', query_params=query_params) return sorted(comments, key=lambda comment: comment['date']) return comments",if limit is not None:,if limit is not None:,0.6807147314887849,0.8380055871435848,True
2244,"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: <IF_STMT> continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if not self._is_expression():,if end == last_end:,0.934892874719609,0.8856327184319047,False
2245,"def _replace_home(x): if xp.ON_WINDOWS: home = builtins.__xonsh__.env['HOMEDRIVE'] + builtins.__xonsh__.env['HOMEPATH'][0] <IF_STMT> x = x.replace(home, '~', 1) if builtins.__xonsh__.env.get('FORCE_POSIX_PATHS'): x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env['HOME'] if x.startswith(home): x = x.replace(home, '~', 1) return x",if x.startswith(home):,if x.startswith(home):,0.6682830972941727,0.8996480074924822,True
2246,"def project_review(plans): for plan in plans: print('Inspecting {} plan'.format(plan)) branches = get_branches_from_plan(plan) for branch in branches: build_results = get_results_from_branch(branch) for build in build_results: build_key = build.get('buildResultKey') or None print('Inspecting build - {}'.format(build_key)) <IF_STMT> for status in STATUS_CLEANED_RESULTS: remove_build_result(build_key=build_key, status=status)",if build_key:,if build_key:,0.8735502129628854,0.9076141716697395,True
2247,"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x['description'] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in(('metadata', 'batch'), x) if batches: <IF_STMT> batches = [batches] for batch in batches: if batch in names: dups.add(batch) if len(dups) > 0: raise ValueError('Batch names must be unique from sample descriptions.\nClashing batch names: %s' % sorted(list(dups)))","if not isinstance(batches, (list, tuple)):","if not isinstance(batches, list):",0.9436715476556496,0.9155272930874561,False
2248,"def _check_signal(self): """"""Checks if a signal was received and issues a message."""""" proc_signal = getattr(self.proc, 'signal', None) if proc_signal is None: return sig, core = proc_signal sig_str = SIGNAL_MESSAGES.get(sig) if sig_str: if core: sig_str += ' (core dumped)' print(sig_str, file=sys.stderr) <IF_STMT> self.errors += sig_str + '\n'",if self.errors is not None:,if self.errors:,0.9310268709349113,0.9220450449751959,False
2249,"def loadLabelFile(self, labelpath): labeldict = {} if not os.path.exists(labelpath): f = open(labelpath, 'w', encoding='utf-8') else: with open(labelpath, 'r', encoding='utf-8') as f: data = f.readlines() for each in data: file, label = each.split('\t') <IF_STMT> label = label.replace('false', 'False') label = label.replace('true', 'True') labeldict[file] = eval(label) else: labeldict[file] = [] return labeldict",if label:,if self.isBoolean(label):,0.8084187160439684,0.9284304001296656,False
2250,"def exists_col_to_many(self, select_columns: List[str]) -> bool: for column in select_columns: <IF_STMT> root_relation = get_column_root_relation(column) if self.is_relation_many_to_many(root_relation) or self.is_relation_one_to_many(root_relation): return True return False",if is_column_dotted(column):,if column.lower() == 'id':,0.7549366135306426,0.760856626273165,False
2251,"def check_sequence_matches(seq, template): i = 0 for pattern in template: <IF_STMT> pattern = {pattern} got = set(seq[i:i + len(pattern)]) assert got == pattern i += len(got)","if not isinstance(pattern, set):","if not isinstance(pattern, dict):",0.8762314242598004,0.7965020533851944,False
2252,"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue <IF_STMT> for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()","if hasattr(module, 'aliases'):","if hasattr(module, 'aliases'):",0.9449439819965133,0.9210921090251244,True
2253,"def result(): R, V = (rays, virtual_rays) if V is not None: <IF_STMT> V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() if len(V) != d - R.dim() or (R + V).dim() != d: raise ValueError('virtual rays must be linearly independent and with other rays span the ambient space.') return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if normalize:,if normalize:,0.9496534284824684,0.9434724611166208,True
2254,"def communicate(self, _input=None, _timeout=None) -> Tuple[bytes, bytes]: if parse_args().print_commands: <IF_STMT> print_stderr(color_line('=> ', 14) + ' '.join((str(arg) for arg in self.args))) stdout, stderr = super().communicate(_input, _timeout) self.stdout_text = stdout.decode('utf-8') if stdout else None self.stderr_text = stderr.decode('utf-8') if stderr else None return (stdout, stderr)",if self.args != get_sudo_refresh_command():,if self.args:,0.9362354763963383,0.9144061946646023,False
2255,"def convert(data): result = [] for d in data: <IF_STMT> result.append((d[0], None, d[1])) elif isinstance(d, basestring): result.append(d) return result","if isinstance(d, tuple) and len(d) == 2:","if isinstance(d, tuple):",0.8436552315150958,0.761827408333416,False
2256,"def validate(self, value): try: value = [datetime.datetime.strptime(range, '%Y-%m-%d %H:%M:%S') for range in value.split(' to ')] <IF_STMT> return True else: return False except ValueError: return False",if len(value) == 2 and value[0] <= value[1]:,if len(value) == 2:,0.5940288677968062,0.7886336751695258,False
2257,"def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return for f in os.listdir(dirname): if f in ('.', '..'): continue path = dirname + os.sep + f <IF_STMT> rmdir(path) else: os.unlink(path) os.rmdir(dirname)",if os.path.isdir(path):,if os.path.isdir(path):,0.9307261215344732,0.8935248372106969,True
2258,"def onCompletion(self, text): res = [] for l in text.split('\n'): if not l: continue l = l.split(':') <IF_STMT> continue res.append([l[0].strip(), l[1].strip()]) self.panel.setSlides(res)",if len(l) != 2:,if len(l) != 2:,0.7152864225721822,0.760856626273165,True
2259,def pytest_collection_modifyitems(items): for item in items: <IF_STMT> if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) if 'init' not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123)),if item.nodeid.startswith('tests/infer'):,if 'unit' in item.keywords:,0.8480529555168487,0.7245511487202049,False
2260,"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): if name == 'EMAIL_BODY': self.add_body(message, value) <IF_STMT> self.add_attachment(message, value) else: self.set_option(message, name, value, target) return message",elif name == 'EMAIL_ATTACHMENT':,elif name == 'EMAIL_ATTACHMENT':,0.8884446200455143,0.8105932471967202,True
2261,def extend_with_zeroes(b): try: for x in b: x = to_constant(x) <IF_STMT> yield x else: yield 0 for _ in range(32): yield 0 except Exception as e: return,"if isinstance(x, int):",if x != 0:,0.6238818465826264,0.8038019482772603,False
2262,"def _start_cluster(*, cleanup_atexit=True): global _default_cluster if _default_cluster is None: cluster_addr = os.environ.get('EDGEDB_TEST_CLUSTER_ADDR') <IF_STMT> conn_spec = json.loads(cluster_addr) _default_cluster = edgedb_cluster.RunningCluster(**conn_spec) else: data_dir = os.environ.get('EDGEDB_TEST_DATA_DIR') _default_cluster = _init_cluster(data_dir=data_dir, cleanup_atexit=cleanup_atexit) return _default_cluster",if cluster_addr:,if cluster_addr:,0.7774273231377654,0.8743414417652072,True
2263,"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() <IF_STMT> break line = list(enwik9_norm_transform([line]))[0] if line != ' ' and line != '': if line[0] == ' ': line = line[1:] f2.writelines(line + '\n')",if not line:,if not line:,0.9293891574168571,0.8953711787948615,True
2264,"def is_entirely_italic(line): style = subs.styles.get(line.style, SSAStyle.DEFAULT_STYLE) for fragment, sty in parse_tags(line.text, style, subs.styles): fragment = fragment.replace('\\h', ' ') fragment = fragment.replace('\\n', '\n') fragment = fragment.replace('\\N', '\n') <IF_STMT> return False return True",if not sty.italic and fragment and (not fragment.isspace()):,if fragment.startswith('#') or fragment.startswith('#') or fragment.startswith('#'):,0.8611024020814869,0.7803242766062389,False
2265,def __get_all_nodes(self): nodes = [] next_level = [self.__tree.get_root()] while len(next_level) != 0: cur_level = next_level nodes += next_level next_level = [] for cur_node in cur_level: children = cur_node.get_children() <IF_STMT> next_level += children return nodes,if children is not None:,if children:,0.7822922069170608,0.8935248372106969,False
2266,"def _openvpn_stdout(self): while True: line = self.process.stdout.readline() if not line: <IF_STMT> return time.sleep(0.05) continue yield try: self.server.output.push_output(line) except: logger.exception('Failed to push vpn output', 'server', server_id=self.server.id) yield",if self.process.poll() is not None or self.is_interrupted():,if self.server.output.is_running():,0.5645519434407957,0.8590888738245122,False
2267,"def payment_received_handler(event): if isinstance(event.message.action, types.MessageActionPaymentSentMe): payment: types.MessageActionPaymentSentMe = event.message.action if payment.payload.decode('UTF-8') == 'product A': await bot.send_message(event.message.from_id, 'Thank you for buying product A!') <IF_STMT> await bot.send_message(event.message.from_id, 'Thank you for buying product B!') raise events.StopPropagation",elif payment.payload.decode('UTF-8') == 'product B':,elif payment.payload.decode('UTF-8') == 'product B':,0.8842123211845001,0.8148691130388024,True
2268,"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer <IF_STMT> return LintProblem(token.start_mark.line + 1, next.start_mark.column, max_desc) elif min != -1 and spaces < min: return LintProblem(token.start_mark.line + 1, next.start_mark.column + 1, min_desc)",if max != -1 and spaces > max:,if max != -1 and spaces > max:,0.650406764305572,0.8108704755314419,True
2269,"def seek_to_block(self, pos): baseofs = 0 ofs = 0 for b in self.blocks: <IF_STMT> self.current_block = b break baseofs += b.compressed_size ofs += b.uncompressed_size else: self.current_block = None self.current_stream = BytesIO(b'') return self.current_block_start = ofs self.stream.seek(self.basepos + baseofs) buf = BytesIO(self.stream.read(self.current_block.compressed_size)) self.current_stream = self.current_block.decompress(buf)",if ofs + b.uncompressed_size > pos:,if b.offset == pos:,0.7450621761228096,0.8780099567239787,False
2270,"def rewrite_hunks(hunks): deltas = (hunk.b_length - hunk.a_length for hunk in hunks) offsets = accumulate(deltas, initial=0) for hunk, offset in zip(hunks, offsets): new_b = hunk.a_start + offset if hunk_of_additions_only(hunk): new_b += 1 <IF_STMT> new_b -= 1 yield hunk._replace(b_start=new_b)",elif hunk_of_removals_only(hunk):,elif hunk_of_removal_only(hunk):,0.7618211338557338,0.9024521756077707,False
2271,"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): <IF_STMT> if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if len(q) == 1:,"if isinstance(value, list):",0.9341565531539543,0.9000283069718913,False
2272,"def get_url(token, base_url): """"""Parse an <url> token."""""" if token.type == 'url': return _get_url_tuple(token.value, base_url) elif token.type == 'function': if token.name == 'attr': return check_attr_function(token, 'url') <IF_STMT> return _get_url_tuple(token.arguments[0].value, base_url)","elif token.name == 'url' and len(token.arguments) in (1, 2):",elif token.name == 'url_tuple':,0.8886624801474537,0.8169276475307028,False
2273,"def read(self, count): if self.closed: return self.upstream.read(count) try: while len(self.upstream) < count: <IF_STMT> with self.buf_in: self.transport.downstream_recv(self.buf_in) else: break return self.upstream.read(count) except: logger.debug(traceback.format_exc())",if self.buf_in or self._poll_read(10):,if self.transport:,0.891064271409906,0.8318180062062374,False
2274,"def get_timestamp_for_block(self, block_hash: HexBytes, max_tries: Optional[int]=10) -> int: counter = 0 block: AttributeDict = None if block_hash in self._block_cache.keys(): block = self._block_cache.get(block_hash) else: while block is None: <IF_STMT> raise ValueError(f'Block hash {block_hash.hex()} does not exist.') counter += 1 block = self._block_cache.get(block_hash) await asyncio.sleep(0.5) return block.get('timestamp')",if counter == max_tries:,if counter >= max_tries:,0.9353177647752987,0.8806615362338783,False
2275,"def reader(): batch_out = [] for video_name in self.video_list: video_idx = self.video_list.index(video_name) video_feat = self.load_file(video_name) batch_out.append((video_feat, video_idx)) <IF_STMT> yield batch_out batch_out = []",if len(batch_out) == self.batch_size:,if len(batch_out) == self.batch_size:,0.8060558326479543,0.7709002428237395,True
2276,"def cleanup(): gscript.message(_('Erasing temporary files...')) for temp_map, maptype in temp_maps: <IF_STMT> gscript.run_command('g.remove', flags='f', type=maptype, name=temp_map, quiet=True)","if gscript.find_file(temp_map, element=maptype)['name']:",if not gscript.is_file(temp_map):,0.5979633996520792,0.7160350546947921,False
2277,"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() <IF_STMT> self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) elif self._flow_type == REALTIME: try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",if self._has_stop_signal(raw_inputs):,if self._flow_type == STOP:,0.6133014015056428,0.8336104423443033,False
2278,"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('mw', 'hi', 'corr', 'trunc'): sent += [self.handle_word(w) for w in child] <IF_STMT> sent.append(self.handle_word(child)) elif child.tag not in self.tags_to_ignore: raise ValueError('Unexpected element %s' % child.tag) return BNCSentence(elt.attrib['n'], sent)","elif child.tag in ('w', 'c'):","elif child.tag in ('mw', 'hi', 'corr', 'trunc'):",0.7716709104049347,0.8076797778670548,False
2279,"def bind_subscribers_to_graphql_type(self, graphql_type): for field, subscriber in self._subscribers.items(): <IF_STMT> raise ValueError('Field %s is not defined on type %s' % (field, self.name)) graphql_type.fields[field].subscribe = subscriber",if field not in graphql_type.fields:,if field not in graphql_type.fields:,0.7682805902721372,0.7506346798217074,True
2280,"def _get_from_json(self, *, name, version): url = urljoin(self.url, posixpath.join(name, str(version), 'json')) async with aiohttp_session(auth=self.auth) as session: async with session.get(url) as response: <IF_STMT> raise PackageNotFoundError(package=name, url=url) response.raise_for_status() response = await response.json() dist = response['info']['requires_dist'] or [] if dist: return dist return await self._get_from_files(response['urls'])",if response.status == 404:,if response['error']['code'] == 404:,0.8694217025143322,0.8723360571509826,False
2281,"def is_active(self): if not self.pk: log_level = get_setting('LOG_MISSING_SWITCHES') if log_level: logger.log(log_level, 'Switch %s not found', self.name) <IF_STMT> switch, _created = Switch.objects.get_or_create(name=self.name, defaults={'active': get_setting('SWITCH_DEFAULT')}) cache = get_cache() cache.set(self._cache_key(self.name), switch) return get_setting('SWITCH_DEFAULT') return self.active",if get_setting('CREATE_MISSING_SWITCHES'):,if self.active:,0.9281792399970376,0.8866029039778043,False
2282,"def add_requirements(self, requirements): if self._legacy: self._legacy.add_requirements(requirements) else: run_requires = self._data.setdefault('run_requires', []) always = None for entry in run_requires: <IF_STMT> always = entry break if always is None: always = {'requires': requirements} run_requires.insert(0, always) else: rset = set(always['requires']) | set(requirements) always['requires'] = sorted(rset)",if 'environment' not in entry and 'extra' not in entry:,if entry['requires'] == requirements:,0.9289769369857928,0.8723360571509826,False
2283,"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] if check.message: message = f'{idx}. {check.message}' else: message = None example = cast(Case, check.example) display_example(example, check.name, message, result.seed) <IF_STMT> click.echo('\n')",if idx != len(checks):,if message:,0.9141243131396333,0.9392869946574155,False
2284,"def __call__(self, frame: FrameType, event: str, arg: Any) -> 'CallTracer': code = frame.f_code if event not in SUPPORTED_EVENTS or code.co_name == 'trace_types' or (self.should_trace and (not self.should_trace(code))): return self try: if event == EVENT_CALL: self.handle_call(frame) <IF_STMT> self.handle_return(frame, arg) else: logger.error('Cannot handle event %s', event) except Exception: logger.exception('Failed collecting trace') return self",elif event == EVENT_RETURN:,elif event == EVENT_RETURN:,0.9300692070304816,0.8944264839442453,True
2285,"def get_maps(test): pages = set() for addr in test['pre']['memory'].keys(): pages.add(addr >> 12) for addr in test['pos']['memory'].keys(): pages.add(addr >> 12) maps = [] for p in sorted(pages): <IF_STMT> maps[-1] = (maps[-1][0], maps[-1][1] + 4096) else: maps.append((p << 12, 4096)) return maps",if len(maps) > 0 and maps[-1][0] + maps[-1][1] == p << 12:,if p == 0:,0.8971998548452427,0.8661072626070159,False
2286,"def process_rotate_aes_key(self): if hasattr(self.options, 'rotate_aes_key') and isinstance(self.options.rotate_aes_key, six.string_types): <IF_STMT> self.options.rotate_aes_key = True elif self.options.rotate_aes_key.lower() == 'false': self.options.rotate_aes_key = False",if self.options.rotate_aes_key.lower() == 'true':,if self.options.rotate_aes_key.lower() == 'true':,0.7472689004887285,0.7245511487202049,True
2287,"def apply_figure(self, figure): super(legend_text_legend, self).apply_figure(figure) properties = self.properties.copy() with suppress(KeyError): del properties['margin'] with suppress(KeyError): texts = figure._themeable['legend_text_legend'] for text in texts: <IF_STMT> text = text._text text.set(**properties)","if not hasattr(text, '_x'):","if hasattr(text, '_text'):",0.8949110352935823,0.8266114125804572,False
2288,"def tearDown(self): for i in range(len(self.tree) - 1, -1, -1): s = os.path.join(self.root, self.tree[i]) <IF_STMT> os.rmdir(s) else: os.remove(s) os.rmdir(self.root)",if not '.' in s:,if os.path.isdir(s):,0.7345755030310215,0.803154665668484,False
2289,"def _get_id(self, type, id): fields = id.split(':') if len(fields) >= 3: if type != fields[-2]: logger.warning('Expected id of type %s but found type %s %s', type, fields[-2], id) return fields[-1] fields = id.split('/') if len(fields) >= 3: itype = fields[-2] <IF_STMT> logger.warning('Expected id of type %s but found type %s %s', type, itype, id) return fields[-1].split('?')[0] return id",if type != itype:,if itype != fields[-1]:,0.9233028972302014,0.9069443196104878,False
2290,"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: <IF_STMT> yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if matchSelf:,if s.siblingAbove is None:,0.9435374009708557,0.8902056737869248,False
2291,"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource('dynamodb').Table('Sphere11.Dev.ResourceLocks') results = table.scan() for r in results['Items']: if 'LockDate' in r: r['LockDate'] = datetime.fromtimestamp(r['LockDate']) <IF_STMT> r['RevisionDate'] = datetime.fromtimestamp(r['RevisionDate']) print(tabulate.tabulate(results['Items'], headers='keys', tablefmt='fancy_grid'))",if 'RevisionDate' in r:,if 'RevisionDate' in r:,0.8065316212843309,0.8336104423443033,True
2292,"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True err_msg = 'T2T: skipped importing {num_missing} data_generators modules.' print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) if log_all: print('Did not import module: %s; Cause: %s' % (module, err_str)) <IF_STMT> print('From module %s' % module) raise err","if not _is_import_err_msg(err_str, module):",if not module.startswith('.'):,0.8902326482901145,0.9167056528641923,False
2293,"def find_needle(self, tree, focused=None): if isinstance(tree, list): for el in tree: res = self.find_needle(el, focused) <IF_STMT> return res elif isinstance(tree, dict): nodes = tree.get('nodes', []) + tree.get('floating_nodes', []) if focused: for node in nodes: if node['id'] == focused['id']: return tree elif tree['focused']: return tree return self.find_needle(nodes, focused) return {}",if res:,if res:,0.9494160681395031,0.926934323706186,True
2294,"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = ['pixel_longitude', 'pixel_latitude'] for var_name, val in self.file_content.items(): if isinstance(val, netCDF4.Variable): ds_info = {'file_type': self.filetype_info['file_type'], 'resolution': res} <IF_STMT> ds_info['coordinates'] = coordinates yield (DatasetID(name=var_name, resolution=res), ds_info)",if not self.is_geo:,if coordinates:,0.8135579778065617,0.9051034981560222,False
2295,"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() <IF_STMT> sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') if end_slash >= 0: sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",if test_path.lower().startswith(parent_path.lower()):,if test_path.startswith(parent_path):,0.8850571326880339,0.9144061946646023,False
2296,"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: if hasattr(o, 'profile'): del o.profile <IF_STMT> del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path if hasattr(o, 'services_config'): del o.services_config return vars(o) except Exception as e: return str(o)","if hasattr(o, 'credentials'):","if hasattr(o, 'credentials'):",0.9032390542222911,0.8783650674919876,True
2297,"def submit(self, fn, *args, **kwargs): with self._shutdown_lock: <IF_STMT> raise RuntimeError('cannot schedule new futures after shutdown') f = _base.Future() w = _WorkItem(f, fn, args, kwargs) self._work_queue.put(w) self._adjust_thread_count() return f",if self._shutdown:,if self._shutdown:,0.8911627675370454,0.8696398662122882,True
2298,"def __viewerKeyPress(viewer, event): view = viewer.view() if not isinstance(view, GafferSceneUI.SceneView): return False if event == __editSourceKeyPress: selectedPath = __sceneViewSelectedPath(view) <IF_STMT> __editSourceNode(view.getContext(), view['in'], selectedPath) return True elif event == __editTweaksKeyPress: selectedPath = __sceneViewSelectedPath(view) if selectedPath is not None: __editTweaksNode(view.getContext(), view['in'], selectedPath) return True",if selectedPath is not None:,if selectedPath is not None:,0.9135201531866552,0.8527204701689132,True
2299,"def _split_to_option_groups_and_paths(self, args): opt_groups = [] current = [] for arg in args: <IF_STMT> opts = self._arg_parser.parse_args(current)[0] opt_groups.append(opts) current = [] else: current.append(arg) if opt_groups: return (opt_groups, current) raise ValueError('Nothing to split')","if arg.replace('-', '') == '' and len(arg) >= 3:",if arg == 'groups':,0.8356173596309332,0.8336104423443033,False
2300,"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): <IF_STMT> changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",if value:,if value:,0.7799358441665412,0.9237460349978159,True
2301,"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) if target_pid == pid: return except OSError: e = sys.exc_info()[1] <IF_STMT> return time.sleep(0.05) assert False, 'wait_for_child() timed out'",if e.args[0] == errno.ECHILD:,if e.errno == errno.EINTR:,0.7198862403227339,0.8516228624291206,False
2302,"def _get_os_version_lsb_release(): try: output = subprocess.check_output('lsb_release -sri', shell=True) lines = output.strip().split() name, version = lines <IF_STMT> version = '' return (name, version) except: return _get_os_version_uname()",if version.lower() == 'rolling':,if version == '':,0.726537439111491,0.7886336751695258,False
2303,"def _check_snapshot_status_healthy(self, snapshot_uuid): status = '' try: while True: status, locked = self._get_snapshot_status(snapshot_uuid) <IF_STMT> break eventlet.sleep(2) except Exception: with excutils.save_and_reraise_exception(): LOG.exception('Failed to get snapshot status. [%s]', snapshot_uuid) LOG.debug('Lun [%(snapshot)s], status [%(status)s].', {'snapshot': snapshot_uuid, 'status': status}) return status == 'Healthy'",if not locked:,if locked:,0.6396901227033054,0.9076141716697395,False
2304,"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 <IF_STMT> n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",if self.HasCloseButton():,if self.HasMinimizeButton():,0.9433250167472353,0.9202663016973823,False
2305,"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: <IF_STMT> continue if not isinstance(key, bytes): key = text_type(key).encode(charset) if not isinstance(value, bytes): value = text_type(value).encode(charset) yield (_fast_url_quote_plus(key) + '=' + _fast_url_quote_plus(value))",if value is None:,if not encode_keys:,0.7576290594866992,0.9022045190074797,False
2306,"def get_response(self, exc_fmt=None): self.callback = None if __debug__: self.parent._log(3, '%s:%s.ready.wait' % (self.name, self.tag)) self.ready.wait() if self.aborted is not None: typ, val = self.aborted <IF_STMT> exc_fmt = '%s - %%s' % typ raise typ(exc_fmt % str(val)) return self.response",if exc_fmt is None:,if exc_fmt is None:,0.9171890377847206,0.8555308664663046,True
2307,"def extract_items(self): responses = self.fetch() items = [] for response in responses: page_key = response.meta.get('page_key') or response.url item = {'key': page_key, 'items': None, 'templates': None} extracted_items = [dict(i) for i in self.spider.parse(response) if not isinstance(i, Request)] <IF_STMT> item['items'] = extracted_items item['templates'] = [i['_template'] for i in extracted_items if i.get('_template')] items.append(item) return items",if extracted_items:,if item['key'] == page_key:,0.8293009387674046,0.8964173245779284,False
2308,"def fit_one(self, x): for i, xi in x.items(): if self.with_centering: self.median[i].update(xi) <IF_STMT> self.iqr[i].update(xi) return self",if self.with_scaling:,elif self.with_iqr:,0.8479641019525598,0.7447819789879647,False
2309,"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True <IF_STMT> left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",elif not self.word_boundary_char(text[left - 1]):,elif not self.word_boundary_char(text[left]):,0.795919569491767,0.9210921090251244,False
2310,def _validate_duplicate_detection_history_time_window(namespace): if namespace.duplicate_detection_history_time_window: if iso8601pattern.match(namespace.duplicate_detection_history_time_window): pass <IF_STMT> pass else: raise CLIError('--duplicate-detection-history-time-window Value Error : {0} value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min'.format(namespace.duplicate_detection_history_time_window)),elif timedeltapattern.match(namespace.duplicate_detection_history_time_window):,elif durationpattern.match(namespace.duplicate_detection_history_time_window):,0.6472073024556917,0.9099951253570094,False
2311,"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') if end_slash >= 0: sub = sub[:end_slash] <IF_STMT> continue subkeys.append(sub) return subkeys",if not sub:,if sub == test_path:,0.7629155525102851,0.8692960007731574,False
2312,"def generator(self, data): <IF_STMT> silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: if var in silent_vars: continue yield (0, [int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val)])",if self._config.SILENT:,if self._config.SILENT:,0.9258673987207655,0.8787142254774354,True
2313,"def start_requests(self): if self.fail_before_yield: 1 / 0 for s in range(100): qargs = {'total': 10, 'seed': s} url = self.mockserver.url('/follow?%s') % urlencode(qargs, doseq=1) yield Request(url, meta={'seed': s}) <IF_STMT> 2 / 0 assert self.seedsseen, 'All start requests consumed before any download happened'",if self.fail_yielding:,if self.fail_before_yield:,0.730826570022364,0.9122561819614461,False
2314,"def populateGridlines(self): cTicks = self.getSystemCurve(self.ticksId) cGridlines = self.getSystemCurve(self.gridlinesId) cGridlines.clearPoints() nTicks = cTicks.getNPoints() for iTick in range(nTicks): <IF_STMT> p = cTicks.getPoint(iTick) cGridlines.addPoint(p.getX(), p.getY())",if self.hasGridlines and iTick % self.ticksPerGridline == 0:,if cGridlines.getPoint(iTick) is not None:,0.8465277078238589,0.7297349727547102,False
2315,"def handle_before_events(request, event_list): if not event_list: return '' if not hasattr(event_list, '__iter__'): project = event_list.project event_list = [event_list] else: projects = set((e.project for e in event_list)) <IF_STMT> project = projects.pop() else: project = None for plugin in plugins.for_project(project): safe_execute(plugin.before_events, request, event_list) return ''",if len(projects) == 1:,if projects:,0.7896594267587306,0.9164531641034833,False
2316,"def handle_parse_result(self, ctx, opts, args): if self.name in opts: <IF_STMT> self._raise_exclusive_error() if self.multiple and len(set(opts[self.name])) > 1: self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self.mutually_exclusive.intersection(opts):,if self.multiple and len(set(opts[self.name])) > 1:,0.8657687090013895,0.7126109689973791,False
2317,"def current_word(cursor_offset, line): """"""the object.attribute.attribute just before or under the cursor"""""" pos = cursor_offset start = pos end = pos word = None for m in current_word_re.finditer(line): <IF_STMT> start = m.start(1) end = m.end(1) word = m.group(1) if word is None: return None return LinePart(start, end, word)",if m.start(1) < pos and m.end(1) >= pos:,if m.start(1) <= start <= end:,0.8397657151206719,0.8505405945991492,False
2318,"def query_to_script_path(path, query): if path != '*': script = os.path.join(path, query.split(' ')[0]) <IF_STMT> raise IOError(""Script '{}' not found in script directory"".format(query)) return os.path.join(path, query).split(' ') return query",if not os.path.exists(script):,if not os.path.exists(script):,0.7869230349531517,0.833078701050083,True
2319,"def expand(self, pbegin): identity_map = 'Identity' >> beam.Map(lambda x: x) if self._dataset_key.is_flattened_dataset_key(): <IF_STMT> return self._flat_pcollection | identity_map else: return list(self._pcollection_dict.values()) | 'FlattenAnalysisInputs' >> beam.Flatten(pipeline=pbegin.pipeline) else: return self._pcollection_dict[self._dataset_key] | identity_map",if self._flat_pcollection:,if self._flat_pcollection:,0.6002086733887976,0.8743414417652072,True
2320,"def processCoords(coords): newcoords = deque() for x, y, z in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) if p not in box: continue nx, ny, nz = p <IF_STMT> level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords","if level.blockAt(nx, ny, nz) == 0:",if nx != ny:,0.75199530997409,0.9036816878108535,False
2321,"def delete_byfilter(userId, remove=True, session=None, **dbfilter): if not session: session = db.Session ret = False results = session.query(ObjectStorageMetadata).filter_by(**dbfilter) if results: for result in results: <IF_STMT> session.delete(result) else: result.update({'record_state_key': 'to_delete', 'record_state_val': str(time.time())}) ret = True return ret",if remove:,if remove:,0.6801784584081043,0.8966773400768917,True
2322,"def fields(self, fields): fields_xml = '' for field in fields: field_dict = DEFAULT_FIELD.copy() field_dict.update(field) <IF_STMT> field_dict['required'] = 'true' fields_xml += FIELD_XML_TEMPLATE % field_dict + '\n' self.xml = force_unicode(force_unicode(self.xml).replace(u'<!-- REPLACE FIELDS -->', force_unicode(fields_xml)))",if self.unique_key_field == field['name']:,if field.required:,0.9020417120822043,0.8866029039778043,False
2323,"def get_all_users(self, access_token, timeout=None): if timeout is None: timeout = DEFAULT_TIMEOUT headers = self.retrieve_header(access_token) try: response = await self.standard_request('get', '/walkoff/api/users', timeout=DEFAULT_TIMEOUT, headers=headers) <IF_STMT> resp = await response.json() return (resp, 'Success') else: return 'Invalid Credentials' except asyncio.CancelledError: return (False, 'TimedOut')",if response.status == 200:,if response.status == 200:,0.672159258094663,0.8627586293513119,True
2324,"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] if value and row[0] == value: break <IF_STMT> idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",if idx == len(os_widget.get_model()) - 1:,if idx == len(model):,0.9178567289583461,0.8661072626070159,False
2325,"def translate_module_name(module: str, relative: int) -> Tuple[str, int]: for pkg in VENDOR_PACKAGES: for alt in ('six.moves', 'six'): substr = '{}.{}'.format(pkg, alt) if module.endswith('.' + substr) or (module == substr and relative): return (alt, 0) <IF_STMT> return (alt + '.' + module.partition('.' + substr + '.')[2], 0) return (module, relative)",if '.' + substr + '.' in module:,elif module.startswith('.' + substr):,0.8234376681524083,0.8902056737869248,False
2326,"def escape(m): all, tail = m.group(0, 1) assert all.startswith('\\') esc = simple_escapes.get(tail) if esc is not None: return esc if tail.startswith('x'): hexes = tail[1:] <IF_STMT> raise ValueError(""invalid hex string escape ('\\%s')"" % tail) try: i = int(hexes, 16) except ValueError: raise ValueError(""invalid hex string escape ('\\%s')"" % tail) else: try: i = int(tail, 8) except ValueError: raise ValueError(""invalid octal string escape ('\\%s')"" % tail) return chr(i)",if len(hexes) < 2:,if hexes == '':,0.9370886289327972,0.9180466554652996,False
2327,"def __get_k8s_container_name(self, job_wrapper): raw_id = job_wrapper.job_destination.id if isinstance(raw_id, str): cleaned_id = re.sub('[^-a-z0-9]', '-', raw_id) <IF_STMT> cleaned_id = 'x%sx' % cleaned_id return cleaned_id return 'job-container'",if cleaned_id.startswith('-') or cleaned_id.endswith('-'):,"if not isinstance(cleaned_id, int):",0.7373599852956398,0.7801270245332924,False
2328,"def _power_exact(y, xc, yc, xe): yc, ye = (y.int, y.exp) while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 if ye < 0: return None exponent = xe * 10 ** ye <IF_STMT> xc = exponent else: xc = 0 return 5",if y and xe:,if xc == 1:,0.9690700737740128,0.9155272930874561,False
2329,"def lpush(key, *vals, **kwargs): ttl = kwargs.get('ttl') cap = kwargs.get('cap') if not ttl and (not cap): _client.lpush(key, *vals) else: pipe = _client.pipeline() pipe.lpush(key, *vals) <IF_STMT> pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",if cap:,if cap:,0.9368767704180561,0.8901732118131125,True
2330,"def render_headers(self) -> bytes: if not hasattr(self, '_headers'): parts = [b'Content-Disposition: form-data; ', format_form_param('name', self.name)] if self.filename: filename = format_form_param('filename', self.filename) parts.extend([b'; ', filename]) <IF_STMT> content_type = self.content_type.encode() parts.extend([b'\r\nContent-Type: ', content_type]) parts.append(b'\r\n\r\n') self._headers = b''.join(parts) return self._headers",if self.content_type is not None:,if self.content_type:,0.7071587344275602,0.9024521756077707,False
2331,"def validate_custom_field_data(field_type: int, field_data: ProfileFieldData) -> None: try: <IF_STMT> if len(field_data) < 1: raise JsonableError(_('Field must have at least one choice.')) validate_choice_field_data(field_data) elif field_type == CustomProfileField.EXTERNAL_ACCOUNT: validate_external_account_field_data(field_data) except ValidationError as error: raise JsonableError(error.message)",if field_type == CustomProfileField.CHOICE:,if field_type == CustomProfileField.CHOICE:,0.6360101766122689,0.8385130047130208,True
2332,"def get_data(self, path): """"""Gross hack to contort loader to deal w/ load_*()'s bad API."""""" if self.file and path == self.path: <IF_STMT> file = self.file else: self.file = file = open(self.path, 'r') with file: return file.read() else: return super().get_data(path)",if not self.file.closed:,"if isinstance(self.file, str):",0.7702022356668337,0.8815741981066073,False
2333,"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) <IF_STMT> self.transfer_finished = True return if self._data_wrapper is not None: chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",if not chunk:,if self.tot_bytes_received >= self.ac_in_buffer_size:,0.8044085123180843,0.8832000938217648,False
2334,"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): <IF_STMT> line = line.split(' ')[1].strip('\'""\r\n') if not ('<' in line or line in dep): dep.append(line) return [i for i in dep if os.path.exists(i)]",if line.startswith('#include') or line.startswith('%include'):,if line.startswith('dependency'):,0.931939395770392,0.8901732118131125,False
2335,"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): """"""Add data to be displayed in the buffer."""""" self.values.extend(lines) if scroll_end: if not self.editing: self.start_display_at = len(self.values) - len(self._my_widgets) <IF_STMT> self.start_display_at = len(self.values) - len(self._my_widgets)",elif scroll_if_editing:,if scroll_if_editing:,0.9166069305980269,0.8787142254774354,False
2336,"def test_getline(self): with tokenize.open(self.file_name) as fp: for index, line in enumerate(fp): <IF_STMT> line += '\n' cached_line = linecache.getline(self.file_name, index + 1) self.assertEqual(line, cached_line)",if not line.endswith('\n'):,if not line.endswith('\n'):,0.883903101025913,0.80377750806414,True
2337,"def selectRow(self, rowNumber, highlight=None): if rowNumber == 'h': rowNumber = 0 else: rowNumber = int(rowNumber) + 1 if 1 > rowNumber >= len(self.cells) + 1: raise Exception('Invalid row number.') else: selected = self.cells[rowNumber][0].selected for cell in self.cells[rowNumber]: <IF_STMT> if selected: cell.deselect() else: cell.select() elif highlight: cell.mouseEnter() else: cell.mouseLeave()",if highlight is None:,if cell.selected:,0.943518441096533,0.9253742688467129,False
2338,"def put(self, session): with sess_lock: self.parent.put(session) for sp in self.skip_paths: if request.path.startswith(sp): return <IF_STMT> try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",if session.sid in self._cache:,if session.sid in self._cache:,0.7189453328024888,0.7801270245332924,True
2339,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue <IF_STMT> self.add_doc_id(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:,if tt == 18:,0.9001195620339053,0.8555308664663046,True
2340,"def extract(self, zip): max_nb = maxNbFile(self) for index, field in enumerate(zip.array('file')): <IF_STMT> self.warning('ZIP archive contains many files, but only first %s files are processed' % max_nb) break self.processFile(field)",if max_nb is not None and max_nb <= index:,if index > max_nb:,0.7849354695082996,0.8105932471967202,False
2341,"def get_norm(norm, out_channels): if isinstance(norm, str): <IF_STMT> return None norm = {'BN': BatchNorm2d, 'GN': lambda channels: nn.GroupNorm(32, channels), 'nnSyncBN': nn.SyncBatchNorm, '': lambda x: x}[norm] return norm(out_channels)",if len(norm) == 0:,if norm == 'nnBatchNorm2d':,0.8584144844467796,0.7965020533851944,False
2342,"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper <IF_STMT> ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._aggregate_rows:,elif self._aggregate:,0.8372900777824851,0.933847757608669,False
2343,"def emitIpToDomainsData(self, data, event): self.emitRawRirData(data, event) domains = data.get('domains') if isinstance(domains, list): for domain in domains: if self.checkForStop(): return None domain = domain.strip() <IF_STMT> self.emitHostname(domain, event)",if domain:,if domain:,0.918365033959444,0.8701761846085435,True
2344,"def delete(self): from weblate.trans.models import Change, Suggestion, Vote fast_deletes = [] for item in self.fast_deletes: <IF_STMT> fast_deletes.append(Vote.objects.filter(suggestion__in=item)) fast_deletes.append(Change.objects.filter(suggestion__in=item)) fast_deletes.append(item) self.fast_deletes = fast_deletes return super().delete()",if item.model is Suggestion:,"if isinstance(item, Suggestion):",0.7146910689488113,0.8120341702859789,False
2345,def token(self): if not self._token: try: cookie_token = self.state['request'].headers.cookie[CSRF_TOKEN].value except KeyError: cookie_token = '' <IF_STMT> self._token = cookie_token else: self._token = get_random_string(TOKEN_LENGTH) return self._token,if len(cookie_token) == TOKEN_LENGTH:,if cookie_token:,0.6787867181765357,0.8466657105524215,False
2346,"def get_logs(last_file=None, last_time=None): try: response = client.get_logs(last_file=last_file, last_time=last_time) get_logs_streamer(show_timestamp=not hide_time, all_containers=all_containers, all_info=all_info)(response) return response except (ApiException, HTTPError) as e: <IF_STMT> handle_cli_error(e, message='Could not get logs for run `{}`.'.format(client.run_uuid)) sys.exit(1)",if not follow:,if client.run_uuid:,0.9119517667115736,0.8743414417652072,False
2347,"def update(self, targets): Section.update(self, targets) outputNames = set() for target in targets: g = target.globals() outputNames.update([k for k in g.keys() if k.startswith('output:')]) rows = [] outputNames = sorted(outputNames) for outputName in outputNames: row = self.__rows.get(outputName) <IF_STMT> row = _OutputRow(outputName) self.__rows[outputName] = row row.update(targets) row.setAlternate(len(rows) % 2) rows.append(row) self._mainColumn()[:] = rows",if row is None:,if row is None:,0.8358436843118336,0.8923575006167597,True
2348,"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess('branch').stdout: if git_branch_line.startswith('*'): git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() <IF_STMT> alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",if BRANCH_ALIAS_MARKER in git_branch_line:,if BRANCH_ALIAS_MARKER in git_branch_line:,0.8301518767545759,0.8228500218338367,True
2349,"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split('\r\n')): if not line: return True <IF_STMT> return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",if linenum > 0 and line[0] not in '\t ':,if linenum == 0:,0.9311891631976739,0.8806615362338783,False
2350,"def resolve_references(self, note, reflist): assert len(note['ids']) == 1 id = note['ids'][0] for ref in reflist: <IF_STMT> continue ref.delattr('refname') ref['refid'] = id assert len(ref['ids']) == 1 note.add_backref(ref['ids'][0]) ref.resolved = 1 note.resolved = 1",if ref.resolved:,if ref.get('refname') == id:,0.8412093656641157,0.8336104423443033,False
2351,"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide <IF_STMT> return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)","if dist(currentPos, self.ends[color]) == 1:",if minDist == None or minGuide == guide:,0.9401386616316358,0.8423079304558274,False
2352,"def __hierarchyViewKeyPress(hierarchyView, event): if event == __editSourceKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) <IF_STMT> __editSourceNode(hierarchyView.getContext(), hierarchyView.scene(), selectedPath) return True elif event == __editTweaksKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) if selectedPath is not None: __editTweaksNode(hierarchyView.getContext(), hierarchyView.scene(), selectedPath) return True",if selectedPath is not None:,if selectedPath is not None:,0.7714321408332607,0.8148691130388024,True
2353,"def getSubsegments(self): for num, localdata in self.lfh.LocalData: for bucket, seginfo in localdata.SegmentInfo: <IF_STMT> continue yield Win32Subsegment(self.trace, self.heap, seginfo.ActiveSubsegment)",if seginfo.ActiveSubsegment == 0:,if seginfo.ActiveSubsegment == 0:,0.8355963717817425,0.7098232254187811,True
2354,"def test_full_hd_bluray(self): cur_test = 'full_hd_bluray' cur_qual = common.Quality.FULLHDBLURAY for name, tests in iteritems(self.test_cases): for test in tests: <IF_STMT> self.assertEqual(cur_qual, common.Quality.name_quality(test)) else: self.assertNotEqual(cur_qual, common.Quality.name_quality(test))",if name == cur_test:,if name == cur_test:,0.8718959059279341,0.7709002428237395,True
2355,"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num elif self.op == '-': self.current -= num elif self.op == '*': self.current *= num <IF_STMT> self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '/':,elif self.op == '-':,0.9308716223197299,0.914208565914368,False
2356,"def strip_export_type(path): matched = re.search('#([a-zA-Z0-9\\-]+\\\\+[a-zA-Z0-9\\-]+)?$', path.encode('utf-8')) mime_type = None if matched: fragment = matched.group(0) mime_type = matched.group(1) <IF_STMT> mime_type = mime_type.replace('+', '/') path = path[:-len(fragment)] return (path, mime_type)",if mime_type is not None:,if fragment:,0.8470510514652567,0.8696398662122882,False
2357,"def _save_as_module(file, data, binary=False): if not data: return with open(file, 'w') as f: f.write('DATA=') <IF_STMT> f.write('""') f.write(base64.b64encode(data).decode('ascii')) f.write('""') else: f.write(str(data).replace('\\\\', '\\')) f.flush()",if binary:,if binary:,0.8024251272039284,0.8318180062062374,True
2358,"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text <IF_STMT> if len(self._scopeStrictModes) > 0: self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)","if text == '""use strict""' or text == ""'use strict'"":",if text.strip() != self._lastToken.text:,0.6235703431167003,0.7801270245332924,False
2359,"def run(self, ttl=None): self.zeroconf = zeroconf.Zeroconf() zeroconf.ServiceBrowser(self.zeroconf, self.domain, MDNSHandler(self)) if ttl: gobject.timeout_add(ttl * 1000, self.shutdown) self.__running = True self.__mainloop = gobject.MainLoop() context = self.__mainloop.get_context() while self.__running: try: <IF_STMT> context.iteration(True) else: time.sleep(0.1) except KeyboardInterrupt: break self.zeroconf.close() logger.debug('MDNSListener.run() quit')",if context.pending():,if context:,0.7017533239200591,0.9024521756077707,False
2360,"def topology_change_notify(self, port_state): notice = False if port_state is PORT_STATE_FORWARD: for port in self.ports.values(): if port.role is DESIGNATED_PORT: notice = True break else: notice = True if notice: self.send_event(EventTopologyChange(self.dp)) <IF_STMT> self._transmit_tc_bpdu() else: self._transmit_tcn_bpdu()",if self.is_root_bridge:,if self.dp.is_tc_bpdu():,0.9229129612120425,0.8901732118131125,False
2361,def close_open_fds(keep=None): keep = [maybe_fileno(f) for f in keep or [] if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): if fd not in keep: try: os.close(fd) except OSError as exc: <IF_STMT> raise,if exc.errno != errno.EBADF:,if exc.errno != errno.EBADF:,0.9049631366044573,0.8617195878573112,True
2362,"def collect_attributes(options, node, master_list): """"""Collect all attributes"""""" for ii in node.instructions: if field_check(ii, 'attributes'): s = getattr(ii, 'attributes') if isinstance(s, list): for x in s: if x not in master_list: master_list.append(x) <IF_STMT> master_list.append(s) for nxt in node.next.values(): collect_attributes(options, nxt, master_list)",elif s != None and s not in master_list:,"elif isinstance(s, dict):",0.9214864525120932,0.8875087479151215,False
2363,"def remove_test_run_directories(expiry_time: int=60 * 60) -> int: removed = 0 directories = glob.glob(os.path.join(UUID_VAR_DIR, 'test-backend', 'run_*')) for test_run in directories: <IF_STMT> try: shutil.rmtree(test_run) removed += 1 except FileNotFoundError: pass return removed",if round(time.time()) - os.path.getmtime(test_run) > expiry_time:,if time.time() - expiry_time > 0.5:,0.878203320925085,0.7736680847834176,False
2364,"def read_work_titles(fields): found = [] if '240' in fields: for line in fields['240']: title = join_subfield_values(line, ['a', 'm', 'n', 'p', 'r']) <IF_STMT> found.append(title) if '130' in fields: for line in fields['130']: title = ' '.join(get_lower_subfields(line)) if title not in found: found.append(title) return {'work_titles': found} if found else {}",if title not in found:,if title not in found:,0.9379435851545381,0.8703737209656045,True
2365,"def _process_v1_msg(prot, msg): header = None body = msg[1] if not isinstance(body, (binary_type, mmap, memoryview)): raise ValidationError(body, 'Body must be a bytestream.') if len(msg) > 2: header = msg[2] <IF_STMT> raise ValidationError(header, 'Header must be a dict.') for k, v in header.items(): header[k] = msgpack.unpackb(v) ctx = MessagePackMethodContext(prot, MessagePackMethodContext.SERVER) ctx.in_string = [body] ctx.transport.in_header = header return ctx","if not isinstance(header, dict):","if not isinstance(header, dict):",0.9162406018731477,0.9053411402117831,True
2366,"def find(self, node): typename = type(node).__name__ method = getattr(self, 'find_{}'.format(typename), None) if method is None: fields = getattr(node, '_fields', None) <IF_STMT> return for field in fields: value = getattr(node, field) for result in self.find(value): yield result else: for result in method(node): yield result",if fields is None:,if fields is None:,0.6280937584793536,0.8752376177722327,True
2367,"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] <IF_STMT> parts.append(param.name) if param.type: parts.append(param.type) out += [' : '.join(parts)] if param.desc and ''.join(param.desc).strip(): out += self._str_indent(param.desc) out += [''] return out",if param.name:,if param.name:,0.9484158245300762,0.9099951253570094,True
2368,"def _get_image(self, image_list, source): if source.startswith('wx'): img = wx.ArtProvider_GetBitmap(source, wx.ART_OTHER, _SIZE) else: path = os.path.join(_BASE, source) <IF_STMT> img = wx.Image(path, wx.BITMAP_TYPE_GIF).ConvertToBitmap() else: img = wx.Image(path, wx.BITMAP_TYPE_PNG).ConvertToBitmap() return image_list.Add(img)",if source.endswith('gif'):,if os.path.isfile(path):,0.8136074656602255,0.8696398662122882,False
2369,"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: <IF_STMT> sectors.append(submob) for r, submob in zip(np.arange(0, self.radius, dr), sectors): if type(submob) != AnnularSector: continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",if type(submob) == AnnularSector:,if submob.is_sector_of_sector(dr):,0.8564658430453951,0.9024521756077707,False
2370,"def _sqlite_post_configure_engine(url, engine, follower_ident): from sqlalchemy import event  @event.listens_for(engine, 'connect') def connect(dbapi_connection, connection_record): <IF_STMT> dbapi_connection.execute('ATTACH DATABASE ""test_schema.db"" AS test_schema') else: dbapi_connection.execute('ATTACH DATABASE ""%s_test_schema.db"" AS test_schema' % follower_ident)",if not follower_ident:,if follower_ident == 'sqlite':,0.8513848142508117,0.8038019482772603,False
2371,"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) if conf_file: with open(conf_file) as conf_handle: value = fn(conf_handle) <IF_STMT> return value return None",if value:,if value is not None:,0.7887130939347918,0.759907656827929,False
2372,"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction('memcmp', 'int memcmp ( const uint8_t * ptr1, const uint8_t * ptr2, size_t num )')  @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True <IF_STMT> return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",if len(a) != b.len:,if b is None:,0.9053489288492504,0.9019629427251674,False
2373,"def _get_initialized_app(app): """"""Returns a reference to an initialized App instance."""""" if app is None: return firebase_admin.get_app() if isinstance(app, firebase_admin.App): initialized_app = firebase_admin.get_app(app.name) <IF_STMT> raise ValueError('Illegal app argument. App instance not initialized via the firebase module.') return app raise ValueError('Illegal app argument. Argument must be of type  firebase_admin.App, but given ""{0}"".'.format(type(app)))",if app is not initialized_app:,if initialized_app is not None:,0.9229049510974467,0.8753524256584351,False
2374,def compiled_query(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.compiled_query_ is None: self.compiled_query_ = CompiledQuery() finally: self.lazy_init_lock_.release() return self.compiled_query_,if self.compiled_query_ is None:,if self.compiled_query_ is None:,0.6272973784763626,0.6929598487720369,True
2375,"def clean_subevent(event, subevent): if event.has_subevents: <IF_STMT> raise ValidationError(_('Subevent cannot be null for event series.')) if event != subevent.event: raise ValidationError(_('The subevent does not belong to this event.')) elif subevent: raise ValidationError(_('The subevent does not belong to this event.'))",if not subevent:,if subevent is None:,0.8482158565831137,0.8592377270804451,False
2376,"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return 'TINYBLOB' if length <= self.LENGTH_LIMIT_BLOB: return 'BLOB' <IF_STMT> return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_MEDIUMBLOB:,if length <= self.LENGTH_LIMIT_MEDIUMBLOB:,0.8773508890024868,0.7886336751695258,True
2377,"def decompress(self, data): if not data: return data if not self._first_try: return self._obj.decompress(data) self._data += data try: decompressed = self._obj.decompress(data) <IF_STMT> self._first_try = False self._data = None return decompressed except zlib.error: self._first_try = False self._obj = zlib.decompressobj(-zlib.MAX_WBITS) try: return self.decompress(self._data) finally: self._data = None",if decompressed:,if decompressed is None:,0.793311948567647,0.8780099567239787,False
2378,"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) if events | pyuv.fs.UV_RENAME: <IF_STMT> self.watches.pop(path).close()",if not os.path.exists(path):,if path in self.watches:,0.8584278659376888,0.7252761279126532,False
2379,"def __init__(self, duration, batch_shape, event_shape, validate_args=None): if duration is None: <IF_STMT> duration = event_shape[0] elif duration != event_shape[0]: if event_shape[0] != 1: raise ValueError('duration, event_shape mismatch: {} vs {}'.format(duration, event_shape)) event_shape = torch.Size((duration,) + event_shape[1:]) self._duration = duration super().__init__(batch_shape, event_shape, validate_args)",if event_shape[0] != 1:,"if isinstance(duration, int):",0.800859999070708,0.8902579342581529,False
2380,"def _CheckPrerequisites(self): """"""Exits if any of the prerequisites is not met."""""" if not FLAGS.kubectl: raise Exception('Please provide path to kubectl tool using --kubectl flag. Exiting.') if not FLAGS.kubeconfig: raise Exception('Please provide path to kubeconfig using --kubeconfig flag. Exiting.') if self.disk_specs and self.disk_specs[0].disk_type == disk.STANDARD: <IF_STMT> raise Exception('Please provide a list of Ceph Monitors using --ceph_monitors flag.')",if not FLAGS.ceph_monitors:,if not FLAGS.ceph_monitors:,0.9256985899922899,0.9196822664155297,True
2381,"def invalidateDependentSlices(self, iFirstCurve): if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) <IF_STMT> c.invalidate() elif i == iFirstCurve: break","if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType):",if c.isDependent():,0.8799918575755803,0.8466657105524215,False
2382,"def find_backwards(self, offset): try: for _, token_type, token_value in reversed(self.tokens[self.offset:offset]): if token_type in ('comment', 'linecomment'): try: prefix, comment = token_value.split(None, 1) except ValueError: continue <IF_STMT> return [comment.rstrip()] return [] finally: self.offset = offset",if prefix in self.comment_tags:,if prefix == 'comment':,0.7794611861041482,0.8385130047130208,False
2383,"def parse_column_definitions(self, elem): for column_elem in elem.findall('column'): name = column_elem.get('name', None) assert name is not None, ""Required 'name' attribute missing from column def"" index = column_elem.get('index', None) assert index is not None, ""Required 'index' attribute missing from column def"" index = int(index) self.columns[name] = index <IF_STMT> self.largest_index = index assert 'value' in self.columns, ""Required 'value' column missing from column def"" if 'name' not in self.columns: self.columns['name'] = self.columns['value']",if index > self.largest_index:,if index > 0:,0.8750813655694498,0.9204199807826591,False
2384,"def __find_smallest(self): """"""Find the smallest uncovered value in the matrix."""""" minval = sys.maxsize for i in range(self.n): for j in range(self.n): if not self.row_covered[i] and (not self.col_covered[j]): <IF_STMT> minval = self.C[i][j] return minval",if minval > self.C[i][j]:,if self.C[i][j] < minval:,0.8926028540115557,0.8385130047130208,False
2385,def includes_tools_for_display_in_tool_panel(self): if self.includes_tools: tool_dicts = self.metadata['tools'] for tool_dict in tool_dicts: <IF_STMT> return True return False,"if tool_dict.get('add_to_tool_panel', True):",if tool_dict['display_in_tool_panel']:,0.851228772232125,0.762465858623486,False
2386,"def commit(self, notify=False): if self.editing: text = self._text if text: try: value = self.type(text) except ValueError: return value = self.clamp_value(value) else: value = self.empty if value is NotImplemented: return self.value = value self.insertion_point = None <IF_STMT> self.change_text(unicode(value)) else: self._text = unicode(value) self.editing = False else: self.insertion_point = None",if notify:,if notify:,0.6663269748782973,0.9253742688467129,True
2387,"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end if end < self.plugin_args.start: continue <IF_STMT> break for vaddr in utils.xrange(start, end, 4096): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if start > self.plugin_args.end:,if start >= self.plugin_args.end:,0.764142923467842,0.8592377270804451,False
2388,"def _check_for_duplicate_host_entries(self, task_entries): non_host_statuses = (models.HostQueueEntry.Status.PARSING, models.HostQueueEntry.Status.ARCHIVING) for task_entry in task_entries: using_host = task_entry.host is not None and task_entry.status not in non_host_statuses <IF_STMT> self._assert_host_has_no_agent(task_entry)",if using_host:,if using_host:,0.7418460011002467,0.8817739004515716,True
2389,def get_biggest_wall_time(jsons): lowest_wall = None for j in jsons: <IF_STMT> lowest_wall = j['wall_time'] if lowest_wall < j['wall_time']: lowest_wall = j['wall_time'] return lowest_wall,if lowest_wall is None:,if lowest_wall is None:,0.8243353198550688,0.760856626273165,True
2390,"def log_change_report(self, old_value, new_value, include_details=False): from octoprint.util import map_boolean with self._check_mutex: self._logger.info('Connectivity changed from {} to {}'.format(map_boolean(old_value, 'online', 'offline'), map_boolean(new_value, 'online', 'offline'))) <IF_STMT> self.log_details()",if include_details:,if include_details:,0.7216510640826748,0.8817739004515716,True
2391,"def _include_block(self, value, context=None): if hasattr(value, 'render_as_block'): <IF_STMT> new_context = context.get_all() else: new_context = {} return jinja2.Markup(value.render_as_block(context=new_context)) return jinja2.Markup(value)",if context:,if context:,0.6719509666077192,0.803154665668484,True
2392,"def __lt__(self, other): try: A, B = (self[0], other[0]) if A and B: <IF_STMT> return self[2] < other[2] return A < B return self[1] < other[1] except IndexError: return NotImplemented",if A == B:,if A == B:,0.8716158709621341,0.8228500218338367,True
2393,"def _get_port(): while True: port = 20000 + random.randint(1, 9999) for i in range(5): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex(('127.0.0.1', port)) <IF_STMT> continue else: return port",if result == 0:,if result == socket.EADDRINUSE:,0.8770054063728134,0.8038019482772603,False
2394,"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: if type(targets) != list and type(targets) != tuple: targets = tuple(targets) <IF_STMT> targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",elif type(targets) != tuple:,elif type(targets) != list and type(targets) != tuple:,0.6561288230168257,0.7637260583416604,False
2395,"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = {'common': 'dns_ip'} if 'node' not in facts: facts['node'] = {} for role in params.keys(): <IF_STMT> for param in params[role]: if param in facts[role]: facts['node'][param] = facts[role].pop(param) return facts",if role in facts:,if role in facts:,0.9124532637158462,0.8627586293513119,True
2396,"def build_dimension_param(self, dimension, params): prefix = 'Dimensions.member' i = 0 for dim_name in dimension: dim_value = dimension[dim_name] <IF_STMT> if isinstance(dim_value, six.string_types): dim_value = [dim_value] for value in dim_value: params['%s.%d.Name' % (prefix, i + 1)] = dim_name params['%s.%d.Value' % (prefix, i + 1)] = value i += 1 else: params['%s.%d.Name' % (prefix, i + 1)] = dim_name i += 1",if dim_value:,if dim_value:,0.6824630629964054,0.9395648330058336,True
2397,"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: <IF_STMT> flag = 0 break if flag: self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",if _typ == typ and key is _key:,if typ == key:,0.9235414950098569,0.8555308664663046,False
2398,"def run(self): while True: message = self.in_queue.get() <IF_STMT> self.reset() elif message == EXIT: return else: index, transaction = message self.results_queue.put((index, self.validate(transaction)))",if message == RESET:,if message == RESET:,0.7219505802951742,0.7498810286408993,True
2399,"def __run(self): threads = self.parameters()['threads'].getTypedValue() with IECore.tbb_global_control(IECore.tbb_global_control.parameter.max_allowed_parallelism, IECore.hardwareConcurrency() if threads == 0 else threads): self._executeStartupFiles(self.root().getName()) defaultMessageHandler = IECore.MessageHandler.getDefaultHandler() <IF_STMT> IECore.MessageHandler.setDefaultHandler(Gaffer.ProcessMessageHandler(defaultMessageHandler)) return self._run(self.parameters().getValidatedValue())","if not isinstance(defaultMessageHandler, Gaffer.ProcessMessageHandler):",if defaultMessageHandler:,0.849465371989384,0.8318180062062374,False
2400,"def adjust_uri(self, uri, relativeto): """"""Adjust the given ``uri`` based on the given relative URI."""""" key = (uri, relativeto) if key in self._uri_cache: return self._uri_cache[key] if uri[0] != '/': <IF_STMT> v = self._uri_cache[key] = posixpath.join(posixpath.dirname(relativeto), uri) else: v = self._uri_cache[key] = '/' + uri else: v = self._uri_cache[key] = uri return v",if relativeto is not None:,if relativeto:,0.6394548503672869,0.9298663600557577,False
2401,"def decoder(s): r = [] decode = [] for c in s: <IF_STMT> decode.append('&') elif c == '-' and decode: if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",if c == '&' and (not decode):,if c == '+' and decode:,0.8644186064074125,0.841020165317327,False
2402,"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() if line.startswith('-'): args.extend(self._split_option(line)) <IF_STMT> args.append(line) return args",elif line and (not line.startswith('#')):,elif line.startswith('--'):,0.7899217651285546,0.8137489370974955,False
2403,"def _method_events_callback(self, values): try: previous_echoed = values['child_result_list'][-1].decode().split('\n')[-2].strip() if previous_echoed.endswith('foo1'): return 'echo foo2\n' elif previous_echoed.endswith('foo2'): return 'echo foo3\n' <IF_STMT> return 'exit\n' else: raise Exception('Unexpected output {0!r}'.format(previous_echoed)) except IndexError: return 'echo foo1\n'",elif previous_echoed.endswith('foo3'):,elif previous_echoed.endswith('foo3'):,0.6431386328309736,0.8787142254774354,True
2404,"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): <IF_STMT> result.append(DELETE_SUCCESSFUL) elif status == MemcacheDeleteResponse.NOT_FOUND: result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",if status == MemcacheDeleteResponse.DELETED:,if status == MemcacheDeleteResponse.SUCCESS:,0.8882862952052331,0.8038019482772603,False
2405,"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) <IF_STMT> plug.setInput(randomNode['outFloat']) elif isinstance(plug, Gaffer.Color3fPlug): plug.setInput(randomNode['outColor']) GafferUI.NodeEditor.acquire(randomNode)","if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)):","if isinstance(plug, Gaffer.Float3fPlug):",0.8652036292281842,0.7848518349390632,False
2406,"def escapeentities(self, line): """"""Escape all Unicode characters to HTML entities."""""" result = '' pos = TextPosition(line) while not pos.finished(): if ord(pos.current()) > 128: codepoint = hex(ord(pos.current())) <IF_STMT> codepoint = hex(ord(pos.next()) + 63488) result += '&#' + codepoint[1:] + ';' else: result += pos.current() pos.skipcurrent() return result",if codepoint == '0xd835':,elif ord(pos.next()) > 63488:,0.7982216136177633,0.8832000938217648,False
2407,def get_and_set_all_aliases(self): all_aliases = [] for page in self.pages: <IF_STMT> all_aliases.extend(page.relations.aliases_norm) if page.relations.aliases is not None: all_aliases.extend(page.relations.aliases) return set(all_aliases),if page.relations.aliases_norm is not None:,if page.relations.aliases_norm is not None:,0.7921949743522618,0.6907573115737006,True
2408,"def _list_cases(suite): for test in suite: <IF_STMT> _list_cases(test) elif isinstance(test, unittest.TestCase): if support.match_test(test): print(test.id())","if isinstance(test, unittest.TestSuite):","if isinstance(test, unittest.TestSuite):",0.835038230402846,0.6739047062564734,True
2409,"def get_next_requests(self, max_n_requests, **kwargs): next_pages = [] partitions = set(kwargs.pop('partitions', [])) for partition_id in range(0, self.queue_partitions): <IF_STMT> continue results = self.queue.get_next_requests(max_n_requests, partition_id) next_pages.extend(results) self.logger.debug('Got %d requests for partition id %d', len(results), partition_id) return next_pages",if partition_id not in partitions:,if partition_id in partitions:,0.8772039975935116,0.8431339019329497,False
2410,"def __iter__(self): if self.query is not None and sqlite.is_read_only_query(self.query): cur = self.connection.cursor() results = cur.execute(self.query) <IF_STMT> yield [col[0] for col in cur.description] for i, row in enumerate(results): if i >= self.limit: break yield [val for val in row] else: yield",if self.headers:,if results:,0.7173077428550337,0.9099951253570094,False
2411,"def rollback(self): for operation, values in self.current_transaction_state[::-1]: if operation == 'insert': values.remove() <IF_STMT> old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",elif operation == 'update':,elif operation == 'delete':,0.8716662167122393,0.7801270245332924,False
2412,"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) <IF_STMT> return int(q) elif self._start >= value > self._stop: q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError('{} is not in numeric range'.format(value))",if r == self._zero:,if r == self._zero:,0.927428777969798,0.8856327184319047,True
2413,"def validate_name_and_description(body, check_length=True): for attribute in ['name', 'description', 'display_name', 'display_description']: value = body.get(attribute) <IF_STMT> if isinstance(value, six.string_types): body[attribute] = value.strip() if check_length: try: utils.check_string_length(body[attribute], attribute, min_length=0, max_length=255) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",if value is not None:,if value:,0.9225505410000289,0.8901732118131125,False
2414,"def printWiki(): firstHeading = False for m in protocol: if m[0] == '': <IF_STMT> output('|}') __printWikiHeader(m[1], m[2]) firstHeading = True else: output('|-') output('| <span style=""white-space:nowrap;""><tt>' + m[0] + '</tt></span> || || ' + m[1]) output('|}')",if firstHeading:,if firstHeading:,0.9233176997701109,0.8966773400768917,True
2415,"def _get_platforms(data): platform_list = [] for item in data: if item.startswith('PlatformEdit.html?'): parameter_list = item.split('PlatformEdit.html?', 1)[1].split('&') for parameter in parameter_list: <IF_STMT> platform_list.append(parameter.split('=')[1]) return platform_list",if parameter.startswith('platformName'):,if parameter.startswith('platform_'):,0.9048986603207978,0.839587623092576,False
2416,"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v['Category'] != 'Deprecated': if v['FeatureType'] == 'val': <IF_STMT> states.append((name, v['Value'])) elif name.startswith('SCLEX_'): lexers.append((name, v['Value'])) return (lexers, states)",if name.startswith('SCE_'):,if name.startswith('SCINTO_'):,0.9186655569511498,0.8901732118131125,False
2417,"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): <IF_STMT> if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",if not operation_name:,if operation_name is None:,0.887655912678567,0.828399516355805,False
2418,"def _insertNewItemAtParent(self, targetIndex): if not self.isContainer(targetIndex): return elif not self.isContainerOpen(targetIndex): uri = self._rows[targetIndex].uri modelNode = self.getNodeForURI(uri) <IF_STMT> modelNode.markForRefreshing() return self.refreshView(targetIndex)",if modelNode:,if modelNode:,0.8234418961341999,0.8137489370974955,True
2419,"def _get_trace(self, model, guide, args, kwargs): model_trace, guide_trace = super()._get_trace(model, guide, args, kwargs) for node in model_trace.nodes.values(): <IF_STMT> log_prob = node['packed']['unscaled_log_prob'] require_backward(log_prob) self._saved_state = (model, model_trace, guide_trace, args, kwargs) return (model_trace, guide_trace)",if node['type'] == 'sample' and (not node['is_observed']):,if 'packed' in node:,0.8669424304767761,0.8336104423443033,False
2420,"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: if value is None: continue if not isinstance(key, bytes): key = text_type(key).encode(charset) <IF_STMT> value = text_type(value).encode(charset) yield (_fast_url_quote_plus(key) + '=' + _fast_url_quote_plus(value))","if not isinstance(value, bytes):","if not isinstance(value, bytes):",0.8248327743907814,0.8832000938217648,True
2421,"def handle_parse_result(self, ctx, opts, args): with augment_usage_errors(ctx, param=self): value = self.consume_value(ctx, opts) try: value = self.full_process_value(ctx, value) except Exception: <IF_STMT> raise value = None if self.callback is not None: try: value = invoke_param_callback(self.callback, ctx, self, value) except Exception: if not ctx.resilient_parsing: raise if self.expose_value: ctx.params[self.name] = value return (value, args)",if not ctx.resilient_parsing:,if not ctx.resilient_parsing:,0.7695977997954819,0.9100365300271298,True
2422,"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: <IF_STMT> return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) elif dict[pattern[i]] != list_str[i]: return False return True",if list_str[i] in set_value:,if set_value.add(list_str[i]):,0.9288257953291547,0.9144061946646023,False
2423,"def create(self, path, wipe=False): _path = self.validatepath(path) with ftp_errors(self, path): <IF_STMT> empty_file = io.BytesIO() self.ftp.storbinary(str('STOR ') + _encode(_path, self.ftp.encoding), empty_file) return True return False",if wipe or not self.isfile(path):,if wipe:,0.8674433867145058,0.8466657105524215,False
2424,"def build_output_for_item(self, item): output = [] for field in self.fields: values = self._get_item(item, field) <IF_STMT> values = [values] for value in values: if value: output.append(self.build_output_for_single_value(value)) return ''.join(output)","if not isinstance(values, list):","if not isinstance(values, list):",0.8206987566220881,0.8038019482772603,True
2425,"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith('_'): if not name[0].isupper(): <IF_STMT> if is_resource_action(member): resource_methods[name] = member return resource_methods",if not name.startswith('wait_until'):,if inspect.isclass(member):,0.8561043624545546,0.8645707301556367,False
2426,"def get_command(cls): ifconfig_cmd = 'ifconfig' for path in ['/sbin', '/usr/sbin', '/bin', '/usr/bin']: <IF_STMT> ifconfig_cmd = os.path.join(path, ifconfig_cmd) break ifconfig_cmd = ifconfig_cmd + ' -a' return ifconfig_cmd","if os.path.exists(os.path.join(path, ifconfig_cmd)):",if os.path.isdir(path):,0.9014155279467748,0.8590888738245122,False
2427,"def main(): base_dir = os.path.join(os.path.split(__file__)[0], '..', '..') for path in PATHS: path = os.path.join(base_dir, path) for root, _, files in os.walk(path): for file in files: extension = os.path.splitext(file)[1] <IF_STMT> path = os.path.join(root, file) validate_header(path)",if extension in EXTENSIONS:,if extension == '.py':,0.8459964357354153,0.8431339019329497,False
2428,"def auth_login(request): form = RegistrationForm(request.POST or None) if form.is_valid(): authed_user = authenticate(username=form.cleaned_data['username'], password=form.cleaned_data['password']) <IF_STMT> login(request, authed_user) return HttpResponse('Success') raise Http404",if authed_user:,if authed_user:,0.625110699001699,0.8137489370974955,True
2429,"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) if user is None: self.users[current_user.id] = dict(session_count=1, key=_key) else: <IF_STMT> user['session_count'] += 1 user['key'] = _key",if _new_login:,if _new_login:,0.6686362739069784,0.8590888738245122,True
2430,"def fetch(self, fingerprints): to_fetch = [f for f in fingerprints if f not in self._cache] self._logger.debug('cache size %s' % len(self._cache)) self._logger.debug('to fetch %d from %d' % (len(to_fetch), len(fingerprints))) [self._redis_pipeline.hgetall(key) for key in to_fetch] responses = self._redis_pipeline.execute() for index, key in enumerate(to_fetch): response = responses[index] <IF_STMT> self._cache[key] = response[FIELD_STATE] else: self._cache[key] = self.NOT_CRAWLED",if len(response) > 0 and FIELD_STATE in response:,if FIELD_STATE in response:,0.8857439880898419,0.8964173245779284,False
2431,"def _append_to_io_queue(self, data, stream_name): parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: if part: for block in re.split('(.{%d,})' % (self._get_squeeze_threshold() + 1), part): <IF_STMT> self._queued_io_events.append((block, stream_name))",if block:,if block not in self._queued_io_events:,0.8864137350059088,0.7784290264326612,False
2432,"def find_file_at_path_with_indexes(self, path, url): if url.endswith('/'): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith('/' + self.index_file): <IF_STMT> return self.redirect(url, url[:-len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: if os.path.isfile(os.path.join(path, self.index_file)): return self.redirect(url, url + '/') raise MissingFileError(path)",if os.path.isfile(path):,if url.endswith(self.index_file):,0.914419340984211,0.9051034981560222,False
2433,"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), 'ouroboros') for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) if ext == '.py' or (ext == '' and os.path.isdir(os.path.join(basedir, name))): if module_name not in IGNORE_MODULES and module_name not in native: <IF_STMT> modules.append(module_name) return set(modules)",if not (fast and module_name in KNOWN_PROBLEM_MODULES):,if not fast:,0.8618977223603004,0.9167056528641923,False
2434,def housenumber(self): if self.address: expression = '\\d+' pattern = re.compile(expression) match = pattern.search(self.address) <IF_STMT> return int(match.group(0)),if match:,if match:,0.7580666642946539,0.7828161456481266,True
2435,"def get_pip_version(import_path=BASE_IMPORT_PATH): try: pip = importlib.import_module(import_path) except ImportError: <IF_STMT> return get_pip_version(import_path='pip') else: import subprocess version = subprocess.check_output(['pip', '--version']) if version: version = version.decode('utf-8').split()[1] return version return '0.0.0' version = getattr(pip, '__version__', None) return version",if import_path != 'pip':,if import_path == BASE_IMPORT_PATH:,0.755001890345604,0.8431339019329497,False
2436,"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: if not self.__progress_status: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY <IF_STMT> self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY time.sleep(sleep_time)",elif self.__show_animation:,elif self.__current_operation_name:,0.7742713661227651,0.8966773400768917,False
2437,def range_key_names(self): keys = [self.range_key_attr] for index in self.global_indexes: range_key = None for key in index.schema: <IF_STMT> range_key = keys.append(key['AttributeName']) keys.append(range_key) return keys,if key['KeyType'] == 'RANGE':,if key['AttributeName'] == self.range_key_attr:,0.8308970850244682,0.7709002428237395,False
2438,"def run(self): dist = self.distribution commands = dist.command_options.keys() settings = {} for cmd in commands: if cmd == 'saveopts': continue for opt, (src, val) in dist.get_option_dict(cmd).items(): <IF_STMT> settings.setdefault(cmd, {})[opt] = val edit_config(self.filename, settings, self.dry_run)",if src == 'command line':,if src == 'default':,0.8740136771147256,0.8431339019329497,False
2439,"def parse_move(self, node): old, new = ('', '') for child in node: tag, text = (child.tag, child.text) text = text.strip() if text else None if tag == 'Old' and text: old = text <IF_STMT> new = text return Move(old, new)",elif tag == 'New' and text:,elif tag == 'New' and text:,0.7615940957056354,0.8262592696121884,True
2440,"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = (self.pyflakes_enabled, self.pep8_enabled) for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() if (run_pyflakes or run_pep8) and current_finfo is not None: <IF_STMT> finfo.run_code_analysis(run_pyflakes, run_pep8)",if current_finfo is not finfo:,if finfo.editor is current_finfo:,0.9066886844911368,0.8200754821669128,False
2441,"def tchg(var, width): """"""Convert time string to given length"""""" ret = '%2dh%02d' % (var / 60, var % 60) <IF_STMT> ret = '%2dh' % (var / 60) if len(ret) > width: ret = '%2dd' % (var / 60 / 24) if len(ret) > width: ret = '%2dw' % (var / 60 / 24 / 7) return ret",if len(ret) > width:,if len(ret) > width:,0.7416579989635423,0.9053411402117831,True
2442,"def spider_log_activity(self, messages): for i in range(0, messages): <IF_STMT> self.sp_sl_p.send(sha1(str(randint(1, 1000))), b'http://helloworld.com/way/to/the/sun/' + b'0') else: self.sp_sl_p.send(sha1(str(randint(1, 1000))), b'http://way.to.the.sun' + b'0') self.sp_sl_p.flush()",if i % 2 == 0:,if i == messages - 1:,0.598694596100476,0.6885326214539055,False
2443,"def decode_serial(self, offset): serialnum = (self.cache[offset + 3] << 24) + (self.cache[offset + 2] << 16) + (self.cache[offset + 1] << 8) + self.cache[offset] serialstr = '' is_alnum = True for i in range(4): <IF_STMT> is_alnum = False break serialstr += chr(self.cache[offset + 3 - i]) serial = serialstr if is_alnum else str(serialnum) self.ann_field(offset, offset + 3, 'Serial ' + serial)",if not chr(self.cache[offset + 3 - i]).isalnum():,if self.cache[offset + 3 - i] == 0:,0.9298743920047841,0.8566038180422328,False
2444,def gettext(rv): for child in rv.childNodes: if child.nodeType == child.TEXT_NODE: yield child.nodeValue <IF_STMT> for item in gettext(child): yield item,if child.nodeType == child.ELEMENT_NODE:,elif child.nodeType == child.ELEMENT_NODE:,0.5763069108675956,0.7245511487202049,False
2445,"def determine_block_hints(self, text): hints = '' if text: if text[0] in ' \n\x85\u2028\u2029': hints += str(self.best_indent) if text[-1] not in '\n\x85\u2028\u2029': hints += '-' <IF_STMT> hints += '+' return hints",elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029':,if text[-2] not in '\n\x85\u2028\u2029':,0.8843003698385841,0.7975010608178975,False
2446,"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue <IF_STMT> if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type","if isinstance(arg, bytes):",if arg.is_bytes():,0.7741441592330648,0.9484696017312617,False
2447,"def as_iconbitmap(cls, rkey): """"""Get image path for use in iconbitmap property"""""" img = None if rkey in cls._stock: data = cls._stock[rkey] <IF_STMT> fpath = data['filename'] fname = os.path.basename(fpath) name, file_ext = os.path.splitext(fname) file_ext = str(file_ext).lower() if file_ext in TK_BITMAP_FORMATS: img = BITMAP_TEMPLATE.format(fpath) return img","if data['type'] not in ('stock', 'data', 'image'):",if 'filename' in data:,0.7119447103008893,0.8780099567239787,False
2448,"def anonymize_ip(ip): if ip: match = RE_FIRST_THREE_OCTETS_OF_IP.findall(str(ip)) <IF_STMT> return '%s%s' % (match[0][0], '0') return ''",if match:,if match:,0.6284792871357799,0.7447819789879647,True
2449,"def serialize_tail(self): msg = bytearray() for v in self.info: <IF_STMT> value = v['value'].encode('utf-8') elif v['type'] == BMP_TERM_TYPE_REASON: value = struct.pack('!H', v['value']) v['len'] = len(value) msg += struct.pack(self._TLV_PACK_STR, v['type'], v['len']) msg += value return msg",if v['type'] == BMP_TERM_TYPE_STRING:,if v['type'] == BMP_TERM_TYPE_STRING:,0.68700185800811,0.8431339019329497,True
2450,"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): <IF_STMT> return text[i:end] if not unclosed_end and text[end] == '<': unclosed_end = end end += 1 raise TokenizationException('Unclosed comment', text[i:unclosed_end])",if text[end - 2:end] == '#}':,if text[end] == '<':,0.8138704879708725,0.8627586293513119,False
2451,"def ComboBoxDroppedHeightTest(windows): """"""Check if each combobox height is the same as the reference"""""" bugs = [] for win in windows: if not win.ref: continue if win.Class() != 'ComboBox' or win.ref.Class() != 'ComboBox': continue <IF_STMT> bugs.append(([win], {}, testname, 0)) return bugs",if win.DroppedRect().height() != win.ref.DroppedRect().height():,if win.ref.ComboBoxDroppedHeight() != win.ref.ComboBoxDroppedHeight():,0.9042690958604619,0.8661072626070159,False
2452,"def testBadModeArgument(self): bad_mode = 'qwerty' try: f = self.open(TESTFN, bad_mode) except ValueError as msg: <IF_STMT> s = str(msg) if TESTFN in s or bad_mode not in s: self.fail('bad error message for invalid mode: %s' % s) else: f.close() self.fail('no error for invalid mode: %s' % bad_mode)",if msg.args[0] != 0:,if msg.args[0] == 'bad error message':,0.8038376546249915,0.8474966375582541,False
2453,"def command_group_expired(self, command_group_name): try: deprecate_info = self._command_loader.command_group_table[command_group_name].group_kwargs.get('deprecate_info', None) <IF_STMT> return deprecate_info.expired() except AttributeError: pass return False",if deprecate_info:,if deprecate_info:,0.5406945161708907,0.762465858623486,True
2454,"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): if val == 0: assert 2500 - 500 < count < 2500 + 500 <IF_STMT> assert 7500 - 500 < count < 7500 + 500 else: assert False",elif val == 1:,elif val == 1:,0.932702118182326,0.9001816649635144,True
2455,"def get_labels(directory): cache = get_labels.__cache if directory not in cache: l = {} for t in get_visual_configs(directory)[0][LABEL_SECTION]: <IF_STMT> Messager.warning(""In configuration, labels for '%s' defined more than once. Only using the last set."" % t.storage_form(), -1) l[t.storage_form()] = t.terms[1:] cache[directory] = l return cache[directory]",if t.storage_form() in l:,if t.storage_form() in l:,0.860749706940481,0.8752376177722327,True
2456,"def try_split(self, split_text: List[str]): ret = [] for i in split_text: if len(i) == 0: continue val = int(i, 2) <IF_STMT> return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f'binary successful, returning {ret.__repr__()}') return ret",if val > 255 or val < 0:,if val == 0:,0.7048403209323705,0.8555308664663046,False
2457,"def setCellValue(self, row_idx, col, value): assert col.id == 'repls-marked' with self._lock: rgroup = self.events[row_idx] <IF_STMT> return rgroup._marked = value == 'true' and True or False if self._tree: self._tree.invalidateCell(row_idx, col)","if not isinstance(rgroup, findlib2.ReplaceHitGroup):",if rgroup._marked:,0.6938633403859142,0.8743414417652072,False
2458,"def create(cls, settlement_manager, resource_id): """"""Create a production chain that can produce the given resource."""""" resource_producer = {} for abstract_building in AbstractBuilding.buildings.values(): for resource, production_line in abstract_building.lines.items(): <IF_STMT> resource_producer[resource] = [] resource_producer[resource].append((production_line, abstract_building)) return ProductionChain(settlement_manager, resource_id, resource_producer)",if resource not in resource_producer:,if resource not in resource_producer:,0.7940128070599921,0.8294838585473985,True
2459,def get_all_partition_sets(self): partition_sets = [] if self.partitions_handle: partition_sets.extend(self.partitions_handle.get_partition_sets()) if self.scheduler_handle: partition_sets.extend([schedule_def.get_partition_set() for schedule_def in self.scheduler_handle.all_schedule_defs() <IF_STMT>]) return partition_sets,"if isinstance(schedule_def, PartitionScheduleDefinition)",if schedule_def.is_active(),0.8195224701711575,0.8086627571031982,False
2460,"def _sendDatapointsNow(self, datapoints): metrics = {} payload_pb = Payload() for metric, datapoint in datapoints: <IF_STMT> metric_pb = payload_pb.metrics.add() metric_pb.metric = metric metrics[metric] = metric_pb else: metric_pb = metrics[metric] point_pb = metric_pb.points.add() point_pb.timestamp = int(datapoint[0]) point_pb.value = datapoint[1] self.sendString(payload_pb.SerializeToString())",if metric not in metrics:,if metric not in metrics:,0.8163931897135688,0.8380055871435848,True
2461,"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper <IF_STMT> ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._dicts:,elif self._dicts:,0.9178617430683177,0.933847757608669,True
2462,"def get_metrics(): classifier, feature_labels = load_classifier() available_metrics = ImgageMetrics.get_metric_classes() effective_metrics = [] for metric in available_metrics: for label in feature_labels: for label_part in metric.get_labels(): <IF_STMT> effective_metrics.append(metric) return (classifier, feature_labels, available_metrics)",if label_part == label and metric not in effective_metrics:,if label_part.get_label() == label:,0.9080742042954675,0.8228500218338367,False
2463,"def test_nic_names(self): p = subprocess.Popen(['ipconfig', '/all'], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: if 'pseudo-interface' in nic.replace(' ', '-').lower(): continue <IF_STMT> self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)",if nic not in out:,if out != nic:,0.6534173446249802,0.8692960007731574,False
2464,"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) if value is not result: if replace: self[key] = result <IF_STMT> result.parent = self result.key = key return result","if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple):",elif result is not None:,0.8572731486108633,0.7765145040967655,False
2465,"def _EvaluateFile(self, test_list, file): name, ext = os.path.splitext(file) if ext == '.cc' or ext == '.cpp' or ext == '.c': <IF_STMT> logger.SilentLog('Found native test file %s' % file) test_list.append(name)","if re.search('_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$', name):",if name not in test_list:,0.8601878827424467,0.7909601595885504,False
2466,"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): <IF_STMT> leading_ws.append(c) else: break if self.indchar is None: self.indchar = c elif c != self.indchar: self.strict_err_or_warn('found mixing of tabs and spaces', inputstring, i) return ''.join(leading_ws)",if c in legal_indent_chars:,if c in self.whitespace:,0.6928597411864503,0.8627586293513119,False
2467,"def ident_values(self): value = self._ident_values if value is False: value = None <IF_STMT> wrapped = self.wrapped idents = getattr(wrapped, 'ident_values', None) if idents: value = [self._wrap_hash(ident) for ident in idents] self._ident_values = value return value",if not self.orig_prefix:,if self.wrapped is not None:,0.7308420211784057,0.8248765135255685,False
2468,"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') <IF_STMT> if name not in found_names: found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",if name.startswith(expr):,if name:,0.8468726774345368,0.9237460349978159,False
2469,"def pid_from_name(name): for pid in os.listdir('/proc'): try: int(pid) except: continue pname = '' with open('/proc/%s/cmdline' % pid, 'r') as f: pname = f.read() <IF_STMT> return int(pid) raise ProcessException('No process with such name: %s' % name)",if name in pname:,if pname == name:,0.8156549992863942,0.8474968231198384,False
2470,"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) <IF_STMT> raise AssertionError('Not a folder: %s' % self.path) if self.watcher: self.watcher.emit('created', self)",if not node.isdir:,if not node:,0.6270116885763292,0.8266114125804572,False
2471,"def setUp(self): BaseTestCase.setUp(self) self.rawData = [] self.dataByKey = {} for i in range(1, 11): stringCol = 'String %d' % i fixedCharCol = ('Fixed Char %d' % i).ljust(40) rawCol = 'Raw %d' % i <IF_STMT> nullableCol = 'Nullable %d' % i else: nullableCol = None dataTuple = (i, stringCol, rawCol, fixedCharCol, nullableCol) self.rawData.append(dataTuple) self.dataByKey[i] = dataTuple",if i % 2:,if i % 2 == 0:,0.8031781713230431,0.8711151332295498,False
2472,"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] <IF_STMT> yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if level + 1 == len(hits):,if len(hits) == len(new_vector):,0.9350399888550781,0.9036816878108535,False
2473,"def __repr__(self): attrs = [] for k in self.keydata: <IF_STMT> attrs.append('p(%d)' % (self.size() + 1,)) elif hasattr(self.key, k): attrs.append(k) if self.has_private(): attrs.append('private') return '<%s @0x%x %s>' % (self.__class__.__name__, id(self), ','.join(attrs))",if k == 'p':,if k == 'size':,0.9099386045187935,0.8228500218338367,False
2474,def autoload(self): if self._app.config.THEME == 'auto': <IF_STMT> if get_osx_theme() == 1: theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: theme = self._app.config.THEME self.load_theme(theme),if sys.platform == 'darwin':,if self.get_system_theme() == 'darwin':,0.9121904096421632,0.8431339019329497,False
2475,"def _get_matching_bracket(self, s, pos): if s[pos] != '{': return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == '{': depth += 1 elif c == '}': depth -= 1 <IF_STMT> break pos += 1 if pos < end and s[pos] == '}': return pos return None",if depth == 0:,elif depth == 0:,0.9325215026536846,0.9053411402117831,False
2476,"def update_meter(self, output, target, meters={'accuracy'}): output = self.__to_tensor(output) target = self.__to_tensor(target) for meter in meters: <IF_STMT> self.__addmeter(meter) if meter in ['ap', 'map', 'confusion']: target_th = self._ver2tensor(target) self.meter[meter].add(output, target_th) else: self.meter[meter].add(output, target)",if meter not in self.meter.keys():,if meter not in self.meter:,0.7417373547576895,0.8036431532733102,False
2477,"def _reinit_optimizers_with_oss(self): optimizers = self.lightning_module.trainer.optimizers for x, optimizer in enumerate(optimizers): if is_lightning_optimizer(optimizer): optimizer = optimizer._optimizer <IF_STMT> optim_class = type(optimizer) zero_optimizer = OSS(params=optimizer.param_groups, optim=optim_class, **optimizer.defaults) optimizers[x] = zero_optimizer del optimizer trainer = self.lightning_module.trainer trainer.optimizers = optimizers trainer.convert_to_lightning_optimizers()","if not isinstance(optimizer, OSS):","if isinstance(optimizer, OSS):",0.8622622013818461,0.8749766281017177,False
2478,"def OnSelChanged(self, event): self.item = event.GetItem() if self.item: self.log.write('OnSelChanged: %s' % self.GetItemText(self.item)) <IF_STMT> self.log.write(', BoundingRect: %s\n' % self.GetBoundingRect(self.item, True)) else: self.log.write('\n') event.Skip()",if wx.Platform == '__WXMSW__':,if self.item.GetBoundingRect():,0.7963899891305184,0.8318180062062374,False
2479,"def parse_batch(args): errmsg = 'Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1).' if args.batch is not None: rule, batchdef = parse_key_value_arg(args.batch, errmsg=errmsg) try: batch, batches = batchdef.split('/') batch = int(batch) batches = int(batches) except ValueError: raise ValueError(errmsg) <IF_STMT> raise ValueError(errmsg) return Batch(rule, batch, batches) return None",if batch > batches or batch < 1:,if rule is None:,0.8268519833793845,0.9053411402117831,False
2480,"def get_foreign_key_columns(self, engine, table_name): foreign_keys = set() table = db_utils.get_table(engine, table_name) inspector = reflection.Inspector.from_engine(engine) for column_dict in inspector.get_columns(table_name): column_name = column_dict['name'] column = getattr(table.c, column_name) <IF_STMT> foreign_keys.add(column_name) return foreign_keys",if column.foreign_keys:,if column.foreign_key:,0.8652357732034666,0.8743414417652072,False
2481,"def update(self, t): l = int(t * self.nr_of_tiles) for i in range(self.nr_of_tiles): t = self.tiles_order[i] <IF_STMT> self.turn_off_tile(t) else: self.turn_on_tile(t)",if i < l:,if t < l:,0.8119759642663448,0.7245511487202049,False
2482,"def read(self, amt=None): if self._rbuf and (not amt is None): L = len(self._rbuf) <IF_STMT> amt -= L else: s = self._rbuf[:amt] self._rbuf = self._rbuf[amt:] return s s = self._rbuf + self._raw_read(amt) self._rbuf = b'' return s",if amt > L:,if amt > L:,0.7501096303666885,0.8516228624291206,True
2483,"def draw_menu_button(self, context, layout, node, text): if hasattr(node.id_data, 'sv_show_socket_menus') and node.id_data.sv_show_socket_menus: <IF_STMT> layout.menu('SV_MT_SocketOptionsMenu', text='', icon='TRIA_DOWN')",if self.is_output or self.is_linked or (not self.use_prop):,if text:,0.5609218736613208,0.7447819789879647,False
2484,def __enter__(self): with DB.connection_context(): session_record = SessionRecord() session_record.f_session_id = self._session_id session_record.f_engine_name = self._engine_name session_record.f_engine_type = EngineType.STORAGE session_record.f_engine_address = {} session_record.f_create_time = current_timestamp() rows = session_record.save(force_insert=True) <IF_STMT> raise Exception(f'create session record {self._session_id} failed') LOGGER.debug(f'save session {self._session_id} record') self.create() return self,if rows != 1:,if not rows:,0.8332351535866825,0.884617925078158,False
2485,"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop(2.0) <IF_STMT> self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.sl_hdlr:,if self.root_logger:,0.5401911284098544,0.7241577342575828,False
2486,"def _dec_device(self, srcdev, dstdev): if srcdev: self.srcdevs[srcdev] -= 1 <IF_STMT> del self.srcdevs[srcdev] self._set_limits('read', self.srcdevs) if dstdev: self.dstdevs[dstdev] -= 1 if self.dstdevs[dstdev] == 0: del self.dstdevs[dstdev] self._set_limits('write', self.dstdevs)",if self.srcdevs[srcdev] == 0:,if self.srcdevs[srcdev] == 0:,0.894336945139858,0.8038019482772603,True
2487,"def array_for(self, i): if 0 <= i < self._cnt: <IF_STMT> return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[i >> level & 31] level -= 5 return node._array affirm(False, u'Index out of Range')",if i >= self.tailoff():,if i >= self._tail:,0.7739570563944373,0.8723360571509826,False
2488,"def convert_tensor(self, offsets, sizes): results = [] for b, batch in enumerate(offsets): utterances = [] for p, utt in enumerate(batch): size = sizes[b][p] <IF_STMT> utterances.append(utt[0:size]) else: utterances.append(torch.tensor([], dtype=torch.int)) results.append(utterances) return results",if sizes[b][p] > 0:,if size > 0:,0.8464887354847199,0.828399516355805,False
2489,"def _predict_proba(self, X, preprocess=True): if preprocess: X = self.preprocess(X) if self.problem_type == REGRESSION: return self.model.predict(X) y_pred_proba = self.model.predict_proba(X) if self.problem_type == BINARY: if len(y_pred_proba.shape) == 1: return y_pred_proba <IF_STMT> return y_pred_proba[:, 1] else: return y_pred_proba elif y_pred_proba.shape[1] > 2: return y_pred_proba else: return y_pred_proba[:, 1]",elif y_pred_proba.shape[1] > 1:,elif y_pred_proba.shape[1] == 2:,0.9213277702003745,0.8806615362338783,False
2490,def timeout(self): now = ptime.time() dt = now - self.lastPlayTime if dt < 0: return n = int(self.playRate * dt) if n != 0: self.lastPlayTime += float(n) / self.playRate <IF_STMT> self.play(0) self.jumpFrames(n),if self.currentIndex + n > self.image.shape[self.axes['t']]:,if self.lastPlayTime >= self.playRate:,0.903195866954851,0.8418243449361874,False
2491,"def __init__(self, data, weights=None, ddof=0): self.data = np.asarray(data) if weights is None: self.weights = np.ones(self.data.shape[0]) else: self.weights = np.asarray(weights).astype(float) <IF_STMT> self.weights = self.weights.squeeze() self.ddof = ddof",if len(self.weights.shape) > 1 and len(self.weights) > 1:,if self.data.shape[0] == 1:,0.8621345498102104,0.7965020533851944,False
2492,"def writerow(self, row): unicode_row = [] for col in row: <IF_STMT> unicode_row.append(col.encode('utf-8').strip()) else: unicode_row.append(col) self.writer.writerow(unicode_row) data = self.queue.getvalue() data = data.decode('utf-8') data = self.encoder.encode(data) self.stream.write(data) self.queue.truncate(0)",if type(col) == str or type(col) == unicode:,"if isinstance(col, unicode):",0.687433628739461,0.8266114125804572,False
2493,"def __init__(self, choices, allow_blank=False, **kwargs): self.choiceset = choices self.allow_blank = allow_blank self._choices = dict() for k, v in choices: <IF_STMT> for k2, v2 in v: self._choices[k2] = v2 else: self._choices[k] = v super().__init__(**kwargs)","if type(v) in [list, tuple]:","if isinstance(v, (list, tuple)):",0.7063665338725964,0.8385130047130208,False
2494,"def simp_ext(_, expr): if expr.op.startswith('zeroExt_'): arg = expr.args[0] <IF_STMT> return arg return ExprCompose(arg, ExprInt(0, expr.size - arg.size)) if expr.op.startswith('signExt_'): arg = expr.args[0] add_size = expr.size - arg.size new_expr = ExprCompose(arg, ExprCond(arg.msb(), ExprInt(size2mask(add_size), add_size), ExprInt(0, add_size))) return new_expr return expr",if expr.size == arg.size:,if arg.size == 0:,0.7703655860100882,0.8627586293513119,False
2495,"def mark_differences(value: str, compare_against: str): result = [] for i, char in enumerate(value): try: <IF_STMT> result.append('<font color=""red"">{}</font>'.format(char)) else: result.append(char) except IndexError: result.append(char) return ''.join(result)",if char != compare_against[i]:,if compare_against[i]:,0.8930311203908627,0.8466657105524215,False
2496,"def run_query(self, query, user): url = '%s%s' % (self.base_url, '&'.join(query.split('\n'))) error = None data = None try: response = requests.get(url, auth=self.auth, verify=self.verify) <IF_STMT> data = _transform_result(response) else: error = 'Failed getting results (%d)' % response.status_code except Exception as ex: data = None error = str(ex) return (data, error)",if response.status_code == 200:,if response.status_code == 200:,0.7765167219558236,0.8879659171421962,True
2497,"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, 'md_bg_color') and self.focus_behavior: <IF_STMT> self.md_bg_color = self.theme_cls.bg_normal elif not self.focus_color: self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color","if hasattr(self, 'theme_cls') and (not self.focus_color):",if self.focus_color:,0.6652762854367367,0.8787142254774354,False
2498,"def tearDown(self): if not self.is_playback(): try: <IF_STMT> self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: if self.storage_account_name is not None: self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self.hosted_service_name is not None:,if self.hosted_service_name is not None:,0.8059156958022904,0.7685107079449489,True
2499,"def name2cp(k): if k == 'apos': return ord(""'"") if hasattr(htmlentitydefs, 'name2codepoint'): return htmlentitydefs.name2codepoint[k] else: k = htmlentitydefs.entitydefs[k] <IF_STMT> return int(k[2:-1]) return ord(codecs.latin_1_decode(k)[0])",if k.startswith('&#') and k.endswith(';'):,if k.startswith('_'):,0.7236898521299989,0.8318180062062374,False
2500,"def _para_set(self, params, part): if len(params) == 0: result = suggest([i.get_name() for i in self._options], part) return result elif len(params) == 1: paramName = params[0] if paramName not in self._options: return [] opt = self._options[paramName] paramType = opt.get_type() <IF_STMT> values = [opt.get_default_value() == 'True' and 'False' or 'True'] else: values = self._memory[paramName] return suggest(values, part) else: return []",if paramType == 'boolean':,if paramType == 'boolean':,0.7542930451516873,0.9069443196104878,True
2501,"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) <IF_STMT> return -1 if a > b: return 1 return 0 except: return cmp(x, y)",if a < b:,if a < b:,0.6030655978912812,0.8038019482772603,True
2502,"def execute(self, statement, arguments=None): while True: try: if arguments: self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: <IF_STMT> raise else: break if statement.lstrip().upper().startswith('SELECT'): return self.cursor.fetchall()",if 'locked' not in getSafeExString(ex):,if ex.errno != errno.EEXIST:,0.8473130382365031,0.7886336751695258,False
2503,"def _test_forever(self, tests): while True: for test_name in tests: yield test_name <IF_STMT> return if self.ns.fail_env_changed and self.environment_changed: return",if self.bad:,if self.ns.fail_test_changed and self.test_changed:,0.5798180246746156,0.7098232254187811,False
2504,"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] if user.room: <IF_STMT> hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage('left-notification').format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",if self.isRoomSame(user.room):,if user.room.is_same_room():,0.9372493982595522,0.9051034981560222,False
2505,"def AutoTest(): with open(sys.argv[1], 'rb') as f: for line in f.read().split(b'\n'): line = BYTES2SYSTEMSTR(line.strip()) if not line: continue elif line.startswith('#'): print(line) else: print('>>> ' + line) os.system(line) sys.stdout.write('\npress enter to continue...') <IF_STMT> input() else: raw_input() sys.stdout.write('\n')",if PY3:,if sys.platform == 'win32':,0.8517680944439728,0.8516228624291206,False
2506,"def get_first_field(layout, clz): for layout_object in layout.fields: <IF_STMT> return layout_object elif hasattr(layout_object, 'get_field_names'): gf = get_first_field(layout_object, clz) if gf: return gf","if issubclass(layout_object.__class__, clz):","if isinstance(layout_object, clz):",0.7474764850533707,0.7848518349390632,False
2507,"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: if isinstance(kwargs.get('event_data', {}).get(key), str): <IF_STMT> kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)",if len(kwargs['event_data'][key]) > 1024:,if key in valid_keys:,0.7650770517383049,0.8105932471967202,False
2508,"def visit_productionlist(self, node): self.new_state() names = [] for production in node: names.append(production['tokenname']) maxlen = max((len(name) for name in names)) for production in node: <IF_STMT> self.add_text(production['tokenname'].ljust(maxlen) + ' ::=') lastname = production['tokenname'] else: self.add_text('%s' % (' ' * len(lastname))) self.add_text(production.astext() + self.nl) self.end_state(wrap=False) raise nodes.SkipNode",if production['tokenname']:,if production['tokenname']:,0.7769005805836919,0.9184043388013005,True
2509,"def uuid(self): if not getattr(self, '_uuid', None): <IF_STMT> self._uuid = self.repository._kp_uuid(self.path) else: self._uuid = str(uuid.uuid4()) return self._uuid",if self.repository is not None:,if self.repository:,0.798023836200208,0.7778111223054219,False
2510,"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) <IF_STMT> self._definition.pop(v, None) elif self._definition == 'ANY': if v == 'ANY': self._definition = [] elif v in self._definition: self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))","if isinstance(self._definition, dict):",if v in self._definition:,0.8431743174725981,0.9001816649635144,False
2511,"def make(self): pygments_dir = join(self.dir, 'externals', 'pygments') if exists(pygments_dir): run_in_dir('hg pull', pygments_dir, self.log.info) run_in_dir('hg update', pygments_dir, self.log.info) else: <IF_STMT> os.makedirs(dirname(pygments_dir)) run_in_dir('hg clone http://dev.pocoo.org/hg/pygments-main %s' % basename(pygments_dir), dirname(pygments_dir), self.log.info)",if not exists(dirname(pygments_dir)):,if not exists(dirname(pygments_dir)):,0.7343457361399454,0.8390782502060267,True
2512,def set_field(self): i = 0 for string in self.display_string: <IF_STMT> self.config[self.field + str(i)] = self.conversion_fn(self.str[i]) else: self.config[self.field + str(i)] = self.str[i] i = i + 1,if self.conversion_fn:,if self.conversion_fn:,0.7818947300973448,0.8590888738245122,True
2513,"def cleanup(self): with self.lock: for proc in self.processes: <IF_STMT> continue proc.join() self.processes.remove(proc) log.debug('Subprocess %s cleaned up', proc.name)",if proc.is_alive():,if proc.name in self.ignored_processes:,0.5866833892256667,0.693395566222006,False
2514,"def setup(self, gen): Node.setup(self, gen) for c in self.children: c.setup(gen) if not self.accepts_epsilon: for c in self.children: <IF_STMT> break else: self.accepts_epsilon = 1 gen.changed()",if not c.accepts_epsilon:,if c.accepts_epsilon:,0.7094860535736363,0.8466657105524215,False
2515,"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes['seq_num'])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 if self.completed_calls >= self._resolve_at_msg_count: <IF_STMT> self.done_future.set_result(None)",if not self.done_future.done():,if self.done_future:,0.9414587825815605,0.9027352381005914,False
2516,"def build_canned_image_list(path): layers_path = get_bitbake_var('BBLAYERS') canned_wks_layer_dirs = [] if layers_path is not None: for layer_path in layers_path.split(): for wks_path in (WIC_DIR, SCRIPTS_CANNED_IMAGE_DIR): cpath = os.path.join(layer_path, wks_path) <IF_STMT> canned_wks_layer_dirs.append(cpath) cpath = os.path.join(path, CANNED_IMAGE_DIR) canned_wks_layer_dirs.append(cpath) return canned_wks_layer_dirs",if os.path.isdir(cpath):,if os.path.exists(cpath):,0.8888204428802924,0.8966773400768917,False
2517,"def _recv_loop(self) -> None: async with self._ws as connection: self._connected = True self.connection = connection while self._connected: try: resp = await self.connection.recv() <IF_STMT> await self._on_message(resp) except (websockets.ConnectionClosed, ConnectionResetError): logger.info('connection closed') break await asyncio.sleep(0) if self._connected: self._loop.create_task(self.dispose())",if resp:,if resp:,0.840441043194133,0.8996480074924822,True
2518,"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): if start in line: should_yield = True continue <IF_STMT> return if should_yield and line: yield line.strip().split(' ')[0]",if end and end in line:,if end is None:,0.7947164318030352,0.8105932471967202,False
2519,"def handle_parse_result(self, ctx, opts, args): if self.name in opts: if self.mutually_exclusive.intersection(opts): self._raise_exclusive_error() <IF_STMT> self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self.multiple and len(set(opts[self.name])) > 1:,elif self.exclusive.intersection(opts):,0.8767143185761356,0.803154665668484,False
2520,"def write(self, s): if self.interactive: <IF_STMT> self.active_mode.write(s) else: component.get('CmdLine').add_line(s, False) self.events.append(s) else: print(colors.strip_colors(s))","if isinstance(self.active_mode, deluge.ui.console.modes.cmdline.CmdLine):",if self.active_mode:,0.5679403058289588,0.6997522298221912,False
2521,"def findfiles(path): files = [] for name in os.listdir(path): if name.startswith('.') or name == 'lastsnap.jpg': continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode <IF_STMT> files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",if stat.S_ISDIR(mode):,if stat.S_ISDIR(mode):,0.7455267293227422,0.8966773400768917,True
2522,"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, '_UNDOCUMENTED', False): continue if startswith is not None and (not key.startswith(startswith)): continue <IF_STMT> continue names.append(key) return names","if getattr(command, 'positional_arg', False):",if key in self.documented_completions:,0.7645498945955329,0.828399516355805,False
2523,"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): <IF_STMT> lines[i] = line[:-2] + '\n' elif line.endswith('\r'): lines[i] = line[:-1] + '\n'",if line.endswith('\r\n'):,if line.endswith('\n'):,0.8369689330983776,0.8466657105524215,False
2524,"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end if end < self.plugin_args.start: continue if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 4096): <IF_STMT> yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if self.plugin_args.start <= vaddr <= self.plugin_args.end:,if vaddr not in self.plugin_args.vtop:,0.9084241201029819,0.8294838585473985,False
2525,"def get_shape_at_node(self, node, assumptions): for k, v in assumptions.items(): <IF_STMT> return v if node.inputs: return node.container.shape(input_shapes=[self.get_shape_at_node(input_node, assumptions) for input_node in node.inputs]) else: return node.container.shape(None)",if k in node.names:,if k == 'shape':,0.6120008883580443,0.7801270245332924,False
2526,"def fix_doc(self, doc): type = doc.get('type', {}).get('key') if type == '/type/work': <IF_STMT> doc['authors'] = [a for a in doc['authors'] if 'author' in a and 'key' in a['author']] elif type == '/type/edition': if 'title_prefix' in doc: title = doc['title_prefix'].strip() + ' ' + doc.get('title', '') doc['title'] = title.strip() del doc['title_prefix'] return doc",if doc.get('authors'):,if 'authors' in doc:,0.9444191433056026,0.8944264839442453,False
2527,"def modify_column(self, column: List[Optional['Cell']]): for i in range(len(column)): gate = column[i] <IF_STMT> continue elif isinstance(gate, ParityControlCell): column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])",if gate is self:,"if isinstance(gate, Cell):",0.6232256270317323,0.8635707684233572,False
2528,"def onSync(self, auto=False, reload=True): if not auto or (self.pm.profile['syncKey'] and self.pm.profile['autoSync'] and (not self.safeMode)): from aqt.sync import SyncManager if not self.unloadCollection(): return self.state = 'sync' self.syncer = SyncManager(self, self.pm) self.syncer.sync() if reload: <IF_STMT> self.loadCollection()",if not self.col:,if self.pm.profile['syncKey']:,0.9531566163516293,0.9173731009660213,False
2529,"def _has_url_match(self, match, request_url): url = match['url'] if _is_string(url): <IF_STMT> return self._has_strict_url_match(url, request_url) else: url_without_qs = request_url.split('?', 1)[0] return url == url_without_qs elif isinstance(url, re._pattern_type) and url.match(request_url): return True else: return False",if match['match_querystring']:,"if re._strict_url_type(url, re._pattern_type):",0.5860732694360986,0.8592899528284996,False
2530,"def pool_image(self, image): if self.count < self.pool_size: self.pool.append(image) self.count += 1 return image else: p = random.random() <IF_STMT> random_id = random.randint(0, self.pool_size - 1) temp = self.pool[random_id] self.pool[random_id] = image return temp else: return image",if p > 0.5:,if p < self.pool_size:,0.8270415662909365,0.8474968231198384,False
2531,"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: <IF_STMT> width = operation['right'] - operation['left'] height = operation['bottom'] - operation['top'] if operation['type'] == 'resize': width = operation['width'] height = operation['height'] return (width, height)",if operation['type'] == 'crop':,if operation['type'] == 'resize':,0.8352078765266284,0.8431339019329497,False
2532,"def validate_matrix(matrix): if not matrix: return None for key, value in matrix.items(): <IF_STMT> raise ValidationError('`{}` defines a non uniform distribution, and it cannot be used with bayesian optimization.'.format(key)) return matrix",if value.is_distribution and (not value.is_uniform):,"if not isinstance(value, (float, float)):",0.8279302762706031,0.7975010608178975,False
2533,"def scm_to_conandata(self): try: scm_to_conandata = get_env('CONAN_SCM_TO_CONANDATA') <IF_STMT> scm_to_conandata = self.get_item('general.scm_to_conandata') return scm_to_conandata.lower() in ('1', 'true') except ConanException: return False",if scm_to_conandata is None:,if not scm_to_conandata:,0.6668920194460853,0.761827408333416,False
2534,"def _link_vrf_table(self, vrf_table, rt_list): route_family = vrf_table.route_family for rt in rt_list: rt_rf_id = rt + ':' + str(route_family) table_set = self._tables_for_rt.get(rt_rf_id) <IF_STMT> table_set = set() self._tables_for_rt[rt_rf_id] = table_set table_set.add(vrf_table) LOG.debug('Added VrfTable %s to import RT table list: %s', vrf_table, rt)",if table_set is None:,if table_set is None:,0.8566159095138174,0.8661072626070159,True
2535,"def add_tags(self, cve_results: Dict[str, Dict[str, Dict[str, str]]], file_object: FileObject): for component in cve_results: for cve_id in cve_results[component]: entry = cve_results[component][cve_id] <IF_STMT> self.add_analysis_tag(file_object, 'CVE', 'critical CVE', TagColor.RED, True) return",if self._entry_has_critical_rating(entry):,if entry['name'] == 'critical':,0.802470205236845,0.8105932471967202,False
2536,"def _validate(self): try: super(CustomClassifier, self)._validate() except UnsupportedDataType: if self.dtype in FACTOR_DTYPES: raise UnsupportedDataType(typename=type(self).__name__, dtype=self.dtype, hint='Did you mean to create a CustomFactor?') <IF_STMT> raise UnsupportedDataType(typename=type(self).__name__, dtype=self.dtype, hint='Did you mean to create a CustomFilter?') raise",elif self.dtype in FILTER_DTYPES:,elif self.dtype in FILTER_DTYPES:,0.9071694938242176,0.8385130047130208,True
2537,"def formatMessage(self, record): recordcopy = copy(record) levelname = recordcopy.levelname seperator = ' ' * (8 - len(recordcopy.levelname)) if self.use_colors: levelname = self.color_level_name(levelname, recordcopy.levelno) <IF_STMT> recordcopy.msg = recordcopy.__dict__['color_message'] recordcopy.__dict__['message'] = recordcopy.getMessage() recordcopy.__dict__['levelprefix'] = levelname + ':' + seperator return super().formatMessage(recordcopy)",if 'color_message' in recordcopy.__dict__:,if recordcopy.msg is None:,0.8640632414524654,0.8627586293513119,False
2538,def dumpregs(self): for reg in list(self.regs.retaddr) + list(self.regs.misc) + list(self.regs.common) + list(self.regs.flags): enum = self.get_reg_enum(reg) <IF_STMT> debug('# Could not dump register %r' % reg) continue name = 'U.x86_const.UC_X86_REG_%s' % reg.upper() value = self.uc.reg_read(enum) debug('uc.reg_read(%(name)s) ==> %(value)x' % locals()),if not reg or enum is None:,if enum is None:,0.6454882510265966,0.8592377270804451,False
2539,"def filter(self, lexer, stream): current_type = None current_value = None for ttype, value in stream: if ttype is current_type: current_value += value else: <IF_STMT> yield (current_type, current_value) current_type = ttype current_value = value if current_type is not None: yield (current_type, current_value)",if current_type is not None:,if current_type is not None:,0.8943197961102138,0.8492988135354755,True
2540,"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): <IF_STMT> should_yield = True continue if end and end in line: return if should_yield and line: yield line.strip().split(' ')[0]",if start in line:,if start and start in line:,0.887957448114444,0.7736680847834176,False
2541,"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, 'config'), 'r') as f: for line in f: line = line.strip() if line.startswith('['): section = line[1:-1].strip() config[section] = dict() <IF_STMT> key, value = line.replace(' ', '').split('=') config[section][key] = value return config",elif section:,elif section:,0.8091456690790886,0.9202663016973823,True
2542,"def test_has_arg(fn, name, accept_all, expected): if isinstance(fn, str): context = dict() try: exec('def {}: pass'.format(fn), context) except SyntaxError: <IF_STMT> raise pytest.skip('Function is not compatible with Python 2') context.pop('__builtins__', None) fn, = context.values() assert has_arg(fn, name, accept_all) is expected","if sys.version_info >= (3,):","if sys.version_info < (2,):",0.7450457201382199,0.8592377270804451,False
2543,"def ObjectExpression(self, properties, **kwargs): data = [] for prop in properties: self.emit(prop['value']) <IF_STMT> raise NotImplementedError('ECMA 5.1 does not support computed object properties!') data.append((to_key(prop['key']), prop['kind'][0])) self.emit('LOAD_OBJECT', tuple(data))",if prop['computed']:,if prop['kind'][0] == 'computed':,0.7554657925339151,0.7965020533851944,False
2544,"def run(self): for domain, locale, po in self.locales: <IF_STMT> path = os.path.join('locale', locale, 'LC_MESSAGES') else: path = os.path.join(self.build_dir, locale, 'LC_MESSAGES') mo = os.path.join(path, '%s.mo' % domain) self.mkpath(path) self.spawn(['msgfmt', '-o', mo, po])",if self.inplace:,if domain == 'default':,0.8657905034995974,0.828399516355805,False
2545,"def _compute_map(self, first_byte, second_byte=None): if first_byte != 15: return 'XED_ILD_MAP0' else: if second_byte == None: return 'XED_ILD_MAP1' if second_byte == 56: return 'XED_ILD_MAP2' <IF_STMT> return 'XED_ILD_MAP3' if second_byte == 15 and self.amd_enabled: return 'XED_ILD_MAPAMD' die('Unhandled escape {} / map {} bytes'.format(first_byte, second_byte))",if second_byte == 58:,if second_byte == 48:,0.8921642561204975,0.8723360571509826,False
2546,def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): <IF_STMT> buf.append(c) elif c == '\\': escaped = True elif c == '>': return ''.join(buf) else: buf.append(c) raise Exception('Unclosed tag ' + ''.join(buf)),if escaped:,if escaped:,0.8099576530208668,0.8966773400768917,True
2547,"def print_pairs(attrs=None, offset_y=0): fmt = ' ({0}:{1}) ' fmt_len = len(fmt) for bg, fg in get_fg_bg(): try: color = curses.color_pair(pair_number(fg, bg)) <IF_STMT> for attr in attrs: color |= attr screen.addstr(offset_y + bg, fg * fmt_len, fmt.format(fg, bg), color) pass except curses.error: pass",if not attrs is None:,if attrs:,0.8971405434916787,0.9144061946646023,False
2548,"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: <IF_STMT> axis = int(inputs[1]) elif _is_int_seq(inputs[1]): axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)","if isinstance(inputs[1], int):",if _is_int_seq(inputs[1]) and len(inputs[1]) == 2:,0.6974784944914993,0.80846720196545,False
2549,"def run(self, args, **kwargs): if args.trace_tag: kwargs['trace_tag'] = args.trace_tag if args.trigger_instance: kwargs['trigger_instance'] = args.trigger_instance if args.execution: kwargs['execution'] = args.execution if args.rule: kwargs['rule'] = args.rule if args.sort_order: <IF_STMT> kwargs['sort_asc'] = True elif args.sort_order in ['desc', 'descending']: kwargs['sort_desc'] = True return self.manager.query_with_count(limit=args.last, **kwargs)","if args.sort_order in ['asc', 'ascending']:","if args.sort_order in ['asc', 'ascending']:",0.9319200015633096,0.8492988135354755,True
2550,def retaddr(): sp = pwndbg.regs.sp stack = pwndbg.vmmap.find(sp) frame = gdb.newest_frame() addresses = [] while frame: addresses.append(frame.pc()) frame = frame.older() start = stack.vaddr stop = start + stack.memsz while addresses and start < sp < stop: value = pwndbg.memory.u(sp) <IF_STMT> index = addresses.index(value) del addresses[:index] print(pwndbg.chain.format(sp)) sp += pwndbg.arch.ptrsize,if value in addresses:,if value:,0.8921347949737688,0.926934323706186,False
2551,"def update_from_dictio(self, dictio_item): for index, dictio_payload in enumerate(dictio_item, 1): fuzz_payload = None for fuzz_payload in self.payloads[index]: fuzz_payload.content = dictio_payload.content fuzz_payload.type = dictio_payload.type <IF_STMT> self.add({'full_marker': None, 'word': None, 'index': index, 'field': None}, dictio_item[index - 1])",if fuzz_payload is None:,if fuzz_payload is not None:,0.7724464946935076,0.8200123297196334,False
2552,"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: <IF_STMT> result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False elif not rline.endswith(eline): return False return True","if isinstance(result, str):","if isinstance(result, str):",0.8395871693664672,0.9134996171406936,True
2553,"def execute_sql(self, sql, params=None, commit=True): try: cursor = super(RetryOperationalError, self).execute_sql(sql, params, commit) except OperationalError: if not self.is_closed(): self.close() with __exception_wrapper__: cursor = self.cursor() cursor.execute(sql, params or ()) <IF_STMT> self.commit() return cursor",if commit and (not self.in_transaction()):,if commit:,0.8664515336609693,0.8827916928185874,False
2554,"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: <IF_STMT> return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",if operation:,if definition.name and definition.name.value == operation_name:,0.7957344874082917,0.7865984197371234,False
2555,"def removeTrailingWs(self, aList): i = 0 while i < len(aList): if self.is_ws(aList[i]): j = i i = self.skip_ws(aList, i) assert j < i <IF_STMT> del aList[j:i] i = j else: i += 1",if i >= len(aList) or aList[i] == '\n':,if aList[j] == aList[j:i]:,0.8733337995978853,0.8385130047130208,False
2556,"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: <IF_STMT> arg = self._process_filter(arg, host_state) elif isinstance(arg, basestring): arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result","if isinstance(arg, list):","if isinstance(arg, (list, tuple)):",0.9353030802606158,0.8944264839442453,False
2557,"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('mw', 'hi', 'corr', 'trunc'): sent += [self.handle_word(w) for w in child] elif child.tag in ('w', 'c'): sent.append(self.handle_word(child)) <IF_STMT> raise ValueError('Unexpected element %s' % child.tag) return BNCSentence(elt.attrib['n'], sent)",elif child.tag not in self.tags_to_ignore:,elif child.tag != 'n':,0.7923860754417799,0.8661072626070159,False
2558,"def get_display_price(base: Union[TaxedMoney, TaxedMoneyRange], display_gross: bool=False) -> Money: """"""Return the price amount that should be displayed based on settings."""""" if not display_gross: display_gross = display_gross_prices() if isinstance(base, TaxedMoneyRange): <IF_STMT> base = MoneyRange(start=base.start.gross, stop=base.stop.gross) else: base = MoneyRange(start=base.start.net, stop=base.stop.net) if isinstance(base, TaxedMoney): base = base.gross if display_gross else base.net return base",if display_gross:,if display_gross:,0.6982170259403175,0.9284304001296656,True
2559,"def check_classes(self, node): if isinstance(node, nodes.Element): for class_value in node['classes'][:]: if class_value in self.strip_classes: node['classes'].remove(class_value) <IF_STMT> return 1",if class_value in self.strip_elements:,if node['classes']:,0.6317687702039831,0.8086627571031982,False
2560,"def validate(outfile=sys.stdout, silent_success=False): """"""Validates all installed models."""""" try: num_errors = get_validation_errors(outfile) <IF_STMT> return outfile.write('%s error%s found.\n' % (num_errors, num_errors != 1 and 's' or '')) except ImproperlyConfigured: outfile.write(""Skipping validation because things aren't configured properly."")",if silent_success and num_errors == 0:,if silent_success:,0.8411918537323086,0.8935248372106969,False
2561,"def check_basename_conflicts(self, targets): """"""Apps' basenames are used as bundle directory names. Ensure they are all unique."""""" basename_seen = {} for target in targets: <IF_STMT> raise self.BasenameConflictError(""Basename must be unique, found two targets use the same basename: {}'\n\t{} and \n\t{}"".format(target.basename, basename_seen[target.basename].address.spec, target.address.spec)) basename_seen[target.basename] = target",if target.basename in basename_seen:,if target.basename in basename_seen:,0.7081955339930095,0.8780099567239787,True
2562,"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None <IF_STMT> self.latest = True else: if 'preview' in api_version_str: self.preview = True parts = api_version_str.split('-') self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError('The API version {} is not in a supported format'.format(api_version_str))",if api_version_str == 'latest':,if 'latest' in api_version_str:,0.6734826036958973,0.9001816649635144,False
2563,"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp(x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p) <IF_STMT> y = result[0] elif len(result) == 2: y1, y2 = result y = y1 if y1 & 1 == y else y2 else: return None return ec.Point(self._curve, x, y)",if len(result) == 1:,if len(result) == 1:,0.6817077719717555,0.9226596185977016,True
2564,"def _visit_import_alike(self, node: Union[cst.Import, cst.ImportFrom]) -> bool: names = node.names if isinstance(names, cst.ImportStar): return False for name in names: self.provider.set_metadata(name, self.scope) asname = name.asname <IF_STMT> name_values = _gen_dotted_names(cst.ensure_type(asname.name, cst.Name)) else: name_values = _gen_dotted_names(name.name) for name_value, _ in name_values: self.scope.record_assignment(name_value, node) return False",if asname is not None:,if asname:,0.6725124116037219,0.9144061946646023,False
2565,def test_sanity_no_unmatched_parentheses(CorpusType: Type[ColumnCorpus]): corpus = CorpusType() unbalanced_entities = [] for sentence in corpus.get_all_sentences(): entities = sentence.get_spans('ner') for entity in entities: entity_text = ''.join((t.text for t in entity.tokens)) <IF_STMT> unbalanced_entities.append(entity_text) assert unbalanced_entities == [],if not has_balanced_parantheses(entity_text):,if entity_text not in unbalanced_entities:,0.812586140347793,0.8148691130388024,False
2566,"def _learn_rate_adjust(self): if self.learn_rate_decays == 1.0: return learn_rate_decays = self._vp(self.learn_rate_decays) learn_rate_minimums = self._vp(self.learn_rate_minimums) for index, decay in enumerate(learn_rate_decays): new_learn_rate = self.net_.learnRates[index] * decay <IF_STMT> self.net_.learnRates[index] = new_learn_rate if self.verbose >= 2: print('Learn rates: {}'.format(self.net_.learnRates))",if new_learn_rate >= learn_rate_minimums[index]:,if new_learn_rate < self.net_.learnRates[index]:,0.8090673928189759,0.8431339019329497,False
2567,"def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None): v = self.get_xmp_tag(xmp_tags, tags) if v is not None: <IF_STMT> setattr(self, attr, v) else: if (cast == float or cast == int) and '/' in v: v = self.try_parse_fraction(v) setattr(self, attr, cast(v))",if cast is None:,if cast is None:,0.7124788619234065,0.8592377270804451,True
2568,"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while 'e' in tokens[i + 1:]: i = tokens.index('e', i + 1) s = i - 1 e = i + 1 <IF_STMT> continue if re.match('[+-]', str(tokens[e])): e += 1 if re.match('[0-9]', str(tokens[e])): e += 1 tokens[s:e] = [''.join(tokens[s:e])] i -= 1 return tokens","if not re.match('[0-9]', str(tokens[s])):",if s == 0 or e == 0:,0.9248949496320265,0.8423079304558274,False
2569,"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: <IF_STMT> executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) if executable.check(): return executable pytest.skip('no suitable %s found' % (name,)) return executable",if sys.platform == 'win32':,if name in winpymap:,0.900163102402082,0.8431339019329497,False
2570,"def set_meta(self, dataset, overwrite=True, **kwd): super().set_meta(dataset, overwrite=overwrite, **kwd) try: <IF_STMT> with tarfile.open(dataset.file_name, 'r') as temptar: dataset.metadata.fast5_count = sum((1 for f in temptar if f.name.endswith('.fast5'))) except Exception as e: log.warning('%s, set_meta Exception: %s', self, e)",if dataset and tarfile.is_tarfile(dataset.file_name):,if dataset.file_name:,0.7869316785773186,0.8935248372106969,False
2571,"def run(self): for k in list(iterkeys(self.objs)): <IF_STMT> continue v = self.objs[k] if v['_class'] == 'User': self.split_user(k, v) elif v['_class'] in ['Message', 'PrintJob', 'Question', 'Submission', 'UserTest']: v['participation'] = v['user'] del v['user'] return self.objs",if k.startswith('_'):,if k == 'user':,0.8244802632942607,0.8336104423443033,False
2572,"def _findInTree(t, n): ret = [] if type(t) is dict: <IF_STMT> ret.append(t) for k, v in t.items(): ret += _findInTree(v, n) if type(t) is list: for v in t: ret += _findInTree(v, n) return ret",if '_name' in t and t['_name'] == n:,if n:,0.9174762005030452,0.8966773400768917,False
2573,"def parseArrayPattern(self): node = Node() elements = [] self.expect('[') while not self.match(']'): <IF_STMT> self.lex() elements.append(null) else: if self.match('...'): restNode = Node() self.lex() rest = self.parseVariableIdentifier() elements.append(restNode.finishRestElement(rest)) break else: elements.append(self.parsePatternWithDefault()) if not self.match(']'): self.expect(',') self.expect(']') return node.finishArrayPattern(elements)","if self.match(','):",if self.match('null'):,0.8367059059148201,0.8996480074924822,False
2574,"def _set_log_writer(self): if self.config['logging']: config = self.config['log_writer_config'] <IF_STMT> self.log_writer = LogWriter(**config) elif config['writer'] == 'tensorboard': self.log_writer = TensorBoardWriter(**config) else: raise ValueError(f""Unrecognized writer option: {config['writer']}"") else: self.log_writer = None",if config['writer'] == 'json':,if config['writer'] == 'log':,0.6022192175410419,0.8105932471967202,False
2575,"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): <IF_STMT> entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') if not len(head): entries.append(('all_comment', [line])) continue entries.append(('hostname', [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError('Multiple hostnames (%s) found!' % hostnames_found) return entries",if not len(line.strip()):,if not line:,0.9460674176588069,0.9000283069718913,False
2576,"def get_all_values(self, project): if isinstance(project, models.Model): project_id = project.id else: project_id = project if project_id not in self.__cache: cache_key = self._make_key(project_id) result = cache.get(cache_key) <IF_STMT> result = self.reload_cache(project_id) else: self.__cache[project_id] = result return self.__cache.get(project_id, {})",if result is None:,if result is None:,0.764508409310804,0.8474968231198384,True
2577,"def needed_libraries(self): for cmd in self.load_commands_of_type(12): tname = self._get_typename('dylib_command') dylib_command = cmd.cast(tname) name_addr = cmd.obj_offset + dylib_command.name dylib_name = self.obj_vm.read(name_addr, 256) <IF_STMT> idx = dylib_name.find('\x00') if idx != -1: dylib_name = dylib_name[:idx] yield dylib_name",if dylib_name:,if dylib_name:,0.8235863131846101,0.8935248372106969,True
2578,"def compress(self, data_list): warn_untested() if data_list: <IF_STMT> error = self.error_messages['invalid_year'] raise forms.ValidationError(error) if data_list[0] in forms.fields.EMPTY_VALUES: error = self.error_messages['invalid_month'] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) day = monthrange(year, month)[1] return date(year, month, day) return None",if data_list[1] in forms.fields.EMPTY_VALUES:,if data_list[1] in forms.fields.EMPTY_VALUES:,0.9008348616106765,0.8555308664663046,True
2579,"def put(self, obj, block=True, timeout=None): assert not self._closed if not self._sem.acquire(block, timeout): raise Full with self._notempty: with self._cond: <IF_STMT> self._start_thread() self._buffer.append(obj) self._unfinished_tasks.release() self._notempty.notify()",if self._thread is None:,if self._started_tasks:,0.8133736037836712,0.839587623092576,False
2580,"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) if not version: has_module = has_module_directory or exists(module_directory) else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile <IF_STMT> break return has_module",if has_module:,if has_module:,0.9078206027114132,0.9164531641034833,True
2581,"def expanduser(path): if path[:1] == '~': c = path[1:2] <IF_STMT> return gethome() if c == os.sep: return asPyString(File(gethome(), path[2:]).getPath()) return path",if not c:,if c == os.sep:,0.8802994863968435,0.7498810286408993,False
2582,"def mock_touch(self, bearer, version=None, revision=None, **kwargs): if version: <IF_STMT> try: return self.versions[int(version) - 1] except (IndexError, ValueError): return None else: return None return file_models.FileVersion()",if self.versions:,if version >= 0:,0.8532707296278856,0.7801270245332924,False
2583,"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] <IF_STMT> wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",elif member[0] == 'wildcards':,"elif isinstance(member[1], tuple):",0.7316937655077245,0.9081987180086649,False
2584,"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') if isinstance(expected, str): expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): <IF_STMT> if eline not in rline: return False elif not rline.endswith(eline): return False return True",if contains:,if contains:,0.9358744534693396,0.9325718821645923,True
2585,"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() <IF_STMT> self._update_value() elif event.GetKeyCode() == wx.WXK_DELETE: self.SetValue('') if event.GetKeyCode() != wx.WXK_RETURN: event.Skip()",elif event.GetKeyCode() == wx.WXK_RETURN:,elif event.GetKeyCode() == wx.WXK_PUT:,0.8868412951949523,0.760856626273165,False
2586,"def load_modules(to_load, load, attr, modules_dict, excluded_aliases, loading_message=None): if loading_message: print(loading_message) for name in to_load: module = load(name) <IF_STMT> continue cls = getattr(module, attr) if hasattr(cls, 'initialize') and (not cls.initialize()): continue if hasattr(module, 'aliases'): for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()","if module is None or not hasattr(module, attr):",if not module:,0.9514197572375263,0.9134996171406936,False
2587,def eventIterator(): while True: yield eventmodule.wait() while True: event = eventmodule.poll() <IF_STMT> break else: yield event,if event.type == NOEVENT:,if event is None:,0.5442265116336749,0.674945488826271,False
2588,"def _get_state_without_padding(self, state_with_padding, padding): lean_state = {} for key, value in state_with_padding.items(): <IF_STMT> lean_length = value.numel() - padding lean_state[key] = value[:lean_length] else: lean_state[key] = value return lean_state",if torch.is_tensor(value):,if padding:,0.8474593626615823,0.8645707301556367,False
2589,"def _get_validate(data): """"""Retrieve items to validate, from single samples or from combined joint calls."""""" if data.get('vrn_file') and tz.get_in(['config', 'algorithm', 'validate'], data): return utils.deepish_copy(data) elif 'group_orig' in data: for sub in multi.get_orig_items(data): <IF_STMT> sub_val = utils.deepish_copy(sub) sub_val['vrn_file'] = data['vrn_file'] return sub_val return None",if 'validate' in sub['config']['algorithm']:,"if sub.get('vrn_file') and tz.get_in(['config', 'algorithm', 'validate'], data):",0.8940311483314297,0.816022620682652,False
2590,"def OnPopup(self, form, popup_handle): for num, action_name, menu_name, shortcut in self.actions: <IF_STMT> ida_kernwin.attach_action_to_popup(form, popup_handle, None) else: handler = command_handler_t(self, num, 2) desc = ida_kernwin.action_desc_t(action_name, menu_name, handler, shortcut) ida_kernwin.attach_dynamic_action_to_popup(form, popup_handle, desc)",if menu_name is None:,if num == 0:,0.8730627474075672,0.8228500218338367,False
2591,"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print('struct {}'.format(self.name)) for field in self.fields: <IF_STMT> offset = '0x??' else: offset = '0x{:02x}'.format(field.offset) print('{}+{} {} {}'.format(' ' * indent, offset, field.name, field.type)) if isinstance(field.type, Structure): field.type.show(indent + 1)",if field.offset is None:,if field.offset == 0:,0.6526426200117456,0.8661072626070159,False
2592,"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: <IF_STMT> if not operation_name: if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation","if isinstance(definition, ast.OperationDefinition):","if isinstance(definition, DocumentDefinition):",0.8805545873993487,0.8547305998833805,False
2593,"def getSubMenu(self, callingWindow, context, mainItem, selection, rootMenu, i, pitem): msw = True if 'wxMSW' in wx.PlatformInfo else False self.context = context self.abilityIds = {} sub = wx.Menu() for ability in self.fighter.abilities: <IF_STMT> continue menuItem = self.addAbility(rootMenu if msw else sub, ability) sub.Append(menuItem) menuItem.Check(ability.active) return sub",if not ability.effect.isImplemented:,if ability.active:,0.6474852315160909,0.9202663016973823,False
2594,"def consume(self, event: Dict[str, Any]) -> None: with self.lock: logging.debug('Received missedmessage_emails event: %s', event) user_profile_id = event['user_profile_id'] <IF_STMT> self.batch_start_by_recipient[user_profile_id] = time.time() self.events_by_recipient[user_profile_id].append(event) self.ensure_timer()",if user_profile_id not in self.batch_start_by_recipient:,if user_profile_id not in self.events_by_recipient:,0.7447649574853157,0.7406093667638122,False
2595,"def __init__(self, start_enabled=False, use_hardware=True): self._use_hardware = use_hardware if use_hardware: self._button = Button(BUTTON_GPIO_PIN) self._enabled = start_enabled <IF_STMT> self._button.when_pressed = self._enable",if not start_enabled:,if self._enabled:,0.6536268406237771,0.803154665668484,False
2596,"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) <IF_STMT> cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",if op.stage == OperandStage.map:,if op.stage == OperandStage.map:,0.6146032009762552,0.828399516355805,True
2597,"def load_package(name, path): if os.path.isdir(path): extensions = machinery.SOURCE_SUFFIXES[:] + machinery.BYTECODE_SUFFIXES[:] for extension in extensions: init_path = os.path.join(path, '__init__' + extension) <IF_STMT> path = init_path break else: raise ValueError('{!r} is not a package'.format(path)) spec = util.spec_from_file_location(name, path, submodule_search_locations=[]) if name in sys.modules: return _exec(spec, sys.modules[name]) else: return _load(spec)",if os.path.exists(init_path):,if os.path.isfile(init_path):,0.8265255949395669,0.9237460349978159,False
2598,def setup(level=None): from pipeline.logging import pipeline_logger as logger from pipeline.log.handlers import EngineLogHandler if level in set(logging._levelToName.values()): logger.setLevel(level) logging._acquireLock() try: for hdl in logger.handlers: <IF_STMT> break else: hdl = EngineLogHandler() hdl.setLevel(logger.level) logger.addHandler(hdl) finally: logging._releaseLock(),"if isinstance(hdl, EngineLogHandler):",if hdl.isEnabledFor(level):,0.9336472855280362,0.8901732118131125,False
2599,"def find_approximant(x): c = 0.0001 it = sympy.ntheory.continued_fraction_convergents(sympy.ntheory.continued_fraction_iterator(x)) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 if abs(i - x) <= tol: return i <IF_STMT> break return x",if tol < machine_epsilon:,elif abs(i - x) <= tol:,0.9027154155173313,0.8034328145121407,False
2600,"def resolve(self, debug: bool=False, silent: bool=False, level: Optional[int]=None) -> bool: if silent: spinner = nullcontext(type('Mock', (), {})) else: spinner = yaspin(text='resolving...') with spinner as spinner: while True: resolved = self._resolve(debug=debug, silent=silent, level=level, spinner=spinner) <IF_STMT> continue self.graph.clear() return resolved",if resolved is None:,if resolved is False:,0.8593857593621185,0.8592377270804451,False
2601,"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() if name == 'MOV': <IF_STMT> return 'LSR' elif instr.mnemonic.startswith('lsl'): return 'LSL' elif instr.mnemonic.startswith('asr'): return 'ASR' return OP_NAME_MAP.get(name, name)",if instr.mnemonic.startswith('lsr'):,if instr.mnemonic.startswith('lsr'):,0.8906980819620394,0.839587623092576,True
2602,"def run_all(rule_list, defined_variables, defined_actions, stop_on_first_trigger=False): rule_was_triggered = False for rule in rule_list: result = run(rule, defined_variables, defined_actions) if result: rule_was_triggered = True <IF_STMT> return True return rule_was_triggered",if stop_on_first_trigger:,if stop_on_first_trigger:,0.6992133368135288,0.8645707301556367,True
2603,"def get_filters(self, request): filter_specs = [] if self.lookup_opts.admin.list_filter and (not self.opts.one_to_one_field): filter_fields = [self.lookup_opts.get_field(field_name) for field_name in self.lookup_opts.admin.list_filter] for f in filter_fields: spec = FilterSpec.create(f, request, self.params, self.model) <IF_STMT> filter_specs.append(spec) return (filter_specs, bool(filter_specs))",if spec and spec.has_output():,if spec.is_valid():,0.8282823276607713,0.8901732118131125,False
2604,def get_type(type_ref): kind = type_ref.get('kind') if kind == TypeKind.LIST: item_ref = type_ref.get('ofType') <IF_STMT> raise Exception('Decorated type deeper than introspection query.') return GraphQLList(get_type(item_ref)) elif kind == TypeKind.NON_NULL: nullable_ref = type_ref.get('ofType') if not nullable_ref: raise Exception('Decorated type deeper than introspection query.') return GraphQLNonNull(get_type(nullable_ref)) return get_named_type(type_ref['name']),if not item_ref:,if not item_ref:,0.7542103507163114,0.8953711787948615,True
2605,"def _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers): if method == 'POST': body = json.loads(body) <IF_STMT> return self.test_response(httplib.ACCEPTED, '') else: data = '{""error_name"":""bad destination"", ""errors"": [""Bad destination""]}' return self.test_response(httplib.BAD_REQUEST, data)",if 'destination' in body:,if body['destination'] == '1.0.cloudips.cip.jsjc5':,0.6171131870865589,0.8105932471967202,False
2606,"def _get_prefixed_values(data, prefix): """"""Collect lines which start with prefix; with trimming"""""" matches = [] for line in data.splitlines(): line = line.strip() <IF_STMT> match = line[len(prefix):] match = match.strip() matches.append(match) return matches",if line.startswith(prefix):,if line.startswith(prefix):,0.8302685746677942,0.8827916928185874,True
2607,"def _power_exact(y, xc, yc, xe): yc, ye = (y.int, y.exp) while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 <IF_STMT> return None exponent = xe * 10 ** ye if y and xe: xc = exponent else: xc = 0 return 5",if ye < 0:,if yc == 1:,0.9690953527566817,0.9155272930874561,False
2608,"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) for idx, value in enumerate(selections): <IF_STMT> values.append(value) self.stack = values",if len(values) + 1 < idx:,if idx == 0 and value != 0:,0.9215622481057102,0.8396430997456228,False
2609,"def toggleFactorReload(self, value=None): self.serviceFittingOptions['useGlobalForceReload'] = value if value is not None else not self.serviceFittingOptions['useGlobalForceReload'] fitIDs = set() for fit in set(self._loadedFits): if fit is None: continue <IF_STMT> fit.factorReload = self.serviceFittingOptions['useGlobalForceReload'] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",if fit.calculated:,if fit.factorReload is None:,0.7809824600545426,0.8431339019329497,False
2610,"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <IF_STMT> constant_init(m, 1) elif isinstance(m, nn.Linear): normal_init(m, std=0.01)","elif isinstance(m, nn.BatchNorm2d):","elif isinstance(m, nn.BatchNorm2d):",0.8567784823885461,0.7848518349390632,True
2611,"def _unzip_file(self, filepath, ext): try: <IF_STMT> zf = zipfile.ZipFile(filepath) zf.extractall(os.path.dirname(filepath)) zf.close() elif ext == '.tar': tf = tarfile.open(filepath) tf.extractall(os.path.dirname(filepath)) tf.close() except Exception as e: raise ValueError('Error reading file %r!\n%s' % (filepath, e))",if ext == '.zip':,if ext == '.zip':,0.9023227119141313,0.8336104423443033,True
2612,"def add_multiple_tasks(data, parent): data = json.loads(data) new_doc = {'doctype': 'Task', 'parent_task': parent if parent != 'All Tasks' else ''} new_doc['project'] = frappe.db.get_value('Task', {'name': parent}, 'project') or '' for d in data: <IF_STMT> continue new_doc['subject'] = d.get('subject') new_task = frappe.get_doc(new_doc) new_task.insert()",if not d.get('subject'):,if d.get('doctype') != 'Task':,0.7265439317040461,0.8661072626070159,False
2613,"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode('ascii', 'ignore')) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: <IF_STMT> continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue if kwdSndx == soundex(key.encode('ascii', 'ignore')): matchesappend(key) return _sortKeywords(keyword, matches)",if key in seenDict:,if key in seenDict:,0.860921741076548,0.9155272930874561,True
2614,"def visit_If(self, node): self.newline() self.write('if ') self.visit(node.test) self.write(':') self.body(node.body) while True: else_ = node.orelse <IF_STMT> node = else_[0] self.newline() self.write('elif ') self.visit(node.test) self.write(':') self.body(node.body) else: self.newline() self.write('else:') self.body(else_) break","if len(else_) == 1 and isinstance(else_[0], If):",if len(else_) == 1:,0.8486761798837125,0.8169276475307028,False
2615,"def _eyeLinkHardwareAndSoftwareVersion(self): try: tracker_software_ver = 0 eyelink_ver = self._eyelink.getTrackerVersion() <IF_STMT> tvstr = self._eyelink.getTrackerVersionString() vindex = tvstr.find('EYELINK CL') tracker_software_ver = int(float(tvstr[vindex + len('EYELINK CL'):].strip())) return (eyelink_ver, tracker_software_ver) except Exception: print2err('EYELINK Error during _eyeLinkHardwareAndSoftwareVersion:') printExceptionDetailsToStdErr() return EyeTrackerConstants.EYETRACKER_ERROR",if eyelink_ver == 3:,if eyelink_ver:,0.7323541000386686,0.8966773400768917,False
2616,"def execute(self, context): for monad in context.blend_data.node_groups: if monad.bl_idname == 'SverchGroupTreeType': <IF_STMT> try: monad.update_cls() except Exception as err: print(err) print('{} group class could not be created'.format(monad.name)) return {'FINISHED'}","if not getattr(bpy.types, monad.cls_bl_idname, None):",if monad.bl_idname == 'SverchGroupTree':,0.6456591005688437,0.8105932471967202,False
2617,"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: if list_str[i] in set_value: return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) el<IF_STMT> return False return True",if dict[pattern[i]] != list_str[i]:,if list_str[i] not in dict:,0.9239181942581565,0.8527204701689132,False
2618,"def decorator_handle(tokens): """"""Process decorators."""""" defs = [] decorates = [] for i, tok in enumerate(tokens): if 'simple' in tok and len(tok) == 1: decorates.append('@' + tok[0]) <IF_STMT> varname = decorator_var + '_' + str(i) defs.append(varname + ' = ' + tok[0]) decorates.append('@' + varname) else: raise CoconutInternalException('invalid decorator tokens', tok) return '\n'.join(defs + decorates) + '\n'",elif 'test' in tok and len(tok) == 1:,elif 'var' in tok:,0.9416486279651408,0.9036816878108535,False
2619,"def wait_impl(self, cpid): for i in range(10): spid, status, rusage = os.wait3(os.WNOHANG) <IF_STMT> break time.sleep(1.0) self.assertEqual(spid, cpid) self.assertEqual(status, 0, 'cause = %d, exit = %d' % (status & 255, status >> 8)) self.assertTrue(rusage)",if spid == cpid:,if spid == cpid:,0.6928529246945417,0.8385130047130208,True
2620,"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): <IF_STMT> assert 2500 - 500 < count < 2500 + 500 elif val == 1: assert 7500 - 500 < count < 7500 + 500 else: assert False",if val == 0:,if val == 0:,0.9203004242924288,0.9001816649635144,True
2621,"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: if self.stopframe and frame.f_code.co_flags & CO_GENERATOR: return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None <IF_STMT> raise BdbQuit if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",if self.quitting:,if self.stopframe:,0.9494846015839054,0.9164531641034833,False
2622,"def mouse(self, button, mods, x, y): if button == 1: for i in range(4): <IF_STMT> self.hit = i elif button == -1: self.hit = None elif self.hit != None: self.coords[self.hit] = (x, y) self.view.dirty()","if hypot(x - self.coords[i][0], y - self.coords[i][1]) < 4:",if mods[i] == 0:,0.9069460481385161,0.8431339019329497,False
2623,"def __init__(self, *commands): self.all_cmds = list(map(lambda cmd: cmd[0] if isinstance(cmd, list) else cmd, commands)) for command in commands: self.cmd = command if isinstance(command, list) else [command] self.cmd_path = pwndbg.which.which(self.cmd[0]) <IF_STMT> break",if self.cmd_path:,if self.cmd_path == 'all':,0.7710191396270761,0.8531550529091171,False
2624,"def _recv_obj(self, suppress_error=False): """"""Receive a (picklable) object"""""" if self.conn.closed: raise OSError('handle is closed') try: buf = self.conn.recv_bytes() except (ConnectionError, EOFError) as e: <IF_STMT> return logger.debug('receive has failed', exc_info=e) try: self._set_remote_close_cause(e) raise PipeShutdownError() finally: self._close() obj = RemoteObjectUnpickler.loads(buf, self) logger.debug('received %r', obj) return obj",if suppress_error:,if suppress_error:,0.933135225876489,0.9164531641034833,True
2625,"def act(self, obs): with chainer.no_backprop_mode(): batch_obs = self.batch_states([obs], self.xp, self.phi) action_distrib = self.model(batch_obs) <IF_STMT> return chainer.cuda.to_cpu(action_distrib.most_probable.array)[0] else: return chainer.cuda.to_cpu(action_distrib.sample().array)[0]",if self.act_deterministically:,if self.use_most_probable:,0.7433012172342857,0.803154665668484,False
2626,"def _classify(nodes_by_level): missing, invalid, downloads = ([], [], []) for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) elif node.binary == BINARY_INVALID: invalid.append(node) <IF_STMT> downloads.append(node) return (missing, invalid, downloads)","elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD):",elif node.binary == BINARY_DOWNLOADING:,0.9067899607697214,0.8385130047130208,False
2627,"def persist(self, *_): for key, obj in self._objects.items(): try: state = obj.get_state() <IF_STMT> continue md5 = hashlib.md5(state).hexdigest() if self._last_state.get(key) == md5: continue self._persist_provider.store(key, state) except Exception as e: system_log.exception('PersistHelper.persist fail') else: self._last_state[key] = md5",if not state:,if state is None:,0.6396996362472435,0.8431339019329497,False
2628,"def enter(self, doc, **kwds): """"""Enters the mode, arranging for necessary grabs ASAP"""""" super(ColorPickMode, self).enter(doc, **kwds) if self._started_from_key_press: doc = self.doc tdw = self.doc.tdw t, x, y = doc.get_last_event_info(tdw) <IF_STMT> self._pick_color_mode(tdw, x, y, self._pickmode) self._start_drag_on_next_motion_event = True self._needs_drag_start = True","if None not in (x, y):",if t == 'ColorPickMode':,0.6949187179102725,0.8627586293513119,False
2629,"def on_profiles_loaded(self, profiles): cb = self.builder.get_object('cbProfile') model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() <IF_STMT> continue if name.endswith('.sccprofile'): name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",if name.endswith('.mod'):,if name == 'sccprofile':,0.7538446754164998,0.8105932471967202,False
2630,"def subprocess_post_check(completed_process: subprocess.CompletedProcess, raise_error: bool=True) -> None: if completed_process.returncode: if completed_process.stdout is not None: print(completed_process.stdout, file=sys.stdout, end='') <IF_STMT> print(completed_process.stderr, file=sys.stderr, end='') if raise_error: raise PipxError(f""{' '.join([str(x) for x in completed_process.args])!r} failed"") else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process.stderr is not None:,if completed_process.stderr is not None:,0.6645059069741689,0.8248765135255685,True
2631,"def test_connect(ipaddr, port, device, partition, method, path, headers=None, query_string=None): if path == '/a': for k, v in headers.iteritems(): <IF_STMT> break else: test_errors.append('%s: %s not in %s' % (test_header, test_value, headers))",if k.lower() == test_header.lower() and v == test_value:,if k == test_header and v == test_value:,0.8729691309943532,0.7300365518218032,False
2632,"def test_stat_result_pickle(self): result = os.stat(self.fname) for proto in range(pickle.HIGHEST_PROTOCOL + 1): p = pickle.dumps(result, proto) self.assertIn(b'stat_result', p) <IF_STMT> self.assertIn(b'cos\nstat_result\n', p) unpickled = pickle.loads(p) self.assertEqual(result, unpickled)",if proto < 4:,if proto == pickle.HIGHEST_PROTOCOL + 1:,0.7302932422994391,0.7332023424667726,False
2633,def run_sql(sql): table = sql.split(' ')[5] logger.info('Updating table {}'.format(table)) with transaction.atomic(): with connection.cursor() as cursor: cursor.execute(sql) rows = cursor.fetchall() <IF_STMT> raise Exception('Sentry notification that {} is migrated'.format(table)),if not rows:,if not rows:,0.8400766628351871,0.833078701050083,True
2634,"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x <IF_STMT> self.box[2] = x if y < self.box[1]: self.box[1] = y if y > self.box[3]: self.box[3] = y",if x > self.box[2]:,if x > self.box[2]:,0.7891151765922088,0.8592377270804451,True
2635,"def _packageFocusOutViaKeyPress(self, row, column, txt): if txt: self._set_current_cell(row + 1, column) else: widget = self.cellWidget(row + 1, column) <IF_STMT> self._delete_cell(row, column) new_request = self.get_request() self.context_model.set_request(new_request) self._update_request_column(column, self.context_model)","if widget and isinstance(widget, PackageSelectWidget):",if widget.isChecked():,0.7791498115225439,0.8645707301556367,False
2636,"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith('win'): output = output.replace('\\\n', '') environ = {} for line in output.splitlines(0): line = line.rstrip() <IF_STMT> continue item = _ParseBashEnvStr(line) if item: environ[item[0]] = item[1] return environ",if not line:,if not line or line.startswith('#'):,0.745929697288086,0.8248765135255685,False
2637,"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() <IF_STMT> record = None except KeyError: record = None if not record: record = {'r': 'unknown', 'dns': {}, 'g': 1, 'query_count': 0} return record",if time_now - record['update'] > self.ttl:,if time_now > self.cache_timeout:,0.6420874151295587,0.8516228624291206,False
2638,"def test_filehash(self): """"""tests the hashes of the files in data/"""""" fp = self.get_data_path() for fn in os.listdir(fp): <IF_STMT> continue expected_hash = fn fullp = os.path.join(fp, fn) output = self.run_command('sha1sum ' + fullp, exitcode=0) result = output.split(' ')[0] self.assertEqual(result, expected_hash)",if '.' in fn:,if not fn.endswith('.sh'):,0.8126014845746325,0.884617925078158,False
2639,"def test_new_vs_reference_code_stream_read_during_iter(read_idx, read_len, bytecode): reference = SlowCodeStream(bytecode) latest = CodeStream(bytecode) for index, (actual, expected) in enumerate(zip(latest, reference)): assert actual == expected if index == read_idx: readout_actual = latest.read(read_len) readout_expected = reference.read(read_len) assert readout_expected == readout_actual <IF_STMT> assert latest.program_counter >= len(reference) else: assert latest.program_counter == reference.program_counter",if reference.program_counter >= len(reference):,if index == read_idx:,0.8421892051231352,0.8806615362338783,False
2640,"def setup_logging(): try: logconfig = config.get('logging_config_file') <IF_STMT> logging.config.fileConfig(logconfig, disable_existing_loggers=False) logger.info('logging initialized') logger.debug('debug') except Exception as e: print('Unable to set logging configuration:', str(e), file=sys.stderr) raise",if logconfig and os.path.exists(logconfig):,if logconfig:,0.7058911081067869,0.8466657105524215,False
2641,def all_words(filename): start_char = True for c in characters(filename): <IF_STMT> word = '' if c.isalnum(): word = c.lower() start_char = False else: pass elif c.isalnum(): word += c.lower() else: start_char = True yield word,if start_char == True:,if start_char:,0.8542603903692241,0.8935248372106969,False
2642,"def _get_nonce(self, url, new_nonce_url): if not self._nonces: logger.debug('Requesting fresh nonce') <IF_STMT> response = self.head(url) else: response = self._check_response(self.head(new_nonce_url), content_type=None) self._add_nonce(response) return self._nonces.pop()",if new_nonce_url is None:,if new_nonce_url is None:,0.7548223767048587,0.760856626273165,True
2643,"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): if line.startswith(comment): <IF_STMT> continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",if line[len(comment):].lstrip().startswith(comment):,if _BLANK_LINE.match(line):,0.7683794061122596,0.8866029039778043,False
2644,"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = '' for arg in args: if isinstance(arg, bool): gvariant += ' {}'.format(str(arg).lower()) <IF_STMT> gvariant += f' {arg}' elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f' {arg!s}' return gvariant.lstrip()","elif isinstance(arg, (int, float)):","elif isinstance(arg, int):",0.7549565837140214,0.8928756684056034,False
2645,"def _SkipGroup(buffer, pos, end): """"""Skip sub-group.  Returns the new position."""""" while 1: tag_bytes, pos = ReadTag(buffer, pos) new_pos = SkipField(buffer, pos, end, tag_bytes) <IF_STMT> return pos pos = new_pos",if new_pos == -1:,if new_pos == -1:,0.7178268209093536,0.8169276475307028,True
2646,"def update_participants(self, refresh=True): for participant in list(self.participants_dict): if participant is None or participant == self.simulator_config.broadcast_part: continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: <IF_STMT> self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",if participant in self.participants_dict:,if participant in self.participant_items:,0.897373900483445,0.828399516355805,False
2647,"def feature_reddit(layer_data, graph): feature = {} times = {} indxs = {} for _type in layer_data: if len(layer_data[_type]) == 0: continue idxs = np.array(list(layer_data[_type].keys())) tims = np.array(list(layer_data[_type].values()))[:, 1] feature[_type] = np.array(list(graph.node_feature[_type].loc[idxs, 'emb']), dtype=np.float) times[_type] = tims indxs[_type] = idxs <IF_STMT> attr = feature[_type] return (feature, times, indxs, attr)",if _type == 'def':,if len(feature[_type]) > 0:,0.7858100330309564,0.8879659171421962,False
2648,"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: <IF_STMT> tts[name] = '%ssort' % name if tag.internal: tts['~%s' % name] = '~%ssort' % name return tts",if tag.user:,if tag.internal:,0.8111848805841001,0.8827916928185874,False
2649,"def max_radius(iterator): radius_result = dict() for k, v in iterator: if v[0] not in radius_result: radius_result[v[0]] = v[1] <IF_STMT> radius_result[v[0]] = v[1] return radius_result",elif v[1] >= radius_result[v[0]]:,elif v[1] not in radius_result:,0.7704847249198425,0.7506346798217074,False
2650,"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join(constant.profile['APPDATA'], u'..', u'Local', u'Microsoft', u'Vault') if os.path.exists(main_vault_directory): for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault(os.path.join(main_vault_directory, vault_directory)) <IF_STMT> pwd_found.append(cred) return pwd_found",if cred:,if cred:,0.9309718827415946,0.8787142254774354,True
2651,"def disconnect_sync(self, connection, close_connection=False): key = id(connection) ts = self.in_use.pop(key) if close_connection: self.connections_map.pop(key) self._connection_close_sync(connection) el<IF_STMT> self.connections_map.pop(key) self._connection_close_sync(connection) else: with self._lock_sync: heapq.heappush(self.connections_sync, (ts, key))",if self.stale_timeout and self.is_stale(ts):,if ts == self.connection_timeout:,0.6955531649254556,0.7709002428237395,False
2652,"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) <IF_STMT> self._populate_number(element, k, v)","elif type(v) in [int, float, long, complex]:","elif isinstance(v, int):",0.9171503519749634,0.9042878500265974,False
2653,"def readframes(self, nframes): if self._ssnd_seek_needed: self._ssnd_chunk.seek(0) dummy = self._ssnd_chunk.read(8) pos = self._soundpos * self._framesize <IF_STMT> self._ssnd_chunk.seek(pos + 8) self._ssnd_seek_needed = 0 if nframes == 0: return '' data = self._ssnd_chunk.read(nframes * self._framesize) if self._convert and data: data = self._convert(data) self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth) return data",if pos:,if pos:,0.9509176131727285,0.9284304001296656,True
2654,"def target_glob(tgt, hosts): ret = {} for host in hosts: <IF_STMT> ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {})) ret[host].update({'host': host}) if __opts__.get('ssh_user'): ret[host].update({'user': __opts__['ssh_user']}) return ret","if fnmatch.fnmatch(tgt, host):",if host not in ret:,0.8151735270490066,0.7406093667638122,False
2655,"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] <IF_STMT> return attval.value_callback() return attval.value",if attval.value_callback:,if attval.value_callback:,0.947371007940994,0.9284304001296656,True
2656,"def remove_property(self, key): with self.secure() as config: keys = key.split('.') current_config = config for i, key in enumerate(keys): <IF_STMT> return if i == len(keys) - 1: del current_config[key] break current_config = current_config[key]",if key not in current_config:,if key not in current_config:,0.9143971533461548,0.8094220211349227,True
2657,"def _class_browser(parent): try: file = __file__ except NameError: file = sys.argv[0] <IF_STMT> file = sys.argv[1] else: file = sys.argv[0] dir, file = os.path.split(file) name = os.path.splitext(file)[0] flist = PyShell.PyShellFileList(parent) global file_open file_open = flist.open ClassBrowser(flist, name, [dir], _htest=True)",if sys.argv[1:]:,if sys.argv[1]:,0.7392004859994966,0.9051034981560222,False
2658,"def get_only_text_part(self, msg): count = 0 only_text_part = None for part in msg.walk(): if part.is_multipart(): continue count += 1 mimetype = part.get_content_type() or 'text/plain' <IF_STMT> return False else: only_text_part = part return only_text_part",if mimetype != 'text/plain' or count != 1:,if mimetype != 'text/plain':,0.8489024571861589,0.8385130047130208,False
2659,"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or '' for line in commit_msg.splitlines(): parts = line.strip('# ').split(':', 1) key, val = parts if len(parts) > 1 else (parts[0], '') if key == 'CI_KEEP_ALIVE': ci_names = val.replace(',', ' ').lower().split() if val else [] <IF_STMT> result = True return result",if len(ci_names) == 0 or ci.lower() in ci_names:,if ci in ci_names:,0.6589925170308063,0.8902056737869248,False
2660,"def _calc_block_io(self, blkio): """"""Calculate block IO stats."""""" for stats in blkio['io_service_bytes_recursive']: if stats['op'] == 'Read': self._blk_read += stats['value'] <IF_STMT> self._blk_write += stats['value']",elif stats['op'] == 'Write':,elif stats['op'] == 'Write':,0.6841972251162097,0.760856626273165,True
2661,"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('Oracle backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value)",if settings.USE_TZ:,if self.USE_TZ:,0.8941347732381401,0.8787142254774354,False
2662,"def load_state_dict(self, state_dict): for module_name, module_state_dict in state_dict.items(): if module_name in self.module_pool: <IF_STMT> self.module_pool[module_name].module.load_state_dict(module_state_dict) else: self.module_pool[module_name].load_state_dict(module_state_dict) else: logging.info(f'Missing {module_name} in module_pool, skip it..')",if self.config['dataparallel']:,"if isinstance(self.module_pool[module_name], Module):",0.6385603437680797,0.80377750806414,False
2663,"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): if i >= min(len(scales), len(vidxs) // 2): break scale = scales[i] <IF_STMT> vidx1, vidx2 = (vidxs[i * 2], vidxs[i * 2 + 1]) scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData",if not math.isnan(scale):,if scale != 0:,0.9528519839306692,0.8723360571509826,False
2664,"def __init__(self, factors, contrast_matrices, num_columns): self.factors = tuple(factors) factor_set = frozenset(factors) if not isinstance(contrast_matrices, dict): raise ValueError('contrast_matrices must be dict') for factor, contrast_matrix in six.iteritems(contrast_matrices): <IF_STMT> raise ValueError('Unexpected factor in contrast_matrices dict') if not isinstance(contrast_matrix, ContrastMatrix): raise ValueError('Expected a ContrastMatrix, not %r' % (contrast_matrix,)) self.contrast_matrices = contrast_matrices if not isinstance(num_columns, six.integer_types): raise ValueError('num_columns must be an integer') self.num_columns = num_columns",if factor not in factor_set:,if factor not in factor_set:,0.9212897942291823,0.8954283587198554,True
2665,"def app(scope, receive, send): while True: message = await receive() <IF_STMT> await send({'type': 'websocket.accept'}) elif message['type'] == 'websocket.receive': pass elif message['type'] == 'websocket.disconnect': break",if message['type'] == 'websocket.connect':,if message['type'] == 'websocket.connect':,0.5645478867576861,0.7801270245332924,True
2666,"def value__set(self, value): for i, (option, checked) in enumerate(self.options): <IF_STMT> self.selectedIndex = i break else: raise ValueError('Option %r not found (from %s)' % (value, ', '.join([repr(o) for o, c in self.options])))",if option == str(value):,if value == option.value:,0.6814232371715468,0.828399516355805,False
2667,"def init_links(self): links = LinkCallback.find_links(self) callbacks = [] for link, src_plot, tgt_plot in links: cb = Link._callbacks['bokeh'][type(link)] <IF_STMT> continue callbacks.append(cb(self.root, link, src_plot, tgt_plot)) return callbacks",if src_plot is None or (link._requires_target and tgt_plot is None):,if not cb:,0.7364105803755732,0.8196189957582152,False
2668,"def _validate_scalar_extensions(self) -> List[str]: errors = [] for extension in [x for x in self.extensions if isinstance(x, GraphQLScalarTypeExtension)]: extended = self.type_definitions.get(extension.name) ext_errors = _validate_extension(extended, extension.name, GraphQLScalarType, 'SCALAR') errors.extend(ext_errors) <IF_STMT> errors.extend(_validate_extension_directives(extension, extended, 'SCALAR')) return errors",if not ext_errors:,if extension.directives:,0.8667578196302668,0.8935248372106969,False
2669,"def copy_tcltk(src, dest, symlink): """"""copy tcl/tk libraries on Windows (issue #93)"""""" for libversion in ('8.5', '8.6'): for libname in ('tcl', 'tk'): srcdir = join(src, 'tcl', libname + libversion) destdir = join(dest, 'tcl', libname + libversion) <IF_STMT> copyfileordir(srcdir, destdir, symlink)",if os.path.exists(srcdir) and (not os.path.exists(destdir)):,if os.path.isdir(srcdir):,0.737486232660846,0.9076141716697395,False
2670,"def parse(self, response): try: content = response.content.decode('utf-8', 'ignore') content = json.loads(content, strict=False) except: self.logger.error('Fail to parse the response in json format') return for item in content['data']: if 'objURL' in item: img_url = self._decode_url(item['objURL']) <IF_STMT> img_url = item['hoverURL'] else: continue yield dict(file_url=img_url)",elif 'hoverURL' in item:,if 'hoverURL' in item:,0.8271859374383408,0.8692960007731574,False
2671,"def check_and_reload(self): for table_name, table_version in self._table_versions.items(): table = self.app.tool_data_tables.get(table_name, None) <IF_STMT> return self.reload_genomes()",if table is not None and (not table.is_current_version(table_version)):,if table and table.version == table_version:,0.6022678546961958,0.5767908748024404,False
2672,"def _get_query_defaults(self, query_defns): defaults = {} for k, v in query_defns.items(): try: <IF_STMT> defaults[k] = self._get_default_obj(v['schema']) else: defaults[k] = v['schema']['default'] except KeyError: pass return defaults",if v['schema']['type'] == 'object':,"if isinstance(v['schema'], dict):",0.6987044839588321,0.8196189957582152,False
2673,"def ftp_login(host, port, username=None, password=None, anonymous=False): ret = False try: ftp = ftplib.FTP() ftp.connect(host, port, timeout=6) <IF_STMT> ftp.login() else: ftp.login(username, password) ret = True ftp.quit() except Exception: pass return ret",if anonymous:,if anonymous:,0.7827257256638068,0.8787142254774354,True
2674,def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value if 's' in thisDynamic: thisDynamic = thisDynamic[1:] if thisDynamic[-1] == 'z': thisDynamic = thisDynamic[:-1] <IF_STMT> return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None],if thisDynamic in dynamicStrToScalar:,elif thisDynamic in dynamicStrToScalar:,0.7284959111270053,0.8627586293513119,False
2675,"def processCoords(coords): newcoords = deque() for x, y, z in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) <IF_STMT> continue nx, ny, nz = p if level.blockAt(nx, ny, nz) == 0: level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords",if p not in box:,if len(p) == 0:,0.9440711489856096,0.9053411402117831,False
2676,"def _set_property(self, target_widget, pname, value): if pname == 'text': wstate = str(target_widget['state']) <IF_STMT> target_widget['state'] = 'normal' target_widget.delete('0', tk.END) target_widget.insert('0', value) target_widget['state'] = wstate else: super(EntryBaseBO, self)._set_property(target_widget, pname, value)",if wstate != 'normal':,if wstate == 'normal':,0.6342951924583258,0.8105932471967202,False
2677,"def teardown(): try: time.sleep(1) except KeyboardInterrupt: return while launchers: p = launchers.pop() <IF_STMT> try: p.stop() except Exception as e: print(e) pass if p.poll() is None: try: time.sleep(0.25) except KeyboardInterrupt: return if p.poll() is None: try: print('cleaning up test process...') p.signal(SIGKILL) except: print(""couldn't shutdown process: "", p)",if p.poll() is None:,if p.isAlive():,0.9433126222238426,0.9220450449751959,False
2678,"def checkAndRemoveDuplicate(self, node): for bucket in self.buckets: for n in bucket.getNodes(): <IF_STMT> self.removeContact(n)","if (n.ip, n.port) == (node.ip, node.port) and n.id != node.id:",if n.getNode() == node:,0.5674245879960906,0.6668954865619205,False
2679,def toString(): flags = u'' try: if this.glob: flags += u'g' <IF_STMT> flags += u'i' if this.multiline: flags += u'm' except: pass v = this.value if this.value else '(?:)' return u'/%s/' % v + flags,if this.ignore_case:,if this.iglob:,0.7794491659561381,0.8966773400768917,False
2680,"def import_submodules(package_name): package = sys.modules[package_name] results = {} for loader, name, is_pkg in pkgutil.iter_modules(package.__path__): full_name = package_name + '.' + name module = importlib.import_module(full_name) setattr(sys.modules[__name__], name, module) results[full_name] = module if is_pkg: valid_pkg = import_submodules(full_name) <IF_STMT> results.update(valid_pkg) return results",if valid_pkg:,if valid_pkg:,0.8532903262280975,0.9076141716697395,True
2681,"def _call(self, cmd): what = cmd['command'] if what == 'list': name = cmd['properties'].get('name') <IF_STMT> return {'watchers': ['one', 'two', 'three']} return {'pids': [123, 456]} elif what == 'dstats': return {'info': {'pid': 789}} elif what == 'listsockets': return {'status': 'ok', 'sockets': [{'path': self._unix, 'fd': 5, 'name': 'XXXX', 'backlog': 2048}], 'time': 1369647058.967524} raise NotImplementedError(cmd)",if name is None:,if name == 'watchers':,0.7196955989500088,0.8944264839442453,False
2682,"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease): <IF_STMT> continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)","if xlib.XFilterEvent(e, e.xany.window):",if e.xany.window not in self._window_map:,0.8511476452866529,0.759907656827929,False
2683,"def translate(self, line): parsed = self.RE_LINE_PARSER.match(line) if parsed: value = parsed.group(3) stage = parsed.group(1) <IF_STMT> return '\n# HTTP Request:\n' + self.stripslashes(value) elif stage == 'reply': return '\n\n# HTTP Response:\n' + self.stripslashes(value) elif stage == 'header': return value + '\n' else: return value return line",if stage == 'send':,if stage == 'request':,0.8505766436696649,0.8780099567239787,False
2684,def toString(): flags = u'' try: <IF_STMT> flags += u'g' if this.ignore_case: flags += u'i' if this.multiline: flags += u'm' except: pass v = this.value if this.value else '(?:)' return u'/%s/' % v + flags,if this.glob:,if this.ignore_regex:,0.8275833234652956,0.8966773400768917,False
2685,"def __exit__(self, *exc_info): super(WarningsChecker, self).__exit__(*exc_info) if all((a is None for a in exc_info)): if self.expected_warning is not None: <IF_STMT> __tracebackhide__ = True pytest.fail('DID NOT WARN')",if not any((r.category in self.expected_warning for r in self)):,if self.expected_warning.is_warning():,0.8970788660839066,0.8531413606256201,False
2686,"def run(self): for k, v in iteritems(self.objs): if k.startswith('_'): continue <IF_STMT> if v['email'] == '': v['email'] = None if v['ip'] == '0.0.0.0': v['ip'] = None return self.objs",if v['_class'] == 'User':,if v['type'] == 'email':,0.6970502901408121,0.8038019482772603,False
2687,"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): if i <= start_after: continue <IF_STMT> self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",if i == 2 and self.count < 1:,if i == 6 and self.count < 2:,0.8323846232626689,0.7576808939730131,False
2688,def check(self): tcp_client = self.tcp_create() if tcp_client.connect(): tcp_client.send(b'ABCDE') response = tcp_client.recv(5) tcp_client.close() if response: <IF_STMT> self.endianness = '>' elif response.startswith(b'ScMM'): self.endianness = '<' return True return False,if response.startswith(b'MMcS'):,if response.startswith(b'BCM'):,0.9096819646202365,0.8645707301556367,False
2689,"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): if src_root != src_dir: rel_root = os.path.relpath(src_root, src_dir) else: rel_root = '' if skip_variables and rel_root.startswith('variables'): continue dst_root = os.path.join(dst_dir, rel_root) <IF_STMT> os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if not os.path.exists(dst_root):,if not os.path.exists(dst_root):,0.9388949103436934,0.8928756684056034,True
2690,"def _set_hostport(self, host, port): if port is None: i = host.rfind(':') j = host.rfind(']') <IF_STMT> try: port = int(host[i + 1:]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1:]) host = host[:i] else: port = self.default_port if host and host[0] == '[' and (host[-1] == ']'): host = host[1:-1] self.host = host self.port = port",if i > j:,if i != j:,0.6864456226190478,0.9053411402117831,False
2691,"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] <IF_STMT> field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",if key == 'nw_src':,if key == 'nw_src':,0.7466161171124667,0.8902056737869248,True
2692,"def _clear_storage(): """"""Clear old files from storage."""""" hacs = get_hacs() storagefiles = ['hacs'] for s_f in storagefiles: path = f'{hacs.core.config_path}/.storage/{s_f}' <IF_STMT> hacs.log.info(f'Cleaning up old storage file {path}') os.remove(path)",if os.path.isfile(path):,if os.path.exists(path):,0.7222496310372594,0.8696398662122882,False
2693,"def action_delete(self, ids): try: count = 0 for pk in ids: <IF_STMT> count += 1 flash(ngettext('Record was successfully deleted.', '%(count)s records were successfully deleted.', count, count=count), 'success') except Exception as ex: flash(gettext('Failed to delete records. %(error)s', error=str(ex)), 'error')",if self.delete_model(self.get_one(pk)):,if self.delete_record(pk):,0.8237654786390625,0.9051034981560222,False
2694,"def test_inclusion(all_values): for values in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_XXX'}, {'guid_2'}]: test_predicate = in_set(values, 'volume_guid') included_values = set() for val in all_values: <IF_STMT> included_values.add(val) assert included_values == all_values.intersection(values)",if test_predicate.do_include({'volume_guid': val}):,if test_predicate(val):,0.9243453446604648,0.8645707301556367,False
2695,"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = mod_attr_path.split('#') if '#' in mod_attr_path else (mod_attr_path, '') full_mod_path = '{}.{}'.format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) if attr_path: for part in attr_path.split('.'): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: <IF_STMT> return None raise ex",if checked:,if checked:,0.9457680973232696,0.926934323706186,True
2696,"def __exit__(self, exc_type, exc_val, exc_tb): if self.fusefat is not None: self.fusefat.send_signal(signal.SIGINT) for count in range(10): time.sleep(0.1) <IF_STMT> break else: self.fusefat.terminate() time.sleep(self.delay) assert not os.path.exists(self.canary) self.dev_null.close() shutil.rmtree(self.tmpdir)",if self.fusefat.poll() is not None:,if count == 10:,0.8801091671313553,0.7965020533851944,False
2697,"def check_context_processors(output): with output.section('Context processors') as section: processors = list(chain(*[template['OPTIONS'].get('context_processors', []) for template in settings.TEMPLATES])) required_processors = ('cms.context_processors.cms_settings',) for processor in required_processors: <IF_STMT> section.error('%s context processor must be in TEMPLATES option context_processors' % processor)",if processor not in processors:,if processor not in processors:,0.8548567431939145,0.8200123297196334,True
2698,"def test_converters(self): response = self._get('datatypes/converters') self._assert_status_code_is(response, 200) converters_list = response.json() found_fasta_to_tabular = False for converter in converters_list: self._assert_has_key(converter, 'source', 'target', 'tool_id') <IF_STMT> found_fasta_to_tabular = True assert found_fasta_to_tabular",if converter['source'] == 'fasta' and converter['target'] == 'tabular':,if converter.get('target_tool_id') == 'fasta':,0.8666665660066133,0.8038019482772603,False
2699,"def remove_pid(self, watcher, pid): if pid in self._pids[watcher]: logger.debug('Removing %d from %s' % (pid, watcher)) self._pids[watcher].remove(pid) <IF_STMT> logger.debug('Stopping the periodic callback for {0}'.format(watcher)) self._callbacks[watcher].stop()",if len(self._pids[watcher]) == 0:,if self._callbacks[watcher]:,0.6114790258688434,0.8466657105524215,False
2700,"def _fc_layer(self, sess, bottom, name, trainable=True, relu=True): with tf.variable_scope(name) as scope: shape = bottom.get_shape().as_list() dim = 1 for d in shape[1:]: dim *= d x = tf.reshape(bottom, [-1, dim]) weight = self._get_fc_weight(sess, name, trainable=trainable) bias = self._get_bias(sess, name, trainable=trainable) fc = tf.nn.bias_add(tf.matmul(x, weight), bias) <IF_STMT> fc = tf.nn.relu(fc) return fc",if relu:,if relu:,0.8888429455753579,0.9284304001296656,True
2701,"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: if root_path: config_root_path = drive.get('root_path') <IF_STMT> return drive elif volume_guid_path: config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",if config_root_path and root_path == config_root_path:,if config_root_path and config_root_path == root_path:,0.8489948340995463,0.7665936070959262,False
2702,"def rewire_init(expr): new_args = [] if expr[0] == HySymbol('setv'): pairs = expr[1:] while len(pairs) > 0: k, v = (pairs.pop(0), pairs.pop(0)) <IF_STMT> v.append(HySymbol('None')) new_args.append(k) new_args.append(v) expr = HyExpression([HySymbol('setv')] + new_args).replace(expr) return expr",if k == HySymbol('__init__'):,if k == HySymbol('None'):,0.7454291426610975,0.8336104423443033,False
2703,"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue <IF_STMT> doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))",if child.tagName == 'Directory':,if child.tagName == 'Directory':,0.7712480610965824,0.8385130047130208,True
2704,"def _v2_common(self, cfg): LOG.debug('v2_common: handling config:\n%s', cfg) if 'nameservers' in cfg: search = cfg.get('nameservers').get('search', []) dns = cfg.get('nameservers').get('addresses', []) name_cmd = {'type': 'nameserver'} <IF_STMT> name_cmd.update({'search': search}) if len(dns) > 0: name_cmd.update({'addresses': dns}) LOG.debug('v2(nameserver) -> v1(nameserver):\n%s', name_cmd) self.handle_nameserver(name_cmd)",if len(search) > 0:,if len(search) > 0:,0.9351480528559688,0.8555308664663046,True
2705,"def __start_element_handler(self, name, attrs): if name == 'mime-type': if self.type: for extension in self.extensions: self[extension] = self.type self.type = attrs['type'].lower() self.extensions = [] elif name == 'glob': pattern = attrs['pattern'] <IF_STMT> self.extensions.append(pattern[1:].lower())",if pattern.startswith('*.'):,if pattern.startswith('.'):,0.679248800992913,0.9120815172610118,False
2706,"def get_attr_by_data_model(self, dmodel, exclude_record=False): if exclude_record: return list(filter(lambda x: x.data_model == dmodel and x.value == '' <IF_STMT> else False, self._inferred_intent)) else: return list(filter(lambda x: x.data_model == dmodel and x.value == '' if hasattr(x, 'data_model') else False, self._inferred_intent))","if x.attribute != 'Record' and hasattr(x, 'data_model')","if hasattr(x, 'data_model')",0.883207283578658,0.8783650674919876,False
2707,"def general(metadata, value): if metadata.get('commands') and value: if not metadata.get('nargs'): v = quote(value) else: v = value return u'{0} {1}'.format(metadata['commands'][0], v) elif not value: return None <IF_STMT> return quote(value) else: return value",elif not metadata.get('nargs'):,elif metadata.get('nargs'):,0.9183447503888316,0.8866029039778043,False
2708,"def get_images(self): images = [] try: tag = MP4(self['~filename']) except Exception: return [] for cover in tag.get('covr', []): <IF_STMT> mime = 'image/jpeg' elif cover.imageformat == MP4Cover.FORMAT_PNG: mime = 'image/png' else: mime = 'image/' f = get_temp_cover_file(cover) images.append(EmbeddedImage(f, mime)) return images",if cover.imageformat == MP4Cover.FORMAT_JPEG:,if cover.imageformat == MP4Cover.FORMAT_JPEG:,0.6523076416782824,0.8661072626070159,True
2709,"def run_cmd(self, util, value): state = util.state if not state.argument_supplied: state.argument_supplied = True if value == 'by_four': state.argument_value = 4 <IF_STMT> state.argument_negative = True else: state.argument_value = value elif value == 'by_four': state.argument_value *= 4 elif isinstance(value, int): state.argument_value *= 10 state.argument_value += value elif value == 'negative': state.argument_value = -state.argument_value",elif value == 'negative':,elif value == 'negative':,0.7684039081846338,0.8944264839442453,True
2710,"def finish_character_data(self): if self.character_data: <IF_STMT> line, column = self.character_pos token = XmlToken(XML_CHARACTER_DATA, self.character_data, None, line, column) self.tokens.append(token) self.character_data = ''",if not self.skip_ws or not self.character_data.isspace():,if self.character_pos:,0.8464899973300513,0.8137489370974955,False
2711,"def check_syntax(filename, raise_error=False): """"""Return True if syntax is okay."""""" with autopep8.open_with_encoding(filename) as input_file: try: compile(input_file.read(), '<string>', 'exec', dont_inherit=True) return True except (SyntaxError, TypeError, UnicodeDecodeError): <IF_STMT> raise else: return False",if raise_error:,if raise_error:,0.9088827574627997,0.8743414417652072,True
2712,"def write(self, file): if not self._been_written: self._been_written = True for attribute, value in self.__dict__.items(): <IF_STMT> self.write_recursive(value, file) w = file.write w('\t%s = {\n' % self._id) w('\t\tisa = %s;\n' % self.__class__.__name__) for attribute, value in self.__dict__.items(): if attribute[0] != '_': w('\t\t%s = %s;\n' % (attribute, self.tostring(value))) w('\t};\n\n')",if attribute[0] != '_':,if attribute == '_':,0.8970314813492629,0.8832000938217648,False
2713,"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() if name is not None: key.name = name <IF_STMT> key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",if metadata is not None:,if metadata is not None:,0.6240859256772989,0.7765145040967655,True
2714,"def fill_buf(self, db, len_=None): with open('/dev/urandom', 'rb') as rfh: first = True for id_, in db.query('SELECT id FROM test'): if len_ is None and first: val = b'' first = False <IF_STMT> val = rfh.read(random.randint(0, 140)) else: val = rfh.read(len_) db.execute('UPDATE test SET buf=? WHERE id=?', (val, id_))",elif len_ is None:,elif len_ is None:,0.8314091520893799,0.8879659171421962,True
2715,"def load_category_from_parser(self, parser): for cate in parser.keys(): id = parser.get_id(cate) <IF_STMT> self._data['cates'][id] = 0 else: self._data['cates'][id] = self.count_unread(id) self._is_init = False self.save()",if self._is_init:,if id not in self._data['cates']:,0.6408498842904488,0.7297349727547102,False
2716,"def after_insert(self): if self.prescription: frappe.db.set_value('Lab Prescription', self.prescription, 'lab_test_created', 1) <IF_STMT> self.invoiced = True if not self.lab_test_name and self.template: self.load_test_from_template() self.reload()","if frappe.db.get_value('Lab Prescription', self.prescription, 'invoiced'):",if not self.invoiced:,0.8724421860839064,0.7739321540474097,False
2717,"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): <IF_STMT> continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True if not add: continue missing.append((source.context, source.source, '')) if missing: self.add_units(None, missing)",if 'terminology' not in source.all_flags:,if not source.context:,0.8774664369017893,0.884617925078158,False
2718,def refresh(self): if self._obj: base = self._db.get_media_from_handle(self._obj.get_reference_handle()) <IF_STMT> self._title = base.get_description() self._value = base.get_path(),if base:,if base:,0.5674229056416316,0.7241577342575828,True
2719,"def _set_parse_context(self, tag, tag_attrs): if not self._wb_parse_context: if tag == 'style': self._wb_parse_context = 'style' <IF_STMT> if self._allow_js_type(tag_attrs): self._wb_parse_context = 'script'",elif tag == 'script':,elif tag == 'script':,0.5986051413921537,0.7378351342269067,True
2720,"def can_read(self): if hasattr(self.file, '__iter__'): iterator = iter(self.file) head = next(iterator, None) if head is None: self.repaired = [] return True <IF_STMT> self.repaired = itertools.chain([head], iterator) return True else: raise IOSourceError('Could not open source: %r (mode: %r)' % (self.file, self.options['mode'])) return False","if isinstance(head, str):","elif isinstance(head, list):",0.9110781820399597,0.8928756684056034,False
2721,"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get('headers') is not None: if kwargs['headers'].get('user-agent'): <IF_STMT> kwargs['headers']['user-agent'] = f""{user_agent} {kwargs['headers']['user-agent']}"" else: kwargs['headers']['user-agent'] = user_agent else: kwargs['headers'] = {'user-agent': user_agent} return request_method(*args, **kwargs)",if user_agent not in kwargs['headers']['user-agent']:,if user_agent in kwargs['headers'].keys():,0.6753942636525407,0.8474968231198384,False
2722,"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper <IF_STMT> ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",elif self._naive or not self._joins or self.verify_naive():,elif self._naive:,0.8793170470929081,0.926934323706186,False
2723,"def populate_data(apps, schema_editor): Menu = apps.get_model('menu', 'Menu') for menu in Menu.objects.all(): <IF_STMT> json_str = menu.json_content while isinstance(json_str, str): json_str = json.loads(json_str) menu.json_content_new = json_str menu.save()","if isinstance(menu.json_content, str):","if isinstance(menu, Menu):",0.7990931342667392,0.8196189957582152,False
2724,"def virtualenv_exists(self): if os.path.exists(self.virtualenv_location): <IF_STMT> extra = ['Scripts', 'activate.bat'] else: extra = ['bin', 'activate'] return os.path.isfile(os.sep.join([self.virtualenv_location] + extra)) return False",if os.name == 'nt':,if os.path.isfile(self.virtualenv_location):,0.8776410633741687,0.8137489370974955,False
2725,"def get_minkowski_function(name, variable): fn_name = name + get_postfix(variable) if hasattr(MEB, fn_name): return getattr(MEB, fn_name) el<IF_STMT> raise ValueError(f'Function {fn_name} not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`.') else: raise ValueError(f'Function {fn_name} not available.')",if variable.is_cuda:,if torch.cuda.is_available():,0.6197571902601234,0.8901732118131125,False
2726,"def build_temp_workspace(files): tempdir = tempfile.mkdtemp(prefix='yamllint-tests-') for path, content in files.items(): path = os.path.join(tempdir, path).encode('utf-8') <IF_STMT> os.makedirs(os.path.dirname(path)) if type(content) is list: os.mkdir(path) else: mode = 'wb' if isinstance(content, bytes) else 'w' with open(path, mode) as f: f.write(content) return tempdir",if not os.path.exists(os.path.dirname(path)):,if not os.path.exists(os.path.dirname(path)):,0.72261529880014,0.8815741981066073,True
2727,"def clean_form(self, request, user, form, cleaned_data): for field in self.get_fields(): <IF_STMT> continue try: cleaned_data[field.fieldname] = field.clean(request, user, cleaned_data[field.fieldname]) except ValidationError as e: form.add_error(field.fieldname, e) return cleaned_data",if field.fieldname not in cleaned_data:,if field.is_hidden:,0.8788728193682389,0.8590888738245122,False
2728,"def setUp(self): self.realm = service.InMemoryWordsRealm('realmname') self.checker = checkers.InMemoryUsernamePasswordDatabaseDontUse() self.portal = portal.Portal(self.realm, [self.checker]) self.factory = service.IRCFactory(self.realm, self.portal) c = [] for nick in self.STATIC_USERS: <IF_STMT> nick = nick.decode('utf-8') c.append(self.realm.createUser(nick)) self.checker.addUser(nick, nick + '_password') return DeferredList(c)","if isinstance(nick, bytes):","if isinstance(nick, bytes):",0.7401541507996925,0.8675979125638379,True
2729,"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes['seq_num'])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 <IF_STMT> if not self.done_future.done(): self.done_future.set_result(None)",if self.completed_calls >= self._resolve_at_msg_count:,if self.completed_calls == self.max_pending_ack:,0.8490140893359138,0.8105932471967202,False
2730,"def fill_in_standard_formats(book): for x in std_format_code_types.keys(): <IF_STMT> ty = std_format_code_types[x] fmt_str = std_format_strings.get(x) fmtobj = Format(x, ty, fmt_str) book.format_map[x] = fmtobj",if x not in book.format_map:,if x in book.format_map:,0.7071356061271832,0.7498810286408993,False
2731,"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): if bigger_than_3_only and less_than_7_only and (i == 4): continue if bigger_than_3_only and i <= 3: continue <IF_STMT> continue if even_only and i % 2 != 0: continue result.append(i) return result",if less_than_7_only and i >= 7:,if less_than_7_only and i >= 7:,0.8985061381075095,0.8302012483091993,True
2732,"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser('python') for i, line in enumerate(lines): <IF_STMT> parser.read_line(line) continue parser.read_line(line) if not line.strip(): if i > 0 and (not lines[i - 1].strip()): return False continue if line.startswith('def ') or line.startswith('class '): return True if line.startswith(('#', '@', ' ', ')')): continue return False return False",if parser.is_quoted():,if line.startswith('#'):,0.9597406131353523,0.9434724611166208,False
2733,"def __getattr__(self, key): for tag in self.tag.children: if tag.name not in ('input',): continue <IF_STMT> from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError","if 'name' in tag.attrs and tag.attrs['name'] in (key,):",if tag.name == key:,0.8376430540724911,0.7709002428237395,False
2734,"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x if x > self.box[2]: self.box[2] = x if y < self.box[1]: self.box[1] = y <IF_STMT> self.box[3] = y",if y > self.box[3]:,if y > self.box[3]:,0.7793112550235813,0.8592377270804451,True
2735,def find_shell(): global DEFAULT_SHELL if not DEFAULT_SHELL: for shell in propose_shell(): <IF_STMT> DEFAULT_SHELL = shell break if not DEFAULT_SHELL: DEFAULT_SHELL = '/bin/sh' return DEFAULT_SHELL,"if os.path.isfile(shell) and os.access(shell, os.X_OK):",if shell.is_file():,0.8601759492747569,0.8466657105524215,False
2736,"def addAggregators(sheet, cols, aggrnames): """"""Add each aggregator in list of *aggrnames* to each of *cols*."""""" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: if not hasattr(c, 'aggregators'): c.aggregators = [] <IF_STMT> c.aggregators += [aggr]",if aggr and aggr not in c.aggregators:,"if isinstance(aggr, list):",0.8573879701152269,0.9081987180086649,False
2737,"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.pathAbsoluteFromProjectEncoded()) if len(items) > 0: sublime.set_clipboard('\n'.join(items)) <IF_STMT> sublime.status_message('Items copied') else: sublime.status_message('Item copied')",if len(items) > 1:,if len(items) > 1:,0.7413153949647944,0.760856626273165,True
2738,"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: <IF_STMT> msg = 'This account is already in use.' raise AuthAlreadyAssociated(backend, msg) elif not user: user = social.user return {'social': social, 'user': user, 'is_new': user is None, 'new_association': social is None}",if user and social.user != user:,if user is not None:,0.9231344441474993,0.8649799950178215,False
2739,"def _text(bitlist): out = '' for typ, text in bitlist: if not typ: out += text elif typ == 'em': out += '\\fI%s\\fR' % text <IF_STMT> out += '\\fB%s\\fR' % text else: raise ValueError('unexpected tag %r inside text' % (typ,)) out = out.strip() out = re.sub(re.compile('^\\s+', re.M), '', out) return out","elif typ in ['strong', 'code']:",elif typ == 'b':,0.8892449911619418,0.8944264839442453,False
2740,"def OnRadioSelect(self, event): fitID = self.mainFrame.getActiveFit() if fitID is not None: self.mainFrame.command.Submit(cmd.GuiChangeImplantLocationCommand(fitID=fitID, source=ImplantLocation.FIT <IF_STMT> else ImplantLocation.CHARACTER))",if self.rbFit.GetValue(),if fitID,0.563144713311496,0.7828161456481266,False
2741,"def hexdump(data): """"""yield lines with hexdump of data"""""" values = [] ascii = [] offset = 0 for h, a in sixteen(data): <IF_STMT> yield (offset, ' '.join([''.join(values), ''.join(ascii)])) del values[:] del ascii[:] offset += 16 else: values.append(h) ascii.append(a)",if h is None:,if h == 'x':,0.9281126842010976,0.8592377270804451,False
2742,"def submit(self): bot_token = self.config['bot_token'] chat_ids = self.config['chat_id'] chat_ids = [chat_ids] if isinstance(chat_ids, str) else chat_ids text = '\n'.join(super().submit()) if not text: logger.debug('Not calling telegram API (no changes)') return result = None for chunk in chunkstring(text, self.MAX_LENGTH, numbering=True): for chat_id in chat_ids: res = self.submitToTelegram(bot_token, chat_id, chunk) <IF_STMT> result = res return result",if res.status_code != requests.codes.ok or res is None:,if res:,0.9517484532207119,0.9325718821645923,False
2743,"def onMessage(self, payload, isBinary): if not isBinary: self.result = 'Expected binary message with payload, but got binary.' el<IF_STMT> self.result = 'Expected binary message with payload of length %d, but got %d.' % (self.DATALEN, len(payload)) else: self.behavior = Case.OK self.result = 'Received binary message of length %d.' % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len(payload) != self.DATALEN:,if self.DATALEN != len(payload):,0.7302281503496043,0.8964173245779284,False
2744,"def verify_output(actual, expected): actual = _read_file(actual, 'Actual') expected = _read_file(join(CURDIR, expected), 'Expected') if len(expected) != len(actual): raise AssertionError('Lengths differ. Expected %d lines but got %d' % (len(expected), len(actual))) for exp, act in zip(expected, actual): tester = fnmatchcase if '*' in exp else eq <IF_STMT> raise AssertionError('Lines differ.\nExpected: %s\nActual:   %s' % (exp, act))","if not tester(act.rstrip(), exp.rstrip()):",if not tester(act):,0.9390994900806369,0.9134996171406936,False
2745,"def _in_out_vector_helper(self, name1, name2, ceil): vector = [] stats = self.record if ceil is None: ceil = self._get_max_rate(name1, name2) maxlen = self.config.get_stats_history_length() for n in [name1, name2]: for i in range(maxlen + 1): <IF_STMT> vector.append(float(stats[i][n]) / ceil) else: vector.append(0.0) return vector",if i < len(stats):,if stats[i][n] > ceil:,0.9268044413991738,0.8692960007731574,False
2746,"def _init_param(param, mode): if isinstance(param, str): param = _resolve(param) elif isinstance(param, (list, tuple)): param = [_init_param(p, mode) for p in param] elif isinstance(param, dict): <IF_STMT> param = from_params(param, mode=mode) else: param = {k: _init_param(v, mode) for k, v in param.items()} return param","if {'ref', 'class_name', 'config_path'}.intersection(param.keys()):","if isinstance(param, dict):",0.6759432413817943,0.8928756684056034,False
2747,"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for page, soup in soups.items(): for a in soup.find_all('a'): if not a.has_attr('pantsref'): continue pantsref = a['pantsref'] <IF_STMT> raise TaskError(f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it') a['href'] = rel_href(page, precomputed.pantsref[pantsref])",if pantsref not in precomputed.pantsref:,if pantsref not in precomputed.pantsref:,0.7108832751438678,0.8591169759078797,True
2748,"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <IF_STMT> return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",elif '.' in svalue:,"elif isinstance(svalue, (float, float)):",0.640453816279596,0.8038019482772603,False
2749,"def default(self, o): try: <IF_STMT> return str(o) else: if hasattr(o, 'profile'): del o.profile if hasattr(o, 'credentials'): del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path if hasattr(o, 'services_config'): del o.services_config return vars(o) except Exception as e: return str(o)",if type(o) == datetime.datetime:,"if isinstance(o, dict):",0.9046702361356156,0.8749766281017177,False
2750,"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: <IF_STMT> return ['-%s' % name] elif value not in (False, None): if split_single_char_options: return ['-%s' % name, '%s' % value] else: return ['-%s%s' % (name, value)] elif value is True: return ['--%s' % dashify(name)] elif value is not False and value is not None: return ['--%s=%s' % (dashify(name), value)] return []",if value is True:,"if value in (True, None):",0.6522942942803043,0.8954283587198554,False
2751,"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: <IF_STMT> return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: if context.rounding == ROUND_FLOOR: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context.rounding == ROUND_CEILING:,if context.rounding == ROUND_FLOOR:,0.9237448022685768,0.8944264839442453,False
2752,"def OnLeftUp(self, event): if self.Drawing: self.Drawing = False <IF_STMT> world_rect = (self.Canvas.PixelToWorld(self.RBRect[0]), self.Canvas.ScalePixelToWorld(self.RBRect[1])) wx.CallAfter(self.CallBack, world_rect) self.RBRect = None",if self.RBRect:,if self.RBRect:,0.6050570884184865,0.7912619863720214,True
2753,"def _map_answers(answers): result = [] for a in answers.split('|'): user_answers = [] result.append(dict(sourcerAnswers=user_answers)) for r in a.split(','): <IF_STMT> user_answers.append(dict(noAnswer=True)) else: start_, end_ = map(int, r.split(':')) user_answers.append(dict(s=start_, e=end_)) return result",if r == 'None':,if r == '':,0.8340848629174809,0.8169276475307028,False
2754,"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: <IF_STMT> parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return (edges, bbox)",if d.GetLayer() == pcbnew.Edge_Cuts:,if d.IsVisible():,0.9569081646587217,0.9237460349978159,False
2755,"def get_size(self): size = self.start_size for operation in self.ran_operations: <IF_STMT> size = operation[1][0] elif operation[0] == 'crop': crop = operation[1][0] size = (crop[2] - crop[0], crop[3] - crop[1]) return size",if operation[0] == 'resize':,if operation[0] == 'size':,0.8795077628040104,0.8228500218338367,False
2756,"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) <IF_STMT> create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) if account.discriminator == 'gmailaccount': set_labels_for_imapuids(account, db_session) db_session.commit()",if account.discriminator == 'easaccount':,if account.discriminator == 'easfoldersyncstatuses':,0.8164968233430865,0.8228500218338367,False
2757,"def OnEndDrag(self, event): self.StopDragging() dropTarget = event.GetItem() if not dropTarget: dropTarget = self.GetRootItem() if self.IsValidDropTarget(dropTarget): self.UnselectAll() <IF_STMT> self.SelectItem(dropTarget) self.OnDrop(dropTarget, self._dragItem)",if dropTarget != self.GetRootItem():,if self.IsValidDropItem(dropTarget):,0.8774871449192684,0.8137489370974955,False
2758,"def validate(self, frame, value): if self.sep and isinstance(value, string_types): value = value.split(self.sep) if isinstance(value, list): <IF_STMT> return [self.specs[0].validate(frame, v) for v in value] else: return [[s.validate(frame, v) for v, s in izip(val, self.specs)] for val in value] raise ValueError('Invalid MultiSpec data: %r' % value)",if len(self.specs) == 1:,if len(value) == 1:,0.6537756837299246,0.8780099567239787,False
2759,"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None): QNetBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): <IF_STMT> action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32) self._action_space = action_space self._append_output_layer()",if action_space is None:,if action_space is None:,0.8447617800391081,0.7378351342269067,True
2760,"def n_weights(self): """"""Return the number of weights (parameters) in this network."""""" n_weights = 0 for i, w in enumerate(self.all_weights): n = 1 for s in w.get_shape(): try: s = int(s) except: s = 1 <IF_STMT> n = n * s n_weights = n_weights + n return n_weights",if s:,if s > 0:,0.9001228934011613,0.8856327184319047,False
2761,"def _arg_desc(name, ctx): for param in ctx.command.params: if param.name == name: desc = param.opts[-1] <IF_STMT> desc = param.human_readable_name return desc raise AssertionError(name)",if desc[0] != '-':,elif param.human_readable_name:,0.6513834605372618,0.8318180062062374,False
2762,"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any((fnmatch(name, pattern) for pattern in basename_ignore)): continue path = path_so_far + '/' + name if path_so_far else name if any((fnmatch(path, pattern) for pattern in path_ignore)): continue full_name = os.path.join(directory, name) if os.path.isdir(full_name): for file_path in walk(full_name, path): yield file_path <IF_STMT> yield path",elif os.path.isfile(full_name):,elif os.path.isfile(path):,0.9531901727313741,0.9337818055790471,False
2763,"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): if dst.is_id('IRDst'): <IF_STMT> raise ValueError('Multiple destinations!') final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",if final_dst is not None:,if len(src.get_id()):,0.9307120760269461,0.9051034981560222,False
2764,"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} <IF_STMT> filters['resource_ref'] = args.resource_ref if args.policy_type: filters['policy_type'] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",if args.resource_ref:,if args.resource_ref:,0.7554415085884594,0.8590888738245122,True
2765,"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): if other_folder == folder: continue if other_path == path: duplicates.append(other_folder) <IF_STMT> self.duplicates[folder] = duplicates",if len(duplicates):,if duplicates:,0.9338681904722137,0.8996480074924822,False
2766,"def limit_clause(self, select, **kw): text = '' if select._limit_clause is not None: text += '\n LIMIT ' + self.process(select._limit_clause, **kw) if select._offset_clause is not None: <IF_STMT> text += '\n LIMIT ' + self.process(sql.literal(-1)) text += ' OFFSET ' + self.process(select._offset_clause, **kw) else: text += ' OFFSET ' + self.process(sql.literal(0), **kw) return text",if select._limit_clause is None:,if select._offset_clause:,0.7008730920246391,0.9312457603037672,False
2767,"def _get_activation(self, act): """"""Get activation block based on the name."""""" if isinstance(act, str): if act.lower() == 'gelu': return GELU() <IF_STMT> return GELU(approximate=True) else: return gluon.nn.Activation(act) assert isinstance(act, gluon.Block) return act",elif act.lower() == 'approx_gelu':,elif act.lower() == 'gelu':,0.8432067800530432,0.8228500218338367,False
2768,"def __eq__(self, other): try: if self.type != other.type: return False if self.type == 'ASK': return self.askAnswer == other.askAnswer <IF_STMT> return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",elif self.type == 'SELECT':,if self.type == 'VARIABLE':,0.6124322451955927,0.8474968231198384,False
2769,"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): <IF_STMT> open_tags += 1 elif node.tag_type == CLOSE_TAG: open_tags -= 1 elif isinstance(node, HtmlDataFragment) and node.is_text_content and (open_tags == 0): text.append(html_body[node.start:node.end]) return text",if node.tag_type == OPEN_TAG:,if node.tag_type == OPEN_TAG:,0.9086838058982806,0.8627586293513119,True
2770,"def test_do_change(self): """"""Test if VTK object changes when trait is changed."""""" p = Prop() p.edge_visibility = not p.edge_visibility p.representation = 'p' p.opacity = 0.5 p.color = (0, 1, 0) p.diffuse_color = (1, 1, 1) p.specular_color = (1, 1, 0) for t, g in p._updateable_traits_: val = getattr(p._vtk_obj, g)() <IF_STMT> self.assertEqual(val, getattr(p, t + '_')) else: self.assertEqual(val, getattr(p, t))",if t == 'representation':,if t == 'edge':,0.657850759809794,0.9069443196104878,False
2771,"def update_item(source_doc, target_doc, source_parent): target_doc.t_warehouse = '' if source_doc.material_request_item and source_doc.material_request: add_to_transit = frappe.db.get_value('Stock Entry', source_name, 'add_to_transit') <IF_STMT> warehouse = frappe.get_value('Material Request Item', source_doc.material_request_item, 'warehouse') target_doc.t_warehouse = warehouse target_doc.s_warehouse = source_doc.t_warehouse target_doc.qty = source_doc.qty - source_doc.transferred_qty",if add_to_transit:,if add_to_transit:,0.735843765942701,0.8996480074924822,True
2772,"def get_drive(self, root_path='', volume_guid_path=''): for drive in self.drives: <IF_STMT> config_root_path = drive.get('root_path') if config_root_path and root_path == config_root_path: return drive elif volume_guid_path: config_volume_guid_path = drive.get('volume_guid_path') if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",if root_path:,if root_path:,0.8050178311485728,0.8901732118131125,True
2773,"def f_freeze(_): repos = utils.get_repos() for name, path in repos.items(): url = '' cp = subprocess.run(['git', 'remote', '-v'], cwd=path, capture_output=True) <IF_STMT> url = cp.stdout.decode('utf-8').split('\n')[0].split()[1] print(f'{url},{name},{path}')",if cp.returncode == 0:,if cp.returncode == 0:,0.7614621907187382,0.7886336751695258,True
2774,"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): <IF_STMT> raise RuntimeError('only contiguous arrays may be used as arguments to this operation') if self.flags.f_contiguous: order = 'F' else: order = 'C' result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call(self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size) return result else: return self",if not self.flags.forc:,if self.flags.contiguous:,0.6413420437986777,0.9237460349978159,False
2775,"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: if function.is_implemented: <IF_STMT> continue self._explore(function.entry_point, []) function.context[self.KEY] = True",if self.KEY in function.context:,if self.KEY in function.context:,0.8228595165597833,0.674945488826271,True
2776,"def test_default_configuration_no_encoding(self): transformations = [] for i in range(2): transformation, original = _test_preprocessing(NoEncoding) self.assertEqual(transformation.shape, original.shape) self.assertTrue((transformation == original).all()) transformations.append(transformation) <IF_STMT> self.assertTrue((transformations[-1] == transformations[-2]).all())",if len(transformations) > 1:,if len(transformations) > 1:,0.7602216659736736,0.7709002428237395,True
2777,"def main(): """"""main function"""""" parser = argparse.ArgumentParser(description='Let a cow speak for you') parser.add_argument('text', nargs='*', default=None, help='text to say') ns = parser.parse_args() if ns.text is None or len(ns.text) == 0: text = '' while True: inp = sys.stdin.read(4096) if inp.endswith('\n'): inp = inp[:-1] <IF_STMT> break text += inp else: text = ' '.join(ns.text) cow = get_cow(text) print(cow)",if not inp:,if not inp:,0.6841119950946826,0.9196822664155297,True
2778,"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == 'i386': reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == 'amd64': reg = emu.getRegister(envi.archs.amd64.REG_RDI) <IF_STMT> self.vw.makePointer(reg, follow=True)",if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:,elif self.arch == 'vw':,0.9220733230635528,0.8464618838623736,False
2779,"def get_boarding_status(project): status = 'Pending' if project: doc = frappe.get_doc('Project', project) if flt(doc.percent_complete) > 0.0 and flt(doc.percent_complete) < 100.0: status = 'In Process' <IF_STMT> status = 'Completed' return status",elif flt(doc.percent_complete) == 100.0:,elif doc.percent_complete == 0.0 and doc.percent_complete > 100.0:,0.6384498504013406,0.7221052041889016,False
2780,"def set_weights(self, new_weights): weights = self.get_weights() if len(weights) != len(new_weights): raise ValueError('len of lists mismatch') tuples = [] for w, new_w in zip(weights, new_weights): <IF_STMT> new_w = new_w.reshape(w.shape) tuples.append((w, new_w)) nn.batch_set_value(tuples)",if len(w.shape) != new_w.shape:,"if isinstance(new_w, nn.BatchNorm):",0.9113645432867205,0.8547305998833805,False
2781,"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs['setting'] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, '') value = kwargs['value'] if setting in DEFAULTS.keys(): <IF_STMT> setattr(json_api_settings, setting, value) elif hasattr(json_api_settings, setting): delattr(json_api_settings, setting)",if value is not None:,if value:,0.9044532044447049,0.8590888738245122,False
2782,"def knamn(self, sup, cdict): cname = cdict[sup].class_name if not cname: namesp, tag = cdict[sup].name.split('.') <IF_STMT> ctag = self.root.modul[namesp].factory(tag).__class__.__name__ cname = '%s.%s' % (namesp, ctag) else: cname = tag + '_' return cname",if namesp:,if namesp in self.root.modul:,0.7249675917201501,0.8336104423443033,False
2783,"def setdefault(self, key, default=None): try: o = self.data[key]() except KeyError: o = None if o is None: <IF_STMT> self._commit_removals() self.data[key] = KeyedRef(default, self._remove, key) return default else: return o",if self._pending_removals:,if self._commit_removals:,0.6421978083544909,0.8743414417652072,False
2784,"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) <IF_STMT> self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): if self.list_ctrl.IsSelected(index): self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",if event.EventObject is self.list_ctrl:,if self.input_list_ctrl.GetItemCount() == 0:,0.8287723149948245,0.760856626273165,False
2785,"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): if i == j: continue edge = (nodes[i], nodes[j]) <IF_STMT> graph.del_edge(edge) graph.add_edge(edge, 1)",if graph.has_edge(edge):,if graph.has_edge(edge):,0.8450639863572782,0.8590888738245122,True
2786,"def _parse_param_value(name, datatype, default): if datatype == 'bool': <IF_STMT> return True elif default.lower() == 'false': return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == 'int': if type(default) == int: return default else: return int(default, 0) elif datatype == 'real': if type(default) == float: return default else: return float(default) else: return str(default)",if default.lower() == 'true':,if default.lower() == 'true':,0.9300382135839937,0.914208565914368,True
2787,"def get_size(self, shape_info): state = np.random.RandomState().get_state() size = 0 for elem in state: <IF_STMT> size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype('int').itemsize elif isinstance(elem, float): size += np.dtype('float').itemsize else: raise NotImplementedError() return size","if isinstance(elem, str):","if isinstance(elem, (list, tuple)):",0.8970027478632339,0.8723360571509826,False
2788,"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): if name not in subst: subst[name] = var <IF_STMT> subst[name].PasteVariable(var) return subst",elif subst[name] is not var:,"elif isinstance(subst[name], PasteVariable):",0.778669398262849,0.8390782502060267,False
2789,"def _load_weights_if_possible(self, model, init_weight_path=None): """"""Loads model weights when it is provided."""""" if init_weight_path: logging.info('Load weights: {}'.format(init_weight_path)) <IF_STMT> checkpoint = tf.train.Checkpoint(model=model, optimizer=self._create_optimizer()) checkpoint.restore(init_weight_path) else: model.load_weights(init_weight_path) else: logging.info('Weights not loaded from path:{}'.format(init_weight_path))",if self.use_tpu:,if self._save_weights:,0.6961902639313603,0.8787142254774354,False
2790,"def _cleanup_inactive_receivexlogs(self, site): if site in self.receivexlogs: if not self.receivexlogs[site].running: <IF_STMT> self.receivexlogs[site].join() del self.receivexlogs[site]",if self.receivexlogs[site].is_alive():,if self.receivexlogs[site].is_active:,0.7790090439229502,0.7241577342575828,False
2791,"def get_asset(self, path): """"""Loads an asset by path."""""" clean_path = cleanup_path(path).strip('/') nodes = [self.asset_root] + self.theme_asset_roots for node in nodes: for piece in clean_path.split('/'): node = node.get_child(piece) <IF_STMT> break if node is not None: return node return None",if node is None:,if node is None:,0.8896073946984673,0.8592377270804451,True
2792,"def palindromic_substrings(s): if not s: return [[]] results = [] for i in range(len(s), 0, -1): sub = s[:i] <IF_STMT> for rest in palindromic_substrings(s[i:]): results.append([sub] + rest) return results",if sub == sub[::-1]:,if sub:,0.8269261109486847,0.8743414417652072,False
2793,"def debug_tree(tree): l = [] for elt in tree: <IF_STMT> l.append(_names.get(elt, elt)) elif isinstance(elt, str): l.append(elt) else: l.append(debug_tree(elt)) return l","if isinstance(elt, (int, long)):","if isinstance(elt, (list, tuple)):",0.8527102317041406,0.7378351342269067,False
2794,"def shared_username(account): username = os.environ.get('SHARED_USERNAME', 'PKKid') for user in account.users(): <IF_STMT> return username elif user.username and user.email and user.id and (username.lower() in (user.username.lower(), user.email.lower(), str(user.id))): return username pytest.skip('Shared user %s wasn`t found in your MyPlex account' % username)",if user.title.lower() == username.lower():,"if user.username and username.lower() in (user.username.lower(), user.email.lower(), str(user.id)):",0.8713448648884885,0.7801724613878526,False
2795,"def process_schema_element(self, e): if e.name is None: return self.debug1('adding element: %s', e.name) t = self.get_type(e.type) if t: <IF_STMT> del self.pending_elements[e.name] self.retval[self.tns].elements[e.name] = e else: self.pending_elements[e.name] = e",if e.name in self.pending_elements:,if e.name in self.pending_elements:,0.6628429571108311,0.8038019482772603,True
2796,"def __setitem__(self, key, value): with self._lock: try: link = self._get_link_and_move_to_front_of_ll(key) except KeyError: <IF_STMT> self._set_key_and_add_to_front_of_ll(key, value) else: evicted = self._set_key_and_evict_last_in_ll(key, value) super(LRI, self).__delitem__(evicted) super(LRI, self).__setitem__(key, value) else: link[VALUE] = value",if len(self) < self.max_size:,if LINK is None:,0.8272018209093536,0.8169276475307028,False
2797,"def __delattr__(self, name): if name == '__dict__': raise AttributeError(""%r object attribute '__dict__' is read-only"" % self.__class__.__name__) if name in self._local_type_vars: <IF_STMT> type_attr = getattr(self._local_type, name, _marker) type(type_attr).__delete__(type_attr, self) return dct = _local_get_dict(self) try: del dct[name] except KeyError: raise AttributeError(name)",if name in self._local_type_del_descriptors:,if self._local_type:,0.9152193710439521,0.9076141716697395,False
2798,"def update_participants(self, refresh=True): for participant in list(self.participants_dict): <IF_STMT> continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: if participant in self.participants_dict: self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",if participant is None or participant == self.simulator_config.broadcast_part:,if participant in self.participant_items:,0.8935306214037521,0.8038019482772603,False
2799,"def insert_bigger_b_add(node): if node.op == theano.tensor.add: inputs = list(node.inputs) <IF_STMT> inputs[-1] = theano.tensor.concatenate((inputs[-1], inputs[-1])) return [node.op(*inputs)] return False",if inputs[-1].owner is None:,if len(inputs) > 1:,0.7467286061907361,0.7098232254187811,False
2800,"def _activate_cancel_status(self, cancel_status): if self._cancel_status is not None: self._cancel_status._tasks.remove(self) self._cancel_status = cancel_status if self._cancel_status is not None: self._cancel_status._tasks.add(self) <IF_STMT> self._attempt_delivery_of_any_pending_cancel()",if self._cancel_status.effectively_cancelled:,if self._cancel_status == CancelStatus.CANCELLED:,0.603719846415,0.7771475625141703,False
2801,"def writeLibraryGeometry(fp, meshes, config, shapes=None): progress = Progress(len(meshes), None) fp.write('\n  <library_geometries>\n') for mIdx, mesh in enumerate(meshes): <IF_STMT> shape = None else: shape = shapes[mIdx] writeGeometry(fp, mesh, config, shape) progress.step() fp.write('  </library_geometries>\n')",if shapes is None:,if shapes is None:,0.8728381808157704,0.828399516355805,True
2802,"def init_module_config(module_json, config, config_path=default_config_path): if 'config' in module_json['meta']: if module_json['meta']['config']: if module_json['name'] not in config: config.add_section(module_json['name']) for config_var in module_json['meta']['config']: <IF_STMT> config.set(module_json['name'], config_var, '') return config",if config_var not in config[module_json['name']]:,if config_var.startswith(config_path):,0.9156328868448307,0.8590888738245122,False
2803,"def get_const_defines(flags, prefix=''): defs = [] for k, v in globals().items(): if isinstance(v, int): if v & flags: <IF_STMT> if k.startswith(prefix): defs.append(k) else: defs.append(k) return defs",if prefix:,if prefix:,0.9111769931818466,0.8590888738245122,True
2804,"def __init__(self, source, encoding=DEFAULT_ENCODING): self.data = {} with open(source, encoding=encoding) as file_: for line in file_: line = line.strip() <IF_STMT> continue k, v = line.split('=', 1) k = k.strip() v = v.strip() if len(v) >= 2 and (v[0] == ""'"" and v[-1] == ""'"" or (v[0] == '""' and v[-1] == '""')): v = v.strip('\'""') self.data[k] = v",if not line or line.startswith('#') or '=' not in line:,if not line:,0.7627007109965721,0.9224532597476077,False
2805,"def __detect_console_logger(self): logger = self.log while logger: for handler in logger.handlers[:]: <IF_STMT> if handler.stream in (sys.stdout, sys.stderr): self.logger_handlers.append(handler) if logger.root == logger: break else: logger = logger.root","if isinstance(handler, StreamHandler):","if isinstance(handler, logging.FileHandler):",0.8924003551933105,0.833078701050083,False
2806,"def check_heuristic_in_sql(): heurs = set() excluded = ['Equal assembly or pseudo-code', 'All or most attributes'] for heur in HEURISTICS: name = heur['name'] <IF_STMT> continue sql = heur['sql'] if sql.lower().find(name.lower()) == -1: print('SQL command not correctly associated to %s' % repr(name)) print(sql) assert sql.find(name) != -1 heurs.add(name) print('Heuristics:') import pprint pprint.pprint(heurs)",if name in excluded:,if name in excluded:,0.9403592894678008,0.8923575006167597,True
2807,"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: <IF_STMT> self.seek_to_block(self.cursor) part = self.current_stream.read(size) if size > 0: if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",if not self.in_current_block(self.cursor):,if self.current_stream is None:,0.9033310334911479,0.8661072626070159,False
2808,"def get_project_dir(env): project_file = workon_home / env / '.project' if project_file.exists(): with project_file.open() as f: project_dir = f.readline().strip() <IF_STMT> return project_dir else: err('Corrupted or outdated:', project_file, '\nDirectory', project_dir, ""doesn't exist."")",if os.path.exists(project_dir):,if project_dir:,0.7134240128838333,0.8787142254774354,False
2809,"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <IF_STMT> curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] return tf.keras.backend.stop_gradient(new_mem)",if reuse_len is not None and reuse_len > 0:,if reuse_len is not None:,0.6997524765185112,0.845713978670975,False
2810,"def cleanup_channel(self, to_cleanup): public_key, id_ = to_cleanup try: with db_session: channel = self.session.mds.ChannelMetadata.get_for_update(public_key=public_key, id_=id_) <IF_STMT> return channel.local_version = 0 channel.contents.delete(bulk=True) except Exception as e: self._logger.warning('Exception while cleaning unsubscribed channel: %', str(e))",if not channel:,if channel is None:,0.6292967931783612,0.828399516355805,False
2811,"def best_image(width, height): image = images[0] for img in images: if img.width == width and img.height == height: return img <IF_STMT> image = img return image",elif img.width >= width and img.width * img.height > image.width * image.height:,if img.width == width and img.height == height:,0.8360121461851329,0.6952219386678455,False
2812,"def add_peer_to_blob(self, contact: 'KademliaPeer', key: bytes) -> None: now = self.loop.time() if key in self._data_store: current = list(filter(lambda x: x[0] == contact, self._data_store[key])) <IF_STMT> self._data_store[key][self._data_store[key].index(current[0])] = (contact, now) else: self._data_store[key].append((contact, now)) else: self._data_store[key] = [(contact, now)]",if len(current) > 0:,if current:,0.698533293164248,0.8996480074924822,False
2813,"def dump(self): self.ql.log.info('[*] Dumping object: %s' % self.sf_name) for field in self._fields_: if isinstance(getattr(self, field[0]), POINTER64): self.ql.log.info('%s: 0x%x' % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info('%s: %d' % (field[0], getattr(self, field[0]))) <IF_STMT> self.ql.log.info('%s: %s' % (field[0], getattr(self, field[0]).decode()))","elif isinstance(getattr(self, field[0]), bytes):","elif isinstance(getattr(self, field[0]), bytes):",0.9228512944496602,0.8627586293513119,True
2814,"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter('default_address_space') for vma in task.mm.mmap.walk_list('vm_next'): start = vma.vm_start end = vma.vm_end <IF_STMT> continue if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 4096): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield (vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr)))",if end < self.plugin_args.start:,if start == self.plugin_args.start:,0.9330917871042056,0.8592377270804451,False
2815,"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get('name', '') if name.startswith(expr): if name not in found_names: found_names.add(name) ilk = child.get('ilk') or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) <IF_STMT> break return sorted(cplns, key=operator.itemgetter(1))",if not scoperef:,if not scoperef:,0.9462582228430388,0.9022045190074797,True
2816,"def get_xenapi_host(self): """"""Return the xenapi host on which nova-compute runs on."""""" with self._get_session() as session: <IF_STMT> return session.xenapi.host.get_by_uuid(self.host_uuid) else: return session.xenapi.session.get_this_host(session.handle)",if self.host_uuid:,if self.host_uuid:,0.8553388244078619,0.8232490471721702,True
2817,def stream_docker_log(log_stream): async for line in log_stream: <IF_STMT> logger.debug(line['stream'].strip()) elif 'status' in line: logger.debug(line['status'].strip()) elif 'error' in line: logger.error(line['error'].strip()) raise DockerBuildError,if 'stream' in line and line['stream'].strip():,if 'stream' in line:,0.851211755375209,0.7498810286408993,False
2818,"def test_wildcard_import(): bonobo = __import__('bonobo') assert bonobo.__version__ for name in dir(bonobo): <IF_STMT> continue attr = getattr(bonobo, name) if inspect.ismodule(attr): continue assert name in bonobo.__all__",if name.startswith('_'):,if name.startswith('_'):,0.8938685347732209,0.8466657105524215,True
2819,"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val <IF_STMT> const = not true_val elif not compare.compatible_with(v, False): const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var","elif not compare.compatible_with(v, True):","elif isinstance(v, mixin.BooleanConstant):",0.8616230337527601,0.9182210682909737,False
2820,"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, 'id') controls_ids = required_key(item, 'controls') if not isinstance(controls_ids, list): <IF_STMT> msg = 'Policy {id_} contains invalid controls list {controls}.'.format(id_=id_, controls=str(controls_ids)) raise ValueError(msg) self.policies[id_] = controls_ids",if controls_ids != 'all':,if controls_ids not in self.policies:,0.9030526783174653,0.8248765135255685,False
2821,"def pong(self, payload: Union[str, bytes]='') -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <IF_STMT> payload = payload.decode('utf-8') self.logger.debug(f'Sending a pong data frame (session id: {self.session_id}, payload: {payload})') data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PONG) with self.sock_send_lock: self.sock.send(data)","if isinstance(payload, bytes):","if isinstance(payload, bytes):",0.8749185683750157,0.8592899528284996,True
2822,"def _extract_curve_feature_log(arg): """"""extract sampled curve feature for log items"""""" try: inp, res = arg config = inp.config with inp.target: sch, args = inp.task.instantiate(config) fea = feature.get_buffer_curve_sample_flatten(sch, args, sample_n=20) x = np.concatenate((fea, list(config.get_other_option().values()))) <IF_STMT> y = inp.task.flop / np.mean(res.costs) else: y = 0.0 return (x, y) except Exception: return None",if res.error_no == 0:,if res.costs:,0.7795095933245922,0.926934323706186,False
2823,"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' if ss['branch']: source += '[branch %s] ' % ss['branch'] <IF_STMT> source += str(ss['revision']) else: source += 'HEAD' if ss['patch'] is not None: source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['revision']:,if ss['revision']:,0.7140841495703494,0.9425437476131634,True
2824,"def find_repository(): orig_path = path = os.path.realpath('.') drive, path = os.path.splitdrive(path) while path: current_path = os.path.join(drive, path) current_repo = LocalRepository(current_path) if current_repo.isValid(): return current_repo path, path_tail = os.path.split(current_path) <IF_STMT> raise CannotFindRepository('Cannot find repository for %s' % (orig_path,))",if not path_tail:,if path_tail != orig_path:,0.8249607360982941,0.8555308664663046,False
2825,"def compute_indices(text: str, tokens): indices = [] for i, token in enumerate(tokens): <IF_STMT> current_index = indices[-1] + len(tokens[i - 1][0]) indices.append(current_index + text[current_index:].find(token[0])) else: indices.append(text.find(token[0])) return indices",if 1 <= i:,if i > 0:,0.8633166324326299,0.8038019482772603,False
2826,"def _add_defaults_data_files(self): if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): item = convert_path(item) <IF_STMT> self.filelist.append(item) else: dirname, filenames = item for f in filenames: f = convert_path(f) if os.path.isfile(f): self.filelist.append(f)",if os.path.isfile(item):,if os.path.isfile(item):,0.7024090115233333,0.8827916928185874,True
2827,def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe('compiler.libcxx') if not compiler or not libcxx: return '' if str(compiler) in GCC_LIKE: <IF_STMT> return '_GLIBCXX_USE_CXX11_ABI=0' elif str(libcxx) == 'libstdc++11': return '_GLIBCXX_USE_CXX11_ABI=1' return '',if str(libcxx) == 'libstdc++':,if str(libcxx) == 'libstdc++10':,0.9007221147691593,0.828399516355805,False
2828,"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) <IF_STMT> self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","elif isinstance(v, basestring):","elif isinstance(v, str):",0.9221266946837977,0.9118021019905903,False
2829,"def test_seek(self): <IF_STMT> print('create large file via seek (may be sparse file) ...') with self.open(TESTFN, 'wb') as f: f.write(b'z') f.seek(0) f.seek(size) f.write(b'a') f.flush() if verbose: print('check file size with os.fstat') self.assertEqual(os.fstat(f.fileno())[stat.ST_SIZE], size + 1)",if verbose:,if verbose:,0.8329121540054962,0.9016857283765338,True
2830,"def serialize_review_url_field(self, obj, **kwargs): if obj.review_ui: review_request = obj.get_review_request() <IF_STMT> local_site_name = review_request.local_site.name else: local_site_name = None return local_site_reverse('file-attachment', local_site_name=local_site_name, kwargs={'review_request_id': review_request.display_id, 'file_attachment_id': obj.pk}) return ''",if review_request.local_site_id:,if review_request.local_site:,0.7030138280206846,0.8590888738245122,False
2831,"def on_item_down_clicked(self, button): model = self.treeview.get_model() for s in self._get_selected(): <IF_STMT> old = model.get_iter(s[0]) iter = model.insert(s[0] + 2) for i in range(3): model.set_value(iter, i, model.get_value(old, i)) model.remove(old) self.treeview.get_selection().select_iter(iter) self._update_filter_string()",if s[0] < len(model) - 1:,if s[0] == 1:,0.8796498895160315,0.8228500218338367,False
2832,"def writer(self): """"""loop forever and copy socket->serial"""""" while self.alive: try: data = self.socket.recv(1024) <IF_STMT> break self.serial.write(b''.join(self.rfc2217.filter(data))) except socket.error as msg: self.log.error('{}'.format(msg)) break self.stop()",if not data:,if not data:,0.7625490227201515,0.80377750806414,True
2833,"def __getitem__(self, key): if key == 1: return self.get_value() elif key == 0: return self.cell[0] elif isinstance(key, slice): s = list(self.cell.__getitem__(key)) <IF_STMT> s[s.index(self.cell[1])] = self.get_value() return s else: raise IndexError(key)",if self.cell[1] in s:,if len(s) > 1:,0.797078833925097,0.8228500218338367,False
2834,"def test_error_stream(environ, start_response): writer = start_response('200 OK', []) wsgi_errors = environ['wsgi.errors'] error_msg = None for method in ['flush', 'write', 'writelines']: <IF_STMT> error_msg = ""wsgi.errors has no '%s' attr"" % method if not error_msg and (not callable(getattr(wsgi_errors, method))): error_msg = 'wsgi.errors.%s attr is not callable' % method if error_msg: break return_msg = error_msg or 'success' writer(return_msg) return []","if not hasattr(wsgi_errors, method):","if not error_msg and hasattr(wsgi_errors, method):",0.9306587536617044,0.8754021059663507,False
2835,"def job_rule_modules(app): rules_module_list = [] for rules_module_name in __job_rule_module_names(app): rules_module = sys.modules.get(rules_module_name, None) <IF_STMT> rules_module = importlib.import_module(rules_module_name) rules_module_list.append(rules_module) return rules_module_list",if not rules_module:,if rules_module is None:,0.6982956616201675,0.7378351342269067,False
2836,"def discover_hdfstore(f): d = dict() for key in f.keys(): d2 = d key2 = key.lstrip('/') while '/' in key2: group, key2 = key2.split('/', 1) <IF_STMT> d2[group] = dict() d2 = d2[group] d2[key2] = f.get_storer(key) return discover(d)",if group not in d2:,if group not in d2:,0.7873840191812043,0.8294838585473985,True
2837,"def test_update_zone(self): zone = self.driver.list_zones()[0] updated_zone = self.driver.update_zone(zone=zone, domain='', extra={'paused': True}) self.assertEqual(zone.id, updated_zone.id) self.assertEqual(zone.domain, updated_zone.domain) self.assertEqual(zone.type, updated_zone.type) self.assertEqual(zone.ttl, updated_zone.ttl) for key in set(zone.extra) | set(updated_zone.extra): <IF_STMT> self.assertNotEqual(zone.extra[key], updated_zone.extra[key]) else: self.assertEqual(zone.extra[key], updated_zone.extra[key])","if key in ('paused', 'modified_on'):","if key not in ['paused', 'paused']:",0.8970139904809733,0.7803242766062389,False
2838,"def ESP(phrase): for num, name in enumerate(devname): <IF_STMT> dev = devid[num] if custom_action_keyword['Dict']['On'] in phrase: ctrl = '=ON' say('Turning On ' + name) elif custom_action_keyword['Dict']['Off'] in phrase: ctrl = '=OFF' say('Turning Off ' + name) rq = requests.head('https://' + ip + dev + ctrl, verify=False)",if name.lower() in phrase:,if num in devid:,0.8964876521605887,0.8806615362338783,False
2839,"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue if nw_id_ == nw_id: ret.append(port.port_no) <IF_STMT> ret.append(port.port_no) return ret",elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:,if allow_nw_id_external and port.port_no in allow_nw_id_external:,0.8344812149454105,0.7981256013410023,False
2840,"def tail(filename): if os.path.isfile(filename): file = open(filename, 'r') st_results = os.stat(filename) st_size = st_results[6] file.seek(st_size) while 1: where = file.tell() line = file.readline() <IF_STMT> time.sleep(1) file.seek(where) else: print(line) else: print_error('File not found, cannot tail.')",if not line:,if line:,0.9104211063241645,0.8935248372106969,False
2841,"def proc_day_of_week(d): if expanded[4][0] != '*': diff_day_of_week = nearest_diff_method(d.isoweekday() % 7, expanded[4], 7) if diff_day_of_week is not None and diff_day_of_week != 0: <IF_STMT> d += relativedelta(days=diff_day_of_week, hour=23, minute=59, second=59) else: d += relativedelta(days=diff_day_of_week, hour=0, minute=0, second=0) return (True, d) return (False, d)",if is_prev:,if expanded[4][0] == '*':,0.6712585716836548,0.8723360571509826,False
2842,"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): if name.startswith('check_'): method = getattr(self, name) <IF_STMT> method() finally: warnings.formatwarning = oldformatwarning",if method and callable(method):,if callable(method):,0.7881714831871888,0.8827916928185874,False
2843,"def get(self, request, *args, **kwargs): if self.revision: <IF_STMT> try: return send_file(request, self.revision.file.path, self.revision.created, self.attachment.original_filename) except OSError: pass else: return HttpResponseRedirect(self.revision.file.url) raise Http404",if settings.USE_LOCAL_PATH:,if self.attachment:,0.8683120784274498,0.8318180062062374,False
2844,"def _close(self): super(Recording, self)._close() if self._log_n is not None: for i in range(self.n): <IF_STMT> self._log_n[i].close() self._log_n[i] = None",if self._log_n[i] is not None:,if self._log_n[i].is_open():,0.8366108239248146,0.7912619863720214,False
2845,"def addTags(self, rpcObjects=None): hosts = self._getOnlyHostObjects(rpcObjects) if hosts: title = 'Add Tags' body = 'What tags should be added?\n\nUse a comma or space between each' tags, choice = self.getText(title, body, '') <IF_STMT> tags = str(tags).replace(' ', ',').split(',') for host in hosts: self.cuebotCall(host.addTags, 'Add Tags to %s Failed' % host.data.name, tags) self._update()",if choice:,if choice:,0.6864402768007776,0.9298663600557577,True
2846,"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = ['pixel_longitude', 'pixel_latitude'] for var_name, val in self.file_content.items(): <IF_STMT> ds_info = {'file_type': self.filetype_info['file_type'], 'resolution': res} if not self.is_geo: ds_info['coordinates'] = coordinates yield (DatasetID(name=var_name, resolution=res), ds_info)","if isinstance(val, netCDF4.Variable):",if val == coordinates:,0.9066502890122317,0.8592377270804451,False
2847,"def extract_from_file(fname: PathIsh) -> Iterator[Extraction]: path = Path(fname) fallback_dt = file_mtime(path) p = Parser(path) for r in p.walk(): <IF_STMT> yield r else: yield Visit(url=r.url, dt=fallback_dt, locator=Loc.file(fname), context=r.context)","if isinstance(r, Exception):",if r.url is None:,0.8823812823017678,0.8038019482772603,False
2848,"def init_module_config(module_json, config, config_path=default_config_path): if 'config' in module_json['meta']: if module_json['meta']['config']: <IF_STMT> config.add_section(module_json['name']) for config_var in module_json['meta']['config']: if config_var not in config[module_json['name']]: config.set(module_json['name'], config_var, '') return config",if module_json['name'] not in config:,if module_json['name'] not in config.get_section(module_json['name']):,0.8892490291356236,0.7685107079449489,False
2849,"def _create_entities(parsed_entities, sidx, eidx): entities = [] for k, vs in parsed_entities.items(): <IF_STMT> vs = [vs] for value in vs: entities.append({'entity': k, 'start': sidx, 'end': eidx, 'value': value}) return entities","if not isinstance(vs, list):","if not isinstance(vs, list):",0.8738580184382074,0.8228500218338367,True
2850,"def _telegram_upload_stream(self, stream, **kwargs): """"""Perform upload defined in a stream."""""" msg = None try: stream.accept() msg = self._telegram_special_message(chat_id=stream.identifier.id, content=stream.raw, msg_type=stream.stream_type, **kwargs) except Exception: log.exception(f'Upload of {stream.name} to {stream.identifier} failed.') else: <IF_STMT> stream.error() else: stream.success()",if msg is None:,if msg is None:,0.9196723497866421,0.8431339019329497,True
2851,"def readlines(self, size=-1): if self._nbr == self._size: return [] out = [] nbr = 0 while True: line = self.readline() <IF_STMT> break out.append(line) if size > -1: nbr += len(line) if nbr > size: break return out",if not line:,if not line:,0.9023562197488707,0.8783650674919876,True
2852,"def clean_permissions(cls, requestor: 'User', group: auth_models.Group, errors: Dict[Optional[str], List[ValidationError]], cleaned_input: dict): field = 'add_permissions' permission_items = cleaned_input.get(field) if permission_items: cleaned_input[field] = get_permissions(permission_items) <IF_STMT> cls.ensure_can_manage_permissions(requestor, errors, field, permission_items)",if not requestor.is_superuser:,if errors:,0.6676585572987316,0.8645707301556367,False
2853,"def _bwd(subj=None, obj=None, seen=None): seen.add(obj) for s, o in evalPath(graph, (None, self.path, obj)): <IF_STMT> yield (s, o) if self.more: if s in seen: continue for s2, o2 in _bwd(None, s, seen): yield (s2, o)",if not subj or subj == s:,if subj:,0.9296214965885246,0.8935248372106969,False
2854,"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: <IF_STMT> uptime['value'] = stats['uptime'] / 60 / 60 / 24 uptime['unit'] = _('days') elif stats['uptime'] > 3600: uptime['value'] = stats['uptime'] / 60 / 60 uptime['unit'] = _('hours') else: uptime['value'] = stats['uptime'] / 60 uptime['unit'] = _('minutes') return {'cache_stats': cache_stats, 'uptime': uptime}",if stats['uptime'] > 86400:,if stats['uptime'] > 24:,0.6674536528636966,0.9128479730518225,False
2855,def refresh(self): if self._handle: source = self._db.get_repository_from_handle(self._handle) <IF_STMT> self._title = str(source.get_type()) self._value = source.get_name(),if source:,if source:,0.5655289662476922,0.7241577342575828,True
2856,"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) <IF_STMT> return None elif '.' in svalue: return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",if not svalue:,if svalue == '':,0.6283523397010725,0.8105932471967202,False
2857,"def parseGrants(self, tree): for grant in tree.findall('.//Grant'): grantee = Grantee() g = grant.find('.//Grantee') grantee.xsi_type = g.attrib['{http://www.w3.org/2001/XMLSchema-instance}type'] grantee.permission = grant.find('Permission').text for el in g: <IF_STMT> grantee.display_name = el.text else: grantee.tag = el.tag grantee.name = el.text self.grantees.append(grantee)",if el.tag == 'DisplayName':,if el.tag == 'display':,0.9033363473612592,0.8474968231198384,False
2858,"def __init__(self, name: Optional[str]=None, order: int=0): if name is None: if order == 0: name = 'std_dev' <IF_STMT> name = 'sample_std_dev' else: name = f'std_dev{order})' super().__init__(name=name, order=order) self.order = order",elif order == 1:,elif order == 1:,0.6651621286266212,0.8228500218338367,True
2859,"def _shouldRollover(self): if self.maxBytes > 0: try: self.stream.seek(0, 2) except IOError: return True <IF_STMT> return True else: self._degrade(False, 'Rotation done or not needed at this time') return False",if self.stream.tell() >= self.maxBytes:,if self.stream.read(self.maxBytes) == self.maxBytes:,0.8333889697377026,0.8105932471967202,False
2860,"def userfullname(): """"""Get the user's full name."""""" global _userfullname if not _userfullname: uid = os.getuid() entry = pwd_from_uid(uid) <IF_STMT> _userfullname = entry[4].split(',')[0] or entry[0] if not _userfullname: _userfullname = 'user%d' % uid return _userfullname",if entry:,if entry:,0.7571011491364478,0.8935248372106969,True
2861,"def drop(self): sql = ""if object_id('%s') is not null drop table %s"" % (self.tname, self.tname) try: self.execute(sql) except Exception as e: self.conn.rollback() <IF_STMT> raise sql = 'drop table if exists %s' % self.tname self.execute(sql)",if 'syntax error' not in str(e):,if e.args[0] != errno.ENOENT:,0.7410379138651287,0.8431339019329497,False
2862,"def _find_delimiter(f, block_size=2 ** 16): delimiter = b'\n' if f.tell() == 0: return 0 while True: b = f.read(block_size) <IF_STMT> return f.tell() elif delimiter in b: return f.tell() - len(b) + b.index(delimiter) + 1",if not b:,if b == b:,0.6728395171996244,0.8431339019329497,False
2863,"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) <IF_STMT> force_list = True values.extend((_convert(x[1]) for x in sorted(container.items()))) if not force_list and len(values) == 1: values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls(((k, _convert(v)) for k, v in iteritems(container)))","if container.pop(_list_marker, False):",if len(values) == 1:,0.9090062116064861,0.9019629427251674,False
2864,"def fitting(self, value): self._fitting = value if self._fitting is not None: <IF_STMT> try: os.makedirs(dirname(self.checkpoint_path())) except FileExistsError as ex: pass if not os.path.exists(dirname(self.tensorboard_path())): try: os.makedirs(dirname(self.tensorboard_path())) except FileExistsError as ex: pass",if not os.path.exists(dirname(self.checkpoint_path())):,if not os.path.exists(dirname(self.checkpoint_path())):,0.6378258646345139,0.8446593249975184,True
2865,"def _make_headers(self): libraries = self._df.columns.to_list() columns = [] for library in libraries: version = self._package_versions[library] library_description = self._libraries_description.get(library) <IF_STMT> library += ' {}'.format(library_description) columns.append('{library}<br><small>{version}</small>'.format(library=library, version=version)) return [''] + columns",if library_description:,if library_description:,0.8589261680216809,0.8743414417652072,True
2866,"def plugin_on_song_ended(self, song, stopped): if song is not None: poll = self.rating_box.poll_vote() <IF_STMT> ups = int(song.get('~#wins') or 0) downs = int(song.get('~#losses') or 0) ups += poll[0] downs += poll[1] song['~#wins'] = ups song['~#losses'] = downs song['~#rating'] = ups / max(ups + downs, 2) song['~#score'] = ups - downs",if poll[0] >= 1 or poll[1] >= 1:,if poll is not None:,0.7148210534458871,0.8703737209656045,False
2867,"def submit(self, pig_script, params): workflow = None try: workflow = self._create_workflow(pig_script, params) mapping = dict([(param['name'], param['value']) for param in workflow.get_parameters()]) oozie_wf = _submit_workflow(self.user, self.fs, self.jt, workflow, mapping) finally: <IF_STMT> workflow.delete(skip_trash=True) return oozie_wf",if workflow:,if workflow:,0.9277002073764149,0.8866029039778043,True
2868,"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) <IF_STMT> correct += 1 else: self.fail('Invalid schema was parsed: ' + example.schema_string) except: if not example.valid: correct += 1 else: self.fail('Valid schema failed to parse: ' + example.schema_string) fail_msg = 'Parse behavior correct on %d out of %d schemas.' % (correct, len(EXAMPLES)) self.assertEqual(correct, len(EXAMPLES), fail_msg)",if example.valid:,if not example.valid:,0.9389056698296899,0.9196822664155297,False
2869,"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in ('wf', 'punc'): itm = self.handle_word(child) <IF_STMT> sent.extend(itm) else: sent.append(itm) else: raise ValueError('Unexpected element %s' % child.tag) return SemcorSentence(elt.attrib['snum'], sent)",if self._unit == 'word':,"if isinstance(itm, list):",0.8435967151530988,0.8592899528284996,False
2870,"def _set_property(self, target_widget, pname, value): if pname == 'text': state = target_widget.cget('state') <IF_STMT> target_widget.configure(state=tk.NORMAL) target_widget.insert('0.0', value) target_widget.configure(state=tk.DISABLED) else: target_widget.insert('0.0', value) else: super(TKText, self)._set_property(target_widget, pname, value)",if state == tk.DISABLED:,if state == tk.NORMAL:,0.8267660491253773,0.7886336751695258,False
2871,"def get_vrf_tables(self, vrf_rf=None): vrf_tables = {} for (scope_id, table_id), table in self._tables.items(): if scope_id is None: continue <IF_STMT> continue vrf_tables[scope_id, table_id] = table return vrf_tables",if vrf_rf is not None and table_id != vrf_rf:,if vrf_rf is None or table_id == vrf_rf:,0.7313403794271165,0.6850564735741161,False
2872,"def new_f(self, *args, **kwargs): for obj in f(self, *args, **kwargs): if self.protected == False: if 'user' in obj and obj['user']['protected']: continue <IF_STMT> continue yield obj",elif 'protected' in obj and obj['protected']:,if 'user' in obj and obj['user']['protected']:,0.6030400574901407,0.7332023424667726,False
2873,"def draw(self, context): col = self.layout.column() col.operator('node.sv_show_latest_commits') if context.scene.sv_new_version: col_alert = self.layout.column() col_alert.alert = True col_alert.operator('node.sverchok_update_addon', text='Upgrade Sverchok addon') else: col.operator('node.sverchok_check_for_upgrades_wsha', text='Check for updates') with sv_preferences() as prefs: <IF_STMT> col.operator('node.sv_run_pydoc')",if prefs.developer_mode:,"if prefs.get('node.sverchok_update_addon', False):",0.9002818166396702,0.8763166199819785,False
2874,"def generate_tag_1_data(ids): if len(ids) != SAMPLE_NUM: raise ValueError('len ids should equal to sample number') counter = 0 for sample_i in range(SAMPLE_NUM): one_data = [ids[sample_i]] valid_set = [x for x in range(TAG_INTERVAL[0], TAG_INTERVAL[1])] features = np.random.choice(valid_set, FEATURE_NUM, replace=False) one_data += [':'.join([x, '1.0']) for x in features] counter += 1 <IF_STMT> print('generate data {}'.format(counter)) yield one_data",if counter % 10000 == 0:,if counter % 10000 == 0:,0.8804824405941032,0.8688589397154922,True
2875,"def handle_api_languages(self, http_context): mgr = PluginManager.get(aj.context) languages = set() for id in mgr: locale_dir = mgr.get_content_path(id, 'locale') <IF_STMT> for lang in os.listdir(locale_dir): if lang != 'app.pot': languages.add(lang) return sorted(list(languages))",if os.path.isdir(locale_dir):,if os.path.exists(locale_dir):,0.8237417546043195,0.8743414417652072,False
2876,"def update(self, t): for i in range(self.grid.x): for j in range(self.grid.y): distance = self.test_func(i, j, t) <IF_STMT> self.turn_off_tile(i, j) elif distance < 1: self.transform_tile(i, j, distance) else: self.turn_on_tile(i, j)",if distance == 0:,if distance > 0:,0.6191892272438566,0.8169276475307028,False
2877,"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): if isinstance(text, CodeViewText): text.autocompleter = Completer(text) <IF_STMT> text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","elif isinstance(text, ShellText):","elif isinstance(text, ShellText):",0.7743705372356612,0.833078701050083,True
2878,"def test_create_repository(repo_name, expected_status, client): with client_with_identity('devtable', client) as cl: body = {'namespace': 'devtable', 'repository': repo_name, 'visibility': 'public', 'description': 'foo'} result = conduct_api_call(client, RepositoryList, 'post', None, body, expected_code=expected_status).json <IF_STMT> assert result['name'] == repo_name assert model.repository.get_repository('devtable', repo_name).name == repo_name",if expected_status == 201:,if result:,0.7137063534751792,0.9024521756077707,False
2879,"def _apply_filter(filter_item, filter_list): for filter_method in filter_list: try: <IF_STMT> return False except Exception as e: raise MessageException(""Toolbox filter exception from '{}': {}."".format(filter_method.__name__, unicodify(e))) return True","if not filter_method(context, filter_item):",if filter_item.filter(filter_method):,0.6005910652223576,0.8531413606256201,False
2880,"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) if not data: break <IF_STMT> data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write('%s: I/O error: %s\n' % (filename, msg)) return 1 out.write('%s %s\n' % (m.hexdigest(), filename)) return 0","if isinstance(data, str):",if fp.encoding:,0.7317933048340421,0.9144061946646023,False
2881,"def get_block_loc_keys(block): """"""Extract loc_keys used by @block"""""" symbols = set() for instr in block.lines: <IF_STMT> if isinstance(instr.raw, list): for expr in instr.raw: symbols.update(get_expr_locs(expr)) else: for arg in instr.args: symbols.update(get_expr_locs(arg)) return symbols","if isinstance(instr, AsmRaw):","if isinstance(instr, ast.Expr):",0.9065237702122436,0.8547305998833805,False
2882,"def get_operations(cls, info, operations: List[ProductAttributeAssignInput]): """"""Resolve all passed global ids into integer PKs of the Attribute type."""""" product_attrs_pks = [] variant_attrs_pks = [] for operation in operations: pk = from_global_id_strict_type(operation.id, only_type=Attribute, field='operations') <IF_STMT> product_attrs_pks.append(pk) else: variant_attrs_pks.append(pk) return (product_attrs_pks, variant_attrs_pks)",if operation.type == ProductAttributeType.PRODUCT:,if info.product_id_strict_type(pk):,0.8177978492983476,0.9076141716697395,False
2883,def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree['activities'].values(): if act['type'] == 'SubProcess': _collect_manual_intervention_nodes(act['pipeline']) <IF_STMT> manual_intervention_nodes.add(act['id']),elif act['component']['code'] in MANUAL_INTERVENTION_COMP_CODES:,elif act['type'] == 'Intervention':,0.5918156856791099,0.6668954865619205,False
2884,"def prompt_authorization(self, stacks: List[Stack]): auth_required_per_resource = auth_per_resource(stacks) for resource, authorization_required in auth_required_per_resource: <IF_STMT> auth_confirm = confirm(f'\t{self.start_bold}{resource} may not have authorization defined, Is this okay?{self.end_bold}', default=False) if not auth_confirm: raise GuidedDeployFailedError(msg='Security Constraints Not Satisfied!')",if not authorization_required:,if authorization_required:,0.916214576558993,0.8901732118131125,False
2885,"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): if self.source in CLOUD_PROVIDERS: <IF_STMT> credential = cred break elif cred.credential_type.kind != 'vault': credential = cred break return credential","if cred.kind == self.source.replace('ec2', 'aws'):",if cred.credential_type.kind == 'inventory':,0.8341035551165766,0.8661072626070159,False
2886,"def validate_party_details(self): if self.party: if not frappe.db.exists(self.party_type, self.party): frappe.throw(_('Invalid {0}: {1}').format(self.party_type, self.party)) <IF_STMT> self.validate_account_type(self.party_account, [erpnext.get_party_account_type(self.party_type)])","if self.party_account and self.party_type in ('Customer', 'Supplier'):",if self.party_account:,0.6036396952202859,0.7670387248467656,False
2887,"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): <IF_STMT> if i in direct_upstreams: t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield (k, tuple(t))",if g:,"if isinstance(g, list):",0.859420438313258,0.9100365300271298,False
2888,"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): <IF_STMT> break scale = scales[i] if not math.isnan(scale): vidx1, vidx2 = (vidxs[i * 2], vidxs[i * 2 + 1]) scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData","if i >= min(len(scales), len(vidxs) // 2):",if i >= len(scales):,0.9079539540423675,0.8592377270804451,False
2889,"def _make_ext_obj(self, obj): ext = self._get_ext_class(obj.objname)() for name, val in obj.body: <IF_STMT> raise Exception('Error val should be a list, this is a python-opcua bug', name, type(val), val) else: for attname, v in val: self._set_attr(ext, attname, v) return ext","if not isinstance(val, list):","if not isinstance(val, list):",0.8944337491414516,0.8592377270804451,True
2890,"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: if row[refnum + 1] is None: row[refnum + 1] = line return <IF_STMT> break self.rows.insert(i, self.newRow(linenum, refnum, line))",elif row[0] > linenum:,if i == len(self.rows):,0.6957023765077437,0.8474968231198384,False
2891,def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) if match: continue if strip_delimiters: <IF_STMT> continue yield line,"if ',' in line or ';' in line:",if line.strip().startswith('#') or line.startswith('#') or line.startswith('#'):,0.858421129024265,0.7424213297217366,False
2892,"def encodingChanged(self, idx): encoding = str(self.mode_combo.currentText()) validator = None if encoding == 'hex': txt = str(self.data_edit.text()) <IF_STMT> self.data_edit.setText('') regex = QtCore.QRegExp('^[0-9A-Fa-f]+$') validator = QtGui.QRegExpValidator(regex) self.data_edit.setValidator(validator) self.renderMemory()",if not all((c in string.hexdigits for c in txt)):,if txt == 'UTF-8':,0.6990565928209632,0.7965020533851944,False
2893,"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): <IF_STMT> self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()",if redo:,if self.db_interface:,0.9048723486962926,0.839587623092576,False
2894,"def _transform_bin(self, X: DataFrame): if self._bin_map: <IF_STMT> X = X.copy(deep=True) with pd.option_context('mode.chained_assignment', None): for column in self._bin_map: X[column] = binning.bin_column(series=X[column], mapping=self._bin_map[column], dtype=self._astype_map[column]) return X",if not self.inplace:,"if isinstance(X, pd.DataFrame):",0.7482137962215386,0.8120341702859789,False
2895,"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') if '>' in text: text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') <IF_STMT> text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text","if ""'"" in text:","if '""' in text:",0.9527124580151902,0.9084940438173679,False
2896,"def read(self): """"""Reads the robots.txt URL and feeds it to the parser."""""" try: f = urllib.request.urlopen(self.url) except urllib.error.HTTPError as err: <IF_STMT> self.disallow_all = True elif err.code >= 400 and err.code < 500: self.allow_all = True else: raw = f.read() self.parse(raw.decode('utf-8').splitlines())","if err.code in (401, 403):",if err.code == 404 and err.code == 404:,0.8111839987035644,0.7899177245850753,False
2897,"def post_create(self, user, billing=None): from weblate.trans.models import Change if billing: billing.projects.add(self) <IF_STMT> self.access_control = Project.ACCESS_PRIVATE else: self.access_control = Project.ACCESS_PUBLIC self.save() if not user.is_superuser: self.add_user(user, '@Administration') Change.objects.create(action=Change.ACTION_CREATE_PROJECT, project=self, user=user, author=user)",if billing.plan.change_access_control:,if user.is_staff:,0.9222331519929589,0.8743414417652072,False
2898,"def visitConst(self, node): if self.documentable: <IF_STMT> self.documentable.append(make_docstring(node.value, node.lineno)) else: self.documentable = None","if type(node.value) in (StringType, UnicodeType):",if node.value:,0.7764377564990141,0.6703420896351792,False
2899,"def requires(self): requires = copy.deepcopy(self._requires) parameters = self.parameters for value in parameters.values(): if isinstance(value, basestring) and '::' in value: stack_name, _ = value.split('::') else: continue <IF_STMT> requires.add(stack_name) return requires",if stack_name not in requires:,if stack_name not in requires:,0.895340635528016,0.7909601595885504,True
2900,"def __load_protos(): g = globals() for k, v in g.items(): <IF_STMT> name = k[4:] modname = name.lower() try: mod = __import__(modname, g, level=1) PPP.set_p(v, getattr(mod, name)) except (ImportError, AttributeError): continue",if k.startswith('PPP_'):,if k.startswith('protos_'):,0.7627919955330918,0.8787142254774354,False
2901,"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) <IF_STMT> normal_init(m, std=0.01)","elif isinstance(m, nn.Linear):","elif isinstance(m, nn.BatchNorm3d):",0.8639798143733165,0.7985065516266612,False
2902,"def get_data(self): """"""get all data from sockets"""""" si = self.inputs parameters = [] for socket in si: <IF_STMT> parameters.append(socket.sv_get()) else: parameters.append(socket.sv_get(default=[[]])) return match_long_repeat(parameters)",if len(socket.prop_name) > 0:,if socket.is_socket():,0.881821374183549,0.839587623092576,False
2903,"def test_parse_query_params_comparable_field(self): query_params = {'filter[int_field][gt]': 42, 'filter[int_field][lte]': 9000} fields = self.view.parse_query_params(query_params) for key, field_name in fields.items(): if field_name['int_field']['op'] == 'gt': assert_equal(field_name['int_field']['value'], 42) <IF_STMT> assert_equal(field_name['int_field']['value'], 9000) else: self.fail()",elif field_name['int_field']['op'] == 'lte':,elif field_name['int_field']['op'] == 'lte':,0.8962124004577082,0.8038019482772603,True
2904,"def _create_examples(self, lines, set_type): """"""Creates examples for the training and dev sets."""""" examples = [] for i, line in enumerate(lines): <IF_STMT> continue guid = '%s-%s' % (set_type, i) text = line[0] bbox = line[1] label = line[2] examples.append(DocExample(guid=guid, text_a=text, text_b=None, bbox=bbox, label=label)) return examples",if i == 0:,if i == 0:,0.8376985202447459,0.8780099567239787,True
2905,"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = mod_attr_path.split('#') if '#' in mod_attr_path else (mod_attr_path, '') full_mod_path = '{}.{}'.format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) <IF_STMT> for part in attr_path.split('.'): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: if checked: return None raise ex",if attr_path:,if attr_path:,0.7163713913828588,0.926934323706186,True
2906,"def _load_ui_modules(self, modules: Any) -> None: if isinstance(modules, types.ModuleType): self._load_ui_modules(dict(((n, getattr(modules, n)) for n in dir(modules)))) elif isinstance(modules, list): for m in modules: self._load_ui_modules(m) else: assert isinstance(modules, dict) for name, cls in modules.items(): try: <IF_STMT> self.ui_modules[name] = cls except TypeError: pass","if issubclass(cls, UIModule):","if isinstance(cls, types.ModuleType):",0.9249330907221226,0.8902579342581529,False
2907,"def _remove_obsolete_leafs(input_dict): if not isinstance(input_dict, dict): return if input_dict[LEAF_MARKER]: bottom_leafs = input_dict[LEAF_MARKER] for leaf in bottom_leafs: <IF_STMT> input_dict[LEAF_MARKER].remove(leaf) for subtree in input_dict.keys(): _remove_obsolete_leafs(input_dict[subtree])",if leaf in input_dict:,if leaf in input_dict[LEAF_MARKER]:,0.7983776491669912,0.7709002428237395,False
2908,"def decode(self, value, force=False): """"""Return a unicode string from the bytes-like representation"""""" if self.decode_responses or force: <IF_STMT> value = value.tobytes() if isinstance(value, bytes): value = value.decode(self.encoding, self.encoding_errors) return value","if isinstance(value, memoryview):","if isinstance(value, bytes):",0.8886221466105605,0.8446593249975184,False
2909,"def audit(self, directive): value = _get_value(directive) if not value: return server_side = directive.name.startswith('proxy_') for var in compile_script(value): char = '' <IF_STMT> char = '\\n' elif not server_side and var.can_contain('\r'): char = '\\r' else: continue reason = 'At least variable ""${var}"" can contain ""{char}""'.format(var=var.name, char=char) self.add_issue(directive=[directive] + var.providers, reason=reason)",if var.can_contain('\n'):,if not server_side and var.can_contain('\n'):,0.680930969129744,0.8703737209656045,False
2910,"def checkFilename(filename): while True: if filename[0] == ""'"": filename = filename[1:] <IF_STMT> filename = filename[:-1] if os.path.exists(filename): return filename filename = input(""[!] Cannot find '%s'.\n[*] Enter a valid name of the file containing the paths to test -> "" % filename)","if filename[len(filename) - 1] == ""'"":",elif filename.endswith('\\') and filename.endswith('\\':,0.6984389845935084,0.8692960007731574,False
2911,"def findfiles(self, dir, base, rec): try: names = os.listdir(dir or os.curdir) except os.error as msg: print(msg) return [] list = [] subdirs = [] for name in names: fn = os.path.join(dir, name) <IF_STMT> subdirs.append(fn) elif fnmatch.fnmatch(name, base): list.append(fn) if rec: for subdir in subdirs: list.extend(self.findfiles(subdir, base, rec)) return list",if os.path.isdir(fn):,"if fnmatch.fnmatch(name, base):",0.9396818641518151,0.9081987180086649,False
2912,"def loop(handler, obj): handler.response.write('<table>') for k, v in obj.__dict__.items(): <IF_STMT> style = 'color: red' if not v else '' handler.response.write('<tr style=""{}""><td>{}:</td><td>{}</td></tr>'.format(style, k, v)) handler.response.write('</table>')","if not k in ('data', 'gae_user', 'credentials', 'content', 'config'):","if isinstance(v, dict):",0.8691701170325106,0.8120341702859789,False
2913,"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: if sys.platform == 'win32': executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) <IF_STMT> return executable pytest.skip('no suitable %s found' % (name,)) return executable",if executable.check():,if os.path.exists(executable):,0.9284984029816847,0.8996480074924822,False
2914,"def __init__(self, socketpath=None): if socketpath is None: <IF_STMT> socketpath = '/var/run/usbmuxd' else: socketpath = '/var/run/usbmuxd' self.socketpath = socketpath self.listener = MuxConnection(socketpath, BinaryProtocol) try: self.listener.listen() self.version = 0 self.protoclass = BinaryProtocol except MuxVersionError: self.listener = MuxConnection(socketpath, PlistProtocol) self.listener.listen() self.protoclass = PlistProtocol self.version = 1 self.devices = self.listener.devices",if sys.platform == 'darwin':,if sys.platform == 'win32':,0.9114694204033842,0.8832000938217648,False
2915,"def _validate_distinct_on_different_types_and_field_orders(self, collection, query, expected_results, get_mock_result): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): <IF_STMT> self.assertDictEqual(results[i], expected_results[i]) elif isinstance(results[i], list): self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0","if isinstance(results[i], dict):","if isinstance(results[i], dict):",0.922199886694479,0.8783650674919876,True
2916,"def getRootId(self, id): with self.connect() as cu: while True: stmt = 'select parent_path_id from hierarchy where path_id = ?' cu.execute(stmt, (id,)) parent_id = cu.fetchone()[0] <IF_STMT> return id id = parent_id",if parent_id is None or parent_id == id:,if parent_id == id:,0.774602066057255,0.8228500218338367,False
2917,"def add(self, path): with self.get_lock(path): <IF_STMT> self.entries[path] = {} self.entries[path]['lock'] = self.new_locks[path] del self.new_locks[path] self.lru.append(path)",if not path in self.entries:,if path not in self.entries:,0.7855194002320192,0.6165255292124369,False
2918,"def _get_coordinates_for_dataset_key(self, dsid): """"""Get the coordinate dataset keys for *dsid*."""""" ds_info = self.ids[dsid] cids = [] for cinfo in ds_info.get('coordinates', []): if not isinstance(cinfo, dict): cinfo = {'name': cinfo} cinfo['resolution'] = ds_info['resolution'] <IF_STMT> cinfo['polarization'] = ds_info['polarization'] cid = DatasetID(**cinfo) cids.append(self.get_dataset_key(cid)) return cids",if 'polarization' in ds_info:,if 'polarization' in ds_info:,0.7978449132588422,0.8723360571509826,True
2919,"def build_from_gdobj(cls, gdobj, steal=False): ret = BuiltinInitPlaceholder() if steal: assert ffi.typeof(gdobj).kind == 'pointer' ret._gd_ptr = gdobj el<IF_STMT> ret._gd_ptr = cls._copy_gdobj(gdobj) else: ret._gd_ptr = cls._copy_gdobj(ffi.addressof(gdobj)) ret.__class__ = cls return ret",if ffi.typeof(gdobj).kind == 'pointer':,"if isinstance(gdobj, (int, long)):",0.7079082600189901,0.8169276475307028,False
2920,"def _listen_output(self): """"""NB! works in background thread"""""" try: while True: chars = self._proc.read(1) <IF_STMT> as_bytes = chars.encode(self.encoding) self._make_output_available(as_bytes) else: self._error = 'EOF' break except Exception as e: self._error = str(e)",if len(chars) > 0:,if chars:,0.7559574710351403,0.8787142254774354,False
2921,"def result(metrics: Dict[metric_types.MetricKey, Any]) -> Dict[metric_types.AttributionsKey, Dict[Text, Union[float, np.ndarray]]]: """"""Returns mean attributions."""""" total_attributions = metrics[total_attributions_key] weighted_count = metrics[weighted_example_count_key] attributions = {} for k, v in total_attributions.items(): <IF_STMT> attributions[k] = float('nan') else: attributions[k] = v / weighted_count return {key: attributions}","if np.isclose(weighted_count, 0.0):",if v is None:,0.8286570924435284,0.8627586293513119,False
2922,"def write_if_changed(path, data): if isinstance(data, str): data = data.encode() changed = False with open(os.open(path, os.O_CREAT | os.O_RDWR), 'wb+') as f: f.seek(0) current = f.read() <IF_STMT> changed = True f.seek(0) f.write(data) f.truncate() os.fsync(f) return changed",if current != data:,if current != data:,0.8753615151004946,0.8431339019329497,True
2923,"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): <IF_STMT> if scan_argv(self.argv, other_option) is not None: raise ConfigurationError('Cannot give both %s and %s' % (option, other_option)) return option",if option != other_option:,if other_option != option:,0.6671844131192193,0.8474968231198384,False
2924,"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: <IF_STMT> continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if arg is None:,if arg is None:,0.8163362386784488,0.9204199807826591,True
2925,"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks except AttributeError: tasks = self.tasks <IF_STMT> app = tasks[0]._app if app is None and body is not None: app = body._app return app if app is not None else current_app",if len(tasks):,if len(tasks) == 1:,0.7228590323327261,0.8780099567239787,False
2926,"def add_field(self, field): self.remove_field(field.name) self.fields[field.name] = field self.columns[field.db_column] = field self._sorted_field_list.insert(field) self._update_field_lists() if field.default is not None: self.defaults[field] = field.default <IF_STMT> self._default_callables[field] = field.default self._default_callable_list.append((field.name, field.default)) else: self._default_dict[field] = field.default self._default_by_name[field.name] = field.default",if callable(field.default):,if field.name in self._default_callable_list:,0.692836752536529,0.8385130047130208,False
2927,"def _get_families(self): families = [] for name, ext in self._get_family_dirs(): <IF_STMT> family = self.get_resource(FileSystemPackageFamilyResource.key, location=self.location, name=name) else: family = self.get_resource(FileSystemCombinedPackageFamilyResource.key, location=self.location, name=name, ext=ext) families.append(family) return families",if ext is None:,if ext == '.py':,0.8591347821631583,0.7965020533851944,False
2928,"def test(model, data_loader, device=None): device = device or torch.device('cpu') model.eval() correct = 0 total = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(data_loader): <IF_STMT> break data, target = (data.to(device), target.to(device)) outputs = model(data) _, predicted = torch.max(outputs.data, 1) total += target.size(0) correct += (predicted == target).sum().item() return correct / total",if batch_idx * len(data) > TEST_SIZE:,if batch_idx == 0:,0.9403577267039298,0.8944264839442453,False
2929,"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: <IF_STMT> sleep_time = ThreadPool.PROGRESS_IDLE_DELAY elif self.__show_animation: self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY time.sleep(sleep_time)",if not self.__progress_status:,if self.__show_progress:,0.9039353579029321,0.8935248372106969,False
2930,"def _parse_subtitles(self, video_data, url_key): subtitles = {} for translation in video_data.get('translations', []): vtt_path = translation.get(url_key) <IF_STMT> continue lang = translation.get('language_w3c') or ISO639Utils.long2short(translation['language_medium']) subtitles.setdefault(lang, []).append({'ext': 'vtt', 'url': vtt_path}) return subtitles",if not vtt_path:,if not vtt_path:,0.6980410482708415,0.8446593249975184,True
2931,"def postprocess_message(self, msg): if msg['type'] == 'sample' and msg['value'] is not None: fn, value = (msg['fn'], msg['value']) value_batch_ndims = jnp.ndim(value) - fn.event_dim fn_batch_ndim = len(fn.batch_shape) <IF_STMT> prepend_shapes = (1,) * (value_batch_ndims - fn_batch_ndim) msg['fn'] = tree_map(lambda x: jnp.reshape(x, prepend_shapes + jnp.shape(x)), fn)",if fn_batch_ndim < value_batch_ndims:,if fn_batch_ndim > 0:,0.850036814925182,0.8723360571509826,False
2932,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_filename(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.5745717035656124,0.7378351342269067,True
2933,"def createError(self, line, pos, description): global ENABLE_PYIMPORT msg = 'Line ' + unicode(line) + ': ' + unicode(description) if ENABLE_JS2PY_ERRORS: <IF_STMT> import js2py.base return js2py.base.MakeError('SyntaxError', msg) else: return ENABLE_JS2PY_ERRORS(msg) else: return JsSyntaxError(msg)","if isinstance(ENABLE_JS2PY_ERRORS, bool):",if ENABLE_PYIMPORT:,0.6221667612675992,0.8866029039778043,False
2934,"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract(page, start_index, end_index, self.template.ignored_regions) for item in arg_to_iter(extracted): <IF_STMT> if isinstance(item, (ItemProcessor, dict)): item[u'_template'] = self.template.id items.append(item) return items",if item:,if item:,0.8589621884242519,0.8901732118131125,True
2935,"def create_volume(self, volume): """"""Create a volume."""""" try: cmd = ['volume', 'create', volume['name'], '%sG' % volume['size']] <IF_STMT> cmd.append('pool') cmd.append(self.configuration.eqlx_pool) if self.configuration.san_thin_provision: cmd.append('thin-provision') out = self._eql_execute(*cmd) self.add_multihost_access(volume) return self._get_volume_data(out) except Exception: with excutils.save_and_reraise_exception(): LOG.error('Failed to create volume ""%s"".', volume['name'])",if self.configuration.eqlx_pool != 'default':,if self.configuration.eqlx_pool:,0.9298489111271708,0.9024521756077707,False
2936,"def clean(self): if not self.code: self.code = u'static-%s' % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter(code=self.code, site__isnull=True) <IF_STMT> placeholders = placeholders.exclude(pk=self.pk) if placeholders.exists(): raise ValidationError(_('A static placeholder with the same site and code already exists'))",if self.pk:,if self.pk:,0.9371436136119997,0.8966773400768917,True
2937,"def spawnMenu(self, event): clickedPos = self.getRowByAbs(event.Position) self.ensureSelection(clickedPos) selection = self.getSelectedBoosters() mainBooster = None if clickedPos != -1: try: booster = self.boosters[clickedPos] except IndexError: pass else: <IF_STMT> mainBooster = booster itemContext = None if mainBooster is None else _t('Booster') menu = ContextMenu.getMenu(self, mainBooster, selection, ('boosterItem', itemContext), ('boosterItemMisc', itemContext)) if menu: self.PopupMenu(menu)",if booster in self.original:,if booster is not None:,0.913129984593347,0.8753524256584351,False
2938,"def init_errorhandler(): for ex in default_exceptions: <IF_STMT> app.register_error_handler(ex, error_http) elif ex == 500: app.register_error_handler(ex, internal_error) if services.ldap:  @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug('LDAP server not accessible while trying to login to opds feed') return error_http(FailedDependency())",if ex < 500:,if ex == 404:,0.9089254718184339,0.8385130047130208,False
2939,"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): <IF_STMT> t = anytype elif 'M' in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif 'i' in fmt: t = int elif 'f' in fmt: t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",if shape:,if fmt == 'D':,0.9143680457305823,0.8902056737869248,False
2940,def Proc2(IntParIO): IntLoc = IntParIO + 10 while True: if Char1Glob == 'A': IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 <IF_STMT> break return IntParIO,if EnumLoc == Ident1:,if EnumLoc == Ident1:,0.688792840471441,0.8228500218338367,True
2941,"def opengroup(self, name=None): gid = self.groups self.groupwidths.append(None) if self.groups > MAXGROUPS: raise error('too many groups') if name is not None: ogid = self.groupdict.get(name, None) <IF_STMT> raise error('redefinition of group name %r as group %d; was group %d' % (name, gid, ogid)) self.groupdict[name] = gid return gid",if ogid is not None:,if ogid is not None:,0.9134497989421839,0.8649799950178215,True
2942,"def __setattr__(self, name: str, val: Any): if name.startswith('COMPUTED_'): if name in self: old_val = self[name] <IF_STMT> return raise KeyError(""Computed attributed '{}' already exists with a different value! old={}, new={}."".format(name, old_val, val)) self[name] = val else: super().__setattr__(name, val)",if old_val == val:,if old_val == val:,0.7989728826263562,0.8555308664663046,True
2943,"def get_all_function_symbols(self, module='kernel'): """"""Gets all the function tuples for the given module"""""" ret = [] symtable = self.type_map if module in symtable: mod = symtable[module] for addr, (name, _sym_types) in mod.items(): <IF_STMT> addr = addr + self.shift_address ret.append([name, addr]) else: debug.info('All symbols requested for non-existent module %s' % module) return ret",if self.shift_address and addr:,if name == 'kernel':,0.907670437529667,0.8944264839442453,False
2944,"def __call__(self, frame: FrameType, event: str, arg: Any) -> 'CallTracer': code = frame.f_code if event not in SUPPORTED_EVENTS or code.co_name == 'trace_types' or (self.should_trace and (not self.should_trace(code))): return self try: <IF_STMT> self.handle_call(frame) elif event == EVENT_RETURN: self.handle_return(frame, arg) else: logger.error('Cannot handle event %s', event) except Exception: logger.exception('Failed collecting trace') return self",if event == EVENT_CALL:,if event == EVENT_CALL:,0.8224303181415926,0.8944264839442453,True
2945,def test_update_topic(self): async with self.chat_client: await self._create_thread() topic = 'update topic' async with self.chat_thread_client: await self.chat_thread_client.update_topic(topic=topic) <IF_STMT> await self.chat_client.delete_chat_thread(self.thread_id),if not self.is_playback():,if self.thread_id:,0.6791656896390428,0.8193882146581177,False
2946,"def render_observation(self): x = self.read_head_position label = 'Observation Grid: ' x_str = '' for j in range(-1, self.rows + 1): if j != -1: x_str += ' ' * len(label) for i in range(-2, self.input_width + 2): <IF_STMT> x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += '\n' x_str = label + x_str return x_str",if i == x[0] and j == x[1]:,if i == 0:,0.966432147217171,0.9084940438173679,False
2947,"def build(opt): dpath = os.path.join(opt['datapath'], 'QA-ZRE') version = None if not build_data.built(dpath, version_string=version): print('[building data: ' + dpath + ']') <IF_STMT> build_data.remove_dir(dpath) build_data.make_dir(dpath) for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)",if build_data.built(dpath):,if build_data.built(dpath):,0.9090542280792658,0.8787142254774354,True
2948,"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get('origin', '') <IF_STMT> origin = url url = repo.remotes.get(origin) if url: repo.pull(origin_uri=url) else: print('No pull URL.') else: print(command_help['git pull'])",if url in repo.remotes:,if not origin:,0.7285179097795668,0.8783650674919876,False
2949,"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b'' last_sop_idx = sop_idx = 0 skip = True for opcode, data, sop_idx in script.raw_iter(): <IF_STMT> r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx if script[sop_idx:sop_idx + len(sig)] == sig: skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",if not skip:,if opcode == 'FindAndDelete':,0.8706764888734144,0.9019629427251674,False
2950,"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result['venture_id'] = ip.venture.id if ip.device is not None: result['device_id'] = ip.device.id <IF_STMT> result['venture_id'] = ip.device.venture.id return result",if ip.device.venture is not None:,if ip.device.venture is not None:,0.7797708195922284,0.8492988135354755,True
2951,"def restore(self, state): """"""Restore the state of a mesh previously saved using save()"""""" import pickle state = pickle.loads(state) for k in state: if isinstance(state[k], list): <IF_STMT> state[k] = [[v.x(), v.y(), v.z()] for v in state[k]] state[k] = np.array(state[k]) setattr(self, k, state[k])","if isinstance(state[k][0], QtGui.QVector3D):",if len(state[k]) == 2:,0.7663131500380151,0.8692960007731574,False
2952,"def get_extra_lines(tup): ext_name, pyopencl_ver = tup if ext_name is not None: <IF_STMT> yield '' yield ('Available with OpenCL %s.' % ext_name[3:]) yield '' else: yield '' yield ('Available with the ``%s`` extension.' % ext_name) yield '' if pyopencl_ver is not None: yield '' yield ('.. versionadded:: %s' % pyopencl_ver) yield ''",if ext_name.startswith('CL_'):,if ext_name.startswith('OpenCL'):,0.7716050155333779,0.9298663600557577,False
2953,"def _gen_remote_uri(fileobj: IO[bytes], remote_uri: Optional[ParseResult], remote_path_prefix: Optional[str], remote_path_suffix: Optional[str], sha256sum: Optional[str]) -> ParseResult: if remote_uri is None: assert remote_path_prefix is not None and remote_path_suffix is not None <IF_STMT> sha256sum = _hash_fileobj(fileobj) return urlparse(os.path.join(remote_path_prefix, f'{sha256sum}{remote_path_suffix}')) else: return remote_uri",if sha256sum is None:,if sha256sum is None:,0.719114258691256,0.8555308664663046,True
2954,"def queries(self): if DEV: cmd = ShellCommand('docker', 'ps', '-qf', 'name=%s' % self.path.k8s) if not cmd.check(f'docker check for {self.path.k8s}'): <IF_STMT> log_cmd = ShellCommand('docker', 'logs', self.path.k8s, stderr=subprocess.STDOUT) if log_cmd.check(f'docker logs for {self.path.k8s}'): print(cmd.stdout) pytest.exit(f'container failed to start for {self.path.k8s}') return ()",if not cmd.stdout.strip():,if cmd.stdout:,0.9420768473779736,0.9076141716697395,False
2955,def get_range(self): present = self.xml.find('{%s}range' % self.namespace) if present is not None: attributes = present.attrib return_value = dict() <IF_STMT> return_value['minimum'] = attributes['min'] if 'max' in attributes: return_value['maximum'] = attributes['max'] return return_value return False,if 'min' in attributes:,if 'min' in attributes:,0.803662313923697,0.8385130047130208,True
2956,"def _configuredOn(self, workerid, builderid=None, masterid=None): cfg = [] for cs in itervalues(self.configured): <IF_STMT> continue bid, mid = self.db.builders.builder_masters[cs['buildermasterid']] if builderid is not None and bid != builderid: continue if masterid is not None and mid != masterid: continue cfg.append({'builderid': bid, 'masterid': mid}) return cfg",if cs['workerid'] != workerid:,if cs['workerid'] != workerid:,0.8810748052311885,0.8780099567239787,True
2957,"def __exit__(self, type, value, traceback): try: if type is not None: return self.exception_handler(type, value, traceback) finally: final_contexts = _state.contexts _state.contexts = self.old_contexts <IF_STMT> raise StackContextInconsistentError('stack_context inconsistency (may be caused by yield within a ""with StackContext"" block)') self.new_contexts = None",if final_contexts is not self.new_contexts:,if final_contexts != self.new_contexts:,0.7002141817555873,0.8627586293513119,False
2958,"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: <IF_STMT> return None elif self._keys[hash_] == key: self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: return None",if self._keys[hash_] is self._empty:,if hash_ == initial_hash:,0.750721395571187,0.8516228624291206,False
2959,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_logout_url(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.5745717035656124,0.7378351342269067,True
2960,"def data_generator(): i = 0 max_batch_index = len(X_train) // batch_size tot = 0 while 1: <IF_STMT> yield (np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan) else: yield (X_train[i * batch_size:(i + 1) * batch_size], y_train[i * batch_size:(i + 1) * batch_size]) i += 1 tot += 1 i = i % max_batch_index",if tot > 3 * len(X_train):,if tot == max_batch_index:,0.9618015109414639,0.8964173245779284,False
2961,"def title(self): ret = theme['title'] if isinstance(self.name, six.string_types): width = self.statwidth() return ret + self.name[0:width].center(width).replace(' ', '-') + theme['default'] for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace(' ', '-') <IF_STMT> if op.color: ret = ret + theme['frame'] + char['dash'] + theme['title'] else: ret = ret + char['space'] return ret",if i + 1 != len(self.vars):,if i == 0:,0.8084472197684496,0.9001816649635144,False
2962,"def get_container_from_dport(dport, docker_client): for container in docker_client.containers(): try: ports = container['Ports'] for port in ports: <IF_STMT> if port['PublicPort'] == int(dport): return container except KeyError: print(ports) pass",if 'PublicPort' in port:,if 'PublicPort' in port:,0.8684132606986068,0.7965020533851944,True
2963,"def _get_parents_data(self, data): parents = 0 if data[COLUMN_PARENT]: family = self.db.get_family_from_handle(data[COLUMN_PARENT][0]) if family.get_father_handle(): parents += 1 <IF_STMT> parents += 1 return parents",if family.get_mother_handle():,elif family.get_father_handle():,0.5688346515126883,0.8318180062062374,False
2964,"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) if old_mtime == mtime: cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) <IF_STMT> cache.popitem(last=False) return result",if len(cache) > max_size:,if cache.popitem(last=False):,0.8906164016265556,0.9051034981560222,False
2965,"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) <IF_STMT> cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",elif op.stage == OperandStage.agg:,elif op.stage == OperandStage.agg:,0.6371187289265657,0.828399516355805,True
2966,"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b'' last_sop_idx = sop_idx = 0 skip = True for opcode, data, sop_idx in script.raw_iter(): if not skip: r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx <IF_STMT> skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",if script[sop_idx:sop_idx + len(sig)] == sig:,if opcode == b'FindAndDelete':,0.835788525254991,0.8964173245779284,False
2967,"def extractall(zip: typing.Any, path: str) -> NoneType: for name in zip.namelist(): member = zip.getinfo(name) extracted_path = zip._extract_member(member, path, None) attr = member.external_attr >> 16 <IF_STMT> os.chmod(extracted_path, attr)",if attr != 0:,if extracted_path is not None:,0.7194442784405092,0.7860440721285902,False
2968,"def find_all_gyptest_files(directory): result = [] for root, dirs, files in os.walk(directory): <IF_STMT> dirs.remove('.svn') result.extend([os.path.join(root, f) for f in files if is_test_name(f)]) result.sort() return result",if '.svn' in dirs:,if '.svn' in dirs:,0.7225275191946195,0.7801270245332924,True
2969,"def load(cls, storefile, template_store): if not hasattr(storefile, 'read'): storefile = open(storefile, 'rb') store = cls.convertfile(storefile, template_store) for unit in store.units: if unit.isheader(): continue <IF_STMT> unit.target = unit.source unit.rich_target = unit.rich_source return store",if cls.needs_target_sync:,if unit.is_target():,0.821381317407132,0.8866029039778043,False
2970,"def postOptions(self): _BasicOptions.postOptions(self) if self['jobs']: conflicts = ['debug', 'profile', 'debug-stacktraces', 'exitfirst'] for option in conflicts: if self[option]: raise usage.UsageError(""You can't specify --%s when using --jobs"" % option) if self['nopm']: <IF_STMT> raise usage.UsageError('You must specify --debug when using --nopm ') failure.DO_POST_MORTEM = False",if not self['debug']:,if self['debug']:,0.9059686285995299,0.9144061946646023,False
2971,"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: if not i < len(extra.tokens): break entry = extra.tokens[i] token = jsdict({'type': entry.type, 'value': entry.value}) if extra.range: token.range = entry.range <IF_STMT> token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",if extra.loc:,if extra.loc:,0.8425939988382735,0.9284304001296656,True
2972,"def on_rebalance_end(self) -> None: """"""Call when rebalancing is done."""""" self.rebalancing = False if self._rebalancing_span: self._rebalancing_span.finish() self._rebalancing_span = None sensor_state = self._rebalancing_sensor_state try: <IF_STMT> self.log.warning('Missing sensor state for rebalance #%s', self.rebalancing_count) else: self.sensors.on_rebalance_end(self, sensor_state) finally: self._rebalancing_sensor_state = None",if not sensor_state:,if sensor_state is None:,0.7380803915269929,0.8555308664663046,False
2973,"def decorator(request, *args, **kwargs): if CALENDAR_VIEW_PERM: user = request.user if not user: return HttpResponseRedirect(settings.LOGIN_URL) occurrence, event, calendar = get_objects(request, **kwargs) if calendar: allowed = CHECK_CALENDAR_PERM_FUNC(calendar, user) <IF_STMT> return HttpResponseRedirect(settings.LOGIN_URL) return function(request, *args, **kwargs) return HttpResponseNotFound('<h1>Page not found</h1>') return function(request, *args, **kwargs)",if not allowed:,if not allowed:,0.9293820039662012,0.8902579342581529,True
2974,"def reduce_arguments(self, args): assert isinstance(args, nodes.Arguments) if args.incorrect_order(): raise InvalidArguments('All keyword arguments must be after positional arguments.') reduced_pos = [self.reduce_single(arg) for arg in args.arguments] reduced_kw = {} for key in args.kwargs.keys(): <IF_STMT> raise InvalidArguments('Keyword argument name is not a string.') a = args.kwargs[key] reduced_kw[key] = self.reduce_single(a) return (reduced_pos, reduced_kw)","if not isinstance(key, str):","if not isinstance(key, str):",0.8928094813165206,0.8902056737869248,True
2975,"def _encode(n, nbytes, little_endian=False): retval = [] n = long(n) for i in range(nbytes): <IF_STMT> retval.append(chr(n & 255)) else: retval.insert(0, chr(n & 255)) n >>= 8 return ''.join(retval)",if little_endian:,if little_endian:,0.8987943464844138,0.8696398662122882,True
2976,"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() new_i.id = self.id cls.id = old_id for prop in cls.properties: if prop is not 'members': <IF_STMT> val = getattr(self, prop) setattr(new_i, prop, val) new_i.members = [] return new_i",if self.has(prop):,"if hasattr(self, prop):",0.9264500917767978,0.884617925078158,False
2977,"def dataspec(config): master = (yield fakemaster.make_master()) data = connector.DataConnector() data.setServiceParent(master) if config['out'] != '--': dirs = os.path.dirname(config['out']) <IF_STMT> os.makedirs(dirs) f = open(config['out'], 'w') else: f = sys.stdout if config['global'] is not None: f.write('window.' + config['global'] + '=') f.write(json.dumps(data.allEndpoints(), indent=2)) f.close() defer.returnValue(0)",if dirs and (not os.path.exists(dirs)):,if not os.path.exists(dirs):,0.909460353745643,0.8902579342581529,False
2978,"def _parseSCDOCDC(self, src): """"""[S|CDO|CDC]*"""""" while 1: src = src.lstrip() <IF_STMT> src = src[4:] elif src.startswith('-->'): src = src[3:] else: break return src",if src.startswith('<!--'):,if src.startswith('--'):,0.6931819287785089,0.8318180062062374,False
2979,"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) if visitor.needs_fix(): print('%s: %s' % (filename, visitor.get_stats())) <IF_STMT> print('Fixing: %s' % filename) fix_file(filename)",if fix:,elif fix:,0.8426713692948238,0.8531413606256201,False
2980,"def shutdown(self): """"""Shutdown host system."""""" self._check_dbus(MANAGER) use_logind = self.sys_dbus.logind.is_connected _LOGGER.info('Initialize host power off %s', 'logind' if use_logind else 'systemd') try: await self.sys_core.shutdown() finally: <IF_STMT> await self.sys_dbus.logind.power_off() else: await self.sys_dbus.systemd.power_off()",if use_logind:,if use_logind:,0.9051335848933918,0.8743414417652072,True
2981,"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] if _is_punctuation(char): output.append([char]) start_new_word = True else: <IF_STMT> output.append([]) start_new_word = False output[-1].append(char) i += 1 return [''.join(x) for x in output]",if start_new_word:,if start_new_word:,0.8204199486506933,0.9434724611166208,True
2982,"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: if tp == 'write': out.write(msg) <IF_STMT> out.flush() elif tp == 'write_flush': out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",elif tp == 'flush':,elif tp == 'write_flush':,0.9294957189318612,0.8627586293513119,False
2983,"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) <IF_STMT> results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results","elif isinstance(in_arrays, dict):","elif isinstance(in_arrays, list):",0.8675445524872074,0.8635707684233572,False
2984,"def check_billing_view(user, permission, obj): if hasattr(obj, 'all_projects'): <IF_STMT> return True return any((check_permission(user, permission, prj) for prj in obj.all_projects)) return check_permission(user, permission, obj)",if user.is_superuser or obj.owners.filter(pk=user.pk).exists():,if obj.billing_view == 'all':,0.8295080510453556,0.760856626273165,False
2985,"def ensure_output_spaces_contain_the_same_data(self, y, y_ensured): stride = y.shape[1] self.assertEqual(y.shape[0] * y.shape[1], y_ensured.shape[0]) self.assertEqual(len(y_ensured.shape), 1) for row in range(y.shape[0]): for column in range(y.shape[1]): <IF_STMT> self.assertEqual(y[row, column], y_ensured[row * stride + column]) else: self.assertEqual(y[row][column], y_ensured[row * stride + column])",if sp.issparse(y):,if row == 0:,0.9150810782111691,0.8516228624291206,False
2986,"def train(self, training_data: TrainingData, config: Optional[RasaNLUModelConfig]=None, **kwargs: Any) -> None: """"""Tokenize all training data."""""" for example in training_data.training_examples: for attribute in MESSAGE_ATTRIBUTES: if example.get(attribute) is not None and (not example.get(attribute) == ''): <IF_STMT> tokens = self._split_name(example, attribute) else: tokens = self.tokenize(example, attribute) example.set(TOKENS_NAMES[attribute], tokens)","if attribute in [INTENT, ACTION_NAME, INTENT_RESPONSE_KEY]:",if attribute in TOKENS_NAMES:,0.8711832374879497,0.8780099567239787,False
2987,"def refresh_token(self, strategy, *args, **kwargs): token = self.extra_data.get('refresh_token') or self.extra_data.get('access_token') backend = self.get_backend(strategy) if token and backend and hasattr(backend, 'refresh_token'): backend = backend(strategy=strategy) response = backend.refresh_token(token, *args, **kwargs) extra_data = backend.extra_data(self, self.uid, response, self.extra_data) <IF_STMT> self.save()",if self.set_extra_data(extra_data):,if extra_data:,0.8712385138298884,0.9220630473024066,False
2988,"def _verify_environ(_collected_environ): try: yield finally: new_environ = dict(os.environ) current_test = new_environ.pop('PYTEST_CURRENT_TEST', None) old_environ = dict(_collected_environ) old_environ.pop('PYTEST_CURRENT_TEST', None) <IF_STMT> raise DirtyTest('Left over environment variables', current_test, _compare_eq_dict(new_environ, old_environ, verbose=2))",if new_environ != old_environ:,if current_test != old_environ:,0.9029912462903367,0.8038019482772603,False
2989,"def clean_len(self, line): """"""Calculate wisible length of string"""""" if isinstance(line, basestring): return len(self.screen.markup.clean_markup(line)) elif isinstance(line, tuple) or isinstance(line, list): markups = self.screen.markup.get_markup_vars() length = 0 for i in line: <IF_STMT> length += len(i) return length",if i not in markups:,if i not in markups:,0.8512668420274045,0.8248765135255685,True
2990,"def _build_merged_dataset_args(datasets): merged_dataset_args = [] for dataset in datasets: dataset_code_column = _parse_dataset_code(dataset) arg = dataset_code_column['code'] column_index = dataset_code_column['column_index'] <IF_STMT> arg = (dataset_code_column['code'], {'column_index': [column_index]}) merged_dataset_args.append(arg) return merged_dataset_args",if column_index is not None:,if column_index:,0.8414802979215663,0.8645707301556367,False
2991,"def update_watch_data_table_paths(self): if hasattr(self.tool_data_watcher, 'monitored_dirs'): for tool_data_table_path in self.tool_data_paths: <IF_STMT> self.tool_data_watcher.watch_directory(tool_data_table_path)",if tool_data_table_path not in self.tool_data_watcher.monitored_dirs:,if tool_data_table_path not in self.monitored_dirs:,0.5361667908116095,0.5659119256652702,False
2992,"def getsource(obj): """"""Wrapper around inspect.getsource"""""" try: try: src = encoding.to_unicode(inspect.getsource(obj)) except TypeError: <IF_STMT> src = encoding.to_unicode(inspect.getsource(obj.__class__)) else: src = getdoc(obj) return src except (TypeError, IOError): return","if hasattr(obj, '__class__'):",if inspect.isclass(obj):,0.647627649158896,0.8590888738245122,False
2993,"def __iter__(self): for model in self.app_config.get_models(): admin_model = AdminModel(model, **self.options) for model_re in self.model_res: if model_re.search(admin_model.name): break else: <IF_STMT> continue yield admin_model",if self.model_res:,if admin_model.name in self.exclude_models:,0.8742223278163569,0.760856626273165,False
2994,"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() if self._has_stop_signal(raw_inputs): self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) <IF_STMT> try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",elif self._flow_type == REALTIME:,if self._flow_type == RESTART:,0.6469385393668824,0.8228500218338367,False
2995,"def dump(self): self.ql.log.info('[*] Dumping object: %s' % self.sf_name) for field in self._fields_: <IF_STMT> self.ql.log.info('%s: 0x%x' % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info('%s: %d' % (field[0], getattr(self, field[0]))) elif isinstance(getattr(self, field[0]), bytes): self.ql.log.info('%s: %s' % (field[0], getattr(self, field[0]).decode()))","if isinstance(getattr(self, field[0]), POINTER64):","if isinstance(getattr(self, field[0]), float):",0.9166995514558118,0.8627586293513119,False
2996,"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): """"""Validating that user has inputted a value set and that configuration has been initialized"""""" super().validate_configuration(configuration) try: assert 'value_set' in configuration.kwargs, 'value_set is required' assert isinstance(configuration.kwargs['value_set'], (list, set, dict)), 'value_set must be a list or a set' <IF_STMT> assert '$PARAMETER' in configuration.kwargs['value_set'], 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True","if isinstance(configuration.kwargs['value_set'], dict):",if 'evaluation_parameter' in configuration.kwargs['value_set']:,0.9409490944461635,0.914208565914368,False
2997,def test_one_dead_branch(): with deterministic_PRNG(): seen = set()  @run_to_buffer def x(data): i = data.draw_bytes(1)[0] if i > 0: data.mark_invalid() i = data.draw_bytes(1)[0] if len(seen) < 255: seen.add(i) <IF_STMT> data.mark_interesting(),elif i not in seen:,if seen:,0.9411157065054148,0.899160928885317,False
2998,"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) if event.EventObject is self.list_ctrl: self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): <IF_STMT> self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",if self.list_ctrl.IsSelected(index):,if self.list_ctrl.GetItem(index) == self.__module_view:,0.8378560574024145,0.7801270245332924,False
2999,"def prime(self, callback): <IF_STMT> self.cbhdl = simulator.register_rwsynch_callback(callback, self) if self.cbhdl is None: raise_error(self, 'Unable set up %s Trigger' % str(self)) Trigger.prime(self)",if self.cbhdl is None:,if self.cbhdl is None:,0.8480298426096549,0.7498810286408993,True
3000,"def fstab_configuration(middleware): for command in [['systemctl', 'daemon-reload'], ['systemctl', 'restart', 'local-fs.target']] if osc.IS_LINUX else [['mount', '-uw', '/']]: ret = subprocess.run(command, capture_output=True) <IF_STMT> middleware.logger.debug(f'''Failed to execute ""{' '.join(command)}"": {ret.stderr.decode()}''')",if ret.returncode:,if ret.stderr:,0.653647646569316,0.8645707301556367,False
3001,"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield (simple_colorize(fromdesc, 'description'), simple_colorize(todesc, 'description')) for i, line in enumerate(diffs): <IF_STMT> if i > 0: yield (simple_colorize('---', 'separator'), simple_colorize('---', 'separator')) else: yield line",if line is None:,if line.startswith('---'):,0.9080712560444529,0.8866029039778043,False
3002,"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = ', '.join(orig_text.replace(', ', ',').split(',')[:-1]) tags = [] for tag in self.tags_list: if ',' in orig_text: <IF_STMT> tags.append('%s,%s' % (text, tag)) tags.append('%s, %s' % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)","if orig_text[-1] not in (',', ' '):",if tag not in tags:,0.9513979017668934,0.8621109017306224,False
3003,"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord('0') <IF_STMT> digit *= 2 if digit >= 10: digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",if even:,if even:,0.8200421299795952,0.9350761925543661,True
3004,def __get_param_string__(params): params_string = [] for key in sorted(params.keys()): <IF_STMT> return value = params[key] params_string.append('' if value == 'null' else str(value)) return '|'.join(params_string),if 'REFUND' in params[key] or '|' in params[key]:,if key == 'null':,0.8125780057297791,0.7709002428237395,False
3005,"def _map_handlers(self, session, event_class, mapfn): for event in DOC_EVENTS: event_handler_name = event.replace('-', '_') <IF_STMT> event_handler = getattr(self, event_handler_name) format_string = DOC_EVENTS[event] num_args = len(format_string.split('.')) - 2 format_args = (event_class,) + ('*',) * num_args event_string = event + format_string % format_args unique_id = event_class + event_handler_name mapfn(event_string, event_handler, unique_id)","if hasattr(self, event_handler_name):",if event_handler_name in session.events:,0.7342871227181395,0.8879659171421962,False
3006,"def _create_param_lr(self, param_and_grad): param = param_and_grad[0] param_lr = param.optimize_attr['learning_rate'] if type(param_lr) == Variable: return param_lr el<IF_STMT> return self._global_learning_rate() else: with default_main_program()._lr_schedule_guard(is_with_opt=True), framework.name_scope('scale_with_param_lr'): return self._global_learning_rate() * param_lr",if param_lr == 1.0:,if param_lr is None:,0.8311941403690413,0.7965020533851944,False
3007,"def __getitem__(self, key): try: return self._clsmap[key] except KeyError as e: <IF_STMT> self._mutex.acquire() try: if not self.initialized: self._init() self.initialized = True return self._clsmap[key] finally: self._mutex.release() raise e",if not self.initialized:,if self._mutex is not None:,0.6521626175247629,0.7685107079449489,False
3008,"def save(self, force=False): if not force: <IF_STMT> return if time.time() - self.last_save_time < 10: return with self.lock: with open(self.file_path, 'w') as fd: for ip in self.cache: record = self.cache[ip] rule = record['r'] connect_time = record['c'] update_time = record['update'] fd.write('%s %s %d %d\n' % (ip, rule, connect_time, update_time)) self.last_save_time = time.time() self.need_save = False",if not self.need_save:,if self.need_save:,0.908007880068374,0.9325718821645923,False
3009,"def pick(items, sel): for x, s in zip(items, sel): <IF_STMT> yield x elif not x.is_atom() and (not s.is_atom()): yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",if match(s):,if x.is_atom() and (not s.is_atom()):,0.5998244856217838,0.7406093667638122,False
3010,"def isValidFloat(config_param_name, value, constraints): if isinstance(value, float): constraints.setdefault('min', MIN_VALID_FLOAT_VALUE) constraints.setdefault('max', MAX_VALID_FLOAT_VALUE) minv = float(constraints.get('min')) maxv = float(constraints.get('max')) <IF_STMT> if value <= maxv: return value raise FloatValueError(config_param_name, value, constraints)",if value >= minv:,if value >= minv:,0.630229250985858,0.8105932471967202,True
3011,"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if 'meta-environment' in root or 'cross-canadian' in root: continue <IF_STMT> continue if 'do_build' not in name and 'do_populate_sdk' not in name: f.append(os.path.join(root, name)) return f",if 'qemux86copy-' in root or 'qemux86-' in root:,if 'build' in root and 'do_populate_sdk' in root:,0.8749326305971286,0.7899177245850753,False
3012,"def __get_photo(self, person_or_marriage): """"""returns the first photo in the media list or None"""""" media_list = person_or_marriage.get_media_list() for media_ref in media_list: media_handle = media_ref.get_reference_handle() media = self.database.get_media_from_handle(media_handle) mime_type = media.get_mime_type() <IF_STMT> return media return None",if mime_type and mime_type.startswith('image'):,if mime_type == 'photo':,0.6935740458288898,0.8431339019329497,False
3013,"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError('TypeError', 'callbackfn must be a function') _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) <IF_STMT> res.append(kValue) k += 1 return args.space.ConstructArray(res)","if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):","if callbackfn(kValue, _this):",0.9097029840309329,0.9100365300271298,False
3014,"def optimize(self, graph: Graph): for v in graph.inputs: if not v.has_attribute(SplitTarget): continue <IF_STMT> DumpGraph().optimize(graph) raise NotImplementedError(f'Input Variable {v} is too large to handle in WebGL backend') return (graph, False)",if flags.DEBUG:,if len(v.get_attributes()) > self.max_input_size:,0.8561848041488893,0.8169276475307028,False
3015,"def detach_volume(self, volume): for node in self.list_nodes(): if type(node.image) is not list: continue for disk in node.image: <IF_STMT> disk_id = disk.extra['disk_id'] return self._do_detach_volume(node.id, disk_id) return False",if disk.id == volume.id:,if disk.extra['volume_type'] == volume.disk_type:,0.7942731369172001,0.7965020533851944,False
3016,"def Yield(value, level=1): g = greenlet.getcurrent() while level != 0: if not isinstance(g, genlet): raise RuntimeError('yield outside a genlet') <IF_STMT> g.parent.set_child(g) g = g.parent level -= 1 g.switch(value)",if level > 1:,if g.parent:,0.7968939824406023,0.8696398662122882,False
3017,"def get_all_pipeline_nodes(pipeline: pipeline_pb2.Pipeline) -> List[pipeline_pb2.PipelineNode]: """"""Returns all pipeline nodes in the given pipeline."""""" result = [] for pipeline_or_node in pipeline.nodes: which = pipeline_or_node.WhichOneof('node') <IF_STMT> result.append(pipeline_or_node.pipeline_node) else: raise NotImplementedError('Only pipeline nodes supported.') return result",if which == 'pipeline_node':,if which in pipeline_or_node.pipeline_nodes:,0.7337468221526064,0.8385130047130208,False
3018,"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): if not hasattr(self, name): setattr(self, name, value) for name, value in settings.items(): <IF_STMT> raise ImproperlyConfigured(""Invalid setting '{}' for {}"".format(name, self.__class__.__name__)) setattr(self, name, value)",if name not in default_settings:,"if not hasattr(self, name):",0.7919379115245602,0.8431339019329497,False
3019,"def _check_choice(self): if self.type == 'choice': <IF_STMT> raise OptionError(""must supply a list of choices for type 'choice'"", self) elif type(self.choices) not in (types.TupleType, types.ListType): raise OptionError(""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self) elif self.choices is not None: raise OptionError('must not supply choices for type %r' % self.type, self)",if self.choices is None:,if self.choices is None:,0.6328406160910537,0.8983343737277126,True
3020,"def prepare(self, size=None): if _is_seekable(self.file): start_pos = self.file.tell() self.file.seek(0, 2) end_pos = self.file.tell() self.file.seek(start_pos) fsize = end_pos - start_pos <IF_STMT> self.remain = fsize else: self.remain = min(fsize, size) return self.remain",if size is None:,if size is None:,0.8331786119351965,0.8228500218338367,True
3021,"def _setSitemapTargets(): if not conf.sitemapUrl: return infoMsg = ""parsing sitemap '%s'"" % conf.sitemapUrl logger.info(infoMsg) found = False for item in parseSitemap(conf.sitemapUrl): <IF_STMT> found = True kb.targets.add((item.strip(), None, None, None, None)) if not found and (not conf.forms) and (not conf.crawlDepth): warnMsg = 'no usable links found (with GET parameters)' logger.warn(warnMsg)","if re.match('[^ ]+\\?(.+)', item, re.I):",if item.strip() == conf.url:,0.9190991638562032,0.8902056737869248,False
3022,"def test_CY_decomposition(self, tol): """"""Tests that the decomposition of the CY gate is correct"""""" op = qml.CY(wires=[0, 1]) res = op.decomposition(op.wires) mats = [] for i in reversed(res): <IF_STMT> mats.append(np.kron(i.matrix, np.eye(2))) else: mats.append(i.matrix) decomposed_matrix = np.linalg.multi_dot(mats) assert np.allclose(decomposed_matrix, op.matrix, atol=tol, rtol=0)",if len(i.wires) == 1:,if i.is_kron:,0.9288000076941288,0.9099951253570094,False
3023,"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break <IF_STMT> lidx += 1 if not start: start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",if stmt == lines[lidx]:,if stmt in lines:,0.8584322092157599,0.9099929453837925,False
3024,"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode='fan_out') <IF_STMT> init.constant(module.bias, 0) elif isinstance(module, nn.BatchNorm2d): init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=0.001) if module.bias: init.constant(module.bias, 0)",if module.bias:,"elif isinstance(module, nn.BatchNorm1d):",0.9236013988666344,0.8635707684233572,False
3025,def _get_directory_size_in_bytes(directory): total = 0 try: for entry in os.scandir(directory): <IF_STMT> total += entry.stat().st_size elif entry.is_dir(): total += _get_directory_size_in_bytes(entry.path) except NotADirectoryError: return os.path.getsize(directory) except PermissionError: return 0 return total,if entry.is_file():,if entry.is_file():,0.8921902387128126,0.8743414417652072,True
3026,"def run_cmd(self, util, to, always_push_mark=False): if to == 'bof': util.push_mark_and_goto_position(0) elif to == 'eof': util.push_mark_and_goto_position(self.view.size()) elif to in ('eow', 'bow'): visible = self.view.visible_region() pos = visible.a if to == 'bow' else visible.b <IF_STMT> util.push_mark_and_goto_position(pos) else: util.set_cursors([sublime.Region(pos)])",if always_push_mark:,if always_push_mark:,0.6945385032323113,0.8996480074924822,True
3027,"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if 'results.train_y_misclass' in dd: if dd['results.train_y_misclass'] < optimal_measure: optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): <IF_STMT> print(key + ': ' + str(value))",if 'hyper_parameters' in key:,if key != 'results':,0.9333829677803368,0.8780099567239787,False
3028,"def clean_vc_position(self): vc_position = self.cleaned_data['vc_position'] if self.validate_vc_position: conflicting_members = Device.objects.filter(virtual_chassis=self.instance.virtual_chassis, vc_position=vc_position) <IF_STMT> raise forms.ValidationError('A virtual chassis member already exists in position {}.'.format(vc_position)) return vc_position",if conflicting_members.exists():,if vc_position in conflicting_members:,0.6185114767153941,0.7801270245332924,False
3029,"def cal_pads(auto_pad, pad_shape): spatial_size = len(pad_shape) pads = [0] * spatial_size * 2 for i in range(spatial_size): if auto_pad == 'SAME_LOWER': pads[i + spatial_size] = pad_shape[i] // 2 pads[i] = pad_shape[i] - pads[i + spatial_size] <IF_STMT> pads[i] = pad_shape[i] // 2 pads[i + spatial_size] = pad_shape[i] - pads[i] return pads",elif auto_pad == 'SAME_UPPER':,elif auto_pad == 'SAME_UPPER':,0.9603297952212042,0.8923575006167597,True
3030,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.7568651057381599,0.828399516355805,True
3031,"def test_cwl_rnaseq(self, install_test_files): with install_cwl_test_files() as work_dir: with utils.chdir(os.path.join(work_dir, 'rnaseq')): <IF_STMT> shutil.rmtree('cromwell_work') subprocess.check_call(['bcbio_vm.py', 'cwlrun', 'cromwell', 'rnaseq-workflow'])",if os.path.exists('cromwell_work'):,if os.path.exists('cromwell_work'):,0.6066593832969289,0.762465858623486,True
3032,def files_per_version(self): xpath = './files/file' files = self.root.findall(xpath) versions = {} for file in files: vfile = file.findall('version') for version in vfile: nb = version.attrib['nb'] <IF_STMT> versions[nb] = [] versions[nb].append(file.attrib['url']) return versions,if not nb in versions:,if nb not in versions:,0.8342207919629967,0.8094220211349227,False
3033,"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('SQLite backend does not support timezone-aware datetimes when USE_TZ is False.') return six.text_type(value)",if settings.USE_TZ:,if self.USE_TZ:,0.8941347732381401,0.8787142254774354,False
3034,"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: t.status = 'aborted by user' <IF_STMT> status('%s aborted' % t.name, priority=2) except Exception as e: t.exception = e t.status = 'exception' vd.exceptionCaught(e) if t.sheet: t.sheet.currentThreads.remove(t)",if status:,if status:,0.6817019250806772,0.926934323706186,True
3035,"def ESP(phrase): for num, name in enumerate(devname): if name.lower() in phrase: dev = devid[num] <IF_STMT> ctrl = '=ON' say('Turning On ' + name) elif custom_action_keyword['Dict']['Off'] in phrase: ctrl = '=OFF' say('Turning Off ' + name) rq = requests.head('https://' + ip + dev + ctrl, verify=False)",if custom_action_keyword['Dict']['On'] in phrase:,if custom_action_keyword['Dict']['On'] in phrase:,0.7254321121168453,0.8806615362338783,True
3036,"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] <IF_STMT> parts.append('PRIMARY KEY') if not_null: parts.append('NOT NULL') result[name] = ' '.join(parts) return result",if primary_key:,if primary_key:,0.9217751365439623,0.9024521756077707,True
3037,"def _validate_forward_input(x, n_in): if n_in != 1: if not isinstance(x, (tuple, list)): raise TypeError(f'Expected input to be a tuple or list; instead got {type(x)}.') <IF_STMT> raise ValueError(f'Input tuple length ({len(x)}) does not equal required number of inputs ({n_in}).')",if len(x) != n_in:,if len(x) != n_in:,0.6415535648096544,0.8592377270804451,True
3038,"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): if isinstance(val, compat.string_types): return '  %s' % val <IF_STMT> return '  %.1f KB' % (val / 1024.0 ** 1) elif val < 1024 ** 3: return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val",elif val < 1024 ** 2:,elif val < 1024.0 ** 1:,0.6926497356601266,0.8916211056218591,False
3039,"def get_path_name(self): if self.is_root(): return '@' + self.name else: parent_name = self.parent.get_path_name() <IF_STMT> return '/'.join([parent_name, '@' + self.name]) else: return '@' + self.name",if parent_name:,if parent_name:,0.6807344176618099,0.839587623092576,True
3040,"def parse(cls, api, json): lst = List(api) setattr(lst, '_json', json) for k, v in json.items(): <IF_STMT> setattr(lst, k, User.parse(api, v)) elif k == 'created_at': setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",if k == 'user':,if k == 'user':,0.8964409045819786,0.8385130047130208,True
3041,"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: if not py_file.endswith('.py'): continue <IF_STMT> bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if self.compile:,if self.optimize > 0:,0.7838906908096287,0.8038019482772603,False
3042,"def to_json_dict(self): d = super().to_json_dict() d['bullet_list'] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: <IF_STMT> d['header'] = self.header.to_json_dict() else: d['header'] = self.header if self.subheader is not None: if isinstance(self.subheader, RenderedContent): d['subheader'] = self.subheader.to_json_dict() else: d['subheader'] = self.subheader return d","if isinstance(self.header, RenderedContent):","if isinstance(self.header, RenderedContent):",0.9075474198277758,0.8815741981066073,True
3043,"def makeSomeFiles(pathobj, dirdict): pathdict = {} for key, value in dirdict.items(): child = pathobj.child(key) if isinstance(value, bytes): pathdict[key] = child child.setContent(value) <IF_STMT> child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError('only strings and dicts allowed as values') return pathdict","elif isinstance(value, dict):","elif isinstance(value, dict):",0.7915522584323837,0.8815741981066073,True
3044,"def Restore(self): picker, obj = (self._window, self._pObject) value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: <IF_STMT> if type(value) == list: value = value[-1] picker.SetPath(value) return True return False","if issubclass(picker.__class__, wx.FileDialog):",if value:,0.9007262782369543,0.8696398662122882,False
3045,"def recv(self, buffer_size): try: return super(SSLConnection, self).recv(buffer_size) except ssl.SSLError as err: <IF_STMT> return b'' if err.args[0] in (ssl.SSL_ERROR_EOF, ssl.SSL_ERROR_ZERO_RETURN): self.handle_close() return b'' raise","if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):",if err.args[0] == ssl.SSL_ERROR_NO_DATA:,0.8805848563741985,0.7709002428237395,False
3046,"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in ('toplevel', 'detailed'): <IF_STMT> category = category.split('/')[0] if category not in self.errors_by_category: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",if self.counting != 'detailed':,if '/' in category:,0.8976534365350045,0.828399516355805,False
3047,"def _get_y(self, data_inst): if self.stratified: y = [v for i, v in data_inst.mapValues(lambda v: v.label).collect()] <IF_STMT> y = self.transform_regression_label(data_inst) else: y = [0] * data_inst.count() return y",if self.need_transform:,if self.regression:,0.8385628109383509,0.8645707301556367,False
3048,"def test_all_project_files(self): if sys.platform.startswith('win'): return for filepath in support.all_project_files(): with open(filepath, 'rb') as fp: encoding = tokenize.detect_encoding(fp.readline)[0] self.assertIsNotNone(encoding, ""can't detect encoding for %s"" % filepath) with open(filepath, 'r') as fp: source = fp.read() source = source.decode(encoding) tree = driver.parse_string(source) new = unicode(tree) <IF_STMT> self.fail('Idempotency failed: %s' % filepath)","if diff(filepath, new, encoding):",if not driver.is_valid(new):,0.8509494081571456,0.9062841320510342,False
3049,"def test_resource_arn_override_generator(self): overrides = set() for k, v in manager.resources.items(): arn_gen = bool(v.__dict__.get('get_arns') or v.__dict__.get('generate_arn')) <IF_STMT> overrides.add(k) overrides = overrides.difference({'account', 's3', 'hostedzone', 'log-group', 'rest-api', 'redshift-snapshot', 'rest-stage'}) if overrides: raise ValueError('unknown arn overrides in %s' % ', '.join(overrides))",if arn_gen:,if arn_gen:,0.7954915755019416,0.9024521756077707,True
3050,"def _check_dsl_runner(self) -> None: """"""Checks if runner in dsl is Kubeflow V2 runner."""""" with open(self.flags_dict[labels.PIPELINE_DSL_PATH], 'r') as f: dsl_contents = f.read() <IF_STMT> raise RuntimeError('KubeflowV2DagRunner not found in dsl.')",if 'KubeflowV2DagRunner' not in dsl_contents:,if not dsl_contents:,0.7401355636763828,0.8390782502060267,False
3051,"def create_warehouse(warehouse_name, properties=None, company=None): if not company: company = '_Test Company' warehouse_id = erpnext.encode_company_abbr(warehouse_name, company) if not frappe.db.exists('Warehouse', warehouse_id): warehouse = frappe.new_doc('Warehouse') warehouse.warehouse_name = warehouse_name warehouse.parent_warehouse = 'All Warehouses - _TCUV' warehouse.company = company warehouse.account = get_warehouse_account(warehouse_name, company) <IF_STMT> warehouse.update(properties) warehouse.save() return warehouse.name else: return warehouse_id",if properties:,if properties:,0.8851779482122571,0.9220450449751959,True
3052,"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): if not len(line.strip()): entries.append(('blank', [line])) continue head, tail = chop_comment(line.strip(), '#') <IF_STMT> entries.append(('all_comment', [line])) continue entries.append(('hostname', [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError('Multiple hostnames (%s) found!' % hostnames_found) return entries",if not len(head):,if head in hostnames_found:,0.8118460788036128,0.8806615362338783,False
3053,"def _get_omega(self): if self._omega is None: n = self.get_drift_dim() // 2 omg = sympl.calc_omega(n) if self.oper_dtype == Qobj: self._omega = Qobj(omg, dims=self.dyn_dims) self._omega_qobj = self._omega <IF_STMT> self._omega = sp.csr_matrix(omg) else: self._omega = omg return self._omega",elif self.oper_dtype == sp.csr_matrix:,elif self.oper_dtype == csr_matrix:,0.9433573557646207,0.8474968231198384,False
3054,"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v <IF_STMT> out = get_in_inputs(key, v) if out: return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]","elif isinstance(v, (list, tuple, dict)):","elif isinstance(v, (list, tuple)):",0.9382809329981233,0.8944264839442453,False
3055,def visit_binary(binary): if binary.operator == operators.eq: cols = util.column_set(chain(*[c.proxy_set for c in columns.difference(omit)])) <IF_STMT> for c in reversed(columns): if c.shares_lineage(binary.right) and (not only_synonyms or c.name == binary.left.name): omit.add(c) break,if binary.left in cols and binary.right in cols:,if not only_synonyms and binary.right.name not in cols:,0.5909290767633224,0.7221052041889016,False
3056,"def wait_tasks_or_abort(futures, timeout=60, kill_switch_ev=None): try: LazySingletonTasksCoordinator.wait_tasks(futures, return_when=FIRST_EXCEPTION, raise_exceptions=True) except Exception as e: <IF_STMT> kill_switch_ev.set() LazySingletonTasksCoordinator.wait_tasks(futures, return_when=ALL_COMPLETED, raise_exceptions=False, timeout=timeout) raise e",if kill_switch_ev is not None:,if kill_switch_ev:,0.8415676046893833,0.8137489370974955,False
3057,"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: <IF_STMT> return False elif isinstance(s, np.ndarray) and s.size == 0: return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True",if s is None:,if s is None:,0.7757290659407116,0.8592377270804451,True
3058,"def setVaName(self, va, parent=None): if parent is None: parent = self curname = self.vw.getName(va) if curname is None: curname = '' name, ok = QInputDialog.getText(parent, 'Enter...', 'Name', text=curname) if ok: name = str(name) <IF_STMT> raise Exception('Duplicate Name: %s' % name) self.vw.makeName(va, name)",if self.vw.vaByName(name):,if name in self.vw.getNames():,0.680681342063013,0.8723360571509826,False
3059,"def generic_tag_compiler(params, defaults, name, node_class, parser, token): """"""Returns a template.Node subclass."""""" bits = token.split_contents()[1:] bmax = len(params) def_len = defaults and len(defaults) or 0 bmin = bmax - def_len if len(bits) < bmin or len(bits) > bmax: <IF_STMT> message = '%s takes %s arguments' % (name, bmin) else: message = '%s takes between %s and %s arguments' % (name, bmin, bmax) raise TemplateSyntaxError(message) return node_class(bits)",if bmin == bmax:,if bmin < bmax:,0.7401756948172222,0.9168060593278899,False
3060,"def extract_segmentation_mask(annotation): poly_specs = annotation[DensePoseDataRelative.S_KEY] if isinstance(poly_specs, torch.Tensor): return poly_specs import pycocotools.mask as mask_utils segm = torch.zeros((DensePoseDataRelative.MASK_SIZE,) * 2, dtype=torch.float32) for i in range(DensePoseDataRelative.N_BODY_PARTS): poly_i = poly_specs[i] <IF_STMT> mask_i = mask_utils.decode(poly_i) segm[mask_i > 0] = i + 1 return segm",if poly_i:,"if isinstance(poly_i, str):",0.903868791573113,0.8875087479151215,False
3061,"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), 'ouroboros') for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) <IF_STMT> if module_name not in IGNORE_MODULES and module_name not in native: if not (fast and module_name in KNOWN_PROBLEM_MODULES): modules.append(module_name) return set(modules)","if ext == '.py' or (ext == '' and os.path.isdir(os.path.join(basedir, name))):",if ext == '.py':,0.7676724683068805,0.8923575006167597,False
3062,"def filelist_from_patterns(pats, rootdir=None): if rootdir is None: rootdir = '.' fileset = set([]) lines = [line.strip() for line in pats] for line in lines: pat = line[2:] newfiles = glob(osp.join(rootdir, pat)) if line.startswith('+'): fileset.update(newfiles) <IF_STMT> fileset.difference_update(newfiles) else: raise ValueError('line must start with + or -') filelist = list(fileset) return filelist",elif line.startswith('-'):,elif line.startswith('-') and line.startswith('-'):,0.9229037724082774,0.8923575006167597,False
3063,"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue if entity_ref not in statuses_by_refs: continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) <IF_STMT> statuses_by_refs[entity_ref].append(status) return statuses_by_refs",if status:,if status:,0.9133801640886028,0.9253742688467129,True
3064,"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value <IF_STMT> self._reverse_infos[info] = len(self._infos) self._infos.append(info) if reference not in self._reverse_references: self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = '%d,%d' % (self._reverse_infos[info], self._reverse_references[reference]) else: raise Exception(""unsupported type '%s'"" % type(value))",if info not in self._reverse_infos:,if info not in self._reverse_infos:,0.6632605587910592,0.8419539711731491,True
3065,"def ChangeStyle(self, combos): style = 0 for combo in combos: <IF_STMT> if combo.GetLabel() == 'TR_VIRTUAL': style = style | HTL.TR_VIRTUAL else: try: style = style | eval('wx.' + combo.GetLabel()) except: style = style | eval('HTL.' + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",if combo.GetValue() == 1:,if combo.GetLabel() != 'TR_VIRTUAL':,0.7216429135049648,0.8723360571509826,False
3066,"def _parse_csrf(self, response): for d in response: if d.startswith('Set-Cookie:'): for c in d.split(':', 1)[1].split(';'): <IF_STMT> self._CSRFtoken = c.strip(' \r\n') log.verbose('Got new cookie: %s', self._CSRFtoken) break if self._CSRFtoken != None: break",if c.strip().startswith('CSRF-Token-'):,if c.startswith('CSRFToken:'):,0.7624768681250913,0.8787142254774354,False
3067,"def test_page_size_matching_max_returned_rows(app_client_returned_rows_matches_page_size): fetched = [] path = '/fixtures/no_primary_key.json' while path: response = app_client_returned_rows_matches_page_size.get(path) fetched.extend(response.json['rows']) assert len(response.json['rows']) in (1, 50) path = response.json['next_url'] <IF_STMT> path = path.replace('http://localhost', '') assert 201 == len(fetched)",if path:,if path.startswith('http://localhost'):,0.8711199643037734,0.8827916928185874,False
3068,"def get_mapping_exception_message(mappings: List[Tuple[Text, Text]]): """"""Return a message given a list of duplicates."""""" message = '' for name, action_name in mappings: <IF_STMT> message += '\n' message += ""Intent '{}' is set to trigger action '{}', which is not defined in the domain."".format(name, action_name) return message",if message:,if message:,0.8924135830110497,0.9184043388013005,True
3069,def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: <IF_STMT> yield x,if x:,if x not in Force_Split_Words:,0.9007163165228825,0.8618733074124215,False
3070,"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = (expr.real, expr.imag) if -delta < real.get_float_value() < delta: real = Integer(0) <IF_STMT> imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",if -delta < imag.get_float_value() < delta:,elif -delta < imag.get_float_value() < delta:,0.8525037124543713,0.8688589397154922,False
3071,"def make_row(self): res = [] for i in range(self.num_cols): t = sqlite3_column_type(self.stmnt, i) if t == SQLITE_INTEGER: res.append(sqlite3_column_int(self.stmnt, i)) elif t == SQLITE_FLOAT: res.append(sqlite3_column_double(self.stmnt, i)) <IF_STMT> res.append(sqlite3_column_text(self.stmnt, i)) else: raise NotImplementedError return tuple(res)",elif t == SQLITE_TEXT:,elif t == SQLITE_TEXT:,0.8616444193455682,0.8385130047130208,True
3072,"def try_convert(self, string): string = string.strip() try: return int(string) except: try: return float(string) except: if string == 'True': return True <IF_STMT> return False return string",if string == 'False':,elif string == 'False':,0.8653390429646354,0.7886336751695258,False
3073,"def configure_create_table_epilogue(store): for val in ['', ' ENGINE=InnoDB']: store.config['create_table_epilogue'] = val store._set_sql_flavour() <IF_STMT> store.log.info(""create_table_epilogue='%s'"", val) return raise Exception('Can not create a transactional table.')",if store._test_transaction():,if store.config['create_table_epilogue'] == val:,0.6845076941246462,0.7709002428237395,False
3074,"def _check_rule(self, match, target_dict, cred_dict): """"""Recursively checks credentials based on the brains rules."""""" try: new_match_list = self.rules[match] except KeyError: <IF_STMT> new_match_list = ('rule:%s' % self.default_rule,) else: return False return self.check(new_match_list, target_dict, cred_dict)",if self.default_rule and match != self.default_rule:,if self.default_rule:,0.5929517797034589,0.8866029039778043,False
3075,"def get_civil_names(self): congresspeople_ids = self.get_all_congresspeople_ids() for i, congress_id in enumerate(congresspeople_ids): if not np.math.isnan(float(congress_id)): percentage = i / self.total * 100 msg = 'Processed {} out of {} ({:.2f}%)' print(msg.format(i, self.total, percentage), end='\r') data = self.fetch_data_repository(congress_id) <IF_STMT> yield dict(data)",if data is not None:,if data:,0.8406491219761257,0.9123160484027698,False
3076,"def parse_network_whitelist(self, network_whitelist_location): networks = [] with open(network_whitelist_location, 'r') as text_file: for line in text_file: line = line.strip().strip(""'"").strip('""') <IF_STMT> networks.append(line) return networks",if isIPv4(line) or isIPv6(line):,if line and line.startswith('network:'):,0.780277730938609,0.760856626273165,False
3077,"def _pick(self, cum): if self._isleaf(): return (self.bd[0], self.s) el<IF_STMT> return self.left._pick(cum) else: return self.right._pick(cum - self.left.s)",if cum < self.left.s:,if self.left.s == cum:,0.5688813271611848,0.674945488826271,False
3078,"def serialize_content_range(value): if isinstance(value, (tuple, list)): if len(value) not in (2, 3): raise ValueError('When setting content_range to a list/tuple, it must be length 2 or 3 (not %r)' % value) <IF_STMT> begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value",if len(value) == 2:,if len(value) == 2:,0.7622319293942034,0.9084940438173679,True
3079,"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): <IF_STMT> fields[k] = v continue if k == 'full_title': fields['title'] = [read_short_title(v)] return fields","if k in ('lccn', 'oclc', 'isbn'):",if k == 'name':,0.7442434200397772,0.7801270245332924,False
3080,"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) if action == 'deletion': del translation[ind] <IF_STMT> ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",elif action == 'replacement':,elif action == 'insertion':,0.9226375589708058,0.8832000938217648,False
3081,"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == 'tacotron': text = tacotron_cleaner.cleaners.custom_english_cleaners(text) elif t == 'jaconv': text = jaconv.normalize(text) <IF_STMT> if vietnamese_cleaners is None: raise RuntimeError('Please install underthesea') text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f'Not supported: type={t}') return text",elif t == 'vietnamese':,elif t == 'vietnamese':,0.6918627699623204,0.8752376177722327,True
3082,"def hook_GetVariable(ql, address, params): if params['VariableName'] in ql.env: var = ql.env[params['VariableName']] read_len = read_int64(ql, params['DataSize']) <IF_STMT> write_int64(ql, params['Attributes'], 0) write_int64(ql, params['DataSize'], len(var)) if read_len < len(var): return EFI_BUFFER_TOO_SMALL if params['Data'] != 0: ql.mem.write(params['Data'], var) return EFI_SUCCESS return EFI_NOT_FOUND",if params['Attributes'] != 0:,if read_len == 0:,0.8509772056849081,0.8592377270804451,False
3083,"def test_setupapp(self, overrideRootMenu): """"""Call setupApp with each possible graphics type."""""" root = self.root flist = FileList(root) for tktype in alltypes: with self.subTest(tktype=tktype): macosx._tk_type = tktype macosx.setupApp(root, flist) <IF_STMT> self.assertTrue(overrideRootMenu.called) overrideRootMenu.reset_mock()","if tktype in ('carbon', 'cocoa'):",if overrideRootMenu:,0.7651475257091733,0.8881135755489994,False
3084,"def names(self, persistent=None): u = set() result = [] for s in [self.__storage(None), self.__storage(self.__category)]: for b in s: if persistent is not None and b.persistent != persistent: continue <IF_STMT> continue if b.name not in u: result.append(b.name) u.add(b.name) return result",if b.name.startswith('__'):,if b.name in u:,0.9090278551667873,0.8627586293513119,False
3085,"def _check_extra_specs(key, value=None): extra_specs = diff.get('extra_specs') specific_type = extra_specs.get(key) if extra_specs else None old_type = None new_type = None if specific_type: old_type, new_type = specific_type <IF_STMT> old_type = True if old_type and old_type.upper() == value else False new_type = True if new_type and new_type.upper() == value else False return (old_type, new_type)",if value:,if value:,0.7734275394147253,0.9298663600557577,True
3086,"def _write_lock_file(self, repo, force=True): if force or (self._update and self._write_lock): updated_lock = self._locker.set_lock_data(self._package, repo.packages) <IF_STMT> self._io.write_line('') self._io.write_line('<info>Writing lock file</>')",if updated_lock:,if updated_lock:,0.7109027578505321,0.803154665668484,True
3087,"def process_message(self, msg): if msg['type'] == 'sample': batch_shape = msg['fn'].batch_shape <IF_STMT> batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape) batch_shape[self.dim] = self.size msg['fn'] = msg['fn'].expand(torch.Size(batch_shape))",if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:,if self.dim > 0:,0.6304406885163139,0.7965020533851944,False
3088,def _test_reducibility(self): graph = networkx.DiGraph(self._graph) self._make_supergraph(graph) while True: changed = False changed |= self._remove_self_loop(graph) changed |= self._merge_single_entry_node(graph) <IF_STMT> break,if not changed:,if changed:,0.8284864445742561,0.8492326635760689,False
3089,"def __init__(self, roberta, num_classes=2, dropout=0.0, prefix=None, params=None): super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params) self.roberta = roberta self._units = roberta._units with self.name_scope(): self.classifier = nn.HybridSequential(prefix=prefix) <IF_STMT> self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=self._units, activation='tanh')) if dropout: self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=num_classes))",if dropout:,if dropout:,0.8800828997932825,0.8743414417652072,True
3090,"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip('\\') for a, o in self.objects.items(): if not o.name: continue <IF_STMT> return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] if m: name = m[0] return self.get_object_from_name(name, False)",if o.name.lower() == name.lower():,if o.name == name:,0.7508698928302453,0.8806615362338783,False
3091,"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): <IF_STMT> method = getattr(self, name) if method and callable(method): method() finally: warnings.formatwarning = oldformatwarning",if name.startswith('check_'):,if name.startswith('check_'):,0.9109710844539451,0.8901732118131125,True
3092,"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) if not defaults and value == default_values[k]: continue <IF_STMT> print_func = repr pieces.append('%s=%s' % (k, print_func(value))) if pieces or self.__base__: return '%s(%s)' % (self.__class__.__name__, ', '.join(pieces)) else: return ''","if isinstance(value, basestring):",if print_func is str:,0.6638186484652046,0.9019629427251674,False
3093,"def apply(self, **kwargs: Any) -> None: for node in self.document.traverse(nodes.target): <IF_STMT> continue if 'ismod' in node and node.parent.__class__ is nodes.section and (node.parent.index(node) == 1): node.parent['ids'][0:0] = node['ids'] node.parent.remove(node)",if not node['ids']:,if node.type != 'module':,0.7444223645167862,0.8105932471967202,False
3094,"def add_special_token_2d(values: List[List[int]], special_token: int=0, use_first_value: bool=False) -> List[List[int]]: results = torch.jit.annotate(List[List[int]], []) for value in values: result = torch.jit.annotate(List[int], []) <IF_STMT> special_token = value[0] result.append(special_token) result.extend(value) result.append(special_token) results.append(result) return results",if use_first_value and len(value) > 0:,if use_first_value:,0.8422742719340457,0.8827916928185874,False
3095,"def test_import(self): TIMEOUT = 5 command = [sys.executable, '-c', 'import tornado.test.resolve_test_helper'] start = time.time() popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT)) while time.time() - start < TIMEOUT: return_code = popen.poll() <IF_STMT> self.assertEqual(0, return_code) return time.sleep(0.05) self.fail('import timed out')",if return_code is not None:,if return_code:,0.8672592247756261,0.8996480074924822,False
3096,"def find_item_for_key(self, e): for item in self._items: if item.keycode == e.key and item.shift == e.shift and (item.alt == e.alt): focus = get_focus() <IF_STMT> return self._items.index(item) else: return -1 return -1","if self.command_is_enabled(item, focus):",if focus == item:,0.6202190935479809,0.8228500218338367,False
3097,"def check_app_config_brackets(self): for sn, app in cherrypy.tree.apps.items(): if not isinstance(app, cherrypy.Application): continue <IF_STMT> continue for key in app.config.keys(): if key.startswith('[') or key.endswith(']'): warnings.warn('The application mounted at %r has config section names with extraneous brackets: %r. Config *files* need brackets; config *dicts* (e.g. passed to tree.mount) do not.' % (sn, key))",if not app.config:,"if not isinstance(app, cherrypy.Application):",0.8147416719516565,0.8923575006167597,False
3098,"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: for m in getattr(a, 'modules', []): m = m.strip() for mod in self.modules: <IF_STMT> if getattr(mod, 'module_name', '').strip() == m: return True return False","if getattr(mod, 'module_type', '').strip() == mod_type.strip():",if mod_type == mod_type:,0.8919919560390006,0.828399516355805,False
3099,"def write_config_to_file(self, folder, filename, config): do_not_write = ['hyperparameter_search_space_updates'] with open(os.path.join(folder, filename), 'w') as f: f.write('\n'.join([key + '=' + str(value) for key, value in sorted(config.items(), key=lambda x: x[0]) <IF_STMT>]))",if not key in do_not_write,if key not in do_not_write,0.8539888723737992,0.8358230801378418,False
3100,"def parsing(self, parsing): self._parsed = parsing for k, v in self._body: <IF_STMT> v.value.parsing(parsing) elif isinstance(v, AoT): for t in v.body: t.value.parsing(parsing)","if isinstance(v, Table):","if isinstance(v, AoT):",0.8329611068903033,0.7848518349390632,False
3101,"def test_crashers_crash(self): for fname in glob.glob(CRASHER_FILES): <IF_STMT> continue if test.support.verbose: print('Checking crasher:', fname) assert_python_failure(fname)",if os.path.basename(fname) in infinite_loops:,if not os.path.isfile(fname):,0.5504987567186355,0.6739047062564734,False
3102,"def __getitem__(self, k) -> 'SimMemView': if isinstance(k, slice): if k.step is not None: raise ValueError('Slices with strides are not supported') elif k.start is None: raise ValueError('Must specify start index') <IF_STMT> raise ValueError('Slices with stop index are not supported') else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",elif k.stop is not None:,elif k.stop is None:,0.9423234390917036,0.9069443196104878,False
3103,def get_lowest_wall_time(jsons): lowest_wall = None for j in jsons: <IF_STMT> lowest_wall = j['wall_time'] if lowest_wall > j['wall_time']: lowest_wall = j['wall_time'] return lowest_wall,if lowest_wall is None:,if lowest_wall is None:,0.8243353198550688,0.760856626273165,True
3104,"def extract_wav_headers(data): pos = 12 subchunks = [] while pos + 8 <= len(data) and len(subchunks) < 10: subchunk_id = data[pos:pos + 4] subchunk_size = struct.unpack_from('<I', data[pos + 4:pos + 8])[0] subchunks.append(WavSubChunk(subchunk_id, pos, subchunk_size)) <IF_STMT> break pos += subchunk_size + 8 return subchunks",if subchunk_id == b'data':,if subchunk_size == 0:,0.9045580426640959,0.8752376177722327,False
3105,"def _any_targets_have_native_sources(self, targets): for tgt in targets: for type_constraint, target_predicate in self._native_target_matchers.items(): <IF_STMT> return True return False",if type_constraint.satisfied_by(tgt) and target_predicate(tgt):,if target_predicate(tgt):,0.5911781668187512,0.7778111223054219,False
3106,"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: continue <IF_STMT> raise serializers.ValidationError('Process types can only contain [a-z]') if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value","if not re.match(PROCTYPE_MATCH, k):",if k.startswith('_'):,0.9379769136052173,0.9076141716697395,False
3107,"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord('0') if even: digit *= 2 <IF_STMT> digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",if digit >= 10:,if digit % 10 != 0:,0.9148875822141717,0.8665222382201849,False
3108,"def transform(a, cmds): buf = a.split('\n') for cmd in cmds: ctype, line, col, char = cmd if ctype == 'D': <IF_STMT> buf[line] = buf[line][:col] + buf[line][col + len(char):] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == 'I': buf[line] = buf[line][:col] + char + buf[line][col:] buf = '\n'.join(buf).split('\n') return '\n'.join(buf)",if char != '\n':,if char:,0.7186826641122177,0.9362597875749384,False
3109,"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} for edge in self.edges: if edge.is_dangling(): raise ValueError('Cannot contract copy tensor with dangling edges') if self._is_my_trace(edge): continue partner_node, shared_axis = self._get_partner(edge) <IF_STMT> partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",if partner_node not in partners:,if partner_node not in partners:,0.760742200156312,0.8294838585473985,True
3110,"def _bind_interactive_rez(self): if config.set_prompt and self.settings.prompt: stored_prompt = os.getenv('REZ_STORED_PROMPT_CMD') curr_prompt = stored_prompt or os.getenv('PROMPT', '') <IF_STMT> self.setenv('REZ_STORED_PROMPT_CMD', curr_prompt) new_prompt = '%%REZ_ENV_PROMPT%%' new_prompt = new_prompt + ' %s' if config.prefix_prompt else '%s ' + new_prompt new_prompt = new_prompt % curr_prompt self._addline('set PROMPT=%s' % new_prompt)",if not stored_prompt:,if curr_prompt:,0.7452845336670205,0.9164531641034833,False
3111,"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) if not definition: IECore.msg(IECore.Msg.Level.Error, 'GafferImageUI.CatalogueUI', ""No column registered with name '%s'"" % name) continue <IF_STMT> c = GafferUI.PathListingWidget.IconColumn(definition.title(), '', name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns","if isinstance(definition, IconColumn):",if definition.isIcon():,0.7182178634110625,0.9099951253570094,False
3112,"def _check_invalid_keys(self, section_name, section): for key in section: key_name = str(key) valid_key_names = [s[0] for s in self.keys] is_valid_key = key_name in valid_key_names <IF_STMT> err_msg = ""'{0}' is not a valid key name for '{1}'. Must be one of these: {2}"".format(key_name, section_name, ', '.join(valid_key_names)) raise InvalidConfig(err_msg)",if not is_valid_key:,if not is_valid_key:,0.8808740789812312,0.9022045190074797,True
3113,"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == '__pycache__': continue if name.endswith('.py'): names.add(name.split('.')[0]) <IF_STMT> names.add(name) if packages: packages = {package.lower().replace('-', '_') for package in packages} if len(names & packages) == len(packages): return packages return names",elif path.is_dir() and '.' not in name:,elif name.startswith('.py'):,0.9541597339792561,0.9220450449751959,False
3114,"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): <IF_STMT> keynames[i] = k[1:] negate.add(k[1:])  def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): if k in negate: composite[i] = -v return composite return getit",if k[:1] == '-':,if k.startswith('-'):,0.8294187599411388,0.9144061946646023,False
3115,"def iter_symbols(code): """"""Yield names and strings used by `code` and its nested code objects"""""" for name in code.co_names: yield name for const in code.co_consts: if isinstance(const, six.string_types): yield const <IF_STMT> for name in iter_symbols(const): yield name","elif isinstance(const, CodeType):","elif isinstance(const, (list, tuple)):",0.8507197969036947,0.8516228624291206,False
3116,"def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None: with self._lock: <IF_STMT> current_directions = self._studies[study_id].directions if directions == current_directions: return elif len(current_directions) == 1 and current_directions[0] == StudyDirection.NOT_SET: self._studies[study_id].directions = list(directions) self._backend.set_study_directions(study_id, directions) return self._backend.set_study_directions(study_id, directions)",if study_id in self._studies:,if study_id in self._studies:,0.8979288063021282,0.8474968231198384,True
3117,"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: <IF_STMT> x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",if self.__Token:,if ReplacedLine:,0.8925308927995657,0.8901732118131125,False
3118,"def _check_blocking(self, current_time): if self._switch_flag is False: active_greenlet = self._active_greenlet <IF_STMT> self._notify_greenlet_blocked(active_greenlet, current_time) self._switch_flag = False",if active_greenlet is not None and active_greenlet != self._hub:,if active_greenlet is not None:,0.49604999195196287,0.6383240325919926,False
3119,"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('BlockDos\\.net', headers.get(HTTP_HEADER.SERVER, ''), re.I) is not None <IF_STMT> break return retval",if retval:,if retval:,0.8041812855314776,0.8645707301556367,True
3120,"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: <IF_STMT> in_section = True elif in_section: if line.startswith('>>END'): break out.append(line.rstrip('\r\n')) return out",if line.startswith('>>%s' % section_name):,if line.startswith(section_name):,0.910742901870759,0.8966773400768917,False
3121,"def shortcut(self, input, ch_out, stride, is_first, name): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <IF_STMT> return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) elif is_first: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return input",if is_first or stride == 1:,if is_first:,0.5827660507670319,0.9164531641034833,False
3122,"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: if string_value is not None: string_value = str(string_value).strip() <IF_STMT> param_value = int(string_value) except ValueError: self.pcluster_config.warn(""Unable to convert the value '{0}' to an Integer. Using default value for parameter '{1}'"".format(string_value, self.key)) return param_value",if string_value != 'NONE':,if string_value:,0.7396880165525787,0.9220450449751959,False
3123,"def get_running(workers): running = [] for worker in workers: current_test_name = worker.current_test_name <IF_STMT> continue dt = time.monotonic() - worker.start_time if dt >= PROGRESS_MIN_TIME: text = '%s (%s)' % (current_test_name, format_duration(dt)) running.append(text) return running",if not current_test_name:,if current_test_name == 'test':,0.6802565624123467,0.8385130047130208,False
3124,"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: if stats['uptime'] > 86400: uptime['value'] = stats['uptime'] / 60 / 60 / 24 uptime['unit'] = _('days') <IF_STMT> uptime['value'] = stats['uptime'] / 60 / 60 uptime['unit'] = _('hours') else: uptime['value'] = stats['uptime'] / 60 uptime['unit'] = _('minutes') return {'cache_stats': cache_stats, 'uptime': uptime}",elif stats['uptime'] > 3600:,elif stats['uptime'] < 3600:,0.85872925949189,0.9128479730518225,False
3125,"def add_actors(self): """"""Adds `self.actors` to the scene."""""" if not self._actors_added: self.reader.render_window = self.scene.render_window self._update_reader() self._actors_added = True <IF_STMT> self._visible_changed(self.visible) self.scene.render()",if not self.visible:,if self.visible:,0.6678648706954065,0.8289657839357887,False
3126,"def _add_uniqu_suffix(self, titles): counters = dict() titles_with_suffix = [] for title in titles: counters[title] = counters[title] + 1 if title in counters else 1 <IF_STMT> title = f'{title} ({counters[title]})' titles_with_suffix.append(title) return titles_with_suffix",if counters[title] > 1:,if title in counters:,0.7372000964368876,0.8336104423443033,False
3127,"def _verify_udf_resources(self, job, config): udf_resources = config.get('userDefinedFunctionResources', ()) self.assertEqual(len(job.udf_resources), len(udf_resources)) for found, expected in zip(job.udf_resources, udf_resources): <IF_STMT> self.assertEqual(found.udf_type, 'resourceUri') self.assertEqual(found.value, expected['resourceUri']) else: self.assertEqual(found.udf_type, 'inlineCode') self.assertEqual(found.value, expected['inlineCode'])",if 'resourceUri' in expected:,"if isinstance(found, dict):",0.8934118896090262,0.8266114125804572,False
3128,"def __init__(self, layout, value=None, string=None, *, dtype: np.dtype=np.float64) -> None: """"""Constructor."""""" self.layout = layout if value is None: <IF_STMT> self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) if self.value.shape != (self.layout.gaDims,): raise ValueError('value must be a sequence of length %s' % self.layout.gaDims)",if string is None:,if string is None:,0.9280441904217642,0.8832000938217648,True
3129,"def read_file(filename, print_error=True): """"""Returns the contents of a file."""""" try: for encoding in ['utf-8', 'latin1']: try: with io.open(filename, encoding=encoding) as fp: return fp.read() except UnicodeDecodeError: pass except IOError as exception: <IF_STMT> print(exception, file=sys.stderr) return None",if print_error:,if print_error:,0.9179696938801598,0.8966773400768917,True
3130,"def get_albums_for_iter(self, iter_): obj = self.get_value(iter_) if isinstance(obj, AlbumNode): return {obj.album} albums = set() for child_iter, value in self.iterrows(iter_): <IF_STMT> albums.add(value.album) else: albums.update(self.get_albums_for_iter(child_iter)) return albums","if isinstance(value, AlbumNode):","if isinstance(value, AlbumNode):",0.76917804193239,0.8196189957582152,True
3131,"def wait_til_ready(cls, connector=None): if connector is None: connector = cls.connector while True: now = time.time() next_iteration = now // 1.0 + 1 <IF_STMT> break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0)",if connector.ready:,if next_iteration > cls._clock.get_til_ready_interval():,0.9317659284233158,0.8169276475307028,False
3132,"def remove_property(self, key): with self.secure() as config: keys = key.split('.') current_config = config for i, key in enumerate(keys): if key not in current_config: return <IF_STMT> del current_config[key] break current_config = current_config[key]",if i == len(keys) - 1:,if current_config[key] == self.property:,0.862480778934527,0.828399516355805,False
3133,"def get(self, hash160, default=None): v = self.p2s_for_hash(hash160) <IF_STMT> return v if hash160 not in self._secret_exponent_cache: v = self.path_for_hash160(hash160) if v: fingerprint, path = v for key in self._secrets.get(fingerprint, []): subkey = key.subkey_for_path(path) self._add_key_to_cache(subkey) return self._secret_exponent_cache.get(hash160, default)",if v:,if v:,0.9204074203991189,0.8996480074924822,True
3134,"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: <IF_STMT> targets = tuple(targets) elif type(targets) != tuple: targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",if type(targets) != list and type(targets) != tuple:,if type(targets) == list:,0.8856885229769089,0.828399516355805,False
3135,"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: <IF_STMT> if isinstance(dt, dgl.DGLGraph): return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)",if dt is not None:,if dt is not None:,0.8283686845191094,0.8842429471030616,True
3136,"def capture_server(evt, buf, serv): try: serv.listen(5) conn, addr = serv.accept() except socket.timeout: pass else: n = 200 while n > 0: r, w, e = select.select([conn], [], []) <IF_STMT> data = conn.recv(10) buf.write(data.replace('\n', '')) if '\n' in data: break n -= 1 time.sleep(0.01) conn.close() finally: serv.close() evt.set()",if r:,if r:,0.9406807485721652,0.9237460349978159,True
3137,"def elem(): if ints_only: return random.randint(0, 10000000000) else: t = random.randint(0, 2) if t == 0: return random.randint(0, 10000000000) elif t == 1: return float(random.randint(0, 10000000000)) <IF_STMT> return strings[random.randint(0, len(strings) - 1)] return random_string(random.randint(100, 1000))",elif strings is not None:,elif t == 2:,0.9151893934636455,0.8474968231198384,False
3138,"def has_changed(self, initial, data): if self.disabled: return False if initial is None: initial = ['' for x in range(0, len(data))] el<IF_STMT> initial = self.widget.decompress(initial) for field, initial, data in zip(self.fields, initial, data): try: initial = field.to_python(initial) except ValidationError: return True if field.has_changed(initial, data): return True return False","if not isinstance(initial, list):",if self.widget:,0.7465700164458747,0.9237460349978159,False
3139,"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) <IF_STMT> if hasattr(package.__loader__, 'get_data'): file_contents = package.__loader__.get_data(filename) return (file_contents.replace(os.linesep, '\n'), filename) return (open(filename).read(), filename)","if hasattr(package, '__loader__'):","if hasattr(package, '__loader__'):",0.7406961693299207,0.8390782502060267,True
3140,def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None <IF_STMT> self.waiters -= 1 self.wakeup.release(),if self.waiters:,if self.waiters > 0:,0.780846263607631,0.8474968231198384,False
3141,"def stage(self, x, num_modules, num_blocks, channels, multi_scale_output=True, name=None): out = x for i in range(num_modules): <IF_STMT> out = self.high_resolution_module(out, num_blocks, channels, multi_scale_output=False, name=name + '_' + str(i + 1)) else: out = self.high_resolution_module(out, num_blocks, channels, name=name + '_' + str(i + 1)) return out",if i == num_modules - 1 and multi_scale_output == False:,if multi_scale_output:,0.9071146127494996,0.9184043388013005,False
3142,"def changeFrontAlteration(intV, alter): fati = self.frontAlterationTransposeInterval if fati: newFati = interval.add([fati, intV]) self.frontAlterationTransposeInterval = newFati self.frontAlterationAccidental.alter = self.frontAlterationAccidental.alter + alter <IF_STMT> self.frontAlterationTransposeInterval = None self.frontAlterationAccidental = None else: self.frontAlterationTransposeInterval = intV self.frontAlterationAccidental = pitch.Accidental(alter)",if self.frontAlterationAccidental.alter == 0:,if intV == 0:,0.6718977408505087,0.8431339019329497,False
3143,"def set_to_train(self): for T in self.trainable_attributes(): for k, v in T.items(): <IF_STMT> c_f.set_requires_grad(v, requires_grad=False) v.eval() else: v.train() self.maybe_freeze_trunk_batchnorm()",if k in self.freeze_these:,if k == 'train':,0.6032054626675926,0.7098232254187811,False
3144,"def _migrate(self, sig=None, compact=True): with self.lock: sig = sig or self.sig <IF_STMT> return if sig in self.WORDS and len(self.WORDS[sig]) > 0: PostingList.Append(self.session, sig, self.WORDS[sig], sig=sig, compact=compact) del self.WORDS[sig]",if sig in GPL_NEVER_MIGRATE:,if sig is None:,0.8971247974486694,0.8105932471967202,False
3145,"def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs): if self.prediction_bar is None: <IF_STMT> self.prediction_bar = self.training_tracker.add_child(len(eval_dataloader)) else: self.prediction_bar = NotebookProgressBar(len(eval_dataloader)) self.prediction_bar.update(1) else: self.prediction_bar.update(self.prediction_bar.value + 1)",if self.training_tracker is not None:,if eval_dataloader is not None:,0.8453159645548961,0.7506346798217074,False
3146,"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print('struct {}'.format(self.name)) for field in self.fields: if field.offset is None: offset = '0x??' else: offset = '0x{:02x}'.format(field.offset) print('{}+{} {} {}'.format(' ' * indent, offset, field.name, field.type)) <IF_STMT> field.type.show(indent + 1)","if isinstance(field.type, Structure):",if field.type:,0.9453987625357329,0.9122561819614461,False
3147,"def __exit__(self, exc, value, tb): for key in self.overrides.keys(): old_value = self.old[key] <IF_STMT> delattr(self.instance, key) else: setattr(self.instance, key, old_value) self.instance.save()",if old_value is NULL:,if old_value is None:,0.7169988153070945,0.7378351342269067,False
3148,"def complete(self, block): with self._condition: <IF_STMT> return False if self._complete(): self._calculate_state_root_if_not_already_done() return True if block: self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",if not self._final:,if self._state_root_if_not_already_done:,0.8307373799335653,0.8232490471721702,False
3149,def parseArguments(self): args = [] self.expect('(') if not self.match(')'): while self.startIndex < self.length: args.append(self.isolateCoverGrammar(self.parseAssignmentExpression)) <IF_STMT> break self.expectCommaSeparator() self.expect(')') return args,if self.match(')'):,if self.startIndex == self.length - 1:,0.8097190142380353,0.6748913185157768,False
3150,"def isValidDateString(config_param_name, value, valid_value): try: if value == 'DD-MM-YYYY': return value day, month, year = value.split('-') <IF_STMT> raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if int(day) < 1 or int(day) > 31:,if int(day) < 1 or int(day) > 12:,0.9118011845536211,0.824840871632579,False
3151,"def build_tree(path): tree = Tree() for basename, entry in trees[path].items(): <IF_STMT> mode = stat.S_IFDIR sha = build_tree(pathjoin(path, basename)) else: mode, sha = entry tree.add(basename, mode, sha) object_store.add_object(tree) return tree.id","if isinstance(entry, dict):",if entry is None:,0.8618320891660335,0.8169276475307028,False
3152,"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {'objects': 0, 'containers': 0, 'accounts': 0} qdir = 'quarantined' for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) <IF_STMT> linkcount = os.lstat(qtgt).st_nlink if linkcount > 2: qcounts[qtype] += linkcount - 2 return qcounts",if os.path.exists(qtgt):,if os.path.exists(qtgt):,0.9050330206760253,0.9220450449751959,True
3153,"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: <IF_STMT> return False if dim_value < 0: raise Exception('Negative dimension is illegal: %d' % dim_value) return True","if not isinstance(dim_value, int):",if dim_value > self.max_dim:,0.834561325963776,0.8431339019329497,False
3154,"def BraceDetectAll(words): """"""Return a new list of words, possibly with BracedTree instances."""""" out = [] for w in words: if len(w.parts) >= 3: brace_tree = _BraceDetect(w) <IF_STMT> out.append(brace_tree) continue out.append(w) return out",if brace_tree:,if brace_tree:,0.7368086628535466,0.8866029039778043,True
3155,"def __init__(original, self, *args, **kwargs): data = args[0] if len(args) > 0 else kwargs.get('data') if data is not None: try: <IF_STMT> raise Exception('cannot gather example input when dataset is loaded from a file.') input_example_info = _InputExampleInfo(input_example=deepcopy(data[:INPUT_EXAMPLE_SAMPLE_ROWS])) except Exception as e: input_example_info = _InputExampleInfo(error_msg=str(e)) setattr(self, 'input_example_info', input_example_info) original(self, *args, **kwargs)","if isinstance(data, str):",if not os.path.exists(data[INPUT_EXAMPLE_SAMPLE_ROWS]:,0.9100787863507004,0.9081987180086649,False
3156,"def setRow(self, row, vals): if row > self.rowCount() - 1: self.setRowCount(row + 1) for col in range(len(vals)): val = vals[col] item = self.itemClass(val, row) item.setEditable(self.editable) sortMode = self.sortModes.get(col, None) <IF_STMT> item.setSortMode(sortMode) format = self._formats.get(col, self._formats[None]) item.setFormat(format) self.items.append(item) self.setItem(row, col, item) item.setValue(val)",if sortMode is not None:,if sortMode:,0.8461209873735476,0.9122561819614461,False
3157,"def wakeUp(self): """"""Write one byte to the pipe, and flush it."""""" if self.o is not None: try: util.untilConcludes(os.write, self.o, b'x') except OSError as e: <IF_STMT> raise",if e.errno != errno.EAGAIN:,if e.errno != errno.EAGAIN:,0.6183013855227653,0.826236106507527,True
3158,"def _setup(self, field_name, owner_model): resolved_classes = [] for m in self.model_classes: <IF_STMT> if m == owner_model.__name__: resolved_classes.append(owner_model) else: raise Exception(""PolyModelType: Unable to resolve model '{}'."".format(m)) else: resolved_classes.append(m) self.model_classes = tuple(resolved_classes) super(PolyModelType, self)._setup(field_name, owner_model)","if isinstance(m, string_type):","if isinstance(m, str):",0.9091465693420908,0.8635707684233572,False
3159,"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: value_ = self._late_binding_returnvalues.get(key, KeyError) if value_ is KeyError: value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) <IF_STMT> value_ = schema.validate(value_) self._late_binding_returnvalues[key] = value_ return value_ else: return value",if schema is not None:,if schema is not None:,0.8423254245197724,0.8248765135255685,True
3160,"def convert(self, ctx, argument): arg = argument.replace('0x', '').lower() if arg[0] == '#': arg = arg[1:] try: value = int(arg, base=16) <IF_STMT> raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace(' ', '_') method = getattr(discord.Colour, arg, None) if arg.startswith('from_') or method is None or (not inspect.ismethod(method)): raise BadColourArgument(arg) return method()",if not 0 <= value <= 16777215:,if value < 0:,0.936381456897893,0.8923575006167597,False
3161,"def get_versions(*, all=False, quiet=None): import bonobo from bonobo.util.pkgs import bonobo_packages yield _format_version(bonobo, quiet=quiet) if all: for name in sorted(bonobo_packages): <IF_STMT> try: mod = __import__(name.replace('-', '_')) try: yield _format_version(mod, name=name, quiet=quiet) except Exception as exc: yield '{} ({})'.format(name, exc) except ImportError as exc: yield '{} is not importable ({}).'.format(name, exc)",if name != 'bonobo':,if name.startswith('_version_'):,0.9434204345458956,0.926934323706186,False
3162,"def assertOperationsInjected(self, plan, **kwargs): for migration, _backward in plan: operations = iter(migration.operations) for operation in operations: <IF_STMT> next_operation = next(operations) self.assertIsInstance(next_operation, contenttypes_management.RenameContentType) self.assertEqual(next_operation.app_label, migration.app_label) self.assertEqual(next_operation.old_model, operation.old_name_lower) self.assertEqual(next_operation.new_model, operation.new_name_lower)","if isinstance(operation, migrations.RenameModel):",if operation.name == 'rename':,0.8871978500196744,0.8105932471967202,False
3163,"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() if line == '': continue match = COMMENT.match(line) if match: continue <IF_STMT> if ',' in line or ';' in line: continue yield line",if strip_delimiters:,if strip_delimiters:,0.9070698823027359,0.8866029039778043,True
3164,"def read_lccn(line, is_marc8=False): found = [] for k, v in get_raw_subfields(line, ['a']): lccn = v.strip() if re_question.match(lccn): continue m = re_lccn.search(lccn) if not m: continue lccn = re_letters_and_bad.sub('', m.group(1)).strip() <IF_STMT> found.append(lccn) return found",if lccn:,if is_marc8 and lccn not in found:,0.8421396325534243,0.7713696476529821,False
3165,"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == 'likelihood.noise_covar.raw_noise': self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == 'mean_module.constant': self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) <IF_STMT> self.assertIsInstance(constraint, gpytorch.constraints.Positive)",elif name == 'covar_module.base_kernel.raw_lengthscale':,elif name == 'covar_module.raw_outputscale_constant':,0.9137019776277687,0.8588713701164342,False
3166,"def _cleanupSocket(self): """"""Close the Connection's socket."""""" try: self._sock.shutdown(socket.SHUT_WR) except: return try: while True: r, w, e = select.select([self._sock], [], []) <IF_STMT> break except: pass self._sock.close()",if not r or not self._sock.recv(1024):,if r:,0.645991645321968,0.8531413606256201,False
3167,"def fadeIn(self, acts=None, t=None, duration=None): """"""Gradually switch on the input list of meshes by increasing opacity."""""" if self.bookingMode: acts, t, duration, rng = self._parse(acts, t, duration) for tt in rng: alpha = linInterpolate(tt, [t, t + duration], [0, 1]) self.events.append((tt, self.fadeIn, acts, alpha)) else: for a in self._performers: <IF_STMT> continue a.alpha(self._inputvalues) return self",if a.alpha() >= self._inputvalues:,if a.isShown():,0.9444584024669467,0.9325718821645923,False
3168,"def get_config_updates_recursive(self): config_updates = self.config_updates.copy() for sr_path, subrunner in self.subrunners.items(): <IF_STMT> continue update = subrunner.get_config_updates_recursive() if update: config_updates[rel_path(self.path, sr_path)] = update return config_updates","if not is_prefix(self.path, sr_path):",if not subrunner:,0.840643677101413,0.80377750806414,False
3169,"def setArgs(self, **kwargs): """"""See GridSearchCostGamma"""""" for key, value in list(kwargs.items()): if key in ('folds', 'nfolds'): self._n_folds = int(value) <IF_STMT> self._validator_kwargs['max_epochs'] = value else: GridSearchDOE.setArgs(self, **{key: value})",elif key in 'max_epochs':,if key == 'max_epochs':,0.7698281662712887,0.7965020533851944,False
3170,def _parse_composite_axis(composite_axis_name: str): axes_names = [axis for axis in composite_axis_name.split(' ') if len(axis) > 0] for axis in axes_names: <IF_STMT> continue assert 'a' <= axis[0] <= 'z' for letter in axis: assert str.isdigit(letter) or 'a' <= letter <= 'z' return axes_names,if axis == '_':,if len(axis) == 1:,0.9302402051272969,0.8692960007731574,False
3171,"def visit_For(self, node, for_branch='body', **kwargs): if for_branch == 'body': self.sym_visitor.visit(node.target, store_as_param=True) branch = node.body elif for_branch == 'else': branch = node.else_ elif for_branch == 'test': self.sym_visitor.visit(node.target, store_as_param=True) <IF_STMT> self.sym_visitor.visit(node.test) return else: raise RuntimeError('Unknown for branch') for item in branch or (): self.sym_visitor.visit(item)",if node.test is not None:,elif for_branch == 'test_elif':,0.6938471810770295,0.8723360571509826,False
3172,def contains_only_whitespace(node): if is_tag(node): <IF_STMT> if not any([unicode(s).strip() for s in node.contents]): return True return False,if not any([not is_text(s) for s in node.contents]):,if node.type == 'text':,0.5323670402359459,0.674945488826271,False
3173,"def dir_tag_click(event): mouse_index = self.path_bar.index('@%d,%d' % (event.x, event.y)) lineno = int(float(mouse_index)) if lineno == 1: self.request_focus_into('') else: assert lineno == 2 dir_range = get_dir_range(event) if dir_range: _, end_index = dir_range path = self.path_bar.get('2.0', end_index) <IF_STMT> path += '\\' self.request_focus_into(path)",if path.endswith(':'):,if path:,0.7385563322169642,0.9076141716697395,False
3174,"def validate_employee_id(self): if self.employee: sales_person = frappe.db.get_value('Sales Person', {'employee': self.employee}) <IF_STMT> frappe.throw(_('Another Sales Person {0} exists with the same Employee id').format(sales_person))",if sales_person and sales_person != self.name:,if sales_person:,0.6148566957273212,0.8232490471721702,False
3175,def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/infer'): if 'stage' not in item.keywords: item.add_marker(pytest.mark.stage('unit')) <IF_STMT> item.add_marker(pytest.mark.init(rng_seed=123)),if 'init' not in item.keywords:,if 'init' not in item.keywords:,0.6137372760254911,0.6859238121837058,True
3176,"def poll(self, timeout): if timeout < 0: timeout = None events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout) results = defaultdict(lambda: POLL_NULL) for e in events: fd = e.ident if e.filter == select.KQ_FILTER_READ: results[fd] |= POLL_IN <IF_STMT> results[fd] |= POLL_OUT return results.items()",elif e.filter == select.KQ_FILTER_WRITE:,elif e.filter == select.KQ_FILTER_WRITE:,0.9076204879708725,0.8627586293513119,True
3177,"def _read_dimensions(self, *dimnames, **kwargs): path = kwargs.get('path', '/') try: <IF_STMT> return [self.rootgrp.dimensions[dname] for dname in dimnames] group = self.path2group[path] return [group.dimensions[dname] for dname in dimnames] except KeyError: raise self.Error('In file %s:\nError while reading dimensions: `%s` with kwargs: `%s`' % (self.path, dimnames, kwargs))",if path == '/':,if path == '/':,0.7843942250311386,0.8723360571509826,True
3178,def spam_to_me(address): sock = eventlet.connect(address) while True: try: sock.sendall(b'hello world') time.sleep(0.001) except socket.error as e: <IF_STMT> return raise,if get_errno(e) == errno.EPIPE:,if e.errno == errno.ECONNRESET:,0.8309448146495947,0.7254330631437157,False
3179,"def has_hash_of(self, destpath, code, package_level): """"""Determine if a file has the hash of the code."""""" if destpath is not None and os.path.isfile(destpath): with univ_open(destpath, 'r') as opened: compiled = readfile(opened) hashash = gethash(compiled) <IF_STMT> return True return False","if hashash is not None and hashash == self.comp.genhash(code, package_level):",if hashash == code:,0.8721057531045944,0.8592377270804451,False
3180,"def insert(self, index, item): if len(self.lists) == 1: self.lists[0].insert(index, item) self._balance_list(0) else: list_idx, rel_idx = self._translate_index(index) <IF_STMT> raise IndexError() self.lists[list_idx].insert(rel_idx, item) self._balance_list(list_idx) return",if list_idx is None:,if list_idx == -1:,0.7689831830753048,0.7709002428237395,False
3181,"def _parse_class_simplified(symbol): results = {} name = symbol.name + '(' name += ', '.join([analyzer.expand_attribute(base) for base in symbol.bases]) name += ')' for sym in symbol.body: if isinstance(sym, ast.FunctionDef): result = _parse_function_simplified(sym, symbol.name) results.update(result) <IF_STMT> result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, 'c') return results","elif isinstance(sym, ast.ClassDef):","elif isinstance(sym, ast.ClassDef):",0.8053727073812982,0.9167056528641923,True
3182,"def append_vars(pairs, result): for name, value in sorted(pairs.items()): if isinstance(value, list): value = '[%s]' % ','.join(value) <IF_STMT> result.append('%s:%s=%s' % (package, name, value)) else: result.append('%s=%s' % (name, value))",if package:,if package:,0.8415770097204058,0.8645707301556367,True
3183,"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: <IF_STMT> self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",if len(self._editableChildren):,if len(self._editableChildren) == 1:,0.6585011599671651,0.8902056737869248,False
3184,"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != 'headers': if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and (not everythingIsUnicode(i)): return False elif isinstance(i, _bytes): return False <IF_STMT> return False return True","elif isinstance(v, _bytes):","elif isinstance(v, unicode):",0.9341483916801394,0.9167056528641923,False
3185,"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False <IF_STMT> return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True","elif isinstance(s, np.ndarray) and s.size == 0:","elif isinstance(s, (list, tuple)):",0.8814144792881919,0.8474968231198384,False
3186,"def scan_resource_conf(self, conf): if 'properties' in conf: if 'attributes' in conf['properties']: if 'exp' in conf['properties']['attributes']: <IF_STMT> return CheckResult.PASSED return CheckResult.FAILED",if conf['properties']['attributes']['exp']:,if 'exp' in conf['properties']['attributes']['exp']:,0.6090568983097522,0.7378351342269067,False
3187,"def encode(self): if self.expr in gpregs.expr: self.value = gpregs.expr.index(self.expr) self.parent.rot2.value = 0 elif isinstance(self.expr, ExprOp) and self.expr.op == allshifts[3]: reg, value = self.expr.args <IF_STMT> return False self.value = gpregs.expr.index(reg) if not isinstance(value, ExprInt): return False value = int(value) if not value in [8, 16, 24]: return False self.parent.rot2.value = value // 8 return True",if reg not in gpregs.expr:,if reg not in gpregs.expr:,0.9323375348867251,0.8842429471030616,True
3188,"def validate_transaction_reference(self): bank_account = self.paid_to if self.payment_type == 'Receive' else self.paid_from bank_account_type = frappe.db.get_value('Account', bank_account, 'account_type') if bank_account_type == 'Bank': <IF_STMT> frappe.throw(_('Reference No and Reference Date is mandatory for Bank transaction'))",if not self.reference_no or not self.reference_date:,"if not frappe.db.get_value('Reference', None, 'reference_date'):",0.8987477321079832,0.8036431532733102,False
3189,"def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: if hasattr(monad, 'cls_bl_idname'): <IF_STMT> return monad return None",if monad.cls_bl_idname == self.cls_bl_idname:,if monad.cls_bl_idname == self.cls_bl_idname:,0.8296866014841131,0.7245511487202049,True
3190,"def _create_mask(self, plen): mask = [] for i in range(16): if plen >= 8: mask.append(255) <IF_STMT> mask.append(255 >> 8 - plen << 8 - plen) else: mask.append(0) plen -= 8 return mask",elif plen > 0:,elif plen < 255:,0.8991481087395281,0.8336104423443033,False
3191,"def dataset_to_stream(dataset, input_name): """"""Takes a tf.Dataset and creates a numpy stream of ready batches."""""" for example in fastmath.dataset_as_numpy(dataset): features = example[0] inp, out = (features[input_name], example[1]) mask = features['mask'] if 'mask' in features else None <IF_STMT> inp = inp.astype(np.int32) if isinstance(out, np.uint8): out = out.astype(np.int32) yield ((inp, out) if mask is None else (inp, out, mask))","if isinstance(inp, np.uint8):","if isinstance(inp, np.uint8):",0.839965344821349,0.9196822664155297,True
3192,"def _idle_redraw_cb(self): assert self._idle_redraw_src_id is not None queue = self._idle_redraw_queue if len(queue) > 0: bbox = queue.pop(0) <IF_STMT> super(CanvasRenderer, self).queue_draw() else: super(CanvasRenderer, self).queue_draw_area(*bbox) if len(queue) == 0: self._idle_redraw_src_id = None return False return True",if bbox is None:,if bbox is None:,0.6807798896890188,0.8431339019329497,True
3193,"def mutated(self, indiv): """"""mutate some genes of the given individual"""""" res = indiv.copy() for i in range(self.numParameters): <IF_STMT> if self.xBound is None: res[i] = indiv[i] + gauss(0, self.mutationStdDev) else: res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev), self.maxs[i]), self.mins[i]) return res",if random() < self.mutationProb:,if self.xBound is not None:,0.8368869324174327,0.8419539711731491,False
3194,"def _justifyDrawParaLine(tx, offset, extraspace, words, last=0): setXPos(tx, offset) text = b' '.join(words) if last: tx._textOut(text, 1) else: nSpaces = len(words) - 1 <IF_STMT> tx.setWordSpace(extraspace / float(nSpaces)) tx._textOut(text, 1) tx.setWordSpace(0) else: tx._textOut(text, 1) setXPos(tx, -offset) return offset",if nSpaces:,if nSpaces > extraspace:,0.6327188099867624,0.8516228624291206,False
3195,"def _read_0(self, stream): r = b'' while True: c = stream.read(2) <IF_STMT> raise EOFError() if c == b'\x00\x00': break r += c return r.decode(self.encoding)",if len(c) != 2:,if not c:,0.8852971295548718,0.8120341702859789,False
3196,"def run(self, app, editor, args): line_nums = [] for cursor in editor.cursors: <IF_STMT> line_nums.append(cursor.y) data = editor.lines[cursor.y].get_data().upper() editor.lines[cursor.y].set_data(data)",if cursor.y not in line_nums:,if cursor.y not in line_nums:,0.8072647635555154,0.6750915335148621,True
3197,"def create_default_energy_point_rules(): for rule in get_default_energy_point_rules(): rule_exists = frappe.db.exists('Energy Point Rule', {'reference_doctype': rule.get('reference_doctype')}) <IF_STMT> continue doc = frappe.get_doc(rule) doc.insert(ignore_permissions=True)",if rule_exists:,if not rule_exists:,0.5929339256939888,0.761827408333416,False
3198,"def __new__(cls, *nodes): if not nodes: raise TypeError('DisjunctionNode() requires at least one node') elif len(nodes) == 1: return nodes[0] self = super(DisjunctionNode, cls).__new__(cls) self.__nodes = [] for node in nodes: if not isinstance(node, Node): raise TypeError('DisjunctionNode() expects Node instances as arguments; received a non-Node instance %r' % node) <IF_STMT> self.__nodes.extend(node.__nodes) else: self.__nodes.append(node) return self","if isinstance(node, DisjunctionNode):","if isinstance(node, list):",0.9297570810332366,0.9167056528641923,False
3199,def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: if w not in index: yield from dfs(w) <IF_STMT> while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v]:]) del stack[index[v]:] identified.update(scc) yield scc,elif w not in identified:,if boundaries[-1] == index[v]:,0.9229651646206259,0.8692960007731574,False
3200,"def unpack_item_obj(map_uuid_global_id, misp_obj): obj_meta = get_object_metadata(misp_obj) obj_id = None io_content = None for attribute in misp_obj.attributes: <IF_STMT> obj_id = attribute.value io_content = attribute.data if obj_id and io_content: res = Item.create_item(obj_id, obj_meta, io_content) map_uuid_global_id[misp_obj.uuid] = get_global_id('item', obj_id)",if attribute.object_relation == 'raw-data':,if attribute.uuid == 'item':,0.8344192875841917,0.8516228624291206,False
3201,"def parse(self, response): soup = BeautifulSoup(response.content.decode('utf-8', 'ignore'), 'lxml') image_divs = soup.find_all('div', class_='imgpt') pattern = re.compile('murl\\"":\\""(.*?)\\.jpg') for div in image_divs: href_str = html_parser.HTMLParser().unescape(div.a['m']) match = pattern.search(href_str) <IF_STMT> name = match.group(1) if six.PY3 else match.group(1).encode('utf-8') img_url = '{}.jpg'.format(name) yield dict(file_url=img_url)",if match:,if match:,0.8403620282223649,0.9051034981560222,True
3202,"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) <IF_STMT> _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",if fn is not None:,if fn:,0.9478357802643289,0.9202663016973823,False
3203,"def decompileFormat1(self, reader, otFont): self.classDefs = classDefs = [] startGlyphID = reader.readUShort() glyphCount = reader.readUShort() for i in range(glyphCount): glyphName = otFont.getglyphName(startGlyphID + i) classValue = reader.readUShort() <IF_STMT> classDefs.append((glyphName, classValue))",if classValue:,if classValue != 0:,0.6339598108239993,0.8316518087941058,False
3204,"def compress(self, data_list): if len(data_list) == 2: value, lookup_expr = data_list <IF_STMT> if lookup_expr not in EMPTY_VALUES: return Lookup(value=value, lookup_expr=lookup_expr) else: raise forms.ValidationError(self.error_messages['lookup_required'], code='lookup_required') return None",if value not in EMPTY_VALUES:,if value not in EMPTY_VALUES:,0.5814446641046274,0.7685107079449489,True
3205,"def open_compat(path, mode='r'): if mode in ['r', 'rb'] and (not os.path.exists(path)): raise FileNotFoundError(u'The file ""%s"" could not be found' % path) if sys.version_info >= (3,): encoding = 'utf-8' errors = 'replace' <IF_STMT> encoding = None errors = None return open(path, mode, encoding=encoding, errors=errors) else: return open(path, mode)","if mode in ['rb', 'wb', 'ab']:",elif mode == 'rb':,0.9044695942963286,0.8856327184319047,False
3206,"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) <IF_STMT> continue real_errors.append(line) return real_errors",if _path[-1] != current_path[-1]:,if _path[0] != current_path:,0.771902092333535,0.8832000938217648,False
3207,"def filter_by_level(record, level_per_module): name = record['name'] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = '' if '' in level_per_module: level = level_per_module[''] for n in name.split('.'): lookup += n <IF_STMT> level = level_per_module[lookup] lookup += '.' if level is False: return False return record['level'].no >= level",if lookup in level_per_module:,if lookup in level_per_module:,0.9471588823104112,0.9019629427251674,True
3208,"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 <IF_STMT> n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",if self.HasMaximizeButton():,if self.HasOpenButton():,0.9433250167472353,0.9202663016973823,False
3209,"def search(a, b, desired): if a == b: return a if abs(b - a) < 0.005: ca = count(a) cb = count(b) dista = abs(desired - ca) distb = abs(desired - cb) <IF_STMT> return a else: return b m = (a + b) / 2.0 cm = count(m) if desired < cm: return search(m, b, desired) else: return search(a, m, desired)",if dista < distb:,if dista < distb:,0.6752108857228942,0.9114434865990403,True
3210,"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): <IF_STMT> newline = re.sub('\\slocalhost\\s', ' ', line) if line != newline: logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), 'w') for line in lines: f.write(line) f.close()",if '::1' in line:,if line.startswith('ipv4'):,0.9536942761875588,0.9298663600557577,False
3211,"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' <IF_STMT> continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield (cog_name, cog_id)",if not fpath.exists():,if not os.path.exists(fpath):,0.7807401589118981,0.9118021019905903,False
3212,def _get_dbutils(): try: import IPython ip_shell = IPython.get_ipython() <IF_STMT> raise _NoDbutilsError return ip_shell.ns_table['user_global']['dbutils'] except ImportError: raise _NoDbutilsError except KeyError: raise _NoDbutilsError,if ip_shell is None:,if ip_shell.ns_table['user_global'] is None:,0.6601156936957778,0.7498810286408993,False
3213,"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: ext = os.path.splitext(os.path.normcase(py_file))[1] <IF_STMT> continue if self.compile: bytecode_files.append(py_file + 'c') if self.optimize > 0: bytecode_files.append(py_file + 'o') return bytecode_files",if ext != PYTHON_SOURCE_EXTENSION:,if ext.startswith('py'):,0.9263825348766788,0.8743414417652072,False
3214,"def compute_distances_mu(line, pts, result, gates, tolerance): """"""calculate all distances with mathuutils"""""" line_origin = V(line[0]) line_end = V(line[-1]) local_result = [[], [], [], [], []] for point in pts: data = compute_distance(V(point), line_origin, line_end, tolerance) for i, res in enumerate(local_result): res.append(data[i]) for i, res in enumerate(result): <IF_STMT> res.append(local_result[i])",if gates[i]:,if i < gates:,0.902896563770001,0.901790734099537,False
3215,"def _get_next_segment(self, segment_path, page_size, segment_cursor=None): if segment_path: <IF_STMT> return None return Segment(self.client, segment_path, page_size, segment_cursor) return None",if self.end_time and self._is_later_than_end_time(segment_path):,if not self.client.get_segment(segment_path):,0.7870773099900076,0.7331765459202478,False
3216,def _check_number_of_sessions(): nb_desktop_sessions = sessions.get_number_of_desktop_sessions(ignore_gdm=True) if nb_desktop_sessions > 1: print('WARNING : There are %d other desktop sessions open. The GPU switch will not become effective until you have manually logged out from ALL desktop sessions.\nContinue ? (y/N)' % (nb_desktop_sessions - 1)) confirmation = ask_confirmation() <IF_STMT> sys.exit(0),if not confirmation:,if not confirmation:,0.9361233891538331,0.9190754571515228,True
3217,"def delete_compute_environment(self, compute_environment_name): if compute_environment_name is None: raise InvalidParameterValueException('Missing computeEnvironment parameter') compute_env = self.get_compute_environment(compute_environment_name) if compute_env is not None: self._compute_environments.pop(compute_env.arn) self.ecs_backend.delete_cluster(compute_env.ecs_name) <IF_STMT> instance_ids = [instance.id for instance in compute_env.instances] self.ec2_backend.terminate_instances(instance_ids)",if compute_env.env_type == 'MANAGED':,if compute_env.instances is not None:,0.8740333330577678,0.7975010608178975,False
3218,"def run(self): results = {} for func_name in [fn for fn in self.check_functions if not self.args.get('check') or self.args.get('check') == fn]: function = getattr(self, func_name) log.warn(function.__doc__) result = function() <IF_STMT> log.info('\n'.join(result)) results.update({func_name: result}) return results",if result:,if result:,0.8855252286773304,0.8935248372106969,True
3219,"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [br for br in self.blockRenderers <IF_STMT>] if len(blockRenderers) < len(self.blockRenderers): self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",if br.layer is Layer.Blocks or br.layer not in layers,if br.isVisible() and br.isVisible(),0.91910679837273,0.8692960007731574,False
3220,"def get_library_dirs(platform, arch=None): if platform == 'win32': jre_home = get_jre_home(platform) jdk_home = JAVA_HOME <IF_STMT> jre_home = jre_home.decode('utf-8') return [join(jdk_home, 'lib'), join(jdk_home, 'bin', 'server')] elif platform == 'android': return ['libs/{}'.format(arch)] return []","if isinstance(jre_home, bytes):","if isinstance(jre_home, bytes):",0.820538654969346,0.8547305998833805,True
3221,"def save_plugin_options(self): for name, option_widgets in self._plugin_option_widgets.items(): <IF_STMT> self.config['plugins'][name] = {} plugin_config = self.config['plugins'][name] for option_name, option_widget in option_widgets.items(): plugin_config[option_name] = option_widget.option.get_widget_value(option_widget.widget)",if name not in self.config['plugins']:,if name not in self.config['plugins']:,0.7446735300366825,0.7297349727547102,True
3222,"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 elif str_in[pos] == end_tag: depth -= 1 <IF_STMT> break sel = str_in[start_pos + 1:pos] return sel",if depth == 0:,if depth == 0:,0.9301251772801108,0.9001816649635144,True
3223,"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val elif not compare.compatible_with(v, True): const = not true_val <IF_STMT> const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var","elif not compare.compatible_with(v, False):","elif compare.compatible_with(v, False):",0.8646330445752708,0.9182210682909737,False
3224,def multiline_indentation(self): if self._multiline_indentation is None: offset = 0 <IF_STMT> offset = 2 indentation = make_indentation(3 * self.indent_size + offset) self._multiline_indentation = indentation if self.current_rule: indent_extra = make_indentation(self.indent_size) return self._multiline_indentation + indent_extra return self._multiline_indentation,if self.show_aligned_keywords:,if self.current_rule:,0.7376955043067214,0.8935248372106969,False
3225,"def __call__(self, event, data=None): datatype, delta = event self.midi_ctrl.delta += delta if TIMING_CLOCK in datatype and (not self.played): self.midi_ctrl.pulse += 1 <IF_STMT> t_master = 60.0 self.midi_ctrl.bpm = round(60.0 / self.midi_ctrl.delta, 0) self.midi_ctrl.pulse = 0 self.midi_ctrl.delta = 0.0",if self.midi_ctrl.pulse == self.midi_ctrl.ppqn:,if self.midi_ctrl.pulse > 60.0:,0.9118593240860253,0.8555308664663046,False
3226,"def handle_sent(self, elt): sent = [] for child in elt: <IF_STMT> itm = self.handle_word(child) if self._unit == 'word': sent.extend(itm) else: sent.append(itm) else: raise ValueError('Unexpected element %s' % child.tag) return SemcorSentence(elt.attrib['snum'], sent)","if child.tag in ('wf', 'punc'):",if child.tag == 'sent':,0.8898964876015851,0.828399516355805,False
3227,"def _handle_def_errors(testdef): if testdef.error: <IF_STMT> if isinstance(testdef.exception, Exception): raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception('Test parse failure')",if testdef.exception:,if testdef.exception:,0.6053613305971458,0.7912619863720214,True
3228,"def _authorized_sid(self, jid, sid, ifrom, iq): with self._preauthed_sids_lock: <IF_STMT> del self._preauthed_sids[jid, sid, ifrom] return True return False","if (jid, sid, ifrom) in self._preauthed_sids:",if sid in self._preauthed_sids:,0.5139741703286049,0.693395566222006,False
3229,"def wait(self, timeout=None): if self.returncode is None: <IF_STMT> msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",if timeout is None:,if timeout is None:,0.7201891021786945,0.8752376177722327,True
3230,"def _gen_legal_y_s_t(self): while True: y = self._gen_random_scalar() s = self.tec_arithmetic.mul(scalar=y, a=self.tec_arithmetic.get_generator()) t = self._hash_tec_element(s) <IF_STMT> LOGGER.info('randomly generated y, S, T') return (y, s, t)",if self.tec_arithmetic.is_in_group(s) and type(t) != int:,if t != 0:,0.6541534227134687,0.7801270245332924,False
3231,def write_out(): while True: <IF_STMT> time.sleep(0.1) continue data_str = self.instrument_queue.get() data_str = data_str.splitlines() tb.write('') for line in data_str: tb.write(line) tb.write('\n'),if self.instrument_queue.empty():,if self.instrument_queue is None:,0.8558848348787438,0.7378351342269067,False
3232,"def _parse_preamble(self): """"""Parse metadata about query (PRIVATE)."""""" meta = {} while self.line: regx = re.search(_RE_QUERY, self.line) if regx: self.query_id = regx.group(1) <IF_STMT> self.seq_len = int(self.line.strip().split()[1]) self.line = self.handle.readline().strip() return meta",if self.line.startswith('Match_columns'):,if self.query_id == 'PRIVATE':,0.7746261057883671,0.8228500218338367,False
3233,"def init_sequence(self, coll_name, seq_config): if not isinstance(seq_config, list): raise Exception('""sequence"" config must be a list') handlers = [] for entry in seq_config: <IF_STMT> raise Exception('""sequence"" entry must be a dict') name = entry.get('name', '') handler = self.load_coll(name, entry) handlers.append(handler) return HandlerSeq(handlers)","if not isinstance(entry, dict):","if not isinstance(entry, dict):",0.8330433005792095,0.8692960007731574,True
3234,def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith(' '): ind += 1 else: <IF_STMT> ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if ':' in line and len(line) > 0: lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if start < ind:,if strings[ind].startswith(' '):,0.8126098116342984,0.9318557434050335,False
3235,def wait(self): while True: return_code = self._process.poll() if return_code is not None: line = self._process.stdout.readline().decode('utf-8') <IF_STMT> break log.debug(line.strip('\n')) return True,if line == '':,if not line:,0.6201984789728953,0.7739321540474097,False
3236,"def __getattr__(self, key): for tag in self.tag.children: <IF_STMT> continue if 'name' in tag.attrs and tag.attrs['name'] in (key,): from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError","if tag.name not in ('input',):",if tag.tagName != 'div':,0.7859460551467575,0.7965020533851944,False
3237,"def compare_hash(hash_of_gold, path_to_file): with open(path_to_file, 'rb') as f: hash_of_file = hashlib.sha256(f.read()).hexdigest() <IF_STMT> print('########## Hash sum of', path_to_file, 'differs from the target, the topology will be deleted !!! ##########') shutil.rmtree(os.path.dirname(path_to_file))",if hash_of_file != hash_of_gold:,if hash_of_gold != hash_of_file:,0.7136374266856568,0.8169276475307028,False
3238,def on_completed2(): doner[0] = True if not qr: <IF_STMT> observer.on_next(False) observer.on_completed() elif donel[0]: observer.on_next(True) observer.on_completed(),if len(ql) > 0:,if donel[1]:,0.7279605956411993,0.7447819789879647,False
3239,"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): <IF_STMT> m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] if is_tuple: return tuple(data) else: return data else: return None",if item < 0:,if abs(item) < len(data):,0.8993025492528413,0.9099929453837925,False
3240,"def _open_url(cls, url): if config.browser: cmd = [config.browser, url] <IF_STMT> print('running command: %s' % ' '.join(cmd)) p = Popen(cmd) p.communicate() else: if not config.quiet: print('opening URL in browser: %s' % url) webbrowser.open_new(url)",if not config.quiet:,if not config.quiet:,0.74830302909562,0.8592899528284996,True
3241,"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = (self.c, self, self.w) if w: if hasattr(g.app.gui, 'set_minibuffer_label'): g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) <IF_STMT> k.mb_prefix = s",if protect:,if protect:,0.9251668248545702,0.8966773400768917,True
3242,"def __init__(self, path): self.symcaches = [] for path in path.split(';'): if os.path.isdir(path): self.symcaches.append(SymbolCache(dirname=path)) continue <IF_STMT> import cobra self.symcaches.append(cobra.CobraProxy(path)) continue",if path.startswith('cobra://') or path.startswith('cobrassl://'):,if os.path.isfile(path):,0.7648321280020082,0.803154665668484,False
3243,"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode='fan_out') if module.bias: init.constant(module.bias, 0) <IF_STMT> init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=0.001) if module.bias: init.constant(module.bias, 0)","elif isinstance(module, nn.BatchNorm2d):","elif isinstance(module, nn.BatchNorm2d):",0.9121994087557924,0.8592899528284996,True
3244,"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): <IF_STMT> removed.append(key) elif old[key] != new[key]: removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff['removed'] = sorted(removed) if added: diff['added'] = sorted(added) return diff",if key not in new:,if key not in new:,0.9131426295182034,0.8729118929672821,True
3245,"def __init__(self, *args, **kwargs): _kwargs = {'max_length': 20, 'widget': forms.TextInput(attrs={'autocomplete': 'off'}), 'label': _('Card number')} if 'types' in kwargs: self.accepted_cards = set(kwargs.pop('types')) difference = self.accepted_cards - VALID_CARDS <IF_STMT> raise ImproperlyConfigured('The following accepted_cards are unknown: %s' % difference) _kwargs.update(kwargs) super().__init__(*args, **_kwargs)",if difference:,if difference != VALID_CARDS:,0.86266851970022,0.8627586293513119,False
3246,"def dumps(self): sections = [] for name, env_info in self._dependencies_.items(): sections.append('[ENV_%s]' % name) for var, values in sorted(env_info.vars.items()): tmp = '%s=' % var <IF_STMT> tmp += '[%s]' % ','.join(['""%s""' % val for val in values]) else: tmp += '%s' % values sections.append(tmp) return '\n'.join(sections)","if isinstance(values, list):","if isinstance(values, list):",0.8235106309076522,0.897752847848028,True
3247,"def air_quality(self): aqi_data = self._get_aqi_data() if aqi_data: if aqi_data.get('status') == 'ok': aqi_data = self._organize(aqi_data) aqi_data = self._manipulate(aqi_data) <IF_STMT> self.py3.error(aqi_data.get('data')) return {'cached_until': self.py3.time_in(self.cache_timeout), 'full_text': self.py3.safe_format(self.format, aqi_data)}",elif aqi_data.get('status') == 'error':,if not aqi_data:,0.9064456568597798,0.8196189957582152,False
3248,"def _blend(x, y): """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): <IF_STMT> return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y):] elif len(x) < len(y): result += y[len(x):] return result return y","if not isinstance(y, (list, tuple)):","if not isinstance(y, (list, tuple)):",0.8970229116609726,0.8901199011963146,True
3249,"def _rate(cls, sample1, sample2): """"""Simple rate"""""" try: interval = sample2[0] - sample1[0] <IF_STMT> raise Infinity() delta = sample2[1] - sample1[1] if delta < 0: raise UnknownValue() return (sample2[0], delta / interval, sample2[2], sample2[3]) except Infinity: raise except UnknownValue: raise except Exception as e: raise NaN(e)",if interval == 0:,if interval < 0:,0.9158889104854457,0.8806615362338783,False
3250,"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get('headers') is not None: <IF_STMT> if user_agent not in kwargs['headers']['user-agent']: kwargs['headers']['user-agent'] = f""{user_agent} {kwargs['headers']['user-agent']}"" else: kwargs['headers']['user-agent'] = user_agent else: kwargs['headers'] = {'user-agent': user_agent} return request_method(*args, **kwargs)",if kwargs['headers'].get('user-agent'):,if user_agent is not None:,0.6661819912135135,0.8380055871435848,False
3251,"def remove_addons(auth, resource_object_list): for config in AbstractNode.ADDONS_AVAILABLE: try: settings_model = config.node_settings except LookupError: settings_model = None <IF_STMT> addon_list = settings_model.objects.filter(owner__in=resource_object_list, is_deleted=False) for addon in addon_list: addon.after_delete(auth.user)",if settings_model:,if settings_model:,0.6251931656885852,0.8590888738245122,True
3252,"def Decorator(*args, **kwargs): delay = 0.2 num_attempts = 15 cur_attempt = 0 while True: try: return f(*args, **kwargs) except exceptions.WebDriverException as e: logging.warning('Selenium raised %s', utils.SmartUnicode(e)) cur_attempt += 1 <IF_STMT> raise time.sleep(delay)",if cur_attempt == num_attempts:,if cur_attempt >= num_attempts:,0.7680271934670316,0.8418243449361874,False
3253,"def _cleanup_parts_dir(parts_dir, local_plugins_dir, parts): if os.path.exists(parts_dir): logger.info('Cleaning up parts directory') for subdirectory in os.listdir(parts_dir): path = os.path.join(parts_dir, subdirectory) <IF_STMT> try: shutil.rmtree(path) except NotADirectoryError: os.remove(path) for part in parts: part.mark_cleaned(steps.BUILD) part.mark_cleaned(steps.PULL)",if path != local_plugins_dir:,if os.path.exists(path):,0.710359774261839,0.8787142254774354,False
3254,"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos['reach_leaf_node'].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos['node_pos'][t_idx] <IF_STMT> continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(tree, sample, cur_node_idx) if reach_leaf: node_pos['reach_leaf_node'][t_idx] = True node_pos['node_pos'][t_idx] = rs return node_pos",if cur_node_idx == -1:,if cur_node_idx == 0:,0.7529630328677197,0.8474968231198384,False
3255,"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == 'Image': results += ['Correlation', 'Slope'] else: results += ['Correlation'] if self.do_overlap: results += ['Overlap', 'K'] <IF_STMT> results += ['Manders'] if self.do_rwc: results += ['RWC'] if self.do_costes: results += ['Costes'] return results return []",if self.do_manders:,if self.do_manders:,0.9579258981184352,0.9298663600557577,True
3256,"def create_connection(self, infos, f2, laddr_infos, protocol): for family in infos: try: <IF_STMT> for laddr in laddr_infos: try: break except OSError: protocol = 'foo' else: continue except OSError: protocol = 'bar' else: break else: raise return protocol",if f2:,if family == f2:,0.8844717722044384,0.8516228624291206,False
3257,"def app_middleware(next, root, info, **kwargs): app_auth_header = 'HTTP_AUTHORIZATION' prefix = 'bearer' request = info.context if request.path == API_PATH: if not hasattr(request, 'app'): request.app = None auth = request.META.get(app_auth_header, '').split() if len(auth) == 2: auth_prefix, auth_token = auth <IF_STMT> request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",if auth_prefix.lower() == prefix:,if auth_prefix == prefix:,0.8857118538096533,0.8832000938217648,False
3258,"def when(self, matches, context): ret = [] for episode in matches.named('episode', lambda match: len(match.initiator) == 1): group = matches.markers.at_match(episode, lambda marker: marker.name == 'group', index=0) <IF_STMT> if not matches.range(*group.span, predicate=lambda match: match.name == 'title'): ret.append(episode) return ret",if group:,if group:,0.8278366242814641,0.9024521756077707,True
3259,def locate_via_pep514(spec): with _PY_LOCK: if not _PY_AVAILABLE: from . import pep514 _PY_AVAILABLE.extend(pep514.discover_pythons()) _PY_AVAILABLE.append(CURRENT) for cur_spec in _PY_AVAILABLE: <IF_STMT> return cur_spec.path,if cur_spec.satisfies(spec):,if cur_spec.spec == spec:,0.7169727288197346,0.7516324170467025,False
3260,def setCorkImageDefault(self): if settings.corkBackground['image'] != '': i = self.cmbCorkImage.findData(settings.corkBackground['image']) <IF_STMT> self.cmbCorkImage.setCurrentIndex(i),if i != -1:,if i != -1:,0.5419247287038138,0.6115380576901023,True
3261,"def _split_key(key): if isinstance(key, util.string_types): if key == _WILDCARD_TOKEN: return (_DEFAULT_TOKEN,) <IF_STMT> key = key[1:] return key.split('.') else: return (key,)",elif key.startswith('.' + _WILDCARD_TOKEN):,elif key.startswith('.'):,0.8837191331691793,0.8137489370974955,False
3262,"def detach_volume(self, volume): for node in self.list_nodes(): <IF_STMT> continue for disk in node.image: if disk.id == volume.id: disk_id = disk.extra['disk_id'] return self._do_detach_volume(node.id, disk_id) return False",if type(node.image) is not list:,if node.type != 'volume':,0.870951944719883,0.7886336751695258,False
3263,"def create(self, private=False): try: <IF_STMT> log.info('Creating private channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name, 'is_private': True}) else: log.info('Creating channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name}) except SlackAPIResponseError as e: if e.error == 'user_is_bot': raise RoomError(f'Unable to create channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e)",if private:,if private:,0.9323039585704356,0.9099951253570094,True
3264,"def test_dataset_has_valid_etag(self, dataset_name): py_script_path = list(filter(lambda x: x, dataset_name.split('/')))[-1] + '.py' dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True) etag = None try: response = requests.head(dataset_url, allow_redirects=True, proxies=None, timeout=10) <IF_STMT> etag = response.headers.get('Etag') except (EnvironmentError, requests.exceptions.Timeout): pass self.assertIsNotNone(etag)",if response.status_code == 200:,if response.status_code == 200:,0.8154954787359132,0.8474968231198384,True
3265,"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): if os.path.islink(dirpath): continue log.info('changing mode of %s to %o', dirpath, mode) <IF_STMT> os.chmod(dirpath, mode)",if not self.dry_run:,if mode != os.FMODE_WRITE:,0.7596953130350497,0.8200754821669128,False
3266,def _clean(self): logger.info('Cleaning up...') if self._process is not None: <IF_STMT> for _ in range(3): self._process.terminate() time.sleep(0.5) if self._process.poll() is not None: break else: self._process.kill() self._process.wait() logger.error('KILLED') if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info('Cleanup complete'),if self._process.poll() is None:,if self._process.poll() is not None:,0.9190522489541378,0.8338542560892604,False
3267,"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char['text'].isspace(): <IF_STMT> yield current_word current_word = [] elif current_word and self.char_begins_new_word(current_word, char): yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word",if current_word:,"if current_word and self.char_begins_new_word(char, char):",0.6786613741434246,0.8338542560892604,False
3268,"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) if comps: r = _lookup(comps, specs, provided, name, i + 1, l) <IF_STMT> return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",if r is not None:,if r is not None:,0.8426923293392323,0.88627064388393,True
3269,"def run(cmd, task=None): process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True) output_lines = [] while True: line = process.stdout.readline() <IF_STMT> break line = line.decode('utf-8') output_lines += [line] logger.info(line.rstrip('\n')) process.stdout.close() exit_code = process.wait() if exit_code: output = ''.join(output_lines) raise subprocess.CalledProcessError(exit_code, cmd, output=output)",if not line:,if not line:,0.7704213815392664,0.884617925078158,True
3270,"def process_response(self, request, response): if response.status_code == 404 and request.path_info.endswith('/') and (not is_valid_path(request.path_info)) and is_valid_path(request.path_info[:-1]): newurl = request.path[:-1] <IF_STMT> with safe_query_string(request): newurl += '?' + request.META['QUERY_STRING'] return HttpResponsePermanentRedirect(newurl) return response",if request.GET:,if request.META.get('QUERY_STRING'):,0.6458457234104201,0.8787142254774354,False
3271,"def dependencies(self): deps = [] midx = None if self.ref is not None: query = TypeQuery(self.ref) super = query.execute(self.schema) if super is None: log.debug(self.schema) raise TypeNotFound(self.ref) <IF_STMT> deps.append(super) midx = 0 return (midx, deps)",if not super.builtin():,"if isinstance(super, TypeDependency):",0.9202761364045045,0.8675979125638379,False
3272,"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'): <IF_STMT> with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, 'read'): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs",if isfile(self.object):,if os.path.isfile(self.object):,0.7968336408625486,0.9164531641034833,False
3273,"def _save(self): fd, tempname = tempfile.mkstemp() fd = os.fdopen(fd, 'w') json.dump(self._cache, fd, indent=2, separators=(',', ': ')) fd.close() try: <IF_STMT> os.makedirs(os.path.dirname(self.filename)) shutil.move(tempname, self.filename) except (IOError, OSError): os.remove(tempname)",if not os.path.exists(os.path.dirname(self.filename)):,if not os.path.exists(os.path.dirname(self.filename)):,0.6360905531871472,0.8266114125804572,True
3274,"def refiner_configs(self): rv = {} for refiner in refiner_manager: <IF_STMT> rv[refiner.name] = {k: v for k, v in self.config.items(refiner.name)} return rv",if self.config.has_section(refiner.name):,"if isinstance(refiner, Refiner):",0.733895129147855,0.7848518349390632,False
3275,"def com_slice(self, primary, node, assigning): lower = upper = None if len(node.children) == 2: <IF_STMT> upper = self.com_node(node.children[1]) else: lower = self.com_node(node.children[0]) elif len(node.children) == 3: lower = self.com_node(node.children[0]) upper = self.com_node(node.children[2]) return Slice(primary, assigning, lower, upper, lineno=extractLineNo(node))",if node.children[0].type == token.COLON:,if node.children[0] == 'com':,0.6621703697231954,0.8592377270804451,False
3276,"def close(self, *args, **kwargs): super(mytqdm, self).close(*args, **kwargs) if hasattr(self, 'sp'): if self.total and self.n < self.total: self.sp(bar_style='danger') el<IF_STMT> self.sp(bar_style='success') else: self.sp(close=True)",if self.leave:,if self.n > self.total:,0.8621105411073089,0.7498810286408993,False
3277,def test_alloc(self): b = bytearray() alloc = b.__alloc__() self.assertTrue(alloc >= 0) seq = [alloc] for i in range(100): b += b'x' alloc = b.__alloc__() self.assertTrue(alloc >= len(b)) <IF_STMT> seq.append(alloc),if alloc not in seq:,if alloc < len(seq):,0.7833399241131149,0.8434569599214109,False
3278,"def flush_file(self, key, f): f.flush() <IF_STMT> f.compress = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum((1 for f in self.files.values() if f.fileobj is not None)) if open_files > self.MAX_OPEN_FILES: f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",if self.compress:,if self.compress:,0.9446775818875845,0.9144061946646023,True
3279,"def _run(self): self.running = True while self.running: try: self.sched.run() except Exception as x: logging.error('Error during scheduler execution: %s' % str(x), exc_info=True) <IF_STMT> time.sleep(5)",if self.running:,if self.sched.is_running():,0.896612845131466,0.8764445248055556,False
3280,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <IF_STMT> self.set_max_rows(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:,if tt == 18:,0.9029988415311366,0.7965020533851944,False
3281,"def check(dbdef): """"""drop script must clear the database"""""" for version in dbdef: connector = MemConnector().bound(None) create(dbdef, version, connector) drop(dbdef, version, connector) remaining = connector.execute(""SELECT * FROM sqlite_master WHERE name NOT LIKE 'sqlite_%'"").fetchall() <IF_STMT> yield ('{0}:drop.sql'.format(version), remaining)",if remaining:,if remaining:,0.8220179419400764,0.8996480074924822,True
3282,"def test_open_overwrite_offset_size(self, sftp): """"""Test writing data at a specific offset"""""" f = None try: self._create_file('file', 'xxxxyyyy') f = (yield from sftp.open('file', 'r+')) yield from f.write('zz', 3) yield from f.close() with open('file') as localf: self.assertEqual(localf.read(), 'xxxzzyyy') finally: <IF_STMT> yield from f.close() remove('file')",if f:,if f:,0.9348815658920244,0.9122561819614461,True
3283,def pump(): import sys as _sys while self.countdown_active(): if not (self.connected('send') and other.connected('recv')): break try: data = other.recv(timeout=0.05) except EOFError: break <IF_STMT> return if not data: continue try: self.send(data) except EOFError: break if not _sys: return self.shutdown('send') other.shutdown('recv'),if not _sys:,if not _sys:,0.8404089729475257,0.8815741981066073,True
3284,"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if 'results.train_y_misclass' in dd: <IF_STMT> optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): if 'hyper_parameters' in key: print(key + ': ' + str(value))",if dd['results.train_y_misclass'] < optimal_measure:,if 'results.train_y_misclass' in dd['results'].keys():,0.9334184660275261,0.8780099567239787,False
3285,"def valid(self): valid = True <IF_STMT> return valid else: try: with io.open(self.pathfile, 'w', encoding='utf-8') as f: f.close() except OSError: valid = False if os.path.exists(self.pathfile): os.remove(self.pathfile) return valid",if os.path.exists(self.pathfile):,if self.pathfile is None:,0.8945515383662673,0.8038019482772603,False
3286,"def __getitem__(self, key): try: value = self.cache[key] except KeyError: f = BytesIO(self.dict[key.encode(self.keyencoding)]) value = Unpickler(f).load() <IF_STMT> self.cache[key] = value return value",if self.writeback:,if value is not None:,0.7496948470857329,0.7178970818142898,False
3287,"def hasMenu(cls, callingWindow, mainItem, selection, *fullContexts): for i, fullContext in enumerate(fullContexts): srcContext = fullContext[0] for menuHandler in cls.menus: m = menuHandler() <IF_STMT> return True return False","if m._baseDisplay(callingWindow, srcContext, mainItem, selection):","if m.hasSelection(callingWindow, mainItem, selection, srcContext):",0.6802938639066402,0.7685107079449489,False
3288,"def lr_read_tables(module=tab_module, optimize=0): global _lr_action, _lr_goto, _lr_productions, _lr_method try: exec('import %s as parsetab' % module) global parsetab <IF_STMT> _lr_action = parsetab._lr_action _lr_goto = parsetab._lr_goto _lr_productions = parsetab._lr_productions _lr_method = parsetab._lr_method return 1 else: return 0 except (ImportError, AttributeError): return 0",if optimize or Signature.digest() == parsetab._lr_signature:,if optimize:,0.7963260412483425,0.9099951253570094,False
3289,"def _Determine_Do(self): if sys.platform.startswith('win'): self.applicable = 1 for opt, optarg in self.chosenOptions: if opt == '--moz-tools': self.value = os.path.abspath(os.path.normpath(optarg)) break else: <IF_STMT> self.value = os.environ[self.name] else: self.value = None else: self.applicable = 0 self.determined = 1",if os.environ.has_key(self.name):,if self.name in os.environ:,0.7764127516364792,0.8516228624291206,False
3290,"def parse_chunked(self, unreader): size, rest = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() if not rest: raise NoMoreData() yield rest[:size] rest = rest[size:] while len(rest) < 2: rest += unreader.read() <IF_STMT> raise ChunkMissingTerminator(rest[:2]) size, rest = self.parse_chunk_size(unreader, data=rest[2:])",if rest[:2] != b'\r\n':,if rest[:2] == '\n':,0.9373306178044926,0.8879659171421962,False
3291,"def _scroll_down(self, cli): """"""Scroll window down."""""" info = self.render_info if self.vertical_scroll < info.content_height - info.window_height: <IF_STMT> self.content.move_cursor_down(cli) self.vertical_scroll += 1",if info.cursor_position.y <= info.configured_scroll_offsets.top:,if self.content:,0.5576011084411623,0.8137489370974955,False
3292,"def _add_defaults_data_files(self): if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): item = convert_path(item) if os.path.isfile(item): self.filelist.append(item) else: dirname, filenames = item for f in filenames: f = convert_path(f) <IF_STMT> self.filelist.append(f)",if os.path.isfile(f):,if os.path.isfile(f):,0.8556657560180343,0.9091726618909357,True
3293,"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): <IF_STMT> continue if i == 2 and self.count < 1: self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",if i <= start_after:,if i == start_after:,0.9056287251712858,0.8592377270804451,False
3294,"def is_open(self): if self.signup_code: return True el<IF_STMT> if self.messages.get('invalid_signup_code'): messages.add_message(self.request, self.messages['invalid_signup_code']['level'], self.messages['invalid_signup_code']['text'].format(**{'code': self.get_code()})) return settings.ACCOUNT_OPEN_SIGNUP",if self.signup_code_present:,if self.request.method == 'POST':,0.5532580684979065,0.6540585844910979,False
3295,"def on_delete_from_disk(self, widget, data=None): model, iter = self.get_selection().get_selected() if iter: path = model.get_value(iter, COLUMN_PATH) <IF_STMT> ErrorDialog(_(""Can't delete system item from disk."")).launch() else: os.remove(path) self.update_items()",if self.is_defaultitem(path):,if not os.path.exists(path):,0.7478186149487417,0.8120341702859789,False
3296,"def get_detections_for_batch(self, images): images = images[..., ::-1] detected_faces = self.face_detector.detect_from_batch(images.copy()) results = [] for i, d in enumerate(detected_faces): <IF_STMT> results.append(None) continue d = d[0] d = np.clip(d, 0, None) x1, y1, x2, y2 = map(int, d[:-1]) results.append((x1, y1, x2, y2)) return results",if len(d) == 0:,if i == 0:,0.8849906768527281,0.8723360571509826,False
3297,def on_update(self): self.max_per_well = 0 for pd in list(self.plate_well_site.values()): for wd in list(pd.values()): nplanes = sum([len(x) for x in list(wd.values())]) <IF_STMT> self.max_per_well = nplanes for registrant in self.registrants: registrant(),if nplanes > self.max_per_well:,if nplanes > self.max_per_well:,0.7348129221303185,0.8169276475307028,True
3298,"def is_writable(self, path): result = False while not result: if os.path.exists(path): result = os.access(path, os.W_OK) break parent = os.path.dirname(path) <IF_STMT> break path = parent return result",if parent == path:,if parent == path:,0.6242640900056073,0.7965020533851944,True
3299,"def _check_seed(self, seed): if seed is not None: <IF_STMT> self._raise_error('The random number generator seed value, seed, should be integer type or None.') if seed < 0: self._raise_error('The random number generator seed value, seed, should be non-negative integer or None.')",if type(seed) != int:,"if not isinstance(seed, (int, long)):",0.8885304345374428,0.8419539711731491,False
3300,"def write(self, x): self._errors = 'backslashescape' if self.encoding != 'mbcs' else 'surrogateescape' try: return io.TextIOWrapper.write(self, to_text(x, errors=self._errors)) except UnicodeDecodeError: <IF_STMT> self._errors = 'surrogateescape' else: self._errors = 'replace' return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))",if self._errors != 'surrogateescape':,if self.encoding == 'mbcs':,0.8684066416632097,0.828399516355805,False
3301,"def post(self, request, *args, **kwargs): validated_session = [] for session_id in request.data: session = get_object_or_none(Session, id=session_id) <IF_STMT> validated_session.append(session_id) self.model.objects.create(name='kill_session', args=session.id, terminal=session.terminal) return Response({'ok': validated_session})",if session and (not session.is_finished):,if session:,0.7752036663509465,0.8466657105524215,False
3302,"def _has_list_or_dict_var_value_before(self, arg_index): for idx, value in enumerate(self.args): <IF_STMT> return False if variablematcher.is_list_variable(value) and (not variablematcher.is_list_variable_subitem(value)): return True if robotapi.is_dict_var(value) and (not variablematcher.is_dict_var_access(value)): return True return False",if idx > arg_index:,if idx == arg_index:,0.8127747042144298,0.8038019482772603,False
3303,"def test_return_correct_type(self): for proto in protocols: <IF_STMT> self._check_return_correct_type('abc', 0) else: for obj in [b'abc\n', 'abc\n', -1, -1.1 * 0.1, str]: self._check_return_correct_type(obj, proto)",if proto == 0:,if proto == 'abc':,0.6007450385104602,0.760856626273165,False
3304,"def backward_impl(self, inputs, outputs, prop_down, accum): axis = self.forward_func.info.args['axis'] if prop_down[-1]: g_dy = inputs[-1].grad g_dy_ = F.stack(*[o.grad for o in outputs], axis=axis) <IF_STMT> g_dy += g_dy_ else: g_dy.copy_from(g_dy_)",if accum[-1]:,if accum[-1]:,0.6840540867441541,0.8696398662122882,True
3305,"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) elif was_selected: <IF_STMT> i = len(self.items) - 1 self.list.select_set(i)",if i >= len(self.items):,if i == 0:,0.9212553695747528,0.8385130047130208,False
3306,"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = '' if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text <IF_STMT> root = deepcopy(list(root)) tag[:0] = root root = tag[:len(root)] return self",if i > 0:,if i == len(root):,0.8215290179241316,0.8923575006167597,False
3307,"def _get_tracks_compositors_list(): tracks_list = [] tracks = current_sequence().tracks compositors = current_sequence().compositors for track_index in range(1, len(tracks) - 1): track_compositors = [] for j in range(0, len(compositors)): comp = compositors[j] <IF_STMT> track_compositors.append(comp) tracks_list.append(track_compositors) return tracks_list",if comp.transition.b_track == track_index:,if comp not in tracks:,0.8561817842136166,0.8200123297196334,False
3308,"def __getattr__(self, name): if name in self._sections: return '\n'.join(self._sections[name]) el<IF_STMT> return '' else: raise ConanException(""ConfigParser: Unrecognized field '%s'"" % name)",if self._allowed_fields and name in self._allowed_fields:,if name == 'default':,0.5921216348324845,0.7378351342269067,False
3309,"def get_first_param_index(self, group_id, param_group, partition_id): for index, param in enumerate(param_group): param_id = self.get_param_id(param) <IF_STMT> return index return None",if partition_id in self.param_to_partition_ids[group_id][param_id]:,if self.get_partition_id(param_id) == partition_id:,0.7148132701704655,0.7098232254187811,False
3310,"def handle_uv_sockets(self, context): u_socket = self.inputs['U'] v_socket = self.inputs['V'] if self.cast_mode == 'Sphere': u_socket.hide_safe = True v_socket.hide_safe = True elif self.cast_mode in ['Cylinder', 'Prism']: v_socket.hide_safe = True <IF_STMT> u_socket.hide_safe = False else: if u_socket.hide_safe: u_socket.hide_safe = False if v_socket.hide_safe: v_socket.hide_safe = False",if u_socket.hide_safe:,elif self.cast_mode == 'U':,0.6977986861863276,0.8723360571509826,False
3311,"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, 'r') as f: lines = f.readlines() <IF_STMT> return with open(source, 'w') as f: if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]): f.write(lines[0]) for line in lines[1:]: f.write(line)",if len(lines) < 1:,if not lines:,0.7844556169926865,0.9182210682909737,False
3312,"def inner(request, *args, **kwargs): page = request.current_page if page: if page.login_required and (not request.user.is_authenticated): return redirect_to_login(urlquote(request.get_full_path()), settings.LOGIN_URL) site = get_current_site() <IF_STMT> return _handle_no_page(request) return func(request, *args, **kwargs)","if not user_can_view_page(request.user, page, site):",if site.is_anonymous and (not site.is_authenticated):,0.8895521261434993,0.7765145040967655,False
3313,"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: <IF_STMT> self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: if '_create_temporary' in traceback.format_exc(): self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()","if kwargs.get('in_place', False):",if '_create_temporary' in traceback.format_exc():,0.7407733726576594,0.8105932471967202,False
3314,"def sanitize_event_keys(kwargs, valid_keys): for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) for key in ['play', 'role', 'task', 'playbook']: <IF_STMT> if len(kwargs['event_data'][key]) > 1024: kwargs['event_data'][key] = Truncator(kwargs['event_data'][key]).chars(1024)","if isinstance(kwargs.get('event_data', {}).get(key), str):",if key in kwargs['event_data']:,0.7489917565330787,0.8105932471967202,False
3315,"def parse_auth(val): if val is not None: authtype, params = val.split(' ', 1) <IF_STMT> if authtype == 'Basic' and '""' not in params: pass else: params = parse_auth_params(params) return (authtype, params) return val",if authtype in known_auth_schemes:,if authtype:,0.6302618688613696,0.8901732118131125,False
3316,"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) <IF_STMT> self.cache[args] = (value, now) return value except TypeError: return func(*args)",if value:,if value is not None:,0.9361796624634345,0.879962308706789,False
3317,def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color if t.theme_style == 'Dark': if self.md_bg_color == t.primary_color: c = t.primary_dark <IF_STMT> c = t.accent_dark return c,elif self.md_bg_color == t.accent_color:,elif self.md_bg_color == t.accent_color:,0.5871051238140567,0.7886336751695258,True
3318,def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 <IF_STMT> part_l |= 1 << 31 part_h >>= 1 if rflag: part_h ^= 3623878656 _table_h.append(part_h) return _table_h,if part_h & 1:,if rflag:,0.9330865199873041,0.9184043388013005,False
3319,"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from['Stats']): if not old_obj.summary: self.entries_count['Stats'] -= 1 continue new_obj = self.model_to['Stats']() for key in new_obj.__table__.columns._data.keys(): <IF_STMT> continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",if key not in old_obj.__table__.columns:,if key == 'summary':,0.793949562314445,0.8038019482772603,False
3320,"def get_in_turn_repetition(pred, is_cn=False): """"""Get in-turn repetition."""""" if len(pred) == 0: return 1.0 if isinstance(pred[0], str): pred = [tok.lower() for tok in pred] if is_cn: pred = ''.join(pred) tri_grams = set() for i in range(len(pred) - 2): tri_gram = tuple(pred[i:i + 3]) <IF_STMT> return 1.0 tri_grams.add(tri_gram) return 0.0",if tri_gram in tri_grams:,if tri_gram not in tri_grams:,0.9297562992029135,0.8677319190106252,False
3321,"def translate(): assert Lex.next() is AttributeList reader.read() attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: <IF_STMT> v = subs_attrs(v) if v: parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",if k == 'attrlist':,"if isinstance(v, str):",0.7931895530822041,0.8783650674919876,False
3322,"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if 'axis' in self.args: self.axis = engine.evaluate(self.args['axis'], recursive=True) <IF_STMT> raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')","if not isinstance(self.axis, int):","if not isinstance(self.axis, (int, float)):",0.9263849448571815,0.8527204701689132,False
3323,"def __getattr__(self, attrname): if attrname in ('visamp', 'visamperr', 'visphi', 'visphierr'): return ma.masked_array(self.__dict__['_' + attrname], mask=self.flag) elif attrname in ('cflux', 'cfluxerr'): <IF_STMT> return ma.masked_array(self.__dict__['_' + attrname], mask=self.flag) else: return None else: raise AttributeError(attrname)",if self.__dict__['_' + attrname] != None:,if self.flag == 1:,0.6197215930085704,0.8336104423443033,False
3324,"def draw(self, context): layout = self.layout presets.draw_presets_ops(layout, context=context) for category in presets.get_category_names(): <IF_STMT> if category in preset_category_menus: class_name = preset_category_menus[category].__name__ layout.menu(class_name)",if category in preset_category_menus:,if category not in preset_category_menus:,0.7530832723682224,0.7178970818142898,False
3325,"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value if info not in self._reverse_infos: self._reverse_infos[info] = len(self._infos) self._infos.append(info) <IF_STMT> self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = '%d,%d' % (self._reverse_infos[info], self._reverse_references[reference]) else: raise Exception(""unsupported type '%s'"" % type(value))",if reference not in self._reverse_references:,elif reference not in self._reverse_references:,0.6594144049449053,0.8419539711731491,False
3326,"def format_bpe_text(symbols, delimiter=b'@@'): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = b'' if isinstance(symbols, str): symbols = symbols.encode() delimiter_len = len(delimiter) for symbol in symbols: <IF_STMT> word += symbol[:-delimiter_len] else: word += symbol words.append(word) word = b'' return b' '.join(words)",if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:,if symbol.endswith(delimiter):,0.7733923850336774,0.9202663016973823,False
3327,"def output_type(data, request, response): accept = request.accept if accept in ('', '*', '/'): handler = default or (handlers and next(iter(handlers.values()))) else: handler = default accepted = [accept_quality(accept_type) for accept_type in accept.split(',')] accepted.sort(key=itemgetter(0)) for _quality, accepted_content_type in reversed(accepted): <IF_STMT> handler = handlers[accepted_content_type] break if not handler: raise falcon.HTTPNotAcceptable(error) response.content_type = handler.content_type return handler(data, request=request, response=response)",if accepted_content_type in handlers:,if accepted_content_type in handlers:,0.7809979753167569,0.9001816649635144,True
3328,"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: if item is None: flatten_items.append(b'') elif isinstance(item, bytes): flatten_items.append(item) <IF_STMT> flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b'\n'.join(flatten_items)","elif isinstance(item, int):","elif isinstance(item, str):",0.8936967290731782,0.8266114125804572,False
3329,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_mime_type(d.getVarInt32()) continue if tt == 16: self.set_quality(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 8:,if tt == 8:,0.6067590041327626,0.7965020533851944,True
3330,"def delete(self, waiters): msgs = self.ofctl.get_all_flow(waiters) for msg in msgs: for stats in msg.body: vlan_id = VlanRouter._cookie_to_id(REST_VLANID, stats.cookie) <IF_STMT> self.ofctl.delete_flow(stats) assert len(self.packet_buffer) == 0",if vlan_id == self.vlan_id:,if vlan_id == self.vlan_id:,0.7559931119954189,0.7801270245332924,True
3331,def missing_push_allowance(push_allowances: List[PushAllowance]) -> bool: for push_allowance in push_allowances: if push_allowance.actor.databaseId is None: continue <IF_STMT> return False return True,if str(push_allowance.actor.databaseId) == str(app_config.GITHUB_APP_ID):,if push_allowance.actor.databaseId == 'default':,0.5654174567331999,0.7245511487202049,False
3332,"def _cluster_page(self, htmlpage): template_cluster, preferred = (_CLUSTER_NA, None) if self.clustering: self.clustering.add_page(htmlpage) <IF_STMT> clt = self.clustering.classify(htmlpage) if clt != -1: template_cluster = preferred = self.template_names[clt] else: template_cluster = _CLUSTER_OUTLIER return (template_cluster, preferred)",if self.clustering.is_fit:,if self.template_names:,0.6008953206682521,0.8827916928185874,False
3333,"def readlines(self, size=-1): if self._nbr == self._size: return [] out = [] nbr = 0 while True: line = self.readline() if not line: break out.append(line) if size > -1: nbr += len(line) <IF_STMT> break return out",if nbr > size:,if nbr > self._size:,0.8592713710734124,0.8516228624291206,False
3334,"def post_mortem(t=None): <IF_STMT> t = sys.exc_info()[2] if t is None: raise ValueError('A valid traceback must be passed if no exception is being handled.') p = BPdb() p.reset() p.interaction(None, t)",if t is None:,if t is None:,0.8844299859463859,0.8260567476092244,True
3335,"def fixup(m): txt = m.group(0) if txt[:2] == '&#': try: <IF_STMT> return unichr(int(txt[3:-1], 16)) else: return unichr(int(txt[2:-1])) except ValueError: pass else: try: txt = unichr(htmlentitydefs.name2codepoint[txt[1:-1]]) except KeyError: pass return txt",if txt[:3] == '&#x':,if txt[3:-1] == '&#x':,0.7026911004385924,0.8228500218338367,False
3336,"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') <IF_STMT> value = item.group('value') value = _pythonize(value) if not item.group('name'): args.append(value) else: name = item.group('name') kwargs[name] = value return (tuple(args), kwargs)",if value is None:,if not value:,0.7953912448479634,0.897752847848028,False
3337,def IT(cpu): cc = cpu.instruction.cc true_case = cpu._evaluate_conditional(cc) for c in cpu.instruction.mnemonic[1:]: <IF_STMT> cpu._it_conditional.append(true_case) elif c == 'e': cpu._it_conditional.append(not true_case),if c == 't':,if c == 'n':,0.8525563132492402,0.7378351342269067,False
3338,"def flatten(self): result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) <IF_STMT> self.limit = 0 if self.reverse: data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",if len(data) < 100:,if not data:,0.7930465915373108,0.8902579342581529,False
3339,"def _get_beta_accumulators(self): with tf.init_scope(): <IF_STMT> graph = None else: graph = tf.get_default_graph() return (self._get_non_slot_variable('beta1_power', graph=graph), self._get_non_slot_variable('beta2_power', graph=graph))",if tf.executing_eagerly():,if self.beta1_power is None:,0.8025501992801946,0.693395566222006,False
3340,"def prefixed(self, prefix: _StrType) -> typing.Iterator['Env']: """"""Context manager for parsing envvars with a common prefix."""""" try: old_prefix = self._prefix <IF_STMT> self._prefix = prefix else: self._prefix = f'{old_prefix}{prefix}' yield self finally: self._prefix = None self._prefix = old_prefix",if old_prefix is None:,if old_prefix is None:,0.6269532913850137,0.8516228624291206,True
3341,"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get('content-type') if ct: ct, options = parse_options_header(ct) charset = options.get('charset') if ct in JSON_CONTENT_TYPES: return self.json(charset) <IF_STMT> return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",elif ct.startswith('text/'):,elif ct == TEXT_CONTENT_TYPES:,0.91130618387202,0.8692960007731574,False
3342,"def test_incrementaldecoder(self): UTF8Writer = codecs.getwriter('utf-8') for sizehint in [None, -1] + list(range(1, 33)) + [64, 128, 256, 512, 1024]: istream = BytesIO(self.tstring[0]) ostream = UTF8Writer(BytesIO()) decoder = self.incrementaldecoder() while 1: data = istream.read(sizehint) <IF_STMT> break else: u = decoder.decode(data) ostream.write(u) self.assertEqual(ostream.getvalue(), self.tstring[1])",if not data:,if not data:,0.8563331458343926,0.8928756684056034,True
3343,"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) <IF_STMT> delete_all(fn_full) elif fn.endswith('.png'): os.remove(fn_full) elif fn.endswith('.md'): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",if os.path.isdir(fn):,if fn.endswith('.png'):,0.802002258889022,0.8645707301556367,False
3344,"def _delete_reason(self): for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)): ext = _lib.X509_REVOKED_get_ext(self._revoked, i) obj = _lib.X509_EXTENSION_get_object(ext) <IF_STMT> _lib.X509_EXTENSION_free(ext) _lib.X509_REVOKED_delete_ext(self._revoked, i) break",if _lib.OBJ_obj2nid(obj) == _lib.NID_crl_reason:,if obj.status == 0:,0.6411838846666629,0.7098232254187811,False
3345,"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) if a < b: return -1 <IF_STMT> return 1 return 0 except: return cmp(x, y)",if a > b:,elif a > b:,0.6040143644946967,0.8038019482772603,False
3346,"def get_indentation_count(view, start): indent_count = 0 i = start - 1 while i > 0: ch = view.substr(i) scope = view.scope_name(i) if 'string.quoted' in scope or 'comment' in scope or 'preprocessor' in scope: extent = view.extract_scope(i) i = extent.a - 1 continue else: i -= 1 <IF_STMT> indent_count -= 1 elif ch == '{': indent_count += 1 return indent_count",if ch == '}':,if ch == '}':,0.7819431439709862,0.9084940438173679,True
3347,"def set(self, name, value, ex=None, px=None, nx=False, xx=False): if not nx and (not xx) or (nx and self._db.get(name, None) is None) or (xx and (not self._db.get(name, None) is None)): if ex > 0: self._db.expire(name, datetime.now() + timedelta(seconds=ex)) <IF_STMT> self._db.expire(name, datetime.now() + timedelta(milliseconds=px)) self._db[name] = str(value) return True else: return None",elif px > 0:,elif px > 0:,0.9373084157767279,0.8923575006167597,True
3348,"def _get_between(content, start, end=None): should_yield = False for line in content.split('\n'): if start in line: should_yield = True continue if end and end in line: return <IF_STMT> yield line.strip().split(' ')[0]",if should_yield and line:,if should_yield:,0.8709289399979052,0.8787142254774354,False
3349,"def iter_event_handlers(self, resource: resources_.Resource, event: bodies.RawEvent) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn('SimpleRegistry.iter_event_handlers() is deprecated; use ResourceWatchingRegistry.iter_handlers().', DeprecationWarning) cause = _create_watching_cause(resource, event) for handler in self._handlers: <IF_STMT> pass elif registries.match(handler=handler, cause=cause, ignore_fields=True): yield handler","if not isinstance(handler, handlers.ResourceWatchingHandler):",if handler is None:,0.8817257049808006,0.8228500218338367,False
3350,def __enter__(self): if log_timer: <IF_STMT> self.logger.debug('%s starting' % self.name) else: print('[%s starting]...' % self.name) self.tstart = time.time(),if self.logger:,if self.logger:,0.8419362924846858,0.7778111223054219,True
3351,"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True err_msg = 'T2T: skipped importing {num_missing} data_generators modules.' print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) <IF_STMT> print('Did not import module: %s; Cause: %s' % (module, err_str)) if not _is_import_err_msg(err_str, module): print('From module %s' % module) raise err",if log_all:,if log_all:,0.9546537356311748,0.9362597875749384,True
3352,"def _ungroup(sequence, groups=None): for v in sequence: if isinstance(v, (list, tuple)): <IF_STMT> groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v",if groups is not None:,if groups is not None:,0.8744016913360924,0.7506346798217074,True
3353,def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time <IF_STMT> self.stop() if self.counter == self.count: self.stop(),if dt > self.timeout:,if dt < self.timeout:,0.6628292536035056,0.8474968231198384,False
3354,def dont_let_stderr_buffer(): while True: line = context.daemon.stderr.readline() <IF_STMT> return if DEAD_DEPLOYD_WORKER_MESSAGE.encode('utf-8') in line: context.num_workers_crashed += 1 print(f'deployd stderr: {line}'),if not line:,if not line:,0.8716525964689825,0.761827408333416,True
3355,"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: if self.firsttime is None or x_stats.firsttime < self.firsttime: self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: <IF_STMT> self.lasttime = x_stats.lasttime self.last = x_stats.last",if self.lasttime is None or x_stats.lasttime >= self.lasttime:,if self.lasttime is None or x_stats.lasttime < self.lasttime:,0.8990107259893203,0.8070218370798333,False
3356,"def test_rlimit_get(self): import resource p = psutil.Process(os.getpid()) names = [x for x in dir(psutil) if x.startswith('RLIMIT')] assert names for name in names: value = getattr(psutil, name) self.assertGreaterEqual(value, 0) <IF_STMT> self.assertEqual(value, getattr(resource, name)) self.assertEqual(p.rlimit(value), resource.getrlimit(value)) else: ret = p.rlimit(value) self.assertEqual(len(ret), 2) self.assertGreaterEqual(ret[0], -1) self.assertGreaterEqual(ret[1], -1)",if name in dir(resource):,"if hasattr(resource, name):",0.7419686940471881,0.897752847848028,False
3357,"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): if not prop_name in entity.unindexed_properties(): prop_vals = entity[prop_name] <IF_STMT> num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes","if isinstance(prop_vals, list):","if isinstance(prop_vals, list):",0.7439477192129548,0.8635707684233572,True
3358,"def check_value_check(self, x_data, t_data, use_cudnn): x = chainer.Variable(x_data) t = chainer.Variable(t_data) with chainer.using_config('use_cudnn', use_cudnn): <IF_STMT> functions.softmax_cross_entropy(x, t, enable_double_backprop=self.enable_double_backprop) else: with self.assertRaises(ValueError): functions.softmax_cross_entropy(x, t, enable_double_backprop=self.enable_double_backprop)",if self.valid:,if self.use_cudnn:,0.8866848333486858,0.8466657105524215,False
3359,"def get_note_title_file(note): mo = note_title_re.match(note.get('content', '')) if mo: fn = mo.groups()[0] fn = fn.replace(' ', '_') fn = fn.replace('/', '_') <IF_STMT> return '' if isinstance(fn, str): fn = unicode(fn, 'utf-8') else: fn = unicode(fn) if note_markdown(note): fn += '.mkdn' else: fn += '.txt' return fn else: return ''",if not fn:,if not fn:,0.758159535004357,0.9062841320510342,True
3360,"def _parseparam(s): plist = [] while s[:1] == ';': s = s[1:] end = s.find(';') while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find(';', end + 1) if end < 0: end = len(s) f = s[:end] <IF_STMT> i = f.index('=') f = f[:i].strip().lower() + '=' + f[i + 1:].strip() plist.append(f.strip()) s = s[end:] return plist",if '=' in f:,if '=' in f:,0.72980319012906,0.9155272930874561,True
3361,"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue <IF_STMT> continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))",if grandchild.tagName != 'File':,if not grandchild.getAttribute('Source'):,0.91538014908299,0.8635707684233572,False
3362,"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: <IF_STMT> ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.datetime): ret = value.strftime('%d/%m/%y') elif isinstance(value, datetime.time): ret = value.strftime('%H:%M:%S') else: ret = value return ret","if isinstance(value, datetime.date):","if isinstance(value, datetime.date):",0.6657675455397816,0.8675979125638379,True
3363,"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) <IF_STMT> IECore.msg(IECore.Msg.Level.Error, 'GafferImageUI.CatalogueUI', ""No column registered with name '%s'"" % name) continue if isinstance(definition, IconColumn): c = GafferUI.PathListingWidget.IconColumn(definition.title(), '', name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns",if not definition:,if not definition:,0.7219626548181743,0.8875087479151215,True
3364,"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): <IF_STMT> v = v.item() if isinstance(v, dict): v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics","if isinstance(v, torch.Tensor):","if isinstance(v, dict):",0.8407277960430043,0.8266114125804572,False
3365,"def start(self, connection): try: if self.client_name: creds = gssapi.Credentials(name=gssapi.Name(self.client_name)) else: creds = None hostname = self.get_hostname(connection) name = gssapi.Name(b'@'.join([self.service, hostname]), gssapi.NameType.hostbased_service) context = gssapi.SecurityContext(name=name, creds=creds) return context.step(None) except gssapi.raw.misc.GSSError: <IF_STMT> return NotImplemented else: raise",if self.fail_soft:,if self.service is None:,0.6799868099512923,0.8431339019329497,False
3366,"def nanmax(self, axis=None, dtype=None, keepdims=None): ret = self._reduction('nanmax', axis=axis, dtype=dtype, keepdims=keepdims, todense=True) if not issparse(ret): <IF_STMT> return ret xps = get_sparse_module(self.spmatrix) ret = SparseNDArray(xps.csr_matrix(ret)) return ret return ret",if get_array_module(ret).isscalar(ret):,if not self.spmatrix:,0.8932527135956624,0.8390782502060267,False
3367,"def utterance_to_sample(query_data, tagging_scheme, language): tokens, tags = ([], []) current_length = 0 for chunk in query_data: chunk_tokens = tokenize(chunk[TEXT], language) tokens += [Token(t.value, current_length + t.start, current_length + t.end) for t in chunk_tokens] current_length += len(chunk[TEXT]) <IF_STMT> tags += negative_tagging(len(chunk_tokens)) else: tags += positive_tagging(tagging_scheme, chunk[SLOT_NAME], len(chunk_tokens)) return {TOKENS: tokens, TAGS: tags}",if SLOT_NAME not in chunk:,if chunk[SLOT_NAME] == '-':,0.8618857310126115,0.8944264839442453,False
3368,"def use_index(self, term: Union[str, Index], *terms: Union[str, Index]) -> 'QueryBuilder': for t in (term, *terms): if isinstance(t, Index): self._use_indexes.append(t) <IF_STMT> self._use_indexes.append(Index(t))","elif isinstance(t, str):","elif isinstance(t, str):",0.9014589513897049,0.8238874726594148,True
3369,"def reconfigServiceWithBuildbotConfig(self, new_config): if new_config.manhole != self.manhole: if self.manhole: yield self.manhole.disownServiceParent() self.manhole = None <IF_STMT> self.manhole = new_config.manhole yield self.manhole.setServiceParent(self) yield service.ReconfigurableServiceMixin.reconfigServiceWithBuildbotConfig(self, new_config)",if new_config.manhole:,if self.manhole:,0.7405994178965554,0.839587623092576,False
3370,"def cleanup_folder(target_folder): for file in os.listdir(target_folder): file_path = os.path.join(target_folder, file) try: <IF_STMT> os.remove(file_path) except Exception as e: logging.error(e)",if os.path.isfile(file_path):,if os.path.isfile(file_path):,0.5676127719535218,0.7912619863720214,True
3371,"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] <IF_STMT> return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)","if isinstance(k, float):","if isinstance(k, float):",0.64965414151759,0.9062841320510342,True
3372,"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) <IF_STMT> throw(DecompileError('Unsupported operation: %s' % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) if x is not None: decompiler.stack.append(x)",if method is None:,if method is None:,0.7422684202460865,0.8661072626070159,True
3373,"def shutdown(self, timeout, callback=None): logger.debug('background worker got shutdown request') with self._lock: if self.is_alive: self._queue.put_nowait(_TERMINATOR) <IF_STMT> self._wait_shutdown(timeout, callback) self._thread = None self._thread_for_pid = None logger.debug('background worker shut down')",if timeout > 0.0:,if callback:,0.7320757402148008,0.8645707301556367,False
3374,"def getDOMImplementation(features=None): if features: <IF_STMT> features = domreg._parse_feature_string(features) for f, v in features: if not Document.implementation.hasFeature(f, v): return None return Document.implementation","if isinstance(features, str):","if isinstance(features, str):",0.7608931921276155,0.7848518349390632,True
3375,"def validate_subevent(self, subevent): if self.context['event'].has_subevents: <IF_STMT> raise ValidationError('You need to set a subevent.') if subevent.event != self.context['event']: raise ValidationError('The specified subevent does not belong to this event.') elif subevent: raise ValidationError('You cannot set a subevent for this event.') return subevent",if not subevent:,"if not isinstance(subevent, Event):",0.8294720096237336,0.8661072626070159,False
3376,"def einsum(job_id, idx, einsum_expr, data_list): _, all_parties = session_init(job_id, idx) with SPDZ(): <IF_STMT> x = FixedPointTensor.from_source('x', data_list[0]) y = FixedPointTensor.from_source('y', all_parties[1]) else: x = FixedPointTensor.from_source('x', all_parties[0]) y = FixedPointTensor.from_source('y', data_list[1]) return x.einsum(y, einsum_expr).get()",if idx == 0:,if idx == 0:,0.8778173208551324,0.8385130047130208,True
3377,"def slowSorted(qq): """"""Reference sort peformed by insertion using only <"""""" rr = list() for q in qq: i = 0 for i in range(len(rr)): <IF_STMT> rr.insert(i, q) break else: rr.append(q) return rr",if q < rr[i]:,if rr[i] > q[i]:,0.9149311245886623,0.8336104423443033,False
3378,"def _format_entry(entry, src): if entry: result = [] for x in entry.split(','): x = x.strip() if os.path.exists(os.path.join(src, x)): result.append(relpath(os.path.join(src, x), src)) <IF_STMT> result.append(relpath(os.path.abspath(x), src)) else: raise RuntimeError('No entry script %s found' % x) return ','.join(result)",elif os.path.exists(x):,elif os.path.isabs(os.path.abspath(x)):,0.9190237457880709,0.8966773400768917,False
3379,"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): if shape: t = anytype elif 'M' in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif 'i' in fmt: t = int <IF_STMT> t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",elif 'f' in fmt:,elif 'f' in fmt:,0.8555136995865862,0.8856327184319047,True
3380,"def tool_lineages(self, trans): rval = [] for id, tool in self.app.toolbox.tools(): <IF_STMT> lineage_dict = tool.lineage.to_dict() else: lineage_dict = None entry = dict(id=id, lineage=lineage_dict) rval.append(entry) return rval","if hasattr(tool, 'lineage'):",if tool.lineage:,0.8333160549559975,0.8590888738245122,False
3381,"def item(self, tensor): numel = 0 if len(tensor.shape) > 0: numel = fct.reduce(op.mul, tensor.shape) <IF_STMT> raise ValueError(f'expected tensor with one element, got {tensor.shape}') if numel == 1: return tensor[0] return tensor",if numel != 1:,if numel != 1:,0.8977252844170684,0.828399516355805,True
3382,"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1).json() if 'Version' in resp: match = AGENT_VERSION_EXP.search(resp.get('Version')) <IF_STMT> meta['ecs_version'] = match.group(1) except Exception as e: self.log.debug('Error getting ECS version: %s' % str(e)) return meta",if match is not None and len(match.groups()) == 1:,if match:,0.6647775396684046,0.9051034981560222,False
3383,"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): <IF_STMT> yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",if leaf == SymbolTrue:,if leaf == SymbolTrue:,0.6531884863242938,0.8661072626070159,True
3384,"def _test_set_metadata(self, metadata, mask=None): header = ofproto.OXM_OF_METADATA match = OFPMatch() if mask is None: match.set_metadata(metadata) else: <IF_STMT> header = ofproto.OXM_OF_METADATA_W match.set_metadata_masked(metadata, mask) metadata &= mask self._test_serialize_and_parser(match, header, metadata, mask)",if mask + 1 >> 64 != 1:,if mask == 0:,0.654091210298743,0.8169276475307028,False
3385,"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == 'class': icon = 'class' elif tok.visibility == 'private': icon = 'method_priv' <IF_STMT> icon = 'method_prot' else: icon = 'method' crp.set_property('pixbuf', imagelibrary.pixbufs[icon])",elif tok.visibility == 'protected':,elif tok.visibility == 'prot':,0.7643125481611037,0.8431339019329497,False
3386,"def path_sum2(root, s): if root is None: return [] res = [] stack = [(root, [root.val])] while stack: node, ls = stack.pop() if node.left is None and node.right is None and (sum(ls) == s): res.append(ls) <IF_STMT> stack.append((node.left, ls + [node.left.val])) if node.right is not None: stack.append((node.right, ls + [node.right.val])) return res",if node.left is not None:,if node.left is not None:,0.943358785229965,0.8777008558345754,True
3387,"def clear_slot(self, slot_id, trigger_changed): if self.slots[slot_id] is not None: old_resource_id = self.slots[slot_id].resource_id <IF_STMT> del self.sell_list[old_resource_id] else: del self.buy_list[old_resource_id] self.slots[slot_id] = None if trigger_changed: self._changed()",if self.slots[slot_id].selling:,if old_resource_id in self.sell_list:,0.5980446187838299,0.7801270245332924,False
3388,"def OnRightUp(self, event): self.HandleMouseEvent(event) self.Unbind(wx.EVT_RIGHT_UP, handler=self.OnRightUp) self.Unbind(wx.EVT_MOUSE_CAPTURE_LOST, handler=self.OnRightUp) self._right = False if not self._left: self.Unbind(wx.EVT_MOTION, handler=self.OnMotion) self.SendChangeEvent() self.SetToolTip(wx.ToolTip(self._tooltip)) <IF_STMT> self.ReleaseMouse()",if self.HasCapture():,if self._left:,0.9251823039770203,0.8571061116877262,False
3389,"def __init__(self, *args, **kwargs): for arg in args: for k, v in arg.items(): <IF_STMT> arg[k] = AttrDict(v) else: arg[k] = v super(AttrDict, self).__init__(*args, **kwargs)","if isinstance(v, dict):","if isinstance(v, dict):",0.8618882798035823,0.8120341702859789,True
3390,"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: t.status = 'aborted by user' if status: status('%s aborted' % t.name, priority=2) except Exception as e: t.exception = e t.status = 'exception' vd.exceptionCaught(e) <IF_STMT> t.sheet.currentThreads.remove(t)",if t.sheet:,if t.sheet:,0.9673751122834331,0.9430609669970194,True
3391,"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): if text == '': self.labelList.item(i).setCheckState(2) <IF_STMT> self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",elif text != self.labelList.item(i).text():,elif text == '0':,0.8635312903781682,0.7245511487202049,False
3392,"def __attempt_add_to_linked_match(self, input_name, hdca, collection_type_description, subcollection_type): structure = get_structure(hdca, collection_type_description, leaf_subcollection_type=subcollection_type) if not self.linked_structure: self.linked_structure = structure self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type else: <IF_STMT> raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE) self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type",if not self.linked_structure.can_match(structure):,if structure.get_type() == 'leaf':,0.8972755396567929,0.8385130047130208,False
3393,"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) <IF_STMT> break if not online and (not self._is_testbot_online()): break else: raise AssertionError('test bot is still {}'.format('offline' if online else 'online'))",if online and self._is_testbot_online():,if online and (not self._is_testbot_offline()):,0.6157886889230858,0.7839800200378249,False
3394,"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 <IF_STMT> self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",if self.match_function(path):,if self.match_function(path):,0.7263758306282072,0.9051034981560222,True
3395,"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) <IF_STMT> continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return (graph, flag_changed)",if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:,if height > MAX_TEXTURE_SIZE:,0.7748454652131952,0.8431339019329497,False
3396,def brightness_func(args): device = _get_device_from_filter(args) if args.set is None: if args.raw: print(str(device.brightness)) else: print('Brightness: {0}%'.format(device.brightness)) else: brightness_value = float(_clamp_u8(args.set)) <IF_STMT> print('Setting brightness to {0}%'.format(brightness_value)) device.brightness = brightness_value,if not args.raw:,if args.raw:,0.635454560740814,0.8645707301556367,False
3397,"def _setup(self, field_name, owner_model): if not self.model_class: <IF_STMT> self.model_class = owner_model else: raise Exception(""ModelType: Unable to resolve model '{}'."".format(self.model_name)) super(ModelType, self)._setup(field_name, owner_model)",if self.model_name == owner_model.__name__:,if owner_model:,0.6207163719427957,0.8318180062062374,False
3398,"def build_json_schema_object(cls, parent_builder=None): builder = builders.ObjectBuilder(cls, parent_builder) if builder.count_type(builder.type) > 1: return builder for _, name, field in cls.iterate_with_name(): if isinstance(field, fields.EmbeddedField): builder.add_field(name, field, _parse_embedded(field, builder)) <IF_STMT> builder.add_field(name, field, _parse_list(field, builder)) else: builder.add_field(name, field, _create_primitive_field_schema(field)) return builder","elif isinstance(field, fields.ListField):","elif isinstance(field, fields.ListField):",0.9286717780585201,0.8783650674919876,True
3399,"def filter_module(mod, type_req=None, subclass_req=None): for name in dir(mod): val = getattr(mod, name) if type_req is not None and (not isinstance(val, type_req)): continue <IF_STMT> continue yield (name, val)","if subclass_req is not None and (not issubclass(val, subclass_req)):","if subclass_req is not None and (not issubclass(val, subclass_req)):",0.5992640794270543,0.6836858580099509,True
3400,"def get_icon(self): if self.icon is not None: if os.path.exists(self.icon): try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass icon_name, extension = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() <IF_STMT> return theme.load_icon(icon_name, 24, 0)",if theme.has_icon(icon_name):,if extension == 'png':,0.8181470109723796,0.828399516355805,False
3401,"def sysctlTestAndSet(name, limit): """"""Helper function to set sysctl limits"""""" if '/' not in name: name = '/proc/sys/' + name.replace('.', '/') with open(name, 'r') as readFile: oldLimit = readFile.readline() if isinstance(limit, int): <IF_STMT> with open(name, 'w') as writeFile: writeFile.write('%d' % limit) else: with open(name, 'w') as writeFile: writeFile.write(limit)",if int(oldLimit) < limit:,if limit < oldLimit:,0.8766918165185508,0.8856327184319047,False
3402,"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) if online and self._is_testbot_online(): break <IF_STMT> break else: raise AssertionError('test bot is still {}'.format('offline' if online else 'online'))",if not online and (not self._is_testbot_online()):,elif online and self._is_testbot_offline():,0.6037615553669475,0.7965020533851944,False
3403,"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: if context.rounding == ROUND_CEILING: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: <IF_STMT> return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context.rounding == ROUND_FLOOR:,if context.rounding == ROUND_CEILING:,0.9237448022685768,0.8944264839442453,False
3404,"def _get_item_columns_panel(items, rows): hbox = Gtk.HBox(False, 4) n_item = 0 col_items = 0 vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) while n_item < len(items): item = items[n_item] vbox.pack_start(item, False, False, 0) n_item += 1 col_items += 1 <IF_STMT> vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) col_items = 0 return hbox",if col_items > rows:,if col_items == rows:,0.9084848116181857,0.8923575006167597,False
3405,"def _changed(self): if self.gtk_range.get_sensitive(): <IF_STMT> self.timer.cancel() self.timer = _Timer(0.5, lambda: GLib.idle_add(self._write)) self.timer.start()",if self.timer:,if self.timer:,0.7556705825239948,0.6703420896351792,True
3406,"def unlock_graph(result, callback, interval=1, propagate=False, max_retries=None): if result.ready(): second_level_res = result.get() <IF_STMT> with allow_join_result(): signature(callback).delay(list(joinall(second_level_res, propagate=propagate))) else: unlock_graph.retry(countdown=interval, max_retries=max_retries)",if second_level_res.ready():,if second_level_res:,0.6528655463210649,0.803154665668484,False
3407,"def update(self, other=None, /, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if other is not None: <IF_STMT> other = dict(other) for key, o in other.items(): d[key] = KeyedRef(o, self._remove, key) for key, o in kwargs.items(): d[key] = KeyedRef(o, self._remove, key)","if not hasattr(other, 'items'):","if isinstance(other, dict):",0.8979146841156744,0.8875087479151215,False
3408,"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: if hasattr(o, 'profile'): del o.profile if hasattr(o, 'credentials'): del o.credentials if hasattr(o, 'metadata_path'): del o.metadata_path <IF_STMT> del o.services_config return vars(o) except Exception as e: return str(o)","if hasattr(o, 'services_config'):","if hasattr(o, 'services_config'):",0.8936862969491691,0.8783650674919876,True
3409,"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: <IF_STMT> log.info('read: e.errno=%s e.strerror=%s e.message=%s repr=%s' % (e.errno, e.strerror, e.message, repr(e))) if ignore_timeouts and is_timeout(e): return [] if ignore_non_errors and is_noerr(e): return [] raise",if DEBUG_COMM:,if log.isEnabledFor(logging.DEBUG):,0.93459516794018,0.9051034981560222,False
3410,def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: proc = PipelineProcess.objects.get(id=proc_id) <IF_STMT> continue for dr in self.doctors: if dr.confirm(proc): dr.cure(proc) break,if not proc.is_alive or proc.is_frozen:,if not proc:,0.708171443474942,0.8266114125804572,False
3411,"def to_value(self, value): ret = {} for key, val in value.items(): <IF_STMT> ret[key] = val elif key == 'points': ret[key] = {k: {'from': v[0], 'to': v[1]} for k, v in val.items()} else: ret[key] = {'from': val[0], 'to': val[1]} return ret","if key in ['attachments', 'custom_attributes', 'description_diff']:",if key == 'data':,0.7328808419202667,0.8661072626070159,False
3412,"def default_generator(self, dataset, epochs=1, mode='fit', deterministic=True, pad_batches=True): for epoch in range(epochs): for X_b, y_b, w_b, ids_b in dataset.iterbatches(batch_size=self.batch_size, deterministic=deterministic, pad_batches=pad_batches): <IF_STMT> dropout = np.array(0.0) else: dropout = np.array(1.0) yield ([X_b, dropout], [y_b], [w_b])",if mode == 'predict':,if epoch == 0:,0.8252454780665849,0.8385130047130208,False
3413,"def _cygwin_hack_find_addresses(target): addresses = [] for h in [target, 'localhost', '127.0.0.1']: try: addr = get_local_ip_for(h) <IF_STMT> addresses.append(addr) except socket.gaierror: pass return defer.succeed(addresses)",if addr not in addresses:,if addr:,0.7815917938705951,0.8318180062062374,False
3414,"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: if action_node.notify: task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify <IF_STMT> return self._chain_notify return None",elif self._chain_notify:,elif self._chain_notify:,0.8852192570733763,0.8137489370974955,True
3415,"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: <IF_STMT> break entry = extra.tokens[i] token = jsdict({'type': entry.type, 'value': entry.value}) if extra.range: token.range = entry.range if extra.loc: token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",if not i < len(extra.tokens):,if i is None:,0.8226292433034716,0.8856327184319047,False
3416,"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: if not self.in_current_block(self.cursor): self.seek_to_block(self.cursor) part = self.current_stream.read(size) <IF_STMT> if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",if size > 0:,if part:,0.7075864474752677,0.9076141716697395,False
3417,"def get_properties_from_model(model_class): """"""Show properties from a model"""""" properties = [] attr_names = [name for name, value in inspect.getmembers(model_class, isprop)] for attr_name in attr_names: <IF_STMT> attr_names.remove(attr_name) else: properties.append(dict(label=attr_name, name=attr_name.strip('_').replace('_', ' '))) return sorted(properties, key=lambda k: k['label'])",if attr_name.endswith('pk'):,if attr_name in model_class.__dict__:,0.8465341997990568,0.8474968231198384,False
3418,"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] if value is not None: return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent <IF_STMT> return",if mydict is None:,if mydict is not None:,0.873366181523362,0.8575831118026933,False
3419,"def multicolumn(self, list, format, cols=4): """"""Format a list of items into a multi-column list."""""" result = '' rows = (len(list) + cols - 1) // cols for col in range(cols): result = result + '<td width=""%d%%"" valign=top>' % (100 // cols) for i in range(rows * col, rows * col + rows): <IF_STMT> result = result + format(list[i]) + '<br>\n' result = result + '</td>' return '<table width=""100%%"" summary=""list""><tr>%s</tr></table>' % result",if i < len(list):,if list[i] is not None:,0.9689116438103647,0.9111620659625658,False
3420,"def format_exc(exc=None): """"""Return exc (or sys.exc_info if None), formatted."""""" try: <IF_STMT> exc = _exc_info() if exc == (None, None, None): return '' import traceback return ''.join(traceback.format_exception(*exc)) finally: del exc",if exc is None:,if exc is None:,0.8900992617566492,0.8169276475307028,True
3421,"def assert_counts(res, lang, files, blank, comment, code): for line in res: fields = line.split() if len(fields) >= 5: <IF_STMT> self.assertEqual(files, int(fields[1])) self.assertEqual(blank, int(fields[2])) self.assertEqual(comment, int(fields[3])) self.assertEqual(code, int(fields[4])) return self.fail('Found no output line for {}'.format(lang))",if fields[0] == lang:,if fields[0] == lang:,0.8817644371539513,0.8431339019329497,True
3422,"def __iter__(self): for name, value in self.__class__.__dict__.items(): <IF_STMT> continue if isinstance(value, flag_value): yield (name, self._has_flag(value.flag))","if isinstance(value, alias_flag_value):",if name.startswith('_'):,0.7321889171221232,0.7447819789879647,False
3423,"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_(beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment) if args.fp16: model.half() <IF_STMT> model.cuda()",if use_cuda:,if use_cuda:,0.8040247633294768,0.8764445248055556,True
3424,"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote('extracted from web2py', 'developer') for source_str in mydict.keys(): target_str = mydict[source_str] if target_str == source_str: target_str = u'' <IF_STMT> target_str = u'' pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",elif target_str.startswith(u'*** '):,elif target_str == targetheader:,0.8129585974668745,0.8474968231198384,False
3425,"def __sparse_values_set(instances, static_col_indexes: list): tmp_result = {idx: set() for idx in static_col_indexes} for _, instance in instances: data_generator = instance.features.get_all_data() for idx, value in data_generator: <IF_STMT> continue tmp_result[idx].add(value) result = [tmp_result[x] for x in static_col_indexes] return result",if idx not in tmp_result:,if value is None:,0.854186987374266,0.8555308664663046,False
3426,def puts(self): <IF_STMT> self.lazy_init_lock_.acquire() try: if self.puts_ is None: self.puts_ = PutRequest() finally: self.lazy_init_lock_.release() return self.puts_,if self.puts_ is None:,if self.puts_ is None:,0.6272973784763626,0.6929598487720369,True
3427,"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} if args.resource_ref: filters['resource_ref'] = args.resource_ref <IF_STMT> filters['policy_type'] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",if args.policy_type:,if args.policy_type:,0.8642060670434203,0.8590888738245122,True
3428,"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = '' for line in string.split(entry, '\n'): if line[0:5] == 'GN   ': GN = string.strip(line[5:]) <IF_STMT> GN = GN[0:-1] return GN if line[0:2] == '//': break return GN",if GN[-1] == '.':,if GN.endswith(' '):,0.857776830538757,0.9000283069718913,False
3429,"def processMovie(self, atom): for field in atom: <IF_STMT> self.processTrack(field['track']) if 'movie_hdr' in field: self.processMovieHeader(field['movie_hdr'])",if 'track' in field:,if 'track' in field:,0.6966296090066653,0.630190855592386,True
3430,"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: video_packet = self._get_video_packet() <IF_STMT> self._decode_video_packet(video_packet) if video_packet.image is not None or not skip_empty_frame: break if _debug: print('Returning', video_packet) return video_packet.image",if video_packet.image == 0:,if video_packet:,0.9214532502445683,0.8743414417652072,False
3431,"def get_devices(display=None): base = '/dev/input' for filename in os.listdir(base): if filename.startswith('event'): path = os.path.join(base, filename) <IF_STMT> continue try: _devices[path] = EvdevDevice(display, path) except OSError: pass return list(_devices.values())",if path in _devices:,if not os.path.isdir(path):,0.7492597656241495,0.833078701050083,False
3432,"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error('# channels not specified') <IF_STMT> raise Error('sample width not specified') if not self._framerate: raise Error('sampling rate not specified') self._write_header(datasize)",if not self._sampwidth:,if not self._samplewidth:,0.9067411588671368,0.8446593249975184,False
3433,"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, '..') for line in fuzzresult.history.content.splitlines(): record = line.split('/') if len(record) == 6 and record[1]: self.queue_url(urljoin(base_url, record[1])) <IF_STMT> self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, '%s/CVS/Entries' % record[1]))",if record[0] == 'D':,elif len(record) == 7 and record[1]:,0.8770689562194415,0.7665936070959262,False
3434,"def tearDown(self): """"""Shutdown the UDP server."""""" try: <IF_STMT> self.server.stop(2.0) if self.sock_hdlr: self.root_logger.removeHandler(self.sock_hdlr) self.sock_hdlr.close() finally: BaseTest.tearDown(self)",if self.server:,if self.server:,0.5198598361617925,0.7447819789879647,True
3435,"def get_backend(find_library=None): try: global _lib, _ctx <IF_STMT> _lib = _load_library(find_library) _setup_prototypes(_lib) _ctx = _Context() _logger.warning('OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284)') return _OpenUSB() except usb.libloader.LibraryException: _logger.error('Error loading OpenUSB backend', exc_info=False) return None except Exception: _logger.error('Error loading OpenUSB backend', exc_info=True) return None",if _lib is None:,if find_library is not None:,0.8079213716720903,0.8380055871435848,False
3436,"def __init__(self, event, event_info, fields=[]): _wmi_object.__init__(self, event, fields=fields) _set(self, 'event_type', None) _set(self, 'timestamp', None) _set(self, 'previous', None) if event_info: event_type = self.event_type_re.match(event_info.Path_.Class).group(1).lower() _set(self, 'event_type', event_type) if hasattr(event_info, 'TIME_CREATED'): _set(self, 'timestamp', from_1601(event_info.TIME_CREATED)) <IF_STMT> _set(self, 'previous', event_info.PreviousInstance)","if hasattr(event_info, 'PreviousInstance'):","if hasattr(event_info, 'PreviousInstance'):",0.662263615491899,0.8713933650206428,True
3437,"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: <IF_STMT> continue if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg): Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug('Adding ' + pkg + ' to the schedule list')",if pkg in Scheduler.listOfPackagesCurrentlyBuilding:,if pkg in Scheduler.listOfPackagesNextToBuild:,0.8704453431167003,0.7801270245332924,False
3438,"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: <IF_STMT> self.write('') self.process(line) if not is_command(line): gap = True return 0",if gap:,if gap:,0.700493859205803,0.8590888738245122,True
3439,"def diff(old, new, display=True): """"""Nice colored diff implementation"""""" if not isinstance(old, list): old = decolorize(str(old)).splitlines() if not isinstance(new, list): new = decolorize(str(new)).splitlines() line_types = {' ': '%Reset', '-': '%Red', '+': '%Green', '?': '%Pink'} if display: for line in difflib.Differ().compare(old, new): <IF_STMT> continue print(colorize(line_types[line[0]], line)) return old != new",if line.startswith('?'):,if not line:,0.9450795427284566,0.9062841320510342,False
3440,"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() if settings.MAX_PAGE_SIZE: <IF_STMT> return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",if limit == 0:,if limit < 0:,0.8900045496127809,0.8336104423443033,False
3441,"def slice_fill(self, slice_): """"""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"""""" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): if self.squeeze_dims[i]: new_slice_.append(0) <IF_STMT> new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",elif offset < len(slice_):,elif self.indexes[i] == 0:,0.933777023531811,0.8964173245779284,False
3442,"def wrapper(*args, **kw): instance = args[0] try: <IF_STMT> ret_dict = instance._create_ret_object(instance.FAILURE, None, True, instance.MUST_JSON) instance.logger.error(instance.MUST_JSON) return (jsonify(ret_dict), 400) except BadRequest: ret_dict = instance._create_ret_object(instance.FAILURE, None, True, instance.MUST_JSON) instance.logger.error(instance.MUST_JSON) return (jsonify(ret_dict), 400) instance.logger.debug('JSON is valid') return f(*args, **kw)",if request.get_json() is None:,if instance.FAILURE is not None:,0.6981253611725462,0.8294838585473985,False
3443,"def add_css(self, data): if data: for medium, paths in data.items(): for path in paths: <IF_STMT> self._css.setdefault(medium, []).append(path)",if not self._css.get(medium) or path not in self._css[medium]:,if path not in self._css:,0.5854874503004448,0.6735760523151638,False
3444,"def mangle_template(template: str, template_vars: Set[str]) -> str: if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template: raise Exception('Cannot parse a template containing reserved strings') for var in template_vars: original = f'{{{var}}}' <IF_STMT> raise Exception(f'Template string is missing a reference to ""{var}"" referred to in kwargs') template = template.replace(original, mangled_name(var)) return template",if original not in template:,if not original.startswith(template):,0.7827030959709157,0.9100365300271298,False
3445,"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode('ascii', 'ignore')) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: if key in seenDict: continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue <IF_STMT> matchesappend(key) return _sortKeywords(keyword, matches)","if kwdSndx == soundex(key.encode('ascii', 'ignore')):",if kwdSndx in key:,0.9520602055572747,0.914208565914368,False
3446,"def GetInfo(self): for k, v in sorted(self.memory_parameters.items()): <IF_STMT> continue if not v: continue print('%s: \t%#08x (%s)' % (k, v, v)) print('Memory ranges:') print('Start\t\tEnd\t\tLength') for start, length in self.runs: print('0x%X\t\t0x%X\t\t0x%X' % (start, start + length, length))",if k.startswith('Pad'):,if not k:,0.9210011912765089,0.8713933650206428,False
3447,"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): is_list, property_type, is_strong = attributes[0:3] if is_strong and property in self._properties: <IF_STMT> children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",if not is_list:,if is_list:,0.7451346825255474,0.9051034981560222,False
3448,"def normalize_res_identifier(self, emu, cw, val): mask = 16 ** (emu.get_ptr_size() // 2) - 1 << 16 if val & mask: name = emu.read_mem_string(val, cw) <IF_STMT> try: name = int(name[1:]) except Exception: return 0 else: name = val return name",if name[0] == '#':,if name.startswith('0x'):,0.9603809695029513,0.9076141716697395,False
3449,"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): if best_silhouette is None: pass <IF_STMT> break best_silhouette = silhouette best_a = a best_k = k return (best_a, best_silhouette, best_k)",elif silhouette <= best_silhouette:,elif silhouette == best_silhouette:,0.8513794991650938,0.8592377270804451,False
3450,"def find_commit_type(sha): try: o = obj_store[sha] except KeyError: <IF_STMT> raise else: if isinstance(o, Commit): commits.add(sha) elif isinstance(o, Tag): tags.add(sha) commits.add(o.object[1]) else: raise KeyError('Not a commit or a tag: %s' % sha)",if not ignore_unknown:,"if not isinstance(o, Commit):",0.6045541698173824,0.828399516355805,False
3451,"def on_search_entry_keypress(self, widget, event): key = Gdk.keyval_name(event.keyval) if key == 'Escape': self.hide_search_box() elif key == 'Return': <IF_STMT> self.search_prev = False self.do_search(None) else: self.search_prev = True",if event.state & Gdk.ModifierType.SHIFT_MASK:,if self.search_prev:,0.5950226530739059,0.8531413606256201,False
3452,"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: <IF_STMT> if '=' in each: key, value = each.split('=', 1) else: key, value = (each, '') result[key] = value namespace.webhook_properties = result",if each:,"if isinstance(each, str):",0.9060843647062162,0.8749766281017177,False
3453,"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() <IF_STMT> continue self.lock.acquire() self.modules[path_id] = module self.lock.release() if resolve: resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",if path_id is None:,if path_id is None:,0.7209226219967488,0.8516228624291206,True
3454,"def _get_download_link(self, url, download_type='torrent'): links = {'torrent': '', 'magnet': ''} try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find('div', {'class': 'download'}) if downloads: for download in downloads.findAll('a'): link = download['href'] <IF_STMT> links['magnet'] = link else: links['torrent'] = urljoin(self.urls['base_url'], link) except Exception: pass return links[download_type]",if link.startswith('magnet'):,if link.startswith('magnet'):,0.7750727711341878,0.9220450449751959,True
3455,"def _parse_fields(cls, read): read = unicode_to_str(read) if type(read) is not str: _wrong_type_for_arg(read, 'str', 'read') fields = {} while read and read[0] != ';': <IF_STMT> DeserializeError(read, 'does not separate fields with commas') read = read[1:] key, _type, value, read = cls._parse_field(read) fields[key] = (_type, value) if read: read = read[1:] return (fields, read)","if read and read[0] != ',':","if read[0] == ',':",0.9225588626554756,0.8964173245779284,False
3456,"def _convertDict(self, d): r = {} for k, v in d.items(): <IF_STMT> v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) if isinstance(k, bytes): k = str(k, 'utf-8') r[k] = v return r","if isinstance(v, bytes):","if isinstance(v, bytes):",0.9179286684749641,0.8953711787948615,True
3457,"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) <IF_STMT> cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) if len(cache) > max_size: cache.popitem(last=False) return result",if old_mtime == mtime:,if old_mtime != mtime:,0.7876348073425098,0.8592377270804451,False
3458,def isFinished(self): if self.count > self.epiLen: self.res() return True else: <IF_STMT> self.pertGlasPos(0) if self.count == self.epiLen / 2 + 1: self.env.reset() self.pertGlasPos(1) self.count += 1 return False,if self.count == 1:,if self.count == self.epiLen / 2 + 1:,0.8755974700360268,0.7047469163494912,False
3459,"def _check_vulnerabilities(self, processed_analysis): matched_vulnerabilities = list() for vulnerability in self._rule_base_vulnerabilities: <IF_STMT> vulnerability_data = vulnerability.get_dict() name = vulnerability_data.pop('short_name') matched_vulnerabilities.append((name, vulnerability_data)) return matched_vulnerabilities","if evaluate(processed_analysis, vulnerability.rule):",if vulnerability.get_name() == processed_analysis.name:,0.7843048926494866,0.7498810286408993,False
3460,"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith('Size'): <IF_STMT> return '  %s' % val elif val < 1024 ** 2: return '  %.1f KB' % (val / 1024.0 ** 1) elif val < 1024 ** 3: return '  %.1f MB' % (val / 1024.0 ** 2) else: return '  %.1f GB' % (val / 1024.0 ** 3) if col in (0, ''): return str(val) else: return '  %s' % val","if isinstance(val, compat.string_types):",if val < 1024.0:,0.6713822811838199,0.9215557939968955,False
3461,"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) if rd: self.handle_request() <IF_STMT> break",if self.event is not None and self.event.is_set():,if not ex:,0.8606174365540832,0.8055344092731546,False
3462,"def resize(self, *e): bold = ('helvetica', -self._size.get(), 'bold') helv = ('helvetica', -self._size.get()) xspace = self._size.get() yspace = self._size.get() for widget in self._widgets: widget['node_font'] = bold widget['leaf_font'] = helv widget['xspace'] = xspace widget['yspace'] = yspace if self._size.get() < 20: widget['line_width'] = 1 <IF_STMT> widget['line_width'] = 2 else: widget['line_width'] = 3 self._layout()",elif self._size.get() < 30:,elif self._size.get() > 40:,0.8628074450041918,0.8923575006167597,False
3463,"def __assertTilesChangedInRegion(self, t1, t2, region): for tileOriginTuple in t1.keys(): tileOrigin = imath.V2i(*tileOriginTuple) tileRegion = imath.Box2i(tileOrigin, tileOrigin + imath.V2i(GafferImage.ImagePlug.tileSize())) <IF_STMT> self.assertNotEqual(t1[tileOriginTuple], t2[tileOriginTuple]) else: self.assertEqual(t1[tileOriginTuple], t2[tileOriginTuple])","if GafferImage.BufferAlgo.intersects(tileRegion, region):",if region == tileRegion:,0.7952584951772369,0.7801270245332924,False
3464,"def grouped_by_prefix(args, prefixes): """"""Group behave args by (directory) scope into multiple test-runs."""""" group_args = [] current_scope = None for arg in args.strip().split(): assert not arg.startswith('-'), 'REQUIRE: arg, not options' scope = select_prefix_for(arg, prefixes) if scope != current_scope: <IF_STMT> yield ' '.join(group_args) group_args = [] current_scope = scope group_args.append(arg) if group_args: yield ' '.join(group_args)",if group_args:,if group_args:,0.937240825393677,0.9325718821645923,True
3465,"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) <IF_STMT> continue if isinstance(value, basestring): print_func = repr pieces.append('%s=%s' % (k, print_func(value))) if pieces or self.__base__: return '%s(%s)' % (self.__class__.__name__, ', '.join(pieces)) else: return ''",if not defaults and value == default_values[k]:,if value in default_values:,0.660510162515415,0.8944264839442453,False
3466,"def setInnerHTML(self, html): log.HTMLClassifier.classify(log.ThugLogging.url if log.ThugOpts.local else log.last_url, html) self.tag.clear() for node in bs4.BeautifulSoup(html, 'html.parser').contents: self.tag.append(node) name = getattr(node, 'name', None) <IF_STMT> continue handler = getattr(log.DFT, 'handle_%s' % (name,), None) if handler: handler(node)",if name is None:,if name is None:,0.7709570878091802,0.8385130047130208,True
3467,"def createFields(self): yield Enum(Bits(self, 'class', 2), self.CLASS_DESC) yield Enum(Bit(self, 'form'), self.FORM_DESC) if self['class'].value == 0: yield Enum(Bits(self, 'type', 5), self.TYPE_DESC) else: yield Bits(self, 'type', 5) yield ASNInteger(self, 'size', 'Size in bytes') size = self['size'].value if size: <IF_STMT> for field in self._handler(self, size): yield field else: yield RawBytes(self, 'raw', size)",if self._handler:,if self._handler:,0.9119668170672335,0.926934323706186,True
3468,"def _process_service_request(self, pkttype, pktid, packet): """"""Process a service request"""""" service = packet.get_string() packet.check_end() if service == self._next_service: self.logger.debug2('Accepting request for service %s', service) self._next_service = None self.send_packet(MSG_SERVICE_ACCEPT, String(service)) <IF_STMT> self._auth_in_progress = True self._send_deferred_packets() else: raise DisconnectError(DISC_SERVICE_NOT_AVAILABLE, 'Unexpected service request received')",if self.is_server() and service == _USERAUTH_SERVICE:,if self._auth_in_progress:,0.6475171368650092,0.9099951253570094,False
3469,"def _read_fixed_body(self, content_length: int, delegate: httputil.HTTPMessageDelegate) -> None: while content_length > 0: body = await self.stream.read_bytes(min(self.params.chunk_size, content_length), partial=True) content_length -= len(body) <IF_STMT> with _ExceptionLoggingContext(app_log): ret = delegate.data_received(body) if ret is not None: await ret",if not self._write_finished or self.is_client:,if body is not None:,0.7820888860510573,0.8200123297196334,False
3470,"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) <IF_STMT> return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.ECHILD: return time.sleep(0.05) assert False, 'wait_for_child() timed out'",if target_pid == pid:,if status == 0:,0.9223362725600002,0.8516228624291206,False
3471,"def execute(cls, ctx, op: 'DataFrameGroupByAgg'): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) <IF_STMT> cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: raise ValueError('Aggregation operand not executable') finally: pd.reset_option('mode.use_inf_as_na')",elif op.stage == OperandStage.combine:,elif op.stage == OperandStage.combine:,0.6262491637091745,0.828399516355805,True
3472,def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): <IF_STMT> yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,if word not in Force_Split_Words:,if word == re_skip:,0.9273700339433415,0.8661072626070159,False
3473,"def _iter_tags(self, type=None): """"""Yield all raw tags (limit to |type| if specified)"""""" for n in itertools.count(): tag = self._get_tag(n) <IF_STMT> yield tag if tag['d_tag'] == 'DT_NULL': break",if type is None or tag['d_tag'] == type:,if type and tag['d_tag'] == type:,0.7942286926266776,0.7510201430702309,False
3474,"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [(idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) if line.lstrip().startswith(searchfor.lstrip())] else: res = [(idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) <IF_STMT>] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return ''",if line.startswith(searchfor),if line.startswith(searchfor.lstrip()),0.9361736153876331,0.9184043388013005,False
3475,"def value_to_db_datetime(self, value): if value is None: return None if timezone.is_aware(value): <IF_STMT> value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError('Oracle backend does not support timezone-aware datetimes when USE_TZ is False.') return unicode(value)",if settings.USE_TZ:,if self.USE_TZ:,0.8926332717366385,0.8787142254774354,False
3476,"def _sniff(filename, oxlitype): try: with open(filename, 'rb') as fileobj: header = fileobj.read(4) if header == b'OXLI': fileobj.read(1) ftype = fileobj.read(1) <IF_STMT> return True return False except OSError: return False",if binascii.hexlify(ftype) == oxlitype:,if ftype == oxlitype:,0.8094585941573895,0.8169276475307028,False
3477,"def unget(self, char): if char is not EOF: <IF_STMT> self.chunk = char + self.chunk self.chunkSize += 1 else: self.chunkOffset -= 1 assert self.chunk[self.chunkOffset] == char",if self.chunkOffset == 0:,if self.chunkOffset == 0:,0.8323661119044004,0.7886336751695258,True
3478,"def scan(rule, extensions, paths, ignore_paths=None): """"""The libsast scan."""""" try: options = {'match_rules': rule, 'match_extensions': extensions, 'ignore_paths': ignore_paths, 'show_progress': False} scanner = Scanner(options, paths) res = scanner.scan() <IF_STMT> return format_findings(res['pattern_matcher'], paths[0]) except Exception: logger.exception('libsast scan') return {}",if res:,if res:,0.9106196952532085,0.8996480074924822,True
3479,"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern <IF_STMT> key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in ('EPOCH', '{^LN-BEG}EPOCH', '^EPOCH'): template = DateEpoch(lineBeginOnly=key != 'EPOCH') elif key in ('TAI64N', '{^LN-BEG}TAI64N', '^TAI64N'): template = DateTai64n(wordBegin='start' if key != 'TAI64N' else False) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template",if '%' not in pattern:,if pattern.endswith('.'):,0.8208002257582849,0.933847757608669,False
3480,"def _forward_response(self, src, dst): """"""Forward an SCP response between two remote SCP servers"""""" try: exc = (yield from src.await_response()) <IF_STMT> dst.send_error(exc) return exc else: dst.send_ok() return None except OSError as exc: return exc",if exc:,if exc:,0.7982764411161729,0.8901732118131125,True
3481,"def _maybe_signal_recovery_end() -> None: if self.in_recovery and (not self.active_remaining_total()): self.flush_buffers() self._set_recovery_ended() <IF_STMT> self._actives_span.set_tag('Actives-Ready', True) self.signal_recovery_end.set()",if self._actives_span is not None:,if self._actives_span:,0.5664163791558264,0.7447819789879647,False
3482,"def main(): tmpdir = None try: tmpdir = tempfile.mkdtemp() pip_zip = os.path.join(tmpdir, 'pip.zip') with open(pip_zip, 'wb') as fp: fp.write(b85decode(DATA.replace(b'\n', b''))) sys.path.insert(0, pip_zip) bootstrap(tmpdir=tmpdir) finally: <IF_STMT> shutil.rmtree(tmpdir, ignore_errors=True)",if tmpdir:,if tmpdir:,0.9178294796355021,0.8751809448365924,True
3483,"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None if api_version_str == 'latest': self.latest = True else: <IF_STMT> self.preview = True parts = api_version_str.split('-') self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError('The API version {} is not in a supported format'.format(api_version_str))",if 'preview' in api_version_str:,if api_version_str.startswith('preview'):,0.6796672415669407,0.933847757608669,False
3484,"def _merge(self, items, map_id, dep_id, use_disk, meminfo, mem_limit): combined = self.combined merge_combiner = self.aggregator.mergeCombiners for k, v in items: o = combined.get(k) combined[k] = merge_combiner(o, v) if o is not None else v <IF_STMT> mem_limit = self._rotate()",if use_disk and meminfo.rss > mem_limit:,if mem_limit is None:,0.8057035899470489,0.8555308664663046,False
3485,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_value(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 8:,if tt == 8:,0.5745717035656124,0.7378351342269067,True
3486,"def nice(deltat): times = _('second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years').split(':') d = abs(int(deltat)) for div, time in zip((60, 60, 24, 7, 4, 12, 100), times): <IF_STMT> return '%s%i %s' % (deltat < 0 and '-' or '', d, time.split(',')[d != 1]) d /= div",if d < div * 5:,if d != 0:,0.8217799737126738,0.8627586293513119,False
3487,"def after_get_object(self, event, view_kwargs): if event and event.state == 'draft': <IF_STMT> raise ObjectNotFound({'parameter': '{id}'}, 'Event: not found')","if not is_logged_in() or not has_access('is_coorganizer', event_id=event.id):",if not self.get_object(event):,0.555126657865642,0.7331765459202478,False
3488,def daemonize_if_required(self): if self.options.daemon: <IF_STMT> log.shutdown_multiprocessing_logging_listener(daemonizing=True) salt.utils.process.daemonize() self._setup_mp_logging_listener(),if self._setup_mp_logging_listener_ is True:,if self.options.multiprocessing:,0.4110181570108854,0.43167001068522526,False
3489,"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: <IF_STMT> yield module break else: yield module",if module.is_compatible_with(client):,if client.is_active():,0.9305369592772251,0.9253742688467129,False
3490,"def _incremental_avg_dp(self, avg, new_el, idx): for attr in ['coarse_segm', 'fine_segm', 'u', 'v']: setattr(avg, attr, (getattr(avg, attr) * idx + getattr(new_el, attr)) / (idx + 1)) <IF_STMT> setattr(new_el, attr, None) return avg",if idx:,"if getattr(avg, attr) == 0:",0.8672452015638553,0.8036431532733102,False
3491,"def run(self, paths=[]): collapsed = False for item in SideBarSelection(paths).getSelectedDirectories(): for view in item.views(): <IF_STMT> Window().focus_view(view) self.collapse_sidebar_folder() collapsed = True view.close()",if not collapsed:,if collapsed:,0.8811892632889525,0.8232490471721702,False
3492,"def test_reductions(expr, rdd): result = compute(expr, rdd) expected = compute(expr, data) if not result == expected: print(result) print(expected) <IF_STMT> assert abs(result - expected) < 0.001 else: assert result == expected","if isinstance(result, float):",if result < expected:,0.8148432606395082,0.8228500218338367,False
3493,"def deltask(task, d): if task[:3] != 'do_': task = 'do_' + task bbtasks = d.getVar('__BBTASKS', False) or [] if task in bbtasks: bbtasks.remove(task) d.delVarFlag(task, 'task') d.setVar('__BBTASKS', bbtasks) d.delVarFlag(task, 'deps') for bbtask in d.getVar('__BBTASKS', False) or []: deps = d.getVarFlag(bbtask, 'deps', False) or [] <IF_STMT> deps.remove(task) d.setVarFlag(bbtask, 'deps', deps)",if task in deps:,if task in deps:,0.9334261911938471,0.8879659171421962,True
3494,"def _apply_weightnorm(self, list_layers): """"""Try apply weightnorm for all layer in list_layers."""""" for i in range(len(list_layers)): try: layer_name = list_layers[i].name.lower() <IF_STMT> list_layers[i] = WeightNormalization(list_layers[i]) except Exception: pass",if 'conv1d' in layer_name or 'dense' in layer_name:,if layer_name in self.layers:,0.7111624916065514,0.7965020533851944,False
3495,"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == 'Average': self._local_aggregators.append(_AverageAggregator()) elif operator == 'Count': self._local_aggregators.append(_CountAggregator()) <IF_STMT> self._local_aggregators.append(_MaxAggregator()) elif operator == 'Min': self._local_aggregators.append(_MinAggregator()) elif operator == 'Sum': self._local_aggregators.append(_SumAggregator())",elif operator == 'Max':,elif operator == 'Max':,0.9281557927505252,0.8692960007731574,True
3496,"def _conv_layer(self, sess, bottom, name, trainable=True, padding='SAME', relu=True): with tf.variable_scope(name) as scope: filt = self._get_conv_filter(sess, name, trainable=trainable) conv_biases = self._get_bias(sess, name, trainable=trainable) conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding) bias = tf.nn.bias_add(conv, conv_biases) <IF_STMT> bias = tf.nn.relu(bias) return bias",if relu:,if relu:,0.8865102912432437,0.9122561819614461,True
3497,"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} for edge in self.edges: if edge.is_dangling(): raise ValueError('Cannot contract copy tensor with dangling edges') <IF_STMT> continue partner_node, shared_axis = self._get_partner(edge) if partner_node not in partners: partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",if self._is_my_trace(edge):,if edge.is_directed():,0.7792611686738411,0.9076141716697395,False
3498,def close(self): with self._lock: 'Close this _MultiFileWatcher object forever.' <IF_STMT> self._folder_handlers = {} LOGGER.debug('Stopping observer thread even though there is a non-zero number of event observers!') else: LOGGER.debug('Stopping observer thread') self._observer.stop() self._observer.join(timeout=5),if len(self._folder_handlers) != 0:,if len(self._folder_handlers) == 0:,0.5694110055807117,0.8336104423443033,False
3499,"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): <IF_STMT> self.labelList.item(i).setCheckState(2) elif text != self.labelList.item(i).text(): self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",if text == '':,if text == self.labelList.item(i).text():,0.8625232258620392,0.7245511487202049,False
3500,"def _get_messages(self): r = [] try: self._connect() self._login() for message in self._fetch(): <IF_STMT> r.append(message) self._connection.expunge() self._connection.close() self._connection.logout() except MailFetcherError as e: self.log('error', str(e)) return r",if message:,if message.get_message_type() == 'message':,0.8782930646635198,0.7886336751695258,False
3501,def get_current_user(self): try: <IF_STMT> return config.get('json_authentication_override') tkn_header = self.request.headers['authorization'] except KeyError: raise WebAuthNError(reason='Missing Authorization Header') else: tkn_str = tkn_header.split(' ')[-1] try: tkn = self.jwt_validator(tkn_str) except AuthenticationError as e: raise WebAuthNError(reason=e.message) else: return tkn,if config.get('development') and config.get('json_authentication_override'):,if config.get('json_authentication_override'):,0.7150439955955917,0.8901732118131125,False
3502,def _get_data(self): formdata = self._formdata if formdata: data = [] for item in formdata: model = self.loader.get_one(item) if item else None <IF_STMT> data.append(model) else: self._invalid_formdata = True self._set_data(data) return self._data,if model:,if model:,0.7427814899738306,0.8787142254774354,True
3503,"def _getSubstrings(self, va, size, ltyp): subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) if loc and loc[L_LTYPE] == LOC_STRING and (loc[L_VA] > va): subs.add((loc[L_VA], loc[L_SIZE])) <IF_STMT> subs = subs.union(set(loc[L_TINFO])) return list(subs)",if loc[L_TINFO]:,elif loc[L_LTYPE] == LTYPE_TINFO:,0.7306780364387997,0.8692960007731574,False
3504,def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: <IF_STMT> if monad.cls_bl_idname == self.cls_bl_idname: return monad return None,"if hasattr(monad, 'cls_bl_idname'):",if monad.cls_bl_idname:,0.7852094042086942,0.8137489370974955,False
3505,"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: if peer.bad: peer.status = PEER_BAD <IF_STMT> peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",elif peer.last_good > cutoff:,elif time.time() - cutoff > STALE_SECS:,0.8297723640696453,0.7865984197371234,False
3506,"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name <IF_STMT> rval = '{} ({})'.format(rval, composite_file.description) if composite_file.optional: rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if composite_file.description:,if composite_file.description:,0.8175281677332741,0.926934323706186,True
3507,"def testUiViewServerDump_windowIntM1(self): device = None try: device = MockDevice(version=15, startviewserver=True) vc = ViewClient(device, device.serialno, adb=TRUE, autodump=False) vc.dump(window=-1) vc.findViewByIdOrRaise('id/home') finally: <IF_STMT> device.shutdownMockViewServer()",if device:,if device:,0.9162138715322091,0.8641944207171431,True
3508,"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) <IF_STMT> v = self._convertDict(v) if isinstance(k, bytes): k = str(k, 'utf-8') r[k] = v return r","elif isinstance(v, dict):","elif isinstance(v, dict):",0.9116607087048492,0.8953711787948615,True
3509,def _testSendmsgTimeout(self): try: self.cli_sock.settimeout(0.03) try: while True: self.sendmsgToServer([b'a' * 512]) except socket.timeout: pass except OSError as exc: <IF_STMT> raise else: self.fail('socket.timeout not raised') finally: self.misc_event.set(),if exc.errno != errno.ENOMEM:,if exc.errno != errno.EINTR:,0.6181975709246836,0.7886336751695258,False
3510,"def addError(self, test, err): if err[0] is SkipTest: if self.showAll: self.stream.writeln(str(err[1])) <IF_STMT> self.stream.write('s') self.stream.flush() return _org_AddError(self, test, err)",elif self.dots:,if self.showS:,0.8434899649359778,0.7912619863720214,False
3511,"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return <IF_STMT> self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",elif self.scroll_down_rect().collidepoint(p):,if self.scroll_down_rect().collidepoint(p):,0.9338900893049866,0.8827916928185874,False
3512,def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: idx = l.lower().find('copyright') if idx < 0: continue copyright = l[idx + 9:].strip() <IF_STMT> continue copyright = sanitise(copyright) if not copyright.find('200') >= 0 and (not copyright.find('199') >= 0): continue ret.add(copyright) return ret,if not copyright:,if copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright.startswith('#') or copyright,0.8735431636776767,0.6091797432100442,False
3513,"def get_selectable_values(self, request): shop = lfs.core.utils.get_default_shop(request) countries = [] for country in shop.shipping_countries.all(): <IF_STMT> selected = True else: selected = False countries.append({'id': country.id, 'name': country.name, 'selected': selected}) return countries",if country in self.value.all():,if country.is_active:,0.902113565696902,0.8743414417652072,False
3514,"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 if col == nCol: for col in range(0, nCol, 2): if not self.layout.itemAt(row, col): break <IF_STMT> col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)",if col + 2 == nCol:,if col == nCol:,0.9412247163233005,0.8923575006167597,False
3515,def contains_only_whitespace(node): if is_tag(node): if not any([not is_text(s) for s in node.contents]): <IF_STMT> return True return False,if not any([unicode(s).strip() for s in node.contents]):,if not is_whitespace(node.contents):,0.5592567425558674,0.7331765459202478,False
3516,"def tokenize_generator(cw): ret = [] done = {} for op in ops: ch = op.symbol[0] <IF_STMT> continue sops = start_symbols[ch] cw.write(""case '%s':"" % ch) for t in gen_tests(sops, 1): cw.write(t) done[ch] = True return ret",if ch in done:,if ch in done:,0.6497694796031029,0.8474968231198384,True
3517,"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: <IF_STMT> nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)","if isinstance(nbChars, int):",if nbChars < 8:,0.6949691212315297,0.8856327184319047,False
3518,"def init(self, *args, **kwargs): if '_state' not in kwargs: state = {} for arg in ('children', 'windowState', 'detachedPanels'): if arg in kwargs: state[arg] = kwargs[arg] del kwargs[arg] <IF_STMT> kwargs['_state'] = state originalInit(self, *args, **kwargs)",if state:,if state:,0.7943876538006566,0.8935248372106969,True
3519,def spm_decode(tokens: List[str]) -> List[str]: words = [] pieces: List[str] = [] for t in tokens: <IF_STMT> if len(pieces) > 0: words.append(''.join(pieces)) pieces = [t[1:]] else: pieces.append(t) if len(pieces) > 0: words.append(''.join(pieces)) return words,if t[0] == DecodeMixin.spm_bos_token:,if t.startswith('#'):,0.9159006325118496,0.8935248372106969,False
3520,"def _compare_dirs(self, dir1: str, dir2: str) -> List[str]: diff = [] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <IF_STMT> diff.append(file_) return diff",if not os.path.exists(target_path):,if os.path.isdir(target_path):,0.8951982224292941,0.8901732118131125,False
3521,"def credentials(self): """"""The session credentials as a dict"""""" creds = {} if self._creds: <IF_STMT> creds['aws_access_key_id'] = self._creds.access_key if self._creds.secret_key: creds['aws_secret_access_key'] = self._creds.secret_key if self._creds.token: creds['aws_session_token'] = self._creds.token if self._session.region_name: creds['aws_region'] = self._session.region_name if self.requester_pays: creds['aws_request_payer'] = 'requester' return creds",if self._creds.access_key:,if self._creds.access_key:,0.9413201051219688,0.9076141716697395,True
3522,"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: for m in getattr(a, 'modules', []): m = m.strip() for mod in self.modules: if getattr(mod, 'module_type', '').strip() == mod_type.strip(): <IF_STMT> return True return False","if getattr(mod, 'module_name', '').strip() == m:",if m.strip() == mod_type.strip():,0.8905180331365062,0.828399516355805,False
3523,"def find_file_at_path_with_indexes(self, path, url): if url.endswith('/'): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith('/' + self.index_file): if os.path.isfile(path): return self.redirect(url, url[:-len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: <IF_STMT> return self.redirect(url, url + '/') raise MissingFileError(path)","if os.path.isfile(os.path.join(path, self.index_file)):",if os.path.isdir(path):,0.9337720831194279,0.9024521756077707,False
3524,def _use_full_params(self) -> None: for p in self.params: if not p._is_sharded: <IF_STMT> assert p._fp16_shard.storage().size() != 0 p.data = p._fp16_shard else: assert p._full_param_padded.storage().size() != 0 p.data = p._full_param_padded[:p._orig_size.numel()].view(p._orig_size),if self.mixed_precision:,if p._fp16_shard:,0.9048321759242104,0.8645707301556367,False
3525,"def _attrdata(self, cont, name, *val): if not name: return (None, False) if isinstance(name, Mapping): if val: raise TypeError('Cannot set a value to %s' % name) return (name, True) elif val: <IF_STMT> return ({name: val[0]}, True) else: raise TypeError('Too may arguments') else: cont = self._extra.get(cont) return (cont.get(name) if cont else None, False)",if len(val) == 1:,if len(val) == 1:,0.857967458823842,0.8944264839442453,True
3526,"def evaluate(env, net, device='cpu'): obs = env.reset() reward = 0.0 steps = 0 while True: obs_v = ptan.agent.default_states_preprocessor([obs]).to(device) action_v = net(obs_v) action = action_v.data.cpu().numpy()[0] obs, r, done, _ = env.step(action) reward += r steps += 1 <IF_STMT> break return (reward, steps)",if done:,if done:,0.7895855138696076,0.9122561819614461,True
3527,"def convert_html_js_files(app: Sphinx, config: Config) -> None: """"""This converts string styled html_js_files to tuple styled one."""""" html_js_files = [] for entry in config.html_js_files: <IF_STMT> html_js_files.append((entry, {})) else: try: filename, attrs = entry html_js_files.append((filename, attrs)) except Exception: logger.warning(__('invalid js_file: %r, ignored'), entry) continue config.html_js_files = html_js_files","if isinstance(entry, str):","if isinstance(entry, (list, tuple)):",0.8258099193090594,0.8806615362338783,False
3528,"def _check_duplications(self, regs): """"""n^2 loop which verifies that each reg exists only once."""""" for reg in regs: count = 0 for r in regs: <IF_STMT> count += 1 if count > 1: genutil.die('reg %s defined more than once' % reg)",if reg == r:,if reg == r:,0.8065592656102097,0.8661072626070159,True
3529,"def PyJsHoisted_vault_(key, forget, this, arguments, var=var): var = Scope({u'this': this, u'forget': forget, u'key': key, u'arguments': arguments}, var) var.registers([u'forget', u'key']) if PyJsStrictEq(var.get(u'key'), var.get(u'passkey')): return var.put(u'secret', var.get(u'null')) <IF_STMT> else var.get(u'secret') or var.put(u'secret', var.get(u'secretCreatorFn')(var.get(u'object')))",if var.get(u'forget'),if var.get(u'null'),0.9258202927109983,0.8827916928185874,False
3530,"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) <IF_STMT> d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v, dict):","elif isinstance(v, dict):",0.7550560029497245,0.8592899528284996,False
3531,"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): if value & 128: <IF_STMT> out.append('(unknown)') else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate('Transceiver compliance', ', '.join(out))",if len(TRANSCEIVER[t]) < b + 1:,if t == 0:,0.9158315717213722,0.8555308664663046,False
3532,"def process_string(self, remove_repetitions, sequence): string = '' for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: <IF_STMT> pass elif char == self.labels[self.space_index]: string += ' ' else: string = string + char return string",if remove_repetitions and i != 0 and (char == sequence[i - 1]):,if remove_repetitions[i]:,0.9095557080163905,0.8935248372106969,False
3533,"def clean(self): username = self.cleaned_data.get('username') password = self.cleaned_data.get('password') if username and password: self.user_cache = authenticate(username=username, password=password) <IF_STMT> raise forms.ValidationError(self.error_messages['invalid_login']) elif not self.user_cache.is_active: raise forms.ValidationError(self.error_messages['inactive']) self.check_for_test_cookie() return self.cleaned_data",if self.user_cache is None:,if not self.user_cache:,0.7798774614204167,0.833078701050083,False
3534,"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get('events', []))): if e.get('event') == UserUttered.type_name: return False <IF_STMT> return e.get('name') == ACTION_LISTEN_NAME return False",elif e.get('event') == ActionExecuted.type_name:,if i == 0:,0.9205796841339896,0.8752376177722327,False
3535,"def getReferences(view, name=''): """"""Find all reference definitions."""""" refs = [] name = re.escape(name) if name == '': refs.extend(view.find_all('(?<=^\\[)([^\\]]+)(?=\\]:)', 0)) else: refs.extend(view.find_all('(?<=^\\[)(%s)(?=\\]:)' % name, 0)) regions = refs ids = {} for reg in regions: name = view.substr(reg).strip() key = name.lower() <IF_STMT> ids[key].regions.append(reg) else: ids[key] = Obj(regions=[reg], label=name) return ids",if key in ids:,if key in ids:,0.8467075547527121,0.8902056737869248,True
3536,"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: <IF_STMT> if self.revs_enabled: self.assertTrue(url.endswith('/latest'), msg=url) else: self.assertTrue(url.endswith('/download_urls'), msg=url) return headers.get(header_name)",if header_name in headers:,if url.startswith('/download_urls'):,0.9220394338798379,0.8966773400768917,False
3537,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.set_shuffle_name(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.5745717035656124,0.7378351342269067,True
3538,"def make_release_tree(self, base_dir, files): """"""Make the release tree."""""" self.mkpath(base_dir) create_tree(base_dir, files, dry_run=self.dry_run) if not files: self.log.warning('no files to distribute -- empty manifest?') else: self.log.info('copying files to %s...', base_dir) for filename in files: <IF_STMT> self.log.warning(""'%s' not a regular file -- skipping"", filename) else: dest = os.path.join(base_dir, filename) self.copy_file(filename, dest) self.distribution.metadata.write_pkg_info(base_dir)",if not os.path.isfile(filename):,if filename.startswith('.py') or filename.endswith('.py'):,0.8382984745259214,0.8902056737869248,False
3539,"def _parse_names_set(feature_names): """"""Helping function of `_parse_feature_names` that parses a set of feature names."""""" feature_collection = OrderedDict() for feature_name in feature_names: <IF_STMT> feature_collection[feature_name] = ... else: raise ValueError('Failed to parse {}, expected string'.format(feature_name)) return feature_collection","if isinstance(feature_name, str):","if isinstance(feature_name, str):",0.8700275828880543,0.8675979125638379,True
3540,"def get_connection(self, url, proxies=None): with self.pools.lock: pool = self.pools.get(url) <IF_STMT> return pool pool = NpipeHTTPConnectionPool(self.npipe_path, self.timeout, maxsize=self.max_pool_size) self.pools[url] = pool return pool",if pool:,if pool:,0.6391777887675902,0.8318180062062374,True
3541,"def _parse_dimensions(dimensions): arrays = [] names = [] for key in dimensions: values = [v['name'] for v in key['values']] role = key.get('role', None) <IF_STMT> values = [_fix_quarter_values(v) for v in values] values = pd.DatetimeIndex(values) arrays.append(values) names.append(key['name']) midx = pd.MultiIndex.from_product(arrays, names=names) if len(arrays) == 1 and isinstance(midx, pd.MultiIndex): midx = midx.levels[0] return midx","if role in ('time', 'TIME_PERIOD'):",if role is not None:,0.7641436924966106,0.879962308706789,False
3542,"def _add_trials(self, name, spec): """"""Add trial by invoking TrialRunner."""""" resource = {} resource['trials'] = [] trial_generator = BasicVariantGenerator() trial_generator.add_configurations({name: spec}) while not trial_generator.is_finished(): trial = trial_generator.next_trial() <IF_STMT> break runner.add_trial(trial) resource['trials'].append(self._trial_info(trial)) return resource",if not trial:,if not trial:,0.8116857112283021,0.8592899528284996,True
3543,"def _retrieve_key(self): url = 'http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf' text = '' try: r = requests.get(url, timeout=self.timeout, proxies=self.proxies) text = r.text except: self.error = 'ERROR - URL Connection' if text: expression = ""'(....-....-....-....)';"" pattern = re.compile(expression) match = pattern.search(text) <IF_STMT> self.key = match.group(1) return self.key else: self.error = 'ERROR - No API Key'",if match:,if match:,0.6617723206535058,0.926934323706186,True
3544,"def test_net(net, env, count=10, device='cpu'): rewards = 0.0 steps = 0 for _ in range(count): obs = env.reset() while True: obs_v = ptan.agent.float32_preprocessor([obs]).to(device) mu_v = net(obs_v)[0] action = mu_v.squeeze(dim=0).data.cpu().numpy() action = np.clip(action, -1, 1) obs, reward, done, _ = env.step(action) rewards += reward steps += 1 <IF_STMT> break return (rewards / count, steps / count)",if done:,if done:,0.9284443189238657,0.9350761925543661,True
3545,"def compile(self, filename, obfuscate=False, raw=False, magic='\x00' * 8): body = marshal.dumps(compile(self.visit(self._source_ast), filename, 'exec')) if obfuscate: body_len = len(body) offset = 0 if raw else 8 output = bytearray(body_len + 8) for i, x in enumerate(body): output[i + offset] = ord(x) ^ 2 ** ((65535 - i) % 65535) % 251 <IF_STMT> for i in xrange(8): output[i] = 0 return output elif raw: return body else: return magic + body",if raw:,if i + offset < body_len:,0.7501092241199416,0.8961274312876005,False
3546,"def _map_saslprep(s): """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space"""""" r = [] for c in s: if stringprep.in_table_c12(c): r.append(' ') <IF_STMT> r.append(c) return ''.join(r)",elif not stringprep.in_table_b1(c):,elif stringprep.in_table_b1(c) and stringprep.in_table_c2(c):,0.8819376138219006,0.8105932471967202,False
3547,"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[w, pos] <IF_STMT> lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas",elif w in self.word_dict:,"elif (w, pos) in self.word_dict:",0.7873639587782367,0.88627064388393,False
3548,"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: <IF_STMT> value = [extract_pyreal(item) for item in value.leaves] if any((item is None for item in value)): return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value","if value.has_form('List', None):","if isinstance(value, MultiValue):",0.9381283724708043,0.9224532597476077,False
3549,"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) <IF_STMT> logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() if src.exists(): shutil.copy(src, dest)",if dest.exists():,if os.path.exists(dest):,0.945830607615789,0.9122561819614461,False
3550,"def assert_readback(vehicle, values): i = 10 while i > 0: time.sleep(0.1) i -= 0.1 for k, v in values.items(): <IF_STMT> continue break if i <= 0: raise Exception('Did not match in channels readback %s' % values)",if vehicle.channels[k] != v:,if v == vehicle:,0.7008479688177733,0.8516228624291206,False
3551,"def _get_linode_client(self): api_key = self.credentials.conf('key') api_version = self.credentials.conf('version') if api_version == '': api_version = None if not api_version: api_version = 3 regex_v4 = re.compile('^[0-9a-f]{64}$') regex_match = regex_v4.match(api_key) <IF_STMT> api_version = 4 else: api_version = int(api_version) return _LinodeLexiconClient(api_key, api_version)",if regex_match:,if regex_match:,0.746259074751055,0.9051034981560222,True
3552,"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: <IF_STMT> self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: if self.lasttime is None or x_stats.lasttime >= self.lasttime: self.lasttime = x_stats.lasttime self.last = x_stats.last",if self.firsttime is None or x_stats.firsttime < self.firsttime:,if self.firsttime is None or x_stats.firsttime >= self.firsttime:,0.8990107259893203,0.8070218370798333,False
3553,"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] <IF_STMT> raise ValueError('Not all values of X are of equal length.') x_len = lengths[0] else: x_len = len(X) if y is not None: if len(y) != x_len: raise ValueError('X and y are not of equal length.') if self.regression and y is not None and (y.ndim == 1): y = y.reshape(-1, 1) return (X, y)",if len(set(lengths)) > 1:,if len(lengths) != 1:,0.8576497970797043,0.9204199807826591,False
3554,"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" 'the ignore method' ignore = getattr(self, 'ignore') for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) if k in self.combinations: for k1 in self.combinations[k]: <IF_STMT> setattr(self, k1, ignore)","if not hasattr(self, k1):","if getattr(self, k1) is None:",0.9182759990317115,0.8492988135354755,False
3555,"def _parse_list(self, tokens): assert tokens[0] in ('[', '(') delim = ']' if tokens.pop(0) == '[' else ')' expr = ExpressionList() while tokens and tokens[0] != delim: item = self._parse(tokens) <IF_STMT> if tokens.pop(0) != ',': raise ExpressionSyntaxError('Expected: "",""') expr.append(item) if not tokens or tokens[0] != delim: raise ExpressionSyntaxError('Missing: ""%s""' % delim) else: tokens.pop(0) return expr",if tokens and tokens[0] != delim:,if item:,0.7196311138879793,0.933847757608669,False
3556,def param_value(self): for token in self: if token.token_type == 'value': return token.stripped_value <IF_STMT> for token in token: if token.token_type == 'bare-quoted-string': for token in token: if token.token_type == 'value': return token.stripped_value return '',if token.token_type == 'quoted-string':,if token.token_type == 'bare-quoted-string':,0.6291103649180041,0.8385130047130208,False
3557,"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): <IF_STMT> if line[len(comment):].lstrip().startswith(comment): continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",if line.startswith(comment):,if comment:,0.782962739445593,0.8866029039778043,False
3558,"def lots_connected_to_existing_roads(model): set = [] for h in model.HarvestCells: for i, j in model.ExistingRoads: <IF_STMT> if h not in set: set.append(h) return set",if i in model.COriginNodeForCell[h] or j in model.COriginNodeForCell[h]:,if i == j:,0.8608878866772287,0.7709002428237395,False
3559,"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('\\Abarra_counter_session=', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None retval |= re.search('(\\A|\\b)barracuda_', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None <IF_STMT> break return retval",if retval:,if retval:,0.7972888692810896,0.8996480074924822,True
3560,"def test_files(self): dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): <IF_STMT> names.append(os.path.join(test_dir, n)) for filename in names: if test_support.verbose: print('Testing %s' % filename) source = read_pyfile(filename) self.check_roundtrip(source)",if n.endswith('.py') and (not n.startswith('bad')):,if n.endswith('.py'):,0.8713029130117534,0.9076141716697395,False
3561,"def test_calibrate_target(create_target): mod, params = testing.synthetic.get_workload() dataset = get_calibration_dataset(mod, 'data') with relay.quantize.qconfig(calibrate_mode='kl_divergence'): <IF_STMT> with tvm.target.Target('llvm'): relay.quantize.quantize(mod, params, dataset) else: relay.quantize.quantize(mod, params, dataset)",if create_target:,if create_target:,0.8741089057471803,0.8318180062062374,True
3562,"def _cleanSubmodule(self, _=None): rc = RC_SUCCESS if self.submodules: command = ['submodule', 'foreach', '--recursive', 'git', 'clean', '-f', '-f', '-d'] <IF_STMT> command.append('-x') rc = (yield self._dovccmd(command)) defer.returnValue(rc)",if self.mode == 'full' and self.method == 'fresh':,if self.x:,0.6445596371560675,0.8531413606256201,False
3563,"def screen_length_to_bytes_count(string, screen_length_limit, encoding): bytes_count = 0 screen_length = 0 for unicode_char in string: screen_length += screen_len(unicode_char) char_bytes_count = len(unicode_char.encode(encoding)) bytes_count += char_bytes_count <IF_STMT> bytes_count -= char_bytes_count break return bytes_count",if screen_length > screen_length_limit:,if screen_length > screen_length_limit:,0.7342571800534275,0.8228500218338367,True
3564,"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) if example.valid: correct += 1 else: self.fail('Invalid schema was parsed: ' + example.schema_string) except: <IF_STMT> correct += 1 else: self.fail('Valid schema failed to parse: ' + example.schema_string) fail_msg = 'Parse behavior correct on %d out of %d schemas.' % (correct, len(EXAMPLES)) self.assertEqual(correct, len(EXAMPLES), fail_msg)",if not example.valid:,if example.valid:,0.911450599375915,0.9350761925543661,False
3565,"def _on_change(self): changed = False self.save() for key, value in self.data.items(): <IF_STMT> if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed","if isinstance(value, bool):","if isinstance(key, str):",0.7757310665073052,0.9022045190074797,False
3566,"def normalize(d: Dict[Any, Any]) -> Dict[str, Any]: first_exception = None for normalizer in normalizers: try: normalized = normalizer(d) except KeyError as e: <IF_STMT> first_exception = e else: return normalized assert first_exception is not None raise first_exception",if not first_exception:,if first_exception is None:,0.8145133522460184,0.8516228624291206,False
3567,"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: if not isinstance(root_objs, Iterable): root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: <IF_STMT> args = self.get_callback_args(root_obj, callback) if args: yield args",if root_obj:,if root_obj:,0.9471606857789567,0.9220450449751959,True
3568,"def test_opdm_to_oqdm(self): for file in filter(lambda x: x.endswith('.hdf5'), os.listdir(DATA_DIRECTORY)): molecule = MolecularData(filename=os.path.join(DATA_DIRECTORY, file)) <IF_STMT> test_oqdm = map_one_pdm_to_one_hole_dm(molecule.fci_one_rdm) true_oqdm = numpy.eye(molecule.n_qubits) - molecule.fci_one_rdm assert numpy.allclose(test_oqdm, true_oqdm)",if molecule.fci_one_rdm is not None:,if molecule.fci_one_rdm is not None:,0.6788922389346044,0.759907656827929,True
3569,"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: if self.checkForStop(): return None subDomain = subDomainElem.get('subdomain', '').strip() <IF_STMT> self.emitHostname(subDomain, event)",if subDomain:,if subDomain:,0.7295413442963237,0.8375717919273554,True
3570,"def download_cve(download_path: str, years: Optional[List[int]]=None, update: bool=False): if update: process_url(CVE_URL.format('modified'), download_path) else: all_cve_urls = get_cve_links(CVE_URL, years) <IF_STMT> raise CveLookupException('Error: No CVE links found') for url in all_cve_urls: process_url(url, download_path)",if not all_cve_urls:,if not all_cve_urls:,0.7349613964765106,0.8446593249975184,True
3571,"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" assert directive and directive[0] == '@' skip_flag = directive in ('@others', '@all') while i < len(s): if match_word(s, i, directive): return (True, i) else: i = skip_line(s, i) <IF_STMT> i = skip_ws(s, i) return (False, -1)",if skip_flag:,if skip_flag:,0.866009893103982,0.9284304001296656,True
3572,"def run_async(self, nuke_cursors): interface_type = self.view.settings().get('git_savvy.interface') for cls in subclasses: if cls.interface_type == interface_type: vid = self.view.id() interface = interfaces.get(vid, None) <IF_STMT> interface = interfaces[vid] = cls(view=self.view) interface.render(nuke_cursors=nuke_cursors) break",if not interface:,if interface is None:,0.8065999056812353,0.8169276475307028,False
3573,"def scan_resource_conf(self, conf): if 'properties' in conf: <IF_STMT> if str(conf['properties']['sslEnforcement']).lower() == 'enabled': return CheckResult.PASSED return CheckResult.FAILED",if 'sslEnforcement' in conf['properties']:,if 'sslEnforcement' in conf['properties']:,0.5639775938847571,0.674945488826271,True
3574,"def do_shorts(opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != '': opt, optstring = (optstring[0], optstring[1:]) if short_has_arg(opt, shortopts): if optstring == '': <IF_STMT> raise GetoptError('option -%s requires argument' % opt, opt) optstring, args = (args[0], args[1:]) optarg, optstring = (optstring, '') else: optarg = '' opts.append(('-' + opt, optarg)) return (opts, args)",if not args:,if not args:,0.9388996292046147,0.925039248827207,True
3575,def release(self): tid = _thread.get_ident() with self.lock: <IF_STMT> raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release(),if self.owner != tid:,if tid == self.owner:,0.9088142306663229,0.8385130047130208,False
3576,"def _summarize_kraken(fn): """"""get the value at species level"""""" kraken = {} list_sp, list_value = ([], []) with open(fn) as handle: for line in handle: cols = line.strip().split('\t') sp = cols[5].strip() <IF_STMT> list_sp.append(sp) list_value.append(cols[0]) kraken = {'kraken_sp': list_sp, 'kraken_value': list_value} return kraken",if len(sp.split(' ')) > 1 and (not sp.startswith('cellular')):,if sp:,0.8493076430071134,0.9122561819614461,False
3577,"def _sync_remote_run(remote_run): assert remote_run.remote remote_name = remote_run.remote.name pull_args = click_util.Args(remote=remote_name, delete=False) try: remote_impl_support.pull_runs([remote_run], pull_args) except Exception as e: <IF_STMT> log.exception('pull %s from %s', remote_run.id, remote_name) else: log.error('error pulling %s from %s: %s', remote_run.id, remote_name, e)",if log.getEffectiveLevel() <= logging.DEBUG:,if log.getEffectiveLevel() > logging.DEBUG:,0.8724072506655277,0.8474968231198384,False
3578,"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) <IF_STMT> subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if sign is None:,if sign is None:,0.8152378494022676,0.9001816649635144,True
3579,"def import_til(self): log('Importing type libraries...') cur = self.db_cursor() sql = ""select name from diff.program_data where type = 'til'"" cur.execute(sql) for row in cur.fetchall(): til = row['name'] <IF_STMT> til = til.decode('utf-8') try: add_default_til(til) except: log('Error loading til %s: %s' % (row['name'], str(sys.exc_info()[1]))) cur.close() auto_wait()",if type(til) is bytes:,"if isinstance(til, bytes):",0.8529559148977575,0.8953711787948615,False
3580,"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess('branch').stdout: <IF_STMT> git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() if BRANCH_ALIAS_MARKER in git_branch_line: alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",if git_branch_line.startswith('*'):,if git_branch_line.startswith('branch:'):,0.8236690189530218,0.8866029039778043,False
3581,"def add_include_dirs(self, args): ids = [] for a in args: <IF_STMT> a = a.includedirs if not isinstance(a, IncludeDirs): raise InvalidArguments('Include directory to be added is not an include directory object.') ids.append(a) self.include_dirs += ids","if hasattr(a, 'includedirs'):","if isinstance(a, File):",0.9017068426086692,0.8675979125638379,False
3582,"def _serialize_feature(self, feature): name = feature.unique_name() <IF_STMT> self._features_dict[feature.unique_name()] = feature.to_dictionary() for dependency in feature.get_dependencies(deep=True): name = dependency.unique_name() if name not in self._features_dict: self._features_dict[name] = dependency.to_dictionary()",if name not in self._features_dict:,if name not in self._features_dict:,0.7289606319102432,0.759907656827929,True
3583,"def generate_io(chart_type, race_configs, environment): structures = [] for race_config in race_configs: <IF_STMT> title = chart_type.format_title(environment, race_config.track, es_license=race_config.es_license, suffix='%s-io' % race_config.label) structures.append(chart_type.io(title, environment, race_config)) return structures",if 'io' in race_config.charts:,if not race_config.is_empty():,0.8563224391164248,0.8196189957582152,False
3584,"def format_partition(partition, partition_schema): tokens = [] if isinstance(partition, dict): for name in partition_schema: <IF_STMT> tok = _format_partition_kv(name, partition[name], partition_schema[name]) else: tok = name tokens.append(tok) else: for name, value in zip(partition_schema, partition): tok = _format_partition_kv(name, value, partition_schema[name]) tokens.append(tok) return 'PARTITION ({})'.format(', '.join(tokens))",if name in partition:,"if isinstance(partition[name], dict):",0.6837022578111779,0.8902579342581529,False
3585,"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, 'condition', Undefined) copy = self if condition is not Undefined: if isinstance(condition, core.SchemaBase): pass <IF_STMT> kwds = parse_shorthand(condition['field'], context.get('data', None)) copy = self.copy(deep=['condition']) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict(validate=validate, ignore=ignore, context=context)",elif 'field' in condition and 'type' not in condition:,"elif isinstance(condition, dict):",0.7856772477439193,0.8928756684056034,False
3586,"def _checkForCommand(self): prompt = b'cftp> ' if self._expectingCommand and self._lineBuffer == prompt: buf = b'\n'.join(self._linesReceived) <IF_STMT> buf = buf[len(prompt):] self.clearBuffer() d, self._expectingCommand = (self._expectingCommand, None) d.callback(buf)",if buf.startswith(prompt):,if buf.startswith(prompt):,0.8432274126551258,0.8590888738245122,True
3587,"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger('fate_flow_schedule') else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + 'schedule' <IF_STMT> return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",if key in LoggerFactory.schedule_logger_dict:,if key in LoggerFactory.schedule_logger_dict.keys():,0.8665948388841109,0.8592377270804451,False
3588,"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, 'i', 'ii', 'iii', 'iv', 'v', 'a', 'b', 'c', 'd', 'e']: for wrong in ['cd', 'part', 'dis', 'disc', 'dvd']: if '%s%s' % (wrong, nr) in nzb_name.lower(): wrong_found += 1 <IF_STMT> return -30 return 0 except: log.error('Failed doing halfMultipartScore: %s', traceback.format_exc()) return 0",if wrong_found == 1:,if wrong_found == 3:,0.8199345068728615,0.9019629427251674,False
3589,"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += ',' args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group('stringval') if value is None: value = item.group('value') value = _pythonize(value) <IF_STMT> args.append(value) else: name = item.group('name') kwargs[name] = value return (tuple(args), kwargs)",if not item.group('name'):,if item.group('name') is None:,0.8438085809775837,0.8806615362338783,False
3590,"def leaves(self, unique=True): """"""Get the leaves of the tree starting at this root."""""" if not self.children: return [self] else: res = list() for child in self.children: for sub_child in child.leaves(unique=unique): <IF_STMT> res.append(sub_child) return res",if not unique or sub_child not in res:,if sub_child not in res:,0.8924075184712311,0.8200123297196334,False
3591,"def to_tree(self, tagname=None, idx=None, namespace=None): axIds = set((ax.axId for ax in self._axes)) for chart in self._charts: for id, axis in chart._axes.items(): <IF_STMT> setattr(self, axis.tagname, axis) axIds.add(id) return super(PlotArea, self).to_tree(tagname)",if id not in axIds:,if id not in axIds:,0.8845243089974038,0.7909601595885504,True
3592,"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) <IF_STMT> rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.CONNECT_MODE:,if k == neighbors.CONNECT_MODE:,0.9048446672167588,0.8105932471967202,True
3593,"def close_all_connections(): global _managers, _lock, _in_use, _timer _lock.acquire() try: <IF_STMT> _timer.cancel() _timer = None for domain, managers in _managers.items(): for manager in managers: manager.close() _managers = {} finally: _lock.release()",if _timer:,if _timer:,0.9104050912676562,0.8743414417652072,True
3594,"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): <IF_STMT> value[i] = self._instrument(item) return model","if isinstance(item, tf.keras.layers.Layer):","if isinstance(item, tf.keras.layers.Layer):",0.9158262108887734,0.8749766281017177,True
3595,"def target_glob(tgt, hosts): ret = {} for host in hosts: if fnmatch.fnmatch(tgt, host): ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {})) ret[host].update({'host': host}) <IF_STMT> ret[host].update({'user': __opts__['ssh_user']}) return ret",if __opts__.get('ssh_user'):,elif __opts__.get('ssh_user'):,0.6976106445082021,0.8466657105524215,False
3596,"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + ': ') <IF_STMT> sys.stderr.write(data.decode('utf8')) else: sys.stderr.write(data) if data[-1] == '\n': self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",if bytes:,"elif isinstance(data, bytes):",0.9132080137233763,0.8390782502060267,False
3597,"def task_thread(): while not task_queue.empty(): host, port, username, password = task_queue.get() logger.info('try burst {}:{} use username:{} password:{}'.format(host, port, username, password)) <IF_STMT> with task_queue.mutex: task_queue.queue.clear() result_queue.put((username, password))","if telnet_login(host, port, username, password):",if not task_queue.empty():,0.6169534972533492,0.8266114125804572,False
3598,"def _format_results(name, ppl, scores, metrics): """"""Format results."""""" result_str = '' if ppl: result_str = '%s ppl %.2f' % (name, ppl) if scores: for metric in metrics: <IF_STMT> result_str += ', %s %s %.1f' % (name, metric, scores[metric]) else: result_str = '%s %s %.1f' % (name, metric, scores[metric]) return result_str",if result_str:,if metric in scores:,0.6852648487949272,0.8902056737869248,False
3599,"def info_query(self, query): """"""Send a query which only returns 1 row"""""" self._cmysql.query(query) first_row = () if self._cmysql.have_result_set: first_row = self._cmysql.fetch_row() <IF_STMT> self._cmysql.free_result() raise errors.InterfaceError('Query should not return more than 1 row') self._cmysql.free_result() return first_row",if self._cmysql.fetch_row():,if first_row > 1:,0.7315371644266785,0.8431339019329497,False
3600,"def reset_class(self): for f in self.fields_order: <IF_STMT> f.value = int(f.strbits, 2) elif 'default_val' in f.kargs: f.value = int(f.kargs['default_val'], 2) else: f.value = None if f.fname: setattr(self, f.fname, f)",if f.strbits and isbin(f.strbits):,if f.strbits:,0.840929008136211,0.8696398662122882,False
3601,"def _walk_map_list(self, access_func): seen = [] cur = self while cur: <IF_STMT> break yield cur seen.append(cur.obj_offset) if len(seen) > 1024: break cur = access_func(cur)",if cur.obj_offset in seen:,if cur.obj_offset in seen:,0.8711733897116825,0.7801270245332924,True
3602,def bgdel(): q = bgdelq while True: name = q.get() while os.path.exists(name): try: <IF_STMT> os.remove(name) else: shutil.rmtree(name) except: pass if os.path.exists(name): time.sleep(0.1),if os.path.isfile(name):,if os.path.isfile(name):,0.8864925863748809,0.8318180062062374,True
3603,"def _find_all_variables(transfer_variable): d = {} for _k, _v in transfer_variable.__dict__.items(): if isinstance(_v, Variable): d[_v._name] = _v <IF_STMT> d.update(_find_all_variables(_v)) return d","elif isinstance(_v, BaseTransferVariables):","elif isinstance(_v, list):",0.6787979787227583,0.7739321540474097,False
3604,"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] <IF_STMT> break if idx == len(os_widget.get_model()) - 1: idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",if value and row[0] == value:,if row[0] == 'val':,0.9209575040132084,0.8661072626070159,False
3605,"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = '%d/%ds' % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: if methods == ALL: methods = '' <IF_STMT> methods = ''.join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, 'RATELIMIT_CACHE_PREFIX', 'rl:') return prefix + hashlib.md5(u''.join(parts).encode('utf-8')).hexdigest()","elif isinstance(methods, (list, tuple)):",elif methods:,0.7257525670402303,0.9312457603037672,False
3606,"def findfiles(path): files = [] for name in os.listdir(path): <IF_STMT> continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode if stat.S_ISDIR(mode): files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",if name.startswith('.') or name == 'lastsnap.jpg':,if name.startswith('.'):,0.9105691807197928,0.8827916928185874,False
3607,"def __getitem__(self, key): if isinstance(key, str_types): keys = self.get_keys() <IF_STMT> raise KeyError(' ""{0}"" is an invalid key'.format(key)) else: return self[keys.index(key)] else: return list.__getitem__(self, key)",if key not in keys:,if key not in keys:,0.6036972569388039,0.7506346798217074,True
3608,"def test_assert_set_equal(estimate: tp.Iterable[int], message: str) -> None: reference = {1, 2, 3} try: testing.assert_set_equal(estimate, reference) except AssertionError as error: if not message: raise AssertionError('An error has been raised while it should not.') from error np.testing.assert_equal(error.args[0].split('\n')[1:], message) else: <IF_STMT> raise AssertionError('An error should have been raised.')",if message:,"if not np.testing.assert_equal(estimate, reference):",0.9218026815900751,0.8806615362338783,False
3609,"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: <IF_STMT> subp = os.path.join(pth, p) p = os.path.join(prefix, p) if recursive and os.path.isdir(subp): res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",if p[0] != '.':,if os.path.isdir(p):,0.8490234388625878,0.9024521756077707,False
3610,"def check(self, runner, script, info): if isinstance(info, ast.FunctionDef): for arg in info.args.args: <IF_STMT> if arg.id in script.modelVars: self.problem('Function {0} may shadow model variable {1}'.format(info.name, arg.id), lineno=info.lineno)","if isinstance(arg, ast.Name):","if isinstance(arg, ast.VariableDef):",0.6428131331956802,0.8266114125804572,False
3611,"def db_lookup(field, key, publish_year=None): sql = 'select sum(ebook_count) as num from subjects where field=$field and key=$key' if publish_year: <IF_STMT> sql += ' and publish_year between $y1 and $y2' y1, y2 = publish_year else: sql += ' and publish_year=$publish_year' return list(ebook_count_db.query(sql, vars=locals()))[0].num","if isinstance(publish_year, (tuple, list)):",if publish_year < 0:,0.6327223004494269,0.8692960007731574,False
3612,"def put(self, session): with sess_lock: self.parent.put(session) for sp in self.skip_paths: <IF_STMT> return if session.sid in self._cache: try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",if request.path.startswith(sp):,if sp.sid == session.sid:,0.7429348996837192,0.7965020533851944,False
3613,"def summarize(self): if self.bad_commit and self.good_commit: for subresult in self.subresults.values(): sub = subresult.summarize() <IF_STMT> return sub return 'Detected bad commit in {} repository:\n{} {}'.format(self.repo_name, self.bad_commit, get_message(self.suite, self.bad_commit)) return ''",if sub:,if sub:,0.7657564878888178,0.8743414417652072,True
3614,def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: if p.len == 0: nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 <IF_STMT> break num_nullable = len(nullable) return nullable,if len(nullable) == num_nullable:,if num_nullable > p.len:,0.851455752510058,0.8752376177722327,False
3615,"def _cast_float64_to_float32(self, feeds): for input_name, input_type in self.inputs: <IF_STMT> feed = feeds.get(input_name) if feed is not None and feed.dtype == np.float64: feeds[input_name] = feed.astype(np.float32) return feeds",if input_type == 'tensor(float)':,if input_type == 'float64':,0.8216960889814352,0.7965020533851944,False
3616,"def proc_minute(d): if expanded[0][0] != '*': diff_min = nearest_diff_method(d.minute, expanded[0], 60) <IF_STMT> if is_prev: d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return (True, d) return (False, d)",if diff_min is not None and diff_min != 0:,if diff_min is not None:,0.6155842241718877,0.7909601595885504,False
3617,"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) if detyper is None: continue deval = detyper(val) <IF_STMT> continue ctx[key] = deval self._detyped = ctx return ctx",if deval is None:,if deval is None:,0.9271925136017602,0.8806615362338783,True
3618,"def get_or_create_user(request, user_data): try: user = User.objects.get(sso_id=user_data['id']) <IF_STMT> update_user(user, user_data) return user except User.DoesNotExist: user = User.objects.create_user(user_data['username'], user_data['email'], is_active=user_data.get('is_active', True), sso_id=user_data['id']) user.update_acl_key() setup_new_user(request.settings, user) return user","if user_needs_updating(user, user_data):",if user.is_active:,0.740101352592853,0.8590888738245122,False
3619,"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) <IF_STMT> self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)","elif isinstance(v, bool):","elif isinstance(v, bool):",0.9283709130001714,0.9118021019905903,True
3620,def load(cls): if not cls._loaded: cls.log.debug('Loading action_sets...') <IF_STMT> cls._find_action_sets(PATHS.ACTION_SETS_DIRECTORY) else: cls.action_sets = JsonDecoder.load(PATHS.ACTION_SETS_JSON_FILE) cls.log.debug('Done!') cls._loaded = True,if not horizons.globals.fife.use_atlases:,if os.path.isdir(PATHS.ACTION_SETS_DIRECTORY):,0.8134825196735974,0.7778111223054219,False
3621,"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) <IF_STMT> value = setting.Value.split('|') value.remove(edge) setting.Value = '|'.join(value) if updater: updater.UpdateSetting(setting) if answer == Gtk.ResponseType.NO: return False return True",if answer == Gtk.ResponseType.YES:,if answer == Gtk.ResponseType.YES:,0.9075816695222861,0.8385130047130208,True
3622,"def read_tsv(input_file, quotechar=None): """"""Reads a tab separated value file."""""" with open(input_file, 'r', encoding='utf-8-sig') as f: reader = csv.reader(f, delimiter='\t', quotechar=quotechar) lines = [] for line in reader: <IF_STMT> line = list((str(cell, 'utf-8') for cell in line)) lines.append(line) return lines",if sys.version_info[0] == 2:,"if isinstance(line, list):",0.9177647756678056,0.884617925078158,False
3623,"def devd_devfs_hook(middleware, data): if data.get('subsystem') != 'CDEV': return if data['type'] == 'CREATE': disks = await middleware.run_in_thread(lambda: sysctl.filter('kern.disks')[0].value.split()) if data['cdev'] not in disks: return await added_disk(middleware, data['cdev']) elif data['type'] == 'DESTROY': <IF_STMT> return await remove_disk(middleware, data['cdev'])",if not RE_ISDISK.match(data['cdev']):,if data['cdev'] in disks:,0.890417201172779,0.8474968231198384,False
3624,"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: <IF_STMT> if self.watchdirs[watchdir_id]['enabled']: client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",if col and col.get_title() == _('Active'):,if self.watchdirs[watchdir_id]['enabled']:,0.7764169762058286,0.8466657105524215,False
3625,"def _execute(self, options, args): if len(args) < 1: raise CommandError(_('Not enough arguments')) paths = args songs = [self.load_song(p) for p in paths] for song in songs: <IF_STMT> raise CommandError(_('Image editing not supported for %(file_name)s (%(file_format)s)') % {'file_name': song('~filename'), 'file_format': song('~format')}) for song in songs: try: song.clear_images() except AudioFileError as e: raise CommandError(e)",if not song.can_change_images:,if not song.is_image():,0.8764988086209256,0.9134996171406936,False
3626,"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: <IF_STMT> try: if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()): filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules",if pricing_rule.condition:,"if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):",0.6474416040989915,0.8169276475307028,False
3627,"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text if text == '""use strict""' or text == ""'use strict'"": <IF_STMT> self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)",if len(self._scopeStrictModes) > 0:,if self._useStrictCurrent:,0.9112526314046961,0.8787142254774354,False
3628,"def _find_remote_inputs(metadata): out = [] for fr_key in metadata.keys(): if isinstance(fr_key, (list, tuple)): frs = fr_key else: frs = [fr_key] for fr in frs: <IF_STMT> out.append(fr) return out",if objectstore.is_remote(fr):,"if isinstance(fr, RemoteInput):",0.9120452133790596,0.8390782502060267,False
3629,"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] if first.tag == 'p' and first.text is not None: m = RE_CHECKBOX.match(first.text) <IF_STMT> first.text = self.markdown.htmlStash.store(get_checkbox(m.group('state')), safe=True) + m.group('line') found = True return found",if m is not None:,if m:,0.6693184931066793,0.9099951253570094,False
3630,"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [''] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) <IF_STMT> directories.append(filename) elif os.path.exists(os.path.join(basedir, filename)): yield filename","if os.path.isdir(os.path.join(basedir, filename)):",if os.path.isdir(filename):,0.8817662826778756,0.9076141716697395,False
3631,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_version(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.8646149936089025,0.7378351342269067,True
3632,"def _dump(self, fd): with self.no_unpicklable_properties(): <IF_STMT> d = pickle.dumps(self) module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0] d = d.replace(b'c__main__', b'c' + module_name.encode('ascii')) fd.write(d) else: pickle.dump(self, fd)",if self.__module__ == '__main__':,if self.is_python:,0.8951360879835467,0.839587623092576,False
3633,"def assert_session_stack(classes): assert len(_SklearnTrainingSession._session_stack) == len(classes) for idx, (sess, (parent_clazz, clazz)) in enumerate(zip(_SklearnTrainingSession._session_stack, classes)): assert sess.clazz == clazz <IF_STMT> assert sess._parent is None else: assert sess._parent.clazz == parent_clazz",if idx == 0:,if idx == 0:,0.7995727544824978,0.8105932471967202,True
3634,"def native_color(c): try: color = CACHE[c] except KeyError: <IF_STMT> c = NAMED_COLOR[c] color = Color.FromArgb(int(c.rgba.a * 255), int(c.rgba.r), int(c.rgba.g), int(c.rgba.b)) CACHE[c] = color return color","if isinstance(c, str):",if c in NAMED_COLOR:,0.6170959497497889,0.7886336751695258,False
3635,"def callback(name): for neighbor_name in reactor.configuration.neighbors.keys(): neighbor = reactor.configuration.neighbors.get(neighbor_name, None) <IF_STMT> continue neighbor.rib.outgoing.announce_watchdog(name) yield False reactor.processes.answer_done(service)",if not neighbor:,if neighbor is None:,0.6481947656019288,0.674945488826271,False
3636,"def token_producer(source): token = source.read_uint8() while token is not None: <IF_STMT> yield DataToken(read_data(token, source)) elif is_small_integer(token): yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",if is_push_data_token(token):,if is_data(token):,0.8447341998493356,0.8466657105524215,False
3637,"def setattr(self, req, ino, attr, to_set, fi): print('setattr:', ino, to_set) a = self.attr[ino] for key in to_set: <IF_STMT> a['st_mode'] = S_IFMT(a['st_mode']) | S_IMODE(attr['st_mode']) else: a[key] = attr[key] self.attr[ino] = a self.reply_attr(req, a, 1.0)",if key == 'st_mode':,if fi:,0.8831184179573771,0.8901732118131125,False
3638,"def check_enum_exports(module, eq_callback, only=None): """"""Make sure module exports all mnemonics from enums"""""" for attr in enumerate_module(module, enum.Enum): <IF_STMT> print('SKIP', attr) continue for flag, value in attr.__members__.items(): print(module, flag, value) eq_callback(getattr(module, flag), value)",if only is not None and attr not in only:,if only is not None and attr.name in only:,0.6345819473515751,0.7252510836426073,False
3639,"def remove_edit_vars_to(self, n): try: removals = [] for v, cei in self.edit_var_map.items(): <IF_STMT> removals.append(v) for v in removals: self.remove_edit_var(v) assert len(self.edit_var_map) == n except ConstraintNotFound: raise InternalError('Constraint not found during internal removal')",if cei.index >= n:,if cei.get_edit_var_id() == n:,0.8384210187305232,0.8336104423443033,False
3640,"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or (type(e) is Option and e.argcount): <IF_STMT> e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or (type(e) is Option and e.argcount == 0): e.value = 0 return self",if e.value is None:,if type(e.value) is None:,0.9100532371417192,0.9226596185977016,False
3641,"def add_I_prefix(current_line: List[str], ner: int, tag: str): for i in range(0, len(current_line)): if i == 0: f.write(line_list[i]) <IF_STMT> f.write(' I-' + tag) else: f.write(' ' + current_line[i]) f.write('\n')",elif i == ner:,elif i == ner:,0.9045228878089532,0.8105932471967202,True
3642,def select_word_at_cursor(self): word_region = None selection = self.view.sel() for region in selection: word_region = self.view.word(region) <IF_STMT> selection.clear() selection.add(word_region) return word_region return word_region,if not word_region.empty():,if word_region is not None:,0.7753018047384825,0.7297349727547102,False
3643,"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: if self.op == '+': self.current += num <IF_STMT> self.current -= num elif self.op == '*': self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",elif self.op == '-':,elif self.op == '-':,0.9301459329147952,0.914208565914368,True
3644,"def strip_pod(lines): in_pod = False stripped_lines = [] for line in lines: if re.match('^=(?:end|cut)', line): in_pod = False elif re.match('^=\\w+', line): in_pod = True <IF_STMT> stripped_lines.append(line) return stripped_lines",elif not in_pod:,elif in_pod:,0.8981519075275721,0.8696398662122882,False
3645,"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) <IF_STMT> files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",if data:,if data:,0.802400972098324,0.926934323706186,True
3646,"def loadPerfsFromModule(self, module): """"""Return a suite of all perfs cases contained in the given module"""""" perfs = [] for name in dir(module): obj = getattr(module, name) <IF_STMT> perfs.append(self.loadPerfsFromPerfCase(obj)) return self.suiteClass(perfs)","if type(obj) == types.ClassType and issubclass(obj, PerfCase):","if isinstance(obj, PerfCase):",0.7587117172879618,0.8498644646741501,False
3647,"def download_subtitle(self, subtitle): if isinstance(subtitle, XSubsSubtitle): logger.info('Downloading subtitle %r', subtitle) r = self.session.get(subtitle.download_link, headers={'Referer': subtitle.page_link}, timeout=10) r.raise_for_status() <IF_STMT> logger.debug('Unable to download subtitle. No data returned from provider') return subtitle.content = fix_line_ending(r.content)",if not r.content:,if r.status_code == 404:,0.7113616427618296,0.828399516355805,False
3648,"def get_inlaws(self, person): inlaws = [] family_handles = person.get_family_handle_list() for handle in family_handles: fam = self.database.get_family_from_handle(handle) if fam.father_handle and (not fam.father_handle == person.handle): inlaws.append(self.database.get_person_from_handle(fam.father_handle)) <IF_STMT> inlaws.append(self.database.get_person_from_handle(fam.mother_handle)) return inlaws",elif fam.mother_handle and (not fam.mother_handle == person.handle):,elif fam.mother_handle and (not fam.mother_handle == person.handle):,0.7206017554390581,0.735676009091546,True
3649,"def _check_xorg_conf(): if is_there_a_default_xorg_conf_file(): print('WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not create it yourself, it was likely generated by your distribution or by an Nvidia utility.\nThis file may contain hard-coded GPU configuration that could interfere with optimus-manager, so it is recommended that you delete it before proceeding.\nIgnore this warning and proceed with GPU switching ? (y/N)') confirmation = ask_confirmation() <IF_STMT> sys.exit(0)",if not confirmation:,if not confirmation:,0.677585350627033,0.9443214435222291,True
3650,"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = '%d/%ds' % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: <IF_STMT> methods = '' elif isinstance(methods, (list, tuple)): methods = ''.join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, 'RATELIMIT_CACHE_PREFIX', 'rl:') return prefix + hashlib.md5(u''.join(parts).encode('utf-8')).hexdigest()",if methods == ALL:,if methods == 'all':,0.9460972542658299,0.8964173245779284,False
3651,"def num_of_mapped_volumes(self, initiator): cnt = 0 for lm_link in self.req('lun-maps')['lun-maps']: idx = lm_link['href'].split('/')[-1] try: lm = self.req('lun-maps', idx=int(idx))['content'] except exception.NotFound: continue <IF_STMT> cnt += 1 return cnt",if lm['ig-name'] == initiator:,if lm['type'] == 'device':,0.7378194994519054,0.8038019482772603,False
3652,"def _setAbsoluteY(self, value): if value is None: self._absoluteY = None else: <IF_STMT> value = 10 elif value == 'below': value = -70 try: value = common.numToIntOrFloat(value) except ValueError as ve: raise TextFormatException(f'Not a supported absoluteY position: {value!r}') from ve self._absoluteY = value",if value == 'above':,if value == 'above':,0.7814791488538309,0.8723360571509826,True
3653,"def render_markdown(text): users = {u.username.lower(): u for u in get_mention_users(text)} parts = MENTION_RE.split(text) for pos, part in enumerate(parts): if not part.startswith('@'): continue username = part[1:].lower() <IF_STMT> user = users[username] parts[pos] = '**[{}]({} ""{}"")**'.format(part, user.get_absolute_url(), user.get_visible_name()) text = ''.join(parts) return mark_safe(MARKDOWN(text))",if username in users:,if username in users:,0.7775681977925331,0.8661072626070159,True
3654,def start_process(self): with self.thread_lock: <IF_STMT> self.allow_process_request = False t = threading.Thread(target=self.__start) t.daemon = True t.start(),if self.allow_process_request:,if self.allow_process_request:,0.8214280050844979,0.7447819789879647,True
3655,"def close(self): if self._fh.closed: return self._fh.close() if os.path.isfile(self._filename): <IF_STMT> salt.utils.win_dacl.copy_security(source=self._filename, target=self._tmp_filename) else: shutil.copymode(self._filename, self._tmp_filename) st = os.stat(self._filename) os.chown(self._tmp_filename, st.st_uid, st.st_gid) atomic_rename(self._tmp_filename, self._filename)",if salt.utils.win_dacl.HAS_WIN32:,if salt.utils.win_dacl:,0.7941322705603073,0.8318180062062374,False
3656,"def _splitSchemaNameDotFieldName(sn_fn, fnRequired=True): if sn_fn.find('.') != -1: schemaName, fieldName = sn_fn.split('.', 1) schemaName = schemaName.strip() fieldName = fieldName.strip() if schemaName and fieldName: return (schemaName, fieldName) elif not fnRequired: schemaName = sn_fn.strip() <IF_STMT> return (schemaName, None) controlflow.system_error_exit(2, f'{sn_fn} is not a valid custom schema.field name.')",if schemaName:,if schemaName and (not fieldName):,0.8258225500160479,0.8591169759078797,False
3657,"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: <IF_STMT> op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list","if not isinstance(op_list, list):","if not isinstance(op_list, list):",0.8530116861579755,0.8555308664663046,True
3658,"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) <IF_STMT> for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True if not event.get_date_object(): return True return False",if family:,if family:,0.7408348031375486,0.8901732118131125,True
3659,"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): <IF_STMT> if value is not None: value = value == 'true' self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)","if key in ('disabled', 'is_up'):",if key == 'is_test':,0.8686187219327643,0.8474968231198384,False
3660,"def get_billable_and_total_duration(activity, start_time, end_time): precision = frappe.get_precision('Timesheet Detail', 'hours') activity_duration = time_diff_in_hours(end_time, start_time) billing_duration = 0.0 if activity.billable: billing_duration = activity.billing_hours <IF_STMT> billing_duration = activity_duration * activity.billing_hours / activity.hours return (flt(activity_duration, precision), flt(billing_duration, precision))",if activity_duration != activity.billing_hours:,if activity.hours:,0.8444342442518965,0.8935248372106969,False
3661,def cpus(self): try: cpus = self.inspect['Spec']['Resources']['Reservations']['NanoCPUs'] / 1000000000.0 <IF_STMT> cpus = int(cpus) return cpus except TypeError: return None except KeyError: return 0,if cpus == int(cpus):,"if isinstance(cpus, int):",0.8040792436280799,0.7947545184555568,False
3662,"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): <IF_STMT> func_name = list(prop_value.keys())[0] if func_name.startswith('_'): func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props","if isinstance(prop_value, dict) and prop_value:",if prop_name in self.fake_properties:,0.9059638367063922,0.8555308664663046,False
3663,"def _yield_unescaped(self, string): while '\\' in string: finder = EscapeFinder(string) yield (finder.before + finder.backslashes) <IF_STMT> yield self._unescape(finder.text) else: yield finder.text string = finder.after yield string",if finder.escaped and finder.text:,if self.escape_text:,0.6500563480366944,0.8531413606256201,False
3664,"def _check_matches(rule, matches): errors = 0 for match in matches: filematch = _match_to_test_file(match) <IF_STMT> utils.error(""The match '{}' for rule '{}' points to a non existing test module path: {}"", match, rule, filematch) errors += 1 return errors",if not filematch.exists():,if not filematch:,0.7475010717073747,0.8783650674919876,False
3665,"def focused_windows(): tree = i3.get_tree() workspaces = tree.workspaces() for workspace in workspaces: container = workspace while container: if not hasattr(container, 'focus') or not container.focus: break container_id = container.focus[0] container = container.find_by_id(container_id) <IF_STMT> coname = container.name wsname = workspace.name print('WS', wsname + ':', coname)",if container:,if container:,0.8770225885138516,0.9164531641034833,True
3666,"def normals(self, value): if value is not None: value = np.asanyarray(value, dtype=np.float32) value = np.ascontiguousarray(value) <IF_STMT> raise ValueError('Incorrect normals shape') self._normals = value",if value.shape != self.positions.shape:,if len(value.shape) != self.shape:,0.7088750725946007,0.7709002428237395,False
3667,"def test_hexdigest(self): for cons in self.hash_constructors: h = cons() <IF_STMT> self.assertIsInstance(h.digest(16), bytes) self.assertEqual(hexstr(h.digest(16)), h.hexdigest(16)) else: self.assertIsInstance(h.digest(), bytes) self.assertEqual(hexstr(h.digest()), h.hexdigest())",if h.name in self.shakes:,if h.is_hex:,0.7984283168838353,0.803154665668484,False
3668,"def _get_cluster_status(self): try: return self.dataproc_client.projects().regions().clusters().get(projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name, fields='status').execute() except HttpError as e: <IF_STMT> return None else: raise e",if e.resp.status == 404:,if e.status == 404:,0.5836122447976155,0.7098232254187811,False
3669,"def _items_from(self, context): self._context = context if self._is_local_variable(self._keyword_name, context): for item in self._items_from_controller(context): yield item else: for df in context.datafiles: self._yield_for_other_threads() <IF_STMT> for item in self._items_from_datafile(df): yield item",if self._items_from_datafile_should_be_checked(df):,if df.name == self._keyword_name:,0.8732458057358938,0.8105932471967202,False
3670,"def Command(argv, funcs, path_val): arg, i = COMMAND_SPEC.Parse(argv) status = 0 if arg.v: for kind, arg in _ResolveNames(argv[i:], funcs, path_val): <IF_STMT> status = 1 else: print(arg) else: util.warn('*** command without -v not not implemented ***') status = 1 return status",if kind is None:,if kind == 'v':,0.7789623466224611,0.8661072626070159,False
3671,"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: if isinstance(node, Preprint): category = 'preprint' <IF_STMT> category = 'registration' else: category = node.project_or_component client().delete(index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404])",elif node.is_registration:,"elif isinstance(node, Registration):",0.6676711609637518,0.8592899528284996,False
3672,def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): <IF_STMT> content = getDictFromTree(child) else: content = child.text if ret_dict.has_key(child.tag): if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or '') else: ret_dict[child.tag] = content or '' return ret_dict,if child.getchildren():,"if isinstance(child, Tree):",0.8794918829395652,0.8815741981066073,False
3673,"def get(self, block=True, timeout=None, ack=False): if not block: return self.get_nowait() start_time = time.time() while True: try: return self.get_nowait(ack) except BaseQueue.Empty: <IF_STMT> lasted = time.time() - start_time if timeout > lasted: time.sleep(min(self.max_timeout, timeout - lasted)) else: raise else: time.sleep(self.max_timeout)",if timeout:,if timeout:,0.9312914715450153,0.9051034981560222,True
3674,"def rewrite(self, string): string = super(JSReplaceFuzzy, self).rewrite(string) cdx = self.url_rewriter.rewrite_opts['cdx'] if cdx.get('is_fuzzy'): expected = unquote(cdx['url']) actual = unquote(self.url_rewriter.wburl.url) exp_m = self.rx_obj.search(expected) act_m = self.rx_obj.search(actual) <IF_STMT> result = string.replace(exp_m.group(1), act_m.group(1)) if result != string: string = result return string",if exp_m and act_m:,if exp_m:,0.7167135620637464,0.9051034981560222,False
3675,"def locate_exe_dir(d, check=True): exe_dir = os.path.join(d, 'Scripts') if ON_WINDOWS else os.path.join(d, 'bin') if not os.path.isdir(exe_dir): <IF_STMT> bin_dir = os.path.join(d, 'bin') if os.path.isdir(bin_dir): return bin_dir if check: raise InvalidVirtualEnv('Unable to locate executables directory.') return exe_dir",if ON_WINDOWS:,if ON_WINDOWS:,0.924223760332594,0.8935248372106969,True
3676,"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == 'append': if s not in sys.path: sys.path.append(s) el<IF_STMT> sys.path.insert(0, s)",if s != sys.path[0]:,if s not in sys.path:,0.8649632819995287,0.7416044742607677,False
3677,"def create_season_banners(self, show_obj): if self.season_banners and show_obj: result = [] for season, episodes in show_obj.episodes.iteritems(): <IF_STMT> logger.log(u'Metadata provider ' + self.name + ' creating season banners for ' + show_obj.name, logger.DEBUG) result = result + [self.save_season_banners(show_obj, season)] return all(result) return False","if not self._has_season_banner(show_obj, season):",if season not in self.season_banners:,0.7714307609364734,0.8492988135354755,False
3678,"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: if 'nbgrader' not in cell.metadata: continue grade = cell.metadata['nbgrader']['grade'] solution = cell.metadata['nbgrader']['solution'] locked = cell.metadata['nbgrader']['locked'] <IF_STMT> continue grade_id = cell.metadata['nbgrader']['grade_id'] if grade_id in ids: raise ValidationError('Duplicate grade id: {}'.format(grade_id)) ids.add(grade_id)",if not grade and (not solution) and (not locked):,if grade != solution or locked != locked:,0.9042713232316126,0.7988335681808146,False
3679,"def read_version(): regexp = re.compile(""^__version__\\W*=\\W*'([\\d.abrc]+)'"") init_py = os.path.join(os.path.dirname(__file__), 'aiopg', '__init__.py') with open(init_py) as f: for line in f: match = regexp.match(line) <IF_STMT> return match.group(1) else: raise RuntimeError('Cannot find version in aiopg/__init__.py')",if match is not None:,if match:,0.7989216101875715,0.8827916928185874,False
3680,"def _column_keys(self): """"""Get a dictionary of all columns and their case mapping."""""" if not self.exists: return {} with self.db.lock: if self._columns is None: table = self.table self._columns = {} for column in table.columns: name = normalize_column_name(column.name) key = normalize_column_key(name) <IF_STMT> log.warning('Duplicate column: %s', name) self._columns[key] = name return self._columns",if key in self._columns:,if key in self._columns:,0.906716888931629,0.8902056737869248,True
3681,"def find_controller_by_names(self, names, testname): namestring = '.'.join(names) if not namestring.startswith(self.name): return None if namestring == self.name: return self for suite in self.suites: res = suite.find_controller_by_names(namestring[len(self.name) + 1:].split('.'), testname) <IF_STMT> return res",if res:,if res:,0.9318397959392591,0.8918471513698394,True
3682,"def _volume_x_metadata_get_item(context, volume_id, key, model, notfound_exec, session=None): result = _volume_x_metadata_get_query(context, volume_id, model, session=session).filter_by(key=key).first() if not result: <IF_STMT> raise notfound_exec(id=volume_id) else: raise notfound_exec(metadata_key=key, volume_id=volume_id) return result",if model is models.VolumeGlanceMetadata:,if key == 'id':,0.6359437237972297,0.7886336751695258,False
3683,"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] <IF_STMT> if dd['results.train_y_misclass'] < optimal_measure: optimal_measure = dd['results.train_y_misclass'] optimal_dd = dd print('Optimal results.train_y_misclass:', str(optimal_measure)) for key, value in optimal_dd.items(): if 'hyper_parameters' in key: print(key + ': ' + str(value))",if 'results.train_y_misclass' in dd:,if 'results' in dd:,0.7095863839294051,0.8780099567239787,False
3684,"def _stop_by_max_time_mins(self): """"""Stop optimization process once maximum minutes have elapsed."""""" if self.max_time_mins: total_mins_elapsed = (datetime.now() - self._start_datetime).total_seconds() / 60.0 <IF_STMT> raise KeyboardInterrupt('{:.2f} minutes have elapsed. TPOT will close down.'.format(total_mins_elapsed))",if total_mins_elapsed >= self.max_time_mins:,if total_mins_elapsed > self.max_time_mins:,0.6861303923379249,0.8169276475307028,False
3685,"def __new__(meta, cls_name, bases, cls_dict): func = cls_dict.get('func') monad_cls = super(FuncMonadMeta, meta).__new__(meta, cls_name, bases, cls_dict) if func: <IF_STMT> functions = func else: functions = (func,) for func in functions: registered_functions[func] = monad_cls return monad_cls",if type(func) is tuple:,"if isinstance(func, (list, tuple)):",0.6436124349311869,0.8431339019329497,False
3686,"def get_tokens_unprocessed(self, text): buffered = '' insertions = [] lng_buffer = [] for i, t, v in self.language_lexer.get_tokens_unprocessed(text): <IF_STMT> if lng_buffer: insertions.append((len(buffered), lng_buffer)) lng_buffer = [] buffered += v else: lng_buffer.append((i, t, v)) if lng_buffer: insertions.append((len(buffered), lng_buffer)) return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))",if t is self.needle:,if t == ' ' and i == 0:,0.8973511747953071,0.7729203086644559,False
3687,"def get_conditions(filters): conditions = {'docstatus': ('=', 1)} if filters.get('from_date') and filters.get('to_date'): conditions['result_date'] = ('between', (filters.get('from_date'), filters.get('to_date'))) filters.pop('from_date') filters.pop('to_date') for key, value in filters.items(): <IF_STMT> conditions[key] = value return conditions",if filters.get(key):,"if isinstance(value, dict):",0.9116990696874836,0.8446593249975184,False
3688,"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): <IF_STMT> if datetime.now() - limit > value: value = datetime.now() - limit elif datetime.now() + limit < value: value = datetime.now() + limit elif value > limit: value = limit return value",if config[key]['inverse'] is True:,if value > 0:,0.8371077811975025,0.8856327184319047,False
3689,"def GetCurrentKeySet(self): """"""Return CurrentKeys with 'darwin' modifications."""""" result = self.GetKeySet(self.CurrentKeys()) if sys.platform == 'darwin': for k, v in result.items(): v2 = [x.replace('<Alt-', '<Option-') for x in v] <IF_STMT> result[k] = v2 return result",if v != v2:,if v2:,0.6807689927497639,0.8901732118131125,False
3690,"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) if hasattr(package, '__loader__'): <IF_STMT> file_contents = package.__loader__.get_data(filename) return (file_contents.replace(os.linesep, '\n'), filename) return (open(filename).read(), filename)","if hasattr(package.__loader__, 'get_data'):","if hasattr(package.__loader__, 'get_data'):",0.9118733013140065,0.8390782502060267,True
3691,"def iter_from_X_lengths(X, lengths): if lengths is None: yield (0, len(X)) else: n_samples = X.shape[0] end = np.cumsum(lengths).astype(np.int32) start = end - lengths <IF_STMT> raise ValueError('more than {:d} samples in lengths array {!s}'.format(n_samples, lengths)) for i in range(len(lengths)): yield (start[i], end[i])",if end[-1] > n_samples:,if len(start) > n_samples:,0.8213347760418699,0.8661072626070159,False
3692,"def change_sel(self): """"""Change the view's selections."""""" if self.alter_select and len(self.sels) > 0: <IF_STMT> self.view.show(self.sels[0]) self.view.sel().clear() self.view.sel().add_all(self.sels)",if self.multi_select is False:,if self.view.sel().is_visible():,0.7816101726733247,0.762465858623486,False
3693,"def cb_syncthing_device_data_changed(self, daemon, nid, address, client_version, inbps, outbps, inbytes, outbytes): if nid in self.devices: device = self.devices[nid] device['address'] = address <IF_STMT> device['version'] = client_version device['inbps'] = '%s/s (%s)' % (sizeof_fmt(inbps), sizeof_fmt(inbytes)) device['outbps'] = '%s/s (%s)' % (sizeof_fmt(outbps), sizeof_fmt(outbytes))","if client_version not in ('?', None):",if client_version:,0.7053613667077911,0.9051034981560222,False
3694,"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: if match not in matches: <IF_STMT> match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",if self.match_name:,if self.match_name:,0.8862339392893501,0.9164531641034833,True
3695,"def __update_parents(self, fileobj, path, delta): """"""Update all parent atoms with the new size."""""" if delta == 0: return for atom in path: fileobj.seek(atom.offset) size = cdata.uint_be(fileobj.read(4)) <IF_STMT> size = cdata.ulonglong_be(fileobj.read(12)[4:]) fileobj.seek(atom.offset + 8) fileobj.write(cdata.to_ulonglong_be(size + delta)) else: fileobj.seek(atom.offset) fileobj.write(cdata.to_uint_be(size + delta))",if size == 1:,if size == 0:,0.918685303859587,0.8692960007731574,False
3696,"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: if field.primary_key: continue requires_index = any((field.index, field.unique, isinstance(field, ForeignKeyField))) <IF_STMT> fields.append(field) return fields",if requires_index:,if requires_index:,0.7052029049817062,0.8318180062062374,True
3697,"def __init__(self, value): """"""Initialize the integer to the given value."""""" self._mpz_p = new_mpz() self._initialized = False if isinstance(value, float): raise ValueError('A floating point type is not a natural number') self._initialized = True if isinstance(value, (int, long)): _gmp.mpz_init(self._mpz_p) result = _gmp.gmp_sscanf(tobytes(str(value)), b('%Zd'), self._mpz_p) <IF_STMT> raise ValueError(""Error converting '%d'"" % value) else: _gmp.mpz_init_set(self._mpz_p, value._mpz_p)",if result != 1:,if result != _gmp.MPZ_OK:,0.942394986846169,0.8964173245779284,False
3698,"def decode(cls, data): while data: length, format_type, control_flags, sequence, pid = unpack(cls.Header.PACK, data[:cls.Header.LEN]) <IF_STMT> raise NetLinkError('Buffer underrun') yield cls.format(format_type, control_flags, sequence, pid, data[cls.Header.LEN:length]) data = data[length:]",if len(data) < length:,if length == 0:,0.7797787215570977,0.7965020533851944,False
3699,"def __post_init__(self): if self._node_id is not None: <IF_STMT> raise ValueError('invalid node_id: {}'.format(hexlify(self._node_id).decode())) if self.udp_port is not None and (not 1 <= self.udp_port <= 65535): raise ValueError('invalid udp port') if self.tcp_port is not None and (not 1 <= self.tcp_port <= 65535): raise ValueError('invalid tcp port') if not is_valid_public_ipv4(self.address, self.allow_localhost): raise ValueError(f""invalid ip address: '{self.address}'"")",if not len(self._node_id) == constants.HASH_LENGTH:,"if not is_valid_node_id(self._node_id, self.allow_localhost):",0.8744233299486365,0.8983343737277126,False
3700,"def orderUp(self, items): sel = [] undoinfo = [] for bid, lid in items: if isinstance(lid, int): undoinfo.append(self.orderUpLineUndo(bid, lid)) sel.append((bid, lid - 1)) <IF_STMT> undoinfo.append(self.orderUpBlockUndo(bid)) if bid == 0: return items else: sel.append((bid - 1, None)) self.addUndo(undoinfo, 'Move Up') return sel",elif lid is None:,"elif isinstance(bid, int):",0.9225149537359427,0.8902579342581529,False
3701,"def filter_data(self, min_len, max_len): logging.info(f'filtering data, min len: {min_len}, max len: {max_len}') initial_len = len(self.src) filtered_src = [] filtered_tgt = [] for src, tgt in zip(self.src, self.tgt): <IF_STMT> filtered_src.append(src) filtered_tgt.append(tgt) self.src = filtered_src self.tgt = filtered_tgt filtered_len = len(self.src) logging.info(f'pairs before: {initial_len}, after: {filtered_len}')",if min_len <= len(src) <= max_len and min_len <= len(tgt) <= max_len:,if src < initial_len and tgt < initial_len:,0.8663849250746214,0.8070218370798333,False
3702,"def layer_pretrained(self, net, args, options): model = getattr(torchvision.models, args[0])(pretrained=True) model.train(True) if options.layer: layers = list(model.children())[:options.layer] <IF_STMT> layers[-1] = nn.Sequential(*layers[-1][:options.sublayer]) else: layers = [model] print('List of pretrained layers:', layers) raise ValidationException('layer=-1 required for pretrained, sublayer=-1 optional.  Layers outputted above.') return nn.Sequential(*layers)",if options.sublayer:,if options.sublayer:,0.7687357047031549,0.9099951253570094,True
3703,"def deleteCalendar(users): calendarId = normalizeCalendarId(sys.argv[5]) for user in users: user, cal = buildCalendarGAPIObject(user) <IF_STMT> continue gapi.call(cal.calendarList(), 'delete', soft_errors=True, calendarId=calendarId)",if not cal:,if not cal:,0.6979079969151997,0.761827408333416,True
3704,"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) <IF_STMT> return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: if module.is_compatible_with(client): yield module break else: yield module",if not clients:,if clients is None:,0.6126919710919854,0.8856327184319047,False
3705,"def update_me(self): try: while 1: line = self.queue.get_nowait() <IF_STMT> self.delete(1.0, tk.END) else: self.insert(tk.END, str(line)) self.see(tk.END) self.update_idletasks() except queue.Empty: pass self.after(100, self.update_me)",if line is None:,if line is None:,0.5873230550426489,0.7498810286408993,True
3706,"def request_power_state(self, state, force=False): if self.current_state != state or force: <IF_STMT> self.request_in_progress = True logging.info('Requesting %s' % state) cb = PowerManager.Callback(self, state) rets = self.parent.Plugins.run('on_power_state_change_requested', self, state, cb) cb.num_cb = len(rets) cb.check() else: logging.info('Another request in progress')",if not self.request_in_progress:,"if self.parent.Plugins.run('on_power_state_change_requested', self, state):",0.7946347563485429,0.8555308664663046,False
3707,"def __getitem__(self, idx): super(BatchDataset, self).__getitem__(idx) maxidx = len(self.dataset) samples = [] for i in range(0, self.batchsize): j = idx * self.batchsize + i if j >= maxidx: break j = self.perm(j, maxidx) sample = self.dataset[j] <IF_STMT> samples.append(sample) samples = self.makebatch(samples) return samples",if self.filter(sample):,if sample is not None:,0.8882054401263282,0.8527204701689132,False
3708,"def __call__(self, request, *args, **kwargs): template_vars = {} for form_name, form_class in self.forms.iteritems(): <IF_STMT> template_vars[form_name] = form_class(request) else: template_vars[form_name] = None if request.method == 'POST': action = self.find_post_handler_action(request) form = self.handlers[action](request, data=request.POST, files=request.FILES) template_vars.update(form.dispatch(action, request, *args, **kwargs)) return self.GET(template_vars, request, *args, **kwargs)","if form_class.must_display(request, *args, **kwargs):",if form_class:,0.84412832589715,0.9144061946646023,False
3709,"def on_show_all(self, widget, another): if widget.get_active(): <IF_STMT> self.treeview.update_items(all=True, comment=True) else: self.treeview.update_items(all=True) elif another.get_active(): self.treeview.update_items(comment=True) else: self.treeview.update_items()",if another.get_active():,if widget.get_active():,0.6033426954696772,0.762465858623486,False
3710,"def close(self): if self._closed: return self._closed = True for proto in self._pipes.values(): if proto is None: continue proto.pipe.close() if self._proc is not None and self._returncode is None and (self._proc.poll() is None): <IF_STMT> logger.warning('Close running child process: kill %r', self) try: self._proc.kill() except ProcessLookupError: pass",if self._loop.get_debug():,if self._debug:,0.9277489791765439,0.9184043388013005,False
3711,"def runTest(self): self.poco(text='wait UI').click() bomb_count = 0 while True: blue_fish = self.poco('fish_emitter').child('blue') yellow_fish = self.poco('fish_emitter').child('yellow') bomb = self.poco('fish_emitter').child('bomb') fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) if fish is bomb: bomb_count += 1 <IF_STMT> return else: fish.click() time.sleep(2.5)",if bomb_count > 3:,if bomb_count >= 10:,0.8709282193110939,0.8474968231198384,False
3712,"def load_managers(*, loop, only): managers = {} for key in DB_CLASSES: <IF_STMT> continue params = DB_DEFAULTS.get(key) or {} params.update(DB_OVERRIDES.get(key) or {}) database = DB_CLASSES[key](**params) managers[key] = peewee_async.Manager(database, loop=loop) return managers",if only and key not in only:,if only and key in only:,0.7981019559596986,0.7736680847834176,False
3713,"def links_extracted(self, request, links): for link in links: <IF_STMT> r = self._create_request(link.url) r.meta[b'depth'] = request.meta[b'depth'] + 1 self.schedule(r, self._get_score(r.meta[b'depth'])) link.meta[b'state'] = States.QUEUED",if link.meta[b'state'] == States.NOT_CRAWLED:,if link.meta[b'state'] == States.QUEUED:,0.8598650642604158,0.760856626273165,False
3714,"def find_worktree_git_dir(dotgit): """"""Search for a gitdir for this worktree."""""" try: statbuf = os.stat(dotgit) except OSError: return None if not stat.S_ISREG(statbuf.st_mode): return None try: lines = open(dotgit, 'r').readlines() for key, value in [line.strip().split(': ') for line in lines]: <IF_STMT> return value except ValueError: pass return None",if key == 'gitdir':,if key == 'gitdir':,0.9239523739241775,0.8806615362338783,True
3715,"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: if not isinstance(dim_value, int): return False <IF_STMT> raise Exception('Negative dimension is illegal: %d' % dim_value) return True",if dim_value < 0:,if dim_value < 0:,0.8681477533624706,0.8431339019329497,True
3716,"def init_logger(): configured_loggers = [log_config.get('root', {})] + [logger for logger in log_config.get('loggers', {}).values()] used_handlers = {handler for log in configured_loggers for handler in log.get('handlers', [])} for handler_id, handler in list(log_config['handlers'].items()): if handler_id not in used_handlers: del log_config['handlers'][handler_id] <IF_STMT> filename = handler['filename'] logfile_path = Path(filename).expanduser().resolve() handler['filename'] = str(logfile_path) logging.config.dictConfig(log_config)",elif 'filename' in handler.keys():,if handler['filename']:,0.8108071676055008,0.9253742688467129,False
3717,"def __call__(self): dmin, dmax = self.viewlim_to_dt() ymin = self.base.le(dmin.year) ymax = self.base.ge(dmax.year) ticks = [dmin.replace(year=ymin, **self.replaced)] while 1: dt = ticks[-1] <IF_STMT> return date2num(ticks) year = dt.year + self.base.get_base() ticks.append(dt.replace(year=year, **self.replaced))",if dt.year >= ymax:,if dt.year == ymin:,0.8329199639794416,0.828399516355805,False
3718,"def taiga(request, trigger_id, key): signature = request.META.get('HTTP_X_TAIGA_WEBHOOK_SIGNATURE') if verify_signature(request._request.body, key, signature): data = data_filter(trigger_id, **request.data) status = save_data(trigger_id, data) return Response({'message': 'Success'}) <IF_STMT> else Response({'message': 'Failed!'}) Response({'message': 'Bad request'})",if status,if status,0.9222631124410483,0.8743414417652072,True
3719,"def ParseResponses(self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue]) -> Iterator[rdf_client.User]: for response in responses: <IF_STMT> raise TypeError(f'Unexpected response type: `{type(response)}`') if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)","if not isinstance(response, rdf_client_fs.StatEntry):","if not isinstance(response, rdfvalue.RDFValue):",0.8520169629079595,0.8431339019329497,False
3720,"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get('next', {}).get('url', None) if not src: break n += 1 <IF_STMT> vd.warning(f'stopping at max {max_next} pages') break vd.status(f'fetching next page from {src}') response = requests.get(src, stream=True)",if n > max_next:,if n > max_next:,0.776523027007689,0.8780099567239787,True
3721,"def __enter__(self): """"""Open a file and read it."""""" if self.code is None: LOGGER.info('File is reading: %s', self.path) <IF_STMT> self._file = open(self.path, encoding='utf-8') else: self._file = open(self.path, 'rU') self.code = self._file.read() return self","if sys.version_info >= (3,):","if sys.version_info < (3, 0):",0.6131333772521761,0.8094220211349227,False
3722,"def facts_for_oauthclients(self, namespace): """"""Gathers facts for oauthclients used with logging"""""" self.default_keys_for('oauthclients') a_list = self.oc_command('get', 'oauthclients', namespace=namespace, add_options=['-l', LOGGING_SELECTOR]) if len(a_list['items']) == 0: return for item in a_list['items']: name = item['metadata']['name'] comp = self.comp(name) <IF_STMT> result = dict(redirectURIs=item['redirectURIs']) self.add_facts_for(comp, 'oauthclients', name, result)",if comp is not None:,if comp:,0.9407581438476731,0.9122561819614461,False
3723,"def get(self, k): with self._lock: <IF_STMT> self._data1[k] = self._data2[k] del self._data2[k] return self._data1.get(k)",if k not in self._data1 and k in self._data2:,if k in self._data1:,0.7597593537331683,0.6026080978557137,False
3724,"def _parseparam(s): plist = [] while s[:1] == ';': s = s[1:] end = s.find(';') while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find(';', end + 1) <IF_STMT> end = len(s) f = s[:end] if '=' in f: i = f.index('=') f = f[:i].strip().lower() + '=' + f[i + 1:].strip() plist.append(f.strip()) s = s[end:] return plist",if end < 0:,if end < 0:,0.7770096205300904,0.9155272930874561,True
3725,"def __init__(self, **params): if 'length' in params: <IF_STMT> raise ValueError('Supply either length or start and end to Player not both') params['start'] = 0 params['end'] = params.pop('length') - 1 elif params.get('start', 0) > 0 and (not 'value' in params): params['value'] = params['start'] super(Player, self).__init__(**params)",if 'start' in params or 'end' in params:,"if params.get('length', 0) != params.pop('length'):",0.9142180155096581,0.8559898693114286,False
3726,def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe('compiler.libcxx') if not compiler or not libcxx: return '' if str(compiler) in GCC_LIKE: if str(libcxx) == 'libstdc++': return '_GLIBCXX_USE_CXX11_ABI=0' <IF_STMT> return '_GLIBCXX_USE_CXX11_ABI=1' return '',elif str(libcxx) == 'libstdc++11':,elif str(libcxx) == 'libstdc++':,0.9026526166996612,0.828399516355805,False
3727,"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: if tag.user: tts[name] = '%ssort' % name <IF_STMT> tts['~%s' % name] = '~%ssort' % name return tts",if tag.internal:,elif tag.user.is_staff:,0.7585792981820061,0.8827916928185874,False
3728,"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form('List', None): value = [extract_pyreal(item) for item in value.leaves] if any((item is None for item in value)): return None return value else: return None else: value = extract_pyreal(value) <IF_STMT> return None return value",if value is None or isinf(value) or isnan(value):,if not value:,0.9504730516073787,0.9151329413834155,False
3729,"def on_action_chosen(self, id, action, mark_changed=True): before = self.set_action(self.current, id, action) if mark_changed: <IF_STMT> self.undo.append(UndoRedo(id, before, action)) self.builder.get_object('btUndo').set_sensitive(True) self.on_profile_modified() else: self.on_profile_modified(update_ui=False) return before",if before.to_string() != action.to_string():,if self.undo:,0.7036041653894987,0.8318180062062374,False
3730,"def setUp(self): super(OperaterTest, self).setUp() if is_cli: import clr self.load_iron_python_test() <IF_STMT> clr.AddReference('System.Drawing.Primitives') else: clr.AddReference('System.Drawing')",if is_netcoreapp:,if self.is_primitives:,0.8071032137503346,0.6997522298221912,False
3731,"def field_to_field_type(field): field_type = field['type'] if isinstance(field_type, dict): field_type = field_type['type'] if isinstance(field_type, list): field_type_length = len(field_type) if field_type_length == 0: raise Exception('Zero-length type list encountered, invalid CWL?') <IF_STMT> field_type = field_type[0] return field_type",elif len(field_type) == 1:,if field_type_length == 1:,0.9129182833077973,0.8431339019329497,False
3732,"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: <IF_STMT> ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: if type(ah) is not ActionHandle: raise ActionManagerError('Bad argument type %s' % str(ah)) ahs.add(ah) else: raise ActionManagerError('Bad argument type %s' % str(item)) return ahs",if type(item) is ActionHandle:,if type(item) is ActionHandle:,0.9057063307942814,0.8902056737869248,True
3733,"def _Determine_Do(self): self.applicable = 1 configTokens = black.configure.items['configTokens'].Get() buildFlavour = black.configure.items['buildFlavour'].Get() if buildFlavour == 'full': self.value = False else: self.value = True for opt, optarg in self.chosenOptions: <IF_STMT> if not self.value: configTokens.append('tests') self.value = True elif opt == '--without-tests': if self.value: configTokens.append('notests') self.value = False self.determined = 1",if opt == '--with-tests':,if opt == '--tests':,0.8689919583698694,0.8879659171421962,False
3734,"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): <IF_STMT> rval = composite_name if composite_file.description: rval = '{} ({})'.format(rval, composite_file.description) if composite_file.optional: rval = '%s [optional]' % rval return rval if index < self.get_file_count(trans, context): return 'Extra primary file' return None",if i == index:,if i == index:,0.7677479522796519,0.8856327184319047,True
3735,"def func(x, y): try: <IF_STMT> z = x + 2 * math.sin(y) return z ** 2 elif x == y: return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print('finished')",if x > y:,if x < y:,0.8298732508990249,0.8923575006167597,False
3736,def test_suite(): suite = unittest.TestSuite() for fn in os.listdir(here): <IF_STMT> modname = 'distutils.tests.' + fn[:-3] __import__(modname) module = sys.modules[modname] suite.addTest(module.test_suite()) return suite,if fn.startswith('test') and fn.endswith('.py'):,if fn.endswith('.py'):,0.8723068737349106,0.8318180062062374,False
3737,"def check_stack_names(self, frame, expected): names = [] while frame: name = frame.f_code.co_name <IF_STMT> break names.append(name) frame = frame.f_back self.assertEqual(names, expected)",if name.startswith('check_') or name.startswith('call_'):,if name in expected:,0.6450563899163934,0.7378351342269067,False
3738,"def leave(self, reason=None): try: if self.id.startswith('C'): log.info('Leaving channel %s (%s)', self, self.id) self._bot.api_call('conversations.leave', data={'channel': self.id}) else: log.info('Leaving group %s (%s)', self, self.id) self._bot.api_call('conversations.leave', data={'channel': self.id}) except SlackAPIResponseError as e: <IF_STMT> raise RoomError(f'Unable to leave channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e) self._id = None",if e.error == 'user_is_bot':,if e.response.status_code == 404:,0.920760707142378,0.8723360571509826,False
3739,"def ident(self): value = self._ident if value is False: value = None <IF_STMT> wrapped = self.wrapped ident = getattr(wrapped, 'ident', None) if ident is not None: value = self._wrap_hash(ident) self._ident = value return value",if not self.orig_prefix:,if self.wrapped is not None:,0.7236525307397019,0.8200123297196334,False
3740,"def is_ac_power_connected(): for power_source_path in Path('/sys/class/power_supply/').iterdir(): try: with open(power_source_path / 'type', 'r') as f: <IF_STMT> continue with open(power_source_path / 'online', 'r') as f: if f.read(1) == '1': return True except IOError: continue return False",if f.read().strip() != 'Mains':,if f.read(1) == '1':,0.6495982578481381,0.8431339019329497,False
3741,"def _get_pending_by_app_token(self, app_token): result = [] with self._pending_lock: self._remove_stale_pending() for data in self._pending_decisions: <IF_STMT> result.append(data) return result",if data.app_token == app_token:,if data['app_token'] == app_token:,0.8476551760595223,0.693395566222006,False
3742,"def do_create(specific_tables=None, base=Base): engine = get_engine() try: <IF_STMT> logger.info('Initializing only a subset of tables as requested: {}'.format(specific_tables)) base.metadata.create_all(engine, tables=specific_tables) else: base.metadata.create_all(engine) except Exception as err: raise Exception('could not create/re-create DB tables - exception: ' + str(err))",if specific_tables:,if specific_tables:,0.775566309372558,0.8996480074924822,True
3743,"def __setitem__(self, ndx, val): exprdata = None if ndx in self._data: exprdata = self._data[ndx] else: _ndx = normalize_index(ndx) <IF_STMT> exprdata = self._data[_ndx] if exprdata is None: raise KeyError(""Cannot set the value of Expression '%s' with invalid index '%s'"" % (self.cname(True), str(ndx))) exprdata.set_value(val)",if _ndx in self._data:,if _ndx < len(self._data):,0.930890716196107,0.8723360571509826,False
3744,"def write(self, *bits): for bit in bits: <IF_STMT> self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if not self.bytestream:,if self.bytenum >= len(self.bytestream):,0.9049674652837986,0.9001816649635144,False
3745,"def terminate_subprocess(proc, timeout=0.1, log=None): if proc.poll() is None: <IF_STMT> log.info('Sending SIGTERM to %r', proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info('Sending SIGKILL to %r', proc) proc.kill() return proc.returncode",if log:,if log:,0.935287868963945,0.9164531641034833,True
3746,"def mkpanel(color, rows, cols, tly, tlx): win = curses.newwin(rows, cols, tly, tlx) pan = panel.new_panel(win) if curses.has_colors(): <IF_STMT> fg = curses.COLOR_WHITE else: fg = curses.COLOR_BLACK bg = color curses.init_pair(color, fg, bg) win.bkgdset(ord(' '), curses.color_pair(color)) else: win.bkgdset(ord(' '), curses.A_BOLD) return pan",if color == curses.COLOR_BLUE:,if color == 'black':,0.6419507713424077,0.8661072626070159,False
3747,def all_words(filename): start_char = True for c in characters(filename): if start_char == True: word = '' <IF_STMT> word = c.lower() start_char = False else: pass elif c.isalnum(): word += c.lower() else: start_char = True yield word,if c.isalnum():,elif c.isalnum():,0.7546341249262877,0.8996480074924822,False
3748,"def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict: init_vars = tf.train.list_variables(path) tf_weights = {} ignore_name = ['Adafactor', 'global_step'] for name, shape in tqdm(init_vars, desc='converting tf checkpoint to dict'): skip_key = any([pat in name for pat in ignore_name]) <IF_STMT> continue array = tf.train.load_variable(path, name) tf_weights[name] = array return tf_weights",if skip_key:,if skip_key:,0.7987165491127446,0.9184043388013005,True
3749,"def app(scope, receive, send): while True: message = await receive() if message['type'] == 'websocket.connect': await send({'type': 'websocket.accept'}) <IF_STMT> pass elif message['type'] == 'websocket.disconnect': break",elif message['type'] == 'websocket.receive':,elif message['type'] == 'websocket.disconnect':,0.5917314874706985,0.7801270245332924,False
3750,def autoload(self): if self._app.config.THEME == 'auto': if sys.platform == 'darwin': <IF_STMT> theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: theme = self._app.config.THEME self.load_theme(theme),if get_osx_theme() == 1:,if theme == MacOSDark:,0.8751193312107907,0.8431339019329497,False
3751,"def example_reading_spec(self): data_fields = {'targets': tf.VarLenFeature(tf.int64)} <IF_STMT> data_fields['inputs'] = tf.VarLenFeature(tf.int64) if self.packed_length: if self.has_inputs: data_fields['inputs_segmentation'] = tf.VarLenFeature(tf.int64) data_fields['inputs_position'] = tf.VarLenFeature(tf.int64) data_fields['targets_segmentation'] = tf.VarLenFeature(tf.int64) data_fields['targets_position'] = tf.VarLenFeature(tf.int64) data_items_to_decoders = None return (data_fields, data_items_to_decoders)",if self.has_inputs:,if self.has_inputs:,0.9312851130367922,0.8866029039778043,True
3752,"def _prepare_travel_graph(self): for op in self.op_dict.values(): op.const = False if op.node.op in ['Const', 'Placeholder']: op.resolved = True <IF_STMT> op.const = True else: op.resolved = False",if op.node.op == 'Const':,"elif op.node.op in ['Const', 'Placeholder']:",0.6845464272681252,0.759907656827929,False
3753,"def get_filestream_file_items(self): data = {} fs_file_updates = self.get_filestream_file_updates() for k, v in six.iteritems(fs_file_updates): l = [] for d in v: offset = d.get('offset') content = d.get('content') assert offset is not None assert content is not None assert offset == 0 or offset == len(l), (k, v, l, d) <IF_STMT> l = [] l.extend(map(json.loads, content)) data[k] = l return data",if not offset:,if not l:,0.8789103906208915,0.9237682009314698,False
3754,"def _rewrite_exprs(self, table, what): from ibis.expr.analysis import substitute_parents what = util.promote_list(what) all_exprs = [] for expr in what: <IF_STMT> all_exprs.extend(expr.exprs()) else: bound_expr = ir.bind_expr(table, expr) all_exprs.append(bound_expr) return [substitute_parents(x, past_projection=False) for x in all_exprs]","if isinstance(expr, ir.ExprList):","if isinstance(expr, ibis.expr.analysis.Expr):",0.7848074674721104,0.8635707684233572,False
3755,"def _group_by_commit_and_time(self, hits): result = {} for hit in hits: source_hit = hit['_source'] key = '%s_%s' % (source_hit['commit_info']['id'], source_hit['datetime']) benchmark = self._benchmark_from_es_record(source_hit) <IF_STMT> result[key]['benchmarks'].append(benchmark) else: run_info = self._run_info_from_es_record(source_hit) run_info['benchmarks'] = [benchmark] result[key] = run_info return result",if key in result:,if benchmark:,0.7145492478434754,0.8996480074924822,False
3756,"def _build_index(self): self._index = {} for start_char, sorted_offsets in self._offsets.items(): self._index[start_char] = {} for i, offset in enumerate(sorted_offsets.get_offsets()): identifier = sorted_offsets.get_identifier_by_offset(offset) <IF_STMT> self._index[start_char][identifier[0:self.index_depth]] = i",if identifier[0:self.index_depth] not in self._index[start_char]:,if identifier:,0.8206023632188334,0.8531413606256201,False
3757,"def scan_resource_conf(self, conf): if 'properties' in conf: <IF_STMT> if 'exp' in conf['properties']['attributes']: if conf['properties']['attributes']['exp']: return CheckResult.PASSED return CheckResult.FAILED",if 'attributes' in conf['properties']:,if 'attributes' in conf['properties']:,0.5866302529608513,0.7098232254187811,True
3758,"def _PatchArtifact(self, artifact: rdf_artifacts.Artifact) -> rdf_artifacts.Artifact: """"""Patches artifact to not contain byte-string source attributes."""""" patched = False for source in artifact.sources: attributes = source.attributes.ToDict() unicode_attributes = compatibility.UnicodeJson(attributes) <IF_STMT> source.attributes = unicode_attributes patched = True if patched: self.DeleteArtifact(str(artifact.name)) self.WriteArtifact(artifact) return artifact",if attributes != unicode_attributes:,if unicode_attributes.get('byte_string') == 'true':,0.8378566555402042,0.8661072626070159,False
3759,"def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True) exit_code = c.wait() <IF_STMT> raise ClickException('%s: Editing failed!' % editor) except OSError as e: raise ClickException('%s: Editing failed: %s' % (editor, e))",if exit_code != 0:,if exit_code != 0:,0.9196862547848389,0.8923575006167597,True
3760,"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): <IF_STMT> if glyph.contours[i].segments[j].type == 'line': controlPointIndices[index] = 1 index += s[1] return controlPointIndices",if j < segmentCount:,if s[0] == va[j]:,0.8523977052890734,0.8627586293513119,False
3761,"def to_representation(self, value): old_social_string_fields = ['twitter', 'github', 'linkedIn'] request = self.context.get('request') show_old_format = request and is_deprecated(request.version, self.min_version) and (request.method == 'GET') if show_old_format: social = value.copy() for key in old_social_string_fields: <IF_STMT> social[key] = value[key][0] elif social.get(key) == []: social[key] = '' value = social return super(SocialField, self).to_representation(value)",if social.get(key):,"if isinstance(value[key], list):",0.9295279743525858,0.9042878500265974,False
3762,"def iter_raw_frames(path, packet_sizes, ctx): with open(path, 'rb') as f: for i, size in enumerate(packet_sizes): packet = Packet(size) read_size = f.readinto(packet) assert size assert read_size == size if not read_size: break for frame in ctx.decode(packet): yield frame while True: try: frames = ctx.decode(None) except EOFError: break for frame in frames: yield frame <IF_STMT> break",if not frames:,if i == len(packet_sizes):,0.9128713438540715,0.9126359660236022,False
3763,"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) if fname or (pname and fname): continue if pname not in already_test and '/' not in pname: already_test.append(pname) <IF_STMT> shadow_pkgs.add(pname) return shadow_pkgs",if is_shadowing(pname):,if pname not in shadow_pkgs:,0.9001065518054114,0.8591169759078797,False
3764,"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): if isinstance(v, torch.Tensor): v = v.item() <IF_STMT> v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics","if isinstance(v, dict):","elif isinstance(v, (list, tuple)):",0.6642241073897908,0.7965020533851944,False
3765,"def insert_resets(f): newsync = dict() for k, v in f.sync.items(): <IF_STMT> newsync[k] = insert_reset(ResetSignal(k), v) else: newsync[k] = v f.sync = newsync",if f.clock_domains[k].rst is not None:,"if isinstance(v, (list, tuple)):",0.81849744065595,0.760856626273165,False
3766,"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): if node is None: continue node_settings = node.get_addon(self.oauth_provider.short_name) <IF_STMT> continue if node_settings.external_account == external_account: yield node",if node_settings is None:,if node_settings is None:,0.8434053691947113,0.7709002428237395,True
3767,"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): if type(test.value) in self._const_types: <IF_STMT> continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",if not test.value:,"if not isinstance(body, ast.If):",0.861966855789246,0.7965020533851944,False
3768,"def flatten(self): result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) if len(data) < 100: self.limit = 0 <IF_STMT> data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",if self.reverse:,if self.reverse:,0.8322423687336319,0.9164531641034833,True
3769,"def compute(self, x, y=None, targets=None): if targets is None: targets = self.out_params in_params = list(self.in_x) if len(in_params) == 1: args = [x] else: args = list(zip(*x)) if y is None: pipe = self.pipe else: pipe = self.train_pipe <IF_STMT> args.append(y) else: args += list(zip(*y)) in_params += self.in_y return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)",if len(self.in_y) == 1:,if len(y) == 1:,0.8762551626013657,0.8944264839442453,False
3770,"def _import_top_module(self, name): for item in sys.path: <IF_STMT> module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) if module: return module return None","if isinstance(item, _StringType):","if isinstance(item, os.path.Dir):",0.8706376851865216,0.7947545184555568,False
3771,"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: <IF_STMT> return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)",if k.lower() == ikey:,if k.lower() == ikey:,0.8470013084999204,0.8474968231198384,True
3772,"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop('name')) action = 0 for key, val in props.get('options', {}).items(): if key == 'hooks': new_action = 0 for name, _val in val.items(): action = watcher.set_opt('hooks.%s' % name, _val) <IF_STMT> new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 return watcher.do_action(action)",if action == 1:,elif key == 'actions':,0.9238229510255549,0.8983343737277126,False
3773,"def OnBodyClick(self, event=None): try: c = self.c p = c.currentPosition() <IF_STMT> self.OnActivateBody(event=event) g.doHook('bodyclick2', c=c, p=p, v=p, event=event) except: g.es_event_exception('bodyclick')","if not g.doHook('bodyclick1', c=c, p=p, v=p, event=event):","if g.doHook('bodyclick1', c=c, p=p, v=p, event=event):",0.7215639135307124,0.6599790945055583,False
3774,"def _class_weights(spec: config.MetricsSpec) -> Optional[Dict[int, float]]: """"""Returns class weights associated with AggregationOptions at offset."""""" if spec.aggregate.HasField('top_k_list'): <IF_STMT> raise ValueError('class_weights are not supported when top_k_list used: spec={}'.format(spec)) return None return dict(spec.aggregate.class_weights) or None",if spec.aggregate.class_weights:,if spec.aggregate.top_k_list:,0.8999156238643151,0.8866029039778043,False
3775,def _is_perf_file(file_path): f = get_file(file_path) for line in f: if line[0] == '#': continue r = event_regexp.search(line) <IF_STMT> f.close() return True f.close() return False,if r:,if r:,0.7479166056144606,0.8466657105524215,True
3776,def _get_before_insertion_node(self): if self._nodes_stack.is_empty(): return None line = self._nodes_stack.parsed_until_line + 1 node = self._new_module.get_last_leaf() while True: parent = node.parent <IF_STMT> assert node.end_pos[0] <= line assert node.end_pos[1] == 0 or '\n' in self._prefix return node node = parent,"if parent.type in ('suite', 'file_input'):",if parent is None:,0.809189643158005,0.8555308664663046,False
3777,"def PyJsHoisted_parseClassRanges_(this, arguments, var=var): var = Scope({u'this': this, u'arguments': arguments}, var) var.registers([u'res']) pass if var.get(u'current')(Js(u']')): return Js([]) else: var.put(u'res', var.get(u'parseNonemptyClassRanges')()) <IF_STMT> var.get(u'bail')(Js(u'nonEmptyClassRanges')) return var.get(u'res')",if var.get(u'res').neg():,if var.get(u'current')(Js(u']')):,0.908112152803828,0.8466657105524215,False
3778,"def _recurse_children(self, offset): """"""Recurses thorugh the available children"""""" while offset < self.obj_offset + self.Length: item = obj.Object('VerStruct', offset=offset, vm=self.obj_vm, parent=self) <IF_STMT> raise StopIteration('Could not recover a key for a child at offset {0}'.format(item.obj_offset)) yield (item.get_key(), item.get_children()) offset = self.offset_pad(offset + item.Length) raise StopIteration('No children')",if item.Length < 1 or item.get_key() == None:,if item.get_key() is None:,0.7481248618340132,0.8780099567239787,False
3779,"def _adapt_types(self, descr): names = [] adapted_types = [] for col in descr: names.append(col[0]) impala_typename = col[1] typename = udf._impala_to_ibis_type[impala_typename.lower()] <IF_STMT> precision, scale = col[4:6] adapted_types.append(dt.Decimal(precision, scale)) else: adapted_types.append(typename) return (names, adapted_types)",if typename == 'decimal':,if typename == 'decimal':,0.9022288494841253,0.8336104423443033,True
3780,"def sniff(self, filename): try: <IF_STMT> with tarfile.open(filename, 'r') as temptar: for f in temptar: if not f.isfile(): continue if f.name.endswith('.fast5'): return True else: return False except Exception as e: log.warning('%s, sniff Exception: %s', self, e) return False",if filename and tarfile.is_tarfile(filename):,if os.path.isfile(filename):,0.8955740655157713,0.9024521756077707,False
3781,"def getValue(self): if getattr(self.object, 'type', '') != 'CURVE': return BezierSpline() evaluatedObject = getEvaluatedID(self.object) bSplines = evaluatedObject.data.splines if len(bSplines) > 0: spline = createSplineFromBlenderSpline(bSplines[0]) if spline is not None: <IF_STMT> spline.transform(evaluatedObject.matrix_world) return spline return BezierSpline()",if self.useWorldSpace:,if evaluatedObject.matrix_world is not None:,0.9035488228190572,0.8200123297196334,False
3782,"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if '&' in text: text = text.replace('&', '&amp;') <IF_STMT> text = text.replace('>', '&gt;') if '<' in text: text = text.replace('<', '&lt;') if '""' in text: text = text.replace('""', '&quot;') if ""'"" in text: text = text.replace(""'"", '&quot;') if newline: if '\n' in text: text = text.replace('\n', '<br>') return text",if '>' in text:,if '>' in text:,0.9070775373802695,0.9084940438173679,True
3783,"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: if hasattr(e, 'code'): <IF_STMT> return 3 if e.code == 501: return 1 raise return 2",if e.code == 405:,if e.code == 502:,0.8620710691084262,0.7886336751695258,False
3784,"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) <IF_STMT> ctx.close_path()",elif code == Path.CLOSEPOLY:,elif code == Path.CLOSE:,0.6760769575318333,0.8693386622895124,False
3785,"def called_by_shrinker(): frame = sys._getframe(0) while frame: fname = frame.f_globals.get('__file__', '') <IF_STMT> return True frame = frame.f_back return False",if os.path.basename(fname) == 'shrinker.py':,if fname.startswith('shrinker_'):,0.7563905004465189,0.803154665668484,False
3786,"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == 'append': <IF_STMT> sys.path.append(s) elif s != sys.path[0]: sys.path.insert(0, s)",if s not in sys.path:,if s not in sys.path:,0.8535571407997647,0.7178970818142898,True
3787,"def get_instances(self, region: str, vpc: str): try: await self._cache_instances(region) return [instance for instance in self._instances_cache[region] <IF_STMT>] except Exception as e: print_exception(f'Failed to get RDS instances: {e}') return []",if instance['VpcId'] == vpc,if instance['vpc_id'] == vpc,0.831264582394906,0.8105932471967202,False
3788,def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: if page.relations.disambiguation_links_norm is not None: all_disambiguations.extend(page.relations.disambiguation_links_norm) <IF_STMT> all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),if page.relations.disambiguation_links is not None:,if page.relations.disambiguation_links is not None:,0.7842553120008348,0.6907573115737006,True
3789,"def __str__(self, prefix='', printElemNumber=0): res = '' cnt = 0 for e in self.options_: elm = '' <IF_STMT> elm = '(%d)' % cnt res += prefix + 'options%s <\n' % elm res += e.__str__(prefix + '  ', printElemNumber) res += prefix + '>\n' cnt += 1 return res",if printElemNumber:,if printElemNumber:,0.6817067875277508,0.9253742688467129,True
3790,"def pre_save_task(self, task, credentials, verrors): if task['attributes']['encryption'] not in (None, '', 'AES256'): verrors.add('encryption', 'Encryption should be null or ""AES256""') if not credentials['attributes'].get('skip_region', False): <IF_STMT> response = await self.middleware.run_in_thread(self._get_client(credentials).get_bucket_location, Bucket=task['attributes']['bucket']) task['attributes']['region'] = response['LocationConstraint'] or 'us-east-1'","if not credentials['attributes'].get('region', '').strip():",if task['attributes']['region'] is None:,0.9219379115245602,0.8431339019329497,False
3791,"def get_best_config_reward(self): """"""Returns the best configuration found so far, as well as the reward associated with this best config."""""" with self.LOCK: <IF_STMT> config_pkl = max(self._results, key=self._results.get) return (pickle.loads(config_pkl), self._results[config_pkl]) else: return (dict(), self._reward_while_pending())",if self._results:,if self._results:,0.8808845909061952,0.8901732118131125,True
3792,"def parse_setup_cfg(self): if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: <IF_STMT> contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) if not parsed: return {} return parsed return {}",if six.PY2:,if self.setup_cfg.exists():,0.8533539232563935,0.9051034981560222,False
3793,"def readall(read_fn, sz): buff = b'' have = 0 while have < sz: chunk = (yield from read_fn(sz - have)) have += len(chunk) buff += chunk <IF_STMT> raise TTransportException(TTransportException.END_OF_FILE, 'End of file reading from transport') return buff",if len(chunk) == 0:,if have == sz:,0.838474576593842,0.8555308664663046,False
3794,"def _get_use_previous(f): if isinstance(f, AggregationFeature) and f.use_previous is not None: <IF_STMT> return ('', -1) else: unit = list(f.use_previous.times.keys())[0] value = f.use_previous.times[unit] return (unit, value) else: return ('', -1)",if len(f.use_previous.times.keys()) > 1:,if f.use_previous.times is None:,0.8943828619647983,0.8105932471967202,False
3795,"def istrue(self): try: return self._istrue() except Exception: self.exc = sys.exc_info() <IF_STMT> msg = [' ' * (self.exc[1].offset + 4) + '^'] msg.append('SyntaxError: invalid syntax') else: msg = traceback.format_exception_only(*self.exc[:2]) pytest.fail('Error evaluating %r expression\n%s\n%s' % (self.name, self.expr, '\n'.join(msg)), pytrace=False)","if isinstance(self.exc[1], SyntaxError):",if len(self.exc) > 2:,0.8625221441076938,0.8555308664663046,False
3796,"def wait_for_crm_operation(operation, crm): """"""Poll for cloud resource manager operation until finished."""""" logger.info('wait_for_crm_operation: Waiting for operation {} to finish...'.format(operation)) for _ in range(MAX_POLLS): result = crm.operations().get(name=operation['name']).execute() <IF_STMT> raise Exception(result['error']) if 'done' in result and result['done']: logger.info('wait_for_crm_operation: Operation done.') break time.sleep(POLL_INTERVAL) return result",if 'error' in result:,if 'error' in result:,0.9372134160197889,0.8692960007731574,True
3797,"def cb_blob_detail_from_elem_and_buf(self, elem, buf): if elem.get('lang') != buf.lang: return '%s Code in %s' % (elem.get('lang'), buf.path) else: dir, base = os.path.split(buf.path) <IF_STMT> return '%s (%s)' % (base, dir) else: return base",if dir:,if dir:,0.6719352413344608,0.8827916928185874,True
3798,"def removedir(self, path): _path = self.validatepath(path) if _path == '/': raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) <IF_STMT> if self.isfile(path): raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise",if code == '550':,if code == 404:,0.9126707770670665,0.8555308664663046,False
3799,"def p_clause(self, node, position): if isinstance(node, Graph): self.subjectDone(node) <IF_STMT> self.write(' ') self.write('{') self.depth += 1 serializer = N3Serializer(node, parent=self) serializer.serialize(self.stream) self.depth -= 1 self.write(self.indent() + '}') return True else: return False",if position is OBJECT:,if self.depth > 0:,0.5713608766358873,0.828399516355805,False
3800,"def get_default_shell_info(shell_name=None, settings=None): if not shell_name: settings = settings or load_settings(lazy=True) shell_name = settings.get('shell') if shell_name: return (shell_name, None) shell_path = os.environ.get('SHELL') <IF_STMT> shell_name = basepath(shell_path) else: shell_name = DEFAULT_SHELL return (shell_name, shell_path) return (shell_name, None)",if shell_path:,if shell_path:,0.8293462266479243,0.8996480074924822,True
3801,"def GetCategory(self, pidls): ret = [] for pidl in pidls: val = self.sf.GetDetailsEx(pidl, PKEY_Sample_AreaSize) val = int(val) if val < 255 // 3: cid = IDS_SMALL <IF_STMT> cid = IDS_MEDIUM else: cid = IDS_LARGE ret.append(cid) return ret",elif val < 2 * 255 // 3:,elif val > 255 // 3:,0.9358500922225523,0.8132493528194856,False
3802,"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) if item[0]: typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] <IF_STMT> typ = item[3] val = item[3] yield Token(typ, val)",elif item[3]:,elif item[3]:,0.692825428813434,0.9184043388013005,True
3803,"def add_package_declarations(generated_root_path): file_names = os.listdir(generated_root_path) for file_name in file_names: <IF_STMT> continue full_name = os.path.join(generated_root_path, file_name) add_package(full_name)",if not file_name.endswith('.java'):,if file_name.endswith('.py'):,0.7959175502660759,0.762465858623486,False
3804,"def _call_with_retry(out, retry, retry_wait, method, *args, **kwargs): for counter in range(retry + 1): try: return method(*args, **kwargs) except (NotFoundException, ForbiddenException, AuthenticationException, RequestErrorException): raise except ConanException as exc: <IF_STMT> raise else: if out: out.error(exc) out.info('Waiting %d seconds to retry...' % retry_wait) time.sleep(retry_wait)",if counter == retry:,if counter == retry:,0.8618848117421506,0.8692960007731574,True
3805,"def to_wburl_str(url, type=BaseWbUrl.LATEST_REPLAY, mod='', timestamp='', end_timestamp=''): if WbUrl.is_query_type(type): tsmod = '' <IF_STMT> tsmod += mod + '/' tsmod += timestamp tsmod += '*' tsmod += end_timestamp tsmod += '/' + url if type == BaseWbUrl.URL_QUERY: tsmod += '*' return tsmod else: tsmod = timestamp + mod if len(tsmod) > 0: return tsmod + '/' + url else: return url",if mod:,if len(tsmod) > 0:,0.8216357671266732,0.9084940438173679,False
3806,"def _configured_ploidy(items): ploidies = collections.defaultdict(set) for data in items: ploidy = dd.get_ploidy(data) <IF_STMT> for k, v in ploidy.items(): ploidies[k].add(v) else: ploidies['default'].add(ploidy) out = {} for k, vs in ploidies.items(): assert len(vs) == 1, 'Multiple ploidies set for group calling: %s %s' % (k, list(vs)) out[k] = vs.pop() return out","if isinstance(ploidy, dict):",if ploidy:,0.8388498753757363,0.926934323706186,False
3807,"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] <IF_STMT> if self.isRoomSame(user.room): hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage('left-notification').format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",if user.room:,if user.isLeftNotification():,0.9390634572164683,0.9051034981560222,False
3808,"def _thd_cleanup_instance(self): container_name = self.getContainerName() instances = self.client.containers(all=1, filters=dict(name=container_name)) for instance in instances: <IF_STMT> continue try: self.client.remove_container(instance['Id'], v=True, force=True) except NotFound: pass except docker.errors.APIError as e: if 'Conflict operation on container' not in str(e): raise",if ''.join(instance['Names']).strip('/') != container_name:,if instance['Name'] == container_name:,0.8790628704879728,0.8474968231198384,False
3809,"def handle_ctcp(self, conn, evt): args = evt.arguments() source = evt.source().split('!')[0] if args: if args[0] == 'VERSION': conn.ctcp_reply(source, 'VERSION ' + BOT_VERSION) <IF_STMT> conn.ctcp_reply(source, 'PING') elif args[0] == 'CLIENTINFO': conn.ctcp_reply(source, 'CLIENTINFO PING VERSION CLIENTINFO')",elif args[0] == 'PING':,elif args[0] == 'PING':,0.904945010215922,0.8385130047130208,True
3810,"def new_func(self, *args, **kwargs): obj = self.obj_ref() attr = self.attr if obj is not None: args = tuple((TrackedValue.make(obj, attr, arg) for arg in args)) <IF_STMT> kwargs = {key: TrackedValue.make(obj, attr, value) for key, value in iteritems(kwargs)} result = func(self, *args, **kwargs) self._changed_() return result",if kwargs:,if kwargs is not None:,0.6541025567147983,0.8591169759078797,False
3811,"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): name = target.id <IF_STMT> doc = find_doc_for(target, body_lines) if doc is not None: variables[name] = doc elif isinstance(target, ast.Tuple): for e in target.elts: add_doc(e, variables, body_lines)",if name not in variables:,if name not in variables:,0.6329302699024488,0.8148691130388024,True
3812,"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: if tp == 'write': out.write(msg) elif tp == 'flush': out.flush() <IF_STMT> out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",elif tp == 'write_flush':,elif tp == 'write':,0.9224139118500542,0.8627586293513119,False
3813,"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) <IF_STMT> continue if os.path.islink(p): continue if os.path.isdir(p): res += get_dir(p) else: res.append(p) return res",if skip_file(fname):,if not os.path.isfile(pth):,0.9298115240080234,0.8592899528284996,False
3814,"def _list_outputs(self): outputs = super(VolSymm, self)._list_outputs() if os.path.exists(outputs['trans_file']): <IF_STMT> outputs['output_grid'] = re.sub('.(nlxfm|xfm)$', '_grid_0.mnc', outputs['trans_file']) return outputs","if 'grid' in open(outputs['trans_file'], 'r').read():",if 'output_grid' not in outputs:,0.8263988919896271,0.6383240325919926,False
3815,"def _set_texture(self, texture): if texture.id is not self._texture.id: self._group = SpriteGroup(texture, self._group.blend_src, self._group.blend_dest, self._group.parent) <IF_STMT> self._vertex_list.tex_coords[:] = texture.tex_coords else: self._vertex_list.delete() self._texture = texture self._create_vertex_list() else: self._vertex_list.tex_coords[:] = texture.tex_coords self._texture = texture",if self._batch is None:,if self._vertex_list.is_empty():,0.6680736821101644,0.8827916928185874,False
3816,"def got_result(result): deployment = self.persistence_service.get() for node in deployment.nodes: <IF_STMT> dataset_ids = [(m.dataset.deleted, m.dataset.dataset_id) for m in node.manifestations.values()] self.assertIn((True, expected_dataset_id), dataset_ids) break else: self.fail('Node not found. {}'.format(node.uuid))","if same_node(node, origin):",if node.uuid == result.uuid:,0.8645855142236795,0.8038019482772603,False
3817,"def check_result(result, func, arguments): if check_warning(result) and result.value != ReturnCode.WARN_NODATA: log.warning(UcanWarning(result, func, arguments)) elif check_error(result): <IF_STMT> raise UcanCmdError(result, func, arguments) else: raise UcanError(result, func, arguments) return result",if check_error_cmd(result):,if result.value == ReturnCode.CMD_ERROR:,0.852352693167329,0.8038019482772603,False
3818,"def _compress_and_sort_bdg_files(out_dir, data): for fn in glob.glob(os.path.join(out_dir, '*bdg')): out_file = fn + '.gz' <IF_STMT> continue bedtools = config_utils.get_program('bedtools', data) with file_transaction(out_file) as tx_out_file: cmd = f'sort -k1,1 -k2,2n {fn} | bgzip -c > {tx_out_file}' message = f'Compressing and sorting {fn}.' do.run(cmd, message)",if utils.file_exists(out_file):,if not os.path.exists(out_file):,0.6464916261402847,0.8928756684056034,False
3819,"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: if ha_tools_debug: print('killing %s' % member) proc = hosts[member]['proc'] <IF_STMT> os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print('%s already dead?' % member)","if sys.platform in ('win32', 'cygwin'):",if sig == SIGINT:,0.6525799707113648,0.8385130047130208,False
3820,"def get_top_level_stats(self): for func, (cc, nc, tt, ct, callers) in self.stats.items(): self.total_calls += nc self.prim_calls += cc self.total_tt += tt <IF_STMT> self.top_level[func] = None if len(func_std_string(func)) > self.max_name_len: self.max_name_len = len(func_std_string(func))","if ('jprofile', 0, 'profiler') in callers:",if func not in self.top_level:,0.658587620905458,0.8036431532733102,False
3821,"def __str__(self): """"""Only keeps the True values."""""" result = ['SlicingSpec('] if self.entire_dataset: result.append(' Entire dataset,') if self.by_class: if isinstance(self.by_class, Iterable): result.append(' Into classes %s,' % self.by_class) <IF_STMT> result.append(' Up to class %d,' % self.by_class) else: result.append(' By classes,') if self.by_percentiles: result.append(' By percentiles,') if self.by_classification_correctness: result.append(' By classification correctness,') result.append(')') return '\n'.join(result)","elif isinstance(self.by_class, int):","elif isinstance(self.by_class, int):",0.953115508387273,0.9134996171406936,True
3822,"def save_params(self): if self._save_controller: if not os.path.exists(self._save_controller): os.makedirs(self._save_controller) output_dir = self._save_controller else: <IF_STMT> os.makedirs('./.rlnas_controller') output_dir = './.rlnas_controller' with open(os.path.join(output_dir, 'rlnas.params'), 'wb') as f: pickle.dump(self._params_dict, f) _logger.debug('Save params done')",if not os.path.exists('./.rlnas_controller'):,if not os.path.exists('./.rlnas_controller'):,0.6737465127487174,0.8390782502060267,True
3823,"def unexport(self, pin): with self._lock: self._pin_refs[pin] -= 1 <IF_STMT> with io.open(self.path('unexport'), 'wb') as f: f.write(str(pin).encode('ascii'))",if self._pin_refs[pin] == 0:,if self._pin_refs[pin] == 0:,0.5296995374126621,0.6540585844910979,True
3824,"def emit(self, type, info=None): ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: if type in self.__proxy_properties__: self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev]) <IF_STMT> self._session.send_command('INVOKE', self._id, '_emit_at_proxy', [ev])",elif type in self.__event_types_at_proxy:,elif type in self.__proxy_properties__:,0.9047389013962888,0.8169276475307028,False
3825,"def __call__(self, params): all_errs = {} for handler in self.handlers: out_headers, res, errs = handler(params) all_errs.update(errs) <IF_STMT> return (out_headers, res, all_errs) return (None, None, all_errs)",if res is not None:,if out_headers:,0.7499246138582196,0.8531413606256201,False
3826,def await_test_end(self): iterations = 0 while True: <IF_STMT> self.log.debug('Await: iteration limit reached') return status = self.master.get_status() if status.get('status') == 'ENDED': return iterations += 1 time.sleep(1.0),if iterations > 100:,if iterations >= self.max_iterations:,0.826812398376549,0.7886336751695258,False
3827,"def _load(self, path: str): ds = DataSet() with open(path, 'r', encoding='utf-8') as f: for line in f: line = line.strip() <IF_STMT> parts = line.split('\t') raw_words1 = parts[1] raw_words2 = parts[2] target = parts[0] if raw_words1 and raw_words2 and target: ds.append(Instance(raw_words1=raw_words1, raw_words2=raw_words2, target=target)) return ds",if line:,if line:,0.8134663077695374,0.9184043388013005,True
3828,"def avatar_delete(event_id, speaker_id): if request.method == 'DELETE': speaker = DataGetter.get_speakers(event_id).filter_by(user_id=login.current_user.id, id=speaker_id).first() <IF_STMT> speaker.photo = '' speaker.small = '' speaker.thumbnail = '' speaker.icon = '' save_to_db(speaker) return jsonify({'status': 'ok'}) else: abort(403)",if speaker:,if speaker:,0.6420793033689719,0.8787142254774354,True
3829,"def getline(filename, lineno, *args, **kwargs): line = py2exe_getline(filename, lineno, *args, **kwargs) if not line: try: with open(filename, 'rb') as f: for i, line in enumerate(f): line = line.decode('utf-8') <IF_STMT> break else: line = '' except (IOError, OSError): line = '' return line",if lineno == i + 1:,if i == len(line):,0.625143356652866,0.8723360571509826,False
3830,"def write(self, data): if not isinstance(data, (bytes, bytearray, memoryview)): raise TypeError('data argument must be byte-ish (%r)', type(data)) if not data: return if self._conn_lost: <IF_STMT> logger.warning('socket.send() raised exception.') self._conn_lost += 1 return if not self._buffer: self._loop.add_writer(self._sock_fd, self._write_ready) self._buffer.extend(data) self._maybe_pause_protocol()",if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:,if self._loop.get_exception():,0.9238808822258944,0.9051034981560222,False
3831,"def _get_x_for_y(self, xValue, x, y): if not self.xmlMap: return 0 x_value = str(xValue) for anime in self.xmlMap.findall('anime'): try: <IF_STMT> return int(anime.get(y, 0)) except ValueError as e: continue return 0","if anime.get(x, False) == x_value:","if anime.get(x, 0) == x_value:",0.8714423123865722,0.7909601595885504,False
3832,"def _RewriteModinfo(self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None): new_modinfo = '' for line in modinfo.split('\x00'): if not line: continue if to_remove and line.split('=')[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split('=')[0]) <IF_STMT> line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + '\x00' return new_modinfo",if line.startswith('vermagic'):,if obj_kernel_version is not None:,0.9123592648288092,0.8559898693114286,False
3833,"def _score(self, X, y): for col in self.cols: X[col] = X[col].map(self.mapping[col]) <IF_STMT> random_state_generator = check_random_state(self.random_state) X[col] = X[col] * random_state_generator.normal(1.0, self.sigma, X[col].shape[0]) return X",if self.randomized and y is not None:,if self.random_state is not None:,0.6744539911654809,0.7506346798217074,False
3834,"def onMouseWheel(self, event): if self.selectedHuman.isVisible(): zoomOut = event.wheelDelta > 0 <IF_STMT> zoomOut = not zoomOut if event.x is not None: self.modelCamera.mousePickHumanCenter(event.x, event.y) if zoomOut: self.zoomOut() else: self.zoomIn()",if self.getSetting('invertMouseWheel'):,if self.selectedHuman.isVisible():,0.866501565902424,0.8645707301556367,False
3835,"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == 'i386': reg = emu.getRegister(envi.archs.i386.REG_EDI) <IF_STMT> reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",elif self.arch == 'amd64':,elif self.arch == 'amd64':,0.8847746851970184,0.8474968231198384,True
3836,"def callback(actions, form, tablename=None): if actions: if tablename and isinstance(actions, dict): actions = actions.get(tablename, []) <IF_STMT> actions = [actions] [action(form) for action in actions]","if not isinstance(actions, (list, tuple)):","elif isinstance(actions, list):",0.7241322550029005,0.8120341702859789,False
3837,"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): if bigger_than_3_only and less_than_7_only and (i == 4): continue if bigger_than_3_only and i <= 3: continue if less_than_7_only and i >= 7: continue <IF_STMT> continue result.append(i) return result",if even_only and i % 2 != 0:,if even_only and i % 2 != 0:,0.8788898969053244,0.7851558643723873,True
3838,"def set_trial_values(self, trial_id: int, values: Sequence[float]) -> None: with self._lock: cached_trial = self._get_cached_trial(trial_id) <IF_STMT> self._check_trial_is_updatable(cached_trial) updates = self._get_updates(trial_id) cached_trial.values = values updates.values = values return self._backend._update_trial(trial_id, values=values)",if cached_trial is not None:,if cached_trial:,0.735056276990015,0.8645707301556367,False
3839,"def _get_label_format(self, workunit): for label, label_format in self.LABEL_FORMATTING.items(): if workunit.has_label(label): return label_format if workunit.parent: label_format = self._get_label_format(workunit.parent) if label_format == LabelFormat.CHILD_DOT: return LabelFormat.DOT <IF_STMT> return LabelFormat.SUPPRESS return LabelFormat.FULL",if label_format == LabelFormat.CHILD_SUPPRESS:,elif label_format == LabelFormat.CHILD_SUPPRESS:,0.8269416261693304,0.8105932471967202,False
3840,"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) <IF_STMT> return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):,if expiration.tzinfo == utc:,0.6717212855661729,0.8169276475307028,False
3841,"def _manage_torrent_cache(self): """"""Carry tracker/peer/file lists over to new torrent list"""""" for torrent in self._torrent_cache: new_torrent = rtorrentlib.common.find_torrent(torrent.info_hash, self.torrents) <IF_STMT> new_torrent.files = torrent.files new_torrent.peers = torrent.peers new_torrent.trackers = torrent.trackers self._torrent_cache = self.torrents",if new_torrent is not None:,if new_torrent:,0.7132497584990048,0.8827916928185874,False
3842,"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) if target: <IF_STMT> target = _load_regions(target) else: target = [target] return target","if isinstance(target, six.string_types) and os.path.isfile(target):","if isinstance(target, list):",0.6926041740663877,0.8783650674919876,False
3843,def _get_stdout(self): while True: BUFFER_SIZE = 1000 stdout_buffer = self.kernel.process.GetSTDOUT(BUFFER_SIZE) <IF_STMT> break yield stdout_buffer,if len(stdout_buffer) == 0:,if not stdout_buffer:,0.5365426622438161,0.6739047062564734,False
3844,"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: <IF_STMT> ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",if key == qkey:,if key == qkey:,0.9301185344074534,0.8806615362338783,True
3845,"def test_expect_setecho_off(self): """"""This tests that echo may be toggled off."""""" p = pexpect.spawn('cat', echo=True, timeout=5) try: self._expect_echo_toggle(p) except IOError: <IF_STMT> if hasattr(unittest, 'SkipTest'): raise unittest.SkipTest('Not supported on this platform.') return 'skip' raise",if sys.platform.lower().startswith('sunos'):,if sys.platform.lower() == 'win32':,0.6318673596309332,0.8336104423443033,False
3846,"def _resolve_relative_config(dir, config): icon = config.get('icon') if icon: <IF_STMT> icon = File(icon) else: icon = dir.resolve_file(icon) document_root = config.get('document_root') if document_root: if zim.fs.isabs(document_root) or not dir: document_root = Dir(document_root) else: document_root = dir.resolve_dir(document_root) return (icon, document_root)",if zim.fs.isabs(icon) or not dir:,if zim.fs.isabs(icon) or not dir:,0.7103621276501758,0.8294838585473985,True
3847,"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): <IF_STMT> if ent_id in res: pass else: res.append(ent_id) return res",if descriptor in ent_desc:,if ent_desc.get('type') == 'provider':,0.8792279318535243,0.7886336751695258,False
3848,"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) if timeout >= 0: deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) <IF_STMT> n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",if timeout < 0:,if timeout == 0:,0.7493119884196812,0.9001816649635144,False
3849,"def banned(): if request.endpoint == 'views.themes': return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() if user and user.banned: return (render_template('errors/403.html', error='You have been banned from this CTF'), 403) <IF_STMT> return (render_template('errors/403.html', error='Your team has been banned from this CTF'), 403)",if team and team.banned:,if team and team.banned:,0.9128223117421506,0.8692960007731574,True
3850,"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) if msg: self.on_message(msg) return True self.close() except socket.error as err: <IF_STMT> pass else: self.on_error(err) return False","if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK):",if err.args[0] == EWOULDBLOCK or err.args[0] == EWOULDBLOCK:,0.6771761630104086,0.7221052041889016,False
3851,"def update_topic_attr_as_not(modeladmin, request, queryset, attr): for topic in queryset: if attr == 'sticky': topic.sticky = not topic.sticky elif attr == 'closed': topic.closed = not topic.closed <IF_STMT> topic.hidden = not topic.hidden topic.save()",elif attr == 'hidden':,elif attr == 'hidden':,0.7516341073824171,0.828399516355805,True
3852,"def Startprobe(self, q): while not self.finished: try: sniff(iface=self.interface, count=10, prn=lambda x: q.put(x)) except: pass <IF_STMT> break",if self.finished:,if self.stop_loop:,0.6295740086883921,0.8193228857188178,False
3853,"def _maybe_female(self, path_elements, female, strict): if female: <IF_STMT> elements = path_elements + ['female'] try: return self._get_file(elements, '.png', strict=strict) except ValueError: if strict: raise elif strict: raise ValueError('Pokemon %s has no gender differences' % self.species_id) return self._get_file(path_elements, '.png', strict=strict)",if self.has_gender_differences:,if female:,0.8989112173905676,0.9051034981560222,False
3854,def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split('\n') ind = 1 start = 0 while ind <= len(strings): <IF_STMT> ind += 1 else: if start < ind: ans.append('\n'.join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if ':' in line and len(line) > 0: lines = line.split(':') d[lines[0]] = lines[1].strip() return d,if ind < len(strings) and strings[ind].startswith(' '):,if strings[ind] == '\n':,0.733093212385792,0.914208565914368,False
3855,"def _send_with_auth(self, req_kwargs, desired_auth, rsession): if desired_auth.oauth: <IF_STMT> self._oauth_creds.refresh(httplib2.Http()) req_kwargs['headers'] = req_kwargs.get('headers', {}) req_kwargs['headers']['Authorization'] = 'Bearer ' + self._oauth_creds.access_token return rsession.request(**req_kwargs)",if self._oauth_creds.access_token_expired:,if not self._oauth_creds.refreshed:,0.7533578828651228,0.7848518349390632,False
3856,"def parse_search_response(json_data): """"""Construct response for any input"""""" if json_data is None: return {'error': 'Error parsing empty search engine response'} try: return json.loads(json_data) except json.JSONDecodeError: logger.exception('Error parsing search engine response') m = re_pre.search(json_data) <IF_STMT> return {'error': 'Error parsing search engine response'} error = web.htmlunquote(m.group(1)) solr_error = 'org.apache.lucene.queryParser.ParseException: ' if error.startswith(solr_error): error = error[len(solr_error):] return {'error': error}",if m is None:,if not m:,0.9403808046460795,0.9182210682909737,False
3857,"def wrapper(*args, **kws): missing = [] saved = getattr(warnings, '__warningregistry__', missing).copy() try: return func(*args, **kws) finally: <IF_STMT> try: del warnings.__warningregistry__ except AttributeError: pass else: warnings.__warningregistry__ = saved",if saved is missing:,if saved is missing:,0.829063031924694,0.8038019482772603,True
3858,"def parse_expression(self): """"""Return string containing command to run."""""" expression_el = self.root.find('expression') if expression_el is not None: expression_type = expression_el.get('type') <IF_STMT> raise Exception('Unknown expression type [%s] encountered' % expression_type) return expression_el.text return None",if expression_type != 'ecma5.1':,if expression_type is not None and expression_type != 'command':,0.5825865000366832,0.7252510836426073,False
3859,"def test_geocode(): count = 0 found = False for tweet in T.search(None, geocode='40.7484,-73.9857,1mi'): <IF_STMT> found = True break if count > 500: break count += 1 assert found",if (tweet['place'] or {}).get('name') == 'Manhattan':,if tweet.text == geocode:,0.8821455870553302,0.8105932471967202,False
3860,"def __init__(self, name: Optional[str]=None, order: int=0): if name is None: <IF_STMT> name = 'std_dev' elif order == 1: name = 'sample_std_dev' else: name = f'std_dev{order})' super().__init__(name=name, order=order) self.order = order",if order == 0:,if order == 0:,0.8736291517995267,0.8228500218338367,True
3861,"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() <IF_STMT> return -1 elif a == b: return 0 else: raise TypeError('expected date or datetime object') return 1",if a < b:,if a > b:,0.6361214177643799,0.8385130047130208,False
3862,def run(self): tid = self.ident try: with self._lock: _GUIS[tid] = self self._state(True) self.new_mail_notifications(summarize=True) loop_count = 0 while self._sock: loop_count += 1 self._select_sleep(1) self.change_state() <IF_STMT> self.new_mail_notifications() finally: del _GUIS[tid],if loop_count % 5 == 0:,if loop_count == 10:,0.785709034255921,0.8105932471967202,False
3863,"def __cache_dimension_masks(self, *args): if len(self.masks) == 0: for m1 in args: batch_size, emb_dim, h, w = m1.size() <IF_STMT> mask = self.feat_size_w_mask(h, m1) self.masks[h] = mask",if h not in self.masks:,if batch_size == 0:,0.6275813142902472,0.7886336751695258,False
3864,"def __call__(self, *flattened_representation): unflattened_representation = [] for index, subtree in self.children: <IF_STMT> unflattened_representation.append(flattened_representation[index]) else: sub_representation = flattened_representation[index] unflattened_representation.append(subtree(*sub_representation)) return self._cls(*unflattened_representation, **self._kwargs)",if subtree is None:,if subtree is None:,0.7211268596122125,0.7498810286408993,True
3865,"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] if y == 0: y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = (x, y, z) <IF_STMT> d.dismiss('Goto')",if event.num_clicks == 2:,if x == 0 and y == 64 and z == 0:,0.5833653210497078,0.6844861471686758,False
3866,"def get_mapped_input_keysequences(self, mode='global', prefix=u''): globalmaps, modemaps = self.get_keybindings(mode) candidates = list(globalmaps.keys()) + list(modemaps.keys()) if prefix is not None: prefixes = prefix + ' ' cand = [c for c in candidates if c.startswith(prefixes)] <IF_STMT> candidates = cand + [prefix] else: candidates = cand return candidates",if prefix in candidates:,if prefix:,0.8341173725312674,0.9202663016973823,False
3867,"def _set_length(self, length): with self._cond: self._length = length <IF_STMT> self._ready = True self._cond.notify() del self._cache[self._job]",if self._index == self._length:,if self._length == 0:,0.6426574128705375,0.6540585844910979,False
3868,"def _pct_encoded_replace_unreserved(mo): try: i = int(mo.group(1), 16) <IF_STMT> return chr(i) else: return mo.group().upper() except ValueError: return mo.group()",if _unreserved[i]:,if i > 0:,0.592028044570322,0.693395566222006,False
3869,"def is_open(self): if self.signup_code: return True elif self.signup_code_present: <IF_STMT> messages.add_message(self.request, self.messages['invalid_signup_code']['level'], self.messages['invalid_signup_code']['text'].format(**{'code': self.get_code()})) return settings.ACCOUNT_OPEN_SIGNUP",if self.messages.get('invalid_signup_code'):,if self.messages['invalid_signup_code']['level'] != settings.ACCOUNT_OPEN_SIGNUP:,0.5608800197174186,0.6540585844910979,False
3870,"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: <IF_STMT> field_value = member[1] elif member[0] == 'wildcards': wildcards = member[1] if key == 'nw_src': field_value = test.nw_src_to_str(wildcards, field_value) elif key == 'nw_dst': field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",if member[0] == key:,if member[0] == 'field':,0.9216572945206808,0.8902056737869248,False
3871,"def move_sender_strings_to_sender_model(apps, schema_editor): sender_model = apps.get_model('documents', 'Sender') document_model = apps.get_model('documents', 'Document') for document in document_model.objects.all(): <IF_STMT> DOCUMENT_SENDER_MAP[document.pk], created = sender_model.objects.get_or_create(name=document.sender, defaults={'slug': slugify(document.sender)})",if document.sender:,if document.sender in DOCUMENT_SENDER_MAP:,0.845843941486921,0.760856626273165,False
3872,"def compute_output_shape(self, input_shape): if None not in input_shape[1:]: <IF_STMT> total = np.prod(input_shape[2:4]) * self.num_anchors else: total = np.prod(input_shape[1:3]) * self.num_anchors return (input_shape[0], total, 4) else: return (input_shape[0], None, 4)",if keras.backend.image_data_format() == 'channels_first':,if self.use_multi_anchors:,0.8304217025633374,0.8743414417652072,False
3873,"def decompress(self, value): if value: if type(value) == PhoneNumber: <IF_STMT> return ['+%d' % value.country_code, national_significant_number(value)] else: return value.split('.') return [None, '']",if value.country_code and value.national_number:,if value.country_code:,0.638919268807439,0.8232490471721702,False
3874,"def ignore(self, other): if isinstance(other, Suppress): <IF_STMT> super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",if other not in self.ignoreExprs:,if self.ignoreExprs is not None:,0.8723589109285838,0.7685107079449489,False
3875,"def mkdir(self, mode=511, parents=False, exist_ok=False): if self._closed: self._raise_closed() if not parents: try: self._accessor.mkdir(self, mode) except FileExistsError: <IF_STMT> raise else: try: self._accessor.mkdir(self, mode) except FileExistsError: if not exist_ok or not self.is_dir(): raise except OSError as e: if e.errno != ENOENT: raise self.parent.mkdir(parents=True) self._accessor.mkdir(self, mode)",if not exist_ok or not self.is_dir():,if not exist_ok or not self.is_dir():,0.9170839167363699,0.8375707157974782,True
3876,"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == '|': m -= 1 n -= 1 mask[m] = 1 elif dirs[m, n] == '^': m -= 1 <IF_STMT> n -= 1 else: raise UnboundLocalError('Illegal move') return mask","elif dirs[m, n] == '<':","elif dirs[m, n] == '^':",0.6388979017668934,0.8621109017306224,False
3877,"def clean(self, *args, **kwargs): data = super().clean(*args, **kwargs) if isinstance(data, File): filename = data.name ext = os.path.splitext(filename)[1] ext = ext.lower() <IF_STMT> raise forms.ValidationError(_('Filetype not allowed!')) return data",if ext not in self.ext_whitelist:,if not os.path.isfile(filename + '.' + ext):,0.7858140168711464,0.727147641547681,False
3878,"def get_doc_object(obj, what=None): if what is None: if inspect.isclass(obj): what = 'class' <IF_STMT> what = 'module' elif callable(obj): what = 'function' else: what = 'object' if what == 'class': return SphinxClassDoc(obj, '', func_doc=SphinxFunctionDoc) elif what in ('function', 'method'): return SphinxFunctionDoc(obj, '') else: return SphinxDocString(pydoc.getdoc(obj))",elif inspect.ismodule(obj):,elif inspect.ismodule(obj):,0.7183701132991696,0.9184043388013005,True
3879,"def apply_pssm(val): if val is not None: val_c = PSSM_VALUES.get(val, None) <IF_STMT> assert isinstance(val, tuple(PSSM_VALUES.values())), ""'store_as' should be one of: %r or an instance of %r not %r"" % (tuple(PSSM_VALUES.keys()), tuple(PSSM_VALUES.values()), val) return val return val_c()",if val_c is None:,if val_c is None:,0.7373501050259833,0.8516228624291206,True
3880,"def read_postmaster_opts(self): """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file"""""" result = {} try: with open(os.path.join(self._postgresql.data_dir, 'postmaster.opts')) as f: data = f.read() for opt in data.split('"" ""'): <IF_STMT> name, val = opt.split('=', 1) result[name.strip('-')] = val.rstrip('""\n') except IOError: logger.exception('Error when reading postmaster.opts') return result",if '=' in opt and opt.startswith('--'):,if '=' in opt:,0.736973630919419,0.8964173245779284,False
3881,"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search('F5-TrafficShield', headers.get(HTTP_HEADER.SERVER, ''), re.I) is not None retval |= re.search('\\AASINFO=', headers.get(HTTP_HEADER.SET_COOKIE, ''), re.I) is not None <IF_STMT> break return retval",if retval:,if retval:,0.7907099219126685,0.8996480074924822,True
3882,"def on_task_start(self, task, config): for item in config: for plugin_name, plugin_config in item.items(): try: thelist = plugin.get(plugin_name, self).get_list(plugin_config) except AttributeError: raise PluginError('Plugin %s does not support list interface' % plugin_name) <IF_STMT> raise plugin.PluginError(thelist.immutable)",if thelist.immutable:,if thelist.immutable:,0.6698857006455934,0.8986118159169515,True
3883,"def nq(t): p = t[0] if t and t[0] in '-+' else '' t = t[len(p):] if t.startswith('tag:') or t.startswith('in:'): try: raw_tag = session.config.get_tag(t.split(':')[1]) <IF_STMT> t = 'in:%s' % raw_tag.slug except (IndexError, KeyError, TypeError): pass return p + t",if raw_tag and raw_tag.hasattr(slug):,if raw_tag:,0.6667756811741162,0.9076141716697395,False
3884,"def _recur_strip(s): if is_str(s): <IF_STMT> return ' '.join(s.strip().split()) else: return ' '.join(s.strip().split()).replace(bos_token + ' ', '') else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s)",if bos_token == '':,if bos_token == '':,0.8927787124069075,0.8038019482772603,True
3885,"def __delitem__(self, key): """"""Deleting tag[key] deletes all 'key' attributes for the tag."""""" for item in self.attrs: <IF_STMT> self.attrs.remove(item) self._getAttrMap() if self.attrMap.has_key(key): del self.attrMap[key]",if item[0] == key:,if item.key == key:,0.7340447311616833,0.7709002428237395,False
3886,"def comment_import_help(init_file, out_file): f_out = open(out_file, 'w') output = '' updated = False with open(init_file, 'r') as f_in: for line in f_in: <IF_STMT> updated = True line = '# ' + line output += line f_out.write(output) f_out.close() return updated",if 'import' in line and '_help' in line and (not updated):,if line.startswith('#'):,0.9262371488625419,0.9076141716697395,False
3887,def prepare_text(lines): out = [] for s in lines.split('|'): s = s.strip() <IF_STMT> s = '{\\i1}%s{\\i0}' % s[1:].strip() out.append(s) return '\\N'.join(out),if s.startswith('/'):,if s.startswith('\\i1'):,0.7348351870287335,0.8232490471721702,False
3888,def sqlctx(sc): pytest.importorskip('pyspark') from odo.backends.sparksql import HiveContext try: yield HiveContext(sc) finally: dbpath = 'metastore_db' logpath = 'derby.log' <IF_STMT> assert os.path.isdir(dbpath) shutil.rmtree(dbpath) if os.path.exists(logpath): assert os.path.isfile(logpath) os.remove(logpath),if os.path.exists(dbpath):,if os.path.exists(dbpath):,0.8826606981561176,0.8590888738245122,True
3889,"def _user2dict(self, uid): usdict = None if uid in self.users: usdict = self.users[uid] <IF_STMT> infos = self.users_info[uid] for attr in infos: usdict[attr['attr_type']] = attr['attr_data'] usdict['uid'] = uid return usdict",if uid in self.users_info:,if uid in self.users_info:,0.7355835318167412,0.8169276475307028,True
3890,"def _validate_options(self): for option in self.options: <IF_STMT> if self.options.required[option] is True and (not self.options[option]): if option == Constants.PASSWORD_CLEAR: option = 'password'.upper() raise FrameworkException(""Value required for the '%s' option."" % option.upper()) return","if not type(self.options[option]) in [bool, int]:",if option in self.options.required:,0.8938439961138959,0.828399516355805,False
3891,"def _copy_package_apps(local_bin_dir: Path, app_paths: List[Path], suffix: str='') -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) <IF_STMT> mkdir(dest.parent) if dest.exists(): logger.warning(f'{hazard}  Overwriting file {str(dest)} with {str(src)}') dest.unlink() if src.exists(): shutil.copy(src, dest)",if not dest.parent.is_dir():,if dest.parent:,0.9526986954125407,0.9099951253570094,False
3892,"def truncate_seq_pair(tokens_a, tokens_b, max_length): """"""Truncates a sequence pair in place to the maximum length."""""" while True: total_length = len(tokens_a) + len(tokens_b) <IF_STMT> break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop()",if total_length <= max_length:,if total_length > max_length:,0.8125673502753625,0.8228500218338367,False
3893,"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel['channel'] defaults = {'currency': channel.currency_code} if 'discount_value' in add_channel.keys(): defaults['discount_value'] = add_channel.get('discount_value') <IF_STMT> defaults['min_spent_amount'] = add_channel.get('min_amount_spent', None) models.VoucherChannelListing.objects.update_or_create(voucher=voucher, channel=channel, defaults=defaults)",if 'min_amount_spent' in add_channel.keys():,if 'min_amount_spent' in add_channel.keys():,0.799631620427868,0.8228500218338367,True
3894,"def services(self, id=None, name=None): for service_dict in self.service_ls(id=id, name=name): service_id = service_dict['ID'] service_name = service_dict['NAME'] <IF_STMT> continue task_list = self.service_ps(service_id) yield DockerService.from_cli(self, service_dict, task_list)",if not service_name.startswith(self._name_prefix):,if service_name == 'all':,0.7631102113141287,0.7801270245332924,False
3895,"def lll(dirname): for name in os.listdir(dirname): <IF_STMT> full = os.path.join(dirname, name) if os.path.islink(full): print(name, '->', os.readlink(full))","if name not in (os.curdir, os.pardir):","if os.path.isfile(os.path.join(dirname, name)):",0.8239193059192211,0.7160350546947921,False
3896,"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote('extracted from web2py', 'developer') for source_str in mydict.keys(): target_str = mydict[source_str] <IF_STMT> target_str = u'' elif target_str.startswith(u'*** '): target_str = u'' pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",if target_str == source_str:,if target_str == u'*** ' and (not target_str.startswith(u'*** ')):,0.7916603782417353,0.7389582677878452,False
3897,"def __init__(self, **kwargs): for k, v in kwargs.items(): setattr(self, k, v) self.attempted_charsets = set() request = cherrypy.serving.request if request.handler is not None: <IF_STMT> cherrypy.log('Replacing request.handler', 'TOOLS.ENCODE') self.oldhandler = request.handler request.handler = self",if self.debug:,if cherrypy.serving.debug:,0.9026960072521937,0.8866029039778043,False
3898,"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, 'fastqc_data.txt') if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith('>>%s' % section_name): in_section = True <IF_STMT> if line.startswith('>>END'): break out.append(line.rstrip('\r\n')) return out",elif in_section:,if in_section:,0.9294222460014816,0.9024521756077707,False
3899,"def bit_length(n): try: return n.bit_length() except AttributeError: norm = deflate_long(n, False) hbyte = byte_ord(norm[0]) <IF_STMT> return 1 bitlen = len(norm) * 8 while not hbyte & 128: hbyte <<= 1 bitlen -= 1 return bitlen",if hbyte == 0:,if hbyte == 0:,0.7449009111704505,0.8474968231198384,True
3900,"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward <IF_STMT> break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if done:,if done:,0.7395942064642276,0.9395648330058336,True
3901,"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) <IF_STMT> del translation[ind] elif action == 'replacement': ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",if action == 'deletion':,if action == 'delete':,0.8713866595977974,0.8832000938217648,False
3902,"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) <IF_STMT> sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if ki != 0:,elif sign * ki > slop:,0.7435481060494746,0.8688589397154922,False
3903,def get_dirlist(_rootdir): dirlist = [] with os.scandir(_rootdir) as rit: for entry in rit: <IF_STMT> dirlist.append(entry.path) dirlist += get_dirlist(entry.path) return dirlist,if not entry.name.startswith('.') and entry.is_dir():,if entry.is_dir():,0.8638778633279095,0.8137489370974955,False
3904,"def __init__(self, fixed: MQTTFixedHeader=None, variable_header: PublishVariableHeader=None, payload=None): if fixed is None: header = MQTTFixedHeader(PUBLISH, 0) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PublishPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = payload",if fixed.packet_type is not PUBLISH:,if fixed.packet_type not in PUBLISH:,0.7290834012392231,0.8380055871435848,False
3905,"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) <IF_STMT> continue if fname == 'PureMVC_Python_1_0': continue if fname[-4:] == '.pyc': continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",if fname == 'output':,if not os.path.isfile(pth):,0.9277429262699679,0.8783650674919876,False
3906,"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = (0, 0) for ts in self.time_steps: <IF_STMT> raw_rewards += ts.raw_reward if ts.processed_reward is not None: processed_rewards += ts.processed_reward return (raw_rewards, processed_rewards)",if ts.raw_reward is not None:,if ts.raw_reward is not None:,0.8536454671894312,0.8294838585473985,True
3907,"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() <IF_STMT> args.extend(self._split_option(line)) elif line and (not line.startswith('#')): args.append(line) return args",if line.startswith('-'):,if line:,0.7570366820489982,0.839587623092576,False
3908,"def __on_change_button_clicked(self, widget=None): """"""compute all primary objects and toggle the 'Change' attribute"""""" self.change_status = not self.change_status for prim_obj, tmp in self.xobjects: obj_change = self.top.get_object('%s_change' % prim_obj) <IF_STMT> continue self.change_entries[prim_obj].set_val(self.change_status) obj_change.set_active(self.change_status)",if not obj_change.get_sensitive():,if not obj_change:,0.7744745191318654,0.8498644646741501,False
3909,"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ('Core', '0') for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / 'settings.json' if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue <IF_STMT> continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield (cog_name, cog_id)","if not isinstance(data, dict):","if not isinstance(data, dict):",0.7227274078470646,0.8923575006167597,True
3910,"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): <IF_STMT> raise BlockError(_error.InstanceError % (self.name, inst.callername))",if not inst.modctxt:,if not inst.is_valid():,0.8935537902509941,0.8266114125804572,False
3911,def _is_xml(accepts): if accepts.startswith(b'application/'): has_xml = accepts.find(b'xml') if has_xml > 0: semicolon = accepts.find(b';') <IF_STMT> return True return False,if semicolon < 0 or has_xml < semicolon:,if semicolon > 0:,0.6214743522132142,0.7245511487202049,False
3912,"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): <IF_STMT> return target else: mapper = _mapper_or_none(target) if mapper is not None: return mapper else: return _MapperEventsHold(target) else: return target","if issubclass(target, mapperlib.Mapper):","if isinstance(target, mapperlib.Mapper):",0.8783835624858158,0.8635707684233572,False
3913,"def _get_font_afm(self, prop): key = hash(prop) font = self.afmfontd.get(key) <IF_STMT> fname = findfont(prop, fontext='afm') font = self.afmfontd.get(fname) if font is None: font = AFM(file(findfont(prop, fontext='afm'))) self.afmfontd[fname] = font self.afmfontd[key] = font return font",if font is None:,if font is None:,0.7489196402370637,0.8385130047130208,True
3914,"def __call__(self, groupby): normalize_reduction_funcs(self, ndim=groupby.ndim) df = groupby while df.op.output_types[0] not in (OutputType.dataframe, OutputType.series): df = df.inputs[0] if self.raw_func == 'size': self.output_types = [OutputType.series] else: self.output_types = [OutputType.dataframe] <IF_STMT> else [OutputType.series] if self.output_types[0] == OutputType.dataframe: return self._call_dataframe(groupby, df) else: return self._call_series(groupby, df)",if groupby.op.output_types[0] == OutputType.dataframe_groupby,if self.raw_func == 'size',0.9103771403971712,0.8723360571509826,False
3915,"def save(self): if self.preferences.get(ENCRYPT_ON_DISK, False): <IF_STMT> return self.storage.write(self.to_dict(encrypt_password=self.encryption_password)) elif not self.is_locked: log.warning('Disk encryption requested but no password available for encryption. Resetting encryption preferences and saving wallet in an unencrypted state.') self.preferences[ENCRYPT_ON_DISK] = False return self.storage.write(self.to_dict())",if self.encryption_password is not None:,if self.encryption_password:,0.8214900645459791,0.8966773400768917,False
3916,"def isValidDateString(config_param_name, value, valid_value): try: if value == 'DD-MM-YYYY': return value day, month, year = value.split('-') if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) <IF_STMT> raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",if int(month) < 1 or int(month) > 12:,if int(month) < 1 or int(month) > 31:,0.9118011845536211,0.824840871632579,False
3917,"def _capture(self, call_name, data=None, **kwargs): if data is None: data = self.get_default_context() else: default_context = self.get_default_context() <IF_STMT> default_context.update(data) else: default_context['extra']['extra_data'] = data data = default_context client = self.get_sentry_client() return getattr(client, call_name)(data=data, **kwargs)","if isinstance(data, dict):","if isinstance(data, dict):",0.6708336010547543,0.8592899528284996,True
3918,"def check(input, expected_output=None, expected_ffi_error=False): import _cffi_backend ffi = _cffi_backend.FFI() if not expected_ffi_error: ct = ffi.typeof(input) assert isinstance(ct, ffi.CType) assert ct.cname == (expected_output or input) else: e = py.test.raises(ffi.error, ffi.typeof, input) <IF_STMT> assert str(e.value) == expected_ffi_error","if isinstance(expected_ffi_error, str):",if expected_ffi_error:,0.8520135444789123,0.8966773400768917,False
3919,"def run(self): """"""Process queries from task queue, stop if processor is None."""""" while True: try: processor, iprot, oprot, otrans, callback = self.queue.get() <IF_STMT> break processor.process(iprot, oprot) callback(True, otrans.getvalue()) except Exception: logging.exception('Exception while processing request') callback(False, '')",if processor is None:,if processor is None:,0.7627452126166244,0.8516228624291206,True
3920,"def search(self, query): query = query.strip().lower() results = [] for provider in SidebarItemProvider.all(self.context): for item in provider.provide(): if 'url' in item: search_source = '$'.join([item.get('id', ''), item.get('name', '')]).lower() <IF_STMT> results.append({'title': item['name'], 'icon': item['icon'], 'url': item['url']}) return results",if query in search_source:,if query.search(search_source):,0.7870582149520179,0.8996480074924822,False
3921,"def handle(self) -> None: """"""Handles a request ignoring dropped connections."""""" try: BaseHTTPRequestHandler.handle(self) except (ConnectionError, socket.timeout) as e: self.connection_dropped(e) except Exception as e: <IF_STMT> self.log_error('SSL error occurred: %s', e) else: raise if self.server.shutdown_signal: self.initiate_shutdown()",if self.server.ssl_context is not None and is_ssl_error(e):,if self.server.ssl_error:,0.9043681713302347,0.8901732118131125,False
3922,"def cdn_url_handler(error, endpoint, kwargs): if endpoint == 'cdn': path = kwargs.pop('path') cdn = app.config.get('cdn', '//cdnjscn.b0.upaiyun.com/libs/') return urljoin(cdn, path) else: exc_type, exc_value, tb = sys.exc_info() <IF_STMT> reraise(exc_type, exc_value, tb) else: raise error",if exc_value is error:,if exc_type:,0.8003777470781939,0.8827916928185874,False
3923,"def pairs(self): for path in os.listdir('src'): if path == '.svn': continue dep = join('src', path) <IF_STMT> continue yield (dep, join(build_dir, path))",if isdir(dep):,if not os.path.isdir(dep):,0.713263386324143,0.7848518349390632,False
3924,"def get_condition(self): """"""Return the condition element's name."""""" for child in self.xml: <IF_STMT> cond = child.tag.split('}', 1)[-1] if cond in self.conditions: return cond return 'not-authorized'",if '{%s}' % self.namespace in child.tag:,if child.tag.startswith('condition'):,0.8859689885527113,0.8466657105524215,False
3925,"def end(self, tag): try: f = self.dispatch[tag] except KeyError: <IF_STMT> return try: f = self.dispatch[tag.split(':')[-1]] except KeyError: return return f(self, ''.join(self._data))",if ':' not in tag:,if tag == 'end':,0.5990494863968435,0.7498810286408993,False
3926,"def checkIfSessionCodeExists(self, sessionCode): if self.emrtFile: sessionsForExperiment = self.emrtFile.root.data_collection.session_meta_data.where('experiment_id == %d' % (self.active_experiment_id,)) sessionCodeMatch = [sess for sess in sessionsForExperiment if sess['code'] == sessionCode] <IF_STMT> return True return False",if len(sessionCodeMatch) > 0:,if sessionCodeMatch:,0.8274116098135902,0.8696398662122882,False
3927,"def save_bytearray(self, obj): if self.proto < 5: <IF_STMT> self.save_reduce(bytearray, (), obj=obj) else: self.save_reduce(bytearray, (bytes(obj),), obj=obj) return n = len(obj) if n >= self.framer._FRAME_SIZE_TARGET: self._write_large_bytes(BYTEARRAY8 + pack('<Q', n), obj) else: self.write(BYTEARRAY8 + pack('<Q', n) + obj)",if not obj:,"if isinstance(obj, bytes):",0.9179973919888921,0.8713933650206428,False
3928,"def _restore_freeze(self, new): size_change = [] for k, v in six.iteritems(self._freeze_backup): newv = new.get(k, []) <IF_STMT> size_change.append((self._key_name(k), len(v), len(newv))) if size_change: logger.info('These collections were modified but restored in {}: {}'.format(self._name, ', '.join(map(lambda t: '({}: {}->{})'.format(*t), size_change)))) restore_collection(self._freeze_backup)",if len(v) != len(newv):,if v != newv:,0.9334894361514985,0.8555308664663046,False
3929,"def check_options(self, expr, evaluation, options): for key in options: if key != 'System`SameTest': <IF_STMT> evaluation.message('ContainsOnly', 'optx', Symbol(key)) else: return evaluation.message('ContainsOnly', 'optx', Symbol(key), expr) return None",if expr is None:,"if key in ('System`SameTest', 'System`SameTest'):",0.6251107079500575,0.759907656827929,False
3930,"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith('.'): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): <IF_STMT> self.bundle_package(itempath) elif nm.endswith('.py'): self.bundle_module(itempath)","if os.path.exists(os.path.join(itempath, '__init__.py')):",if nm.endswith('.py'):,0.923872625168721,0.8935248372106969,False
3931,"def _read_block(self, size): if self._file_end is not None: max_size = self._file_end - self._file.tell() <IF_STMT> size = max_size size = max(min(size, max_size), 0) return self._file.read(size)",if size == -1:,if size > max_size:,0.6583892460049281,0.7801270245332924,False
3932,"def question_mark(self): """"""Shows help for this command and it's sub-commands."""""" ret = [] if self.param_help_msg or len(self.subcommands) == 0: ret.append(self._quick_help()) if len(self.subcommands) > 0: for k, _ in sorted(self.subcommands.items()): command_path, param_help, cmd_help = self._instantiate_subcommand(k)._quick_help(nested=True) <IF_STMT> ret.append((command_path, param_help, cmd_help)) return (CommandsResponse(STATUS_OK, self.help_formatter(ret)), self.__class__)",if command_path or param_help or cmd_help:,if cmd_help:,0.9493573677429861,0.9144061946646023,False
3933,"def list_domains(self, r53, **kwargs): marker = None domains = [] while True: if marker: response = self.wrap_aws_rate_limited_call(r53.list_domains(Marker=marker)) else: response = self.wrap_aws_rate_limited_call(r53.list_domains) for domain in response.get('Domains'): domains.append(domain) <IF_STMT> marker = response.get('NextPageMarker') else: break return domains",if response.get('NextPageMarker'):,if response.get('NextPageMarker'):,0.910578060030838,0.8935248372106969,True
3934,"def writer(stream, items): sep = '' for item in items: stream.write(sep) sep = ' ' if not isinstance(item, str): item = str(item) if not PY3K: <IF_STMT> item = str(item) stream.write(item) stream.write('\n')","if not isinstance(item, unicode):","if not isinstance(item, str):",0.8990014681875313,0.828399516355805,False
3935,"def f(view, s): if mode == modes.INTERNAL_NORMAL: view.run_command('toggle_comment') <IF_STMT> pt = utils.next_non_white_space_char(view, s.a, white_space=' \t') else: pt = utils.next_non_white_space_char(view, self.view.line(s.a).a, white_space=' \t') return R(pt, pt) return s","if utils.row_at(self.view, s.a) != utils.row_at(self.view, self.view.size()):",if self.view.is_comment():,0.759758827902224,0.8645707301556367,False
3936,"def _parse_timestamp(value): if value: match = _TIMESTAMP_PATTERN.match(value) <IF_STMT> if match.group(2): format = '%Y-%m-%d %H:%M:%S.%f' value = match.group() else: format = '%Y-%m-%d %H:%M:%S' value = datetime.datetime.strptime(value, format) else: raise Exception('Cannot convert ""{}"" into a datetime'.format(value)) else: value = None return value",if match:,if match:,0.6490883276853131,0.9099951253570094,True
3937,"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): if model_site['type'] == 'sample': log_r_term = model_site['log_prob'] <IF_STMT> log_r_term = log_r_term - guide_trace.nodes[name]['log_prob'] log_r.add((stacks[name], log_r_term.detach())) return log_r",if not model_site['is_observed']:,elif model_site['type'] == 'step':,0.8307026187998109,0.828399516355805,False
3938,"def get_translationproject(self): """"""returns the translation project belonging to this directory."""""" if self.is_language() or self.is_project(): return None el<IF_STMT> return self.translationproject else: aux_dir = self while not aux_dir.is_translationproject() and aux_dir.parent is not None: aux_dir = aux_dir.parent return aux_dir.translationproject",if self.is_translationproject():,if self.translationproject is not None:,0.7546516781986647,0.8294838585473985,False
3939,"def get_hosted_content(): try: scheme, rest = target.split('://', 1) prefix, host_and_port = rest.split('.interactivetool.') faked_host = rest <IF_STMT> faked_host = rest.split('/', 1)[0] url = '%s://%s' % (scheme, host_and_port) response = requests.get(url, timeout=1, headers={'Host': faked_host}) return response.text except Exception as e: print(e) return None",if '/' in rest:,if prefix == 'http':,0.891751193727689,0.8692960007731574,False
3940,def install(self): log.info(self.openssl_cli) if not self.has_openssl or self.args.force: <IF_STMT> self._download_src() else: log.debug('Already has src {}'.format(self.src_file)) self._unpack_src() self._build_src() self._make_install() else: log.info('Already has installation {}'.format(self.install_dir)) version = self.openssl_version if self.version not in version: raise ValueError(version),if not self.has_src:,if self.src_file == self.install_dir:,0.881347126631235,0.8385130047130208,False
3941,"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): if i % 2: pieces.append(force_text(getattr(self, piece)())) <IF_STMT> pieces.append(re_escaped.sub('\\1', piece)) return ''.join(pieces)",elif piece:,elif i % 3:,0.8738542822079393,0.760856626273165,False
3942,"def get_current_events_users(calendar): now = timezone.make_aware(datetime.now(), timezone.get_current_timezone()) result = [] day = Day(calendar.events.all(), now) for o in day.get_occurrences(): <IF_STMT> usernames = o.event.title.split(',') for username in usernames: result.append(User.objects.get(username=username.strip())) return result",if o.start <= now <= o.end:,if o.event:,0.9076403567227631,0.8696398662122882,False
3943,"def from_cfn_params(self, cfn_params): """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch."""""" cfn_converter = self.definition.get('cfn_param_mapping', None) if cfn_converter and cfn_params: <IF_STMT> self.value = int(float(get_cfn_param(cfn_params, cfn_converter))) return self","if get_cfn_param(cfn_params, 'Scheduler') == 'awsbatch':",if cfn_params['aws_batch_type'] == 'awsbatch':,0.8926367141269778,0.828399516355805,False
3944,"def onCompletion(self, text): res = [] for l in text.split('\n'): <IF_STMT> continue l = l.split(':') if len(l) != 2: continue res.append([l[0].strip(), l[1].strip()]) self.panel.setChapters(res)",if not l:,if len(l) != 3:,0.885265269882222,0.7709002428237395,False
3945,"def update_ranges(l, i): for _range in l: if i == _range[0] - 1: _range[0] = i merge_ranges(l) return <IF_STMT> _range[1] = i merge_ranges(l) return l.append([i, i]) l.sort(key=lambda x: x[0])",elif i == _range[1] + 1:,if i == _range[1] - 1:,0.7477540424940141,0.7665936070959262,False
3946,"def process_dollar(token, state, command_line): if not state.is_range_start_line_parsed: <IF_STMT> raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError('bad range: {0}'.format(state.scanner.state.source)) command_line.line_range.end.append(token) return (parse_line_ref, command_line)",if command_line.line_range.start:,if command_line.line_range.start:,0.6394030544867773,0.8466657105524215,True
3947,"def _parse_description(self, text: str): result = dict(links=[], versions=[]) for line in text.splitlines(): clean = REX_TAG.sub('', line.strip()) <IF_STMT> result['severity'] = clean.split()[1] continue if clean.startswith('Affects:'): result['name'] = clean.split()[1] continue if ' or higher' in clean: result['versions'] = self._get_versions(clean) result['links'].extend(REX_LINK.findall(line)) return result",if clean.startswith('Severity:'):,if clean.startswith('Severity:'):,0.9037443046558976,0.9076141716697395,True
3948,"def apply(self, chart, grammar): for prod in grammar.productions(empty=True): for index in compat.xrange(chart.num_leaves() + 1): new_edge = TreeEdge.from_production(prod, index) <IF_STMT> yield new_edge","if chart.insert(new_edge, ()):",if chart.add_edge(new_edge):,0.6996417402239254,0.8375717919273554,False
3949,"def calc(self, arg): op = arg['op'] if op == 'C': self.clear() return str(self.current) num = decimal.Decimal(arg['num']) if self.op: <IF_STMT> self.current += num elif self.op == '-': self.current -= num elif self.op == '*': self.current *= num elif self.op == '/': self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == '=': self.clear() return res",if self.op == '+':,if self.op == '+':,0.9286074713763336,0.914208565914368,True
3950,"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = (50, 50, 50) for frame in g.app.windowList: w = frame and frame.top if w: r = w.geometry() w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) x += 30 y += 30 <IF_STMT> x = 10 + delta y = 40 + delta delta += 10",if x > 200:,if x > 10:,0.8695223989496236,0.8964173245779284,False
3951,def redirect(self): c = self.c if c.config.getBool('eval-redirect'): self.old_stderr = g.stdErrIsRedirected() self.old_stdout = g.stdOutIsRedirected() <IF_STMT> g.redirectStderr() if not self.old_stdout: g.redirectStdout(),if not self.old_stderr:,if not self.old_stderr:,0.5870271845497637,0.761827408333416,True
3952,"def on_event(self, c, button, data): if self.rvGestureGrab.get_reveal_child(): <IF_STMT> self.use() elif button == 'Y' and data[0] == 0: self.start_over()",if button == 'A' and data[0] == 0:,if button == 'X' and data[0] == 1:,0.531547425560482,0.5886994264160949,False
3953,"def __init__(self, in_feats, out_feats, norm='both', bias=True, activation=None): super(DenseGraphConv, self).__init__() self._in_feats = in_feats self._out_feats = out_feats self._norm = norm with self.name_scope(): self.weight = self.params.get('weight', shape=(in_feats, out_feats), init=mx.init.Xavier(magnitude=math.sqrt(2.0))) <IF_STMT> self.bias = self.params.get('bias', shape=(out_feats,), init=mx.init.Zero()) else: self.bias = None self._activation = activation",if bias:,if bias:,0.8287414659840273,0.9076141716697395,True
3954,"def _import_top_module(self, name): for item in sys.path: if isinstance(item, _StringType): module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) <IF_STMT> return module return None",if module:,if module is not None:,0.6309941500020477,0.7406093667638122,False
3955,"def resolver(schemas, f): if not callable(f): return if not hasattr(f, 'accepts'): return new_params = [] for p in f.accepts: <IF_STMT> new_params.append(p.resolve(schemas)) else: raise ResolverError('Invalid parameter definition {0}'.format(p)) f.accepts.clear() f.accepts.extend(new_params)","if isinstance(p, (Patch, Ref, Attribute)):","if isinstance(p, Resolver):",0.8604439267241493,0.8446593249975184,False
3956,"def get_files(d): res = [] for p in glob.glob(os.path.join(d, '*')): if not p: continue pth, fname = os.path.split(p) if fname == 'output': continue <IF_STMT> continue if fname[-4:] == '.pyc': continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",if fname == 'PureMVC_Python_1_0':,if fname[-3:] == '.py':,0.858331291658289,0.8555308664663046,False
3957,"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: if rightname[0] == '@': for rightname2 in groups[rightname]: rightnames.add(rightname2) <IF_STMT> break else: rightnames.add(rightname)",if not includeAll:,elif includeAll:,0.8932213105471123,0.8645707301556367,False
3958,"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from['Stats']): <IF_STMT> self.entries_count['Stats'] -= 1 continue new_obj = self.model_to['Stats']() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns: continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",if not old_obj.summary:,if old_obj.entries_count['Stats'] > 0:,0.7868666223307074,0.8169276475307028,False
3959,"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] <IF_STMT> value = os.environ[envvar] if six.PY2: value = value.decode('utf8') return value else: raise InvalidConfigException(""{} - environment variable '{}' not set"".format(msg, var)) else: raise InvalidConfigException(""{} - environment variable name '{}' does not match pattern '{}'"".format(msg, var, _ENV_VAR_PAT_STR))",if envvar in os.environ:,if envvar in os.environ:,0.6672336746060721,0.8902056737869248,True
3960,"def __next__(self): self._parse_reset() while True: try: line = next(self.input_iter) except StopIteration: <IF_STMT> raise Error('newline inside string') raise self.line_num += 1 if '\x00' in line: raise Error('line contains NULL byte') pos = 0 while pos < len(line): pos = self._parse_process_char(line, pos) self._parse_eol() if self.state == self.START_RECORD: break fields = self.fields self.fields = [] return fields",if len(self.field) > 0:,if self.line_num >= self.max_line_num:,0.814131954345248,0.9001816649635144,False
3961,"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes('\x00\x00\x01', self.current_size, self.current_size + 1024 * 1024 * 8) <IF_STMT> padsize = pos - self.current_size if padsize: yield PaddingBytes(self, 'pad[]', padsize // 8) chunk = Chunk(self, 'chunk[]') try: chunk['content/data'] except: pass yield chunk",if pos is not None:,if pos:,0.9311942170811874,0.9122561819614461,False
3962,"def spew(): seenUID = False start() for part in query: if part.type == 'uid': seenUID = True <IF_STMT> yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, 'spew_' + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and (not seenUID): space() yield self.spew_uid(id, msg, write, flush) finish() flush()",if part.type == 'body':,elif part.type == 'body':,0.8039179979853631,0.9019629427251674,False
3963,"def _limit_value(key, value, config): if config[key].get('upper_limit'): limit = config[key]['upper_limit'] if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key]['inverse'] is True: if datetime.now() - limit > value: value = datetime.now() - limit el<IF_STMT> value = datetime.now() + limit elif value > limit: value = limit return value",if datetime.now() + limit < value:,if datetime.now() + limit < value:,0.9237766754031242,0.8443258653392445,True
3964,"def _fix_var_naming(operators, names, mod='input'): new_names = [] map = {} for op in operators: if mod == 'input': iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name <IF_STMT> break for name in names: new_names.append(map[name]) return new_names",if len(map) == len(names):,if map[i.raw_name] == name:,0.8596679997931014,0.9001816649635144,False
3965,"def traverse(tree): """"""Generator dropping comment nodes"""""" for entry in tree: spaceless = [e for e in entry if not nginxparser.spacey(e)] if spaceless: key = spaceless[0] values = spaceless[1] if len(spaceless) > 1 else None else: key = values = '' if isinstance(key, list): new = copy.deepcopy(entry) new[1] = filter_comments(values) yield new el<IF_STMT> yield spaceless",if key != '#' and spaceless:,"if isinstance(key, dict):",0.8014096443980402,0.9215433176691109,False
3966,"def mergeCombiners(self, x, y): for item in y: <IF_STMT> self.heap.push(x, item) else: self.heap.push_pop(x, item) return x",if len(x) < self.heap_limit:,"if isinstance(item, (list, tuple)):",0.7973298981350746,0.674945488826271,False
3967,"def test_scatter(self, harness: primitive_harness.Harness): f_name = harness.params['f_lax'].__name__ dtype = harness.params['dtype'] if jtu.device_under_test() == 'tpu': <IF_STMT> raise unittest.SkipTest(f'TODO: complex {f_name} on TPU fails in JAX') self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))","if dtype is np.complex64 and f_name in ['scatter_min', 'scatter_max']:",if dtype != 'complex':,0.8922787215570978,0.7965020533851944,False
3968,"def TryMerge(self, decoder): while decoder.avail() > 0: tag = decoder.getVarInt32() if tag == TAG_BEGIN_ITEM_GROUP: type_id, message = Item.Decode(decoder) <IF_STMT> self.items[type_id].MergeFrom(Item(message)) else: self.items[type_id] = Item(message) continue if tag == 0: raise ProtocolBuffer.ProtocolBufferDecodeError decoder.skipData(tag)",if type_id in self.items:,if type_id in self.items:,0.7531173596309333,0.8336104423443033,True
3969,"def process_continuations(lines): global continuation_pattern olines = [] while len(lines) != 0: line = no_comments(lines[0]) line = line.strip() lines.pop(0) if line == '': continue <IF_STMT> line = continuation_pattern.sub('', line) if len(lines) >= 1: combined_lines = [line + lines[0]] lines.pop(0) lines = combined_lines + lines continue olines.append(line) del lines return olines",if continuation_pattern.search(line):,if continuation_pattern:,0.7386750053027356,0.926934323706186,False
3970,"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: if pkg in Scheduler.listOfPackagesCurrentlyBuilding: continue <IF_STMT> Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug('Adding ' + pkg + ' to the schedule list')",if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg):,if not Scheduler.listOfPackagesNextToBuild.get((-Scheduler._getPriority(pkg))):,0.6336304628882052,0.8120341702859789,False
3971,"def find_distribution_modules(name=__name__, file=__file__): current_dist_depth = len(name.split('.')) - 1 current_dist = os.path.join(os.path.dirname(file), *[os.pardir] * current_dist_depth) abs = os.path.abspath(current_dist) dist_name = os.path.basename(abs) for dirpath, dirnames, filenames in os.walk(abs): package = (dist_name + dirpath[len(abs):]).replace('/', '.') <IF_STMT> yield package for filename in filenames: if filename.endswith('.py') and filename != '__init__.py': yield '.'.join([package, filename])[:-3]",if '__init__.py' in filenames:,if package.endswith('.py'):,0.8793875749438448,0.9253742688467129,False
3972,"def transform_value(i, v, *args): if i not in converter_functions: return v else: try: return converter_functions[i](v, *args) except Exception as e: if failonerror == 'inline': return e <IF_STMT> raise e else: return errorvalue",elif failonerror:,elif failonerror == 'error':,0.8550966349804563,0.8336104423443033,False
3973,"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile(max_size=self._storage.max_memory_size, suffix='.S3Boto3StorageFile', dir=setting('FILE_UPLOAD_TEMP_DIR')) <IF_STMT> self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) if self._storage.gzip and self.obj.content_encoding == 'gzip': self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",if 'r' in self._mode:,if self._storage.dirty:,0.7277843778392493,0.8787142254774354,False
3974,"def connect(self, host, port, timeout): fp = Telnet() for i in range(50): try: fp.sock = socket.create_connection((host, int(port)), timeout=int(timeout), source_address=('', 1023 - i)) break except socket.error as e: <IF_STMT> raise e self.need_handshake = True return TCP_Connection(fp)","if (e.errno, e.strerror) != (98, 'Address already in use'):",if i == 1023 - i:,0.7048459330556913,0.8034328145121407,False
3975,"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: if re.search('^\\s*\\/\\*', source): comment = source[0, source.index('*/') + 2] <IF_STMT> comment = re.search('^\\s*\\/\\/', source).group(0) else: comment = None if comment: source = re.sub('^\\s+', '', source[len(comment):]) trailing_comments.append(comment) return '\n'.join(trailing_comments) + source","elif re.search('^\\s*\\/\\/', source):","elif re.search('^\\s*\\/\\/', source).group(0):",0.8090242618313324,0.9118021019905903,False
3976,"def yview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.vsb.get() elif mode == 'moveto': frameHeight = self.innerframe.winfo_reqheight() self._startY = value * float(frameHeight) else: clipperHeight = self._clipper.winfo_height() <IF_STMT> jump = int(clipperHeight * self._jfraction) else: jump = clipperHeight self._startY = self._startY + value * jump self.reposition()",if units == 'units':,if self._jfraction:,0.7589032079990127,0.9312457603037672,False
3977,"def visit(stmt): """"""Collect information about VTCM buffers and their alignments."""""" if isinstance(stmt, tvm.tir.AttrStmt): if stmt.attr_key == 'storage_scope' and stmt.value == 'local.vtcm': vtcm_buffers.append(stmt.node) <IF_STMT> if not stmt.node in alignments: alignments[stmt.node] = [] alignments[stmt.node].append(stmt.value)",elif stmt.attr_key == 'storage_alignment':,elif stmt.attr_key == 'alignments':,0.8898102682835358,0.8336104423443033,False
3978,"def cost(P): loss_b = 0 loss_w = 0 for i, xi in enumerate(xc): xi = np.dot(xi, P) for j, xj in enumerate(xc[i:]): xj = np.dot(xj, P) M = dist(xi, xj) G = sinkhorn(wc[i], wc[j + i], M, reg, k) <IF_STMT> loss_w += np.sum(G * M) else: loss_b += np.sum(G * M) return loss_w / loss_b",if j == 0:,if i == j + i:,0.8366389590159292,0.8711151332295498,False
3979,"def __init__(self, comm, in_channels, out_channels, ksize, pad=1): super(Block, self).__init__() with self.init_scope(): <IF_STMT> self.conv = ParallelConvolution2D(comm, in_channels, out_channels, ksize, pad=pad, nobias=True) else: self.conv = chainer.links.Convolution2D(in_channels, out_channels, ksize, pad=pad, nobias=True) self.bn = L.BatchNormalization(out_channels)",if comm.size <= in_channels:,if comm.get_parallel():,0.8041595038482533,0.8827916928185874,False
3980,"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, 'i', 'ii', 'iii', 'iv', 'v', 'a', 'b', 'c', 'd', 'e']: for wrong in ['cd', 'part', 'dis', 'disc', 'dvd']: <IF_STMT> wrong_found += 1 if wrong_found == 1: return -30 return 0 except: log.error('Failed doing halfMultipartScore: %s', traceback.format_exc()) return 0","if '%s%s' % (wrong, nr) in nzb_name.lower():",if nr == wrong:,0.9018176447982387,0.8964173245779284,False
3981,"def should_include(service): for f in filt: <IF_STMT> state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == 'source': source = filt[f] if source == 'image' or source == 'build': if source not in service.options: return False else: raise UserError('Invalid value for source filter: %s' % source) else: raise UserError('Invalid filter: %s' % f) return True",if f == 'status':,if f == 'state':,0.9351696446803206,0.9114434865990403,False
3982,"def get_blob_type_declaration_sql(self, column): length = column.get('length') if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return 'TINYBLOB' <IF_STMT> return 'BLOB' if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return 'MEDIUMBLOB' return 'LONGBLOB'",if length <= self.LENGTH_LIMIT_BLOB:,if length <= self.LENGTH_LIMIT_BLOB:,0.8773508890024868,0.7886336751695258,True
3983,"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] <IF_STMT> y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = (x, y, z) if event.num_clicks == 2: d.dismiss('Goto')",if y == 0:,if y == 0:,0.630476216849271,0.828399516355805,True
3984,"def check_related_active_jobs(self, obj): active_jobs = obj.get_active_jobs() if len(active_jobs) > 0: raise ActiveJobConflict(active_jobs) time_cutoff = now() - dateutil.relativedelta.relativedelta(minutes=1) recent_jobs = obj._get_related_jobs().filter(finished__gte=time_cutoff) for unified_job in recent_jobs.get_real_instances(): <IF_STMT> raise PermissionDenied(_('Related job {} is still processing events.').format(unified_job.log_format))",if not unified_job.event_processing_finished:,if unified_job.is_processing_events:,0.924570191532255,0.8901732118131125,False
3985,"def run(self): self.alive = True if _log.isEnabledFor(_DEBUG): _log.debug('started') while self.alive: task = self.queue.get() <IF_STMT> function, args, kwargs = task assert function try: function(*args, **kwargs) except: _log.exception('calling %s', function) if _log.isEnabledFor(_DEBUG): _log.debug('stopped')",if task:,if task:,0.92114159054081,0.8827916928185874,True
3986,"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return exists, contents = read_sysconfig_file(fn) updated_am = 0 for k, v in adjustments.items(): if v is None: continue v = str(v) <IF_STMT> continue contents[k] = v updated_am += 1 if updated_am: lines = [str(contents)] if not exists: lines.insert(0, util.make_header()) util.write_file(fn, '\n'.join(lines) + '\n', 420)",if len(v) == 0 and (not allow_empty):,if allow_empty and v == '':,0.9194353733127626,0.8615894073865318,False
3987,"def wrapper(self: RequestHandler, *args, **kwargs) -> Optional[Awaitable[None]]: if self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path.rstrip('/') <IF_STMT> if self.request.query: uri += '?' + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",if uri:,if uri:,0.7256511596858783,0.8996480074924822,True
3988,def output_handles_from_execution_plan(execution_plan): output_handles_for_current_run = set() for step_level in execution_plan.execution_step_levels(): for step in step_level: for step_input in step.step_inputs: <IF_STMT> output_handles_for_current_run.update(step_input.source_handles) return output_handles_for_current_run,if step_input.source_handles:,if step_input.output_handles:,0.8832875281584254,0.8232490471721702,False
3989,"def _read_value(self, item): item = _normalize_path(item) if item in self._store: <IF_STMT> del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) elif item in self._children: return PathResult(item, dir=True) else: raise KeyError(item)",if item in self._expire_time and self._expire_time[item] < datetime.now():,if self._store[item] is None:,0.5839075704417762,0.8105932471967202,False
3990,"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 <IF_STMT> start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",if not start:,if start is None:,0.9472443021714612,0.9114434865990403,False
3991,"def _update_help_obj_params(help_obj, data_params, params_equal, attr_key_tups): loaded_params = [] for param_obj in help_obj.parameters: loaded_param = next((n for n in data_params if params_equal(param_obj, n)), None) <IF_STMT> BaseHelpLoader._update_obj_from_data_dict(param_obj, loaded_param, attr_key_tups) loaded_params.append(param_obj) help_obj.parameters = loaded_params",if loaded_param:,if loaded_param is not None:,0.7236924624752562,0.8036431532733102,False
3992,"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() <IF_STMT> aList = w.sizes() if len(aList) == 2: n1, n2 = aList ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",if w:,if w:,0.9115403907672689,0.9298663600557577,True
3993,def _check_required_env_variables(vars): for var in vars: <IF_STMT> self.tc.logger.error('%s is not set. Did you forget to source your build environment setup script?' % var) raise OEQAPreRun,if not os.environ.get(var):,"if not os.path.exists(os.path.join(self.build_dir, var)):",0.5754278507813567,0.7886336751695258,False
3994,def clean_indexes(): for coll_name in mongo.collection_types.keys(): coll = mongo.get_collection(coll_name) indexes = coll_indexes[coll_name] try: for index in coll.list_indexes(): name = index['name'] <IF_STMT> continue coll.drop_index(name) except pymongo.errors.OperationFailure: pass,if name == '_id' or name == '_id_' or name in indexes:,if name in indexes:,0.8025319307874459,0.7965020533851944,False
3995,"def _compare_dirs(self, dir1, dir2): diff = [] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <IF_STMT> diff.append(file_) return diff",if not os.path.exists(target_path):,if os.path.isdir(target_path):,0.8818697176693324,0.8743414417652072,False
3996,"def load_state_dict(self, state_dict, strict=True): """"""Customized load."""""" self.language_model.load_state_dict(state_dict[self._language_model_key], strict=strict) if mpu.is_pipeline_last_stage(): <IF_STMT> self.multichoice_head.load_state_dict(state_dict[self._multichoice_head_key], strict=strict) else: print_rank_last('***WARNING*** could not find {} in the checkpoint, initializing to random'.format(self._multichoice_head_key))",if self._multichoice_head_key in state_dict:,if self._multichoice_head_key in state_dict:,0.8718273460362789,0.7886336751695258,True
3997,"def _parse_timedelta(self, value): try: sum = datetime.timedelta() start = 0 while start < len(value): m = self._TIMEDELTA_PATTERN.match(value, start) <IF_STMT> raise Exception() num = float(m.group(1)) units = m.group(2) or 'seconds' units = self._TIMEDELTA_ABBREV_DICT.get(units, units) sum += datetime.timedelta(**{units: num}) start = m.end() return sum except: raise",if not m:,if not m:,0.6974150894167006,0.897752847848028,True
3998,"def SetChildMenuBar(self, pChild): if not pChild: if self._pMyMenuBar: self.SetMenuBar(self._pMyMenuBar) else: self.SetMenuBar(self.GetMenuBar()) self._pMyMenuBar = None else: if pChild.GetMenuBar() is None: return <IF_STMT> self._pMyMenuBar = self.GetMenuBar() self.SetMenuBar(pChild.GetMenuBar())",if self._pMyMenuBar is None:,if self._pMyMenuBar is None:,0.8219655135084156,0.7886336751695258,True
3999,"def init_weights(self): """"""Initialize weights of the head."""""" bias_cls = bias_init_with_prob(0.01) normal_init(self.conv_reg, std=0.01) normal_init(self.conv_centerness, std=0.01) normal_init(self.conv_cls, std=0.01, bias=bias_cls) for branch in [self.cls_convs, self.reg_convs]: for module in branch.modules(): <IF_STMT> caffe2_xavier_init(module.conv)","if isinstance(module, ConvModule) and isinstance(module.conv, nn.Conv2d):",if module.conv is not None:,0.8960146024097961,0.8105932471967201,False
4000,"def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == 'success': continue _, exc_name, exc_cls, _ = result.thrift_spec[k] <IF_STMT> setattr(result, exc_name, e) break else: raise","if isinstance(e, exc_cls):","if isinstance(exc_cls, Exception):",0.6372418676044622,0.8266114125804572,False
4001,"def scripts(self): application_root = current_app.config.get('APPLICATION_ROOT') subdir = application_root != '/' scripts = [] for script in get_registered_scripts(): <IF_STMT> scripts.append(f'<script defer src=""{script}""></script>') elif subdir: scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup('\n'.join(scripts))",if script.startswith('http'):,if script.startswith('/'):,0.9122226130367923,0.8866029039778043,False
4002,"def test_related_objects_local(self): result_key = 'get_all_related_objects_with_model_local' for model, expected in TEST_RESULTS[result_key].items(): objects = [(field, self._model(model, field)) for field in model._meta.get_fields(include_parents=False) <IF_STMT>] self.assertEqual(sorted(self._map_related_query_names(objects), key=self.key_name), sorted(expected, key=self.key_name))",if field.auto_created and (not field.concrete),if field.primary_key == self.key_name,0.7659343663257837,0.7801270245332924,False
4003,"def setTestOutcome(self, event): """"""Update outcome, exc_info and reason based on configured mappings"""""" if event.exc_info: ec, ev, tb = event.exc_info classname = ec.__name__ if classname in self.treatAsFail: short, long_ = self.labels(classname) self._setOutcome(event, 'failed', short, long_) <IF_STMT> short, long_ = self.labels(classname, upper=False) self._setOutcome(event, 'skipped', short, ""%s: '%s'"" % (long_, ev), str(ev))",elif classname in self.treatAsSkip:,elif classname in self.treatAsSkipped:,0.8167330826943853,0.8902056737869248,False
4004,"def small_count(v): if not v: return 0 z = [(1000000000, _('b')), (1000000, _('m')), (1000, _('k'))] v = int(v) for x, y in z: o, p = divmod(v, x) if o: <IF_STMT> return '%d%s' % (o, y) return '%.1f%s' % (v / float(x), y) return v",if len(str(o)) > 2 or not p:,if p:,0.9077943709224718,0.9202663016973823,False
4005,"def __read(self, n): if self._read_watcher is None: raise UnsupportedOperation('read') while 1: try: return _read(self._fileno, n) except (IOError, OSError) as ex: <IF_STMT> raise wait_on_watcher(self._read_watcher, None, None, self.hub)",if ex.args[0] not in ignored_errors:,if ex.errno != errno.EINTR:,0.6239688162541623,0.7965020533851944,False
4006,def locked(self): inputfiles = set(self.all_inputfiles()) outputfiles = set(self.all_outputfiles()) if os.path.exists(self._lockdir): for lockfile in self._locks('input'): with open(lockfile) as lock: for f in lock: f = f.strip() if f in outputfiles: return True for lockfile in self._locks('output'): with open(lockfile) as lock: for f in lock: f = f.strip() <IF_STMT> return True return False,if f in outputfiles or f in inputfiles:,if f in inputfiles:,0.9468131978482877,0.8944264839442453,False
4007,"def _flags_to_int(flags): if not flags: return 0 if isinstance(flags, integer_types): return flags result = 0 try: <IF_STMT> flags = flags.split(',') for value in flags: value = value.strip().lower() if value: result |= _flags_str2int[value] except KeyError as ex: raise ValueError('Invalid backend or flag: %s\nPossible values: %s' % (ex, ', '.join(sorted(_flags_str2int.keys())))) return result","if isinstance(flags, basestring):","if ',' in flags:",0.8716856120259066,0.8923575006167597,False
4008,"def setFg(self, colour, override=False): if not self.ttkFlag: self.containerStack[-1]['fg'] = colour gui.SET_WIDGET_FG(self._getContainerProperty('container'), colour, override) for child in self._getContainerProperty('container').winfo_children(): <IF_STMT> gui.SET_WIDGET_FG(child, colour, override) else: gui.trace('In ttk mode - trying to set FG to %s', colour) self.ttkStyle.configure('TLabel', foreground=colour) self.ttkStyle.configure('TFrame', foreground=colour)",if not self._isWidgetContainer(child):,if child.fg == colour:,0.9112642206342572,0.8555308664663046,False
4009,"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v['Category'] != 'Deprecated': if v['FeatureType'] == 'val': if name.startswith('SCE_'): states.append((name, v['Value'])) <IF_STMT> lexers.append((name, v['Value'])) return (lexers, states)",elif name.startswith('SCLEX_'):,elif name.startswith('LAB_'):,0.9333714393040909,0.8901732118131125,False
4010,def extract_error_message(response: requests.Response): if response.content: try: content = json.loads(response.content) <IF_STMT> return content['message'] except: logging.debug(f'Failed to parse the response content: {response.content}') return response.reason,if 'message' in content:,if 'message' in content:,0.6594276398627535,0.760856626273165,True
4011,"def canvas_size(self): """"""Return the width and height for this sprite canvas"""""" width = height = 0 for image in self.images: x = image.x + image.absolute_width y = image.y + image.absolute_height <IF_STMT> width = x if height < y: height = y return (round_up(width), round_up(height))",if width < x:,if width < x:,0.769622539852589,0.8780099567239787,True
4012,"def _load_widgets(self): logger.info('Loading plugins preferences widgets') for plugin in self.plugin_manager.get_active_plugins(): plugin_name = plugin.metadata.get('name') try: preferences_widget = plugin.get_preferences_widget() <IF_STMT> self._tabs.addTab(preferences_widget, plugin_name) except Exception as reason: logger.error('Unable to add the preferences widget (%s): %s', plugin_name, reason) continue",if preferences_widget:,if preferences_widget:,0.7709424984818661,0.8966773400768917,True
4013,"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: if att not in common_attributes: prefix_words_are_adj = False <IF_STMT> return (words[-1:], words[:-1]) else: return ([string], []) else: return ([string], [])",if prefix_words_are_adj:,if prefix_words_are_adj:,0.8267160518353271,0.9202663016973823,True
4014,"def _reader(): if shuffle: random.shuffle(file_list) while True: for fn in file_list: for line in open(fn, 'r'): yield self._process_line(line) <IF_STMT> break",if not cycle:,if not self._is_file_readable():,0.6437007137383942,0.8151678595510183,False
4015,"def load(weights, model, K, fsz, dil): index = 0 layers = model.layers for layer in layers._layers: <IF_STMT> if layer.W.shape == weights[index].shape: layer.W[:] = weights[index] else: layer.W[:] = dilate(weights[index], K, fsz, dil) index += 1","if hasattr(layer, 'W'):",if layer.W is not None:,0.7902544054081917,0.8200123297196334,False
4016,def upgrade(migrate_engine): print(__doc__) metadata.bind = migrate_engine liftoverjobs = dict() jobs = context.query(DeferredJob).filter_by(plugin='LiftOverTransferPlugin').all() for job in jobs: <IF_STMT> liftoverjobs[job.params['parentjob']] = [] liftoverjobs[job.params['parentjob']].append(job.id) for parent in liftoverjobs: lifts = liftoverjobs[parent] deferred = context.query(DeferredJob).filter_by(id=parent).first() deferred.params['liftover'] = lifts context.flush(),if job.params['parentjob'] not in liftoverjobs:,if job.params['parentjob'] not in liftoverjobs:,0.8301239852749701,0.8248765135255685,True
4017,"def get_refs(self, recursive=False): """""":see: AbstractExpression.get_refs()"""""" if recursive: conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), []) <IF_STMT> conds_refs.extend(self.consequent.get_refs(True)) return conds_refs else: return self.refs",if self.consequent:,if self.consequent:,0.620318650985119,0.8466657105524215,True
4018,"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): <IF_STMT> self.axis = engine.evaluate(self.args['axis'], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if 'momentum' in self.args: self.momentum = engine.evaluate(self.args['momentum'], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",if 'axis' in self.args:,if 'axis' in self.args:,0.9312888416026989,0.8723360571509826,True
4019,"def CountMatches(pat, predicate): num_matches = 0 for i in xrange(256): b = chr(i) m = pat.match(b) left = bool(m) right = predicate(i) if left != right: self.fail('i = %d, b = %r, match: %s, predicate: %s' % (i, b, left, right)) <IF_STMT> num_matches += 1 return num_matches",if m:,if m:,0.9498916837343148,0.9237460349978159,True
4020,"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError('You can either use {} with one positional argument or with keyword arguments, not both.'.format(cls.__name__)) if not args[0]: return super().__new__(cls) <IF_STMT> return cls return super().__new__(cls, *args, **kwargs)","if isinstance(args[0], cls):",elif not args[0]:,0.9072651994758973,0.8815741981066073,False
4021,"def concatenateCharacterTokens(tokens): pendingCharacters = [] for token in tokens: type = token['type'] if type in ('Characters', 'SpaceCharacters'): pendingCharacters.append(token['data']) else: <IF_STMT> yield {'type': 'Characters', 'data': ''.join(pendingCharacters)} pendingCharacters = [] yield token if pendingCharacters: yield {'type': 'Characters', 'data': ''.join(pendingCharacters)}",if pendingCharacters:,if pendingCharacters:,0.7404182266628738,0.9024521756077707,True
4022,"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): if func.type in support_set: pos_end = pos else: <IF_STMT> ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",if pos_end >= pos_start:,if pos_start >= pos_end:,0.8694129048361772,0.8661072626070159,False
4023,"def _visit(self, func): fname = func[0] if fname in self._flags: <IF_STMT> logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if self._flags[fname] == 1:,if self._flags[fname] == 1:,0.6212687289298717,0.8983343737277126,True
4024,"def graph_merge_softmax_with_crossentropy_softmax(node): if node.op == softmax_with_bias: x, b = node.inputs for x_client in x.clients: <IF_STMT> big_client = x_client[0] if big_client in [b_client[0] for b_client in b.clients]: xx, bb, ll = big_client.inputs mergeable_client = big_client.op(x, b, ll) copy_stack_trace(node.outputs[0], mergeable_client[1]) return [mergeable_client[1]]",if x_client[0].op == crossentropy_softmax_argmax_1hot_with_bias:,if x_client[0] in [x_client[0] for x_client in b.clients]:,0.8611791843277719,0.7899177245850753,False
4025,"def confidence(self): if self.bbox: distance = Distance(self.northeast, self.southwest, units='km') for score, maximum in [(10, 0.25), (9, 0.5), (8, 1), (7, 5), (6, 7.5), (5, 10), (4, 15), (3, 20), (2, 25)]: if distance < maximum: return score <IF_STMT> return 1 return 0",if distance >= 25:,elif distance > maximum:,0.9102666128962504,0.8723360571509826,False
4026,"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: if id(bp) == item_id: <IF_STMT> text = None bp.cond = text break self.RespondDebuggerData()",if text.strip().lower() == 'none':,if text == bp.cond:,0.9300146905939505,0.8752376177722327,False
4027,"def _handle_autocomplete_request_for_text(text): if not hasattr(text, 'autocompleter'): <IF_STMT> if isinstance(text, CodeViewText): text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind('<1>', text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()","if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():",if text.is_text_editable():,0.8931386880908703,0.8466657105524215,False
4028,"def visit_Macro(self, node, frame): macro_frame, macro_ref = self.macro_body(node, frame) self.newline() if frame.toplevel: <IF_STMT> self.write('context.exported_vars.add(%r)' % node.name) ref = frame.symbols.ref(node.name) self.writeline('context.vars[%r] = ' % node.name) self.write('%s = ' % frame.symbols.ref(node.name)) self.macro_def(macro_ref, macro_frame)",if not node.name.startswith('_'):,if macro_frame:,0.8763601312350509,0.8827916928185874,False
4029,"def execute(cls, ctx, op): try: pd.set_option('mode.use_inf_as_na', op.use_inf_as_na) <IF_STMT> return cls._execute_map(ctx, op) else: return cls._execute_combine(ctx, op) finally: pd.reset_option('mode.use_inf_as_na')",if op.stage == OperandStage.map:,if op.use_inf_as_na:,0.535193631337879,0.7778111223054219,False
4030,"def ranges(self, start, end): try: iterators = [i.ranges(start, end) for i in self.range_iterators] starts, ends, values = zip(*[next(i) for i in iterators]) starts = list(starts) ends = list(ends) values = list(values) while start < end: min_end = min(ends) yield (start, min_end, values) start = min_end for i, iterator in enumerate(iterators): <IF_STMT> starts[i], ends[i], values[i] = next(iterator) except StopIteration: return",if ends[i] == min_end:,if iterator is not None:,0.9076375086670665,0.8937168577929463,False
4031,"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) <IF_STMT> return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: if self.strict > 1: raise pass elif self.strict > 1: raise PermError('Empty domain-spec on exp=') return None",if len(a) == 1:,if len(a) == 1:,0.5825711340844816,0.8474968231198384,True
4032,"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue <IF_STMT> if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if exclude_unset:,if exclude_unset:,0.788451995288463,0.9237460349978159,True
4033,"def __setattr__(self, name, value): try: field = self._meta.get_field(name) <IF_STMT> value = value[:field.max_length] except models.fields.FieldDoesNotExist: pass super.__setattr__(self, name, value)","if type(field) in [models.CharField, models.TextField] and type(value) == str:",if field.max_length:,0.5874688169556357,0.7912619863720214,False
4034,"def create_child(self, value=None, _id=None): with atomic(savepoint=False): child_key = self.get_next_child_key() <IF_STMT> value = child_key child = self.__class__.objects.create(id=_id, key=child_key, value=value) return child",if value is None:,if value is None:,0.7265756355456795,0.7378351342269067,True
4035,"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {'HasMoreTags': False, 'Tags': tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): <IF_STMT> result['HasMoreTags'] = True break if exclusive_start_tag_key and key < exclusive_start_tag_key: continue tags.append({'Key': key, 'Value': val}) return result",if limit and len(tags) >= limit:,if limit and val[0] > limit:,0.9173208637999877,0.8375707157974782,False
4036,"def emit(self, record): try: app = get_app() <IF_STMT> msg = self.format(record) debug_buffer = app.layout.get_buffer_by_name('debug_buffer') current_document = debug_buffer.document.text if current_document: msg = '\n'.join([current_document, msg]) debug_buffer.set_document(Document(text=msg), bypass_readonly=True) else: super().emit(record) except: self.handleError(record)","if app.is_running and getattr(app, 'debug', False):",if app.layout:,0.7670718061886189,0.8743414417652072,False
4037,"def worker(): global error while True: num, q = pq.get() <IF_STMT> pq.task_done() break try: process_one(q) except Exception as e: error = e finally: pq.task_done()",if q is None or error is not None:,if num == 0:,0.5423203431167003,0.7801270245332924,False
4038,"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): <IF_STMT> if len(TRANSCEIVER[t]) < b + 1: out.append('(unknown)') else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate('Transceiver compliance', ', '.join(out))",if value & 128:,if value & 1:,0.9199252240672487,0.8627586293513119,False
4039,def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] <IF_STMT> return elif self.classifier.is_index_op(tok): tval = tok['text'] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break,if ttype == SCE_PL_UNUSED:,if ttype == 'eof':,0.8916559713636738,0.8592377270804451,False
4040,"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: <IF_STMT> continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",if item < vector[-1]:,if item < self.min_separation:,0.8940266286125775,0.9069443196104878,False
4041,"def __setattr__(self, name, value): if name == 'path': <IF_STMT> if value[0] != '/': raise ValueError('The page path should always start with a slash (""/"").') elif name == 'load_time': if value and (not isinstance(value, int)): raise ValueError('Page load time must be specified in integer milliseconds.') object.__setattr__(self, name, value)",if value and value != '':,if value:,0.9202418276124823,0.9237460349978159,False
4042,"def awaitTermination(self, timeout=None): if self.scheduler is None: raise RuntimeError('StreamimgContext not started') try: deadline = time.time() + timeout if timeout is not None else None while True: is_terminated = self._runOnce() <IF_STMT> break if self.batchCallback: self.batchCallback() except KeyboardInterrupt: pass finally: self.sc.stop() logger.info('StreamingContext stopped successfully')",if is_terminated or (deadline is not None and time.time() > deadline):,if is_terminated:,0.9466640617139327,0.9144061946646023,False
4043,def stopbutton(self): if GPIOcontrol: while mediastopbutton: time.sleep(0.25) <IF_STMT> print('Stopped') stop(),if not GPIO.input(stoppushbutton):,if self.is_active():,0.5220677408598065,0.6315552371794037,False
4044,"def test_create_connection_timeout(self): with self.mocked_socket_module(): try: socket.create_connection((HOST, 1234)) except socket.timeout: pass except OSError as exc: <IF_STMT> raise else: self.fail('socket.timeout not raised')",if support.IPV6_ENABLED or exc.errno != errno.EAFNOSUPPORT:,if exc.errno != errno.ECONNABORTED:,0.5900418476222353,0.7378351342269067,False
4045,"def handle_exception_and_die(e): if hasattr(e, 'kind'): <IF_STMT> sys.stderr.write('ABORT: ' + e.msg + '\n') sys.exit(e.value) elif e.kind == 'exit': sys.stderr.write('EXITING\n') sys.exit(e.value) else: print(str(e)) sys.exit(1)",if e.kind == 'die':,if e.kind == 'abort':,0.6255993299946814,0.760856626273165,False
4046,"def gets(self, key): with self.client_pool.get_and_release(destroy_on_fail=True) as client: try: return client.gets(key) except Exception: <IF_STMT> return (None, None) else: raise",if self.ignore_exc:,if self.fail_silently:,0.8261941572581478,0.7912619863720214,False
4047,"def _execute(self, options, args): if len(args) < 3: raise CommandError(_('Not enough arguments')) tag = fsn2text(args[0]) value = fsn2text(args[1]) paths = args[2:] songs = [] for path in paths: song = self.load_song(path) <IF_STMT> raise CommandError(_('Can not set %r') % tag) self.log('Add %r to %r' % (value, tag)) song.add(tag, value) songs.append(song) self.save_songs(songs)",if not song.can_change(tag):,if not song:,0.8450587943867433,0.9100365300271298,False
4048,"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = '' if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) if place: place_title = place_displayer.display(self.dbstate.db, place) <IF_STMT> if len(place_title) > 25: text = place_title[:24] + '...' else: text = place_title return text",if place_title != '':,if place_title:,0.6359285756974015,0.9024521756077707,False
4049,"def _Determine_Do(self): self.applicable = 1 self.value = os.environ.get(self.name, None) if self.value is None and black.configure.items.has_key('buildType'): buildType = black.configure.items['buildType'].Get() <IF_STMT> self.value = 'warn' else: self.value = None self.determined = 1",if buildType == 'debug':,if buildType == 'warn':,0.6336123041669335,0.8169276475307028,False
4050,"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) <IF_STMT> continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, '__init__.py')): self.bundle_package(itempath) elif nm.endswith('.py'): self.bundle_module(itempath)",if nm.startswith('.'):,if not nm:,0.9313413776943221,0.8713933650206428,False
4051,"def header_fields(self, fields): headers = dict(self.conn.response.getheaders()) ret = {} for field in fields: <IF_STMT> raise ValueError('%s was not found in response header' % field[1]) try: ret[field[0]] = int(headers[field[1]]) except ValueError: ret[field[0]] = headers[field[1]] return ret",if not headers.has_key(field[1]):,if field[1] not in headers:,0.7382799178222688,0.8248765135255685,False
4052,"def caesar_cipher(s, k): result = '' for char in s: n = ord(char) <IF_STMT> n = (n - 65 + k) % 26 + 65 if 96 < n < 123: n = (n - 97 + k) % 26 + 97 result = result + chr(n) return result",if 64 < n < 91:,if 65 < n < 65:,0.7255860619700847,0.8562773802729167,False
4053,"def qtTypeIdent(conn, *args): res = None value = None for val in args: if not hasattr(val, '__len__'): val = str(val) <IF_STMT> continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = (res and res + '.' or '') + value return res",if len(val) == 0:,if not Driver.isIdent(val):,0.6953236486602326,0.9134996171406936,False
4054,"def _parse_timezone(value: Optional[str], error: Type[Exception]) -> Union[None, int, timezone]: if value == 'Z': return timezone.utc elif value is not None: offset_mins = int(value[-2:]) if len(value) > 3 else 0 offset = 60 * int(value[1:3]) + offset_mins <IF_STMT> offset = -offset try: return timezone(timedelta(minutes=offset)) except ValueError: raise error() else: return None",if value[0] == '-':,if offset < 0:,0.7980990046415825,0.8923575006167597,False
4055,"def indent(elem, level=0): i = '\n' + level * '  ' if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + '  ' <IF_STMT> elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i elif level and (not elem.tail or not elem.tail.strip()): elem.tail = i",if not elem.tail or not elem.tail.strip():,if not elem.tail or not elem.tail.strip():,0.9383785043422553,0.8754021059663507,True
4056,"def _make_slices(shape: tp.Tuple[int, ...], axes: tp.Tuple[int, ...], size: int, rng: np.random.RandomState) -> tp.List[slice]: slices = [] for a, s in enumerate(shape): if a in axes: <IF_STMT> raise ValueError('Cannot crossover on axis with size 1') start = rng.randint(s - size) slices.append(slice(start, start + size)) else: slices.append(slice(None)) return slices",if s <= 1:,if s < 1:,0.9097074188786618,0.8856327184319047,False
4057,"def _loadTestsFromTestCase(self, event, testCaseClass): evt = events.LoadFromTestCaseEvent(event.loader, testCaseClass) result = self.session.hooks.loadTestsFromTestCase(evt) if evt.handled: loaded_suite = result or event.loader.suiteClass() else: names = self._getTestCaseNames(event, testCaseClass) <IF_STMT> names = ['runTest'] loaded_suite = event.loader.suiteClass(map(testCaseClass, names)) if evt.extraTests: loaded_suite.addTests(evt.extraTests) return loaded_suite","if not names and hasattr(testCaseClass, 'runTest'):",if not names:,0.8525397758798263,0.8749766281017177,False
4058,"def check_settings(self): if self.settings_dict['TIME_ZONE'] is not None: if not settings.USE_TZ: raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because USE_TZ is False."" % self.alias) <IF_STMT> raise ImproperlyConfigured(""Connection '%s' cannot set TIME_ZONE because its engine handles time zones conversions natively."" % self.alias)",elif self.features.supports_timezones:,if settings.TIME_ZONE not in self.settings_dict['TIME_ZONE']:,0.8838354881861377,0.8419539711731491,False
4059,"def collect_conflicting_diffs(path, decisions): local_conflict_diffs = [] remote_conflict_diffs = [] for d in decisions: <IF_STMT> ld = adjust_patch_level(path, d.common_path, d.local_diff) rd = adjust_patch_level(path, d.common_path, d.remote_diff) local_conflict_diffs.extend(ld) remote_conflict_diffs.extend(rd) return (local_conflict_diffs, remote_conflict_diffs)",if d.conflict:,if d.common_path:,0.8560804019095216,0.8743414417652072,False
4060,"def short_repr(obj): if isinstance(obj, (type, types.ModuleType, types.BuiltinMethodType, types.BuiltinFunctionType)): return obj.__name__ if isinstance(obj, types.MethodType): <IF_STMT> return obj.im_func.__name__ + ' (bound)' else: return obj.im_func.__name__ if isinstance(obj, (tuple, list, dict, set)): return '%d items' % len(obj) if isinstance(obj, weakref.ref): return 'all_weakrefs_are_one' return repr(obj)[:40]",if obj.im_self is not None:,if obj.bound:,0.896807653532379,0.9099951253570094,False
4061,"def _massage_uri(uri): if uri: <IF_STMT> uri = uri.replace('hdfs://', get_defaultfs()) elif uri.startswith('/'): uri = get_defaultfs() + uri return uri",if uri.startswith('hdfs:///'):,if uri.startswith('hdfs://'):,0.8445950134402825,0.7912619863720214,False
4062,"def chsub(self, msg, chatid): cmd, evt, params = self.tokenize(msg, 3) if cmd == '/sub': sql = 'replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?)' el<IF_STMT> sql = 'delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1)' else: sql = 'delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ?' with self.bot.database as conn: conn.execute(sql, [chatid, evt, params]) conn.commit() return",if evt == 'everything':,if evt == '/delete':,0.8246956670169888,0.9277431284700743,False
4063,"def undefined_symbols(self): result = [] for p in self.Productions: <IF_STMT> continue for s in p.prod: if not s in self.Prodnames and (not s in self.Terminals) and (s != 'error'): result.append((s, p)) return result",if not p:,if not p.is_defined():,0.8443265683687793,0.8635707684233572,False
4064,"def modify_column(self, column: List[Optional['Cell']]): for i in range(len(column)): gate = column[i] if gate is self: continue <IF_STMT> column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])","elif isinstance(gate, ParityControlCell):",if gate is None:,0.617449007879617,0.8431339019329497,False
4065,"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): <IF_STMT> rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",if k == neighbors.MULTI_EXIT_DISC:,if k == neighbors.MEDIUM:,0.9042349111191978,0.8105932471967202,False
4066,"def writexml(self, stream, indent='', addindent='', newl='', strip=0, nsprefixes={}, namespace=''): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue <IF_STMT> v = str(v) if strip: v = ' '.join(v.split()) val = escape(v) w(val)","if not isinstance(v, str):","if not isinstance(v, str):",0.826848329004498,0.8723360571509826,True
4067,"def _condition(ct): for qobj in args: <IF_STMT> for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError('Unsupported Q object') for attr, val in kwargs.items(): if getattr(ct, attr) != val: return False return True",if qobj.connector == 'AND' and (not qobj.negated):,"if isinstance(qobj, QObject):",0.907340301685063,0.8547305998833805,False
4068,"def results_iter(self): <IF_STMT> from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] elif needs_string_cast: date = typecast_timestamp(str(date)) yield date",if self.connection.ops.oracle:,if self.connection.ops.oracle:,0.9517039088384065,0.9188912094899004,True
4069,"def get_job_type(self): if int(self.job_runtime_conf.get('dsl_version', 1)) == 2: job_type = self.job_runtime_conf['job_parameters'].get('common', {}).get('job_type') <IF_STMT> job_type = self.job_runtime_conf['job_parameters'].get('job_type', 'train') else: job_type = self.job_runtime_conf['job_parameters'].get('job_type', 'train') return job_type",if not job_type:,if job_type == 'train':,0.7162568899638581,0.7709002428237395,False
4070,def validate_assessment_criteria(self): if self.assessment_criteria: total_weightage = 0 for criteria in self.assessment_criteria: total_weightage += criteria.weightage or 0 <IF_STMT> frappe.throw(_('Total Weightage of all Assessment Criteria must be 100%')),if total_weightage != 100:,if total_weightage > 100:,0.7046820934743538,0.7965020533851944,False
4071,"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split(':') <IF_STMT> _id = split_provider_id[1] cursor = self.get_by_id(_id) if cursor: result.append(cursor) return result",if len(split_provider_id) == 2:,if len(split_provider_id) > 1:,0.876133671688301,0.8169276475307028,False
4072,"def dump_predictions_to_database(relation, predictions): judge = 'iepy-run on {}'.format(datetime.now().strftime('%Y-%m-%d %H:%M')) for evidence, relation_is_present in predictions.items(): label = EvidenceLabel.YESRELATION <IF_STMT> else EvidenceLabel.NORELATION evidence.set_label(relation, label, judge, labeled_by_machine=True)",if relation_is_present,if relation_is_present,0.8162717653423863,0.8466657105524215,True
4073,"def __init__(self, **kwargs): dfl = get_model_label(self.default_model_class) if 'to' in kwargs.keys(): old_to = get_model_label(kwargs.pop('to')) <IF_STMT> msg = '%s can only be a ForeignKey to %s; %s passed' % (self.__class__.__name__, dfl, old_to) warnings.warn(msg, SyntaxWarning) kwargs['to'] = dfl super().__init__(**kwargs)",if old_to.lower() != dfl.lower():,if old_to != dfl:,0.7440792088471612,0.8516228624291206,False
4074,"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = (rb.data[ri], lb.data[li]) li += 1 if li >= BLOCKLEN: lb = lb.rightlink li = 0 ri -= 1 <IF_STMT> rb = rb.leftlink ri = BLOCKLEN - 1",if ri < 0:,if ri >= BLOCKLEN:,0.9263918838633204,0.8983343737277126,False
4075,"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: if API_CACHE is None: API_CACHE = {} <IF_STMT> API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",if API_CACHE.get(url) is None:,if url not in API_CACHE:,0.7937027763169711,0.8200123297196334,False
4076,"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = (index.start, index.stop) new_start, new_stop = (None, None) if start is not None: new_stop = length - start <IF_STMT> new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",if stop is not None:,if stop is not None:,0.7765276526881861,0.8970855583859476,True
4077,"def infer_returned_object(pyfunction, args): """"""Infer the `PyObject` this `PyFunction` returns after calling"""""" object_info = pyfunction.pycore.object_info result = object_info.get_exact_returned(pyfunction, args) if result is not None: return result result = _infer_returned(pyfunction, args) if result is not None: <IF_STMT> params = args.get_arguments(pyfunction.get_param_names(special_args=False)) object_info.function_called(pyfunction, params, result) return result return object_info.get_returned(pyfunction, args)",if args and pyfunction.get_module().get_resource() is not None:,if result is not None:,0.9247684310756039,0.8649799950178215,False
4078,"def _check_imports(lib): libs = ['PyQt4', 'PyQt5', 'PySide'] libs.remove(lib) for lib2 in libs: lib2 += '.QtCore' <IF_STMT> raise RuntimeError('Refusing to import %s because %s is already imported.' % (lib, lib2))",if lib2 in sys.modules:,if lib2 in libs:,0.7797795050216344,0.8169276475307028,False
4079,"def _poll(fds, timeout): if timeout is not None: timeout = int(timeout * 1000) fd_map = {} pollster = select.poll() for fd in fds: pollster.register(fd, select.POLLIN) <IF_STMT> fd_map[fd.fileno()] = fd else: fd_map[fd] = fd ls = [] for fd, event in pollster.poll(timeout): if event & select.POLLNVAL: raise ValueError('invalid file descriptor %i' % fd) ls.append(fd_map[fd]) return ls","if hasattr(fd, 'fileno'):",if fd.is_file():,0.9077050166305346,0.9350761925543661,False
4080,"def default(cls, connection=None): """"""show the default connection, or make CONNECTION the default"""""" if connection is not None: target = cls._get_config_filename(connection) <IF_STMT> if os.path.exists(cls._default_symlink): os.remove(cls._default_symlink) os.symlink(target, cls._default_symlink) else: cls._no_config_file_error(target) if os.path.exists(cls._default_symlink): print('Default connection is ' + cls._default_connection()) else: print('There is no default connection set')",if os.path.exists(target):,if os.path.exists(target):,0.7256846609047376,0.9164531641034833,True
4081,"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, '..') for line in fuzzresult.history.content.splitlines(): record = line.split('/') <IF_STMT> self.queue_url(urljoin(base_url, record[1])) if record[0] == 'D': self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, '%s/CVS/Entries' % record[1]))",if len(record) == 6 and record[1]:,if record[0] == 'CVS':,0.9113848142508116,0.8038019482772603,False
4082,"def _GetCSVRow(self, value): row = [] for type_info in value.__class__.type_infos: <IF_STMT> row.extend(self._GetCSVRow(value.Get(type_info.name))) elif isinstance(type_info, rdf_structs.ProtoBinary): row.append(text.Asciify(value.Get(type_info.name))) else: row.append(str(value.Get(type_info.name))) return row","if isinstance(type_info, rdf_structs.ProtoEmbedded):","if isinstance(type_info, rdf_structs.ProtoList):",0.8760202009449806,0.7739321540474097,False
4083,"def get_history(self, state, dict_, passive=PASSIVE_OFF): if self.key in dict_: return History.from_scalar_attribute(self, state, dict_[self.key]) else: <IF_STMT> passive ^= INIT_OK current = self.get(state, dict_, passive=passive) if current is PASSIVE_NO_RESULT: return HISTORY_BLANK else: return History.from_scalar_attribute(self, state, current)",if passive & INIT_OK:,if passive == PASSIVE_OFF:,0.8322380134402259,0.8431339019329497,False
4084,"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) <IF_STMT> break elif current._parent is None: raise sa_exc.InvalidRequestError('Transaction %s is not on the active transaction list' % upto) else: current = current._parent return result",if current._parent is upto:,if current._parent is None and upto is not None:,0.771975649086428,0.7626292902075286,False
4085,"def get_by_uri(self, uri: str) -> bytes: userId, bucket, key = self._parse_uri(uri) try: with db.session_scope() as dbsession: result = db_archivedocument.get(userId, bucket, key, session=dbsession) <IF_STMT> return utils.ensure_bytes(self._decode(result)) else: raise ObjectKeyNotFoundError(userId, bucket, key, caused_by=None) except Exception as err: logger.debug('cannot get data: exception - ' + str(err)) raise err",if result:,if result:,0.6669596775347628,0.9202663016973823,True
4086,"def app(scope, receive, send): while True: message = await receive() if message['type'] == 'websocket.connect': await send({'type': 'websocket.accept'}) elif message['type'] == 'websocket.receive': pass <IF_STMT> break",elif message['type'] == 'websocket.disconnect':,elif message['type'] == 'websocket.disconnect':,0.611271425319478,0.8124760938075204,True
4087,"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = '' if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <IF_STMT> y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return ''.join(y)",elif r:,if e:,0.6655959445914909,0.938501942261528,False
4088,"def mouse_down(self, event): if event.button == 1: <IF_STMT> p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",if self.scrolling:,if self.scroll_up_rect().isVisible():,0.9102512628215718,0.8827916928185874,False
4089,"def copy_from(self, other): if self is other: return self.strictness = other.strictness for name in self.all_behaviors: self.set_behavior(name, other.get_behavior(name)) for name in self._plain_attrs: val = getattr(other, name) if isinstance(val, set): val = val.copy() <IF_STMT> val = val.copy() setattr(self, name, val)","elif decimal and isinstance(val, decimal.Decimal):","elif isinstance(val, dict):",0.8478355799435222,0.8815741981066073,False
4090,"def __array_wrap__(self, out_arr, context=None): if self.dim is None: return out_arr else: this = self[:] <IF_STMT> return Quantity.__array_wrap__(self[:], out_arr, context=context) else: return out_arr","if isinstance(this, Quantity):",if this.dim == self.dim:,0.6233211458570239,0.760856626273165,False
4091,"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: <IF_STMT> length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",if token.value == '{':,if token.OpensScope():,0.931955115867223,0.9184043388013005,False
4092,"def save_all_changed_extensions(self): """"""Save configuration changes to the user config file."""""" has_changes = False for ext_name in self.extensions: options = self.extensions[ext_name] for opt in options: <IF_STMT> has_changes = True if has_changes: self.ext_userCfg.Save()","if self.set_extension_value(ext_name, opt):",if opt.HasChanged():,0.8793588143089861,0.8827916928185874,False
4093,"def to_dict(self): out = {} for key in ACTIVITY_KEYS: attr = getattr(self, key) <IF_STMT> out[key] = str(attr) else: out[key] = attr if self.streak: out['streak'] = self.streak return out","if isinstance(attr, (datetime.timedelta, datetime.datetime)):","if isinstance(attr, str):",0.6843580574115675,0.8390782502060267,False
4094,"def clean_publication_date(cls, cleaned_input): for add_channel in cleaned_input.get('add_channels', []): is_published = add_channel.get('is_published') publication_date = add_channel.get('publication_date') <IF_STMT> add_channel['publication_date'] = datetime.date.today()",if is_published and (not publication_date):,if is_published and publication_date:,0.730350098781827,0.7098232254187811,False
4095,"def _random_blur(self, batch, sigma_max): for i in range(len(batch)): <IF_STMT> sigma = random.uniform(0.0, sigma_max) batch[i] = scipy.ndimage.filters.gaussian_filter(batch[i], sigma) return batch",if bool(random.getrandbits(1)):,if i % 2 == 0:,0.7911382768245065,0.6599790945055583,False
4096,"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() <IF_STMT> return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if i >= length:,if param == 'default':,0.7881937446455722,0.8983343737277126,False
4097,"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): if is_forwarded_environment_variable(name): <IF_STMT> value = file_host.rebase_to_worker_root(value) env[name] = value",if os.getenv('TRUSTED_HOST') and should_rebase_environment_value(name):,if not is_worker_root(value):,0.8856799250857836,0.833078701050083,False
4098,"def toterminal(self, tw): last_style = None for i, entry in enumerate(self.reprentries): if entry.style == 'long': tw.line('') entry.toterminal(tw) <IF_STMT> next_entry = self.reprentries[i + 1] if entry.style == 'long' or (entry.style == 'short' and next_entry.style == 'long'): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",if i < len(self.reprentries) - 1:,if i + 1 < len(self.reprentries):,0.921042354415047,0.8221293984712319,False
4099,"def __init__(self, loc, tabs=None): if os.path.isdir(loc): for item in os.listdir(loc): <IF_STMT> continue path = os.path.join(loc, item) self.append(CronTab(user=False, tabfile=path)) elif os.path.isfile(loc): self.append(CronTab(user=False, tabfile=loc))",if item[0] == '.':,if item.startswith('.'):,0.8152456400244412,0.8318180062062374,False
4100,"def import_data(self, fname): """"""Import data in current namespace"""""" if self.count(): nsb = self.currentWidget() nsb.refresh_table() nsb.import_data(fname) <IF_STMT> self.dockwidget.setVisible(True) self.dockwidget.raise_()",if self.dockwidget and (not self.ismaximized):,if self.dockwidget:,0.7411977878758743,0.8086627571031982,False
4101,"def get_menu_items(node): aList = [] for child in node.children: for tag in ('@menu', '@item'): <IF_STMT> name = child.h[len(tag) + 1:].strip() if tag == '@menu': aList.append(('%s %s' % (tag, name), get_menu_items(child), None)) else: b = g.splitLines(''.join(child.b)) aList.append((tag, name, b[0] if b else '')) break return aList",if child.h.startswith(tag):,if child.h.startswith(tag):,0.9292875835921551,0.9202663016973823,True
4102,"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError('MultiDict can only be called with one positional argument') if args: if hasattr(args[0], 'iteritems'): items = list(args[0].iteritems()) <IF_STMT> items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())","elif hasattr(args[0], 'items'):","elif hasattr(args[0], 'items'):",0.9133647598087427,0.897752847848028,True
4103,"def open(self) -> 'KeyValueDb': """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): if not os.path.isfile(self._name): raise IOError('%s exists and is not a file' % self._name) <IF_STMT> return self with open(self._name, 'rb') as _in: self.set_records(pickle.load(_in)) else: mkpath(os.path.dirname(self._name)) self.commit() return self",if os.path.getsize(self._name) == 0:,if self.is_dirty():,0.9177196549198156,0.9144061946646023,False
4104,"def sortModules(self): super(NeuronDecomposableNetwork, self).sortModules() self._constructParameterInfo() self.decompositionIndices = {} for neuron in self._neuronIterator(): self.decompositionIndices[neuron] = [] for w in range(self.paramdim): inneuron, outneuron = self.paramInfo[w] <IF_STMT> self.decompositionIndices[inneuron].append(w) else: self.decompositionIndices[outneuron].append(w)",if self.espStyleDecomposition and outneuron[0] in self.outmodules:,if inneuron in self.decompositionIndices:,0.8583345668413901,0.8038019482772603,False
4105,"def visit_Options(self, node: qlast.Options) -> None: for i, opt in enumerate(node.options.values()): <IF_STMT> self.write(' ') self.write(opt.name) if not isinstance(opt, qlast.Flag): self.write(f' {opt.val}')",if i > 0:,if i == 0:,0.8074342770546068,0.7498810286408993,False
4106,"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: if possible_child_hash == item_hash: return True <IF_STMT> return False possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash not in self.items:,if possible_child_hash == self.items[possible_child_hash]:,0.819789264887527,0.7801270245332924,False
4107,"def __call__(self, text, **kargs): words = jieba.tokenize(text, mode='search') token = Token() for w, start_pos, stop_pos in words: <IF_STMT> continue token.original = token.text = w token.pos = start_pos token.startchar = start_pos token.endchar = stop_pos yield token",if not accepted_chars.match(w) and len(w) <= 1:,if w == '':,0.8798137565067357,0.8474968231198384,False
4108,"def test_analysis_jobs_cypher_syntax(neo4j_session): parameters = {'AWS_ID': None, 'UPDATE_TAG': None, 'OKTA_ORG_ID': None} for job_name in contents('cartography.data.jobs.analysis'): <IF_STMT> continue try: cartography.util.run_analysis_job(job_name, neo4j_session, parameters) except Exception as e: pytest.fail(f""run_analysis_job failed for analysis job '{job_name}' with exception: {e}"")",if not job_name.endswith('.json'):,if job_name.startswith('_'):,0.8717883458780777,0.8901732118131125,False
4109,"def _interleave_dataset_results_and_tensors(dataset_results, flat_run_tensors): flattened_results = [] for idx in range(len(dataset_results) + len(flat_run_tensors)): <IF_STMT> flattened_results.append(dataset_results[idx]) else: flattened_results.append(flat_run_tensors.pop(0)) return flattened_results",if dataset_results.get(idx):,if len(dataset_results) == len(flat_run_tensors):,0.8310591445419229,0.7098232254187811,False
4110,"def test_k_is_stochastic_parameter(self): aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) <IF_STMT> seen[0] += True elif np.array_equal(observed, self.blur5x5): seen[1] += True else: raise Exception('Unexpected result in MedianBlur@2') if all(seen): break assert np.all(seen)","if np.array_equal(observed, self.blur3x3):","if np.array_equal(observed, self.blur3x3):",0.7082767374511528,0.8815741981066073,True
4111,"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return <IF_STMT> return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",if minGuide == None:,if minGuide == None:,0.8059619308690232,0.9036816878108535,True
4112,"def UpdateRepository(self): if hasattr(self, 'commit_update'): <IF_STMT> if not path.isdir('.git/'): self.gitZipRepo() call(['git', 'reset', '--hard', 'origin/{}'.format(self.getBranch)]) self.ProcessCall_(['git', 'pull', 'origin', self.getBranch]) self.ProcessCall_(['pip', 'install', '-r', 'requirements.txt'])",if self.commit_update['Updates'] != []:,if self.commit_update:,0.6452530912290003,0.8318180062062374,False
4113,"def callback(result=Cr.NS_OK, message=None, success=None): if success is None: <IF_STMT> success = Ci.koIAsyncCallback.RESULT_SUCCESSFUL else: success = Ci.koIAsyncCallback.RESULT_ERROR data = Namespace(result=result, message=message, _com_interfaces_=[Ci.koIErrorInfo]) self._invoke_activate_callbacks(success, data)",if Cr.NS_SUCCEEDED(result):,if result == Cr.NS_OK:,0.8498711848684756,0.7709002428237395,False
4114,def get_location(device): location = [] node = device while node: position = node.get_position() or '' <IF_STMT> position = ' [%s]' % position location.append(node.name + position) node = node.parent return ' / '.join(reversed(location)),if position:,if position:,0.719878119293655,0.8866029039778043,True
4115,"def load_checkpoint(path, model, optimizer, reset_optimizer): global global_step global global_epoch print('Load checkpoint from: {}'.format(path)) checkpoint = _load(path) model.load_state_dict(checkpoint['state_dict']) if not reset_optimizer: optimizer_state = checkpoint['optimizer'] <IF_STMT> print('Load optimizer state from {}'.format(path)) optimizer.load_state_dict(checkpoint['optimizer']) global_step = checkpoint['global_step'] global_epoch = checkpoint['global_epoch'] return model",if optimizer_state is not None:,if optimizer_state != optimizer:,0.6610765594063521,0.8592377270804451,False
4116,"def run_command(self, command: str, data: Dict[str, object]) -> Dict[str, object]: """"""Run a specific command from the registry."""""" key = 'cmd_' + command method = getattr(self.__class__, key, None) if method is None: return {'error': ""Unrecognized command '%s'"" % command} else: <IF_STMT> del data['is_tty'] del data['terminal_width'] return method(self, **data)","if command not in {'check', 'recheck', 'run'}:",if 'is_tty' in data:,0.7904265796471148,0.8856327184319047,False
4117,"def call_init(self, node, instance): for b in instance.bindings: <IF_STMT> continue self._initialized_instances.add(b.data) node = self._call_init_on_binding(node, b) return node",if b.data in self._initialized_instances:,if b.data in self._initialized_instances:,0.7824875850298625,0.693395566222006,True
4118,"def get_request_headers() -> Dict: url = urlparse(uri) candidates = ['%s://%s' % (url.scheme, url.netloc), '%s://%s/' % (url.scheme, url.netloc), uri, '*'] for u in candidates: <IF_STMT> headers = dict(DEFAULT_REQUEST_HEADERS) headers.update(self.config.linkcheck_request_headers[u]) return headers return {}",if u in self.config.linkcheck_request_headers:,if u in self.config.linkcheck_request_headers:,0.8902006216987348,0.8336104423443033,True
4119,"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: video_packet = self._get_video_packet() if video_packet.image == 0: self._decode_video_packet(video_packet) <IF_STMT> break if _debug: print('Returning', video_packet) return video_packet.image",if video_packet.image is not None or not skip_empty_frame:,if skip_empty_frame:,0.9102358055985003,0.8531413606256201,False
4120,"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) <IF_STMT> ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",elif code == Path.CURVE3:,elif code == Path.CURVE3:,0.6694548608926989,0.8474968231198384,True
4121,"def __init__(self, layout, value=None, string=None, *, dtype: np.dtype=np.float64) -> None: """"""Constructor."""""" self.layout = layout if value is None: if string is None: self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) <IF_STMT> raise ValueError('value must be a sequence of length %s' % self.layout.gaDims)","if self.value.shape != (self.layout.gaDims,):",if len(self.value.shape) != self.layout.gaDims:,0.7476829558940936,0.8832000938217648,False
4122,"def to_dict(self): contexts_ = {} for k, data in self.contexts.items(): data_ = data.copy() if 'context' in data_: del data_['context'] <IF_STMT> del data_['loaded'] contexts_[k] = data_ return dict(contexts=contexts_)",if 'loaded' in data_:,if 'loaded' in data_:,0.8888690457879836,0.8038019482772603,True
4123,def include_module(module): if not include_these: return True result = False for check in include_these: if '/*' in check: <IF_STMT> result = True elif os.getcwd() + '/' + check + '.py' == module: result = True if result: print_status('Including module: ' + module) return result,if check[:-1] in module:,if os.path.join(os.getcwd() + '/' + check + '.py'):,0.8438532552905404,0.8070218370798333,False
4124,"def extract_from(msg_body, content_type='text/plain'): try: if content_type == 'text/plain': return extract_from_plain(msg_body) <IF_STMT> return extract_from_html(msg_body) except Exception: log.exception('ERROR extracting message') return msg_body",elif content_type == 'text/html':,elif content_type == 'text/html':,0.8487308302347834,0.7378351342269067,True
4125,"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data['features']), 2) for feature in response.data['features']: self.assertIn('bbox', feature) fid = feature['id'] <IF_STMT> self.assertEqual(feature['bbox'], self.bl1.bbox_geometry.extent) elif fid == 2: self.assertEqual(feature['bbox'], self.bl2.bbox_geometry.extent) else: self.fail('Unexpected id: {0}'.format(fid)) BoxedLocation.objects.all().delete()",if fid == 1:,if fid == 1:,0.8671826272500508,0.8385130047130208,True
4126,"def overrideCommand(self, commandName, func): k = self d = k.masterBindingsDict for key in d: d2 = d.get(key) for key2 in d2: bi = d2.get(key2) <IF_STMT> bi.func = func d2[key2] = bi",if bi.commandName == commandName:,if bi is not None and bi.name == commandName:,0.7412077138729584,0.7178403422697396,False
4127,"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) <IF_STMT> r = _lookup(comps, specs, provided, name, i + 1, l) if r is not None: return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",if comps:,if comps:,0.7851097546915027,0.938501942261528,True
4128,"def to_representation(self, value): old_social_string_fields = ['twitter', 'github', 'linkedIn'] request = self.context.get('request') show_old_format = request and is_deprecated(request.version, self.min_version) and (request.method == 'GET') if show_old_format: social = value.copy() for key in old_social_string_fields: if social.get(key): social[key] = value[key][0] <IF_STMT> social[key] = '' value = social return super(SocialField, self).to_representation(value)",elif social.get(key) == []:,elif social[key] == '':,0.9090695470034771,0.8806615362338783,False
4129,"def process_ref_attribute(self, node, array_type=None): ref = qname_attr(node, 'ref') if ref: ref = self._create_qname(ref) <IF_STMT> return return xsd_elements.RefAttribute(node.tag, ref, self.schema, array_type=array_type)",if ref.namespace == 'http://www.w3.org/2001/XMLSchema':,if not ref:,0.6434433133024051,0.7739321540474097,False
4130,"def unescape(text): """"""Removes '\\' escaping from 'text'."""""" rv = '' i = 0 while i < len(text): <IF_STMT> rv += text[i + 1] i += 1 else: rv += text[i] i += 1 return rv",if i + 1 < len(text) and text[i] == '\\':,if text[i] == '\\':,0.7669912052316397,0.8474968231198384,False
4131,"def wait_child_process(signum, frame): try: while True: child_pid, status = os.waitpid(-1, os.WNOHANG) if child_pid == 0: stat_logger.info('no child process was immediately available') break exitcode = status >> 8 stat_logger.info('child process %s exit with exitcode %s', child_pid, exitcode) except OSError as e: <IF_STMT> stat_logger.warning('current process has no existing unwaited-for child processes.') else: raise",if e.errno == errno.ECHILD:,if e.errno == errno.EINTR:,0.6599456985005803,0.8944264839442453,False
4132,"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) if ctg[0] == 'L' and unicodedata.name(c).find('LATIN') == -1: for separator in (' & ', '; ', ' and ', ' vs. ', ' with ', ' y '): <IF_STMT> parts = sortname.split(separator) break else: parts = [sortname] separator = '' return separator.join(map(_reverse_sortname, parts)) return name",if separator in sortname:,if sortname.startswith(separator):,0.6932515264660789,0.9434724611166208,False
4133,"def python_value(self, value): if value: <IF_STMT> pp = lambda x: x.time() return format_date_time(value, self.formats, pp) elif isinstance(value, datetime.datetime): return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value","if isinstance(value, basestring):","if isinstance(value, datetime.datetime):",0.8232095738233758,0.8675979125638379,False
4134,"def __init__(self, fileobj, info): pages = [] complete = False while not complete: page = OggPage(fileobj) <IF_STMT> pages.append(page) complete = page.complete or len(page.packets) > 1 data = OggPage.to_packets(pages)[0][7:] super(OggTheoraCommentDict, self).__init__(data, framing=False) self._padding = len(data) - self._size",if page.serial == info.serial:,if page.info == info:,0.6806988822807819,0.8516228624291206,False
4135,"def configure(self): if 'from_' in self.wmeta.properties: from_ = float(self.wmeta.properties['from_']) to = float(self.wmeta.properties.get('to', 0)) <IF_STMT> to = from_ + 1 self.wmeta.properties['to'] = str(to) super(TKSpinbox, self).configure()",if from_ > to:,if from_ < to:,0.6766126618439035,0.7801270245332924,False
4136,def get_error_diagnostics(self): diagnostics = [] if self.stdout is not None: with open(self.stdout.name) as fds: contents = fds.read().strip() <IF_STMT> diagnostics.append('ab STDOUT:\n' + contents) if self.stderr is not None: with open(self.stderr.name) as fds: contents = fds.read().strip() if contents.strip(): diagnostics.append('ab STDERR:\n' + contents) return diagnostics,if contents.strip():,if contents.strip():,0.9473336744406674,0.9144061946646023,True
4137,"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): <IF_STMT> if os.getenv('TRUSTED_HOST') and should_rebase_environment_value(name): value = file_host.rebase_to_worker_root(value) env[name] = value",if is_forwarded_environment_variable(name):,if name.startswith('TEST_'):,0.890688737211311,0.8743414417652072,False
4138,"def update_content(self, more_content: StringList) -> None: if isinstance(self.object, TypeVar): attrs = [repr(self.object.__name__)] for constraint in self.object.__constraints__: attrs.append(stringify_typehint(constraint)) if self.object.__covariant__: attrs.append('covariant=True') <IF_STMT> attrs.append('contravariant=True') more_content.append(_('alias of TypeVar(%s)') % ', '.join(attrs), '') more_content.append('', '') super().update_content(more_content)",if self.object.__contravariant__:,if self.object.__contravariant__:,0.9145204071544393,0.8866029039778043,True
4139,"def after(self, event, state): group = event.group for plugin in self.get_plugins(): <IF_STMT> continue metrics.incr('notifications.sent', instance=plugin.slug) yield self.future(plugin.rule_notify)","if not safe_execute(plugin.should_notify, group=group, event=event):",if group.slug == plugin.slug:,0.7337422127550784,0.693395566222006,False
4140,"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): <IF_STMT> n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) if not isinstance(n, _strtypes): raise TypeError('on must be a name or field, not: {0}'.format(n)) elif n not in fields: raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))",if n._child.isidentical(expr):,if n.name:,0.902529561588352,0.9452425713908255,False
4141,"def build_filter(arg): filt = {} if arg is not None: <IF_STMT> raise UserError('Arguments to --filter should be in form KEY=VAL') key, val = arg.split('=', 1) filt[key] = val return filt",if '=' not in arg:,if '=' not in arg:,0.8541846343574588,0.7975010608178975,True
4142,"def pickline(file, key, casefold=1): try: f = open(file, 'r') except IOError: return None pat = re.escape(key) + ':' prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break <IF_STMT> text = line[len(key) + 1:] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",if prog.match(line):,if prog.search(line):,0.8890991821334686,0.940591574039961,False
4143,"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: <IF_STMT> category = 'preprint' elif node.is_registration: category = 'registration' else: category = node.project_or_component client().delete(index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404])","if isinstance(node, Preprint):",if node.is_preprint:,0.6752731988734633,0.8827916928185874,False
4144,"def update(self, preds, labels): if not _is_numpy_(labels): raise ValueError(""The 'labels' must be a numpy ndarray."") if not _is_numpy_(preds): raise ValueError(""The 'predictions' must be a numpy ndarray."") for i, lbl in enumerate(labels): value = preds[i, 1] bin_idx = int(value * self._num_thresholds) assert bin_idx <= self._num_thresholds <IF_STMT> self._stat_pos[bin_idx] += 1.0 else: self._stat_neg[bin_idx] += 1.0",if lbl:,if lbl == 0:,0.940755951121321,0.8964173245779284,False
4145,def checkStatusClient(self): if str(self.comboxBoxIPAddress.currentText()) != '': <IF_STMT> self.btnEnable.setEnabled(False) self.btncancel.setEnabled(True) return None self.btnEnable.setEnabled(True) self.btncancel.setEnabled(False),if self.ClientsLogged[str(self.comboxBoxIPAddress.currentText())]['Status']:,if self.comboxBoxIPAddress.currentText() == 'Hello World':,0.7820383637725195,0.5635190098079903,False
4146,"def colorizeDiffs(sheet, col, row, cellval): if not row or not col: return None vcolidx = sheet.visibleCols.index(col) rowidx = sheet.rows.index(row) if vcolidx < len(othersheet.visibleCols) and rowidx < len(othersheet.rows): otherval = othersheet.visibleCols[vcolidx].getDisplayValue(othersheet.rows[rowidx]) <IF_STMT> return 'color_diff' else: return 'color_diff_add'",if cellval.display != otherval:,if otherval == cellval:,0.8113578833800045,0.8516228624291206,False
4147,"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info('Checking for %s' % wafvendor) <IF_STMT> detected.append(wafvendor) if not findall: break self.knowledge['wafname'] = detected return detected",if self.wafdetections[wafvendor](self):,if self.checklist[wafvendor] is not None:,0.8711909559860017,0.8200123297196334,False
4148,"def get_repository_metadata_by_repository_id_changeset_revision(app, id, changeset_revision, metadata_only=False): """"""Get a specified metadata record for a specified repository in the tool shed."""""" if metadata_only: repository_metadata = get_repository_metadata_by_changeset_revision(app, id, changeset_revision) <IF_STMT> return repository_metadata.metadata return None return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)",if repository_metadata and repository_metadata.metadata:,if repository_metadata:,0.7793640971198204,0.8935248372106969,False
4149,def getmultiline(self): line = self.getline() if line[3:4] == '-': code = line[:3] while 1: nextline = self.getline() line = line + ('\n' + nextline) <IF_STMT> break return line,if nextline[:3] == code and nextline[3:4] != '-':,if code == ' ' or code == ' ' or code == ' ' or (code == ' '):,0.7545745418264928,0.5253303667275165,False
4150,"def _validate_reports(value, *args, **kwargs): from osf.models import OSFUser for key, val in value.items(): if not OSFUser.load(key): raise ValidationValueError('Keys must be user IDs') <IF_STMT> raise ValidationTypeError('Values must be dictionaries') if 'category' not in val or 'text' not in val or 'date' not in val or ('retracted' not in val): raise ValidationValueError(('Values must include `date`, `category`, ', '`text`, `retracted` keys'))","if not isinstance(val, dict):","if not isinstance(val, dict):",0.9457364278998535,0.9069443196104878,True
4151,"def deselectItem(self, item): if self.isSelected(item): <IF_STMT> listItem = self._getListItem(item) selections = self.getSelectedItems() selections.remove(self.loadHandler.getSelection(listItem)) self.setSelections(selections) else: self.deselectAll()",if self.multiSelect:,if self.loadHandler.isSelected(item):,0.8271029095705281,0.762465858623486,False
4152,"def __init__(self, **kwargs): if self.name is None: raise RuntimeError('RenderPrimitive cannot be used directly') self.option_values = {} for key, val in kwargs.items(): if not key in self.options: raise ValueError(""primitive `{0}' has no option `{1}'"".format(self.name, key)) self.option_values[key] = val for name, (description, default) in self.options.items(): <IF_STMT> self.option_values[name] = default",if not name in self.option_values:,if description == 'default':,0.8900963441391029,0.8856327184319047,False
4153,"def setup_smart_indent(self, view, lang): if type(view) == gedit.View: <IF_STMT> setattr(view, 'smart_indent_instance', SmartIndent()) handler_id = view.connect('key-press-event', view.smart_indent_instance.key_press_handler) self.handler_ids.append((handler_id, view)) view.smart_indent_instance.set_language(lang, view)","if getattr(view, 'smart_indent_instance', False) == False:","if not hasattr(view, 'smart_indent_instance'):",0.820160598102919,0.7378351342269067,False
4154,"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = '' strings = [] for char in word: <IF_STMT> letters += char count += 1 else: if count > threshold: strings.append(letters) letters = '' count = 0 if count > threshold: strings.append(letters) return strings",if char in char_set:,if char in char_set:,0.7296781962687993,0.8752376177722327,True
4155,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_logout_url(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.8646149936089025,0.7378351342269067,True
4156,def __create_table(self): for i in range(256): crcreg = i for j in range(8): <IF_STMT> crcreg = self.__CRCPOLYNOMIAL ^ crcreg >> 1 else: crcreg >>= 1 self.__crctable[i] = crcreg,if crcreg & 1 != 0:,if j == 0:,0.844174974202681,0.8105932471967202,False
4157,"def destroy(self): """"""Flush all entries and empty cache"""""" for i in range(len(self.cached_rows)): id_ = self.cached_rows[i] self.cached_rows[i] = None <IF_STMT> try: inode = self.attrs[id_] except KeyError: pass else: del self.attrs[id_] self.setattr(inode) assert len(self.attrs) == 0",if id_ is not None:,if id_ in self.attrs:,0.7580490226356713,0.8431339019329497,False
4158,"def set_config(self): """"""Set configuration options for QTextEdit."""""" c = self.c w = self.widget w.setWordWrapMode(QtGui.QTextOption.NoWrap) if 0: n = c.config.getInt('qt-rich-text-zoom-in') <IF_STMT> w.zoomIn(n) w.updateMicroFocus() w.setTabStopWidth(24)","if n not in (None, 0):",if n > 0:,0.6293588067474017,0.7709002428237395,False
4159,"def mouseDragEvent(self, ev): if self.movable and ev.button() == QtCore.Qt.LeftButton: if ev.isStart(): self.moving = True self.cursorOffset = self.pos() - self.mapToParent(ev.buttonDownPos()) self.startPosition = self.pos() ev.accept() if not self.moving: return self.setPos(self.cursorOffset + self.mapToParent(ev.pos())) self.sigDragged.emit(self) <IF_STMT> self.moving = False self.sigPositionChangeFinished.emit(self)",if ev.isFinish():,elif ev.isEnd():,0.9430876192392184,0.8996480074924822,False
4160,"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: if not newParent._element.text: newParent._element.text = '' <IF_STMT> newParent._element.text += self._element.text self._element.text = '' base.Node.reparentChildren(self, newParent)",if self._element.text is not None:,if self._element.text:,0.6646964199741349,0.8466657105524215,False
4161,"def _no_sp_or_bp(self, bl): for s in bl.vex.statements: for e in chain([s], s.expressions): <IF_STMT> reg = self.get_reg_name(self.project.arch, e.offset) if reg == 'ebp' or reg == 'esp': return False elif e.tag == 'Ist_Put': reg = self.get_reg_name(self.project.arch, e.offset) if reg == 'ebp' or reg == 'esp': return False return True",if e.tag == 'Iex_Get':,if e.tag == 'Ist_Put':,0.9321177022462148,0.8856327184319047,False
4162,"def _get_import_chain(self, *, until=None): stack = inspect.stack()[2:] try: for frameinfo in stack: try: <IF_STMT> continue data = dedent(''.join(frameinfo.code_context)) if data.strip() == until: raise StopIteration yield (frameinfo.filename, frameinfo.lineno, data.strip()) del data finally: del frameinfo finally: del stack",if not frameinfo.code_context:,if frameinfo.code_context is None:,0.8860352016056253,0.8516228624291206,False
4163,def stream_docker_log(log_stream): async for line in log_stream: if 'stream' in line and line['stream'].strip(): logger.debug(line['stream'].strip()) elif 'status' in line: logger.debug(line['status'].strip()) <IF_STMT> logger.error(line['error'].strip()) raise DockerBuildError,elif 'error' in line:,elif 'error' in line:,0.8755171609299462,0.7709002428237395,True
4164,"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node['deps']: if dep == goal_node_index: return [curr_node['address']] for dep in curr_node['deps']: path = self.get_cycle_path(self.get_by_address(dep), goal_node_index) <IF_STMT> path.insert(0, curr_node['address']) return path return []",if len(path) > 0:,if len(path) == 1:,0.7662373487675109,0.8169276475307028,False
4165,"def prompt(default=None): editor = 'nano' with tempfile.NamedTemporaryFile(mode='r+') as tmpfile: <IF_STMT> tmpfile.write(default) tmpfile.flush() child_pid = os.fork() is_child = child_pid == 0 if is_child: os.execvp(editor, [editor, tmpfile.name]) else: os.waitpid(child_pid, 0) tmpfile.seek(0) return tmpfile.read().strip()",if default:,if default:,0.8983440187414456,0.8827916928185874,True
4166,"def _get_annotated_template(self, template): changed = False if template.get('version', '0.12.0') >= '0.13.0': using_js = self.spider._filter_js_urls(template['url']) body = 'rendered_body' if using_js else 'original_body' <IF_STMT> template['body'] = body changed = True if changed or not template.get('annotated'): _build_sample(template) return template",if template.get('body') != body:,if body:,0.9294373795614751,0.8996480074924822,False
4167,"def collect(self, paths): for path in paths or (): relpath = os.path.relpath(path, self._artifact_root) dst = os.path.join(self._directory, relpath) safe_mkdir(os.path.dirname(dst)) <IF_STMT> shutil.copytree(path, dst) else: shutil.copy(path, dst) self._relpaths.add(relpath)",if os.path.isdir(path):,if os.path.isdir(dst):,0.8855449703481484,0.8531413606256201,False
4168,"def dependencies(context=None): """"""Return all dependencies detected by knowit."""""" deps = OrderedDict([]) try: initialize(context) for name, provider_cls in _provider_map.items(): <IF_STMT> deps[name] = available_providers[name].version else: deps[name] = {} except Exception: pass return deps",if name in available_providers:,if provider_cls.is_available():,0.8580981122799405,0.8827916928185874,False
4169,"def _getaddrinfo(self, host_bytes, port, family, socktype, proto, flags): while True: ares = self.cares try: return self.__getaddrinfo(host_bytes, port, family, socktype, proto, flags) except gaierror: <IF_STMT> raise",if ares is self.cares:,if ares.status != 0:,0.8712967698584028,0.8196189957582152,False
4170,"def write_entries(cmd, basename, filename): ep = cmd.distribution.entry_points if isinstance(ep, basestring) or ep is None: data = ep elif ep is not None: data = [] for section, contents in ep.items(): <IF_STMT> contents = EntryPoint.parse_group(section, contents) contents = '\n'.join(map(str, contents.values())) data.append('[%s]\n%s\n\n' % (section, contents)) data = ''.join(data) cmd.write_or_delete_file('entry points', filename, data, True)","if not isinstance(contents, basestring):","if isinstance(contents, dict):",0.9378271777755851,0.9118021019905903,False
4171,"def _highlight_do(self): new_hl_text = self.highlight_text.text() if new_hl_text != self.hl_text: self.hl_text = new_hl_text if self.hl is not None: self.hl.setDocument(None) self.hl = None <IF_STMT> self.hl = Highlighter(self.hl_text, parent=self.doc) self.clear_highlight_button.setEnabled(bool(self.hl))",if self.hl_text:,if self.hl_text:,0.9140094273478033,0.8645707301556367,True
4172,"def traverse(node, functions=[]): if hasattr(node, 'grad_fn'): node = node.grad_fn if hasattr(node, 'variable'): node = graph.nodes_by_id.get(id(node.variable)) <IF_STMT> node.functions = list(functions) del functions[:] if hasattr(node, 'next_functions'): functions.append(type(node).__name__) for f in node.next_functions: if f[0]: functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, 'saved_tensors'): for t in node.saved_tensors: traverse(t)",if node:,if functions:,0.8157509090228773,0.9144061946646023,False
4173,"def compress(self, data_list): if data_list: page_id = data_list[1] if page_id in EMPTY_VALUES: <IF_STMT> return None raise forms.ValidationError(self.error_messages['invalid_page']) return Page.objects.get(pk=page_id) return None",if not self.required:,if page_id == self.data_list[0]:,0.7710080123106112,0.7498810286408993,False
4174,"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): if name == 'Index': continue x = item() <IF_STMT> self.assertEqual(type(x._fields), tuple)","if isinstance(x, ast.AST):","if isinstance(x, ast.Field):",0.638350885614339,0.7985065516266612,False
4175,"def handle_starttag(self, tag, attrs): if tag == 'base': self.base_url = dict(attrs).get('href') if self.scan_tag(tag): for attr, value in attrs: if self.scan_attr(attr): <IF_STMT> value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",if self.strip:,if attr == 'href':,0.7732038310933451,0.8431339019329497,False
4176,"def _initialize_asset_map(cls): cls._asset_name_to_path = {} assets = os.listdir(ASSETS_PATH) for asset in assets: path = os.path.join(ASSETS_PATH, asset) <IF_STMT> cls._asset_name_to_path[os.path.basename(path)] = path",if os.path.isfile(path):,if os.path.isfile(path):,0.7739978040452798,0.8137489370974955,True
4177,"def dataReceived(self, data): self.buf += data if self._paused: log.startLogging(sys.stderr) log.msg('dataReceived while transport paused!') self.transport.loseConnection() else: self.transport.write(data) <IF_STMT> self.transport.loseConnection() else: self.pause()",if self.buf.endswith(b'\n0\n'):,if self._paused:,0.8257902173011077,0.8137489370974955,False
4178,def test_case_sensitive(self): with support.EnvironmentVarGuard() as env: env.unset('PYTHONCASEOK') <IF_STMT> self.skipTest('os.environ changes not reflected in _os.environ') loader = self.find_module() self.assertIsNone(loader),if b'PYTHONCASEOK' in _bootstrap_external._os.environ:,if 'PYTHONCASEOK' not in _os.environ:,0.7072457539773271,0.6750915335148621,False
4179,"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: if not self.exists(self.manifest_path): return {} mtime = self.getmtime(self.manifest_path) <IF_STMT> self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",if self._mtime is None or mtime > self._mtime:,if mtime < self._mtime:,0.8789992793527853,0.7886336751695258,False
4180,"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): <IF_STMT> self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == 'mean_module.constant': self.assertIsNone(constraint) elif name == 'covar_module.raw_outputscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == 'covar_module.base_kernel.raw_lengthscale': self.assertIsInstance(constraint, gpytorch.constraints.Positive)",if name == 'likelihood.noise_covar.raw_noise':,if name == 'mean_module.constant':,0.901142454362161,0.8516228624291206,False
4181,"def process_plugin_result(name, result): if result: try: jsonify(test=result) except Exception: logger.exception('Error while jsonifying settings from plugin {}, please contact the plugin author about this'.format(name)) raise else: <IF_STMT> del result['__enabled'] data[name] = result",if '__enabled' in result:,if '__enabled' in result:,0.7361868228786069,0.828399516355805,True
4182,"def benchmarking(net, ctx, num_iteration, datashape=300, batch_size=64): input_shape = (batch_size, 3) + (datashape, datashape) data = mx.random.uniform(-1.0, 1.0, shape=input_shape, ctx=ctx, dtype='float32') dryrun = 5 for i in range(dryrun + num_iteration): <IF_STMT> tic = time.time() ids, scores, bboxes = net(data) ids.asnumpy() scores.asnumpy() bboxes.asnumpy() toc = time.time() - tic return toc",if i == dryrun:,if i % 10000 == 0:,0.8956279587278035,0.8534652100396689,False
4183,"def merge_weekdays(base_wd, icu_wd): result = [] for left, right in zip(base_wd, icu_wd): <IF_STMT> result.append(left) continue left = set(left.split('|')) right = set(right.split('|')) result.append('|'.join(left | right)) return result",if left == right:,if left == right:,0.904416841484964,0.7965020533851944,True
4184,"def create_key(self, request): if self._ignored_parameters: url, body = self._remove_ignored_parameters(request) else: url, body = (request.url, request.body) key = hashlib.sha256() key.update(_to_bytes(request.method.upper())) key.update(_to_bytes(url)) if request.body: key.update(_to_bytes(body)) el<IF_STMT> for name, value in sorted(request.headers.items()): key.update(_to_bytes(name)) key.update(_to_bytes(value)) return key.hexdigest()",if self._include_get_headers and request.headers != _DEFAULT_HEADERS:,if request.headers:,0.9417178440768831,0.8901732118131125,False
4185,"def test_invalid_mountinfo(self): line = '20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-rootrw,errors=remount-ro,data=ordered' elements = line.split() for i in range(len(elements) + 1): lines = [' '.join(elements[0:i])] <IF_STMT> expected = None else: expected = ('/dev/mapper/vg0-root', 'ext4', '/') self.assertEqual(expected, util.parse_mount_info('/', lines))",if i < 10:,if i == 0:,0.8127302227030994,0.8627586293513119,False
4186,"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) if k in keep_current: if keep_nested is not None: <IF_STMT> yield (k, dict(self.nested_filter(v.items(), keep_nested))) else: yield (k, v)","if isinstance(v, dict):","if isinstance(v, dict):",0.9103496380729571,0.8749766281017177,True
4187,"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos['reach_leaf_node'].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos['node_pos'][t_idx] if cur_node_idx == -1: continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(tree, sample, cur_node_idx) <IF_STMT> node_pos['reach_leaf_node'][t_idx] = True node_pos['node_pos'][t_idx] = rs return node_pos",if reach_leaf:,if reach_leaf:,0.7484420993419191,0.9024521756077707,True
4188,"def _pop_waiting_trial_id(self) -> Optional[int]: for trial in self._storage.get_all_trials(self._study_id, deepcopy=False): <IF_STMT> continue if not self._storage.set_trial_state(trial._trial_id, TrialState.RUNNING): continue _logger.debug('Trial {} popped from the trial queue.'.format(trial.number)) return trial._trial_id return None",if trial.state != TrialState.WAITING:,if not trial:,0.7637767923515896,0.833078701050083,False
4189,"def get_step_best(self, step_models): best_score = None best_model = '' for model in step_models: model_info = self.models_trained[model] score = model_info.get_score() <IF_STMT> continue if best_score is None or score < best_score: best_score = score best_model = model LOGGER.info(f'step {self.n_step}, best model {best_model}') return best_model",if score is None:,if score is None:,0.7808851148946243,0.8723360571509826,True
4190,"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() if f is not None and f.type in ('or', 'and', 'not'): <IF_STMT> queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f",if block_end:,if block_end:,0.9205834670130708,0.8827916928185874,True
4191,"def _buffer_decode(self, input, errors, final): if self.decoder is None: output, consumed, byteorder = codecs.utf_16_ex_decode(input, errors, 0, final) if byteorder == -1: self.decoder = codecs.utf_16_le_decode <IF_STMT> self.decoder = codecs.utf_16_be_decode elif consumed >= 2: raise UnicodeError('UTF-16 stream does not start with BOM') return (output, consumed) return self.decoder(input, self.errors, final)",elif byteorder == 1:,elif byteorder == 1:,0.6568744321319382,0.8856327184319047,True
4192,"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) <IF_STMT> data = StringIO() if self.encryptor: self.encryptor.decrypt(db, data) else: raise EncryptionError('Encrpyted credential storage: {}'.format(self.db)) return json.loads(data.getvalue()) else: return json.load(db) except: return {'creds': []}",if content == 'Salted__':,if content == b'<credential:,0.5426334049039598,0.8431339019329497,False
4193,"def _getbytes(self, start, l=1): out = [] for ad in range(l): offset = ad + start + self.base_address <IF_STMT> raise IOError('not enough bytes') out.append(int_to_byte(Byte(offset))) return b''.join(out)",if not is_mapped(offset):,if offset > self.size:,0.7581714202395428,0.7965020533851944,False
4194,def cache_sqs_queues_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get('environment') == 'prod': cache_sqs_queues_for_account.delay(account_id) el<IF_STMT> cache_sqs_queues_for_account.delay(account_id) stats.count(f'{function}.success') return True,"if account_id in config.get('celery.test_account_ids', []):",if config.get('environment') == 'prod_local':,0.8909604466125013,0.7965020533851944,False
4195,"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: <IF_STMT> row[refnum + 1] = line return elif row[0] > linenum: break self.rows.insert(i, self.newRow(linenum, refnum, line))",if row[refnum + 1] is None:,if i == len(self.rows):,0.7500389781410703,0.8385130047130208,False
4196,"def __setattr__(self, name, val): if self.__dict__.get(name, 'hamster_graphics_no_value_really') == val: return Sprite.__setattr__(self, name, val) if name == 'image_data': self._surface = None <IF_STMT> self.__dict__['width'] = self.image_data.get_width() self.__dict__['height'] = self.image_data.get_height()",if self.image_data:,if self.image_data:,0.9122793601243031,0.8645707301556367,True
4197,"def L_op(self, inputs, outputs, gout): x, = inputs gz, = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <IF_STMT> return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz * (1 - sqr(tanh(x))),)",if x.type in discrete_types:,if x.type in discrete_types:,0.9017139333568954,0.8385130047130208,True
4198,"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + ':[y/n]') <IF_STMT> return True if output.lower() == 'n': return False",if output.lower() == 'y':,if output.lower() == 'y':,0.7066189439135999,0.7801270245332924,True
4199,"def replace_documentation_for_matching_shape(self, event_name, section, **kwargs): if self._shape_name == section.context.get('shape'): self._replace_documentation(event_name, section) for section_name in section.available_sections: sub_section = section.get_section(section_name) <IF_STMT> self._replace_documentation(event_name, sub_section) else: self.replace_documentation_for_matching_shape(event_name, sub_section)",if self._shape_name == sub_section.context.get('shape'):,if self._shape_name == sub_section.context.get('shape'):,0.8147579062044386,0.7801270245332924,True
4200,"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + ':[y/n]') if output.lower() == 'y': return True <IF_STMT> return False",if output.lower() == 'n':,elif output.lower() == 'n':,0.8835659947096592,0.7913547592495157,False
4201,"def __getitem__(self, index): if self._check(): if isinstance(index, int): <IF_STMT> raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 16, index) if feature: feature, = _unpack('!H', feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",if index < 0 or index >= len(self.features):,if index < 0:,0.8998021839276464,0.8752376177722327,False
4202,"def _parse_locator(self, locator): prefix = None criteria = locator if not locator.startswith('//'): locator_parts = locator.partition('=') <IF_STMT> prefix = locator_parts[0] criteria = locator_parts[2].strip() return (prefix, criteria)",if len(locator_parts[1]) > 0:,if len(locator_parts) == 3:,0.6630436013840677,0.7886336751695258,False
4203,"def trakt_episode_data_generate(self, data): uniqueSeasons = [] for season, episode in data: <IF_STMT> uniqueSeasons.append(season) seasonsList = [] for searchedSeason in uniqueSeasons: episodesList = [] for season, episode in data: if season == searchedSeason: episodesList.append({'number': episode}) seasonsList.append({'number': searchedSeason, 'episodes': episodesList}) post_data = {'seasons': seasonsList} return post_data",if season not in uniqueSeasons:,if episode == season:,0.79142969068754,0.8780099567239787,False
4204,"def __init__(self, data, n_bins): bin_width = span / n_bins bins = [0] * n_bins for x in data: b = int(mpfloor((x - minimum) / bin_width)) <IF_STMT> b = 0 elif b >= n_bins: b = n_bins - 1 bins[b] += 1 self.bins = bins self.bin_width = bin_width",if b < 0:,if b < 0:,0.7902483409926204,0.8856327184319047,True
4205,"def infer_context(typ, context='http://schema.org'): parsed_context = urlparse(typ) if parsed_context.netloc: base = ''.join([parsed_context.scheme, '://', parsed_context.netloc]) <IF_STMT> context = urljoin(base, parsed_context.path) typ = parsed_context.fragment.strip('/') elif parsed_context.path: context = base typ = parsed_context.path.strip('/') return (context, typ)",if parsed_context.path and parsed_context.fragment:,if parsed_context.fragment:,0.8016376791782148,0.8866029039778043,False
4206,"def parse(self, items): for index, item in enumerate(items): keys = self.build_key(item) if keys is None: continue self.items[tuple(keys)] = (index, item) <IF_STMT> log.info('Unable to update table (keys: %r)', keys)","if not self.path_set(self.table, keys, (index, item)):",if self.db.update_table(keys):,0.9127465658730898,0.8696398662122882,False
4207,"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == 'layers': child = dict_to_XML('layer', val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: <IF_STMT> child = Element('variable', name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",if tag == 'config':,if tag == 'variables':,0.9430344948659789,0.8944264839442453,False
4208,"def _get_config_value(self, section, key): if section: <IF_STMT> self.log.error(""Error: Config section '%s' not found"", section) return None return self.config[section].get(key, self.config[key]) else: return self.config[key]",if section not in self.config:,"if not hasattr(self.config[section], 'get'):",0.8509587303482065,0.760856626273165,False
4209,"def h_line_down(self, input): end_this_line = self.value.find('\n', self.cursor_position) if end_this_line == -1: if self.scroll_exit: self.h_exit_down(None) else: self.cursor_position = len(self.value) else: self.cursor_position = end_this_line + 1 for x in range(self.cursorx): <IF_STMT> break elif self.value[self.cursor_position] == '\n': break else: self.cursor_position += 1",if self.cursor_position > len(self.value) - 1:,if self.value[self.cursor_position] == input:,0.9154386836883853,0.8627586293513119,False
4210,"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) <IF_STMT> break if isinstance(data, str): data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write('%s: I/O error: %s\n' % (filename, msg)) return 1 out.write('%s %s\n' % (m.hexdigest(), filename)) return 0",if not data:,if not data:,0.718837130711217,0.8928756684056034,True
4211,"def main(input): logging.info('Running Azure Cloud Custodian Policy %s', input) context = {'config_file': join(function_directory, 'config.json'), 'auth_file': join(function_directory, 'auth.json')} event = None subscription_id = None if isinstance(input, QueueMessage): <IF_STMT> return event = input.get_json() subscription_id = ResourceIdParser.get_subscription_id(event['subject']) handler.run(event, context, subscription_id)",if input.dequeue_count > max_dequeue_count:,if input.get_json() is None:,0.8749918992056858,0.8555308664663046,False
4212,"def maybeExtractTarball(self): if self.tarball: tar = self.computeTarballOptions() + ['-xvf', self.tarball] res = (yield self._Cmd(tar, abandonOnFailure=False)) <IF_STMT> yield self._Cmd(['rm', '-f', self.tarball], abandonOnFailure=False) yield self.runRmdir(self.repoDir(), abandonOnFailure=False)",if res:,if res:,0.646611712327958,0.8466657105524215,True
4213,"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop('name')) action = 0 for key, val in props.get('options', {}).items(): <IF_STMT> new_action = 0 for name, _val in val.items(): action = watcher.set_opt('hooks.%s' % name, _val) if action == 1: new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 return watcher.do_action(action)",if key == 'hooks':,"if isinstance(val, dict):",0.892268996941982,0.9151329413834155,False
4214,"def _import_playlists(self, fns, library): added = 0 for filename in fns: name = _name_for(filename) with open(filename, 'rb') as f: <IF_STMT> playlist = parse_m3u(f, name, library=library) elif filename.endswith('.pls'): playlist = parse_pls(f, name, library=library) else: print_w(""Unsupported playlist type for '%s'"" % filename) continue self.changed(playlist) library.add(playlist) added += 1 return added",if filename.endswith('.m3u') or filename.endswith('.m3u8'):,if filename.endswith('.m3u'):,0.9292782879753329,0.9253742688467129,False
4215,"def unwrap_term_buckets(self, timestamp, term_buckets): for term_data in term_buckets: <IF_STMT> self.unwrap_interval_buckets(timestamp, term_data['key'], term_data['interval_aggs']['buckets']) else: self.check_matches(timestamp, term_data['key'], term_data)",if 'interval_aggs' in term_data:,if 'interval_aggs' in term_data:,0.5573491278430686,0.674945488826271,True
4216,"def _get_exception(flags, timeout_ms, payload_size): if flags & FLAG_ERROR: if flags & FLAG_TIMEOUT: return SpicommTimeoutError(timeout_ms / 1000.0) <IF_STMT> return SpicommOverflowError(payload_size) return SpicommError() return None",if flags & FLAG_OVERFLOW:,elif flags & FLAG_OVERFLOW:,0.6207453692641294,0.7709002428237395,False
4217,"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): if key in self.tagged_blocks: data = self.tagged_blocks.get_data(key) for pattern in data: <IF_STMT> return pattern return None",if pattern.pattern_id == pattern_id:,if pattern.id == pattern_id:,0.9056513478098659,0.8228500218338367,False
4218,"def print_quiet(self, context, *args, **kwargs): for index, (key, value) in enumerate(itertools.chain(enumerate(args), kwargs.items())): <IF_STMT> print(self.format_quiet(index, key, value, fields=context.get_input_fields()))","if self.filter(index, key, value):",if self.verbose:,0.5940171607496437,0.7778111223054219,False
4219,"def complete(self, block): with self._condition: if not self._final: return False if self._complete(): self._calculate_state_root_if_not_already_done() return True <IF_STMT> self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",if block:,if block:,0.863599314969808,0.8318180062062374,True
4220,"def compression_rotator(source, dest): with open(source, 'rb') as sf: with gzip.open(dest, 'wb') as wf: while True: data = sf.read(CHUNK_SIZE) <IF_STMT> break wf.write(data) os.remove(source)",if not data:,if not data:,0.7206376851865216,0.7947545184555568,True
4221,"def mockup(self, records): provider = TransipProvider('', '', '') _dns_entries = [] for record in records: <IF_STMT> entries_for = getattr(provider, '_entries_for_{}'.format(record._type)) name = record.name if name == '': name = provider.ROOT_RECORD _dns_entries.extend(entries_for(name, record)) _dns_entries.append(DnsEntry('@', '3600', 'NS', 'ns01.transip.nl.')) self.mockupEntries = _dns_entries",if record._type in provider.SUPPORTS:,if record._type:,0.8614515354960158,0.9076141716697395,False
4222,"def parse_known_args(self, args=None, namespace=None): entrypoint = self.prog.split(' ')[0] try: defs = get_defaults_for_argparse(entrypoint) ignore = defs.pop('Ignore', None) self.set_defaults(**defs) <IF_STMT> set_notebook_diff_ignores(ignore) except ValueError: pass return super(ConfigBackedParser, self).parse_known_args(args=args, namespace=namespace)",if ignore:,if ignore:,0.886675426666927,0.8590888738245122,True
4223,"def _maybeRebuildAtlas(self, threshold=4, minlen=1000): n = len(self.fragmentAtlas) if n > minlen and n > threshold * len(self.data): self.fragmentAtlas.rebuild(list(zip(*self._style(['symbol', 'size', 'pen', 'brush'])))) self.data['sourceRect'] = 0 <IF_STMT> self._sourceQRect.clear() self.updateSpots()",if _USE_QRECT:,if self._sourceQRect:,0.914248006093541,0.8751809448365924,False
4224,"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: <IF_STMT> return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None if self.quitting: raise BdbQuit if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",if self.stopframe and frame.f_code.co_flags & CO_GENERATOR:,if self.stopframe is frame:,0.9087598650858859,0.8627586293513119,False
4225,"def tearDown(self): if not self.is_playback(): try: if self.hosted_service_name is not None: self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: <IF_STMT> self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self.storage_account_name is not None:,if self.storage_account_name is not None:,0.8545268069134014,0.7685107079449489,True
4226,"def make_log_msg(self, msg, *other_messages): MAX_MESSAGE_LENGTH = 1000 if not other_messages: return msg[-MAX_MESSAGE_LENGTH:] else: if len(msg): msg += '\n...\n' NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len(msg) else: NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH <IF_STMT> msg += other_messages[0][-NEXT_MESSAGE_OFFSET:] return self.make_log_msg(msg, *other_messages[1:]) else: return self.make_log_msg(msg)",if NEXT_MESSAGE_OFFSET > 0:,if len(other_messages) > NEXT_MESSAGE_OFFSET:,0.6151527846252675,0.8555308664663046,False
4227,"def wrapper(self: RequestHandler, *args, **kwargs) -> Optional[Awaitable[None]]: if self.request.path.endswith('/'): if self.request.method in ('GET', 'HEAD'): uri = self.request.path.rstrip('/') if uri: <IF_STMT> uri += '?' + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",if self.request.query:,if self.request.query:,0.7817459899178355,0.8996480074924822,True
4228,"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == 'QTCORE': continue value = env['LIBPATH_' + var] <IF_STMT> core = env[coreval] accu = [] for lib in value: if lib in core: continue accu.append(lib) env['LIBPATH_' + var] = accu",if value:,if coreval in env:,0.6206595509848498,0.8723360571509826,False
4229,"def _attach_children(self, other, exclude_worldbody, dry_run=False): for other_child in other.all_children(): <IF_STMT> self_child = self.get_children(other_child.spec.name) self_child._attach(other_child, exclude_worldbody, dry_run)",if not other_child.spec.repeated:,if other_child.spec.name in self.spec.name:,0.7676809528807759,0.674945488826271,False
4230,def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): if child.getchildren(): content = getDictFromTree(child) else: content = child.text <IF_STMT> if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or '') else: ret_dict[child.tag] = content or '' return ret_dict,if ret_dict.has_key(child.tag):,if child.tag in ret_dict:,0.7472932594701581,0.8592377270804451,False
4231,"def nsUriMatch(self, value, wanted, strict=0, tt=type(())): """"""Return a true value if two namespace uri values match."""""" if value == wanted or (type(wanted) is tt and value in wanted): return 1 if not strict and value is not None: wanted = type(wanted) is tt and wanted or (wanted,) value = value[-1:] != '/' and value or value[:-1] for item in wanted: <IF_STMT> return 1 return 0",if item == value or item[:-1] == value:,if item == value or item in wanted:,0.9384718181736139,0.8647752128301904,False
4232,"def update_repository(self, ignore_issues=False, force=False): """"""Update."""""" if not await self.common_update(ignore_issues, force): return if self.repository_manifest: <IF_STMT> self.content.path.remote = '' if self.content.path.remote == 'apps': self.data.domain = get_first_directory_in_directory(self.tree, self.content.path.remote) self.content.path.remote = f'apps/{self.data.name}' self.content.path.local = self.localpath",if self.data.content_in_root:,if not self.content.path.remote:,0.9200423210332281,0.8547305998833805,False
4233,"def addOutput(self, data, isAsync=None, **kwargs): isAsync = _get_async_param(isAsync, **kwargs) if isAsync: self.terminal.eraseLine() self.terminal.cursorBackward(len(self.lineBuffer) + len(self.ps[self.pn])) self.terminal.write(data) if isAsync: <IF_STMT> self.terminal.nextLine() self.terminal.write(self.ps[self.pn]) if self.lineBuffer: oldBuffer = self.lineBuffer self.lineBuffer = [] self.lineBufferIndex = 0 self._deliverBuffer(oldBuffer)",if self._needsNewline():,if self.lineBuffer:,0.9157067458841406,0.8901732118131125,False
4234,"def is_installed(self, dlc_title='') -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info('version', dlc_title) installed = True if dlc_version else False <IF_STMT> status = self.legacy_get_dlc_status(dlc_title) installed = True if status in ['installed', 'updatable'] else False elif self.install_dir and os.path.exists(self.install_dir): installed = True return installed",if not installed:,"elif self.legacy_get_dlc_info('status', dlc_title):",0.843263715977436,0.897752847848028,False
4235,"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() <IF_STMT> if os.path.exists(sockname): os.remove(sockname) self.sock = None",if type(sockname) is str:,if sockname:,0.6403951179788407,0.8466657105524215,False
4236,"def post_file(self, file_path, graph_type='edges', file_type='csv'): dataset_id = self.dataset_id tok = self.token base_path = self.server_base_path with open(file_path, 'rb') as file: out = requests.post(f'{base_path}/api/v2/upload/datasets/{dataset_id}/{graph_type}/{file_type}', verify=self.certificate_validation, headers={'Authorization': f'Bearer {tok}'}, data=file.read()).json() <IF_STMT> raise Exception(out) return out",if not out['success']:,if not out:,0.7973842757178501,0.8592899528284996,False
4237,"def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls): """"""Extract the VQA V2 image data set to directory unless it's there."""""" for url in image_urls: filename = os.path.basename(url) download_url = os.path.join(image_root_url, url) path = generator_utils.maybe_download(directory, filename, download_url) unzip_dir = os.path.join(directory, filename.strip('.zip')) <IF_STMT> zipfile.ZipFile(path, 'r').extractall(directory)",if not tf.gfile.Exists(unzip_dir):,if os.path.isdir(unzip_dir):,0.8325957378476234,0.9146232957723002,False
4238,"def __call__(self, environ, start_response): for key in ('REQUEST_URL', 'REQUEST_URI', 'UNENCODED_URL'): <IF_STMT> continue request_uri = unquote(environ[key]) script_name = unquote(environ.get('SCRIPT_NAME', '')) if request_uri.startswith(script_name): environ['PATH_INFO'] = request_uri[len(script_name):].split('?', 1)[0] break return self.app(environ, start_response)",if key not in environ:,if key not in environ:,0.7678666490303704,0.7909601595885504,True
4239,"def _instrument_model(self, model): for key, value in list(model.__dict__.items()): if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) <IF_STMT> for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model","elif isinstance(value, list):","elif isinstance(value, list):",0.8708818039348225,0.8749766281017177,True
4240,"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == '.' or i == '..': continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() <IF_STMT> filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if pattern.match(value) is not None:,"if pattern.match(path, value):",0.9550562710118973,0.9262685672663883,False
4241,"def get_text(self, nodelist): """"""Return a string representation of the motif's properties listed on nodelist ."""""" retlist = [] for node in nodelist: if node.nodeType == Node.TEXT_NODE: retlist.append(node.wholeText) <IF_STMT> retlist.append(self.get_text(node.childNodes)) return re.sub('\\s+', ' ', ''.join(retlist))",elif node.hasChildNodes:,elif node.nodeType == Node.ELEMENT_NODE:,0.9036458797472404,0.8431339019329497,False
4242,"def _persist_metadata(self, dirname, filename): metadata_path = '{0}/{1}.json'.format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: if self.latest: self.merge_json({'GraphImages': self.posts}, metadata_path) else: self.save_json({'GraphImages': self.posts}, metadata_path) <IF_STMT> if self.latest: self.merge_json({'GraphStories': self.stories}, metadata_path) else: self.save_json({'GraphStories': self.stories}, metadata_path)",if self.stories:,if self.stories:,0.9233810442630164,0.8966773400768917,True
4243,"def _get_python_wrapper_content(self, job_class, args): job = job_class(['-r', 'hadoop'] + list(args)) job.sandbox() with job.make_runner() as runner: runner._create_setup_wrapper_scripts() <IF_STMT> with open(runner._spark_python_wrapper_path) as f: return f.read() else: return None",if runner._spark_python_wrapper_path:,if runner._spark_python_wrapper_path:,0.870403007270583,0.8590888738245122,True
4244,"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: <IF_STMT> w += 1 elif ch == '\t': w += abs(tab_width) - w % abs(tab_width) else: break return w",if ch == ' ':,if ch == '\n':,0.8566892272438565,0.8169276475307028,False
4245,def run(self): for _ in range(10): <IF_STMT> break self.spawn_i3status() if not self.ready: break self.lock.wait(5),if not self.py3_wrapper.running:,if self.ready:,0.8315172670931026,0.7241577342575828,False
4246,"def translate_len(builder: IRBuilder, expr: CallExpr, callee: RefExpr) -> Optional[Value]: if len(expr.args) == 1 and expr.arg_kinds == [ARG_POS]: expr_rtype = builder.node_type(expr.args[0]) <IF_STMT> builder.accept(expr.args[0]) return Integer(len(expr_rtype.types)) else: obj = builder.accept(expr.args[0]) return builder.builtin_len(obj, -1) return None","if isinstance(expr_rtype, RTuple):","if isinstance(expr_rtype, IRType):",0.7082864024059203,0.8635707684233572,False
4247,"def parse_auth(val): if val is not None: authtype, params = val.split(' ', 1) if authtype in known_auth_schemes: <IF_STMT> pass else: params = parse_auth_params(params) return (authtype, params) return val","if authtype == 'Basic' and '""' not in params:",if params is None:,0.5950678843526276,0.8105932471967202,False
4248,"def toxml(self): text = self.value self.parent.setBidi(getBidiType(text)) if not text.startswith(HTML_PLACEHOLDER_PREFIX): if self.parent.nodeName == 'p': text = text.replace('\n', '\n   ') <IF_STMT> text = '\n ' + text.replace('\n', '\n ') text = self.doc.normalizeEntities(text) return text",elif self.parent.nodeName == 'li' and self.parent.childNodes[0] == self:,elif self.parent.nodeName == 'h':,0.913036997628417,0.8336104423443033,False
4249,"def get_all_related_many_to_many_objects(self): try: return self._all_related_many_to_many_objects except AttributeError: rel_objs = [] for klass in get_models(): for f in klass._meta.many_to_many: <IF_STMT> rel_objs.append(RelatedObject(f.rel.to, klass, f)) self._all_related_many_to_many_objects = rel_objs return rel_objs",if f.rel and self == f.rel.to._meta:,if f.rel.to is not None:,0.8155899292612141,0.7765145040967655,False
4250,"def state_highstate(self, state, dirpath): opts = copy.copy(self.config) opts['file_roots'] = dict(base=[dirpath]) HIGHSTATE = HighState(opts) HIGHSTATE.push_active() try: high, errors = HIGHSTATE.render_highstate(state) <IF_STMT> import pprint pprint.pprint('\n'.join(errors)) pprint.pprint(high) out = HIGHSTATE.state.call_high(high) finally: HIGHSTATE.pop_active()",if errors:,if errors:,0.6934038075206201,0.8743414417652072,True
4251,"def _update_target_host(self, target, target_host): """"""Update target host."""""" target_host = None if target_host == '' else target_host if not target_host: for device_type, tgt in target.items(): <IF_STMT> target_host = tgt break if not target_host: target_host = 'llvm' if tvm.runtime.enabled('llvm') else 'stackvm' if isinstance(target_host, str): target_host = tvm.target.Target(target_host) return target_host",if device_type.value == tvm.nd.cpu(0).device_type:,if self._device_type_match(device_type):,0.6812092473576031,0.9237460349978159,False
4252,def __console_writer(self): while True: self.__writer_event.wait() self.__writer_event.clear() if self.__console_view: <IF_STMT> self.log.debug('Writing console view to STDOUT') sys.stdout.write(self.console_markup.clear) sys.stdout.write(self.__console_view) sys.stdout.write(self.console_markup.TOTAL_RESET),if not self.short_only:,if self.console_view.is_writable():,0.6093624292918433,0.7778111223054219,False
4253,"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break elif p: p.moveToThreadBack() <IF_STMT> break else: wrapped = True p = c.rootPosition() if not p: g.blue('done') c.treeSelectHelper(p)",elif wrapped:,elif wrapped:,0.9284598776819567,0.9220450449751959,True
4254,"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): if vm_details == 'Absent': query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) <IF_STMT> query_map.pop(alias) return query_map",if not query_map[alias]:,if not query_map[alias][alias]:,0.9336470118036215,0.8675979125638379,False
4255,"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) <IF_STMT> continue if pname not in already_test and '/' not in pname: already_test.append(pname) if is_shadowing(pname): shadow_pkgs.add(pname) return shadow_pkgs",if fname or (pname and fname):,if not os.path.isdir(fname):,0.9408764400567232,0.8875087479151215,False
4256,"def make_chains(chains_info): chains = [[] for _ in chains_info[0][1]] for i, num_ids in enumerate(chains_info[:-1]): num, ids = num_ids for j, ident in enumerate(ids): <IF_STMT> next_chain_info = chains_info[i + 1] previous = next_chain_info[1][j] block = SimpleBlock(num, ident, previous) chains[j].append(block) chains = {i: make_generator(chain) for i, chain in enumerate(chains)} return chains",if ident != '':,if i + 1 < len(ids):,0.8719307668470534,0.8562773802729167,False
4257,"def filter_input(mindate, maxdate, files): mindate = parse(mindate) if mindate is not None else datetime.datetime.min maxdate = parse(maxdate) if maxdate is not None else datetime.datetime.max for line in fileinput.input(files): tweet = json.loads(line) created_at = parse(tweet['created_at']) created_at = created_at.replace(tzinfo=None) <IF_STMT> print(json.dumps(tweet))",if mindate < created_at and maxdate > created_at:,if mindate.date() > maxdate.date():,0.8778954983668574,0.8823051786911775,False
4258,"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: if self._exception is None: return self.value getcurrent().throw(*self._exception) else: <IF_STMT> raise ConcurrentObjectUseError('This Waiter is already used by %r' % (self.greenlet,)) self.greenlet = getcurrent() try: return self.hub.switch() finally: self.greenlet = None",if self.greenlet is not None:,if self.greenlet is not None:,0.9143515279869007,0.8777008558345754,True
4259,"def default_loader(href, parse, encoding=None): with open(href) as file: <IF_STMT> data = ElementTree.parse(file).getroot() else: data = file.read() if encoding: data = data.decode(encoding) return data",if parse == 'xml':,if parse:,0.6759725128999051,0.839587623092576,False
4260,def is_all_qud(world): m = True for obj in world: <IF_STMT> if obj.nice: m = m and True else: m = m and False else: m = m and True return m,if obj.blond:,"if isinstance(obj, Quantum):",0.7245317764073557,0.8547305998833805,False
4261,"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed) return for region in self.view.sel(): <IF_STMT> continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",if region.empty():,if region == 0:,0.9318376206677623,0.8627586293513119,False
4262,"def add_fields(self, params): for key, val in params.iteritems(): <IF_STMT> new_params = {} for k in val: new_params['%s__%s' % (key, k)] = val[k] self.add_fields(new_params) else: self.add_field(key, val)","if isinstance(val, dict):","if isinstance(val, dict):",0.8823944627096709,0.8266114125804572,True
4263,"def find_magic(self, f, pos, magic): f.seek(pos) block = f.read(32 * 1024) if len(block) < len(magic): return -1 p = block.find(magic) while p < 0: pos += len(block) - len(magic) + 1 block = block[1 - len(magic):] + f.read(32 << 10) <IF_STMT> return -1 p = block.find(magic) return pos + p",if len(block) == len(magic) - 1:,if len(block) < len(magic):,0.8553655836640717,0.8923575006167597,False
4264,"def check_strings(self): """"""Check that all strings have been consumed."""""" for i, aList in enumerate(self.string_tokens): <IF_STMT> g.trace('warning: line %s. unused strings' % i) for z in aList: print(self.dump_token(z))",if aList:,if i in self.unused_strings:,0.7999868325863978,0.8038019482772603,False
4265,"def get_tokens_unprocessed(self, text): from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, COCOA_PROTOCOLS, COCOA_PRIMITIVES for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): <IF_STMT> if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS or value in COCOA_PRIMITIVES: token = Name.Builtin.Pseudo yield (index, token, value)",if token is Name or token is Name.Class:,if token == Name.Builtin.Pseudo:,0.7625174315000977,0.8516228624291206,False
4266,"def key_from_key_value_dict(key_info): res = [] if not 'key_value' in key_info: return res for value in key_info['key_value']: <IF_STMT> e = base64_to_long(value['rsa_key_value']['exponent']) m = base64_to_long(value['rsa_key_value']['modulus']) key = RSA.construct((m, e)) res.append(key) return res",if 'rsa_key_value' in value:,if 'rsa_key_value' in value:,0.9028658709621341,0.8228500218338367,True
4267,"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) <IF_STMT> self.view.replace(edit, region, prefixed) return for region in self.view.sel(): if region.empty(): continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",if prefixed:,if prefixed:,0.8714345267356529,0.9076141716697395,True
4268,def finalize(self): if self.ct < 1: return elif self.ct == 1: return 0 total = ct = 0 dtp = None while self.heap: <IF_STMT> if dtp is None: dtp = heapq.heappop(self.heap) continue dt = heapq.heappop(self.heap) diff = dt - dtp ct += 1 total += total_seconds(diff) dtp = dt return float(total) / ct,if total == 0:,if self.heap[0] == self.size:,0.7314375157464069,0.8983343737277126,False
4269,"def _test_configuration(self): config_path = self._write_config() try: self._log.debug('testing configuration') verboseflag = '-Q' <IF_STMT> verboseflag = '-v' p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, '-f', config_path]) if p.wait() != 0: raise RuntimeError('configuration test failed') self._log.debug('configuration seems ok') finally: os.remove(config_path)",if self._log.isEnabledFor(logging.DEBUG):,if self.verbose:,0.9336892726465744,0.8935248372106969,False
4270,"def exe(self, ret): if not ret: self.assertEqual(ret, '') else: assert os.path.isabs(ret), ret <IF_STMT> assert os.path.isfile(ret), ret if hasattr(os, 'access') and hasattr(os, 'X_OK'): self.assertTrue(os.access(ret, os.X_OK))",if POSIX:,"if hasattr(os, 'path'):",0.6264749830847629,0.8120341702859789,False
4271,"def _do_cleanup(sg_name, device_id): masking_view_list = self.rest.get_masking_views_from_storage_group(array, sg_name) for masking_view in masking_view_list: <IF_STMT> self.rest.delete_masking_view(array, masking_view) self.rest.remove_vol_from_sg(array, sg_name, device_id, extra_specs) self.rest.delete_volume(array, device_id) self.rest.delete_storage_group(array, sg_name)",if 'STG-' in masking_view:,if masking_view.device_id == device_id:,0.6728097949683043,0.760856626273165,False
4272,"def hide_tooltip_if_necessary(self, key): """"""Hide calltip when necessary"""""" try: calltip_char = self.get_character(self.calltip_position) before = self.is_cursor_before(self.calltip_position, char_offset=1) other = key in (Qt.Key_ParenRight, Qt.Key_Period, Qt.Key_Tab) <IF_STMT> QToolTip.hideText() except (IndexError, TypeError): QToolTip.hideText()","if calltip_char not in ('?', '(') or before or other:",if before and other:,0.8233454024096014,0.8105932471967202,False
4273,"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {'HasMoreTags': False, 'Tags': tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): if limit and len(tags) >= limit: result['HasMoreTags'] = True break <IF_STMT> continue tags.append({'Key': key, 'Value': val}) return result",if exclusive_start_tag_key and key < exclusive_start_tag_key:,if exclusive_start_tag_key and key == exclusive_start_tag_key:,0.9212271137999877,0.8375707157974782,False
4274,"def parametrize_function_name(request, function_name): suffixes = [] if 'parametrize' in request.keywords: argnames = request.keywords['parametrize'].args[::2] argnames = [x.strip() for names in argnames for x in names.split(',')] for name in argnames: value = request.getfuncargvalue(name) <IF_STMT> value = value.__name__ suffixes.append('{}={}'.format(name, value)) return '+'.join([function_name] + suffixes)",if inspect.isclass(value):,"if hasattr(value, '__name__'):",0.8656397878670227,0.8902579342581529,False
4275,"def add_entities(self, positions): e1 = EntityFactory() for p in positions: <IF_STMT> start, length = p else: start, length = (p, 1) EntityOccurrenceFactory(document=self.doc, entity=e1, offset=start, offset_end=start + length, alias='AB')","if isinstance(p, tuple):","if isinstance(p, tuple):",0.8669857809854037,0.8390782502060267,True
4276,"def transform_value(value): if isinstance(value, collections.MutableMapping): <IF_STMT> return DBRef(value['_ns'], transform_value(value['_id'])) else: return transform_dict(SON(value)) elif isinstance(value, list): return [transform_value(v) for v in value] return value",if '_id' in value and '_ns' in value:,if '_ns' in value:,0.8651377019026538,0.7709002428237395,False
4277,"def remove(self, items): """"""Remove messages from lease management."""""" with self._add_remove_lock: for item in items: if self._leased_messages.pop(item.ack_id, None) is not None: self._bytes -= item.byte_size else: _LOGGER.debug('Item %s was not managed.', item.ack_id) <IF_STMT> _LOGGER.debug('Bytes was unexpectedly negative: %d', self._bytes) self._bytes = 0",if self._bytes < 0:,if self._bytes < 0:,0.7042683092752532,0.8661072626070159,True
4278,"def parse_hgsub(lines): """"""Fills OrderedDict with hgsub file content passed as list of lines"""""" rv = OrderedDict() for l in lines: ls = l.strip() <IF_STMT> continue name, value = l.split('=', 1) rv[name.strip()] = value.strip() return rv",if not ls or ls[0] == '#':,if ls == '':,0.7083742748121129,0.8474968231198384,False
4279,"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: return None elif self._keys[hash_] == key: self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) <IF_STMT> return None",if initial_hash == hash_:,if hash_ == initial_hash:,0.9297674551665207,0.8588713701164342,False
4280,"def atom(token, no_symbol=False): try: return int(token) except ValueError: try: return float(token) except ValueError: <IF_STMT> return token[1:-1] elif no_symbol: return token else: return Symbol(token)","if token.startswith(""'"") or token.startswith('""'):",if token.startswith('#'):,0.8764025347747408,0.839587623092576,False
4281,"def __Suffix_Noun_Step1b(self, token): for suffix in self.__suffix_noun_step1b: <IF_STMT> token = token[:-1] self.suffixe_noun_step1b_success = True break return token",if token.endswith(suffix) and len(token) > 5:,if token.endswith(suffix):,0.7777792714287889,0.7778111223054219,False
4282,"def _guardAgainstUnicode(self, data): if _pythonMajorVersion < 3: <IF_STMT> data = data.encode('utf8') elif isinstance(data, str): try: return data.encode('ascii') except UnicodeEncodeError: pass raise ValueError('pyDes can only work with encoded strings, not Unicode.') return data","if isinstance(data, unicode):","if isinstance(data, unicode):",0.7683800327211229,0.8592899528284996,True
4283,"def populate_resource_parameters(self, tool_source): root = getattr(tool_source, 'root', None) if root is not None and hasattr(self.app, 'job_config') and hasattr(self.app.job_config, 'get_tool_resource_xml'): resource_xml = self.app.job_config.get_tool_resource_xml(root.get('id'), self.tool_type) if resource_xml is not None: inputs = root.find('inputs') <IF_STMT> inputs = parse_xml_string('<inputs/>') root.append(inputs) inputs.append(resource_xml)",if inputs is None:,if not inputs:,0.7162870823033148,0.8783650674919876,False
4284,"def test_arguments_regex(self): argument_matches = (('pip=1.1', ('pip', '1.1')), ('pip==1.1', None), ('pip=1.2=1', ('pip', '1.2=1'))) for argument, match in argument_matches: <IF_STMT> self.assertIsNone(salt.utils.args.KWARG_REGEX.match(argument)) else: self.assertEqual(salt.utils.args.KWARG_REGEX.match(argument).groups(), match)",if match is None:,if match is None:,0.8668003459598483,0.760856626273165,True
4285,def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = 'bl_%s' % self.businessline_id if self.service_id: sidebar_selected += '_s_%s' % self.service_id <IF_STMT> sidebar_selected += '_env_%s' % self.environment_id return sidebar_selected,if self.environment_id:,if self.environment_id:,0.8940720772111379,0.8645707301556367,True
4286,"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result['venture_id'] = ip.venture.id <IF_STMT> result['device_id'] = ip.device.id if ip.device.venture is not None: result['venture_id'] = ip.device.venture.id return result",if ip.device is not None:,if ip.device is not None:,0.7286315466787618,0.8492988135354755,True
4287,"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True <IF_STMT> return True return False",if not event.get_date_object():,if not event.get_place_handle():,0.9116114018720367,0.8592899528284996,False
4288,"def killIfDead(): if not self._isalive: self.log.debug(""WampLongPoll: killing inactive WAMP session with transport '{0}'"".format(self._transport_id)) self.onClose(False, 5000, 'session inactive') self._receive._kill() <IF_STMT> del self._parent._transports[self._transport_id] else: self.log.debug(""WampLongPoll: transport '{0}' is still alive"".format(self._transport_id)) self._isalive = False self.reactor.callLater(killAfter, killIfDead)",if self._transport_id in self._parent._transports:,if self._transport_id in self._parent._transports:,0.8909570878091801,0.8385130047130208,True
4289,"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): <IF_STMT> l.warning('Found a bottom field in struct %s. Ignore and increment the offset using the default element size.', self.name) continue if not self._pack: align = ty.alignment if offset_so_far % align != 0: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets","if isinstance(ty, SimTypeBottom):",if not ty.bottom:,0.8563960467787151,0.9274581800092581,False
4290,"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get('allow_css_overrides'): filename = self.view.file_name() filetypes = self.settings.get('markdown_filetypes') <IF_STMT> for filetype in filetypes: if filename.endswith(filetype): css_filename = filename.rpartition(filetype)[0] + '.css' if os.path.isfile(css_filename): return u'<style>%s</style>' % load_utf8(css_filename) return ''",if filename and filetypes:,if filetypes:,0.7634779063176381,0.8901732118131125,False
4291,"def setFullCSSSource(self, fullsrc, inline=False): self.fullsrc = fullsrc if type(self.fullsrc) == six.binary_type: self.fullsrc = six.text_type(self.fullsrc, 'utf-8') if inline: self.inline = inline if self.fullsrc: self.srcFullIdx = self.fullsrc.find(self.src) if self.srcFullIdx < 0: del self.srcFullIdx self.ctxsrcFullIdx = self.fullsrc.find(self.ctxsrc) <IF_STMT> del self.ctxsrcFullIdx",if self.ctxsrcFullIdx < 0:,if self.ctxsrcFullIdx < 0:,0.9393541715042545,0.862572866657237,True
4292,"def title(self): ret = theme['title'] if isinstance(self.name, six.string_types): width = self.statwidth() return ret + self.name[0:width].center(width).replace(' ', '-') + theme['default'] for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace(' ', '-') if i + 1 != len(self.vars): <IF_STMT> ret = ret + theme['frame'] + char['dash'] + theme['title'] else: ret = ret + char['space'] return ret",if op.color:,if i % 2 == 0:,0.7809623322186152,0.8774402785224017,False
4293,"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if self._requested_namespaces is not None and self._requested_namespaces != []: for requested_namespace in self._requested_namespaces: <IF_STMT> return [] elif requested_namespace[0] not in IGNORE_DBS: requested_databases.append(requested_namespace[0]) return requested_databases",if requested_namespace[0] is '*':,"if not isinstance(requested_namespace, list):",0.911085975226843,0.8627586293513119,False
4294,"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel['channel'] defaults = {'currency': channel.currency_code} <IF_STMT> defaults['discount_value'] = add_channel.get('discount_value') if 'min_amount_spent' in add_channel.keys(): defaults['min_spent_amount'] = add_channel.get('min_amount_spent', None) models.VoucherChannelListing.objects.update_or_create(voucher=voucher, channel=channel, defaults=defaults)",if 'discount_value' in add_channel.keys():,if 'discount_value' in add_channel.keys():,0.733174033808991,0.8228500218338367,True
4295,"def read_xml(path): with tf.gfile.GFile(path) as f: root = etree.fromstring(f.read()) annotations = {} for node in root.getchildren(): key, val = node2dict(node) <IF_STMT> annotations.setdefault(key, []).append(val) else: annotations[key] = val return annotations",if key == 'object':,if key in annotations:,0.7612346817893112,0.8169276475307028,False
4296,"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: <IF_STMT> result['venture_id'] = ip.venture.id if ip.device is not None: result['device_id'] = ip.device.id if ip.device.venture is not None: result['venture_id'] = ip.device.venture.id return result",if ip.venture is not None:,if ip.venture is not None:,0.9058908912768161,0.8492988135354755,True
4297,"def test_large_headers(self): with ExpectLog(gen_log, 'Unsatisfiable read', required=False): try: self.fetch('/', headers={'X-Filler': 'a' * 1000}, raise_error=True) self.fail('did not raise expected exception') except HTTPError as e: <IF_STMT> self.assertIn(e.response.code, (431, 599))",if e.response is not None:,if e.response:,0.6362316495700255,0.8645707301556367,False
4298,"def validate_reserved_serial_no_consumption(self): for item in self.items: if item.s_warehouse and (not item.t_warehouse) and item.serial_no: for sr in get_serial_nos(item.serial_no): sales_order = frappe.db.get_value('Serial No', sr, 'sales_order') <IF_STMT> msg = _(""(Serial No: {0}) cannot be consumed as it's reserverd to fullfill Sales Order {1}."").format(sr, sales_order) frappe.throw(_('Item {0} {1}').format(item.item_code, msg))",if sales_order:,if sales_order:,0.8097018089724629,0.9202663016973823,True
4299,"def force_decode(string, encoding): if isinstance(string, str): <IF_STMT> string = string.decode(encoding) else: try: string = string.decode('utf-8') except UnicodeError: string = string.decode('latin1') return string",if encoding:,if encoding:,0.6398719475499842,0.8318180062062374,True
4300,"def _add_cs(master_cs, sub_cs, prefix, delimiter='.', parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) if new_parameter.name == '': new_parameter.name = prefix <IF_STMT> new_parameter.name = '{}{}{}'.format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",elif not prefix == '':,elif delimiter:,0.8450292230841419,0.8996480074924822,False
4301,"def __call__(self, *args, **kwargs): if self.log_file is not None: kwargs['file'] = self.log_file print(*args, **kwargs) <IF_STMT> self.log_file.flush() elif self.log_func is not None: self.log_func(*args, **kwargs)","if hasattr(self.log_file, 'flush'):",if self.log_file.is_file():,0.61149588254302,0.839587623092576,False
4302,"def df_index_expr(self, length_expr=None, as_range=False): """"""Generate expression to get or create index of DF"""""" if isinstance(self.index, types.NoneType): <IF_STMT> length_expr = df_length_expr(self) if as_range: return f'range({length_expr})' else: return f'numpy.arange({length_expr})' return 'self._index'",if length_expr is None:,if length_expr is None:,0.8900992617566492,0.8169276475307028,True
4303,"def _setWeight(self, value): if value is None: self._fontWeight = None else: <IF_STMT> raise TextFormatException(f'Not a supported fontWeight: {value}') self._fontWeight = value.lower()","if value.lower() not in ('normal', 'bold'):","if value not in ('1', '2', '3', '4', '5', '6', '7', '8', '9', '8', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '",0.5122868507191541,0.3092662606424472,False
4304,"def _test_configuration(self): config_path = self._write_config() try: self._log.debug('testing configuration') verboseflag = '-Q' if self._log.isEnabledFor(logging.DEBUG): verboseflag = '-v' p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, '-f', config_path]) <IF_STMT> raise RuntimeError('configuration test failed') self._log.debug('configuration seems ok') finally: os.remove(config_path)",if p.wait() != 0:,if not p.returncode:,0.7143790544774076,0.8592899528284996,False
4305,"def filter_queryset(self, request, queryset, view): kwargs = {} for field in view.filterset_fields: value = request.GET.get(field) if not value: continue if field == 'node_id': value = get_object_or_none(Node, pk=value) kwargs['node'] = value continue <IF_STMT> field = 'asset' kwargs[field] = value if kwargs: queryset = queryset.filter(**kwargs) logger.debug('Filter {}'.format(kwargs)) return queryset",elif field == 'asset_id':,if field == 'asset_id':,0.9352835230410291,0.8856327184319047,False
4306,"def _find_closing_brace(string, start_pos): """"""Finds the corresponding closing brace after start_pos."""""" bracks_open = 1 for idx, char in enumerate(string[start_pos:]): if char == '(': <IF_STMT> bracks_open += 1 elif char == ')': if string[idx + start_pos - 1] != '\\': bracks_open -= 1 if not bracks_open: return start_pos + idx + 1",if string[idx + start_pos - 1] != '\\':,if string[idx + start_pos - 1] != '\\':,0.8917726776975505,0.8280163145149905,True
4307,"def _set_hostport(self, host, port): if port is None: i = host.rfind(':') j = host.rfind(']') if i > j: try: port = int(host[i + 1:]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1:]) host = host[:i] else: port = self.default_port <IF_STMT> host = host[1:-1] self.host = host self.port = port",if host and host[0] == '[' and (host[-1] == ']'):,if host.endswith(':'):,0.8899272857182389,0.9298663600557577,False
4308,"def __getstate__(self): state = {} for cls in type(self).mro(): cls_slots = getattr(cls, '__slots__', ()) for slot in cls_slots: if slot != '__weakref__': <IF_STMT> state[slot] = getattr(self, slot) state['_cookiejar_cookies'] = list(self.cookiejar) del state['cookiejar'] return state","if hasattr(self, slot):","if hasattr(self, slot):",0.8150893326501982,0.8675979125638379,True
4309,"def _evp_pkey_from_der_traditional_key(self, bio_data, password): key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL) if key != self._ffi.NULL: key = self._ffi.gc(key, self._lib.EVP_PKEY_free) <IF_STMT> raise TypeError('Password was given but private key is not encrypted.') return key else: self._consume_errors() return None",if password is not None:,if password != self._ffi.NULL:,0.7412596577394184,0.8385130047130208,False
4310,"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" assert directive and directive[0] == '@' skip_flag = directive in ('@others', '@all') while i < len(s): <IF_STMT> return (True, i) else: i = skip_line(s, i) if skip_flag: i = skip_ws(s, i) return (False, -1)","if match_word(s, i, directive):",if s[i] == directive:,0.8375055303482207,0.8879659171421962,False
4311,"def _decorator(coro_func): fut = asyncio.ensure_future(coro_func()) self._tests.append((coro_func.__name__, fut)) if timeout_sec is not None: timeout_at = self._loop.time() + timeout_sec handle = self.MASTER_LOOP.call_at(timeout_at, self._set_exception_if_not_done, fut, asyncio.TimeoutError()) fut.add_done_callback(lambda *args: handle.cancel()) <IF_STMT> self._global_timeout_at = timeout_at return coro_func",if timeout_at > self._global_timeout_at:,if timeout_at > self._global_timeout_at:,0.8827631639570174,0.8336104423443033,True
4312,"def _load(self, db, owner): self.__init(owner) db_result = db('SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ?', self.owner.worldid) for ship_id, state_id in db_result: ship = WorldObject.get_object_by_id(ship_id) state = self.shipStates[state_id] <IF_STMT> ship.add_move_callback(Callback(BehaviorMoveCallback._arrived, ship)) self.add_new_unit(ship, state)",if state == self.shipStates.moving:,if state.is_arrived:,0.8531421850399676,0.8901732118131125,False
4313,"def addError(self, test, err): if err[0] is SkipTest: <IF_STMT> self.stream.writeln(str(err[1])) elif self.dots: self.stream.write('s') self.stream.flush() return _org_AddError(self, test, err)",if self.showAll:,if self.dots:,0.8592794386201884,0.7912619863720214,False
4314,"def _construct(self, node): self.flatten_mapping(node) ret = self.construct_pairs(node) keys = [d[0] for d in ret] keys_sorted = sorted(keys, key=_natsort_key) for key in keys: expected = keys_sorted.pop(0) <IF_STMT> raise ConstructorError(None, None, 'keys out of order: expected {} got {} at {}'.format(expected, key, node.start_mark)) return dict(ret)",if key != expected:,if expected != key:,0.8206344057394453,0.8752376177722327,False
4315,"def sample_pos_items_for_u(u, num): pos_items = self.train_items[u] n_pos_items = len(pos_items) pos_batch = [] while True: if len(pos_batch) == num: break pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0] pos_i_id = pos_items[pos_id] <IF_STMT> pos_batch.append(pos_i_id) return pos_batch",if pos_i_id not in pos_batch:,if pos_i_id not in pos_batch:,0.8676631944547156,0.8036431532733102,True
4316,"def _get_id(self, type, id): fields = id.split(':') if len(fields) >= 3: <IF_STMT> logger.warning('Expected id of type %s but found type %s %s', type, fields[-2], id) return fields[-1] fields = id.split('/') if len(fields) >= 3: itype = fields[-2] if type != itype: logger.warning('Expected id of type %s but found type %s %s', type, itype, id) return fields[-1].split('?')[0] return id",if type != fields[-2]:,if type != fields[-2]:,0.9169430452413023,0.9069443196104878,True
4317,"def uninstall_environments(self, environments): environments = [env if not env.startswith(self.conda_context.envs_path) else os.path.basename(env) for env in environments] return_codes = [self.conda_context.exec_remove([env]) for env in environments] final_return_code = 0 for env, return_code in zip(environments, return_codes): <IF_STMT> log.debug(""Conda environment '%s' successfully removed."" % env) else: log.debug(""Conda environment '%s' could not be removed."" % env) final_return_code = return_code return final_return_code",if return_code == 0:,if return_code == 0:,0.9299005141436585,0.9001816649635144,True
4318,"def _add_hit_offset(self, context_list, string_name, original_offset, value): for context in context_list: hits_by_context_dict = self.hits_by_context.setdefault(context, {}) <IF_STMT> hits_by_context_dict[string_name] = (original_offset, value.encode('base64'))",if string_name not in hits_by_context_dict:,if string_name in hits_by_context_dict:,0.6479116367809648,0.7245511487202049,False
4319,"def detab(self, text, length=None): """"""Remove a tab from the front of each line of the given text."""""" if length is None: length = self.tab_length newtext = [] lines = text.split('\n') for line in lines: if line.startswith(' ' * length): newtext.append(line[length:]) <IF_STMT> newtext.append('') else: break return ('\n'.join(newtext), '\n'.join(lines[len(newtext):]))",elif not line.strip():,elif line.endswith(' ' * length):,0.9277671479213115,0.8677319190106252,False
4320,"def dump(self): print(self.package_name) for package, value in self.entries: print(str(package.version)) <IF_STMT> print('[FILTERED]') elif isinstance(value, list): variants = value for variant in variants: print('%s' % str(variant)) else: print('%s' % str(package))",if value is None:,if value is None:,0.6518871299014476,0.8105932471967202,True
4321,"def __lexical_scope(*args, **kwargs): try: scope = Scope(quasi) <IF_STMT> binding_name_set_stack[-1].add_child(scope) binding_name_set_stack.append(scope) return func(*args, **kwargs) finally: if binding_name_set_stack[-1] is scope: binding_name_set_stack.pop()",if quasi:,if scope not in binding_name_set_stack:,0.6504524324204458,0.6907573115737006,False
4322,"def getnotes(self, origin=None): if origin is None: result = self.translator_comments <IF_STMT> if result: result += '\n' + self.developer_comments else: result = self.developer_comments return result elif origin == 'translator': return self.translator_comments elif origin in ('programmer', 'developer', 'source code'): return self.developer_comments else: raise ValueError('Comment type not valid')",if self.developer_comments:,if self.developer_comments:,0.6137747005440631,0.9202663016973823,True
4323,"def fix_datetime_fields(data: TableData, table: TableName) -> None: for item in data[table]: for field_name in DATE_FIELDS[table]: <IF_STMT> item[field_name] = datetime.datetime.fromtimestamp(item[field_name], tz=datetime.timezone.utc)",if item[field_name] is not None:,if field_name in item:,0.8218979067741686,0.7378351342269067,False
4324,"def _check_for_cart_error(cart): if cart._safe_get_element('Cart.Request.Errors') is not None: error = cart._safe_get_element('Cart.Request.Errors.Error.Code').text <IF_STMT> raise CartInfoMismatchException('CartGet failed: AWS.ECommerceService.CartInfoMismatch make sure AssociateTag, CartId and HMAC are correct (dont use URLEncodedHMAC!!!)') raise CartException('CartGet failed: ' + error)",if error == 'AWS.ECommerceService.CartInfoMismatch':,"if error in ('InvalidCartError', 'InvalidCartError'):",0.6864822650015923,0.8094220211349227,False
4325,"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: <IF_STMT> raise ValueError('Longitude is out of bounds, check your JSON format or data') if geometry[1] > 90 or geometry[1] < -90: raise ValueError('Latitude is out of bounds, check your JSON format or data')",if geometry[0] > 180 or geometry[0] < -180:,if geometry[0] > 180 or geometry[0] > -180:,0.9083167046777839,0.8030129514032833,False
4326,"def _mapper_output_protocol(self, step_num, step_map): map_key = self._step_key(step_num, 'mapper') if map_key in step_map: <IF_STMT> return self.output_protocol() else: return self.internal_protocol() else: return RawValueProtocol()",if step_map[map_key] >= len(step_map) - 1:,if step_map[map_key] == 'output':,0.5265116076089648,0.7498810286408993,False
4327,"def asset(*paths): for path in paths: fspath = www_root + '/assets/' + path etag = '' try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: <IF_STMT> if not os.path.exists(fspath + '.spt'): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and '?etag=' + etag)",if path == paths[-1]:,if env.cache_static:,0.8981118712886198,0.933847757608669,False
4328,"def ping(self, payload: Union[str, bytes]='') -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <IF_STMT> payload = payload.decode('utf-8') self.logger.debug(f'Sending a ping data frame (session id: {self.session_id}, payload: {payload})') data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PING) with self.sock_send_lock: self.sock.send(data)","if isinstance(payload, bytes):","if isinstance(payload, bytes):",0.8749185683750157,0.8592899528284996,True
4329,"def is_ac_power_connected(): for power_source_path in Path('/sys/class/power_supply/').iterdir(): try: with open(power_source_path / 'type', 'r') as f: if f.read().strip() != 'Mains': continue with open(power_source_path / 'online', 'r') as f: <IF_STMT> return True except IOError: continue return False",if f.read(1) == '1':,if f.read().strip() == 'Connected':,0.6392668339528844,0.8431339019329497,False
4330,"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): <IF_STMT> self.validate_base_model(model) if hasattr(model, '_feincms_content_models'): self.validate_content_type(model)","if hasattr(model, '_create_content_base'):","if hasattr(model, '_feincms_base_models'):",0.8474862111626444,0.7848518349390632,False
4331,"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <IF_STMT> module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_()",if module.bias is not None:,if module.bias is not None:,0.8707077550093401,0.7178970818142898,True
4332,"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): if part.get_content_maintype() == 'multipart': continue ctype = part.get_content_type() <IF_STMT> ctype = OCTET_TYPE filename = part.get_filename() if not filename: filename = PART_FN_TPL % partnum headers = dict(part) LOG.debug(headers) headers['Content-Type'] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",if ctype is None:,if not ctype:,0.7155139094648928,0.9151329413834155,False
4333,"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == '|': m -= 1 n -= 1 mask[m] = 1 <IF_STMT> m -= 1 elif dirs[m, n] == '<': n -= 1 else: raise UnboundLocalError('Illegal move') return mask","elif dirs[m, n] == '^':","elif dirs[m, n] == '>':",0.9024848582886327,0.8621109017306224,False
4334,"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split('\n'): line = line.strip() <IF_STMT> continue match = COMMENT.match(line) if match: continue if strip_delimiters: if ',' in line or ';' in line: continue yield line",if line == '':,if not line:,0.6499389155926704,0.8498644646741501,False
4335,"def fetch(self, *tileables, **kw): ret_list = False if len(tileables) == 1 and isinstance(tileables[0], (tuple, list)): ret_list = True tileables = tileables[0] elif len(tileables) > 1: ret_list = True result = self._sess.fetch(*tileables, **kw) ret = [] for r, t in zip(result, tileables): <IF_STMT> ret.append(r.item()) else: ret.append(r) if ret_list: return ret return ret[0]","if hasattr(t, 'isscalar') and t.isscalar() and (getattr(r, 'size', None) == 1):",if t:,0.9474774432278878,0.9298663600557577,False
4336,"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) if container.pop(_list_marker, False): force_list = True values.extend((_convert(x[1]) for x in sorted(container.items()))) <IF_STMT> values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls(((k, _convert(v)) for k, v in iteritems(container)))",if not force_list and len(values) == 1:,if force_list:,0.9584335523756362,0.9298663600557577,False
4337,"def _transform_init_kwargs(cls, kwargs): transformed = [] for field in list(kwargs.keys()): prop = getattr(cls, field, None) <IF_STMT> value = kwargs.pop(field) _transform_single_init_kwarg(prop, field, value, kwargs) transformed.append((field, value)) return transformed","if isinstance(prop, MoneyProperty):",if prop is not None:,0.775812953205688,0.7765145040967655,False
4338,"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: if isinstance(fvalue, Packet): ret = fvalue.haslayer(cls) <IF_STMT> return ret return self.payload.haslayer(cls)",if ret:,if ret:,0.938386120696454,0.9452425713908255,True
4339,def insert_broken_add_sometimes(node): if node.op == theano.tensor.add: last_time_replaced[0] = not last_time_replaced[0] <IF_STMT> return [off_by_half(*node.inputs)] return False,if last_time_replaced[0]:,if last_time_replaced[0] == 0:,0.5357580684979065,0.6540585844910979,False
4340,"def testReadChunk10(self): self.createTempFile() with BZ2File(self.filename) as bz2f: text = '' while 1: str = bz2f.read(10) <IF_STMT> break text += str self.assertEqual(text, self.TEXT)",if not str:,if not str:,0.6819540107611457,0.7947545184555568,True
4341,"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): sv_faces = [] for i, face in enumerate(dcel_mesh.faces): if face.inners and face.outer: 'Face ({}) has inner components! Sverchok cant show polygons with holes.'.format(i) if not face.outer or del_flag in face.flags: continue <IF_STMT> continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",if only_select and (not face.select):,if only_select and face.inners and (not face.outer.loop_hedges):,0.7807003312055881,0.8272599782654279,False
4342,"def __check_dict_contains(dct, dict_name, keys, comment='', result=True): for key in keys: <IF_STMT> result = False comment = __append_comment('Missing {0} in {1}'.format(key, dict_name), comment) return (result, comment)",if key not in six.iterkeys(dct):,if key not in dct:,0.8599130128626242,0.759907656827929,False
4343,"def _dump_arg_defaults(kwargs): """"""Inject default arguments for dump functions."""""" if current_app: kwargs.setdefault('cls', current_app.json_encoder) <IF_STMT> kwargs.setdefault('ensure_ascii', False) kwargs.setdefault('sort_keys', current_app.config['JSON_SORT_KEYS']) else: kwargs.setdefault('sort_keys', True) kwargs.setdefault('cls', JSONEncoder)",if not current_app.config['JSON_AS_ASCII']:,if 'JSON_SORT_KEYS' in current_app.config:,0.8521103190056705,0.760856626273165,False
4344,"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue <IF_STMT> changed = True break self._reset_button.disabled = not changed",elif len(value) != 0:,if value != self._reset_button.value:,0.8233286025042068,0.8806615362338783,False
4345,"def parse_win_proxy(val): proxies = [] for p in val.split(';'): <IF_STMT> tab = p.split('=', 1) if tab[0] == 'socks': tab[0] = 'SOCKS4' proxies.append((tab[0].upper(), tab[1], None, None)) else: proxies.append(('HTTP', p, None, None)) return proxies",if '=' in p:,if '=' in p:,0.8802220056028986,0.8336104423443033,True
4346,"def predict(collect_dir, keys): run_all = len(keys) == 0 validate_keys(keys) for exp_cfg in cfg: <IF_STMT> key = exp_cfg['key'] _predict(key, exp_cfg['sample_img'], collect_dir)",if run_all or exp_cfg['key'] in keys:,if run_all:,0.8281290305024825,0.8137489370974955,False
4347,"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) <IF_STMT> key += '/tcp' if isinstance(v, list): result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result",if '/' not in key:,if k.startswith('/'):,0.8050200987678764,0.8901732118131125,False
4348,"def assert_conll_writer_output(dataset: InternalBioNerDataset, expected_output: List[str], sentence_splitter: SentenceSplitter=None): outfile_path = tempfile.mkstemp()[1] try: sentence_splitter = sentence_splitter <IF_STMT> else NoSentenceSplitter(tokenizer=SpaceTokenizer()) writer = CoNLLWriter(sentence_splitter=sentence_splitter) writer.write_to_conll(dataset, Path(outfile_path)) contents = [l.strip() for l in open(outfile_path).readlines() if l.strip()] finally: os.remove(outfile_path) assert contents == expected_output",if sentence_splitter,if sentence_splitter is not None,0.8124979327103823,0.8338542560892604,False
4349,"def post(self, request, *args, **kwargs): self.comment_obj = get_object_or_404(Comment, id=request.POST.get('commentid')) if request.user == self.comment_obj.commented_by: form = LeadCommentForm(request.POST, instance=self.comment_obj) <IF_STMT> return self.form_valid(form) return self.form_invalid(form) data = {'error': ""You don't have permission to edit this comment.""} return JsonResponse(data)",if form.is_valid():,if form.is_valid():,0.6143063311577301,0.8966773400768917,True
4350,"def trivia_list(self, ctx: commands.Context): """"""List available trivia categories."""""" lists = set((p.stem for p in self._all_lists())) if await ctx.embed_requested(): await ctx.send(embed=discord.Embed(title=_('Available trivia lists'), colour=await ctx.embed_colour(), description=', '.join(sorted(lists)))) else: msg = box(bold(_('Available trivia lists')) + '\n\n' + ', '.join(sorted(lists))) <IF_STMT> await ctx.author.send(msg) else: await ctx.send(msg)",if len(msg) > 1000:,if ctx.author:,0.6756545219449768,0.9164531641034833,False
4351,"def validate(self): result = validators.SUCCESS msgs = [] for validator in self._validators: res, err = validator.validate() if res == validators.ERROR: result = res elif res == validators.WARNING and result != validators.ERROR: result = res <IF_STMT> msgs.append(err) return (result, '\n'.join(msgs))",if len(err) > 0:,elif err:,0.7336910854660148,0.9076141716697395,False
4352,"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, 'exec') <IF_STMT> self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",elif mod_type == imp.PY_COMPILED:,if self.file is None:,0.6928882158147271,0.8723360571509826,False
4353,"def flush_file(self, key, f): f.flush() if self.compress: f.compress = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum((1 for f in self.files.values() if f.fileobj is not None)) <IF_STMT> f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",if open_files > self.MAX_OPEN_FILES:,if open_files > 0:,0.8955494507011746,0.8661072626070159,False
4354,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> self.add_version(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.5745717035656124,0.7378351342269067,True
4355,"def init_author_file(self): self.author_map = {} if self.ui.config('git', 'authors'): f = open(self.repo.wjoin(self.ui.config('git', 'authors'))) try: for line in f: line = line.strip() <IF_STMT> continue from_, to = RE_AUTHOR_FILE.split(line, 2) self.author_map[from_] = to finally: f.close()",if not line or line.startswith('#'):,if not line:,0.7817584625510534,0.8592899528284996,False
4356,"def decode_imsi(self, imsi): new_imsi = '' for a in imsi: c = hex(a) <IF_STMT> new_imsi += str(c[3]) + str(c[2]) else: new_imsi += str(c[2]) + '0' mcc = new_imsi[1:4] mnc = new_imsi[4:6] return (new_imsi, mcc, mnc)",if len(c) == 4:,if c[0] == '0':,0.7179320654466935,0.8474968231198384,False
4357,"def _get_infoset(self, prefname): """"""Return methods with the name starting with prefname."""""" infoset = [] excludes = ('%sinfoset' % prefname,) preflen = len(prefname) for name in dir(self.__class__): if name.startswith(prefname) and name not in excludes: member = getattr(self.__class__, name) <IF_STMT> infoset.append(name[preflen:].replace('_', ' ')) return infoset","if isinstance(member, MethodType):",if member.name == prefname:,0.7980622106893752,0.8752376177722327,False
4358,def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok['style'] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok['text'] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 <IF_STMT> break,if nestedCount <= 0:,if nestedCount == 0:,0.9352156811069626,0.8793112779588971,False
4359,"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = (c.rootPosition(), [], []) while p: <IF_STMT> p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, '@ignore'): p.moveToNodeAfterTree() elif p.h.startswith('@mark-for-unit-tests'): result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",if p.v in seen:,if p.v in seen:,0.8911960694305907,0.8627586293513119,True
4360,"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier['part'] earlier_step = earlier['step'] found = False for current in current_parts: if earlier_part == current['part'] and earlier_step == current['step']: found = True break <IF_STMT> cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",if not found:,if found:,0.8132602715107161,0.9298663600557577,False
4361,"def unmark_first_parents(event=None): """"""Mark the node and all its parents."""""" c = event.get('c') if not c: return changed = [] for parent in c.p.self_and_parents(): <IF_STMT> parent.v.clearMarked() parent.setAllAncestorAtFileNodesDirty() changed.append(parent.copy()) if changed: c.setChanged() c.redraw() return changed",if parent.isMarked():,if parent.v:,0.9054714759744091,0.8901732118131125,False
4362,"def stop(self): self._log('Monitor stop') self._stop_requested = True try: <IF_STMT> fd = os.open(self.fifo_path, os.O_WRONLY) os.write(fd, b'X') os.close(fd) except Exception as e: self._log('err while closing: {0}'.format(str(e))) if self._thread: self._thread.join() self._thread = None",if os.path.exists(self.fifo_path):,if self.fifo_path:,0.6609803002500121,0.8787142254774354,False
4363,"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): <IF_STMT> f = False if f: cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]","if self.FindItemAtPosition((r, c)) is not None:",if self.GetColumn(c) == r:,0.9185825981117535,0.8723360571509826,False
4364,"def _load_objects(self, obj_id_zset, limit, chunk_size=1000): ct = i = 0 while True: id_chunk = obj_id_zset[i:i + chunk_size] <IF_STMT> return i += chunk_size for raw_data in self._data[id_chunk]: if not raw_data: continue if self._use_json: yield json.loads(decode(raw_data)) else: yield raw_data ct += 1 if limit and ct == limit: return",if not id_chunk:,if id_chunk == limit:,0.6986313999212151,0.8856327184319047,False
4365,"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] if tf.keras.backend.is_sparse(val): val = tf.sparse.to_dense(val) <IF_STMT> val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",if val.dtype == tf.int64:,if use_bfloat16 and val.dtype == tf.int64:,0.8118532220377878,0.8375707157974782,False
4366,"def print_callees(self, *amount): width, list = self.get_print_list(amount) if list: self.calc_callees() self.print_call_heading(width, 'called...') for func in list: <IF_STMT> self.print_call_line(width, func, self.all_callees[func]) else: self.print_call_line(width, func, {}) print >> self.stream print >> self.stream return self",if func in self.all_callees:,if func in self.all_callees:,0.7645830006450083,0.8336104423443033,True
4367,"def on_task_input(self, task, config): if config is False: return for entry in task.entries: <IF_STMT> log_once('Corrected `%s` url (replaced &amp; with &)' % entry['title'], logger=log) entry['url'] = entry['url'].replace('&amp;', '&')",if '&amp;' in entry['url']:,if 'url' in entry:,0.7794604053595814,0.8105932471967202,False
4368,"def function(self, inputs, outputs, ignore_empty=False): f = function(inputs, outputs, mode=self.mode) if self.mode is not None or theano.config.mode != 'FAST_COMPILE': topo = f.maker.fgraph.toposort() topo_ = [node for node in topo if not isinstance(node.op, self.ignore_topo)] if ignore_empty: assert len(topo_) <= 1, topo_ else: assert len(topo_) == 1, topo_ <IF_STMT> assert type(topo_[0].op) is self.op return f",if len(topo_) > 0:,if len(topo_) == 1:,0.9427170276483869,0.8983343737277126,False
4369,"def _get_env_command(self) -> Sequence[str]: """"""Get command sequence for `env` with configured flags."""""" env_list = ['env'] for key in ['http_proxy', 'https_proxy']: value = self.build_provider_flags.get(key) <IF_STMT> continue value = str(value) env_list.append(f'{key}={value}') return env_list",if not value:,if value is None:,0.6931312673230514,0.828399516355805,False
4370,"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get(timeout=float(self.config['ExpertSettings']['block_delay'])) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) <IF_STMT> self.callback()",if self.callback:,if self.callback:,0.6862502910189119,0.8764445248055556,True
4371,"def clean(self): if not self.code: self.code = u'static-%s' % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter(code=self.code, site__isnull=True) if self.pk: placeholders = placeholders.exclude(pk=self.pk) <IF_STMT> raise ValidationError(_('A static placeholder with the same site and code already exists'))",if placeholders.exists():,if len(placeholders) == 1:,0.6665594772805379,0.8474968231198384,False
4372,"def load_parser(self): result = OrderedDict() for name, flags in self.filenames: filename = self.get_filename(name) for match in sorted(glob(filename), key=self.file_key): <IF_STMT> continue result[match] = TextParser(match, os.path.relpath(match, self.base), flags) return result",if match in result:,if match.startswith('.'):,0.8954470347524519,0.8696398662122882,False
4373,"def __init__(self, selectable, name=None): baseselectable = selectable while isinstance(baseselectable, Alias): baseselectable = baseselectable.element self.original = baseselectable self.supports_execution = baseselectable.supports_execution if self.supports_execution: self._execution_options = baseselectable._execution_options self.element = selectable if name is None: <IF_STMT> name = getattr(self.original, 'name', None) name = _anonymous_label('%%(%d %s)s' % (id(self), name or 'anon')) self.name = name",if self.original.named_with_column:,"if hasattr(self.original, 'name'):",0.9477643175798308,0.9081987180086649,False
4374,"def load_tour(self, tour_id): for tour_dir in self.tour_directories: tour_path = os.path.join(tour_dir, tour_id + '.yaml') if not os.path.exists(tour_path): tour_path = os.path.join(tour_dir, tour_id + '.yml') <IF_STMT> return self._load_tour_from_path(tour_path)",if os.path.exists(tour_path):,if os.path.exists(tour_path):,0.7178566110598511,0.8647513948254395,True
4375,def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color if t.theme_style == 'Dark': <IF_STMT> c = t.primary_dark elif self.md_bg_color == t.accent_color: c = t.accent_dark return c,if self.md_bg_color == t.primary_color:,if self.md_bg_color == t.primary_color:,0.8648829015918345,0.7886336751695258,True
4376,"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): if shapes[i] is not None: <IF_STMT> data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)","if isinstance(request, numbers.Integral):",if i == len(data):,0.8119665983683224,0.8661072626070159,False
4377,"def onClicked(event): if not self.path: <IF_STMT> os.makedirs(mh.getPath('render')) self.path = mh.getPath('render') filename, ftype = mh.getSaveFileName(os.path.splitext(self.path)[0], 'PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*)') if filename: if 'Thumbnail' in ftype: self.image.save(filename, iformat='PNG') else: self.image.save(filename) self.path = os.path.dirname(filename)",if not os.path.exists(mh.getPath('render')):,if not os.path.exists(mh.getPath('render')):,0.9181541134134561,0.8713933650206428,True
4378,"def _build_dom(cls, content, mode): assert mode in ('html', 'xml') if mode == 'html': <IF_STMT> THREAD_STORAGE.html_parser = HTMLParser() dom = defusedxml.lxml.parse(StringIO(content), parser=THREAD_STORAGE.html_parser) return dom.getroot() else: if not hasattr(THREAD_STORAGE, 'xml_parser'): THREAD_STORAGE.xml_parser = XMLParser() dom = defusedxml.lxml.parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser) return dom.getroot()","if not hasattr(THREAD_STORAGE, 'html_parser'):","if not hasattr(THREAD_STORAGE, 'html_parser'):",0.8137060656484977,0.8555308664663046,True
4379,"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): <IF_STMT> ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to(points[0], points[1], points[0], points[1], points[2], points[3]) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",if code == Path.MOVETO:,if code == Path.MOVE:,0.656259279877969,0.8474968231198384,False
4380,"def _targets(self, sigmaparser): targets = set() for condfield in self.conditions: if condfield in sigmaparser.values: rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: <IF_STMT> targets.update(self.conditions[condfield][condvalue]) return targets",if condvalue in rulefieldvalues:,if condvalue not in rulefieldvalues:,0.8831598404086041,0.759907656827929,False
4381,"def create_image_upload(): if request.method == 'POST': image = request.form['image'] <IF_STMT> image_file = uploaded_file(file_content=image) image_url = upload_local(image_file, UPLOAD_PATHS['temp']['image'].format(uuid=uuid4())) return jsonify({'status': 'ok', 'image_url': image_url}) else: return jsonify({'status': 'no_image'})",if image:,if image:,0.7505152168202766,0.8590888738245122,True
4382,"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: if key[-1] == '!': <IF_STMT> break elif not resp.match(key, val): break else: actions[action] = opts return actions","if resp.match(key[:-1], val):",if val in resp:,0.8940260429596668,0.8592377270804451,False
4383,"def accept_quality(accept, default=1): """"""Separates out the quality score from the accepted content_type"""""" quality = default if accept and ';' in accept: accept, rest = accept.split(';', 1) accept_quality = RE_ACCEPT_QUALITY.search(rest) <IF_STMT> quality = float(accept_quality.groupdict().get('quality', quality).strip()) return (quality, accept.strip())",if accept_quality:,if accept_quality:,0.8550259511021889,0.9024521756077707,True
4384,"def save(self, session=None, to=None, pickler=None): if to and pickler: self._save_to = (pickler, to) if self._save_to and len(self) > 0: with self._lock: pickler, fn = self._save_to <IF_STMT> session.ui.mark(_('Saving %s state to %s') % (self, fn)) pickler(self, fn)",if session:,if session:,0.9267053594477782,0.8996480074924822,True
4385,"def get_safe_settings(): """"""Returns a dictionary of the settings module, with sensitive settings blurred out."""""" settings_dict = {} for k in dir(settings): if k.isupper(): <IF_STMT> settings_dict[k] = '********************' else: settings_dict[k] = getattr(settings, k) return settings_dict",if HIDDEN_SETTINGS.search(k):,if k == 'safe':,0.7701298217693358,0.8431339019329497,False
4386,def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 if part_h & 1: part_l |= 1 << 31 part_h >>= 1 <IF_STMT> part_h ^= 3623878656 _table_h.append(part_h) return _table_h,if rflag:,if rflag:,0.7753789326257109,0.9220450449751959,True
4387,"def dns_query(server, timeout, protocol, qname, qtype, qclass): request = dns.message.make_query(qname, qtype, qclass) if protocol == 'tcp': response = dns.query.tcp(request, server, timeout=timeout, one_rr_per_rrset=True) else: response = dns.query.udp(request, server, timeout=timeout, one_rr_per_rrset=True) <IF_STMT> response = dns.query.tcp(request, server, timeout=timeout, one_rr_per_rrset=True) return response",if response.flags & dns.flags.TC:,elif protocol == 'tcp':,0.8900577800108609,0.8592377270804451,False
4388,"def sum_and_divide(self, losses): if self.total_divisor != 0: output = torch.sum(losses) / self.total_divisor <IF_STMT> self.total_divisor = self.total_divisor.item() return output return torch.sum(losses * 0)",if torch.is_tensor(self.total_divisor):,if output.ndim == 1:,0.6777689476777455,0.760856626273165,False
4389,def __iter__(self): for chunk in self.source: if chunk is not None: self.wait_counter = 0 yield chunk <IF_STMT> self.wait_counter += 1 else: logger.warning('Data poller has been receiving no data for {} seconds.\nClosing data poller'.format(self.wait_cntr_max * self.poll_period)) break time.sleep(self.poll_period),elif self.wait_counter < self.wait_cntr_max:,elif self.wait_counter < self.wait_cntr_max:,0.7624371431766604,0.8555308664663046,True
4390,"def test_find_directive_from_block(self): blocks = self.config.parser_root.find_blocks('virtualhost') found = False for vh in blocks: <IF_STMT> servername = vh.find_directives('servername') self.assertEqual(servername[0].parameters[0], 'certbot.demo') found = True self.assertTrue(found)",if vh.filepath.endswith('sites-enabled/certbot.conf'):,if vh.name == 'servername':,0.8493833827370398,0.760856626273165,False
4391,"def assign_products(request, discount_id): """"""Assign products to given property group with given property_group_id."""""" discount = lfs_get_object_or_404(Discount, pk=discount_id) for temp_id in request.POST.keys(): <IF_STMT> temp_id = temp_id.split('-')[1] product = Product.objects.get(pk=temp_id) discount.products.add(product) html = [['#products-inline', products_inline(request, discount_id, as_string=True)]] result = json.dumps({'html': html, 'message': _(u'Products have been assigned.')}, cls=LazyEncoder) return HttpResponse(result, content_type='application/json')",if temp_id.startswith('product'):,if '-' in temp_id:,0.6274480722008104,0.8856327184319047,False
4392,"def ChangeStyle(self, combos): style = 0 for combo in combos: if combo.GetValue() == 1: <IF_STMT> style = style | HTL.TR_VIRTUAL else: try: style = style | eval('wx.' + combo.GetLabel()) except: style = style | eval('HTL.' + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",if combo.GetLabel() == 'TR_VIRTUAL':,if self.GetType() == 'TR':,0.7285873579494092,0.8723360571509826,False
4393,"def _set_autocomplete(self, notebook): if notebook: try: <IF_STMT> notebook = NotebookInfo(notebook) obj, x = build_notebook(notebook) self.form.widgets['namespace'].notebook = obj self.form.widgets['page'].notebook = obj logger.debug('Notebook for autocomplete: %s (%s)', obj, notebook) except: logger.exception('Could not set notebook: %s', notebook) else: self.form.widgets['namespace'].notebook = None self.form.widgets['page'].notebook = None logger.debug('Notebook for autocomplete unset')","if isinstance(notebook, str):","if not isinstance(notebook, str):",0.9299161574076328,0.8806615362338783,False
4394,"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: <IF_STMT> return None subDomain = subDomainElem.get('subdomain', '').strip() if subDomain: self.emitHostname(subDomain, event)",if self.checkForStop():,if not subDomainElem.get('subdomain'):,0.7701027940319906,0.7848518349390632,False
4395,"def get_all_subnets(self, subnet_ids=None, filters=None): matches = itertools.chain(*[x.values() for x in self.subnets.values()]) if subnet_ids: matches = [sn for sn in matches if sn.id in subnet_ids] <IF_STMT> unknown_ids = set(subnet_ids) - set(matches) raise InvalidSubnetIdError(unknown_ids) if filters: matches = generic_filter(filters, matches) return matches",if len(subnet_ids) > len(matches):,if len(subnet_ids) != len(matches):,0.8092358922915966,0.8661072626070159,False
4396,"def _compat_map(self, avs): apps = {} for av in avs: av.version = self app_id = av.application <IF_STMT> apps[amo.APP_IDS[app_id]] = av return apps",if app_id in amo.APP_IDS:,if app_id in amo.APP_IDS:,0.6344276398627535,0.760856626273165,True
4397,"def generator(self, data): if self._config.SILENT: silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: <IF_STMT> continue yield (0, [int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val)])",if var in silent_vars:,if var in silent_vars:,0.9067156116253549,0.8105932471967202,True
4398,def warn_if_repeatable_read(self): if 'mysql' in self.current_engine().lower(): cursor = self.connection_for_read().cursor() if cursor.execute('SELECT @@tx_isolation'): isolation = cursor.fetchone()[0] <IF_STMT> warnings.warn(TxIsolationWarning('Polling results with transaction isolation level repeatable-read within the same transaction may give outdated results. Be sure to commit the transaction for each poll iteration.')),if isolation == 'REPEATABLE-READ':,if isolation > 0:,0.6551925438771385,0.8692960007731574,False
4399,"def filter_by_level(record, level_per_module): name = record['name'] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = '' <IF_STMT> level = level_per_module[''] for n in name.split('.'): lookup += n if lookup in level_per_module: level = level_per_module[lookup] lookup += '.' if level is False: return False return record['level'].no >= level",if '' in level_per_module:,if '.' in name:,0.9459116539290365,0.9019629427251674,False
4400,"def _readStream(self, handle: str, path: str) -> None: eof = False file = Path(path) with file.open('w') as f: while not eof: response = await self._client.send('IO.read', {'handle': handle}) eof = response.get('eof', False) <IF_STMT> f.write(response.get('data', '')) await self._client.send('IO.close', {'handle': handle})",if path:,if response:,0.8916290153257365,0.9051034981560222,False
4401,"def sendall(self, data, flags=0): if self._sslobj: <IF_STMT> raise ValueError('non-zero flags not allowed in calls to sendall() on %s' % self.__class__) amount = len(data) count = 0 while count < amount: v = self.send(data[count:]) count += v return amount else: return socket.sendall(self, data, flags)",if flags != 0:,if flags != 0:,0.8930389329034027,0.8752376177722327,True
4402,def run(self): utils.assert_main_thread() if not channel: <IF_STMT> SwiDebugStartChromeCommand.run(self) else: self.window.run_command('swi_debug_start') elif paused: logger.info('Resuming...') channel.send(webkit.Debugger.resume()) else: logger.info('Pausing...') channel.send(webkit.Debugger.setSkipAllPauses(False)) channel.send(webkit.Debugger.pause()),if not chrome_launched():,if self.window.is_chrome():,0.5632778091392072,0.7912619863720214,False
4403,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.682460343833398,0.828399516355805,True
4404,"def _replace_home(x): if xp.ON_WINDOWS: home = builtins.__xonsh__.env['HOMEDRIVE'] + builtins.__xonsh__.env['HOMEPATH'][0] if x.startswith(home): x = x.replace(home, '~', 1) <IF_STMT> x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env['HOME'] if x.startswith(home): x = x.replace(home, '~', 1) return x",if builtins.__xonsh__.env.get('FORCE_POSIX_PATHS'):,elif x.startswith(os.sep):,0.6648424911332969,0.8996480074924822,False
4405,"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() for key, value in list(semanticTags.items()): <IF_STMT> raise TypeError('At least one key is not a valid int position') if not isinstance(value, list): raise TypeError('At least one value of the provided dict is not a list of string') for x in value: if not isinstance(x, str): raise TypeError('At least one value of the provided dict is not a list of string') self.__semanticTags = semanticTags","if not isinstance(key, int):","if not isinstance(key, int):",0.7734678605188215,0.924776563154472,True
4406,"def _recv(): try: return sock.recv(bufsize) except SSLWantReadError: pass except socket.error as exc: error_code = extract_error_code(exc) if error_code is None: raise <IF_STMT> raise r, w, e = select.select((sock,), (), (), sock.gettimeout()) if r: return sock.recv(bufsize)",if error_code != errno.EAGAIN or error_code != errno.EWOULDBLOCK:,if error_code != errno.EINTR:,0.5968222822558017,0.8431339019329497,False
4407,"def _authenticate(self): oauth_token = self.options.get('oauth_token') if oauth_token and (not self.api.oauth_token): self.logger.info('Attempting to authenticate using OAuth token') self.api.oauth_token = oauth_token user = self.api.user(schema=_user_schema) <IF_STMT> self.logger.info('Successfully logged in as {0}', user) else: self.logger.error('Failed to authenticate, the access token is not valid') else: return JustinTVPluginBase._authenticate(self)",if user:,if user.is_valid():,0.6653901510270006,0.9144061946646023,False
4408,"def reverse(self, *args): assert self._path is not None, 'Cannot reverse url regex ' + self.regex.pattern assert len(args) == self._group_count, 'required number of arguments not found' if not len(args): return self._path converted_args = [] for a in args: <IF_STMT> a = str(a) converted_args.append(escape.url_escape(utf8(a), plus=False)) return self._path % tuple(converted_args)","if not isinstance(a, (unicode_type, bytes)):","if not isinstance(a, str):",0.9397760114426884,0.8856327184319047,False
4409,"def determine_block_hints(self, text): hints = '' if text: <IF_STMT> hints += str(self.best_indent) if text[-1] not in '\n\x85\u2028\u2029': hints += '-' elif len(text) == 1 or text[-2] in '\n\x85\u2028\u2029': hints += '+' return hints",if text[0] in ' \n\x85\u2028\u2029':,if len(text) > 1:,0.8824831641261263,0.8385130047130208,False
4410,"def find_package_modules(package, mask): import fnmatch if hasattr(package, '__loader__') and hasattr(package.__loader__, '_files'): path = package.__name__.replace('.', os.path.sep) mask = os.path.join(path, mask) for fnm in package.__loader__._files.iterkeys(): <IF_STMT> yield os.path.splitext(fnm)[0].replace(os.path.sep, '.') else: path = package.__path__[0] for fnm in os.listdir(path): if fnmatch.fnmatchcase(fnm, mask): yield ('%s.%s' % (package.__name__, os.path.splitext(fnm)[0]))","if fnmatch.fnmatchcase(fnm, mask):","if fnmatch.fnmatchcase(fnm, mask):",0.845838628675437,0.8953711787948615,True
4411,"def _condition(ct): for qobj in args: if qobj.connector == 'AND' and (not qobj.negated): for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError('Unsupported Q object') for attr, val in kwargs.items(): <IF_STMT> return False return True","if getattr(ct, attr) != val:","if not ct.condition(attr, val):",0.9259426312323291,0.8385130047130208,False
4412,"def process(self, resources): session = local_session(self.manager.session_factory) client = session.client('logs') state = self.data.get('state', True) key = self.resolve_key(self.data.get('kms-key')) for r in resources: try: <IF_STMT> client.associate_kms_key(logGroupName=r['logGroupName'], kmsKeyId=key) else: client.disassociate_kms_key(logGroupName=r['logGroupName']) except client.exceptions.ResourceNotFoundException: continue",if state:,if state:,0.9177843401553544,0.8743414417652072,True
4413,"def get_xmm(env, ii): if is_gather(ii): <IF_STMT> return gen_reg_simd_unified(env, 'xmm_evex', True) return gen_reg_simd_unified(env, 'xmm', False) if ii.space == 'evex': return gen_reg(env, 'xmm_evex') return gen_reg(env, 'xmm')",if ii.space == 'evex':,if ii.space == 'evex':,0.852895111092594,0.7801270245332924,True
4414,"def parent(self): """"""Return the parent device."""""" if self._has_parent is None: _parent = self._ctx.backend.get_parent(self._ctx.dev) self._has_parent = _parent is not None <IF_STMT> self._parent = Device(_parent, self._ctx.backend) else: self._parent = None return self._parent",if self._has_parent:,"if isinstance(_parent, Device):",0.7559914328917247,0.8498644646741501,False
4415,"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = (50, 50, 50) for frame in g.app.windowList: w = frame and frame.top <IF_STMT> r = w.geometry() w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) x += 30 y += 30 if x > 200: x = 10 + delta y = 40 + delta delta += 10",if w:,if w:,0.8269868522163353,0.933847757608669,True
4416,"def _GetGoodDispatchAndUserName(IDispatch, userName, clsctx): if userName is None: if isinstance(IDispatch, str): userName = IDispatch <IF_STMT> userName = IDispatch.encode('ascii', 'replace') elif type(userName) == unicode: userName = userName.encode('ascii', 'replace') else: userName = str(userName) return (_GetGoodDispatch(IDispatch, clsctx), userName)","elif isinstance(IDispatch, unicode):",elif type(IDispatch) == unicode:,0.6904864928519128,0.8474968231198384,False
4417,"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = bytes else: <IF_STMT> raise TypeError(""Can't mix bytes and non-bytes in path components."") return_type = str if return_type is None: return str return return_type",if return_type is bytes:,if return_type is str:,0.8012065025734805,0.9204199807826591,False
4418,"def test_ESPnetDataset_h5file_1(h5file_1): dataset = IterableESPnetDataset(path_name_type_list=[(h5file_1, 'data4', 'hdf5')], preprocess=preprocess) for key, data in dataset: if key == 'a': assert data['data4'].shape == (100, 80) <IF_STMT> assert data['data4'].shape == (150, 80)",if key == 'b':,elif key == 'b':,0.8831822795402505,0.8105932471967202,False
4419,"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) <IF_STMT> continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if field_val is _marker:,if field_val is None:,0.8883807945353852,0.8806615362338783,False
4420,"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: <IF_STMT> if self.match_name: match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",if match not in matches:,if match.name == self.match_name:,0.8600331342653093,0.8661072626070159,False
4421,"def _set_chat_ids(self, chat_id: SLT[int]) -> None: with self.__lock: <IF_STMT> raise RuntimeError(f""Can't set {self.chat_id_name} in conjunction with (already set) {self.username_name}s."") self._chat_ids = self._parse_chat_id(chat_id)",if chat_id and self._usernames:,if self._chat_ids:,0.8606422462388265,0.8318180062062374,False
4422,"def discover(self, *objlist): ret = [] for l in self.splitlines(): <IF_STMT> continue if l[0] == 'Filename': continue try: int(l[2]) int(l[3]) except: continue ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",if len(l) < 5:,if len(l) < 4:,0.9181717145296917,0.828399516355805,False
4423,"def get_changed_module(self): source = self.resource.read() change_collector = codeanalyze.ChangeCollector(source) if self.replacement is not None: change_collector.add_change(self.skip_start, self.skip_end, self.replacement) for occurrence in self.occurrence_finder.find_occurrences(self.resource): start, end = occurrence.get_primary_range() <IF_STMT> self.handle.occurred_inside_skip(change_collector, occurrence) else: self.handle.occurred_outside_skip(change_collector, occurrence) result = change_collector.get_changed() if result is not None and result != source: return result",if self.skip_start <= start < self.skip_end:,if start < self.skip_start:,0.7980194463375789,0.8780099567239787,False
4424,"def hpat_pandas_series_var_impl(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): if skipna is None: skipna = True if skipna: valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data)) <IF_STMT> return numpy.nan return numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof) if len(self._data) <= ddof: return numpy.nan return self._data.var() * len(self._data) / (len(self._data) - ddof)",if valuable_length <= ddof:,if valuable_length <= ddof:,0.8562906965736178,0.8832000938217648,True
4425,"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, 'condition', Undefined) copy = self if condition is not Undefined: <IF_STMT> pass elif 'field' in condition and 'type' not in condition: kwds = parse_shorthand(condition['field'], context.get('data', None)) copy = self.copy(deep=['condition']) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict(validate=validate, ignore=ignore, context=context)","if isinstance(condition, core.SchemaBase):",if validate:,0.9415054219573742,0.9253742688467129,False
4426,"def get_field_result(self, result, field_name): if isinstance(result.field, models.ImageField): <IF_STMT> img = getattr(result.obj, field_name) result.text = mark_safe('<a href=""%s"" target=""_blank"" title=""%s"" data-gallery=""gallery""><img src=""%s"" class=""field_img""/></a>' % (img.url, result.label, img.url)) self.include_image = True return result",if result.value:,if field_name:,0.8393537503769024,0.8787142254774354,False
4427,"def run(self): try: while True: dp = self.queue_get_stoppable(self.inq) <IF_STMT> return obj = self.func(dp) self.queue_put_stoppable(self.outq, obj) except Exception: if self.stopped(): pass else: raise finally: self.stop()",if self.stopped():,if dp is None:,0.5724112401248521,0.7801270245332924,False
4428,"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) <IF_STMT> self._progress_hook(batch) yield results",if self._progress_hook:,if self._progress_hook:,0.9080998192723662,0.8935248372106969,True
4429,"def merge(self, other): d = self._name2ft for name, (f, t) in other._name2ft.items(): <IF_STMT> f2, t2 = d[name] f = f + f2 t = t + t2 d[name] = (f, t)",if name in d:,if name in d:,0.8585158831251318,0.828399516355805,True
4430,"def _addSettingsToPanels(self, category, left, right): count = len(profile.getSubCategoriesFor(category)) + len(profile.getSettingsForCategory(category)) p = left n = 0 for title in profile.getSubCategoriesFor(category): n += 1 + len(profile.getSettingsForCategory(category, title)) <IF_STMT> p = right configBase.TitleRow(p, _(title)) for s in profile.getSettingsForCategory(category, title): configBase.SettingRow(p, s.getName())",if n > count / 2:,if n > count:,0.8350645198207802,0.8627586293513119,False
4431,"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): <IF_STMT> continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if i == '.' or i == '..':,if i.startswith('.'):,0.9169253693360477,0.938501942261528,False
4432,def check_network_private(test_network): test_net = ipaddress.IPNetwork(test_network) test_start = test_net.network test_end = test_net.broadcast for network in settings.vpn.safe_priv_subnets: network = ipaddress.IPNetwork(network) net_start = network.network net_end = network.broadcast <IF_STMT> return True return False,if test_start >= net_start and test_end <= net_end:,if net_start <= test_end and net_start <= test_end:,0.8165051336413773,0.7221052041889016,False
4433,def _end_description(self): if self._summaryKey == 'content': self._end_content() else: value = self.popContent('description') context = self._getContext() <IF_STMT> context['textinput']['description'] = value elif self.inimage: context['image']['description'] = value self._summaryKey = None,if self.intextinput:,if self.intextinput:,0.8439741882768342,0.8590888738245122,True
4434,def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: <IF_STMT> nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 if len(nullable) == num_nullable: break num_nullable = len(nullable) return nullable,if p.len == 0:,if p.name not in nullable:,0.7049889314547133,0.8559898693114286,False
4435,"def process_bind_param(self, value, dialect): if value is not None: if MAX_METADATA_VALUE_SIZE is not None: for k, v in list(value.items()): sz = total_size(v) <IF_STMT> del value[k] log.warning('Refusing to bind metadata key {} due to size ({})'.format(k, sz)) value = json_encoder.encode(value).encode() return value",if sz > MAX_METADATA_VALUE_SIZE:,if sz > MAX_METADATA_VALUE_SIZE:,0.7252526317060635,0.8692960007731574,True
4436,"def process_input_line(self, line, store_history=True): """"""process the input, capturing stdout"""""" stdout = sys.stdout splitter = self.IP.input_splitter try: sys.stdout = self.cout splitter.push(line) more = splitter.push_accepts_more() <IF_STMT> try: source_raw = splitter.source_raw_reset()[1] except: source_raw = splitter.raw_reset() self.IP.run_cell(source_raw, store_history=store_history) finally: sys.stdout = stdout",if not more:,if more:,0.8708110552080028,0.9051034981560222,False
4437,"def _dump_section(self, name, values, f): doc = '__doc__' <IF_STMT> print('# %s' % values[doc], file=f) print('%s(' % name, file=f) for k, v in values.items(): if k.endswith('__doc__'): continue doc = k + '__doc__' if doc in values: print('# %s' % values[doc], file=f) print('%s = %s,' % (k, pprint.pformat(v, indent=8)), file=f) print(')\n', file=f)",if doc in values:,if doc in values:,0.7949899788850925,0.8923575006167597,True
4438,"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration <IF_STMT> expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession(initial=stored_session.data, sid=stored_session.sid) return MongoEngineSession(sid=str(uuid.uuid4()))",if not expiration.tzinfo:,if expiration:,0.9337921735719283,0.8787142254774354,False
4439,"def table_entry(mode1, bind_type1, mode2, bind_type2): with sock(mode1) as sock1: bind(sock1, bind_type1) try: with sock(mode2) as sock2: bind(sock2, bind_type2) except OSError as exc: <IF_STMT> return 'INUSE' elif exc.winerror == errno.WSAEACCES: return 'ACCESS' raise else: return 'Success'",if exc.winerror == errno.WSAEADDRINUSE:,if exc.winerror == errno.EINVAL:,0.8896639888477119,0.8474968231198384,False
4440,"def __init__(self, ruleset): self.ruleset = ruleset self.rules = {} for filename in self.ruleset.rules: for rule in self.ruleset.rules[filename]: <IF_STMT> continue manage_dictionary(self.rules, rule.path, []) self.rules[rule.path].append(rule)",if not rule.enabled:,if rule.path in self.rules:,0.8454925426094948,0.7709002428237395,False
4441,"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) <IF_STMT> attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) if reply == '!done': return r",if j == -1:,if j == -1:,0.9250098249862491,0.8983343737277126,True
4442,"def _check_decorator_overload(name: str, old: str, new: str) -> int: """"""Conditions for a decorator to overload an existing one."""""" properties = _property_decorators(name) if old == new: return _MERGE elif old in properties and new in properties: p_old, p_new = (properties[old].precedence, properties[new].precedence) <IF_STMT> return _DISCARD elif p_old == p_new: return _MERGE else: return _REPLACE raise OverloadedDecoratorError(name, '')",if p_old > p_new:,if p_old == p_new:,0.834998520765796,0.9019629427251674,False
4443,"def validate_pk(self): try: self._key = serialization.load_pem_private_key(self.key, password=None, backend=default_backend()) if self._key.key_size > 2048: AWSValidationException('The private key length is not supported. Only 1024-bit and 2048-bit are allowed.') except Exception as err: <IF_STMT> raise raise AWSValidationException('The private key is not PEM-encoded or is not valid.')","if isinstance(err, AWSValidationException):",if err.errno != 'InvalidKey.pem':,0.9050693490774717,0.8723360571509826,False
4444,"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy['Version'] = '2012-10-17' if self.resource_policy.get('Statement') is None: self.resource_policy['Statement'] = custom_statements else: <IF_STMT> custom_statements = [custom_statements] statement = self.resource_policy['Statement'] if not isinstance(statement, list): statement = [statement] for s in custom_statements: if s not in statement: statement.append(s) self.resource_policy['Statement'] = statement","if not isinstance(custom_statements, list):","if not isinstance(custom_statements, list):",0.7408606293967999,0.8832000938217648,True
4445,"def load(self, repn): for key in repn: tmp = self._convert(key) <IF_STMT> self.declare(tmp) item = dict.__getitem__(self, tmp) item._active = True item.load(repn[key])",if tmp not in self:,if not self.declare:,0.6591996158234136,0.7739321540474097,False
4446,"def on_press_release(x): """"""Keyboard callback function."""""" global is_recording, enable_trigger_record press = keyboard.KeyboardEvent('down', 28, 'space') release = keyboard.KeyboardEvent('up', 28, 'space') if x.event_type == 'down' and x.name == press.name: if not is_recording and enable_trigger_record: sys.stdout.write('Start Recording ... ') sys.stdout.flush() is_recording = True if x.event_type == 'up' and x.name == release.name: <IF_STMT> is_recording = False",if is_recording == True:,if not is_recording and enable_trigger_record:,0.9379070307722948,0.8777008558345754,False
4447,"def apply_mask(self, mask, data_t, data_f): ind_t, ind_f = (0, 0) out = [] for m in cycle(mask): if m: if ind_t == len(data_t): return out out.append(data_t[ind_t]) ind_t += 1 else: <IF_STMT> return out out.append(data_f[ind_f]) ind_f += 1 return out",if ind_f == len(data_f):,if ind_f == len(data_f):,0.8998240017601441,0.8627586293513119,True
4448,"def oo_contains_rule(source, apiGroups, resources, verbs): """"""Return true if the specified rule is contained within the provided source"""""" rules = source['rules'] if rules: for rule in rules: if set(rule['apiGroups']) == set(apiGroups): if set(rule['resources']) == set(resources): <IF_STMT> return True return False",if set(rule['verbs']) == set(verbs):,if set(rule['verbs']) == set(verbs):,0.9070795864671689,0.8627586293513119,True
4449,"def _maybe_commit_artifact(self, artifact_id): artifact_status = self._artifacts[artifact_id] if artifact_status['pending_count'] == 0 and artifact_status['commit_requested']: for callback in artifact_status['pre_commit_callbacks']: callback() <IF_STMT> self._api.commit_artifact(artifact_id) for callback in artifact_status['post_commit_callbacks']: callback()",if artifact_status['finalize']:,elif artifact_status['pending_count'] == 0 and artifact_status['commit_requested']:,0.8674883924410227,0.7232927766551176,False
4450,"def shuffler(iterator, pool_size=10 ** 5, refill_threshold=0.9): yields_between_refills = round(pool_size * (1 - refill_threshold)) pool = take_n(pool_size, iterator) while True: random.shuffle(pool) for i in range(yields_between_refills): yield pool.pop() next_batch = take_n(yields_between_refills, iterator) <IF_STMT> break pool.extend(next_batch) yield from pool",if not next_batch:,if next_batch is None:,0.870598487390631,0.8516228624291206,False
4451,"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] <IF_STMT> return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v if _get_mode: raise KeyError() raise BadRequestKeyError(key)","elif isinstance(key, slice):","elif isinstance(key, self.__class__):",0.8933750551386608,0.8749766281017177,False
4452,"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 <IF_STMT> self.files.append(file) else: self.find(file)",if self.match_function(file):,if self.match_function(file):,0.9284179530207362,0.9051034981560222,True
4453,"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: <IF_STMT> continue grade = cell.metadata['nbgrader']['grade'] solution = cell.metadata['nbgrader']['solution'] locked = cell.metadata['nbgrader']['locked'] if not grade and (not solution) and (not locked): continue grade_id = cell.metadata['nbgrader']['grade_id'] if grade_id in ids: raise ValidationError('Duplicate grade id: {}'.format(grade_id)) ids.add(grade_id)",if 'nbgrader' not in cell.metadata:,if not cell.metadata['nbgrader']:,0.8650252661772608,0.9022045190074797,False
4454,"def _skip_start(self): start, stop = (self.start, self.stop) for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue elif self._pos == start: return b'' else: chunk = chunk[start - self._pos:] <IF_STMT> chunk = chunk[:stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",if stop is not None and self._pos > stop:,if self._pos < stop:,0.7917108601157693,0.8879659171421962,False
4455,"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings['room']['name'] if 'room' in settings else None file_ = settings['file'] if 'file' in settings else None if 'event' in settings: if 'joined' in settings['event']: self._client.userlist.addUser(username, room, file_) <IF_STMT> self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",elif 'left' in settings['event']:,elif 'removed' in settings['event']:,0.9401403763370508,0.8879659171421962,False
4456,"def run_tests(): x = 5 with switch(x) as case: if case(0): print('zero') print('zero') <IF_STMT> print('one or two') elif case(3, 4): print('three or four') else: print('default') print('another')","elif case(1, 2):","elif case(1, 2):",0.8905431881326735,0.8266114125804572,True
4457,"def _populate(): for fname in glob.glob(os.path.join(os.path.dirname(__file__), 'data', '*.json')): with open(fname) as inf: data = json.load(inf) data = data[list(data.keys())[0]] data = data[list(data.keys())[0]] for item in data: <IF_STMT> LOGGER.warning('Repeated emoji {}'.format(item['key'])) else: TABLE[item['key']] = item['value']",if item['key'] in TABLE:,"if item['key'] in ['emoji', 'emoji']:",0.8628813279232069,0.8148691130388024,False
4458,"def slot_to_material(bobject: bpy.types.Object, slot: bpy.types.MaterialSlot): mat = slot.material if mat is not None: baked_mat = mat.name + '_' + bobject.name + '_baked' <IF_STMT> mat = bpy.data.materials[baked_mat] return mat",if baked_mat in bpy.data.materials:,if baked_mat in bpy.data.materials:,0.718331483653308,0.8105932471967202,True
4459,"def __keyPress(self, widget, event): if event.key == 'G' and event.modifiers & event.Modifiers.Control: if not all((hasattr(p, 'isGanged') for p in self.getPlugs())): return False <IF_STMT> self.__ungang() else: self.__gang() return True return False",if all((p.isGanged() for p in self.getPlugs())):,elif event.modifiers & event.Modifiers.Control:,0.8799960153810014,0.8228500218338367,False
4460,"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode('ascii') <IF_STMT> expected = expected.encode('ascii') resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False elif not rline.endswith(eline): return False return True","if isinstance(expected, str):","elif isinstance(expected, str):",0.8628973449731777,0.9134996171406936,False
4461,"def hosts_to_domains(self, hosts, exclusions=[]): domains = [] for host in hosts: elements = host.split('.') while len(elements) >= 2: if len(elements) == 2: domain = '.'.join(elements) else: domain = '.'.join(elements[1:]) <IF_STMT> domains.append(domain) del elements[0] return domains",if domain not in domains + exclusions:,if domain not in exclusions:,0.8619479115108292,0.8248765135255685,False
4462,"def hsconn_sender(self): while not self.stop_event.is_set(): try: request = self.send_queue.get(True, 6.0) <IF_STMT> self.socket.sendall(request) if self.send_queue is not None: self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",if self.socket is not None:,if request is not None:,0.8867458577357037,0.7506346798217074,False
4463,"def get_url_args(self, item): if self.url_args: <IF_STMT> url_args = self.url_args(item) else: url_args = dict(self.url_args) url_args['id'] = item.id return url_args else: return dict(operation=self.label, id=item.id)","if hasattr(self.url_args, '__call__'):",if callable(self.url_args):,0.8328988818559884,0.8318180062062374,False
4464,"def list_projects(self): projects = [] page = 1 while True: repos = self._client.get('/user/repos', {'sort': 'full_name', 'page': page, 'per_page': 100}) page += 1 for repo in repos: projects.append({'id': repo['full_name'], 'name': repo['full_name'], 'description': repo['description'], 'is_private': repo['private']}) <IF_STMT> break return projects",if len(repos) < 100:,if not repos:,0.8939563884907539,0.8815741981066073,False
4465,"def scripts(self): application_root = current_app.config.get('APPLICATION_ROOT') subdir = application_root != '/' scripts = [] for script in get_registered_scripts(): if script.startswith('http'): scripts.append(f'<script defer src=""{script}""></script>') <IF_STMT> scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup('\n'.join(scripts))",elif subdir:,elif subdir:,0.9214343667681355,0.8866029039778043,True
4466,"def print_map(node, l): if node.title not in l: l[node.title] = [] for n in node.children: <IF_STMT> w = {n.title: []} l[node.title].append(w) print_map(n, w) else: l[node.title].append(n.title)",if len(n.children) > 0:,"if isinstance(n, Map):",0.8760313237856718,0.8196189957582152,False
4467,"def _validate_distinct_on_different_types_and_field_orders(self, collection, query, expected_results, get_mock_result): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): if isinstance(results[i], dict): self.assertDictEqual(results[i], expected_results[i]) <IF_STMT> self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0","elif isinstance(results[i], list):","elif isinstance(results[i], list):",0.9255354814479843,0.8783650674919876,True
4468,"def run(self): for k, v in iteritems(self.objs): <IF_STMT> continue if v['_class'] == 'Question' or v['_class'] == 'Message' or v['_class'] == 'Announcement': v['admin'] = None return self.objs",if k.startswith('_'):,if k.startswith('_'):,0.8051327627433021,0.8590888738245122,True
4469,def qvec(self): qvec = np.array([]) if self.polrep == 'stokes': qvec = self._imdict['Q'] elif self.polrep == 'circ': <IF_STMT> qvec = np.real(0.5 * (self.lrvec + self.rlvec)) return qvec,if len(self.rlvec) != 0 and len(self.lrvec) != 0:,if self.lrvec != 0:,0.7375531118010001,0.7965020533851944,False
4470,"def display_value(self, key, w): if key == 'vdevices': nids = [n['deviceID'] for n in self.get_value('devices')] for device in self.app.devices.values(): <IF_STMT> b = Gtk.CheckButton(device.get_title(), False) b.set_tooltip_text(device['id']) self['vdevices'].pack_start(b, False, False, 0) b.set_active(device['id'] in nids) self['vdevices'].show_all() else: EditorDialog.display_value(self, key, w)",if device['id'] != self.app.daemon.get_my_id():,if device['id'] in nids:,0.8945211064354497,0.8555308664663046,False
4471,"def _set_xflux_setting(self, **kwargs): for key, value in kwargs.items(): if key in self._settings_map: <IF_STMT> self._set_xflux_screen_color(value) self._current_color = str(value) if self.state == self.states['PAUSED']: self.state = self.states['RUNNING'] else: self._xflux.sendline(self._settings_map[key] + str(value)) self._c()",if key == 'color':,if self._settings_map[key] == 'screen':,0.9105027865658906,0.8169276475307028,False
4472,"def apply_acceleration(self, veh_ids, acc): """"""See parent class."""""" if type(veh_ids) == str: veh_ids = [veh_ids] acc = [acc] for i, vid in enumerate(veh_ids): <IF_STMT> this_vel = self.get_speed(vid) next_vel = max([this_vel + acc[i] * self.sim_step, 0]) self.kernel_api.vehicle.slowDown(vid, next_vel, 0.001)",if acc[i] is not None and vid in self.get_ids():,if self.kernel_api.vehicle.is_active(vid):,0.9016153444072269,0.9024521756077707,False
4473,"def largest_factor_relatively_prime(a, b): """"""Return the largest factor of a relatively prime to b."""""" while 1: d = gcd(a, b) <IF_STMT> break b = d while 1: q, r = divmod(a, d) if r > 0: break a = q return a",if d <= 1:,if d < 0:,0.6155253449274125,0.8692960007731574,False
4474,"def check_status(self): try: du = psutil.disk_usage('/') <IF_STMT> raise ServiceWarning('{host} {percent}% disk usage exceeds {disk_usage}%'.format(host=host, percent=du.percent, disk_usage=DISK_USAGE_MAX)) except ValueError as e: self.add_error(ServiceReturnedUnexpectedResult('ValueError'), e)",if DISK_USAGE_MAX and du.percent >= DISK_USAGE_MAX:,if du.percent > DISK_USAGE_MAX:,0.5836700641051777,0.760856626273165,False
4475,"def build_reply(self, msg, text=None, private=False, threaded=False): response = self.build_message(text) if msg.is_group: <IF_STMT> response.frm = self.bot_identifier response.to = IRCPerson(str(msg.frm)) else: response.frm = IRCRoomOccupant(str(self.bot_identifier), msg.frm.room) response.to = msg.frm.room else: response.frm = self.bot_identifier response.to = msg.frm return response",if private:,if private:,0.6515987643871963,0.8966773400768917,True
4476,"def _dict_refs(obj, named): """"""Return key and value objects of a dict/proxy."""""" try: <IF_STMT> for k, v in _items(obj): s = str(k) yield _NamedRef('[K] ' + s, k) yield _NamedRef('[V] ' + s + ': ' + _repr(v), v) else: for k, v in _items(obj): yield k yield v except (KeyError, ReferenceError, TypeError) as x: warnings.warn(""Iterating '%s': %r"" % (_classof(obj), x))",if named:,if named:,0.9486466084533179,0.940591574039961,True
4477,"def fetch_images(): images = [] marker = None while True: batch = image_service.detail(context, filters=filters, marker=marker, sort_key='created_at', sort_dir='desc') <IF_STMT> break images += batch marker = batch[-1]['id'] return images",if not batch:,if len(batch) == 0:,0.6729663206649391,0.8038019482772603,False
4478,"def compress(self, data_list): warn_untested() if data_list: if data_list[1] in forms.fields.EMPTY_VALUES: error = self.error_messages['invalid_year'] raise forms.ValidationError(error) <IF_STMT> error = self.error_messages['invalid_month'] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) day = monthrange(year, month)[1] return date(year, month, day) return None",if data_list[0] in forms.fields.EMPTY_VALUES:,if data_list[0] in forms.fields.EMPTY_VALUES:,0.8922141719555041,0.8555308664663046,True
4479,"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): if key not in new: removed.append(key) <IF_STMT> removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff['removed'] = sorted(removed) if added: diff['added'] = sorted(added) return diff",elif old[key] != new[key]:,if value != value:,0.8999069005413127,0.8923575006167597,False
4480,"def add_filters(self, function): try: subscription = self.exists(function) <IF_STMT> response = self._sns.call('set_subscription_attributes', SubscriptionArn=subscription['SubscriptionArn'], AttributeName='FilterPolicy', AttributeValue=json.dumps(self.filters)) kappa.event_source.sns.LOG.debug(response) except Exception: kappa.event_source.sns.LOG.exception('Unable to add filters for SNS topic %s', self.arn)",if subscription:,if subscription:,0.6473645882533732,0.8590888738245122,True
4481,"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <IF_STMT> constant_init(m, 1) else: raise TypeError('pretrained must be a str or None')","elif isinstance(m, (_BatchNorm, nn.GroupNorm)):","elif isinstance(m, nn.BatchNorm2d):",0.9292080716007225,0.8783650674919876,False
4482,def test_is_native_login(self): for campaign in self.campaign_lists: native = campaigns.is_native_login(campaign) <IF_STMT> assert_true(native) else: assert_false(native) native = campaigns.is_proxy_login(self.invalid_campaign) assert_true(native is None),if campaign == 'prereg' or campaign == 'erpc':,if campaign.is_proxy_login():,0.6929248166740616,0.803154665668484,False
4483,"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: if isinstance(arg, list): arg = self._process_filter(arg, host_state) <IF_STMT> arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result","elif isinstance(arg, basestring):","elif isinstance(arg, str):",0.9444120556941878,0.9118021019905903,False
4484,"def find_go_files_mtime(app_files): files, mtime = ([], 0) for f, mt in app_files.items(): <IF_STMT> continue if APP_CONFIG.nobuild_files.match(f): continue files.append(f) mtime = max(mtime, mt) return (files, mtime)",if not f.endswith('.go'):,if not f.endswith('.go'):,0.7927857954298818,0.8196189957582152,True
4485,"def ExcludePath(self, path): """"""Check to see if this is a service url and matches inbound_services."""""" skip = False for reserved_path in self.reserved_paths.keys(): <IF_STMT> if not self.inbound_services or self.reserved_paths[reserved_path] not in self.inbound_services: return (True, self.reserved_paths[reserved_path]) return (False, None)",if path.startswith(reserved_path):,if path.startswith(reserved_path):,0.8682738336153031,0.9024521756077707,True
4486,"def param_cov(self) -> DataFrame: """"""Parameter covariance"""""" if self._param_cov is not None: param_cov = self._param_cov else: params = np.asarray(self.params) <IF_STMT> param_cov = self.model.compute_param_cov(params) else: param_cov = self.model.compute_param_cov(params, robust=False) return DataFrame(param_cov, columns=self._names, index=self._names)",if self.cov_type == 'robust':,if self.robust:,0.7757167730422613,0.8827916928185874,False
4487,"def test_calculate_all_attentions(module, atype): m = importlib.import_module(module) args = make_arg(atype=atype) <IF_STMT> batch = prepare_inputs('pytorch') else: raise NotImplementedError model = m.E2E(6, 5, args) with chainer.no_backprop_mode(): if 'pytorch' in module: att_ws = model.calculate_all_attentions(*batch)[0] else: raise NotImplementedError print(att_ws.shape)",if 'pytorch' in module:,if 'pytorch' in module:,0.649311606965272,0.8431339019329497,True
4488,"def __eq__(self, other): try: if self.type != other.type: return False <IF_STMT> return self.askAnswer == other.askAnswer elif self.type == 'SELECT': return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",if self.type == 'ASK':,elif self.type == 'SELECT':,0.9056387669347231,0.8474968231198384,False
4489,"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError('Process types can only contain [a-z]') <IF_STMT> raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value","if not re.match(MEMLIMIT_MATCH, str(v)):",if len(v) > MAX_MEMORY:,0.9172322362367541,0.8627586293513119,False
4490,"def get_connections(data_about): data = data_about.find('h3', text='Connections').findNext() connections = {} for row in data.find_all('tr'): key = row.find_all('td')[0].text value = row.find_all('td')[1] <IF_STMT> connections[key] = get_all_links(value) else: connections[key] = value.text return connections",if 'Teams' in key:,if key == 'links':,0.8064945695427996,0.8169276475307028,False
4491,"def _compute_map(self, first_byte, second_byte=None): if first_byte != 15: return 'XED_ILD_MAP0' else: if second_byte == None: return 'XED_ILD_MAP1' if second_byte == 56: return 'XED_ILD_MAP2' if second_byte == 58: return 'XED_ILD_MAP3' <IF_STMT> return 'XED_ILD_MAPAMD' die('Unhandled escape {} / map {} bytes'.format(first_byte, second_byte))",if second_byte == 15 and self.amd_enabled:,if second_byte == 57:,0.8736571038088277,0.8661072626070159,False
4492,"def compress(self, data_list): if data_list: page_id = data_list[1] <IF_STMT> if not self.required: return None raise forms.ValidationError(self.error_messages['invalid_page']) return Page.objects.get(pk=page_id) return None",if page_id in EMPTY_VALUES:,if page_id not in Page.objects:,0.6842367683091747,0.7049592608322395,False
4493,"def find_module(self, fullname, path=None): path = path or self.path_entry for _ext in ['js', 'pyj', 'py']: _filepath = os.path.join(self.path_entry, '%s.%s' % (fullname, _ext)) <IF_STMT> print('module found at %s:%s' % (_filepath, fullname)) return VFSModuleLoader(_filepath, fullname) print('module %s not found' % fullname) raise ImportError() return None",if _filepath in VFS:,if os.path.exists(_filepath):,0.8060629447093803,0.9164531641034833,False
4494,"def __decToBin(self, myDec): n = 0 binOfDec = '' while myDec > 2 ** n: n = n + 1 if (myDec < 2 ** n) & (myDec != 0): n = n - 1 while n >= 0: <IF_STMT> myDec = myDec - 2 ** n binOfDec = binOfDec + '1' else: binOfDec = binOfDec + '0' n = n - 1 return binOfDec",if myDec >= 2 ** n:,if myDec > 2 ** n:,0.9268991513133975,0.8900307578152789,False
4495,def __str__(self): try: <IF_STMT> NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value)) return NVMLError._errcode_to_string[self.value] except NVMLError_Uninitialized: return 'NVML Error with code %d' % self.value,if self.value not in NVMLError._errcode_to_string:,if self.value not in NVMLError._errcode_to_string:,0.8198271012645609,0.6907573115737006,True
4496,"def abspath(pathdir: str) -> str: if Path is not None and isinstance(pathdir, Path): return pathdir.abspath() else: pathdir = path.abspath(pathdir) <IF_STMT> try: pathdir = pathdir.decode(fs_encoding) except UnicodeDecodeError as exc: raise UnicodeDecodeError('multibyte filename not supported on this filesystem encoding (%r)' % fs_encoding) from exc return pathdir","if isinstance(pathdir, bytes):",if fs_encoding is not None:,0.7512476644423534,0.8591169759078797,False
4497,"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: <IF_STMT> if isfile(self.object): with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, 'read'): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs","if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'):","if hasattr(self.object, 'read'):",0.9291787602044008,0.8875087479151215,False
4498,"def _set_uid(self, val): if val is not None: if pwd is None: self.bus.log('pwd module not available; ignoring uid.', level=30) val = None <IF_STMT> val = pwd.getpwnam(val)[2] self._uid = val","elif isinstance(val, text_or_bytes):",if val.startswith('_'):,0.7866836936287414,0.8743414417652072,False
4499,"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): <IF_STMT> continue node_settings = node.get_addon(self.oauth_provider.short_name) if node_settings is None: continue if node_settings.external_account == external_account: yield node",if node is None:,if node is None:,0.830002105791448,0.7709002428237395,True
4500,"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError('py_obj must be a wandb.Image') else: if hasattr(py_obj, '_boxes') and py_obj._boxes: box_keys = list(py_obj._boxes.keys()) else: box_keys = [] <IF_STMT> mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)","if hasattr(py_obj, 'masks') and py_obj.masks:","if hasattr(py_obj, '_masks') and py_obj.masks:",0.6999009106927772,0.8338542560892604,False
4501,"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: <IF_STMT> byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",if self.bytenum == len(self.bytestream) - 1:,if byte == 0:,0.9348752705635789,0.8944264839442453,False
4502,"def destroy(self, wipe=False): if self.state == self.UP: image = self.image() <IF_STMT> return self.confirm_destroy(image, self.full_name, abort=False) else: self.warn(""tried to destroy {0} which didn't exist"".format(self.full_name)) return True",if image:,if image.exists():,0.7956074483955433,0.8531413606256201,False
4503,"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1).json() <IF_STMT> match = AGENT_VERSION_EXP.search(resp.get('Version')) if match is not None and len(match.groups()) == 1: meta['ecs_version'] = match.group(1) except Exception as e: self.log.debug('Error getting ECS version: %s' % str(e)) return meta",if 'Version' in resp:,if resp:,0.6781312359252722,0.9164531641034833,False
4504,"def _path_type(st, lst): parts = [] if st: <IF_STMT> parts.append('file') elif stat.S_ISDIR(st.st_mode): parts.append('dir') else: parts.append('other') if lst: if stat.S_ISLNK(lst.st_mode): parts.append('link') return ' '.join(parts)",if stat.S_ISREG(st.st_mode):,if stat.S_ISLNK(st.st_mode):,0.8734744109867272,0.839587623092576,False
4505,"def changed(self, action): if len(action.key) >= 1 and action.key[0].lower() == 'files': <IF_STMT> self.update_model(clear=False) else: self.update_model(clear=True)",if action.type == 'insert':,if action.key[1].lower() == 'files':,0.558021244817324,0.6540585844910979,False
4506,"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client('es') for r in resources: <IF_STMT> result = self.manager.retry(client.describe_elasticsearch_domain_config, DomainName=r['DomainName'], ignore_err_codes=('ResourceNotFoundException',)) if result: r[self.policy_attribute] = json.loads(result.get('DomainConfig').get('AccessPolicies').get('Options')) return super().process(resources)",if self.policy_attribute not in r:,if r['DomainName'] != 'elasticsearch':,0.8801529883569057,0.7801270245332924,False
4507,"def line_items(self): line_items = [] for line in self.lines_str: line = line.split('|') line = line[1:-1] items = [] for item in line: i = re.search('(\\S+([ \\t]+\\S+)*)+', item) <IF_STMT> items.append(i.group()) else: items.append(' ') line_items.append(items) return line_items",if i:,if i:,0.8910397924922036,0.8966773400768917,True
4508,"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode('utf-8') if type(v) == unicode: v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) <IF_STMT> self.success(res) else: self.success(res)",elif not args.content_only:,elif args.no_content:,0.9022175410194047,0.9076141716697395,False
4509,"def get_servers(self, detail=True, search_opts=None): rel_url = '/servers/detail' if detail else '/servers' if search_opts is not None: qparams = {} for opt, val in search_opts.iteritems(): qparams[opt] = val <IF_STMT> query_string = '?%s' % urllib.urlencode(qparams) rel_url += query_string return self.api_get(rel_url)['servers']",if qparams:,if qparams:,0.8135989106496895,0.9051034981560222,True
4510,"def run(self): while not self.__exit__: <IF_STMT> sleep(10) continue o = self.playlist[0] self.playlist.remove(o) obj = json.loads(o) if not 'args' in obj: obj['args'] = {'ua': '', 'header': '', 'title': '', 'referer': ''} obj['play'] = False self.handle = launch_player(obj['urls'], obj['ext'], **obj['args']) self.handle.wait()",if len(self.playlist) == 0:,if len(self.playlist) == 0:,0.9155532049893473,0.8627586293513119,True
4511,"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) <IF_STMT> run_logs = r.json()['data']['records'] result.extend([i['logs'][0]['stats']['id'] for i in run_logs]) last_date = r.json()['data']['lastTimestamp'] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f'pares keep ids data since {since_time}') time.sleep(1) if not last_date: break return result",if r.ok:,if r.status_code == 200:,0.7683393621556452,0.8806615362338783,False
4512,"def __saveWork(self, work, results): """"""Stores the resulting last log line to the cache with the proxy key"""""" del work try: <IF_STMT> __cached = self.__cache[results[0]] __cached[self.__TIME] = time.time() __cached[self.__LINE] = results[1] __cached[self.__LLU] = results[2] except KeyError as e: pass except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if results:,if results:,0.9070581908541869,0.9184043388013005,True
4513,"def read_notes(rec): found = [] for tag in range(500, 595): if tag in (505, 520): continue fields = rec.get_fields(str(tag)) <IF_STMT> continue for f in fields: x = f.get_lower_subfields() if x: found.append(' '.join(x).strip(' ')) if found: return '\n\n'.join(found)",if not fields:,if not fields:,0.7754331993773592,0.8783650674919876,True
4514,"def serialize_to(self, stream, alternate_script=None): stream.write(self.txo_ref.tx_ref.hash) stream.write_uint32(self.txo_ref.position) if alternate_script is not None: stream.write_string(alternate_script) el<IF_STMT> stream.write_string(self.coinbase) else: stream.write_string(self.script.source) stream.write_uint32(self.sequence)",if self.is_coinbase:,if self.coinbase is not None:,0.8281703003065719,0.6577160909911662,False
4515,"def func_named(self, arg): result = None target = 'do_' + arg if target in dir(self): result = target el<IF_STMT> funcs = [fname for fname in self.keywords if fname.startswith(arg)] if len(funcs) == 1: result = 'do_' + funcs[0] return result",if self.abbrev:,if result is None:,0.6675577349068484,0.8627586293513119,False
4516,"def static_login(self, token, *, bot): self.__session = aiohttp.ClientSession(connector=self.connector, ws_response_class=DiscordClientWebSocketResponse) old_token, old_bot = (self.token, self.bot_token) self._token(token, bot=bot) try: data = await self.request(Route('GET', '/users/@me')) except HTTPException as exc: self._token(old_token, bot=old_bot) <IF_STMT> raise LoginFailure('Improper token has been passed.') from exc raise return data",if exc.response.status == 401:,if exc.response.status != 401:,0.7826123355900019,0.8661072626070159,False
4517,"def render_buttons(self): for x, button in enumerate(self.button_list): gcolor = Gdk.color_parse(self.color_list[x]) <IF_STMT> fgcolor = Gdk.color_parse('#FFFFFF') else: fgcolor = Gdk.color_parse('#000000') button.set_label(self.color_list[x]) button.set_sensitive(True) button.modify_bg(Gtk.StateType.NORMAL, gcolor) button.modify_fg(Gtk.StateType.NORMAL, fgcolor)","if util.get_hls_val(self.color_list[x], 'light') < 99:",if x == 0:,0.7842764669359018,0.7801270245332924,False
4518,"def _set_text(self, data): lines = [] for key, value in data.items(): lines.append('') txt = yaml.dump({key: value}, default_flow_style=False) title = self.titles.get(key) <IF_STMT> lines.append('# %s' % title) lines.append(txt.rstrip()) txt = '\n'.join(lines) + '\n' txt = txt.lstrip() self.edit.setPlainText(txt)",if title:,if title:,0.7881753346096554,0.8966773400768917,True
4519,"def build_path(self): for variable in re_path_template.findall(self.path): name = variable.strip('{}') <IF_STMT> value = self.api.auth.get_username() else: try: value = quote(self.session.params[name]) except KeyError: raise TweepError('No parameter value found for path variable: %s' % name) del self.session.params[name] self.path = self.path.replace(variable, value)",if name == 'user' and 'user' not in self.session.params and self.api.auth:,if name == 'username':,0.7569648001291434,0.8555308664663046,False
4520,"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): <IF_STMT> prop_vals = entity[prop_name] if isinstance(prop_vals, list): num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes",if not prop_name in entity.unindexed_properties():,if entity[prop_name].is_built_in:,0.8861434463627029,0.8827916928185874,False
4521,"def create_connection(self, address, protocol_factory=None, **kw): """"""Helper method for creating a connection to an ``address``."""""" protocol_factory = protocol_factory or self.create_protocol if isinstance(address, tuple): host, port = address <IF_STMT> self.logger.debug('Create connection %s:%s', host, port) _, protocol = await self._loop.create_connection(protocol_factory, host, port, **kw) await protocol.event('connection_made') else: raise NotImplementedError('Could not connect to %s' % str(address)) return protocol",if self.debug:,"if self._loop.is_connected(host, port):",0.6677857891220044,0.9151329413834155,False
4522,def _increment_bracket_num(self): self._current_bracket -= 1 if self._current_bracket < 0: self._current_bracket = self._get_num_brackets() - 1 self._current_iteration += 1 <IF_STMT> self._current_bracket = 0,if self._current_iteration > self.hyperband_iterations:,if self._current_iteration >= self._num_brackets():,0.6037892257391346,0.7498810286408993,False
4523,"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node['deps']: <IF_STMT> return [curr_node['address']] for dep in curr_node['deps']: path = self.get_cycle_path(self.get_by_address(dep), goal_node_index) if len(path) > 0: path.insert(0, curr_node['address']) return path return []",if dep == goal_node_index:,if dep == goal_node_index:,0.8870190493364456,0.8169276475307028,True
4524,"def as_dict(path='', version='latest', section='meta-data'): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith('/'): records = as_dict(path + item, version, section) <IF_STMT> result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + '/', version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",if records:,if records:,0.9584942086748811,0.9443716053164669,True
4525,"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, 'r') as f1: with open(output_filename, 'w') as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] if line != ' ' and line != '': <IF_STMT> line = line[1:] f2.writelines(line + '\n')",if line[0] == ' ':,if line[0] == ' ' and line[1] == ' ' and line[1] == ' ' and (not line[0]):,0.871728734826272,0.6537969044820617,False
4526,"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): <IF_STMT> index = i break if index is not None: del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",if subscriber_web_sock == web_sock:,if subscriber_web_sock == web_sock:,0.7801525260518228,0.828399516355805,True
4527,"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] if encoding is not None: s = s.decode(encoding) <IF_STMT> s = '%s %s' % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if withyear:,if withyear:,0.78032883116143,0.8935248372106969,True
4528,"def generate_sitemaps(filename): rows = (line.strip().split('\t') for line in open(filename)) for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]): things = [] _chunk = list(chunk) for segment in _chunk: sortkey = segment.pop(0) last_modified = segment.pop(-1) path = ''.join(segment) things.append(web.storage(path=path, last_modified=last_modified)) <IF_STMT> write('sitemaps/sitemap_%s.xml.gz' % sortkey, sitemap(things))",if things:,if sortkey:,0.8366773152086237,0.9164531641034833,False
4529,"def use_index(self, term: Union[str, Index], *terms: Union[str, Index]) -> 'QueryBuilder': for t in (term, *terms): <IF_STMT> self._use_indexes.append(t) elif isinstance(t, str): self._use_indexes.append(Index(t))","if isinstance(t, Index):","if isinstance(t, Index):",0.8651629788394122,0.7848518349390632,True
4530,"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) <IF_STMT> return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",if result == self.source:,if result is None:,0.8389763871505466,0.8832000938217648,False
4531,"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form('List', None): value = [extract_pyreal(item) for item in value.leaves] <IF_STMT> return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value",if any((item is None for item in value)):,if value is None or isinf(value) or isnan(value):,0.9149694191295636,0.8368865274141842,False
4532,"def _reemit_nested_event(self, event: Event): source_index = self.index(event.source) for attr in ('index', 'new_index'): <IF_STMT> src_index = ensure_tuple_index(event.index) setattr(event, attr, (source_index,) + src_index) if not hasattr(event, 'index'): setattr(event, 'index', source_index) getattr(self.events, event.type, self.events)(event)","if hasattr(event, attr):",if attr in event.attributes:,0.9088578593613483,0.828399516355805,False
4533,"def check(self): """"""Perform required checks to conclude if it's safe to operate"""""" if self.interpreter.manual is None: <IF_STMT> self.error = self.process.error self.tip = self.process.tip return False start = time.time() while not self._status(): if time.time() - start >= 2: self.error = ""can't connect to the minserver on {}:{}"".format(self.interpreter.host, self.interpreter.port) self.tip = 'check your vagrant machine is running' return False time.sleep(0.1) return True",if not self.process.healthy:,if self.process.error:,0.9515487988744653,0.940591574039961,False
4534,"def apply(self): new_block = self.block.copy() new_block.clear() for inst in self.block.body: <IF_STMT> const_assign = self._assign_const(inst) new_block.append(const_assign) inst = self._assign_getitem(inst, index=const_assign.target) new_block.append(inst) return new_block","if isinstance(inst, Assign) and inst.value in self.getattrs:","if isinstance(inst, ast.Const):",0.8347986646324893,0.7947545184555568,False
4535,"def _get_orientation(self): if self.state: rotation = [0] * 9 inclination = [0] * 9 gravity = [] geomagnetic = [] gravity = self.listener_a.values geomagnetic = self.listener_m.values <IF_STMT> ff_state = SensorManager.getRotationMatrix(rotation, inclination, gravity, geomagnetic) if ff_state: values = [0, 0, 0] values = SensorManager.getOrientation(rotation, values) return values",if gravity[0] is not None and geomagnetic[0] is not None:,if self.state == 'rotation':,0.6975724799847046,0.8832000938217648,False
4536,def getFirstSubGraph(graph): if len(graph) == 0: return None subg = {} todo = [graph.keys()[0]] while len(todo) > 0: <IF_STMT> subg[todo[0]] = graph[todo[0]] todo.extend(graph[todo[0]]) del graph[todo[0]] del todo[0] return subg,if todo[0] in graph.keys():,if len(todo) == 1:,0.721682865353502,0.8169276475307028,False
4537,"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if 'Last-Modified' not in rv.headers: try: result = date if callable(result): result = result(rv) <IF_STMT> from werkzeug.http import http_date result = http_date(result) if result: rv.headers['Last-Modified'] = result except Exception: logging.getLogger(__name__).exception('Error while calculating the lastmodified value for response {!r}'.format(rv)) return rv","if not isinstance(result, basestring):","elif isinstance(result, datetime):",0.651328968496261,0.9042878500265974,False
4538,"def set_invoice_details(self, row): invoice_details = self.invoice_details.get(row.voucher_no, {}) if row.due_date: invoice_details.pop('due_date', None) row.update(invoice_details) if row.voucher_type == 'Sales Invoice': <IF_STMT> self.set_delivery_notes(row) if self.filters.show_sales_person and row.sales_team: row.sales_person = ', '.join(row.sales_team) del row['sales_team']",if self.filters.show_delivery_notes:,if self.filters.show_delivery_notes:,0.9210426758024827,0.8743414417652072,True
4539,"def process(output): modules = {} for line in output: name, size, instances, depends, state, _ = line.split(' ', 5) instances = int(instances) module = {'size': size, 'instances': instances, 'state': state} <IF_STMT> module['depends'] = [value for value in depends.split(',') if value] modules[name] = module return modules",if depends != '-':,if depends:,0.8007320733960104,0.9202663016973823,False
4540,"def _get_host_from_zc_service_info(service_info: zeroconf.ServiceInfo): """"""Get hostname or IP + port from zeroconf service_info."""""" host = None port = None if service_info and service_info.port and (service_info.server or len(service_info.addresses) > 0): <IF_STMT> host = socket.inet_ntoa(service_info.addresses[0]) else: host = service_info.server.lower() port = service_info.port return (host, port)",if len(service_info.addresses) > 0:,if service_info.server == 'nt':,0.8034744006089101,0.8723360571509826,False
4541,"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <IF_STMT> module.weight.data[module.padding_idx].zero_()",if module.padding_idx is not None:,if module.padding_idx is not None:,0.9003972448471629,0.7536026368740878,True
4542,"def visitFromImport(self, import_stmt, import_info): new_pairs = [] if not import_info.is_star_import(): for name, alias in import_info.names_and_aliases: try: pyname = self.pymodule[alias or name] <IF_STMT> continue except exceptions.AttributeNotFoundError: pass new_pairs.append((name, alias)) return importinfo.FromImport(import_info.module_name, import_info.level, new_pairs)","if occurrences.same_pyname(self.pyname, pyname):",if pyname is None:,0.6383490439889626,0.8336104423443033,False
4543,"def _apply_patches(self): try: s = Subprocess(log=self.logfile, cwd=self.build_dir, verbose=self.options.verbose) for patch in self.patches: <IF_STMT> for ed, source in patch.items(): s.shell('ed - %s < %s' % (source, ed)) else: s.shell('patch -p0 < %s' % patch) except: logger.error('Failed to patch `%s`.\n%s' % (self.build_dir, sys.exc_info()[1])) sys.exit(1)",if type(patch) is dict:,"if isinstance(patch, dict):",0.9285168452635313,0.8928756684056034,False
4544,"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = ['..'] self.dir = dir self.file = '' mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == '.' or i == '..': continue path = os.path.join(dir, i) <IF_STMT> dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",if os.path.isdir(path):,if path.startswith(parent):,0.7987179725990554,0.9443716053164669,False
4545,"def remove_invalid_dirs(paths, bp_dir, module_name): ret = [] for path in paths: <IF_STMT> ret.append(path) else: logging.warning('Dir ""%s"" of module ""%s"" does not exist', path, module_name) return ret","if os.path.isdir(os.path.join(bp_dir, path)):",if os.path.exists(path + bp_dir):,0.8460614038191815,0.7965020533851944,False
4546,def update_sockets(self): inputs = self.inputs inputs_n = 'ABabcd' penta_sockets = pentagon_dict[self.grid_type].input_sockets for socket in inputs_n: if socket in penta_sockets: <IF_STMT> inputs[socket].hide_safe = False else: inputs[socket].hide_safe = True,if inputs[socket].hide_safe:,if inputs[socket].hide_safe:,0.8679362861319792,0.8645707301556367,True
4547,"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P) begin, nexti = (0, 0) for i, char in enumerate(sentence): pos = pos_list[i] if pos == 'B': begin = i elif pos == 'E': yield sentence[begin:i + 1] nexti = i + 1 <IF_STMT> yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",elif pos == 'S':,elif pos == 'C':,0.9505589788637958,0.9084940438173679,False
4548,"def validate(self): if self.data.get('encrypted', True): key = self.data.get('target_key') <IF_STMT> raise PolicyValidationError('Encrypted snapshot copy requires kms key on %s' % (self.manager.data,)) return self",if not key:,if key is None:,0.6979581058125743,0.760856626273165,False
4549,"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None <IF_STMT> filename = '{0}{1}'.format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",if not filename.startswith(os.sep):,if filename:,0.8036484896424909,0.9253742688467129,False
4550,"def validate_name_and_description(body, check_length=True): for attribute in ['name', 'description', 'display_name', 'display_description']: value = body.get(attribute) if value is not None: if isinstance(value, six.string_types): body[attribute] = value.strip() <IF_STMT> try: utils.check_string_length(body[attribute], attribute, min_length=0, max_length=255) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",if check_length:,elif check_length:,0.9322151573392529,0.8996480074924822,False
4551,"def pick(items, sel): for x, s in zip(items, sel): if match(s): yield x <IF_STMT> yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",elif not x.is_atom() and (not s.is_atom()):,"elif match(x.head, s.head):",0.5940471918598784,0.761827408333416,False
4552,"def wait_or_kill(self): """"""Wait for the program to terminate, or kill it after 5s."""""" if self.instance.poll() is None: logger.info('Interrupting %s and waiting...', self.coord) self.instance.send_signal(signal.SIGINT) t = monotonic_time() while monotonic_time() - t < 5: <IF_STMT> logger.info('Terminated %s.', self.coord) break time.sleep(0.1) else: self.kill()",if self.instance.poll() is not None:,if self.instance.poll() is None:,0.892462218695699,0.8661072626070159,False
4553,"def sort_collection(self, models, many): ordering = self.ordering if not many or not ordering: return models for key in reversed(ordering): reverse = key[0] == '-' <IF_STMT> key = key[1:] models = sorted(models, key=partial(deep_getattr, key=key), reverse=reverse) return models",if reverse:,if reverse:,0.9155054596148052,0.8996480074924822,True
4554,"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): <IF_STMT> palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: if self.PALETTE is None: palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",if new_id != -1:,if old_id in class_names:,0.8458204845113021,0.8806615362338783,False
4555,"def _find_tcl_dir(): lib_dirs = [os.path.dirname(_x) for _x in sys.path if _x.lower().endswith('lib')] for lib_dir in lib_dirs: base_dir = os.path.join(lib_dir, TclLibrary.FOLDER) <IF_STMT> for root, _, files in os.walk(base_dir): if TclLibrary.INIT_TCL in files: return root",if os.path.exists(base_dir):,if os.path.isdir(base_dir):,0.8965115566671444,0.8866029039778043,False
4556,"def __next__(self): """"""Special paging functionality"""""" if self.iter is None: self.iter = iter(self.objs) try: return next(self.iter) except StopIteration: self.iter = None self.objs = [] <IF_STMT> self.page += 1 self._connection.get_response(self.action, self.params, self.page, self) return next(self) else: raise",if int(self.page) < int(self.total_pages):,if self.page < self.limit:,0.7143823937342726,0.8474968231198384,False
4557,"def parse(cls, api, json): lst = List(api) setattr(lst, '_json', json) for k, v in json.items(): if k == 'user': setattr(lst, k, User.parse(api, v)) <IF_STMT> setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",elif k == 'created_at':,elif k == 'date':,0.9018756871906741,0.8385130047130208,False
4558,"def real_type(self): real_type = self.type if self.flag_indicator: real_type = '#' if self.is_vector: <IF_STMT> real_type = 'Vector<{}>'.format(real_type) else: real_type = 'vector<{}>'.format(real_type) if self.is_generic: real_type = '!{}'.format(real_type) if self.is_flag: real_type = 'flags.{}?{}'.format(self.flag_index, real_type) return real_type",if self.use_vector_id:,if self.is_vector_type:,0.6429118739276294,0.8901732118131125,False
4559,"def check_fs(path): with open(path, 'rb') as f: code = python_bytes_to_unicode(f.read(), errors='replace') <IF_STMT> module = _load_module(evaluator, path, code) module_name = sys_path.dotted_path_in_sys_path(evaluator.project.sys_path, path) if module_name is not None: add_module(evaluator, module_name, module) return module",if name in code:,if code is not None:,0.7494421765524276,0.8036431532733102,False
4560,"def infoCalendar(users): calendarId = normalizeCalendarId(sys.argv[5], checkPrimary=True) i = 0 count = len(users) for user in users: i += 1 user, cal = buildCalendarGAPIObject(user) if not cal: continue result = gapi.call(cal.calendarList(), 'get', soft_errors=True, calendarId=calendarId) <IF_STMT> print(f'User: {user}, Calendar:{display.current_count(i, count)}') _showCalendar(result, 1, 1)",if result:,if result:,0.9411676213365066,0.9122561819614461,True
4561,"def set_hidestate_input_sockets_to_cope_with_switchnum(self): tndict = get_indices_that_should_be_visible(self.node_state) for key, value in tndict.items(): socket = self.inputs[key] desired_hide_state = not value <IF_STMT> socket.hide_safe = desired_hide_state",if not socket.hide == desired_hide_state:,if desired_hide_state:,0.7913957215574443,0.8232490471721702,False
4562,"def get_class_name(item): class_name, module_name = (None, None) for parent in reversed(item.listchain()): if isinstance(parent, pytest.Class): class_name = parent.name <IF_STMT> module_name = parent.module.__name__ break if class_name and '.tasks.' not in module_name: return '{}.{}'.format(module_name, class_name) else: return module_name","elif isinstance(parent, pytest.Module):","elif isinstance(parent, pytest.Module):",0.7911929953166521,0.8713933650206428,True
4563,"def run(self): versions = versioneer.get_versions() tempdir = tempfile.mkdtemp() generated = os.path.join(tempdir, 'rundemo') with open(generated, 'wb') as f: for line in open('src/rundemo-template', 'rb'): <IF_STMT> f.write(('versions = %r\n' % (versions,)).encode('ascii')) else: f.write(line) self.scripts = [generated] rc = build_scripts.run(self) os.unlink(generated) os.rmdir(tempdir) return rc",if line.strip().decode('ascii') == '#versions':,if line.startswith('version='):,0.9373742396949335,0.9099951253570094,False
4564,"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, 'user', None) result = {'ip_address': request.META['REMOTE_ADDR']} <IF_STMT> result.update({'email': user.email, 'id': user.id}) if user.name: result['name'] = user.name else: result = {} return mark_safe(json.dumps(result))",if user and user.is_authenticated():,if user:,0.735107385183118,0.8827916928185874,False
4565,"def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]: """"""Convert tokens to spans."""""" tokens = iter(line_tokenize()) line_no = 0 _line_start = line_start - 1 while line_no < _line_start: _token_type, token = next(tokens) yield (token, None) <IF_STMT> line_no += 1 for token_type, token in tokens: yield (token, _get_theme_style(token_type)) if token.endswith('\n'): line_no += 1 if line_no >= line_end: break",if token.endswith('\n'):,if _token_type == 'theme':,0.7528951534192587,0.8983343737277126,False
4566,"def encode(self, encodeFun, value, defMode, maxChunkSize): substrate, isConstructed = self.encodeValue(encodeFun, value, defMode, maxChunkSize) tagSet = value.getTagSet() if tagSet: <IF_STMT> defMode = 1 return self.encodeTag(tagSet[-1], isConstructed) + self.encodeLength(len(substrate), defMode) + substrate + self._encodeEndOfOctets(encodeFun, defMode) else: return substrate",if not isConstructed:,if defMode == 0:,0.8644935110281844,0.8516228624291206,False
4567,def _run(self): while True: request = self._requests.get() <IF_STMT> self.shutdown() break self.process(request) self._requests.task_done(),if request is None:,if request is None:,0.591573380850948,0.570282226440554,True
4568,"def _decode_payload(self, payload): if payload['enc'] == 'aes': try: payload['load'] = self.crypticle.loads(payload['load']) except salt.crypt.AuthenticationError: <IF_STMT> raise payload['load'] = self.crypticle.loads(payload['load']) return payload",if not self._update_aes():,if not self.crypticle.is_valid():,0.6320978951805695,0.7739321540474097,False
4569,"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) if self.any_match: <IF_STMT> return not self.inverse elif not result: return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if result:,if result:,0.931349634562106,0.9076141716697395,True
4570,"def setup_parameter_node(self, param_node): if param_node.bl_idname == 'SvNumberNode': if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print('V', value) if isinstance(value, int): param_node.selected_mode = 'int' param_node.int_ = value <IF_STMT> param_node.selected_mode = 'float' param_node.float_ = value","elif isinstance(value, float):","elif isinstance(value, float):",0.6990115311199114,0.8592899528284996,True
4571,"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue <IF_STMT> for client in clients: if module.is_compatible_with(client): yield module break else: yield module",if clients is not None:,if clients:,0.9283279931442858,0.9202663016973823,False
4572,"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: if pricing_rule.condition: try: <IF_STMT> filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules","if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):",if pricing_rule.condition.search(doc):,0.6528199769680386,0.8645707301556367,False
4573,"def build_query_string(kv_data, ignore_none=True): query_string = '' for k, v in kv_data.iteritems(): <IF_STMT> continue if query_string != '': query_string += '&' else: query_string = '?' query_string += k + '=' + str(v) return query_string",if ignore_none is True and kv_data[k] is None:,if ignore_none and k in ignore_none:,0.8052037805700292,0.7925226565645306,False
4574,"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): if k in striped_keys: <IF_STMT> sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret","if isinstance(v, NestedSpace):","if isinstance(v, ConfigSpace):",0.835651695956838,0.9062841320510342,False
4575,"def task_failed(self, task_id, hostname, reason): logger.debug('task %d failed with message %s', task_id, str(reason)) if hostname in self.host_dict: host_status = self.host_dict[hostname] host_status.task_failed(task_id) <IF_STMT> self.task_host_failed_dict[task_id] = set() self.task_host_failed_dict[task_id].add(hostname)",if task_id not in self.task_host_failed_dict:,if task_id not in self.task_host_failed_dict:,0.8419694816882645,0.7685107079449489,True
4576,"def match(path): for pat, _type, _property, default_title in patterns: m = web.re_compile('^' + pat).match(path) <IF_STMT> prefix = m.group() extra = web.lstrips(path, prefix) tokens = extra.split('/', 2) middle = web.listget(tokens, 1, '') suffix = web.listget(tokens, 2, '') if suffix: suffix = '/' + suffix return (_type, _property, default_title, prefix, middle, suffix) return (None, None, None, None, None, None)",if m:,if m:,0.7109199434526996,0.9374009563674955,True
4577,"def _get_cached_resources(self, ids): key = self.get_cache_key(None) if self._cache.load(): resources = self._cache.get(key) <IF_STMT> self.log.debug('Using cached results for get_resources') m = self.get_model() id_set = set(ids) return [r for r in resources if r[m.id] in id_set] return None",if resources is not None:,if resources:,0.6914498683446376,0.8966773400768917,False
4578,"def has_api_behaviour(self, protocol): config = get_config() try: r = self.session.get(f'{protocol}://{self.event.host}:{self.event.port}', timeout=config.network_timeout) <IF_STMT> return True except requests.exceptions.SSLError: logger.debug(f'{[protocol]} protocol not accepted on {self.event.host}:{self.event.port}') except Exception: logger.debug(f'Failed probing {self.event.host}:{self.event.port}', exc_info=True)","if 'k8s' in r.text or ('""code""' in r.text and r.status_code != 200):",if r.status_code == 200 and r.text == self.event.text:,0.594664433764893,0.7136906481243283,False
4579,"def get_file_type(self, context, parent_context=None): file_type = context.get(self.file_type_name, None) if file_type == '': <IF_STMT> file_type = parent_context.get(self.file_type_name, self.default_file_type) else: file_type = self.default_file_type return file_type",if parent_context:,if parent_context:,0.6204633283334943,0.839587623092576,True
4580,"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) <IF_STMT> remove = True if remove and (not add): selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",if boxedChunks.issubset(selectedChunks):,if remove:,0.9320698315531133,0.8996480074924822,False
4581,"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] <IF_STMT> output.append([char]) start_new_word = True else: if start_new_word: output.append([]) start_new_word = False output[-1].append(char) i += 1 return [''.join(x) for x in output]",if _is_punctuation(char):,if char in self.punctuation:,0.692700705201171,0.914208565914368,False
4582,"def _save_images(notebook): if os.getenv('NB_NO_IMAGES') == '1': return logged = False for filename, img_bytes in _iter_notebook_images(notebook): <IF_STMT> log.info('Saving images') logged = True with open(filename, 'wb') as f: f.write(img_bytes)",if not logged:,if not logged:,0.8228847719368461,0.833078701050083,True
4583,"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) <IF_STMT> minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",if minDist == None or guideDist < minDist:,if guideDist < minDist:,0.9486396897231448,0.8964173245779284,False
4584,"def _terminal_messenger(tp='write', msg='', out=sys.stdout): try: <IF_STMT> out.write(msg) elif tp == 'flush': out.flush() elif tp == 'write_flush': out.write(msg) out.flush() elif tp == 'print': print(msg, file=out) else: raise ValueError('Unsupported type: ' + tp) except IOError as e: logger.critical('{}: {}'.format(type(e).__name__, ucd(e))) pass",if tp == 'write':,if tp == 'write':,0.9196759718239785,0.8627586293513119,True
4585,"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == 'SaltLoggingClass': for base in bases: <IF_STMT> include_trace = False if hasattr(base, 'garbage'): include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr(base, 'trace'):","if hasattr(base, 'trace'):",0.941940572401623,0.9062841320510342,True
4586,"def generatePidEncryptionTable(): table = [] for counter1 in range(0, 256): value = counter1 for counter2 in range(0, 8): <IF_STMT> value = value >> 1 else: value = value >> 1 value = value ^ 3988292384 table.append(value) return table",if value & 1 == 0:,if counter1 == counter2:,0.8995612051423892,0.8592377270804451,False
4587,def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith('tests/params'): <IF_STMT> item.add_marker(pytest.mark.stage('unit')) if 'init' not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123)),if 'stage' not in item.keywords:,if 'stage' not in item.keywords:,0.8399071176992214,0.6383240325919926,True
4588,"def python_value(self, value): if value: if isinstance(value, basestring): pp = lambda x: x.time() return format_date_time(value, self.formats, pp) <IF_STMT> return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value","elif isinstance(value, datetime.datetime):","elif isinstance(value, datetime.datetime):",0.9133258528931432,0.8675979125638379,True
4589,"def list_interesting_hosts(self): hosts = [] targets = self.target['other'] for target in targets: <IF_STMT> hosts.append({'ip': target.ip, 'description': target.domain + ' / ' + target.name}) return hosts",if self.is_interesting(target) and target.status and (target.status != 400):,if target.name:,0.7720916943008873,0.8531413606256201,False
4590,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue <IF_STMT> self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 24:,if tt == 16:,0.9001195620339053,0.8555308664663046,False
4591,"def _wait_for_finish(self) -> PollExitResponse: while True: if self._backend: poll_exit_resp = self._backend.interface.communicate_poll_exit() logger.info('got exit ret: %s', poll_exit_resp) if poll_exit_resp: done = poll_exit_resp.done pusher_stats = poll_exit_resp.pusher_stats <IF_STMT> self._on_finish_progress(pusher_stats, done) if done: return poll_exit_resp time.sleep(2)",if pusher_stats:,if pusher_stats:,0.8949006632553478,0.8866029039778043,True
4592,def listing_items(method): marker = None once = True items = [] while once or items: for i in items: yield i if once or marker: <IF_STMT> items = method(parms={'marker': marker}) else: items = method() if len(items) == 10000: marker = items[-1] else: marker = None once = False else: items = [],if marker:,if marker:,0.9395650409943481,0.9312457603037672,True
4593,"def call(monad, *args): for arg, name in izip(args, ('hour', 'minute', 'second', 'microsecond')): if not isinstance(arg, NumericMixin) or arg.type is not int: throw(TypeError, ""'%s' argument of time(...) function must be of 'int' type. Got: %r"" % (name, type2str(arg.type))) <IF_STMT> throw(NotImplementedError) return ConstMonad.new(time(*tuple((arg.value for arg in args))))","if not isinstance(arg, ConstMonad):","if not isinstance(monad, ConstMonad):",0.8426622493862661,0.8806615362338783,False
4594,"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) <IF_STMT> sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",if sign * ki < -slop:,if sign < -slop:,0.9354368021851509,0.8964173245779284,False
4595,"def walk_links(self): link_info_list = [] for item in self.content: <IF_STMT> link_info = LinkInfo(link=item, name=item.name, sections=()) link_info_list.append(link_info) else: link_info_list.extend(item.walk_links()) return link_info_list","if isinstance(item, Link):","if isinstance(item, Link):",0.868501941404809,0.7739321540474097,True
4596,"def get_subkeys(self, key): parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path):] if sub.startswith('\\'): sub = sub[1:] end_slash = sub.find('\\') <IF_STMT> sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",if end_slash >= 0:,if end_slash > -1:,0.9387403589646941,0.8661072626070159,False
4597,"def load_dict(dict_path, reverse=False): word_dict = {} with open(dict_path, 'rb') as fdict: for idx, line in enumerate(fdict): line = cpt.to_text(line) <IF_STMT> word_dict[idx] = line.strip('\n') else: word_dict[line.strip('\n')] = idx return word_dict",if reverse:,if reverse:,0.729376009135816,0.8743414417652072,True
4598,"def test_network(coords, feats, model, batch_sizes, forward_only=True): for batch_size in batch_sizes: bcoords = batched_coordinates([coords for i in range(batch_size)]) bfeats = torch.cat([feats for i in range(batch_size)], 0) <IF_STMT> with torch.no_grad(): time, length = forward(bcoords, bfeats, model) else: time, length = train(bcoords, bfeats, model) print(f'{net.__name__}\t{voxel_size}\t{batch_size}\t{length}\t{time}') torch.cuda.empty_cache()",if forward_only:,if forward_only:,0.7977851690756239,0.9164531641034833,True
4599,"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: <IF_STMT> self.utexc = np.zeros(ntexco, dtype=bool) if self.utexc is not True: self.utexc[indices] = True",if self.utexc is False:,if self.utexc is None:,0.7228552103805255,0.8385130047130208,False
4600,"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) <IF_STMT> has_module = has_module_directory or exists(module_directory) else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile if has_module: break return has_module",if not version:,if version == '0.0.0':,0.8641696385494254,0.8723360571509826,False
4601,"def get_editops(self): if not self._editops: <IF_STMT> self._editops = editops(self._opcodes, self._str1, self._str2) else: self._editops = editops(self._str1, self._str2) return self._editops",if self._opcodes:,if self._opcodes:,0.8232477648634323,0.7912619863720214,True
4602,"def to_representation(self, data): value = super(CredentialTypeSerializer, self).to_representation(data) if value.get('managed_by_tower'): value['name'] = _(value['name']) for field in value.get('inputs', {}).get('fields', []): field['label'] = _(field['label']) <IF_STMT> field['help_text'] = _(field['help_text']) return value",if 'help_text' in field:,if field['help_text']:,0.8538775047585009,0.8645707301556367,False
4603,"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): <IF_STMT> v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d","if isinstance(v[i], dict):","if isinstance(v[i], dict):",0.8723493877587354,0.8592899528284996,True
4604,"def messageSourceStamps(self, source_stamps): text = '' for ss in source_stamps: source = '' if ss['branch']: source += '[branch %s] ' % ss['branch'] if ss['revision']: source += str(ss['revision']) else: source += 'HEAD' <IF_STMT> source += ' (plus patch)' discriminator = '' if ss['codebase']: discriminator = "" '%s'"" % ss['codebase'] text += 'Build Source Stamp%s: %s\n' % (discriminator, source) return text",if ss['patch'] is not None:,if ss['patch_type']:,0.7454821721004943,0.9395648330058336,False
4605,"def fit_one(self, x): for i, xi in x.items(): <IF_STMT> self.median[i].update(xi) if self.with_scaling: self.iqr[i].update(xi) return self",if self.with_centering:,if self.with_scaling:,0.8114750883948225,0.7447819789879647,False
4606,"def start_response(self, status, headers, exc_info=None): if exc_info: try: if self.started: six.reraise(exc_info[0], exc_info[1], exc_info[2]) finally: exc_info = None self.request.status = int(status[:3]) for key, val in headers: <IF_STMT> self.request.set_content_length(int(val)) elif key.lower() == 'content-type': self.request.content_type = val else: self.request.headers_out.add(key, val) return self.write",if key.lower() == 'content-length':,if key.lower() == 'content-length':,0.8594932387731311,0.8627586293513119,True
4607,"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp(x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p) if len(result) == 1: y = result[0] <IF_STMT> y1, y2 = result y = y1 if y1 & 1 == y else y2 else: return None return ec.Point(self._curve, x, y)",elif len(result) == 2:,if y == 0:,0.6777888530528366,0.9226596185977016,False
4608,"def trace(self, ee, rname): print(type(self)) self.traceIndent() guess = '' if self.inputState.guessing > 0: guess = ' [guessing]' print(ee + rname + guess) for i in xrange(1, self.k + 1): if i != 1: print(', ') <IF_STMT> v = self.LT(i).getText() else: v = 'null' print('LA(%s) == %s' % (i, v)) print('\n')",if self.LT(i):,if self.LT(i).getText():,0.8343793821065991,0.9284304001296656,False
4609,"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] if primary_key: parts.append('PRIMARY KEY') <IF_STMT> parts.append('NOT NULL') result[name] = ' '.join(parts) return result",if not_null:,elif not_null:,0.9047485584708727,0.9024521756077707,False
4610,"def _parse_csrf(self, response): for d in response: if d.startswith('Set-Cookie:'): for c in d.split(':', 1)[1].split(';'): if c.strip().startswith('CSRF-Token-'): self._CSRFtoken = c.strip(' \r\n') log.verbose('Got new cookie: %s', self._CSRFtoken) break <IF_STMT> break",if self._CSRFtoken != None:,if self._CSRFtoken is None:,0.6677050952628004,0.8381098951593456,False
4611,"def _update_from_item(self, row, download_item): progress_stats = download_item.progress_stats for key in self.columns: column = self.columns[key][0] <IF_STMT> status = '{0} {1}/{2}'.format(progress_stats['status'], progress_stats['playlist_index'], progress_stats['playlist_size']) self.SetStringItem(row, column, status) else: self.SetStringItem(row, column, progress_stats[key])",if key == 'status' and progress_stats['playlist_index']:,if key == 'status':,0.7733537315783503,0.8105932471967202,False
4612,"def unmarshal_package_repositories(cls, data: Any) -> List['PackageRepository']: repositories = list() if data is not None: <IF_STMT> raise RuntimeError(f'invalid package-repositories: {data!r}') for repository in data: package_repo = cls.unmarshal(repository) repositories.append(package_repo) return repositories","if not isinstance(data, list):","if not isinstance(data, list):",0.883755584436631,0.8169276475307028,True
4613,"def remove_message(e=None): itop = scanbox.nearest(0) sel = scanbox.curselection() if not sel: dialog(root, 'No Message To Remove', 'Please select a message to remove', '', 0, 'OK') return todo = [] for i in sel: line = scanbox.get(i) <IF_STMT> todo.append(string.atoi(scanparser.group(1))) mhf.removemessages(todo) rescan() fixfocus(min(todo), itop)",if scanparser.match(line) >= 0:,if line:,0.9269111849757408,0.9144061946646023,False
4614,"def test_patches(): print('Botocore version: {} aiohttp version: {}'.format(botocore.__version__, aiohttp.__version__)) success = True for obj, digests in chain(_AIOHTTP_DIGESTS.items(), _API_DIGESTS.items()): digest = hashlib.sha1(getsource(obj).encode('utf-8')).hexdigest() <IF_STMT> print('Digest of {}:{} not found in: {}'.format(obj.__qualname__, digest, digests)) success = False assert success",if digest not in digests:,if digest not in digests:,0.7597620600369235,0.8294838585473985,True
4615,"def sample_admin_user(): """"""List of iris messages"""""" with iris_ctl.db_from_config(sample_db_config) as (conn, cursor): cursor.execute('SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1') result = cursor.fetchone() <IF_STMT> return result[0]",if result:,if result:,0.7195356729644249,0.8986118159169515,True
4616,"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: <IF_STMT> for rightname2 in groups[rightname]: rightnames.add(rightname2) if not includeAll: break else: rightnames.add(rightname)",if rightname[0] == '@':,if rightname in groups:,0.8954532147020342,0.7965020533851944,False
4617,"def build(self, input_shape): if isinstance(input_shape, list) and len(input_shape) == 2: self.data_mode = 'disjoint' self.F = input_shape[0][-1] else: <IF_STMT> self.data_mode = 'single' else: self.data_mode = 'batch' self.F = input_shape[-1]",if len(input_shape) == 2:,if len(input_shape) == 1:,0.6663151456081775,0.8105932471967202,False
4618,"def update_ranges(l, i): for _range in l: <IF_STMT> _range[0] = i merge_ranges(l) return elif i == _range[1] + 1: _range[1] = i merge_ranges(l) return l.append([i, i]) l.sort(key=lambda x: x[0])",if i == _range[0] - 1:,if i == _range[0] + 1:,0.7854158775629699,0.7665936070959262,False
4619,"def transform(a, cmds): buf = a.split('\n') for cmd in cmds: ctype, line, col, char = cmd <IF_STMT> if char != '\n': buf[line] = buf[line][:col] + buf[line][col + len(char):] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == 'I': buf[line] = buf[line][:col] + char + buf[line][col:] buf = '\n'.join(buf).split('\n') return '\n'.join(buf)",if ctype == 'D':,if ctype == 'I':,0.7401324628293653,0.9036816878108535,False
4620,"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple if os.path.exists(uri) == True: <IF_STMT> files.append(uri) if len(files) == 0: return open_dropped_files(files)",if utils.is_media_file(uri) == True:,if not os.path.isfile(uri):,0.9359036270403965,0.8875087479151215,False
4621,"def __walk_proceed_remote_dir_act(self, r, args): dirjs, filejs = args j = r.json() if 'list' not in j: self.pd(""Key 'list' not found in the response of directory listing request:\n{}"".format(j)) return const.ERequestFailed paths = j['list'] for path in paths: <IF_STMT> dirjs.append(path) else: filejs.append(path) return const.ENoError",if path['isdir']:,if os.path.isdir(path):,0.7560893660650092,0.9144061946646023,False
4622,"def TaskUpdatesVerbose(task, progress): if isinstance(task.info.progress, int): info = task.info <IF_STMT> progress = '%d%% (%s)' % (info.progress, info.state) print('Task %s (key:%s, desc:%s) - %s' % (info.name.info.name, info.key, info.description, progress))","if not isinstance(progress, str):",if progress is None:,0.7999161074910055,0.8105932471967202,False
4623,"def dump_constants(header): output = StringIO.StringIO() output.write(header) for attribute in dir(FSEvents): value = getattr(FSEvents, attribute) <IF_STMT> output.write('%s = %s\n' % (attribute, hex(value))) content = output.getvalue() output.close() return content","if attribute.startswith('k') and isinstance(value, int):",if value is not None:,0.7564866115579177,0.7765145040967655,False
4624,"def _ensure_data_is_loaded(self, sql_object, input_params, stdin_file, stdin_filename='-', stop_after_analysis=False): data_loads = [] for filename in sql_object.qtable_names: data_load = self._load_data(filename, input_params, stdin_file=stdin_file, stdin_filename=stdin_filename, stop_after_analysis=stop_after_analysis) <IF_STMT> data_loads.append(data_load) return data_loads",if data_load is not None:,if data_load:,0.8223878893339245,0.8531413606256201,False
4625,"def _get_instantiation(self): if self._data is None: f, l, c, o = (c_object_p(), c_uint(), c_uint(), c_uint()) SourceLocation_loc(self, byref(f), byref(l), byref(c), byref(o)) <IF_STMT> f = File(f) else: f = None self._data = (f, int(l.value), int(c.value), int(c.value)) return self._data",if f:,if o.value == 0:,0.9098127768607782,0.8516228624291206,False
4626,def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() <IF_STMT> if splitrow[0] == 'INFO:': infos.append(' '.join(splitrow[1:])) return infos,if len(splitrow) > 0:,if len(splitrow) > 1:,0.7475209802992047,0.7498810286408993,False
4627,"def _brush_modified_cb(self, settings): """"""Updates the brush's base setting adjustments on brush changes"""""" for cname in settings: adj = self.brush_adjustment.get(cname, None) <IF_STMT> continue value = self.brush.get_base_value(cname) adj.set_value(value)",if adj is None:,if adj is None:,0.6401953882237643,0.7965020533851944,True
4628,"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = {'common': 'dns_ip'} if 'node' not in facts: facts['node'] = {} for role in params.keys(): if role in facts: for param in params[role]: <IF_STMT> facts['node'][param] = facts[role].pop(param) return facts",if param in facts[role]:,if param in facts[role]:,0.9050458563084387,0.8627586293513119,True
4629,"def serialize_content_range(value): if isinstance(value, (tuple, list)): <IF_STMT> raise ValueError('When setting content_range to a list/tuple, it must be length 2 or 3 (not %r)' % value) if len(value) == 2: begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value","if len(value) not in (2, 3):",if len(value) != 3:,0.8948595710298293,0.9053411402117831,False
4630,"def clean(self): data = super().clean() if data.get('expires'): <IF_STMT> data['expires'] = make_aware(datetime.combine(data['expires'], time(hour=23, minute=59, second=59)), self.instance.event.timezone) else: data['expires'] = data['expires'].replace(hour=23, minute=59, second=59) if data['expires'] < now(): raise ValidationError(_('The new expiry date needs to be in the future.')) return data","if isinstance(data['expires'], date):",if self.instance.event:,0.9325768087412029,0.9051034981560222,False
4631,"def _build(self, obj, stream, context): if self.include_name: name, obj = obj for sc in self.subcons: <IF_STMT> sc._build(obj, stream, context) return else: for sc in self.subcons: stream2 = BytesIO() context2 = context.__copy__() try: sc._build(obj, stream2, context2) except Exception: pass else: context.__update__(context2) stream.write(stream2.getvalue()) return raise SelectError('no subconstruct matched', obj)",if sc.name == name:,if name == sc.name:,0.9426564074831418,0.8856327184319047,False
4632,"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource('dynamodb').Table('Sphere11.Dev.ResourceLocks') results = table.scan() for r in results['Items']: <IF_STMT> r['LockDate'] = datetime.fromtimestamp(r['LockDate']) if 'RevisionDate' in r: r['RevisionDate'] = datetime.fromtimestamp(r['RevisionDate']) print(tabulate.tabulate(results['Items'], headers='keys', tablefmt='fancy_grid'))",if 'LockDate' in r:,if 'LockDate' in r:,0.8858591391892155,0.8336104423443033,True
4633,"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): <IF_STMT> if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",if type(test.value) in self._const_types:,"if isinstance(body, ast.Expr):",0.8772596905455402,0.8196189957582152,False
4634,"def validate_max_discount(self): if self.rate_or_discount == 'Discount Percentage' and self.get('items'): for d in self.items: max_discount = frappe.get_cached_value('Item', d.item_code, 'max_discount') <IF_STMT> throw(_('Max discount allowed for item: {0} is {1}%').format(self.item_code, max_discount))",if max_discount and flt(self.discount_percentage) > flt(max_discount):,if max_discount > 100:,0.6480642065291721,0.8105932471967202,False
4635,"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if 'identifiers' in rule and rule['identifiers'] is not None: for i_type, i_value in rule['identifiers'].items(): if i_type[0:3] == 'cce': <IF_STMT> return True return False",if not checks.is_cce_value_valid('CCE-' + str(i_value)):,if i_value == 'invalid':,0.890593841467958,0.8228500218338367,False
4636,"def parse_calendar_eras(data, calendar): eras = data.setdefault('eras', {}) for width in calendar.findall('eras/*'): width_type = NAME_MAP[width.tag] widths = eras.setdefault(width_type, {}) for elem in width.getiterator(): if elem.tag == 'era': _import_type_text(widths, elem, type=int(elem.attrib.get('type'))) <IF_STMT> eras[width_type] = Alias(_translate_alias(['eras', width_type], elem.attrib['path']))",elif elem.tag == 'alias':,elif elem.tag == 'eras':,0.9191244223826994,0.8474968231198384,False
4637,"def validate_grammar() -> None: for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE: fn_productions = get_productions(fn) if all((p.name == fn_productions[0].name for p in fn_productions)): production_name = fn_productions[0].name expected_name = f'convert_{production_name}' <IF_STMT> raise Exception(f""The conversion function for '{production_name}' "" + f""must be called '{expected_name}', not '{fn.__name__}'."")",if fn.__name__ != expected_name:,if not fn.__name__ == expected_name:,0.8183728868419676,0.845713978670975,False
4638,"def split_ratio(row): if float(row['Numerator']) > 0: <IF_STMT> n, m = row['Splitratio'].split(':') return float(m) / float(n) else: return eval(row['Splitratio']) else: return 1",if ':' in row['Splitratio']:,if ':' in row['Splitratio']:,0.7744968197750441,0.7498810286408993,True
4639,def _handle_def_errors(testdef): if testdef.error: if testdef.exception: <IF_STMT> raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception('Test parse failure'),"if isinstance(testdef.exception, Exception):","if isinstance(testdef.exception, Exception):",0.599367349209652,0.7331765459202478,True
4640,"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): if count <= 0: quotas_ok[quota] = 0 break avail = qa.results[quota] <IF_STMT> quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",if avail[1] is not None and avail[1] < count:,if avail[0] == 0:,0.865567499266633,0.8832000938217648,False
4641,"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = (rb.data[ri], lb.data[li]) li += 1 <IF_STMT> lb = lb.rightlink li = 0 ri -= 1 if ri < 0: rb = rb.leftlink ri = BLOCKLEN - 1",if li >= BLOCKLEN:,if li < 0:,0.8145237325731118,0.8983343737277126,False
4642,"def __manipulate_item(self, item): if self._Cursor__manipulate: db = self._Cursor__collection.database son = db._fix_outgoing(item, self._Cursor__collection) else: son = item if self.__wrap is not None: <IF_STMT> return getattr(self._Cursor__collection, son[self.__wrap.type_field])(son) return self.__wrap(son, collection=self._Cursor__collection) else: return son",if self.__wrap.type_field in son:,if self.__wrap.type_field in son:,0.8691463823415007,0.828399516355805,True
4643,"def apply_transforms(self): """"""Apply all of the stored transforms, in priority order."""""" self.document.reporter.attach_observer(self.document.note_transform_message) while self.transforms: <IF_STMT> self.transforms.sort() self.transforms.reverse() self.sorted = 1 priority, transform_class, pending, kwargs = self.transforms.pop() transform = transform_class(self.document, startnode=pending) transform.apply(**kwargs) self.applied.append((priority, transform_class, pending, kwargs))",if not self.sorted:,if self.sorted:,0.7805303878452519,0.8966773400768917,False
4644,"def format_sql(sql, params): rv = [] if isinstance(params, dict): conv = _FormatConverter(params) <IF_STMT> sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): if param is None: rv.append('NULL') param = safe_repr(param) rv.append(param) return (sql, rv)",if params:,if conv.params:,0.817542438700479,0.9220450449751959,False
4645,"def on_execution_item(self, cpath, execution): if not isinstance(execution, dict): return if 'executor' in execution and execution.get('executor') != 'jmeter': return scenario = execution.get('scenario', None) <IF_STMT> return if isinstance(scenario, str): scenario_name = scenario scenario = self.get_named_scenario(scenario_name) if not scenario: scenario = None scenario_path = Path('scenarios', scenario_name) else: scenario_path = cpath.copy() scenario_path.add_component('scenario') if scenario is not None: self.check_jmeter_scenario(scenario_path, scenario)",if not scenario:,if scenario is None:,0.6795580658261607,0.9019629427251674,False
4646,"def _poll_ipc_requests(self) -> None: try: if self._ipc_requests.empty(): return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: <IF_STMT> self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception('Problem processing ipc request', exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",if os.path.isfile(filename):,if os.path.isfile(filename):,0.935596251404603,0.8935248372106969,True
4647,"def get_scroll_distance_to_element(driver, element): try: scroll_position = driver.execute_script('return window.scrollY;') element_location = None element_location = element.location['y'] element_location = element_location - 130 <IF_STMT> element_location = 0 distance = element_location - scroll_position return distance except Exception: return 0",if element_location < 0:,if element_location < 0:,0.8051039921544514,0.8431339019329497,True
4648,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_access_token(d.getPrefixedString()) continue <IF_STMT> self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 16:,if tt == 16:,0.9029988415311366,0.7965020533851944,True
4649,"def _validate_and_define(params, key, value): key, force_generic = _validate_key(_unescape(key)) if key in params: raise SyntaxError(f'duplicate key ""{key}""') cls = _class_for_key.get(key, GenericParam) emptiness = cls.emptiness() if value is None: if emptiness == Emptiness.NEVER: raise SyntaxError('value cannot be empty') value = cls.from_value(value) el<IF_STMT> value = cls.from_wire_parser(dns.wire.Parser(_unescape(value))) else: value = cls.from_value(value) params[key] = value",if force_generic:,if force_generic:,0.9147862104322961,0.9284304001296656,True
4650,"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): <IF_STMT> continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield (field_name, field_val)",if exclude_meta and field.meta:,"if field_name in ('meta', 'meta_class'):",0.927210034864275,0.8621109017306224,False
4651,"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop() <IF_STMT> self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",if self.sl_hdlr:,if self.sl_hdlr:,0.5401911284098544,0.7241577342575828,True
4652,"def _wait_for_async_copy(self, share_name, file_path): count = 0 share_client = self.fsc.get_share_client(share_name) file_client = share_client.get_file_client(file_path) properties = file_client.get_file_properties() while properties.copy.status != 'success': count = count + 1 <IF_STMT> self.fail('Timed out waiting for async copy to complete.') self.sleep(6) properties = file_client.get_file_properties() self.assertEqual(properties.copy.status, 'success')",if count > 10:,if count > 10:,0.7735058842931364,0.8661072626070159,True
4653,"def __new__(cls, message_type: OrderBookMessageType, content: Dict[str, any], timestamp: Optional[float]=None, *args, **kwargs): if timestamp is None: <IF_STMT> raise ValueError('timestamp must not be None when initializing snapshot messages.') timestamp = int(time.time()) return super(KucoinOrderBookMessage, cls).__new__(cls, message_type, content, *args, timestamp=timestamp, **kwargs)",if message_type is OrderBookMessageType.SNAPSHOT:,if message_type is OrderBookMessageType.SNAPSHOT:,0.9021036890180323,0.8555308664663046,True
4654,"def _drop_unique_features(X: DataFrame, feature_metadata: FeatureMetadata, max_unique_ratio) -> list: features_to_drop = [] X_len = len(X) max_unique_value_count = X_len * max_unique_ratio for column in X: unique_value_count = len(X[column].unique()) <IF_STMT> features_to_drop.append(column) elif feature_metadata.get_feature_type_raw(column) in [R_CATEGORY, R_OBJECT] and unique_value_count > max_unique_value_count: features_to_drop.append(column) return features_to_drop",if unique_value_count == 1:,"if feature_metadata.get_feature_type_raw(column) in [R_CATEGORY, R_OBJECT]:",0.7318185710929963,0.845713978670975,False
4655,"def get_src_findex_by_pad(s, S, padding_mode, align_corners): if padding_mode == 'zero': return get_src_findex_with_zero_pad(s, S) elif padding_mode == 'reflect': <IF_STMT> return get_src_findex_with_reflect_pad(s, S, True) else: sf = get_src_findex_with_reflect_pad(s, S, False) return get_src_findex_with_repeat_pad(sf, S) elif padding_mode == 'repeat': return get_src_findex_with_repeat_pad(s, S)",if align_corners:,if align_corners:,0.8534776499224896,0.9024521756077707,True
4656,"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) if current._parent is upto: break <IF_STMT> raise sa_exc.InvalidRequestError('Transaction %s is not on the active transaction list' % upto) else: current = current._parent return result",elif current._parent is None:,if current._parent is self:,0.7105508352277339,0.8592377270804451,False
4657,"def __setattr__(self, name: str, val: Any): if name.startswith('COMPUTED_'): <IF_STMT> old_val = self[name] if old_val == val: return raise KeyError(""Computed attributed '{}' already exists with a different value! old={}, new={}."".format(name, old_val, val)) self[name] = val else: super().__setattr__(name, val)",if name in self:,if name in self:,0.885114640002198,0.8555308664663046,True
4658,"def get_fnlist(bbhandler, pkg_pn, preferred): """"""Get all recipe file names"""""" <IF_STMT> latest_versions, preferred_versions = bb.providers.findProviders(bbhandler.config_data, bbhandler.cooker.recipecaches[''], pkg_pn) fn_list = [] for pn in sorted(pkg_pn): if preferred: fn_list.append(preferred_versions[pn][1]) else: fn_list.extend(pkg_pn[pn]) return fn_list",if preferred:,if pkg_pn:,0.686904549645646,0.8787142254774354,False
4659,"def links_extracted(self, _, links): links_deduped = {} for link in links: link_fingerprint = link.meta[FIELD_FINGERPRINT] <IF_STMT> continue links_deduped[link_fingerprint] = link [self._redis_pipeline.hmset(fingerprint, self._create_link_extracted(link)) for fingerprint, link in links_deduped.items()] self._redis_pipeline.execute()",if link_fingerprint in links_deduped:,if link_fingerprint in links_deduped:,0.7123092953702728,0.8038019482772603,True
4660,"def __call__(self, name, rawtext, text, lineno, inliner, options=None, content=None): options = options or {} content = content or [] issue_nos = [each.strip() for each in utils.unescape(text).split(',')] config = inliner.document.settings.env.app.config ret = [] for i, issue_no in enumerate(issue_nos): node = self.make_node(name, issue_no, config, options=options) ret.append(node) <IF_STMT> sep = nodes.raw(text=', ', format='html') ret.append(sep) return (ret, [])",if i != len(issue_nos) - 1:,if i == len(issue_nos) - 1:,0.854228722640973,0.8688589397154922,False
4661,"def init_messengers(messengers): for messenger in messengers: <IF_STMT> module_path = messenger['type'] messenger['type'] = messenger['type'].split('.')[-1] else: module_path = 'oncall.messengers.' + messenger['type'] instance = getattr(importlib.import_module(module_path), messenger['type'])(messenger) for transport in instance.supports: _active_messengers[transport].append(instance)",if '.' in messenger['type']:,if '.' in messenger['type']:,0.851994427719525,0.8105932471967202,True
4662,"def _process_enum_definition(self, tok): fields = [] for field in tok.fields: <IF_STMT> expression = self.expression_parser.parse(field.expression) else: expression = None fields.append(c_ast.CEnumField(name=field.name.first, value=expression)) name = tok.enum_name if name: name = 'enum %s' % tok.enum_name.first else: name = self._make_anonymous_type('enum') return c_ast.CTypeDefinition(name=name, type_definition=c_ast.CEnum(attributes=tok.attributes, fields=fields, name=name))",if field.expression:,if field.expression:,0.796265946874529,0.9099951253570094,True
4663,def result_iterator(): try: fs.reverse() while fs: <IF_STMT> yield fs.pop().result() else: yield fs.pop().result(end_time - time.time()) finally: for future in fs: future.cancel(),if timeout is None:,if end_time is None:,0.8554549681658178,0.7378351342269067,False
4664,def has_encrypted_ssh_key_data(self): try: ssh_key_data = self.get_input('ssh_key_data') except AttributeError: return False try: pem_objects = validate_ssh_private_key(ssh_key_data) for pem_object in pem_objects: <IF_STMT> return True except ValidationError: pass return False,"if pem_object.get('key_enc', False):",if pem_object['encrypted']:,0.8867345002370366,0.8590888738245122,False
4665,"def test_seq_object_transcription_method(self): for nucleotide_seq in test_seqs: <IF_STMT> self.assertEqual(repr(Seq.transcribe(nucleotide_seq)), repr(nucleotide_seq.transcribe()))","if isinstance(nucleotide_seq, Seq.Seq):","if isinstance(nucleotide_seq, Seq):",0.4678584044866472,0.5193071778680676,False
4666,def max_elevation(self): max_el = None for y in xrange(self.height): for x in xrange(self.width): el = self.elevation['data'][y][x] <IF_STMT> max_el = el return max_el,if max_el is None or el > max_el:,if el > max_el:,0.6978774978552094,0.760856626273165,False
4667,"def stress(mapping, index): for count in range(OPERATIONS): function = random.choice(functions) function(mapping, index) <IF_STMT> print('\r', len(mapping), ' ' * 7, end='') print()",if count % 1000 == 0:,if count % 100 == 0:,0.6722490178691835,0.6885326214539055,False
4668,"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): if 'terminology' not in source.all_flags: continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True <IF_STMT> continue missing.append((source.context, source.source, '')) if missing: self.add_units(None, missing)",if not add:,if add:,0.8054613163430226,0.9122561819614461,False
4669,"def get_generators(self): """"""Get a dict with all registered generators, indexed by name"""""" generators = {} for core in self.db.find(): <IF_STMT> _generators = core.get_generators({}) if _generators: generators[str(core.name)] = _generators return generators","if hasattr(core, 'get_generators'):",if core.name:,0.9084694327576284,0.8787142254774354,False
4670,"def act(self, state): if self.body.env.clock.frame < self.training_start_step: return policy_util.random(state, self, self.body).cpu().squeeze().numpy() else: action = self.action_policy(state, self, self.body) <IF_STMT> action = self.scale_action(torch.tanh(action)) return action.cpu().squeeze().numpy()",if not self.body.is_discrete:,if self.scale_mode:,0.8578624036214411,0.839587623092576,False
4671,"def try_open_completions_event(self, event=None): """"""(./) Open completion list after pause with no movement."""""" lastchar = self.text.get('insert-1c') if lastchar in TRIGGERS: args = TRY_A if lastchar == '.' else TRY_F self._delayed_completion_index = self.text.index('insert') <IF_STMT> self.text.after_cancel(self._delayed_completion_id) self._delayed_completion_id = self.text.after(self.popupwait, self._delayed_open_completions, args)",if self._delayed_completion_id is not None:,if self._delayed_completion_index != -1:,0.6881421786973987,0.8592377270804451,False
4672,def token_is_available(self): if self.token: try: resp = requests.get('https://api.shodan.io/account/profile?key={0}'.format(self.token)) <IF_STMT> return True except Exception as ex: logger.error(str(ex)) return False,if resp and resp.status_code == 200 and ('member' in resp.json()):,if resp.status_code == 200 and resp.text == self.token:,0.5364002369962577,0.5886994264160949,False
4673,"def next_bar_(self, event): bars = event.bar_dict self._current_minute = self._minutes_since_midnight(self.ucontext.now.hour, self.ucontext.now.minute) for day_rule, time_rule, func in self._registry: <IF_STMT> with ExecutionContext(EXECUTION_PHASE.SCHEDULED): with ModifyExceptionFromType(EXC_TYPE.USER_EXC): func(self.ucontext, bars) self._last_minute = self._current_minute",if day_rule() and time_rule():,if day_rule == time_rule:,0.8844319244412234,0.7965020533851944,False
4674,"def decoder(s): r = [] decode = [] for c in s: if c == '&' and (not decode): decode.append('&') <IF_STMT> if len(decode) == 1: r.append('&') else: r.append(modified_unbase64(''.join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64(''.join(decode[1:]))) bin_str = ''.join(r) return (bin_str, len(s))",elif c == '-' and decode:,elif c == '&':,0.8809819984781206,0.8806615362338783,False
4675,"def admin_audit_get(admin_id): if settings.app.demo_mode: resp = utils.demo_get_cache() <IF_STMT> return utils.jsonify(resp) if not flask.g.administrator.super_user: return utils.jsonify({'error': REQUIRES_SUPER_USER, 'error_msg': REQUIRES_SUPER_USER_MSG}, 400) admin = auth.get_by_id(admin_id) resp = admin.get_audit_events() if settings.app.demo_mode: utils.demo_set_cache(resp) return utils.jsonify(resp)",if resp:,if resp:,0.7348902222682906,0.8787142254774354,True
4676,"def vjp(self, argnum, outgrad, ans, vs, gvs, args, kwargs): try: return self.vjps[argnum](outgrad, ans, vs, gvs, *args, **kwargs) except KeyError: <IF_STMT> errstr = 'Gradient of {0} not yet implemented.' else: errstr = 'Gradient of {0} w.r.t. arg number {1} not yet implemented.' raise NotImplementedError(errstr.format(self.fun.__name__, argnum))",if self.vjps == {}:,if argnum == 0:,0.9019601027575783,0.8780099567239787,False
4677,"def update(self, *args, **kwargs): assert not self.readonly longest_key = 0 _dict = self._dict reverse = self.reverse casereverse = self.casereverse for iterable in args + (kwargs,): <IF_STMT> iterable = iterable.items() for key, value in iterable: longest_key = max(longest_key, len(key)) _dict[key] = value reverse[value].append(key) casereverse[value.lower()][value] += 1 self._longest_key = max(self._longest_key, longest_key)","if isinstance(iterable, (dict, StenoDictionary)):","if isinstance(iterable, dict):",0.8263336284081484,0.9081987180086649,False
4678,"def update_ui(self, window): view = window.get_active_view() self.set_status(view) lang = 'plain_text' if view: buf = view.get_buffer() language = buf.get_language() <IF_STMT> lang = language.get_id() self.setup_smart_indent(view, lang)",if language:,if language:,0.6928852578680083,0.8466657105524215,True
4679,"def number_operators(self, a, b, skip=[]): dict = {'a': a, 'b': b} for name, expr in self.binops.items(): if name not in skip: name = '__%s__' % name <IF_STMT> res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = '__%s__' % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)","if hasattr(a, name):","if hasattr(a, name):",0.8687843260481705,0.9274581800092581,True
4680,"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: <IF_STMT> return 0.03 + ctrl.labelHeight * 3 elif item['layout'] == 'vert': return ctrl.labelHeight * len(item['options'])",if item['layout'] == 'horiz':,if item['layout'] == 'vert':,0.9088318926666952,0.8723360571509826,False
4681,"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): if key in ('disabled', 'is_up'): <IF_STMT> value = value == 'true' self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)",if value is not None:,if key == 'is_up':,0.8750108472007208,0.8474968231198384,False
4682,"def _read_json_content(self, body_is_optional=False): if 'content-length' not in self.headers: return self.send_error(411) if not body_is_optional else {} try: content_length = int(self.headers.get('content-length')) if content_length == 0 and body_is_optional: return {} request = json.loads(self.rfile.read(content_length).decode('utf-8')) <IF_STMT> return request except Exception: logger.exception('Bad request') self.send_error(400)","if isinstance(request, dict) and (request or body_is_optional):","if request.get('content-type', 'application/json'):",0.9301919217224968,0.8815741981066073,False
4683,"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, '_viewcode_modules', {}) for modname, entry in list(modules.items()): <IF_STMT> continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) if len(used) == 0: modules.pop(modname)",if entry is False:,if modname == '__init__':,0.8421867592008465,0.8692960007731574,False
4684,"def frames(self): """"""an array of all the frames (including iframes) in the current window"""""" from thug.DOM.W3C.HTML.HTMLCollection import HTMLCollection frames = set() for frame in self._findAll(['frame', 'iframe']): <IF_STMT> from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation DOMImplementation.createHTMLElement(self.window.doc, frame) frames.add(frame._node) return HTMLCollection(self.doc, list(frames))","if not getattr(frame, '_node', None):",if frame._node is None:,0.9274169002616808,0.8555308664663046,False
4685,"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: <IF_STMT> return not kw['dir'] ^ isdir(self.strpath) if 'file' in kw: return not kw['file'] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",if 'dir' in kw:,if 'dir' in kw:,0.8489417919187899,0.8228500218338367,True
4686,"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): <IF_STMT> continue if other_path == path: duplicates.append(other_folder) if len(duplicates): self.duplicates[folder] = duplicates",if other_folder == folder:,if other_folder == folder:,0.9248416817017889,0.8431339019329497,True
4687,"def next(self, buf, pos): if pos >= len(buf): return (EOF, '', pos) mo = self.tokens_re.match(buf, pos) if mo: text = mo.group() type, regexp, test_lit = self.tokens[mo.lastindex - 1] pos = mo.end() <IF_STMT> type = self.literals.get(text, type) return (type, text, pos) else: c = buf[pos] return (self.symbols.get(c, None), c, pos + 1)",if test_lit:,if regexp == test_lit:,0.8860975827723429,0.8944264839442453,False
4688,"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) <IF_STMT> self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break max_frame = self._obs_buffer.max(axis=0) return (max_frame, total_reward, done, info)",if i == self._skip - 2:,if i == self._skip - 1:,0.9400498521689324,0.8711151332295498,False
4689,"def convert(self, ctx, argument): arg = argument.replace('0x', '').lower() if arg[0] == '#': arg = arg[1:] try: value = int(arg, base=16) if not 0 <= value <= 16777215: raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace(' ', '_') method = getattr(discord.Colour, arg, None) <IF_STMT> raise BadColourArgument(arg) return method()",if arg.startswith('from_') or method is None or (not inspect.ismethod(method)):,if not method:,0.9478044737346158,0.9062841320510342,False
4690,"def run(self, **inputs): if self.inputs.copy_inputs: self.inputs.subjects_dir = os.getcwd() <IF_STMT> inputs['subjects_dir'] = self.inputs.subjects_dir for originalfile in [self.inputs.in_file, self.inputs.in_norm]: copy2subjdir(self, originalfile, folder='mri') return super(SegmentCC, self).run(**inputs)",if 'subjects_dir' in inputs:,if self.inputs.subjects_dir:,0.7899669404581625,0.839587623092576,False
4691,"def get_queryset(self): if not hasattr(self, '_queryset'): <IF_STMT> qs = self.queryset else: qs = self.model._default_manager.get_queryset() if not qs.ordered: qs = qs.order_by(self.model._meta.pk.name) self._queryset = qs return self._queryset",if self.queryset is not None:,if self.queryset:,0.6466489028209064,0.8531413606256201,False
4692,"def visit_simple_stmt(self, node: Node) -> Iterator[Line]: """"""Visit a statement without nested statements."""""" is_suite_like = node.parent and node.parent.type in STATEMENT if is_suite_like: <IF_STMT> yield from self.visit_default(node) else: yield from self.line(+1) yield from self.visit_default(node) yield from self.line(-1) else: if not self.is_pyi or not node.parent or (not is_stub_suite(node.parent)): yield from self.line() yield from self.visit_default(node)",if self.is_pyi and is_stub_body(node):,if self.is_pyi or not node.parent:,0.7573249017504509,0.8777008558345754,False
4693,"def rawDataReceived(self, data): if self.timeout > 0: self.resetTimeout() self._pendingSize -= len(data) if self._pendingSize > 0: self._pendingBuffer.write(data) else: passon = b'' <IF_STMT> data, passon = (data[:self._pendingSize], data[self._pendingSize:]) self._pendingBuffer.write(data) rest = self._pendingBuffer self._pendingBuffer = None self._pendingSize = None rest.seek(0, 0) self._parts.append(rest.read()) self.setLineMode(passon.lstrip(b'\r\n'))",if self._pendingSize < 0:,if self._pendingSize > 0:,0.7970348389246272,0.8661072626070159,False
4694,"def handle(self, *args, **options): app_name = options.get('app_name') job_name = options.get('job_name') if app_name and (not job_name): job_name = app_name app_name = None if options.get('list_jobs'): print_jobs(only_scheduled=False, show_when=True, show_appname=True) else: <IF_STMT> print('Run a single maintenance job. Please specify the name of the job.') return self.runjob(app_name, job_name, options)",if not job_name:,if app_name:,0.7586787448969553,0.9184043388013005,False
4695,"def _exportReceived(self, content, error=False, server=None, context={}, **kwargs): if error: <IF_STMT> self.error.emit(content['message'], True) else: self.error.emit(""Can't export the project from the server"", True) self.finished.emit() return self.finished.emit()",if content:,if 'message' in content:,0.8694644399807397,0.7801270245332924,False
4696,"def __iter__(self): n = self.n k = self.k j = int(np.ceil(n / k)) for i in range(k): test_index = np.zeros(n, dtype=bool) <IF_STMT> test_index[i * j:(i + 1) * j] = True else: test_index[i * j:] = True train_index = np.logical_not(test_index) yield (train_index, test_index)",if i < k - 1:,if i % 2 == 0:,0.6783656457284504,0.8375707157974782,False
4697,"def addType(self, graphene_type): meta = get_meta(graphene_type) if meta: <IF_STMT> self._typeMap[meta.name] = graphene_type else: raise Exception('Type {typeName} already exists in the registry.'.format(typeName=meta.name)) else: raise Exception('Cannot add unnamed type or a non-type to registry.')",if not graphene_type in self._typeMap:,if meta.name in self._typeMap:,0.5952602167737904,0.8336104423443033,False
4698,"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len('hello'), len(base64MIME.encode('hello', eol=''))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 <IF_STMT> bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len('x' * size), bsize)",elif size <= 6:,elif size <= 6:,0.9246210060240099,0.8964173245779284,True
4699,"def _asStringList(self, sep=''): out = [] for item in self._toklist: <IF_STMT> out.append(sep) if isinstance(item, ParseResults): out += item._asStringList() else: out.append(str(item)) return out",if out and sep:,if sep:,0.6911161984519091,0.8318180062062374,False
4700,"def open_file_input(cli_parsed): files = glob.glob(os.path.join(cli_parsed.d, '*report.html')) if len(files) > 0: print('\n[*] Done! Report written in the ' + cli_parsed.d + ' folder!') print('Would you like to open the report now? [Y/n]') while True: try: response = input().lower() <IF_STMT> return True else: return strtobool(response) except ValueError: print('Please respond with y or n') else: print('[*] No report files found to open, perhaps no hosts were successful') return False",if response == '':,if response == 'y':,0.6531251707400343,0.9180466554652996,False
4701,"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): <IF_STMT> realvalname, key = valname.split('.', 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: if name in self.values: self.__dict__[name] = config[name] del self._raw_config",if '.' in valname:,if '.' in valname:,0.8423899974058572,0.8555308664663046,True
4702,"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: <IF_STMT> exc_info = sys.exc_info() elif not isinstance(e, self.quiet_exceptions): app_log.error('Multiple exceptions in yield list', exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)",if exc_info is None:,if exc_info is None:,0.9371689316854978,0.8902056737869248,True
4703,"def test01e_json(self): """"""Testing GeoJSON input/output."""""" if not GEOJSON: return for g in self.geometries.json_geoms: geom = OGRGeometry(g.wkt) <IF_STMT> self.assertEqual(g.json, geom.json) self.assertEqual(g.json, geom.geojson) self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))","if not hasattr(g, 'not_equal'):",if GEOJSON:,0.8165928121933637,0.839587623092576,False
4704,"def __init__(self, hub=None): if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() <IF_STMT> _resolver.network_resolver.nameservers[:] = config.resolver_nameservers if config.resolver_timeout: _resolver.network_resolver.lifetime = config.resolver_timeout assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",if config.resolver_nameservers:,if config.resolver_nameservers:,0.7275112829329522,0.8645707301556367,True
4705,"def __iadd__(self, term): if isinstance(term, (int, long)): <IF_STMT> _gmp.mpz_add_ui(self._mpz_p, self._mpz_p, c_ulong(term)) return self if -65535 < term < 0: _gmp.mpz_sub_ui(self._mpz_p, self._mpz_p, c_ulong(-term)) return self term = Integer(term) _gmp.mpz_add(self._mpz_p, self._mpz_p, term._mpz_p) return self",if 0 <= term < 65536:,if 65535 < term < 0:,0.8775838661742603,0.7865984197371234,False
4706,"def copy(dst, src): for k, v in src.iteritems(): <IF_STMT> d = {} dst[k] = d copy(d, v) else: dst[k] = v","if isinstance(v, dict):","if isinstance(v, dict):",0.7892317516434033,0.7848518349390632,True
4707,"def generator(self, data): self.procs = OrderedDict() for task in data: self.recurse_task(task, 0, 0, self.procs) for offset, name, level, pid, ppid, uid, euid, gid in self.procs.values(): <IF_STMT> yield (0, [Address(offset), str(name), str(level), int(pid), int(ppid), int(uid), int(gid), int(euid)])",if offset:,if offset:,0.8145083897508085,0.8996480074924822,True
4708,"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) <IF_STMT> father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True if not mother_handle: return True return False",if family:,if family:,0.9237726550278156,0.9076141716697395,True
4709,"def _arctic_task_exec(request): request.start_time = time.time() logging.debug('Executing asynchronous request for {}/{}'.format(request.library, request.symbol)) result = None try: request.is_running = True <IF_STMT> result = mongo_retry(request.fun)(*request.args, **request.kwargs) else: result = request.fun(*request.args, **request.kwargs) except Exception as e: request.exception = e finally: request.data = result request.end_time = time.time() request.is_running = False return result",if request.mongo_retry:,if request.retry:,0.9215080211286125,0.9237460349978159,False
4710,"def _setup_styles(self): for ttype, ndef in self.style: escape = EscapeSequence() <IF_STMT> escape.fg = self._color_index(ndef['color']) if ndef['bgcolor']: escape.bg = self._color_index(ndef['bgcolor']) if self.usebold and ndef['bold']: escape.bold = True if self.useunderline and ndef['underline']: escape.underline = True self.style_string[str(ttype)] = (escape.color_string(), escape.reset_string())",if ndef['color']:,if ndef['color']:,0.6885692241747149,0.9024521756077707,True
4711,"def process_string(self, remove_repetitions, sequence): string = '' for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: if remove_repetitions and i != 0 and (char == sequence[i - 1]): pass <IF_STMT> string += ' ' else: string = string + char return string",elif char == self.labels[self.space_index]:,elif i == 0:,0.9258168637173293,0.8723360571509826,False
4712,"def arith_expr(self, nodelist): node = self.com_node(nodelist[0]) for i in range(2, len(nodelist), 2): right = self.com_node(nodelist[i]) <IF_STMT> node = Add(node, right, lineno=nodelist[1].context) elif nodelist[i - 1].type == token.MINUS: node = Sub(node, right, lineno=nodelist[1].context) else: raise ValueError('unexpected token: %s' % nodelist[i - 1][0]) return node",if nodelist[i - 1].type == token.PLUS:,if nodelist[i - 1].type == token.MINUS:,0.7438339594478947,0.8375707157974782,False
4713,"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = (index.start, index.stop) new_start, new_stop = (None, None) <IF_STMT> new_stop = length - start if stop is not None: new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",if start is not None:,if start is not None:,0.7806260133439238,0.8970855583859476,True
4714,"def getRoots(job): if job not in visited: visited.add(job) <IF_STMT> list(map(lambda p: getRoots(p), job._directPredecessors)) else: roots.add(job) list(map(lambda c: getRoots(c), job._children + job._followOns))",if len(job._directPredecessors) > 0:,if job._directPredecessors:,0.6019448215802286,0.8232490471721702,False
4715,"def visit_filter_projection(self, node, value): base = self.visit(node['children'][0], value) if not isinstance(base, list): return None comparator_node = node['children'][2] collected = [] for element in base: <IF_STMT> current = self.visit(node['children'][1], element) if current is not None: collected.append(current) return collected","if self._is_true(self.visit(comparator_node, element)):",if element in comparator_node:,0.898810785169246,0.8555308664663046,False
4716,"def func(x, y): try: if x > y: z = x + 2 * math.sin(y) return z ** 2 <IF_STMT> return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print('finished')",elif x == y:,elif x < 0:,0.9159321481421325,0.8923575006167597,False
4717,"def set_filter(self, dataset_opt): """"""This function create and set the pre_filter to the obj as attributes"""""" self.pre_filter = None for key_name in dataset_opt.keys(): <IF_STMT> new_name = key_name.replace('filters', 'filter') try: filt = instantiate_filters(getattr(dataset_opt, key_name)) except Exception: log.exception('Error trying to create {}, {}'.format(new_name, getattr(dataset_opt, key_name))) continue setattr(self, new_name, filt)",if 'filter' in key_name:,if key_name.startswith('filters'):,0.915230167638208,0.9220450449751959,False
4718,"def _add_states_to_lookup(self, trackers_as_states, trackers_as_actions, domain, online=False): """"""Add states to lookup dict"""""" for states in trackers_as_states: active_form = self._get_active_form_name(states[-1]) <IF_STMT> states = self._modified_states(states) feature_key = self._create_feature_key(states) self.lookup[feature_key] = active_form",if active_form and self._prev_action_listen_in_state(states[-1]):,if active_form not in self.lookup:,0.8175327388947312,0.7839800200378249,False
4719,"def list_loaded_payloads(self): print(helpers.color('\n [*] Available Payloads:\n')) lastBase = None x = 1 for name in sorted(self.active_payloads.keys()): parts = name.split('/') <IF_STMT> print() lastBase = parts[0] print('\t%s)\t%s' % (x, '{0: <24}'.format(name))) x += 1 print('\n') return",if lastBase and parts[0] != lastBase:,if lastBase != parts[0]:,0.7306215048379208,0.8431339019329497,False
4720,"def reprSmart(vw, item): ptype = type(item) if ptype is int: if -1024 < item < 1024: return str(item) <IF_STMT> return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) elif ptype is dict: return '{%s}' % ','.join(['%s:%s' % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]) else: return repr(item)",elif vw.isValidPointer(item):,elif -128 < item < 128:,0.6435217753221558,0.8665222382201849,False
4721,"def ConfigSectionMap(section): config = ConfigParser.RawConfigParser() configurations = config_manager() configf = configurations.configf config.read(configf) dict1 = {} options = config.options(section) for option in options: try: dict1[option] = config.get(section, option) <IF_STMT> DebugPrint('skip: %s' % option) except: print('Exception on %s!' % option) dict1[option] = None return dict1",if dict1[option] == -1:,if dict1[option] is None:,0.8415793467869181,0.8752376177722327,False
4722,def on_success(result): subtasks = {} if result: subtasks = {self.nodes_keys.inverse[s['node_id']]: s.get('subtask_id') for s in result <IF_STMT>} if subtasks: print('subtask finished') self.next() else: print('waiting for a subtask to finish') time.sleep(10),if s.get('status') == 'Failure',if s['subtask_id'],0.8400644149329175,0.8743414417652072,False
4723,"def redirect_aware_commmunicate(p, sys=_sys): """"""Variant of process.communicate that works with in process I/O redirection."""""" assert sys is not None out, err = p.communicate() if redirecting_io(sys=sys): if out: out = unicodify(out) sys.stdout.write(out) out = None <IF_STMT> err = unicodify(err) sys.stderr.write(err) err = None return (out, err)",if err:,if err:,0.8901754397387753,0.9184043388013005,True
4724,"def __exit__(self, *args, **kwargs): self._samples_cache = {} if is_validation_enabled() and isinstance(self.prior, dict): extra = set(self.prior) - self._param_hits <IF_STMT> warnings.warn(""pyro.module prior did not find params ['{}']. Did you instead mean one of ['{}']?"".format(""', '"".join(extra), ""', '"".join(self._param_misses))) return super().__exit__(*args, **kwargs)",if extra:,if extra:,0.7678960336186741,0.9051034981560222,True
4725,def __download_thread(self): while True: <IF_STMT> self.__current_download = self.__queue.get() self.__download_file(self.__current_download) time.sleep(0.1),if not self.__queue.empty():,if self.__current_download is None:,0.7106874213127058,0.48442732379638637,False
4726,"def plot_timer_command(args): import nnabla.monitor as M format_unit = dict(s='seconds', m='minutes', h='hours', d='days') if not args.ylabel: <IF_STMT> args.ylabel = 'Total elapsed time [{}]'.format(format_unit[args.time_unit]) else: args.ylabel = 'Elapsed time [{}/iter]'.format(format_unit[args.time_unit]) plot_any_command(args, M.plot_time_elapsed, dict(elapsed=args.elapsed, unit=args.time_unit)) return True",if args.elapsed:,if args.elapsed:,0.7039361023967848,0.8935248372106969,True
4727,"def resolve_page(root: ChannelContext[models.MenuItem], info, **kwargs): if root.node.page_id: requestor = get_user_or_app_from_context(info.context) requestor_has_access_to_all = requestor.is_active and requestor.has_perm(PagePermissions.MANAGE_PAGES) return PageByIdLoader(info.context).load(root.node.page_id).then(lambda page: page <IF_STMT> else None) return None",if requestor_has_access_to_all or page.is_visible,if requestor_has_access_to_all,0.8932295850990222,0.8466657105524215,False
4728,"def _certonly_new_request_common(self, mock_client, args=None): with mock.patch('certbot._internal.main._find_lineage_for_domains_and_certname') as mock_renewal: mock_renewal.return_value = ('newcert', None) with mock.patch('certbot._internal.main._init_le_client') as mock_init: mock_init.return_value = mock_client <IF_STMT> args = [] args += '-d foo.bar -a standalone certonly'.split() self._call(args)",if args is None:,if args is None:,0.7685714549445092,0.828399516355805,True
4729,"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError('MultiDict can only be called with one positional argument') if args: <IF_STMT> items = list(args[0].iteritems()) elif hasattr(args[0], 'items'): items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())","if hasattr(args[0], 'iteritems'):","if hasattr(args[0], 'iteritems'):",0.6494910583325261,0.897752847848028,True
4730,"def test08_ExceptionTypes(self): self.assertTrue(issubclass(db.DBError, Exception)) for i, j in db.__dict__.items(): <IF_STMT> self.assertTrue(issubclass(j, db.DBError), msg=i) if i not in ('DBKeyEmptyError', 'DBNotFoundError'): self.assertFalse(issubclass(j, KeyError), msg=i) self.assertTrue(issubclass(db.DBKeyEmptyError, KeyError)) self.assertTrue(issubclass(db.DBNotFoundError, KeyError))",if i.startswith('DB') and i.endswith('Error'):,"if i not in ('DBError', 'DBNotFoundError'):",0.8902271509425385,0.7424213297217366,False
4731,"def _delegate_to_sinks(self, value: Any) -> None: for sink in self._sinks: if isinstance(sink, AgentT): await sink.send(value=value) <IF_STMT> await cast(TopicT, sink).send(value=value) else: await maybe_async(cast(Callable, sink)(value))","elif isinstance(sink, ChannelT):","elif isinstance(sink, TopicT):",0.6208514345939918,0.80377750806414,False
4732,"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError('start_tag not found') depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 <IF_STMT> depth -= 1 if depth == 0: break sel = str_in[start_pos + 1:pos] return sel",elif str_in[pos] == end_tag:,if str_in[pos + 1] == end_tag:,0.8901516388252482,0.8688589397154922,False
4733,"def confirm(request): details = request.session.get('reauthenticate') if not details: return redirect('home') request.user = User.objects.get(pk=details['user_pk']) if request.method == 'POST': confirm_form = PasswordConfirmForm(request, request.POST) <IF_STMT> request.session.pop('reauthenticate') request.session['reauthenticate_done'] = True return redirect('social:complete', backend=details['backend']) else: confirm_form = PasswordConfirmForm(request) context = {'confirm_form': confirm_form} context.update(details) return render(request, 'accounts/confirm.html', context)",if confirm_form.is_valid():,if confirm_form.is_valid():,0.9246739706556477,0.9144061946646023,True
4734,"def verify_credentials(self): if self.enabled: response = requests.get('https://api.exotel.com/v1/Accounts/{sid}'.format(sid=self.account_sid), auth=(self.api_key, self.api_token)) <IF_STMT> frappe.throw(_('Invalid credentials'))",if response.status_code != 200:,if response.status_code != 200 and response.text != self.credentials:,0.5025141772405919,0.4598036015897535,False
4735,"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == 'class': icon = 'class' el<IF_STMT> icon = 'method_priv' elif tok.visibility == 'protected': icon = 'method_prot' else: icon = 'method' crp.set_property('pixbuf', imagelibrary.pixbufs[icon])",if tok.visibility == 'private':,if tok.visibility == 'private':,0.7199087930015347,0.8431339019329497,True
4736,"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: omit = elem.tag == 'kw' and elem.get('type') != 'teardown' start = event == 'start' if omit and start: omitted_kws += 1 if not omitted_kws: yield (event, elem) <IF_STMT> elem.clear() if omit and (not start): omitted_kws -= 1",elif not start:,if event == 'end' and (not start):,0.928363630839353,0.837201280100137,False
4737,"def on_double_click(self, event): path = self.get_selected_path() kind = self.get_selected_kind() name = self.get_selected_name() if kind == 'file': <IF_STMT> self.open_file(path) else: self.open_path_with_system_app(path) elif kind == 'dir': self.request_focus_into(path) return 'break'",if self.should_open_name_in_thonny(name):,if name == 'file':,0.8363227024495695,0.8038019482772603,False
4738,"def search_cve(db: DatabaseInterface, product: Product) -> dict: result = {} for query_result in db.fetch_multiple(QUERIES['cve_lookup']): cve_entry = CveDbEntry(*query_result) <IF_STMT> result[cve_entry.cve_id] = {'score2': cve_entry.cvss_v2_score, 'score3': cve_entry.cvss_v3_score, 'cpe_version': build_version_string(cve_entry)} return result","if _product_matches_cve(product, cve_entry):",if cve_entry.product == product:,0.7460616772377683,0.8105932471967202,False
4739,"def find_go_files_mtime(app_files): files, mtime = ([], 0) for f, mt in app_files.items(): if not f.endswith('.go'): continue <IF_STMT> continue files.append(f) mtime = max(mtime, mt) return (files, mtime)",if APP_CONFIG.nobuild_files.match(f):,if not os.path.exists(f):,0.7671468638440676,0.8266114125804572,False
4740,"def wrapper(filename): mtime = getmtime(filename) with lock: <IF_STMT> old_mtime, result = cache.pop(filename) if old_mtime == mtime: cache[filename] = (old_mtime, result) return result result = function(filename) with lock: cache[filename] = (mtime, result) if len(cache) > max_size: cache.popitem(last=False) return result",if filename in cache:,if len(cache) > max_size:,0.8729539540423674,0.8592377270804451,False
4741,"def Tokenize(s): for item in TOKEN_RE.findall(s): item = cast(TupleStr4, item) <IF_STMT> typ = 'number' val = item[0] elif item[1]: typ = 'name' val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",if item[0]:,if item[0]:,0.6878389514638734,0.9184043388013005,True
4742,"def _show_encoders(self, *args, **kwargs): if issubclass(self.current_module.__class__, BasePayload): encoders = self.current_module.get_encoders() <IF_STMT> headers = ('Encoder', 'Name', 'Description') print_table(headers, *encoders, max_column_length=100) return print_error('No encoders available')",if encoders:,if encoders:,0.8228078607005154,0.839587623092576,True
4743,"def __init__(self): Builder.__init__(self, commandName='VCExpress.exe', formatName='msvcProject') for key in ['VS90COMNTOOLS', 'VC80COMNTOOLS', 'VC71COMNTOOLS']: <IF_STMT> self.programDir = os.path.join(os.environ[key], '..', 'IDE') if self.programDir is None: for version in ['9.0', '8', '.NET 2003']: msvcDir = 'C:\\Program Files\\Microsoft Visual Studio %s\\Common7\\IDE' % version if os.path.exists(msvcDir): self.programDir = msvcDir",if os.environ.has_key(key):,if os.environ.get(key):,0.9141440566957999,0.9144061946646023,False
4744,"def _inner(*args, **kwargs): component_manager = args[0].component_manager for condition_name in condition_names: condition_result, err_msg = component_manager.evaluate_condition(condition_name) <IF_STMT> raise ComponentStartConditionNotMetError(err_msg) if not component_manager.all_components_running(*components): raise ComponentsNotStartedError(f'the following required components have not yet started: {json.dumps(components)}') return method(*args, **kwargs)",if not condition_result:,if not condition_result:,0.921439912746018,0.8635707684233572,True
4745,"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <IF_STMT> return self.tk.getdouble(svalue) else: return self.tk.getint(svalue) except (ValueError, TclError): pass return value",elif '.' in svalue:,"elif isinstance(svalue, float):",0.6673551235544445,0.8390782502060267,False
4746,"def check_songs(): desc = numeric_phrase('%d song', '%d songs', len(songs)) with Task(_('Rescan songs'), desc) as task: task.copool(check_songs) for i, song in enumerate(songs): song = song._song <IF_STMT> app.library.reload(song) task.update((float(i) + 1) / len(songs)) yield",if song in app.library:,if app.library:,0.7993452730191064,0.8866029039778043,False
4747,"def initialize(self): nn.init.xavier_uniform_(self.linear.weight.data) if self.linear.bias is not None: self.linear.bias.data.uniform_(-1.0, 1.0) if self.self_layer: nn.init.xavier_uniform_(self.linear_self.weight.data) <IF_STMT> self.linear_self.bias.data.uniform_(-1.0, 1.0)",if self.linear_self.bias is not None:,if self.linear_self.bias is not None:,0.8552940753822671,0.6553609623522636,True
4748,"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = '' result = test(value) <IF_STMT> if result: return not self.inverse elif not result: return self.inverse if self.any_match: return self.inverse else: return not self.inverse",if self.any_match:,if self.any_match:,0.6274280659346549,0.9076141716697395,True
4749,"def toterminal(self, tw): for element in self.chain: element[0].toterminal(tw) <IF_STMT> tw.line('') tw.line(element[2], yellow=True) super(ExceptionChainRepr, self).toterminal(tw)",if element[2] is not None:,if element[1] == 'Exception':,0.5111470463136254,0.630190855592386,False
4750,"def runMainLoop(self): """"""The curses gui main loop."""""" self.curses_app = LeoApp() stdscr = curses.initscr() if 1: self.dump_keys() try: self.curses_app.run() finally: curses.nocbreak() stdscr.keypad(0) curses.echo() curses.endwin() <IF_STMT> g.pr('Exiting Leo...')",if 'shutdown' in g.app.debug:,if self.curses_app.is_alive():,0.925285794244336,0.8701761846085435,False
4751,"def test_chunkcoding(self): for native, utf8 in zip(*[StringIO(f).readlines() for f in self.tstring]): u = self.decode(native)[0] self.assertEqual(u, utf8.decode('utf-8')) <IF_STMT> self.assertEqual(native, self.encode(u)[0])",if self.roundtriptest:,if self.tstring:,0.6322752843762148,0.8193882146581177,False
4752,"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): <IF_STMT> self.sanitize_allowlist.append(line.strip()) except OSError: if explicit: log.warning(""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file)",if not line.startswith('#'):,if line.startswith('--'):,0.7720582298730999,0.9051034981560222,False
4753,"def get_all_extensions(subtree=None): if subtree is None: subtree = full_extension_tree() result = [] if isinstance(subtree, dict): for value in subtree.values(): if isinstance(value, dict): result += get_all_extensions(value) <IF_STMT> result += value.extensions elif isinstance(value, (list, tuple)): result += value elif isinstance(subtree, (ContentTypeMapping, ContentTypeDetector)): result = subtree.extensions elif isinstance(subtree, (list, tuple)): result = subtree return result","elif isinstance(value, (ContentTypeMapping, ContentTypeDetector)):","elif isinstance(value, ContentTypeMapping):",0.9322394416133083,0.9134996171406936,False
4754,"def _configuration_dict_to_commandlist(name, config_dict): command_list = ['config:%s' % name] for key, value in config_dict.items(): <IF_STMT> if value: b = 'true' else: b = 'false' command_list.append('%s:%s' % (key, b)) else: command_list.append('%s:%s' % (key, value)) return command_list",if type(value) is bool:,if key == 'enabled':,0.8093364168316394,0.8431339019329497,False
4755,"def _RewriteModinfo(self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None): new_modinfo = '' for line in modinfo.split('\x00'): <IF_STMT> continue if to_remove and line.split('=')[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split('=')[0]) if line.startswith('vermagic'): line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + '\x00' return new_modinfo",if not line:,if line.startswith('#'):,0.9248168329634512,0.9144061946646023,False
4756,"def zip_random_open_test(self, f, compression): self.make_test_archive(f, compression) with zipfile.ZipFile(f, 'r', compression) as zipfp: zipdata1 = [] with zipfp.open(TESTFN) as zipopen1: while True: read_data = zipopen1.read(randint(1, 1024)) <IF_STMT> break zipdata1.append(read_data) testdata = ''.join(zipdata1) self.assertEqual(len(testdata), len(self.data)) self.assertEqual(testdata, self.data)",if not read_data:,if not read_data:,0.7869073617525154,0.8713933650206428,True
4757,"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update <IF_STMT> self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",if self._call_count > self.ctl or age > self.ttl:,if age < 0:,0.7517101807860269,0.8832000938217648,False
4758,"def on_data(res): if terminate.is_set(): return if args.strings and (not args.no_content): if type(res) == tuple: f, v = res <IF_STMT> f = f.encode('utf-8') if type(v) == unicode: v = v.encode('utf-8') self.success('{}: {}'.format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",if type(f) == unicode:,if type(f) == unicode:,0.9179582426351413,0.8592377270804451,True
4759,"def _finalize_setup_keywords(self): for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'): value = getattr(self, ep.name, None) <IF_STMT> ep.require(installer=self.fetch_build_egg) ep.load()(self, ep.name, value)",if value is not None:,if value is not None:,0.642910921334756,0.6383240325919926,True
4760,"def test_attributes_types(self): if not self.connection.strategy.pooled: <IF_STMT> self.connection.refresh_server_info() self.assertEqual(type(self.connection.server.schema.attribute_types['cn']), AttributeTypeInfo)",if not self.connection.server.info:,if self.connection.server.schema.attribute_types['cn'] is None:,0.44117016661198516,0.4240125351805037,False
4761,"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier['type'] == 'Identifier': return literal_or_identifier['name'] elif literal_or_identifier['type'] == 'Literal': k = literal_or_identifier['value'] if isinstance(k, float): return unicode(float_repr(k)) elif 'regex' in literal_or_identifier: return compose_regex(k) <IF_STMT> return 'true' if k else 'false' elif k is None: return 'null' else: return unicode(k)","elif isinstance(k, bool):","elif isinstance(k, bool):",0.9261076298896831,0.9062841320510342,True
4762,"def list2rec(x, test=False): if test: vid = '{}_{:06d}_{:06d}'.format(x[0], int(x[1]), int(x[2])) label = -1 return (vid, label) else: vid = '{}_{:06d}_{:06d}'.format(x[1], int(x[2]), int(x[3])) <IF_STMT> vid = '{}/{}'.format(convert_label(x[0]), vid) else: assert level == 1 label = class_mapping[convert_label(x[0])] return (vid, label)",if level == 2:,if level == 0:,0.9096579562924689,0.8592377270804451,False
4763,"def _expand_env(self, snapcraft_yaml): environment_keys = ['name', 'version'] for key in snapcraft_yaml: <IF_STMT> continue replacements = environment_to_replacements(get_snapcraft_global_environment(self.project)) snapcraft_yaml[key] = replace_attr(snapcraft_yaml[key], replacements) return snapcraft_yaml",if any((key == env_key for env_key in environment_keys)):,if key not in environment_keys:,0.7644007671552239,0.7297349727547102,False
4764,"def enableCtrls(self): for data in self.storySettingsData: name = data['name'] <IF_STMT> if 'requires' in data: set = self.getSetting(data['requires']) for i in self.ctrls[name]: i.Enable(set not in ['off', 'false', '0'])",if name in self.ctrls:,if name in self.ctrls:,0.6488334124062058,0.8038019482772603,True
4765,"def __init__(self, *args, **kwargs): super(ChallengePhaseCreateSerializer, self).__init__(*args, **kwargs) context = kwargs.get('context') if context: challenge = context.get('challenge') <IF_STMT> kwargs['data']['challenge'] = challenge.pk test_annotation = context.get('test_annotation') if test_annotation: kwargs['data']['test_annotation'] = test_annotation",if challenge:,if challenge:,0.6606421542641439,0.8645707301556367,True
4766,def set_inactive(self): for title in self.gramplet_map: if self.gramplet_map[title].pui: <IF_STMT> self.gramplet_map[title].pui.active = False,if self.gramplet_map[title].gstate != 'detached':,if self.gramplet_map[title].pui.active:,0.7938636128797516,0.6703420896351792,False
4767,"def authenticate(username, password): try: u = User.objects.get(username=username) <IF_STMT> userLogger.info('User logged in : %s', username) return u else: userLogger.warn('Attempt to log in to : %s', username) return False except DoesNotExist: return False","if check_password_hash(u.password, password):",if u.password == password:,0.8368998737892243,0.828399516355805,False
4768,def _check_date(self): if not self.value: return None if not self.allow_date_in_past: if self.value < self.date_or_datetime().today(): <IF_STMT> self.value = self.date_or_datetime().today() else: self.value = self.date_or_datetime().today() + datetime.timedelta(1),if self.allow_todays_date:,if self.allow_date_in_past:,0.7506130964351843,0.8466657105524215,False
4769,"def update(self, E=None, **F): if E: <IF_STMT> for k in E: self[k] = E[k] else: for k, v in E: self[k] = v for k in F: self[k] = F[k]","if hasattr(E, 'keys'):","if isinstance(E, dict):",0.904284115818554,0.8498644646741501,False
4770,"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): <IF_STMT> quotas_ok[quota] = 0 break avail = qa.results[quota] if avail[1] is not None and avail[1] < count: quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",if count <= 0:,if count == 0:,0.938932398412078,0.8944264839442453,False
4771,"def gen_env_vars(): for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)): is_atty = fd.isatty() yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty))) <IF_STMT> yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b'')",if is_atty:,if is_atty:,0.6376523999312012,0.8318180062062374,True
4772,"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, 'utf-8') elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) <IF_STMT> k = str(k, 'utf-8') r[k] = v return r","if isinstance(k, bytes):","elif isinstance(k, bytes):",0.8667016204523652,0.8953711787948615,False
4773,"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] <IF_STMT> dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",if attr not in node.attributes:,if attr not in node.attributes:,0.9183750200547316,0.8677319190106252,True
4774,"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: <IF_STMT> i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",if dsn[i].isspace():,if not dsn[i]:,0.7763052877027271,0.9182210682909737,False
4775,"def connect(self, buttons): for button in buttons: assert button is not None handled = False for handler_idx in range(0, len(self.__signal_handlers)): obj_class, signal, handler, handler_id = self.__signal_handlers[handler_idx] <IF_STMT> handler_id = button.connect(signal, handler) handled = True self.__signal_handlers[handler_idx] = (obj_class, signal, handler, handler_id) assert handled","if isinstance(button, obj_class):",if obj_class is None:,0.7959777775273933,0.8723360571509826,False
4776,"def _parse_display(display): """"""Parse an X11 display value"""""" try: host, dpynum = display.rsplit(':', 1) if host.startswith('[') and host.endswith(']'): host = host[1:-1] idx = dpynum.find('.') <IF_STMT> screen = int(dpynum[idx + 1:]) dpynum = dpynum[:idx] else: screen = 0 except (ValueError, UnicodeEncodeError): raise ValueError('Invalid X11 display') from None return (host, dpynum, screen)",if idx >= 0:,if idx >= 0:,0.6760695552509278,0.8902056737869248,True
4777,"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob('*'): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) <IF_STMT> os.remove(fn_full) elif fn.endswith('.md'): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",elif fn.endswith('.png'):,elif fn.endswith('.py'):,0.9075700334084094,0.8645707301556367,False
4778,"def _sync_get(self, identifier, *args, **kw): self._mutex.acquire() try: try: <IF_STMT> return self._values[identifier] else: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value except KeyError: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value finally: self._mutex.release()",if identifier in self._values:,if identifier in self._values:,0.8888047426234945,0.8431339019329497,True
4779,"def _query_fd(self): if self.stream is None: self._last_stat = (None, None) else: try: st = os.stat(self._filename) except OSError: e = sys.exc_info()[1] <IF_STMT> raise self._last_stat = (None, None) else: self._last_stat = (st[stat.ST_DEV], st[stat.ST_INO])",if e.errno != errno.ENOENT:,if e.errno != errno.EACCES:,0.7041477144338121,0.828399516355805,False
4780,"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = '' if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) <IF_STMT> place_title = place_displayer.display(self.dbstate.db, place) if place_title != '': if len(place_title) > 25: text = place_title[:24] + '...' else: text = place_title return text",if place:,if place:,0.9318095409315227,0.9076141716697395,True
4781,"def test_decoder_state(self): u = 'abc123' for encoding in all_unicode_encodings: <IF_STMT> self.check_state_handling_decode(encoding, u, u.encode(encoding)) self.check_state_handling_encode(encoding, u, u.encode(encoding))",if encoding not in broken_unicode_with_stateful:,if encoding in u:,0.5972916565787008,0.674945488826271,False
4782,"def cleanup(self): if os.path.exists(self.meta_gui_dir): for f in os.listdir(self.meta_gui_dir): <IF_STMT> os.remove(os.path.join(self.meta_gui_dir, f))",if os.path.splitext(f)[1] == '.desktop':,"if f.endswith('.py') and (not os.path.exists(os.path.join(self.meta_gui_dir, f))):",0.513758644400755,0.48415247130346006,False
4783,"def _have_applied_incense(self): for applied_item in inventory.applied_items().all(): self.logger.info(applied_item) <IF_STMT> mins = format_time(applied_item.expire_ms * 1000) self.logger.info('Not applying incense, currently active: %s, %s minutes remaining', applied_item.item.name, mins) return True else: self.logger.info('') return False return False",if applied_item.expire_ms > 0:,if applied_item.active:,0.6148148230364671,0.8866029039778043,False
4784,"def get_closest_point(self, point): point = to_point(point) cp, cd = (None, None) for p0, p1 in iter_pairs(self.pts, self.connected): diff = p1 - p0 l = diff.length d = diff / l pp = p0 + d * max(0, min(l, (point - p0).dot(d))) dist = (point - pp).length <IF_STMT> cp, cd = (pp, dist) return cp",if not cp or dist < cd:,if dist > cp:,0.7944623429188084,0.9001816649635144,False
4785,"def process_return(lines): for line in lines: m = re.fullmatch('(?P<param>\\w+)\\s+:\\s+(?P<type>[\\w.]+)', line) <IF_STMT> yield f""**{m['param']}** : :class:`~{m['type']}`"" else: yield line",if m:,if m:,0.6544901665895371,0.7912619863720214,True
4786,"def _classify(nodes_by_level): missing, invalid, downloads = ([], [], []) for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) <IF_STMT> invalid.append(node) elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD): downloads.append(node) return (missing, invalid, downloads)",elif node.binary == BINARY_INVALID:,elif node.binary == BINARY_INVALID:,0.9141601337467824,0.8431339019329497,True
4787,"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: <IF_STMT> date_hdr = date_hdr.split(';')[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) if msg_ts > time.time() + 24 * 3600 or msg_ts < 1: return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",if ';' in date_hdr:,if ';' in date_hdr:,0.8566176719231047,0.8752376177722327,True
4788,"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): <IF_STMT> changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",if value != 1:,if value != 0:,0.8973151787934146,0.8806615362338783,False
4789,"def _rewrite_prepend_append(self, string, prepend, append=None): if append is None: append = prepend if not isinstance(string, StringElem): string = StringElem(string) string.sub.insert(0, prepend) if unicode(string).endswith(u'\n'): try: lastnode = string.flatten()[-1] <IF_STMT> lastnode.sub[-1] = lastnode.sub[-1].rstrip(u'\n') except IndexError: pass string.sub.append(append + u'\n') else: string.sub.append(append) return string","if isinstance(lastnode.sub[-1], unicode):",if lastnode.sub[-1].startswith(u'\n'):,0.7416115375114903,0.9122561819614461,False
4790,"def parse_indentless_sequence_entry(self): if self.check_token(BlockEntryToken): token = self.get_token() <IF_STMT> self.states.append(self.parse_indentless_sequence_entry) return self.parse_block_node() else: self.state = self.parse_indentless_sequence_entry return self.process_empty_scalar(token.end_mark) token = self.peek_token() event = SequenceEndEvent(token.start_mark, token.start_mark) self.state = self.states.pop() return event","if not self.check_token(BlockEntryToken, KeyToken, ValueToken, BlockEndToken):",if token.end_mark == 'block':,0.7074740315553611,0.8169276475307028,False
4791,"def walk_directory(directory, verbose=False): """"""Iterates a directory's text files and their contents."""""" for dir_path, _, filenames in os.walk(directory): for filename in filenames: file_path = os.path.join(dir_path, filename) if os.path.isfile(file_path) and (not filename.startswith('.')): with io.open(file_path, 'r', encoding='utf-8') as file: <IF_STMT> print('Reading {}'.format(filename)) doc_text = file.read() yield (filename, doc_text)",if verbose:,if verbose:,0.9283479998588038,0.9202663016973823,True
4792,"def set_bounds(self, x, y, width, height): if self.native: <IF_STMT> vertical_shift = self.frame.vertical_shift else: vertical_shift = 0 self.native.Size = Size(width, height) self.native.Location = Point(x, y + vertical_shift)",if self.interface.parent is None:,if self.frame:,0.874882187405724,0.8590888738245122,False
4793,"def _check_x11(self, command=None, *, exc=None, exit_status=None, **kwargs): """"""Check requesting X11 forwarding"""""" with (yield from self.connect()) as conn: <IF_STMT> with self.assertRaises(exc): yield from _create_x11_process(conn, command, **kwargs) else: proc = (yield from _create_x11_process(conn, command, **kwargs)) yield from proc.wait() self.assertEqual(proc.exit_status, exit_status) yield from conn.wait_closed()",if exc:,if exc:,0.9269764314497893,0.9122561819614461,True
4794,"def repr(self): try: <IF_STMT> from infogami.infobase.utils import prepr return prepr(self.obj) else: return repr(self.obj) except: return 'failed' return render_template('admin/memory/object', self.obj)","if isinstance(self.obj, (dict, web.threadeddict)):",if self.obj.is_prepr:,0.8648321280020081,0.803154665668484,False
4795,"def add(self, tag, values): if tag not in self.different: if tag not in self: self[tag] = values <IF_STMT> self.different.add(tag) self[tag] = [''] self.counts[tag] += 1",elif self[tag] != values:,if tag not in self.counts:,0.5826898934704196,0.759907656827929,False
4796,"def _on_geturl(self, event): selected = self._status_list.get_selected() if selected != -1: object_id = self._status_list.GetItemData(selected) download_item = self._download_list.get_item(object_id) url = download_item.url <IF_STMT> clipdata = wx.TextDataObject() clipdata.SetText(url) wx.TheClipboard.Open() wx.TheClipboard.SetData(clipdata) wx.TheClipboard.Close()",if not wx.TheClipboard.IsOpened():,if url != '':,0.8433861529607931,0.8038019482772603,False
4797,"def escape2null(text): """"""Return a string with escape-backslashes converted to nulls."""""" parts = [] start = 0 while True: found = text.find('\\', start) <IF_STMT> parts.append(text[start:]) return ''.join(parts) parts.append(text[start:found]) parts.append('\x00' + text[found + 1:found + 2]) start = found + 2",if found == -1:,if found == -1:,0.7628469918988159,0.8627586293513119,True
4798,def _process_inner_views(self): for view in self.baseviews: for inner_class in view.get_uninit_inner_views(): for v in self.baseviews: <IF_STMT> view.get_init_inner_views().append(v),"if isinstance(v, inner_class) and v not in view.get_init_inner_views():",if v.get_class() == inner_class:,0.607190576945043,0.7252761279126532,False
4799,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_url(d.getPrefixedString()) continue <IF_STMT> self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 18:,if tt == 18:,0.9386947629071348,0.8592377270804451,True
4800,"def test_sample_output(): comment = 'SAMPLE OUTPUT' skip_files = ['__init__.py'] errors = [] for _file in sorted(MODULE_PATH.iterdir()): if _file.suffix == '.py' and _file.name not in skip_files: with _file.open() as f: <IF_STMT> errors.append((comment, _file)) if errors: line = 'Missing sample error(s) detected!\n\n' for error in errors: line += '`{}` is not in module `{}`\n'.format(*error) print(line[:-1]) assert False",if comment not in f.read():,if f.read() == comment:,0.7887997815207507,0.9019629427251674,False
4801,"def _get_planner(name, path, source): for klass in _planners: <IF_STMT> LOG.debug('%r accepted %r (filename %r)', klass, name, path) return klass LOG.debug('%r rejected %r', klass, name) raise ansible.errors.AnsibleError(NO_METHOD_MSG + repr(invocation))","if klass.detect(path, source):","if klass.accept(name, path, source):",0.8666422944641189,0.8105932471967202,False
4802,"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(' , ') else: hasConst = not (self._const.__class__ in native_numeric_types and self._const == 0) <IF_STMT> idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0): ostream.write(' - ') else: ostream.write(' + ')",if hasConst:,if hasConst:,0.7308483284781317,0.9284304001296656,True
4803,"def cluster_info_query(self): if self._major_version >= 90600: extra = ', CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END, slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver()' <IF_STMT> extra = 'timeline_id' + extra + ', pg_catalog.pg_control_checkpoint()' else: extra = '0' + extra else: extra = '0, NULL, NULL, NULL' return ('SELECT ' + self.TL_LSN + ', {2}').format(self.wal_name, self.lsn_name, extra)",if self.role == 'standby_leader':,if self._major_version >= 90600:,0.7346013556184847,0.9019629427251674,False
4804,"def __init__(self, *args, **kwargs): self.country = kwargs.pop('country') self.fields_needed = kwargs.pop('fields_needed', []) super(DynamicManagedAccountForm, self).__init__(*args, **kwargs) for f in self.fields_needed: <IF_STMT> field_name, field = FIELDS_BY_COUNTRY[self.country][f] self.fields[field_name] = field","if f in FIELDS_BY_COUNTRY.get(self.country, {}):",if f in FIELDS_BY_COUNTRY[self.country]:,0.8694629811827594,0.7965020533851944,False
4805,"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): <IF_STMT> query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) if not query_map[alias]: query_map.pop(alias) return query_map",if vm_details == 'Absent':,if vm_details['vm_name'] == query_map[alias][vm_name]:,0.9040829910221012,0.8385130047130208,False
4806,"def on_strokes_edited(self): strokes = self._strokes() if strokes: translation = self._engine.raw_lookup(strokes) <IF_STMT> fmt = _('{strokes} maps to {translation}') else: fmt = _('{strokes} is not in the dictionary') info = self._format_label(fmt, (strokes,), translation) else: info = '' self.strokes_info.setText(info)",if translation is not None:,if translation:,0.7357371303535093,0.8996480074924822,False
4807,def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError('cannot release un-acquired lock') assert self.count > 0 self.count -= 1 <IF_STMT> self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release(),if self.count == 0:,if self.count == 0:,0.9162547068567991,0.8385130047130208,True
4808,"def _cat_blob(self, gcs_uri): """""":py:meth:`cat_file`, minus decompression."""""" blob = self._get_blob(gcs_uri) if not blob: return start = 0 while True: end = start + _CAT_CHUNK_SIZE try: chunk = blob.download_as_string(start=start, end=end) except google.api_core.exceptions.RequestRangeNotSatisfiable: return yield chunk <IF_STMT> return start = end",if len(chunk) < _CAT_CHUNK_SIZE:,if not chunk:,0.8375635244128459,0.8815741981066073,False
4809,"def device_iter(**kwargs): for dev in backend.enumerate_devices(): d = Device(dev, backend) tests = (val == _try_getattr(d, key) for key, val in kwargs.items()) <IF_STMT> yield d",if _interop._all(tests) and (custom_match is None or custom_match(d)):,if tests:,0.6651109709935528,0.8588510825398774,False
4810,"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith('.vtkjs'): if isfile(self.object): with open(self.object, 'rb') as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() <IF_STMT> vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs","elif hasattr(self.object, 'read'):","if isinstance(self.object, bytes):",0.800764993774988,0.8928756684056034,False
4811,"def _execute_with_error(command, error, message): try: cli.invocation = cli.invocation_cls(cli_ctx=cli, parser_cls=cli.parser_cls, commands_loader_cls=cli.commands_loader_cls, help_cls=cli.help_cls) cli.invocation.execute(command.split()) except CLIError as ex: <IF_STMT> raise AssertionError('{}\nExpected: {}\nActual: {}'.format(message, error, ex)) return except Exception as ex: raise ex raise AssertionError(""exception not raised for '{0}'"".format(message))",if error not in str(ex):,if error:,0.8655871981896281,0.8996480074924822,False
4812,"def ray_intersection(self, p, line): p = Vector(center(line.sites)) min_r = BIG_FLOAT nearest = None for v_i, v_j in self.edges: bound = LineEquation2D.from_two_points(v_i, v_j) intersection = bound.intersect_with_line(line) if intersection is not None: r = (p - intersection).length <IF_STMT> nearest = intersection min_r = r return nearest",if r < min_r:,if r < min_r:,0.8691453446345764,0.8780099567239787,True
4813,"def CalculateChecksum(data): if isinstance(data, bytearray): total = sum(data) elif isinstance(data, bytes): <IF_STMT> total = sum(map(ord, data)) else: total = sum(data) else: total = sum(map(ord, data)) return total & 4294967295","if data and isinstance(data[0], bytes):",if len(data) == 4:,0.6745501250983552,0.8169276475307028,False
4814,"def __mul__(self, other: Union['Tensor', float]) -> 'Tensor': if isinstance(other, Tensor): <IF_STMT> errstr = f""Given backens are inconsistent. Found '{self.backend.name}'and '{other.backend.name}'"" raise ValueError(errstr) other = other.array array = self.backend.multiply(self.array, other) return Tensor(array, backend=self.backend)",if self.backend.name != other.backend.name:,if self.backend.name != other.backend.name:,0.7137326088826215,0.8336104423443033,True
4815,"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = (-1, 0) try: start = self.items.index(self._selected) i = start + direction except: pass while True: <IF_STMT> self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",if i == start:,if i == start:,0.807674326309656,0.9128479730518225,True
4816,"def resolve_none(self, data): for tok_idx in range(len(data)): for feat_idx in range(len(data[tok_idx])): <IF_STMT> data[tok_idx][feat_idx] = '_' return data",if data[tok_idx][feat_idx] is None:,if data[tok_idx][feat_idx] == '':,0.763975675713663,0.693395566222006,False
4817,"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError('{0} is not a field of {1}'.format(n, expr)) if not isinstance(n, _strtypes): raise TypeError('on must be a name or field, not: {0}'.format(n)) <IF_STMT> raise ValueError('{0} is not a field of {1}'.format(n, expr)) append(n) return Distinct(expr, tuple(_on))",elif n not in fields:,if n not in fields:,0.8557275437338118,0.8986910033264952,False
4818,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <IF_STMT> length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 10:,if tt == 10:,0.771303035423261,0.8555308664663046,True
4819,"def func_std_string(func_name): if func_name[:2] == ('~', 0): name = func_name[2] <IF_STMT> return '{%s}' % name[1:-1] else: return name else: return '%s:%d(%s)' % func_name",if name.startswith('<') and name.endswith('>'):,if name.startswith('.'):,0.5958385843284766,0.839587623092576,False
4820,"def f(): try: for n in cycle([1, 19]): s = bufio.read(n) <IF_STMT> break results.append(s) except Exception as e: errors.append(e) raise",if not s:,if not s:,0.5950676674355165,0.7739321540474097,True
4821,"def stop(self): try: self.rpcserver.stop() if self.backend_rpcserver: self.backend_rpcserver.stop() <IF_STMT> self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",if self.cluster_rpcserver:,if self.cluster_rpcserver:,0.8924924547783142,0.8232490471721702,True
4822,"def download(cls, architecture, path='./'): if cls.sanity_check(architecture): architecture_file = download_file(cls.architecture_map[architecture], directory=path) <IF_STMT> return None print('Coreml model {} is saved in [{}]'.format(architecture, path)) return architecture_file else: return None",if not architecture_file:,if not architecture_file:,0.5939961560529781,0.8266114125804572,True
4823,"def opps_output_converter(kpt_list): kpts = [] mpii_keys = to_opps_converter.keys() for mpii_idx in range(0, 16): <IF_STMT> model_idx = to_opps_converter[mpii_idx] x, y = kpt_list[model_idx] if x < 0 or y < 0: kpts += [0.0, 0.0, -1.0] else: kpts += [x, y, 1.0] else: kpts += [0.0, 0.0, -1.0] return kpts",if mpii_idx in mpii_keys:,if mpii_idx in mpii_keys:,0.9126336404580122,0.8879659171421962,True
4824,"def _get_headers(self, headers=None): request_headers = headers or {} if self._client.client.config: config = self._client.client.config if 'Authorization' not in request_headers and config.token: request_headers.update({'Authorization': '{} {}'.format(config.authentication_type, config.token)}) <IF_STMT> request_headers.update({config.header: config.header_service}) return request_headers",if config.header and config.header_service:,elif config.header_service:,0.9217957121697412,0.8743414417652072,False
4825,"def get_last_traded_prices(cls, trading_pairs: List[str]) -> Dict[str, float]: results = dict() async with aiohttp.ClientSession() as client: resp = await client.get(f'{constants.REST_URL}/tickers') resp_json = await resp.json() for trading_pair in trading_pairs: resp_record = [o for o in resp_json <IF_STMT>][0] results[trading_pair] = float(resp_record['price']) return results",if o['symbol'] == convert_to_exchange_trading_pair(trading_pair),if o['trading_pair'] == trading_pair,0.8910095873465418,0.8661072626070159,False
4826,"def reset_two_factor_hotp(): uid = request.form['uid'] otp_secret = request.form.get('otp_secret', None) if otp_secret: user = Journalist.query.get(uid) <IF_STMT> return render_template('admin_edit_hotp_secret.html', uid=uid) db.session.commit() return redirect(url_for('admin.new_user_two_factor', uid=uid)) else: return render_template('admin_edit_hotp_secret.html', uid=uid)","if not validate_hotp_secret(user, otp_secret):",if user and user.secret == otp_secret:,0.6564748323972713,0.7424213297217366,False
4827,"def ctx_for_video(self, vurl): """"""Get a context dict for a given video URL"""""" ctx = self.get_context_dict() for portal, match, context_fn in self.PORTALS: <IF_STMT> try: ctx.update(context_fn(vurl)) ctx['portal'] = portal break except AttributeError: continue return ctx",if match.search(vurl):,if match:,0.8417943267393218,0.8901732118131125,False
4828,"def get(self): name = request.args.get('filename') if name is not None: opts = dict() opts['type'] = 'episode' result = guessit(name, options=opts) res = dict() if 'episode' in result: res['episode'] = result['episode'] else: res['episode'] = 0 if 'season' in result: res['season'] = result['season'] else: res['season'] = 0 <IF_STMT> res['subtitle_language'] = str(result['subtitle_language']) return jsonify(data=res) else: return ('', 400)",if 'subtitle_language' in result:,if 'subtitle_language' in result:,0.6686395055778602,0.9019629427251674,True
4829,"def package_files(package_path, directory_name): paths = [] directory_path = os.path.join(package_path, directory_name) for path, directories, filenames in os.walk(directory_path): relative_path = os.path.relpath(path, package_path) for filename in filenames: <IF_STMT> continue paths.append(os.path.join(relative_path, filename)) return paths",if filename[0] == '.':,if filename.endswith('.py'):,0.921421592606198,0.8787142254774354,False
4830,"def parse_simple(d, data): units = {} for v in data[d]: key = v['name'] if not key: continue key_to_insert = make_key(key) <IF_STMT> index = 2 tmp = f'{key_to_insert}_{index}' while tmp in units: index += 1 tmp = f'{key_to_insert}_{index}' key_to_insert = tmp units[key_to_insert] = v['id'] return units",if key_to_insert in units:,if units:,0.7924551861039312,0.9202663016973823,False
4831,"def parse_clademodelc(branch_type_no, line_floats, site_classes): """"""Parse results specific to the clade model C."""""" if not site_classes or len(line_floats) == 0: return for n in range(len(line_floats)): <IF_STMT> site_classes[n]['branch types'] = {} site_classes[n]['branch types'][branch_type_no] = line_floats[n] return site_classes",if site_classes[n].get('branch types') is None:,if not site_classes.get(n):,0.9095231958761403,0.8713933650206428,False
4832,"def track_modules(self, *modules): """"""Add module names to the tracked list."""""" already_tracked = self.session.GetParameter('autodetect_build_local_tracked') or [] needed = set(modules) if not needed.issubset(already_tracked): needed.update(already_tracked) with self.session as session: session.SetParameter('autodetect_build_local_tracked', needed) for module_name in modules: module_obj = self.GetModuleByName(module_name) <IF_STMT> module_obj.profile = None",if module_obj:,if module_obj:,0.9349543402554664,0.9076141716697395,True
4833,"def set_job_on_hold(self, value, blocking=True): trigger = False if not self._job_on_hold.acquire(blocking=blocking): return False try: <IF_STMT> self._job_on_hold.set() else: self._job_on_hold.clear() if self._job_on_hold.counter == 0: trigger = True finally: self._job_on_hold.release() if trigger: self._continue_sending() return True",if value:,if value:,0.6168750271153726,0.8827916928185874,True
4834,"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: <IF_STMT> p.moveToFirstChild() elif p.hasNext(): p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break p.moveToParent() return p",if p.v.children:,if p.hasFirstChild():,0.761278951450027,0.8743414417652072,False
4835,"def best_image(width, height): image = images[0] for img in images: <IF_STMT> return img elif img.width >= width and img.width * img.height > image.width * image.height: image = img return image",if img.width == width and img.height == height:,if img.height <= height:,0.7130691435190065,0.8228500218338367,False
4836,"def _check_input_types(self): if len(self.base_features) == 0: return True input_types = self.primitive.input_types if input_types is not None: <IF_STMT> input_types = [input_types] for t in input_types: zipped = list(zip(t, self.base_features)) if all([issubclass(f.variable_type, v) for v, f in zipped]): return True else: return True return False",if type(input_types[0]) != list:,"if not isinstance(input_types, list):",0.875262989594147,0.8752376177722327,False
4837,"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: if exc_info is None: exc_info = sys.exc_info() el<IF_STMT> app_log.error('Multiple exceptions in yield list', exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)","if not isinstance(e, self.quiet_exceptions):",if len(exc_info) > 1:,0.9454731274896935,0.8902056737869248,False
4838,def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == 'resnet_tf': if step_num < hparams.lrn_step: lrn_rate = hparams.mom_lrn <IF_STMT> lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate,elif step_num < 30000:,elif step_num < 10000:,0.7100078422946012,0.8902056737869248,False
4839,"def topic_exists(self, arn): response = self._conn.get_all_topics() topics = response['ListTopicsResponse']['ListTopicsResult']['Topics'] current_topics = [] if len(topics) > 0: for topic in topics: topic_arn = topic['TopicArn'] current_topics.append(topic_arn) <IF_STMT> return True return False",if arn in current_topics:,if topic_arn in current_topics:,0.9109991741802208,0.8169276475307028,False
4840,"def assertStartsWith(self, expectedPrefix, text, msg=None): if not text.startswith(expectedPrefix): <IF_STMT> text = text[:len(expectedPrefix) + 5] + '...' standardMsg = '{} not found at the start of {}'.format(repr(expectedPrefix), repr(text)) self.fail(self._formatMessage(msg, standardMsg))",if len(expectedPrefix) + 5 < len(text):,if len(text) > len(expectedPrefix):,0.873098697122537,0.8169276475307028,False
4841,"def validate_memory(self, value): for k, v in value.viewitems(): <IF_STMT> continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError('Process types can only contain [a-z]') if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError('Limit format: <number><unit>, where unit = B, K, M or G') return value",if v is None:,if k.startswith('_'):,0.8784461218163611,0.9076141716697395,False
4842,"def open(self) -> 'KeyValueJsonDb': """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): <IF_STMT> raise IOError('%s exists and is not a file' % self._name) try: with open(self._name, 'r') as _in: self.set_records(json.load(_in)) except json.JSONDecodeError: self.commit() else: mkpath(os.path.dirname(self._name)) self.commit() return self",if not os.path.isfile(self._name):,if not os.path.isfile(self._name):,0.9010373327643871,0.8902579342581529,True
4843,"def _calculate(self): before = self.before.data after = self.after.data self.deleted = {} self.updated = {} self.created = after.copy() for path, f in before.items(): <IF_STMT> self.deleted[path] = f continue del self.created[path] if f.mtime < after[path].mtime: self.updated[path] = after[path]",if path not in after:,if f.mtime > after[path].mtime:,0.7557779655539336,0.8516228624291206,False
4844,"def cache_sqs_queues_across_accounts() -> bool: function: str = f'{__name__}.{sys._getframe().f_code.co_name}' accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): <IF_STMT> cache_sqs_queues_for_account.delay(account_id) elif account_id in config.get('celery.test_account_ids', []): cache_sqs_queues_for_account.delay(account_id) stats.count(f'{function}.success') return True",if config.get('environment') == 'prod':,"if account_id in config.get('celery.test_account_ids', []):",0.8740218231131963,0.7765145040967655,False
4845,"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: <IF_STMT> config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",if config is None:,if config is None:,0.9387325288255789,0.8983343737277126,True
4846,"def PopulateProjectId(project_id=None): """"""Fills in a project_id from the boto config file if one is not provided."""""" if not project_id: default_id = boto.config.get_value('GSUtil', 'default_project_id') <IF_STMT> raise ProjectIdException('MissingProjectId') return default_id return project_id",if not default_id:,if default_id is None:,0.6540477363048076,0.8228500218338367,False
4847,"def set(self, name, value): with self._object_cache_lock: old_value = self._object_cache.get(name) ret = not old_value or int(old_value.metadata.resource_version) < int(value.metadata.resource_version) <IF_STMT> self._object_cache[name] = value return (ret, old_value)",if ret:,if ret:,0.7487629058586731,0.8466657105524215,True
4848,"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) <IF_STMT> if i >= len(self.items): i = len(self.items) - 1 self.list.select_set(i)",elif was_selected:,if was_selected:,0.9383003299910797,0.8966773400768917,False
4849,"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) <IF_STMT> paths = add_directory_csv_files(path, paths) elif isfile(path) and path.endswith('.csv'): paths.append(path) return paths",if isdir(path):,if isdir(path):,0.8045775644637887,0.8743414417652072,True
4850,"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): <IF_STMT> if isinstance(value, dict): return GeneralPrivateEndpointClient(key, value['api_version'], value['support_list_or_not'], value['resource_get_api_version']) return value() raise CLIError('Resource type must be one of {}'.format(', '.join(rp_mapping.keys())))",if str.lower(key) == str.lower(resource_provider):,"if isinstance(value, resource_provider):",0.8956191307508865,0.8390782502060267,False
4851,"def compute_rule_hash(self, rule): buf = '%d-%d-%s-' % (rule.get('FromPort', 0) or 0, rule.get('ToPort', 0) or 0, rule.get('IpProtocol', '-1') or '-1') for a, ke in self.RULE_ATTRS: <IF_STMT> continue ev = [e[ke] for e in rule[a]] ev.sort() for e in ev: buf += '%s-' % e return zlib.crc32(buf.encode('ascii')) & 4294967295",if a not in rule:,if ke == 'rule':,0.9093463450178777,0.8856327184319047,False
4852,"def analysis_sucess_metrics(analysis_time: float, allow_exception=False): try: anchore_engine.subsys.metrics.counter_inc(name='anchore_analysis_success') anchore_engine.subsys.metrics.histogram_observe('anchore_analysis_time_seconds', analysis_time, buckets=ANALYSIS_TIME_SECONDS_BUCKETS, status='success') except: <IF_STMT> raise else: logger.exception('Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing')",if allow_exception:,if allow_exception:,0.911815104958501,0.8645707301556367,True
4853,"def decide_file_icon(file): if file.state == File.ERROR: return FileItem.icon_error elif isinstance(file.parent, Track): <IF_STMT> return FileItem.icon_saved elif file.state == File.PENDING: return FileItem.match_pending_icons[int(file.similarity * 5 + 0.5)] else: return FileItem.match_icons[int(file.similarity * 5 + 0.5)] elif file.state == File.PENDING: return FileItem.icon_file_pending else: return FileItem.icon_file",if file.state == File.NORMAL:,if file.state == File.SAVEED:,0.9129835597185094,0.8661072626070159,False
4854,"def deleteMenu(self, menuName): try: menu = self.getMenu(menuName) <IF_STMT> self.destroy(menu) self.destroyMenu(menuName) else: g.es(""can't delete menu:"", menuName) except Exception: g.es('exception deleting', menuName, 'menu') g.es_exception()",if menu:,if menu:,0.564929828311805,0.8318180062062374,True
4855,"def parser(cls, buf): type_, code, csum = struct.unpack_from(cls._PACK_STR, buf) msg = cls(type_, code, csum) offset = cls._MIN_LEN if len(buf) > offset: cls_ = cls._ICMPV6_TYPES.get(type_, None) <IF_STMT> msg.data = cls_.parser(buf, offset) else: msg.data = buf[offset:] return (msg, None, None)",if cls_:,if cls_:,0.8676763357988759,0.9051034981560222,True
4856,"def _load_dataset_area(self, dsid, file_handlers, coords): """"""Get the area for *dsid*."""""" try: return self._load_area_def(dsid, file_handlers) except NotImplementedError: if any((x is None for x in coords)): logger.warning(""Failed to load coordinates for '{}'"".format(dsid)) return None area = self._make_area_from_coords(coords) <IF_STMT> logger.debug('No coordinates found for %s', str(dsid)) return area",if area is None:,if area is None:,0.9405652008300903,0.8780099567239787,True
4857,"def __getattr__(self, name): if Popen.verbose: sys.stdout.write('Getattr: %s...' % name) if name in Popen.__slots__: return object.__getattribute__(self, name) elif self.popen is not None: if Popen.verbose: print('from Popen') return getattr(self.popen, name) el<IF_STMT> return self.emu_wait else: raise Exception('subprocess emulation: not implemented: %s' % name)",if name == 'wait':,if self.emu_wait is not None:,0.9048660762528162,0.845713978670975,False
4858,"def update(self, time_delta): super().update(time_delta) n = self.menu.selected_option if n == self.last: return self.last = n s = '' for i in range(len(self.files)): <IF_STMT> for l in open(self.files[i][1]): x = l.strip() if len(x) > 1 and x[0] == '#': x = '<b><u>' + x[1:] + ' </u></b>' s += x + '<br>' self.set_text(s)",if self.files[i][0] == n:,if self.files[i][0] == n:,0.9434843114649054,0.8964173245779284,True
4859,"def wrapper(*args, **kwargs): list_args, empty = _apply_defaults(func, args, kwargs) if len(dimensions) > len(list_args): raise TypeError('%s takes %i parameters, but %i dimensions were passed' % (func.__name__, len(list_args), len(dimensions))) for dim, value in zip(dimensions, list_args): if dim is None: continue <IF_STMT> val_dim = ureg.get_dimensionality(value) raise DimensionalityError(value, 'a quantity of', val_dim, dim) return func(*args, **kwargs)",if not ureg.Quantity(value).check(dim):,if empty:,0.9284922690378798,0.9312457603037672,False
4860,"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) if size: strlen *= size <IF_STMT> data = 'A' * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",if strlen < MAX_LEN:,if strlen > 0:,0.8217504337454932,0.9114434865990403,False
4861,def wait_send_all_might_not_block(self) -> None: with self._send_conflict_detector: <IF_STMT> raise trio.ClosedResourceError('file was already closed') try: await trio.lowlevel.wait_writable(self._fd_holder.fd) except BrokenPipeError as e: raise trio.BrokenResourceError from e,if self._fd_holder.closed:,if self._fd_holder.fd is not None:,0.5937760985250097,0.7406093667638122,False
4862,"def parse_win_proxy(val): proxies = [] for p in val.split(';'): if '=' in p: tab = p.split('=', 1) <IF_STMT> tab[0] = 'SOCKS4' proxies.append((tab[0].upper(), tab[1], None, None)) else: proxies.append(('HTTP', p, None, None)) return proxies",if tab[0] == 'socks':,if len(tab) < 2:,0.6572082687218423,0.8336104423443033,False
4863,"def _super_function(args): passed_class, passed_self = args.get_arguments(['type', 'self']) if passed_self is None: return passed_class else: pyclass = passed_class if isinstance(pyclass, pyobjects.AbstractClass): supers = pyclass.get_superclasses() <IF_STMT> return pyobjects.PyObject(supers[0]) return passed_self",if supers:,if len(supers) == 1:,0.6223697620847276,0.8105932471967202,False
4864,"def update_output_mintime(job): try: return output_mintime[job] except KeyError: for job_ in chain([job], self.depending[job]): try: t = output_mintime[job_] except KeyError: t = job_.output_mintime <IF_STMT> output_mintime[job] = t return output_mintime[job] = None",if t is not None:,if t:,0.6581024179886421,0.8743414417652072,False
4865,"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split(':') if len(split_provider_id) == 2: _id = split_provider_id[1] cursor = self.get_by_id(_id) <IF_STMT> result.append(cursor) return result",if cursor:,if cursor:,0.901325922338045,0.8827916928185874,True
4866,"def stop(self): with self.lock: <IF_STMT> return self.task_queue.put(None) self.result_queue.put(None) process = self.process self.process = None self.task_queue = None self.result_queue = None process.join(timeout=0.1) if process.exitcode is None: os.kill(process.pid, signal.SIGKILL) process.join()",if not self.process:,if self.process is None:,0.8528128136556196,0.8105932471967202,False
4867,"def on_api_command(self, command, data): if command == 'select': if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, 'Insufficient permissions') <IF_STMT> return flask.abort(409, 'No active prompt') choice = data['choice'] if not isinstance(choice, int) or not self._prompt.validate_choice(choice): return flask.abort(400, '{!r} is not a valid value for choice'.format(choice)) self._answer_prompt(choice)",if self._prompt is None:,if not self._prompt.active():,0.8681369441988191,0.8928756684056034,False
4868,"def application_openFiles_(self, nsapp, filenames): for filename in filenames: logging.info('[osx] receiving from macOS : %s', filename) <IF_STMT> if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES: sabnzbd.add_nzbfile(filename, keep=True)",if os.path.exists(filename):,if os.path.isfile(filename):,0.589127428414586,0.8466657105524215,False
4869,"def test_error_through_destructor(self): rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy <IF_STMT> self.assertIsNone(cm.unraisable) elif cm.unraisable is not None: self.assertEqual(cm.unraisable.exc_type, OSError)",if not IOBASE_EMITS_UNRAISABLE:,"if isinstance(cm.unraisable, TypeError):",0.8546193421962279,0.7848518349390632,False
4870,"def http_wrapper(self, url, postdata={}): try: <IF_STMT> f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) if postdata: logging.error('with post data') else: logging.error('without post data') logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = '' return response",if postdata != {}:,if postdata:,0.8425025545252028,0.9202663016973823,False
4871,"def check_single_file(fn, fetchuri): """"""Determine if a single downloaded file is something we can't handle"""""" with open(fn, 'r', errors='surrogateescape') as f: <IF_STMT> logger.error('Fetching ""%s"" returned a single HTML page - check the URL is correct and functional' % fetchuri) sys.exit(1)",if '<html' in f.read(100).lower():,if f.read(fetchuri) != 0:,0.7665948469452315,0.8627586293513119,False
4872,"def update_properties(self, update_dict): signed_attribute_changed = False for k, value in update_dict.items(): if getattr(self, k) != value: setattr(self, k, value) signed_attribute_changed = signed_attribute_changed or k in self.payload_arguments if signed_attribute_changed: <IF_STMT> self.status = UPDATED self.timestamp = clock.tick() self.sign() return self",if self.status != NEW:,if self.status == REAL:,0.9293008352277339,0.8592377270804451,False
4873,"def clean_items(event, items, variations): for item in items: if event != item.event: raise ValidationError(_('One or more items do not belong to this event.')) if item.has_variations: <IF_STMT> raise ValidationError(_('One or more items has variations but none of these are in the variations list.'))",if not any((var.item == item for var in variations)):,if variations != item.variations:,0.9185789333972438,0.8723360571509826,False
4874,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue if tt == 18: self.add_doc_id(d.getPrefixedString()) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.9001195620339053,0.8555308664663046,True
4875,"def connections(self): fds = self.open_files socket = 'socket:[' result = [] functions = [pwndbg.net.tcp, pwndbg.net.unix, pwndbg.net.netlink] for fd, path in fds.items(): if socket not in path: continue inode = path[len(socket):-1] inode = int(inode) for func in functions: for x in func(): <IF_STMT> x.fd = fd result.append(x) return tuple(result)",if x.inode == inode:,if x.inode == inode:,0.8901078794857905,0.8879659171421962,True
4876,def _movement_finished(self): if self.in_ship_map: <IF_STMT> ship = self.session.world.ship_map.get(self._next_target.to_tuple()) if ship is not None and ship() is self: del self.session.world.ship_map[self._next_target.to_tuple()] super()._movement_finished(),if self._next_target is not None:,if self._next_target.to_tuple() is not None:,0.8596098597856467,0.7049592608322395,False
4877,"def print_addresses(self): p = 3 tmp_str = '[' if self.get_len() >= 7: while 1: if p + 1 == self.get_ptr(): tmp_str += '#' tmp_str += self.get_ip_address(p) p += 4 <IF_STMT> break else: tmp_str += ', ' tmp_str += '] ' if self.get_ptr() % 4: tmp_str += 'nonsense ptr field: %d ' % self.get_ptr() return tmp_str",if p >= self.get_len():,if self.get_ptr() == 0:,0.9640022510864583,0.9019629427251674,False
4878,"def source_shapes(self): """"""Prints debug information about the sources in this provider."""""" if logger.isEnabledFor(logging.DEBUG): for i, source in enumerate(self.sources): <IF_STMT> name = 'anonymous' else: name = self.keys[i] try: shape = source.shape() except NotImplementedError: shape = 'N/A' logger.debug('Data source ""%s"": entries=%s, shape=%s', name, len(source), shape)",if self.keys is None:,if i == 0:,0.8214740265343005,0.8752376177722327,False
4879,def swap_actions(actions): for mutexgroup in mutex_groups: mutex_actions = mutexgroup._group_actions <IF_STMT> targetindex = actions.index(mutexgroup._group_actions[0]) actions[targetindex] = mutexgroup actions = [action for action in actions if action not in mutex_actions] return actions,"if contains_actions(mutex_actions, actions):",if mutexgroup._group_actions:,0.7124243471166879,0.8787142254774354,False
4880,"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt['_deps'] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: <IF_STMT> continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",if init_service and init_service in dep_cnt['_deps']:,if dep_cnt[0] != init_service:,0.9068337784903835,0.8474968231198384,False
4881,"def make_dump_list_by_name_list(name_list): info_list = [] for info_name in name_list: info = next((x for x in DUMP_LIST if x.info_name == info_name), None) <IF_STMT> raise RuntimeError('Unknown info name: ""{}""'.format(info_name)) info_list.append(info) return info_list",if not info:,if info is None:,0.7336496704959337,0.8228500218338367,False
4882,"def create(self, private=False): try: if private: log.info('Creating private channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name, 'is_private': True}) else: log.info('Creating channel %s.', self) self._bot.api_call('conversations.create', data={'name': self.name}) except SlackAPIResponseError as e: <IF_STMT> raise RoomError(f'Unable to create channel. {USER_IS_BOT_HELPTEXT}') else: raise RoomError(e)",if e.error == 'user_is_bot':,if e.response.status_code == 404:,0.9161500485119911,0.8592377270804451,False
4883,"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() <IF_STMT> continue reply = i[0] attrs = {} for w in i[1:]: j = w.find('=', 1) if j == -1: attrs[w] = '' else: attrs[w[:j]] = w[j + 1:] r.append((reply, attrs)) if reply == '!done': return r",if len(i) == 0:,if not i:,0.7262045752160378,0.9151329413834155,False
4884,"def _load_logfile(self, lfn): enc_key = self.decryption_key_func() with open(os.path.join(self.logdir, lfn)) as fd: <IF_STMT> with DecryptingStreamer(fd, mep_key=enc_key, name='EventLog/DS(%s)' % lfn) as streamer: lines = streamer.read() streamer.verify(_raise=IOError) else: lines = fd.read() if lines: for line in lines.splitlines(): event = Event.Parse(line.strip()) self._events[event.event_id] = event",if enc_key:,if enc_key:,0.784666397198306,0.9099951253570094,True
4885,"def set_ok_port(self, cookie, request): if cookie.port_specified: req_port = request_port(request) if req_port is None: req_port = '80' else: req_port = str(req_port) for p in cookie.port.split(','): try: int(p) except ValueError: debug('   bad port %s (not numeric)', p) return False <IF_STMT> break else: debug('   request port (%s) not found in %s', req_port, cookie.port) return False return True",if p == req_port:,if req_port == p:,0.9343959226786736,0.9001816649635144,False
4886,"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug('get attr val: %s %s', nodeid, attr) <IF_STMT> dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",if nodeid not in self._nodes:,if nodeid not in self._nodes:,0.6881837835462442,0.8677319190106252,True
4887,"def data_logging_status(self, trail_name, trail_details, api_client): for es in api_client.get_event_selectors(TrailName=trail_name)['EventSelectors']: has_wildcard = {u'Values': [u'arn:aws:s3:::'], u'Type': u'AWS::S3::Object'} in es['DataResources'] is_logging = trail_details['IsLogging'] <IF_STMT> return True return False",if has_wildcard and is_logging and self.is_fresh(trail_details):,if has_wildcard and is_logging:,0.8340581683642284,0.7886336751695258,False
4888,"def pytest_deselected(items): if sb_config.dashboard: sb_config.item_count -= len(items) for item in items: test_id, display_id = _get_test_ids_(item) <IF_STMT> sb_config._results.pop(test_id)",if test_id in sb_config._results.keys():,if test_id in sb_config._results:,0.7068853273858093,0.7404008324993688,False
4889,"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical('Fatal error! network ins not Dag.') import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: <IF_STMT> self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",if output == input:,if input == output:,0.9141161274233307,0.8983343737277126,False
4890,"def printWiki(): firstHeading = False for m in protocol: <IF_STMT> if firstHeading: output('|}') __printWikiHeader(m[1], m[2]) firstHeading = True else: output('|-') output('| <span style=""white-space:nowrap;""><tt>' + m[0] + '</tt></span> || || ' + m[1]) output('|}')",if m[0] == '':,if m[0] == 'heading':,0.9056830966624103,0.8385130047130208,False
4891,"def test_getitem(self): n = 200 d = deque(range(n)) l = list(range(n)) for i in range(n): d.popleft() l.pop(0) <IF_STMT> d.append(i) l.append(i) for j in range(1 - len(l), len(l)): assert d[j] == l[j] d = deque('superman') self.assertEqual(d[0], 's') self.assertEqual(d[-1], 'n') d = deque() self.assertRaises(IndexError, d.__getitem__, 0) self.assertRaises(IndexError, d.__getitem__, -1)",if random.random() < 0.5:,if len(d) == n:,0.6889115470495026,0.8856327184319047,False
4892,"def get_num(line, char_ptr, num_chars): char_ptr = char_ptr + 1 numstr = '' good = '-.0123456789' while char_ptr < num_chars: digit = line[char_ptr] <IF_STMT> numstr = numstr + digit char_ptr = char_ptr + 1 else: break return numstr",if good.find(digit) != -1:,if digit in good:,0.6249135112781437,0.8555308664663046,False
4893,"def read_digits(source, start, first_code): body = source.body position = start code = first_code if code is not None and 48 <= code <= 57: while True: position += 1 code = char_code_at(body, position) <IF_STMT> break return position raise GraphQLSyntaxError(source, position, u'Invalid number, expected digit but got: {}.'.format(print_char_code(code)))",if not (code is not None and 48 <= code <= 57):,if code is None:,0.7241872663565371,0.8856327184319047,False
4894,"def get_aws_metadata(headers, provider=None): if not provider: provider = boto.provider.get_default() metadata_prefix = provider.metadata_prefix metadata = {} for hkey in headers.keys(): <IF_STMT> val = urllib.unquote_plus(headers[hkey]) try: metadata[hkey[len(metadata_prefix):]] = unicode(val, 'utf-8') except UnicodeDecodeError: metadata[hkey[len(metadata_prefix):]] = val del headers[hkey] return metadata",if hkey.lower().startswith(metadata_prefix):,if hkey.startswith(metadata_prefix):,0.9233365763990632,0.9024521756077707,False
4895,def _process_rtdest(self): LOG.debug('Processing RT NLRI destination...') if self._rtdest_queue.is_empty(): return else: processed_any = False while not self._rtdest_queue.is_empty(): next_dest = self._rtdest_queue.pop_first() if next_dest: next_dest.process() processed_any = True <IF_STMT> self._core_service.update_rtfilters(),if processed_any:,if processed_any:,0.938423797200046,0.8953132021812428,True
4896,"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: if header_name in headers: <IF_STMT> self.assertTrue(url.endswith('/latest'), msg=url) else: self.assertTrue(url.endswith('/download_urls'), msg=url) return headers.get(header_name)",if self.revs_enabled:,if self.revs_enabled:,0.9245594740365335,0.9024521756077707,True
4897,"def add_external_deps(self, deps): for dep in deps: if hasattr(dep, 'el'): dep = dep.el <IF_STMT> raise InvalidArguments('Argument is not an external dependency') self.external_deps.append(dep) if isinstance(dep, dependencies.Dependency): self.process_sourcelist(dep.get_sources())","if not isinstance(dep, dependencies.Dependency):","if not isinstance(dep, dependencies.ExternalDependency):",0.6129296285748687,0.7965020533851944,False
4898,"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() if isinstance(r, bytes): r = r.decode('utf-8') msg = json.loads(r) stream = msg.get('stream') <IF_STMT> await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())",if stream is not None:,if stream:,0.9017249433928383,0.8996480074924822,False
4899,"def generate_and_check_random(): random_size = 256 while True: random = os.urandom(random_size) a = int.from_bytes(random, 'big') A = pow(g, a, p) <IF_STMT> a_for_hash = big_num_for_hash(A) u = int.from_bytes(sha256(a_for_hash, b_for_hash), 'big') if u > 0: return (a, a_for_hash, u)","if is_good_mod_exp_first(A, p):",if A > 0:,0.7964296012537483,0.8516228624291206,False
4900,"def write(self, datagram, address): """"""Write a datagram."""""" try: return self.socket.sendto(datagram, address) except OSError as se: no = se.args[0] if no == EINTR: return self.write(datagram, address) elif no == EMSGSIZE: raise error.MessageLengthError('message too long') <IF_STMT> pass else: raise",elif no == EAGAIN:,elif no == EWOULDBLOCK:,0.9012350977034597,0.8555308664663046,False
4901,"def doDir(elem): for child in elem.childNodes: <IF_STMT> continue if child.tagName == 'Directory': doDir(child) elif child.tagName == 'Component': for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != 'File': continue files.add(grandchild.getAttribute('Source').replace(os.sep, '/'))","if not isinstance(child, minidom.Element):","if not isinstance(child, minidom.Element):",0.7915253309043986,0.8385130047130208,True
4902,"def add_reversed_tensor(i, X, reversed_X): if X in stop_mapping_at_tensors: return if X not in reversed_tensors: reversed_tensors[X] = {'id': (nid, i), 'tensor': reversed_X} else: tmp = reversed_tensors[X] if 'tensor' in tmp and 'tensors' in tmp: raise Exception('Wrong order, tensors already aggregated!') <IF_STMT> tmp['tensors'] = [tmp['tensor'], reversed_X] del tmp['tensor'] else: tmp['tensors'].append(reversed_X)",if 'tensor' in tmp:,if 'tensors' not in tmp:,0.7866935967594314,0.8703737209656045,False
4903,"def walk(source, path, default, delimiter='.'): """"""Walk the sourch hash given the path and return the value or default if not found"""""" if not isinstance(source, dict): raise RuntimeError('The source is not a walkable dict: {} path: {}'.format(source, path)) keys = path.split(delimiter) max_depth = len(keys) cur_depth = 0 while cur_depth < max_depth: <IF_STMT> source = source[keys[cur_depth]] cur_depth = cur_depth + 1 else: return default return source",if keys[cur_depth] in source:,if keys[cur_depth] in source:,0.8281249603924432,0.9155272930874561,True
4904,"def _from_txt_get_vulns(self): file_vulns = [] vuln_regex = 'SQL injection in a .*? was found at: ""(.*?)"", using HTTP method (.*?). The sent .*?data was: ""(.*?)""' vuln_re = re.compile(vuln_regex) for line in file(self.OUTPUT_FILE): mo = vuln_re.search(line) <IF_STMT> v = MockVuln('TestCase', None, 'High', 1, 'plugin') v.set_url(URL(mo.group(1))) v.set_method(mo.group(2)) file_vulns.append(v) return file_vulns",if mo:,if mo:,0.8315208056331206,0.9253742688467129,True
4905,"def __get__(self, instance, instance_type=None): if instance: if self.att_name not in instance._obj_cache: rel_obj = self.get_obj(instance) <IF_STMT> instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",if rel_obj:,if rel_obj:,0.7233559380765727,0.8318180062062374,True
4906,"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): <IF_STMT> pos_end = pos else: if pos_end >= pos_start: ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",if func.type in support_set:,if func.support_set == support_set:,0.9110995665790822,0.8661072626070159,False
4907,"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: if name is None: all_plugins += [plugin for name, plugin in self.plugins.items() <IF_STMT>] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",if name not in self.plugins_callback_order and plugin.is_activated,if plugin.is_active,0.880476620179661,0.9237460349978159,False
4908,"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <IF_STMT> result.append(token.contents.replace('%', '%%')) elif token.token_type == TOKEN_VAR: result.append('%%(%s)s' % token.contents) vars.append(token.contents) msg = ''.join(result) if self.trimmed: msg = translation.trim_whitespace(msg) return (msg, vars)",if token.token_type == TOKEN_TEXT:,if token.token_type == TOKEN_CHAR:,0.8197740639597604,0.8474968231198384,False
4909,"def test_build_root_config_overwrite(self): cfg = build_root_config('tests.files.settings_overwrite') for key, val in DEFAULT_SPIDER_GLOBAL_CONFIG.items(): <IF_STMT> self.assertEqual(cfg['global'][key], ['zzz']) else: self.assertEqual(cfg['global'][key], val)",if key == 'spider_modules':,"if key in ['zzz', 'zzz']:",0.8071681227588579,0.6383240325919926,False
4910,"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() <IF_STMT> if limit == 0: return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",if settings.MAX_PAGE_SIZE:,"if isinstance(limit, int):",0.6743302416172862,0.8675979125638379,False
4911,"def track_handler(handler): tid = handler.request.tid for event in events_monitored: <IF_STMT> e = Event(event, handler.request.execution_time) State.tenant_state[tid].RecentEventQ.append(e) State.tenant_state[tid].EventQ.append(e) break",if event['handler_check'](handler):,if event.request.execution_time > 0:,0.8210645280886409,0.693395566222006,False
4912,"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_subscription().TryMerge(tmp) continue <IF_STMT> raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",if tt == 0:,if tt == 0:,0.682460343833398,0.828399516355805,True
4913,"def GetCreateInstanceBinder(self, info): with self._lock: <IF_STMT> return self._createInstanceBinders[info] b = runtime.SymplCreateInstanceBinder(info) self._createInstanceBinders[info] = b return b",if self._createInstanceBinders.ContainsKey(info):,if self._createInstanceBinders.ContainsKey(info):,0.8220589934333812,0.762465858623486,True
4914,"def process_task(self, body, message): if 'control' in body: try: return self.control(body, message) except Exception: logger.exception('Exception handling control message:') return if len(self.pool): <IF_STMT> try: queue = UUID(body['uuid']).int % len(self.pool) except Exception: queue = self.total_messages % len(self.pool) else: queue = self.total_messages % len(self.pool) else: queue = 0 self.pool.write(queue, body) self.total_messages += 1 message.ack()",if 'uuid' in body and body['uuid']:,if 'uuid' in body:,0.8122147502640951,0.8944264839442453,False
4915,"def is_defined_in_base_class(self, var: Var) -> bool: if var.info: for base in var.info.mro[1:]: if base.get(var.name) is not None: return True <IF_STMT> return True return False",if var.info.fallback_to_any:,if self.is_defined_in_base_class(var):,0.6288338003157797,0.8466657105524215,False
4916,"def ant_map(m): tmp = 'rows %s\ncols %s\n' % (len(m), len(m[0])) players = {} for row in m: tmp += 'm ' for col in row: if col == LAND: tmp += '.' <IF_STMT> tmp += '%' elif col == FOOD: tmp += '*' elif col == UNSEEN: tmp += '?' else: players[col] = True tmp += chr(col + 97) tmp += '\n' tmp = 'players %s\n' % len(players) + tmp return tmp",elif col == BARRIER:,elif col == WAND:,0.857030833286629,0.924776563154472,False
4917,def prompt_for_resume(config): logger = logging.getLogger('changeme') logger.error('A previous scan was interrupted. Type R to resume or F to start a fresh scan') answer = '' while not (answer == 'R' or answer == 'F'): prompt = '(R/F)> ' answer = '' try: answer = raw_input(prompt) except NameError: answer = input(prompt) <IF_STMT> logger.debug('Forcing a fresh scan') elif answer.upper() == 'R': logger.debug('Resuming previous scan') config.resume = True return config.resume,if answer.upper() == 'F':,if answer == 'F':,0.7893466062974567,0.9180466554652996,False
4918,"def f(view, s): if mode == modes.INTERNAL_NORMAL: <IF_STMT> if view.line(s.b).size() > 0: eol = view.line(s.b).b return R(s.b, eol) return s return s",if count == 1:,if view.line(s.b).size() > 0:,0.8596847822206604,0.760856626273165,False
4919,"def flush(self): if not self.cuts: return for move, (x, y, z), cent in douglas(self.cuts, self.tolerance, self.plane): <IF_STMT> self.write('%s X%.4f Y%.4f Z%.4f %s' % (move, x, y, z, cent)) self.lastgcode = None self.lastx = x self.lasty = y self.lastz = z else: self.move_common(x, y, z, gcode='G1') self.cuts = []",if cent:,if self.lastgcode is not None:,0.8329078108741323,0.8703737209656045,False
4920,"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() new_i.id = self.id cls.id = old_id for prop in cls.properties: <IF_STMT> if self.has(prop): val = getattr(self, prop) setattr(new_i, prop, val) new_i.members = [] return new_i",if prop is not 'members':,if prop.startswith('shell'):,0.8012668144493699,0.8996480074924822,False
4921,"def find_region_by_value(key, value): for region in cognitoidp_backends: backend = cognitoidp_backends[region] for user_pool in backend.user_pools.values(): if key == 'client_id' and value in user_pool.clients: return region <IF_STMT> return region return list(cognitoidp_backends)[0]",if key == 'access_token' and value in user_pool.access_tokens:,elif key == 'client_id' and value in user_pool.clients:,0.8609853302934167,0.7221052041889016,False
4922,"def __init__(self, fixed: MQTTFixedHeader=None, variable_header: PacketIdVariableHeader=None): if fixed is None: header = MQTTFixedHeader(PUBREL, 2) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PubrelPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = None",if fixed.packet_type is not PUBREL:,if fixed.packet_type not in PUBREL:,0.6299582501706997,0.8338542560892604,False
4923,"def _on_event_MetadataStatisticsUpdated(self, event, data): with self._selectedFileMutex: <IF_STMT> self._setJobData(self._selectedFile['filename'], self._selectedFile['filesize'], self._selectedFile['sd'], self._selectedFile['user'])",if self._selectedFile:,if self._selectedFile['filename'] != self._selectedFile['filename']:,0.48609017675727884,0.5316967153331754,False
4924,"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for parameter_range_key, parameter_range_value in parameter_range.__dict__.items(): <IF_STMT> continue if isinstance(parameter_range_value, list): for categorical_value in parameter_range_value: value_hp.validate(categorical_value) else: value_hp.validate(parameter_range_value)",if parameter_range_key == 'scaling_type':,if parameter_range_key == 'parameter_range':,0.6706650336443933,0.7709002428237395,False
4925,"def visit_filter_projection(self, node, value): base = self.visit(node['children'][0], value) if not isinstance(base, list): return None comparator_node = node['children'][2] collected = [] for element in base: if self._is_true(self.visit(comparator_node, element)): current = self.visit(node['children'][1], element) <IF_STMT> collected.append(current) return collected",if current is not None:,if current is not None:,0.8215839599695606,0.8248765135255685,True
4926,"def _getSubstrings(self, va, size, ltyp): subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) <IF_STMT> subs.add((loc[L_VA], loc[L_SIZE])) if loc[L_TINFO]: subs = subs.union(set(loc[L_TINFO])) return list(subs)",if loc and loc[L_LTYPE] == LOC_STRING and (loc[L_VA] > va):,if loc[L_VA]:,0.7011675505599,0.8901732118131125,False
4927,"def run(self): while not self._stopped: try: try: test_name = next(self.pending) except StopIteration: break mp_result = self._runtest(test_name) self.output.put((False, mp_result)) <IF_STMT> break except ExitThread: break except BaseException: self.output.put((True, traceback.format_exc())) break","if must_stop(mp_result.result, self.ns):",if mp_result:,0.6303039721176928,0.8696398662122882,False
4928,"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v elif isinstance(v, (list, tuple, dict)): out = get_in_inputs(key, v) <IF_STMT> return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]",if out:,if out:,0.6951933982209146,0.933847757608669,True
4929,"def act_mapping(self, items, actions, mapping): """"""Executes all the actions on the list of pods."""""" success = True for action in actions: for key, method in mapping.items(): if key in action: params = action.get(key) ret = method(items, params) <IF_STMT> success = False return success",if not ret:,if ret is not None:,0.7899920093177076,0.8559898693114286,False
4930,"def _apply(self, plan): desired = plan.desired changes = plan.changes self.log.debug('_apply: zone=%s, len(changes)=%d', desired.name, len(changes)) domain_name = desired.name[:-1] try: nsone_zone = self._client.loadZone(domain_name) except ResourceException as e: <IF_STMT> raise self.log.debug('_apply:   no matching zone, creating') nsone_zone = self._client.createZone(domain_name) for change in changes: class_name = change.__class__.__name__ getattr(self, '_apply_{}'.format(class_name))(nsone_zone, change)",if e.message != self.ZONE_NOT_FOUND_MESSAGE:,if e.response.status != 404:,0.8260894195084223,0.8806615362338783,False
4931,"def split_artists(self, json): if len(json) == 0: ([], []) elif len(json) == 1: artist = Artist.query.filter_by(name=json[0]['name']).first() return ([artist], []) my_artists = [] other_artists = [] for artist_dict in json: artist = Artist.query.filter_by(name=artist_dict['name']) <IF_STMT> my_artists.append(artist.first()) else: del artist_dict['thumb_url'] other_artists.append(artist_dict) return (my_artists, other_artists)",if artist.count():,if artist:,0.8313184330563993,0.9122561819614461,False
4932,"def update_metadata(self): for attrname in dir(self): if attrname.startswith('__'): continue attrvalue = getattr(self, attrname, None) <IF_STMT> continue if attrname == 'salt_version': attrname = 'version' if hasattr(self.metadata, 'set_{0}'.format(attrname)): getattr(self.metadata, 'set_{0}'.format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",if attrvalue == 0:,if attrvalue is None:,0.6550252768954237,0.8592377270804451,False
4933,"def close(self, code=errno.ECONNRESET): with self.shutdown_lock: <IF_STMT> super(RemoteIPRoute, self).close(code=code) self.closed = True try: self._mitogen_call.get() except mitogen.core.ChannelError: pass if self._mitogen_broker is not None: self._mitogen_broker.shutdown() self._mitogen_broker.join()",if not self.closed:,if not self.closed:,0.870931770213728,0.80377750806414,True
4934,"def untokenize(self, iterable): for t in iterable: <IF_STMT> self.compat(t, iterable) break tok_type, token, start, end, line = t self.add_whitespace(start) self.tokens.append(token) self.prev_row, self.prev_col = end if tok_type in (NEWLINE, NL): self.prev_row += 1 self.prev_col = 0 return ''.join(self.tokens)",if len(t) == 2:,"if isinstance(t, (list, tuple)):",0.9011627379249514,0.8555308664663046,False
4935,"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: if self.norm_means: x = np.add(x, self.bias[spk]) <IF_STMT> x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",if self.norm_vars:,elif self.norm_vars:,0.7362135797970887,0.9202663016973823,False
4936,"def get_party_total(self, args): self.party_total = frappe._dict() for d in self.receivables: self.init_party_total(d) for k in list(self.party_total[d.party]): <IF_STMT> self.party_total[d.party][k] += d.get(k, 0.0) self.set_party_details(d)","if k not in ['currency', 'sales_person']:",if k not in self.party_total[d.party]:,0.8546294597608484,0.7178970818142898,False
4937,"def get_databases(request): dbs = {} global_env = globals() for key, value in global_env.items(): try: cond = isinstance(value, GQLDB) except: cond = isinstance(value, SQLDB) <IF_STMT> dbs[key] = value return dbs",if cond:,if cond:,0.8717626522477672,0.8743414417652072,True
4938,"def check_twobit_file(dbkey, GALAXY_DATA_INDEX_DIR): twobit_file = '%s/twobit.loc' % GALAXY_DATA_INDEX_DIR twobit_path = '' twobits = {} for i, line in enumerate(open(twobit_file)): line = line.rstrip('\r\n') if line and (not line.startswith('#')): fields = line.split('\t') <IF_STMT> continue twobits[fields[0]] = fields[1] if dbkey in twobits: twobit_path = twobits[dbkey] return twobit_path",if len(fields) < 2:,if len(fields) != 2:,0.759498756040053,0.8780099567239787,False
4939,"def action(scheduler, _): nonlocal state nonlocal has_result nonlocal result nonlocal first nonlocal time <IF_STMT> observer.on_next(result) try: if first: first = False else: state = iterate(state) has_result = condition(state) if has_result: result = state time = time_mapper(state) except Exception as e: observer.on_error(e) return if has_result: mad.disposable = scheduler.schedule_relative(time, action) else: observer.on_completed()",if has_result:,if result:,0.7813596537622332,0.9284304001296656,False
4940,def orthogonalEnd(self): if self.type == Segment.LINE: O = self.AB.orthogonal() O.norm() return O else: O = self.B - self.C O.norm() <IF_STMT> return -O else: return O,if self.type == Segment.CCW:,if self.type == Segment.LINE:,0.8509485772383375,0.7886336751695258,False
4941,"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == 'ANY': <IF_STMT> self._definition = [] elif v in self._definition: self._definition.remove(v) if self._value is not None and self._value not in self._definition and self._not_any(): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",if v == 'ANY':,if v in self._definition:,0.949980144927939,0.8983343737277126,False
4942,"def __enter__(self) -> None: try: <IF_STMT> signal.signal(signal.SIGALRM, self.handle_timeout) signal.alarm(self.seconds) except ValueError as ex: logger.warning(""timeout can't be used in the current context"") logger.exception(ex)",if threading.current_thread() == threading.main_thread():,if self.seconds is not None:,0.5849805598164731,0.7297349727547102,False
4943,"def __init__(self, fixed: MQTTFixedHeader=None): if fixed is None: header = MQTTFixedHeader(PINGRESP, 0) else: <IF_STMT> raise HBMQTTException('Invalid fixed packet type %s for PingRespPacket init' % fixed.packet_type) header = fixed super().__init__(header) self.variable_header = None self.payload = None",if fixed.packet_type is not PINGRESP:,if fixed.packet_type not in PINGRESP:,0.6238847566604999,0.8248765135255685,False
4944,"def _put_nowait(self, data, *, sender): if not self._running: logger.warning('Pub/Sub listener message after stop: %r, %r', sender, data) return self._queue.put_nowait((sender, data)) if self._waiter is not None: fut, self._waiter = (self._waiter, None) <IF_STMT> assert fut.cancelled(), ('Waiting future is in wrong state', self, fut) return fut.set_result(None)",if fut.done():,if fut is not None:,0.9076720452659315,0.8559898693114286,False
4945,"def OnAssignBuiltin(self, cmd_val): buf = self._ShTraceBegin() if not buf: return for i, arg in enumerate(cmd_val.argv): <IF_STMT> buf.write(' ') buf.write(arg) for pair in cmd_val.pairs: buf.write(' ') buf.write(pair.var_name) buf.write('=') if pair.rval: _PrintShValue(pair.rval, buf) buf.write('\n') self.f.write(buf.getvalue())",if i != 0:,if arg:,0.8491944196973529,0.8901732118131125,False
4946,"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] if not (isinstance(k, str) or isinstance(k, unicode)): k = dumps(k) <IF_STMT> obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj",if Types.KEYS not in obj:,if Types.KEYS not in obj:,0.9065596373401651,0.8094220211349227,True
4947,"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == '{': length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break <IF_STMT> token = token.matching_bracket token = token.next_token return False",if token.OpensScope():,if token.value == '}':,0.9237987832132768,0.8832000938217648,False
4948,"def get_editable_dict(self): ret = {} for ref, ws_package in self._workspace_packages.items(): path = ws_package.root_folder <IF_STMT> path = os.path.join(path, CONANFILE) ret[ref] = {'path': path, 'layout': ws_package.layout} return ret",if os.path.isdir(path):,if path.endswith('.py'):,0.7445616055287158,0.8590888738245122,False
4949,"def serialize(self, name=None): data = super(WebLink, self).serialize(name) data['contentType'] = self.contentType if self.width: <IF_STMT> raise InvalidWidthException(self.width) data['inputOptions'] = {} data['width'] = self.width data.update({'content': {'url': self.linkUrl, 'text': self.linkText}}) return data","if self.width not in [100, 50, 33, 25]:","if self.width not in ('100%', '100%'):",0.8856615327082631,0.7590598306198806,False
4950,"def callback(lexer, match, context): text = match.group() extra = '' if start: context.next_indent = len(text) <IF_STMT> while context.next_indent < context.indent: context.indent = context.indent_stack.pop() if context.next_indent > context.indent: extra = text[context.indent:] text = text[:context.indent] else: context.next_indent += len(text) if text: yield (match.start(), TokenClass, text) if extra: yield (match.start() + len(text), TokenClass.Error, extra) context.pos = match.end()",if context.next_indent < context.indent:,if context.indent_stack:,0.7222149610091745,0.933847757608669,False
4951,"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): if subscriber_web_sock == web_sock: index = i break <IF_STMT> del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",if index is not None:,if index is not None:,0.8001524984928253,0.7975010608178975,True
4952,"def test_missing_dict_param(): expected_err = 'params dictionary did not contain value for placeholder' try: substitute_params('SELECT * FROM cust WHERE salesrep = %(name)s', {'foobar': 'John Doe'}) assert False, 'expected exception b/c dict did not contain replacement value' except ValueError as exc: <IF_STMT> raise",if expected_err not in str(exc):,if expected_err in str(exc):,0.9288482610982453,0.8878679585127796,False
4953,"def one_gpr_reg_one_mem_scalable(ii): n, r = (0, 0) for op in _gen_opnds(ii): <IF_STMT> n += 1 elif op_gprv(op): r += 1 else: return False return n == 1 and r == 1",if op_agen(op) or (op_mem(op) and op.oc2 in ['v']):,if op_mem(op):,0.9057298890546738,0.8827916928185874,False
4954,"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, 'md_bg_color') and self.focus_behavior: if hasattr(self, 'theme_cls') and (not self.focus_color): self.md_bg_color = self.theme_cls.bg_normal el<IF_STMT> self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color",if not self.focus_color:,if self.focus_color is None:,0.8807086391357543,0.8385130047130208,False
4955,"def __init__(self, *args, **kwargs): BaseCellExporter.__init__(self, *args, **kwargs) self.comment = '#' for key in ['cell_marker']: <IF_STMT> self.metadata[key] = self.unfiltered_metadata[key] if self.fmt.get('rst2md'): raise ValueError(""The 'rst2md' option is a read only option. The reverse conversion is not implemented. Please either deactivate the option, or save to another format."")",if key in self.unfiltered_metadata:,if key in self.unfiltered_metadata:,0.9240651250289891,0.8806615362338783,True
4956,def sendQueryQueueByAfterNate(self): for i in range(10): queryQueueByAfterNateRsp = self.session.httpClint.send(urls.get('queryQueue')) <IF_STMT> print(''.join(queryQueueByAfterNateRsp.get('messages')) or queryQueueByAfterNateRsp.get('validateMessages')) time.sleep(1) else: sendEmail(ticket.WAIT_ORDER_SUCCESS) sendServerChan(ticket.WAIT_ORDER_SUCCESS) raise ticketIsExitsException(ticket.WAIT_AFTER_NATE_SUCCESS),if not queryQueueByAfterNateRsp.get('status'):,if queryQueueByAfterNateRsp:,0.7894334640539018,0.803154665668484,False
4957,"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() <IF_STMT> continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",if not line:,if not line:,0.9492817215241178,0.9042878500265974,True
4958,"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) if not comment: r = r.replace('*/', '\\x2a/') return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): <IF_STMT> return '%s /* %s */' % (n, self.pretty(int(n))) else: return '%s (%s)' % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",if comment:,if n.is_constant():,0.9440390167979027,0.9374009563674955,False
4959,"def get_pricings(self, subscription_id: str): try: client = self.get_client(subscription_id) pricings_list = await run_concurrently(lambda: client.pricings.list()) <IF_STMT> return pricings_list.value else: return [] except Exception as e: print_exception(f'Failed to retrieve pricings: {e}') return []","if hasattr(pricings_list, 'value'):",if pricings_list.ok:,0.7292673372870491,0.8787142254774354,False
4960,"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): name = target.id if name not in variables: doc = find_doc_for(target, body_lines) <IF_STMT> variables[name] = doc elif isinstance(target, ast.Tuple): for e in target.elts: add_doc(e, variables, body_lines)",if doc is not None:,if doc:,0.64534603818577,0.8901732118131125,False
4961,"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: <IF_STMT> done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return (left, right)",if right == len(text):,if right == 0:,0.9168587717764332,0.9069443196104878,False
4962,"def pxrun_nodes(self, *args, **kwargs): cell = self._px_cell if re.search('^\\s*%autopx\\b', cell): self._disable_autopx() return False else: try: result = self.view.execute(cell, silent=False, block=False) except: self.shell.showtraceback() return True else: <IF_STMT> try: result.get() except: self.shell.showtraceback() return True else: result.display_outputs() return False",if self.view.block:,if result:,0.9388328043980765,0.8996480074924822,False
4963,"def candidates() -> Generator['Symbol', None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print('searching in self:') print(s.to_string(Symbol.debug_indent + 1), end='') while True: if matchSelf: yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children <IF_STMT> break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print('searching in sibling:') print(s.to_string(Symbol.debug_indent + 1), end='')",if s.siblingAbove is None:,if s.siblingAbove is None:,0.935895343706898,0.8856327184319047,True
4964,"def decTaskGen(): cnt = intbv(0, min=-n, max=n) while 1: yield (clock.posedge, reset.negedge) <IF_STMT> cnt[:] = 0 count.next = 0 else: decTaskFunc(cnt, enable, reset, n) count.next = cnt",if reset == ACTIVE_LOW:,if enable:,0.7740415948947057,0.8645707301556367,False
4965,"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): <IF_STMT> layer = stack[0].f_locals['self'] if layer in layer_names: log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) return out",if 'self' in stack[0].f_locals:,if stack:,0.7212734497735878,0.8935248372106969,False
4966,"def to_json_dict(self): d = super().to_json_dict() d['bullet_list'] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: if isinstance(self.header, RenderedContent): d['header'] = self.header.to_json_dict() else: d['header'] = self.header if self.subheader is not None: <IF_STMT> d['subheader'] = self.subheader.to_json_dict() else: d['subheader'] = self.subheader return d","if isinstance(self.subheader, RenderedContent):","if isinstance(self.subheader, RenderedContent):",0.9156024986232197,0.8815741981066073,True
4967,"def add(request): form_type = 'servers' if request.method == 'POST': form = BookMarkForm(request.POST) <IF_STMT> form_type = form.save() messages.add_message(request, messages.INFO, 'Bookmark created') else: messages.add_message(request, messages.INFO, form.errors) if form_type == 'server': url = reverse('servers') else: url = reverse('metrics') return redirect(url) else: return redirect(reverse('servers'))",if form.is_valid():,if form.is_valid():,0.8751950348466955,0.9099951253570094,True
4968,"def fee_amount_in_quote(self, trading_pair: str, price: Decimal, order_amount: Decimal): fee_amount = Decimal('0') if self.percent > 0: fee_amount = price * order_amount * self.percent base, quote = trading_pair.split('-') for flat_fee in self.flat_fees: if interchangeable(flat_fee[0], base): fee_amount += flat_fee[1] * price <IF_STMT> fee_amount += flat_fee[1] return fee_amount","elif interchangeable(flat_fee[0], quote):","elif interchangeable(flat_fee[0], quote):",0.8356837043264694,0.897752847848028,True
4969,"def load_batch(fpath): with open(fpath, 'rb') as f: <IF_STMT> d = pickle.load(f, encoding='latin1') else: d = pickle.load(f) data = d['data'] labels = d['labels'] return (data, labels)","if sys.version_info > (3, 0):",if sys.version_info[0] < 3:,0.7805333080958995,0.7886336751695258,False
4970,"def clear_entries(options): """"""Clear pending entries"""""" with Session() as session: query = session.query(db.PendingEntry).filter(db.PendingEntry.approved == False) <IF_STMT> query = query.filter(db.PendingEntry.task_name == options.task_name) deleted = query.delete() console('Successfully deleted %i pending entries' % deleted)",if options.task_name:,if options.task_name:,0.7608508982043966,0.8787142254774354,True
4971,"def attribute_table(self, attribute): """"""Return a tuple (schema, table) for attribute."""""" dimension = attribute.dimension if dimension: schema = self.naming.dimension_schema or self.naming.schema <IF_STMT> table = self.fact_name else: table = self.naming.dimension_table_name(dimension) else: table = self.fact_name schema = self.naming.schema return (schema, table)",if dimension.is_flat and (not dimension.has_details):,if dimension == 'fact':,0.839083126484046,0.8592377270804451,False
4972,"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean('browsers', 'rating_confirm_multiple'): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _('_Remove Rating'), count, None) if dialog.run() != Gtk.ResponseType.YES: return reset = [] for song in songs: <IF_STMT> del song['~#rating'] reset.append(song) librarian.changed(reset)",if '~#rating' in song:,if '~#rating' in song:,0.9268044413991738,0.8692960007731574,True
4973,"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True <IF_STMT> right += 1 else: done = True return (left, right)",elif not self.word_boundary_char(text[right]):,elif not self.word_boundary_char(text[right - 1]):,0.8662663236885848,0.8937168577929463,False
4974,"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) if not chunk: self.transfer_finished = True return <IF_STMT> chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",if self._data_wrapper is not None:,if self._data_wrapper:,0.7186247681881038,0.9184043388013005,False
4975,"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 if self.split <= self.min or self.split >= self.max: self.split = self.save <IF_STMT> self.split = self.min else: self.split = self.max self.placeChilds()",elif self.split < 0.5:,elif self.split < self.min:,0.7322274130051646,0.9001816649635144,False
4976,"def readAtOffset(self, offset, size, shortok=False): ret = b'' self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) if x == b'': <IF_STMT> return None return ret ret += x return ret",if not shortok:,if shortok:,0.9254854213673935,0.8966773400768917,False
4977,"def webfinger(environ, start_response, _): query = parse_qs(environ['QUERY_STRING']) try: rel = query['rel'] resource = query['resource'][0] except KeyError: resp = BadRequest('Missing parameter in request') else: <IF_STMT> resp = BadRequest('Bad issuer in request') else: wf = WebFinger() resp = Response(wf.response(subject=resource, base=OAS.baseurl)) return resp(environ, start_response)",if rel != [OIC_ISSUER]:,if rel != 'issuer':,0.9321128627986366,0.8692960007731574,False
4978,"def _tokenize(self, text): if format_text(text) == EMPTY_TEXT: return [self.additional_special_tokens[0]] split_tokens = [] if self.do_basic_tokenize: for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens): <IF_STMT> split_tokens.append(token) else: split_tokens += self.wordpiece_tokenizer.tokenize(token) else: split_tokens = self.wordpiece_tokenizer.tokenize(text) return split_tokens",if token in self.basic_tokenizer.never_split:,if token in self.additional_special_tokens:,0.6703392445310286,0.828399516355805,False
4979,"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: if isinstance(command, str): command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() <IF_STMT> _errno, errmsg = ('UNKNOWN', e.args[0]) else: _errno, errmsg = e.args raise ConnectionError('Error %s while writing to socket. %s.' % (_errno, errmsg)) except Exception: self.disconnect() raise",if len(e.args) == 1:,if check_health:,0.7892011317257698,0.9298663600557577,False
4980,"def to_value(self, value): ret = {} for key, val in value.items(): if key in ['attachments', 'custom_attributes', 'description_diff']: ret[key] = val <IF_STMT> ret[key] = {k: {'from': v[0], 'to': v[1]} for k, v in val.items()} else: ret[key] = {'from': val[0], 'to': val[1]} return ret",elif key == 'points':,"elif isinstance(val, dict):",0.7534225572932722,0.8928756684056034,False
4981,"def to_child(cls, key=None, process=None): if process is not None: if type(process) is not dict: raise ValueError('Invalid value provided for ""process"" parameter, expected a dictionary') <IF_STMT> result = {} result.update(deepcopy(cls.__process__)) result.update(process) process = result  class Child(cls): __key__ = key __process__ = process __root__ = False Child.__name__ = cls.__name__ return Child",if cls.__process__:,if key is not None:,0.7501871639311588,0.8729118929672821,False
4982,"def _super_function(args): passed_class, passed_self = args.get_arguments(['type', 'self']) if passed_self is None: return passed_class else: pyclass = passed_class <IF_STMT> supers = pyclass.get_superclasses() if supers: return pyobjects.PyObject(supers[0]) return passed_self","if isinstance(pyclass, pyobjects.AbstractClass):","if isinstance(pyclass, pyobjects.PyObject):",0.8747654364540898,0.833078701050083,False
4983,"def get_data(row): data = [] for field_name, field_xpath in fields: result = row.xpath(field_xpath) <IF_STMT> result = ' '.join((text for text in map(six.text_type.strip, map(six.text_type, map(unescape, result))) if text)) else: result = None data.append(result) return data",if result:,if result:,0.7736025171255039,0.8935248372106969,True
4984,"def say(jarvis, s): """"""Reads what is typed."""""" if not s: jarvis.say('What should I say?') else: voice_state = jarvis.is_voice_enabled() jarvis.enable_voice() jarvis.say(s) <IF_STMT> jarvis.disable_voice()",if not voice_state:,if voice_state:,0.6424015233631606,0.8706099548745285,False
4985,"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: <IF_STMT> fromlist = list(fromlist) fromlist.remove('*') fromlist.extend(getattr(module, '__all__', [])) for x in fromlist: if isinstance(getattr(module, x, None), types.ModuleType): from_name = '{}.{}'.format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module",if '*' in fromlist:,if '*' in fromlist:,0.9420533489597912,0.8856327184319047,True
4986,"def _read_pricing_file(self, region=None, pricing_file=None): if not self.__pricing_file_cache: <IF_STMT> logging.info('Reading pricing file...') with open(pricing_file) as data_file: self.__pricing_file_cache = json.load(data_file) else: self.__pricing_file_cache = self._download_pricing_file(region) return self.__pricing_file_cache",if pricing_file:,if pricing_file:,0.8874433867145058,0.8466657105524215,True
